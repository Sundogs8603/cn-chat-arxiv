<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>CBQ&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37325;&#26500;&#22411;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;CBQ&#36890;&#36807;&#20351;&#29992;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#24314;&#31435;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;CBQ&#36824;&#37319;&#29992;&#20102;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#30340;&#21462;&#25972;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#24182;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2312.07950</link><description>&lt;p&gt;
&#36328;&#22359;&#37327;&#21270;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CBQ: Cross-Block Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.07950
&lt;/p&gt;
&lt;p&gt;
CBQ&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37325;&#26500;&#22411;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;CBQ&#36890;&#36807;&#20351;&#29992;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#24314;&#31435;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;CBQ&#36824;&#37319;&#29992;&#20102;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#30340;&#21462;&#25972;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#24182;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#22312;&#20197;&#26497;&#20302;&#25104;&#26412;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#21482;&#20851;&#27880;&#22788;&#29702;&#21333;&#20010;&#23618;&#25110;&#21333;&#20010;&#22359;&#20869;&#30340;&#24322;&#24120;&#20540;&#65292;&#24573;&#30053;&#20102;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#20302;&#20301;&#35774;&#32622;&#20013;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22359;&#38388;&#37325;&#26500;&#30340;&#36328;&#22359;PTQ&#26041;&#27861;CBQ&#12290;CBQ&#37319;&#29992;&#20102;&#19968;&#31181;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#23454;&#29616;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;&#27492;&#22806;&#65292;CBQ&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#65288;CFP&#65289;&#26469;&#25233;&#21046;&#26435;&#37325;&#21644;&#28608;&#27963;&#20540;&#30340;&#24322;&#24120;&#20540;&#65292;&#24182;&#37197;&#21512;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;LoRA&#21462;&#25972;&#25216;&#26415;&#23454;&#29616;&#31934;&#30830;&#30340;&#26435;&#37325;&#37327;&#21270;&#12290;&#36825;&#20123;&#21019;&#26032;&#20351;CBQ&#19981;&#20165;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#65292;&#36824;&#33021;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CBQ&#22312;&#20302;&#20301;&#37327;&#21270;&#65288;W4A4&#65292;W4A8&#31561;&#65289;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) has played a key role in compressing large language models (LLMs) with ultra-low costs. However, existing PTQ methods only focus on handling the outliers within one layer or one block, which ignores the dependency of blocks and leads to severe performance degradation in low-bit settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ method for LLMs. CBQ employs a cross-block dependency using a homologous reconstruction scheme, establishing long-range dependencies across multiple blocks to minimize error accumulation. Furthermore, CBQ incorporates a coarse-to-fine preprocessing (CFP) strategy for suppressing weight and activation outliers, coupled with an adaptive LoRA-Rounding technique for precise weight quantization. These innovations enable CBQ to not only handle extreme outliers effectively but also improve overall quantization accuracy. Extensive experiments show that CBQ achieves superior low-bit quantization (W4A4, W4A8, W
&lt;/p&gt;</description></item><item><title>Mini-Gemini &#25366;&#25496;&#20102;VLMs&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#39640;&#20998;&#36776;&#29575;&#35270;&#35273;&#26631;&#35760;&#12289;&#39640;&#36136;&#37327;&#25968;&#25454;&#21644;VLM&#24341;&#23548;&#29983;&#25104;&#31561;&#26041;&#24335;&#65292;&#32553;&#23567;&#20102;&#19982;&#20808;&#36827;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;</title><link>https://arxiv.org/abs/2403.18814</link><description>&lt;p&gt;
Mini-Gemini: &#25366;&#25496;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18814
&lt;/p&gt;
&lt;p&gt;
Mini-Gemini &#25366;&#25496;&#20102;VLMs&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#39640;&#20998;&#36776;&#29575;&#35270;&#35273;&#26631;&#35760;&#12289;&#39640;&#36136;&#37327;&#25968;&#25454;&#21644;VLM&#24341;&#23548;&#29983;&#25104;&#31561;&#26041;&#24335;&#65292;&#32553;&#23567;&#20102;&#19982;&#20808;&#36827;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Mini-Gemini&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22686;&#24378;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#12290;&#23613;&#31649;VLMs&#22312;&#20419;&#36827;&#22522;&#26412;&#35270;&#35273;&#23545;&#35805;&#21644;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#19982;GPT-4&#21644;Gemini&#31561;&#20808;&#36827;&#27169;&#22411;&#30456;&#27604;&#20173;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#20174;&#39640;&#20998;&#36776;&#29575;&#35270;&#35273;&#26631;&#35760;&#12289;&#39640;&#36136;&#37327;&#25968;&#25454;&#21644;VLM&#24341;&#23548;&#29983;&#25104;&#19977;&#20010;&#26041;&#38754;&#25366;&#25496;VLMs&#30340;&#28508;&#21147;&#65292;&#20197;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#12290;&#20026;&#20102;&#22686;&#24378;&#35270;&#35273;&#26631;&#35760;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#39069;&#22806;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#36827;&#34892;&#39640;&#20998;&#36776;&#29575;&#32454;&#21270;&#65292;&#32780;&#19981;&#22686;&#21152;&#35270;&#35273;&#26631;&#35760;&#25968;&#37327;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#20419;&#36827;&#31934;&#30830;&#30340;&#22270;&#20687;&#29702;&#35299;&#21644;&#22522;&#20110;&#25512;&#29702;&#30340;&#29983;&#25104;&#65292;&#25299;&#23637;&#20102;&#24403;&#21069;VLMs&#30340;&#25805;&#20316;&#33539;&#22260;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;Mini-Gemini&#36827;&#19968;&#27493;&#25366;&#25496;&#20102;VLMs&#30340;&#28508;&#21147;&#65292;&#24182;&#36171;&#20104;&#24403;&#21069;&#26694;&#26550;&#22270;&#20687;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#29983;&#25104;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18814v1 Announce Type: cross  Abstract: In this work, we introduce Mini-Gemini, a simple and effective framework enhancing multi-modality Vision Language Models (VLMs). Despite the advancements in VLMs facilitating basic visual dialog and reasoning, a performance gap persists compared to advanced models like GPT-4 and Gemini. We try to narrow the gap by mining the potential of VLMs for better performance and any-to-any workflow from three aspects, i.e., high-resolution visual tokens, high-quality data, and VLM-guided generation. To enhance visual tokens, we propose to utilize an additional visual encoder for high-resolution refinement without increasing the visual token count. We further construct a high-quality dataset that promotes precise image comprehension and reasoning-based generation, expanding the operational scope of current VLMs. In general, Mini-Gemini further mines the potential of VLMs and empowers current frameworks with image understanding, reasoning, and gen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#27169;&#22359;&#21270;&#26041;&#27861;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30452;&#25509;&#30340;&#26041;&#27861;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22359;&#22312;&#21516;&#31995;&#21015;PLM&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.18804</link><description>&lt;p&gt;
&#27169;&#22359;&#21270;&#21487;&#36801;&#31227;&#21527;&#65311;&#20197;&#30693;&#35782;&#33976;&#39311;&#35270;&#35282;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Is Modularity Transferable? A Case Study through the Lens of Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#27169;&#22359;&#21270;&#26041;&#27861;&#22312;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30452;&#25509;&#30340;&#26041;&#27861;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22359;&#22312;&#21516;&#31995;&#21015;PLM&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22359;&#21270;&#30340;&#20852;&#36215;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#27169;&#22359;&#21270;&#24050;&#34987;&#35777;&#26126;&#36866;&#29992;&#20110;&#21508;&#31181;&#29992;&#20363;&#65292;&#20174;&#39046;&#22495;&#33258;&#36866;&#24212;&#21040;&#22810;&#35821;&#35328;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#36825;&#39033;&#24037;&#20316;&#28085;&#30422;&#20102;&#27169;&#22359;&#32452;&#20214;&#22312;&#21333;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#20869;&#35757;&#32451;&#21644;&#37096;&#32626;&#30340;&#24773;&#20917;&#12290;&#36825;&#31181;&#29305;&#23450;&#20110;&#27169;&#22411;&#30340;&#35774;&#32622;&#23545;&#27169;&#22359;&#21270;&#26550;&#26500;&#35797;&#22270;&#23454;&#29616;&#30340;&#27169;&#22359;&#21270;&#26500;&#25104;&#20102;&#23454;&#36136;&#24615;&#38480;&#21046;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#24403;&#21069;&#27169;&#22359;&#21270;&#26041;&#27861;&#26159;&#21542;&#21487;&#20197;&#22312;&#27169;&#22411;&#20043;&#38388;&#21487;&#36801;&#31227;&#65292;&#20197;&#21450;&#25105;&#20204;&#26159;&#21542;&#21487;&#20197;&#23558;&#26356;&#24378;&#22823;&#21644;&#26356;&#22823;&#30340;PLM&#20013;&#30340;&#27169;&#22359;&#36716;&#31227;&#21040;&#26356;&#23567;&#30340;PLM&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24120;&#29992;&#20110;&#27169;&#22411;&#21387;&#32553;&#30340;&#30693;&#35782;&#33976;&#39311;&#38236;&#22836;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#30452;&#35266;&#30340;&#26041;&#27861;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#20219;&#21153;&#29305;&#23450;PEFT&#27169;&#22359;&#22312;&#21516;&#31995;&#21015;PLM&#20043;&#38388;&#36827;&#34892;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18804v1 Announce Type: new  Abstract: The rise of Modular Deep Learning showcases its potential in various Natural Language Processing applications. Parameter-efficient fine-tuning (PEFT) modularity has been shown to work for various use cases, from domain adaptation to multilingual setups. However, all this work covers the case where the modular components are trained and deployed within one single Pre-trained Language Model (PLM). This model-specific setup is a substantial limitation on the very modularity that modular architectures are trying to achieve. We ask whether current modular approaches are transferable between models and whether we can transfer the modules from more robust and larger PLMs to smaller ones. In this work, we aim to fill this gap via a lens of Knowledge Distillation, commonly used for model compression, and present an extremely straightforward approach to transferring pre-trained, task-specific PEFT modules between same-family PLMs. Moreover, we pro
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#29992;&#20110;&#38745;&#24577;&#35789;&#23884;&#20837;&#30340;&#26368;&#31616;&#21333;&#25237;&#24433;&#21435;&#20559;&#32622;&#26041;&#27861;&#24212;&#29992;&#20110;BERT&#20869;&#37096;&#34920;&#31034;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#26082;&#21487;&#20197;&#20943;&#23569;&#20869;&#22312;&#20559;&#35265;&#65292;&#21448;&#21487;&#20197;&#32531;&#35299;&#19979;&#28216;&#20219;&#21153;&#20013;&#35266;&#23519;&#21040;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.18803</link><description>&lt;p&gt;
&#29992;&#20110;&#32531;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24615;&#21035;&#20559;&#35265;&#30340;&#25237;&#24433;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Projective Methods for Mitigating Gender Bias in Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18803
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#29992;&#20110;&#38745;&#24577;&#35789;&#23884;&#20837;&#30340;&#26368;&#31616;&#21333;&#25237;&#24433;&#21435;&#20559;&#32622;&#26041;&#27861;&#24212;&#29992;&#20110;BERT&#20869;&#37096;&#34920;&#31034;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#26082;&#21487;&#20197;&#20943;&#23569;&#20869;&#22312;&#20559;&#35265;&#65292;&#21448;&#21487;&#20197;&#32531;&#35299;&#19979;&#28216;&#20219;&#21153;&#20013;&#35266;&#23519;&#21040;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#32531;&#35299;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#19982;&#21435;&#20559;&#32622;&#38745;&#24577;&#35789;&#23884;&#20837;&#26377;&#30528;&#24736;&#20037;&#30340;&#21382;&#21490;&#12290;&#26368;&#36817;&#65292;&#27880;&#24847;&#21147;&#36716;&#21521;&#20102;&#21435;&#20559;&#32622;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#31616;&#21333;&#30340;&#25237;&#24433;&#21435;&#20559;&#32622;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;BERT&#20869;&#37096;&#34920;&#31034;&#26102;&#33021;&#22815;&#24110;&#21161;&#21040;&#20160;&#20040;&#31243;&#24230;&#12290;&#25237;&#24433;&#26041;&#27861;&#23454;&#29616;&#24555;&#36895;&#65292;&#20351;&#29992;&#23569;&#37327;&#20445;&#23384;&#30340;&#21442;&#25968;&#65292;&#24182;&#19988;&#19981;&#23545;&#29616;&#26377;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#26356;&#26032;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#20943;&#23569;BERT&#23545;&#20869;&#22312;&#20559;&#35265;&#30340;&#25928;&#26524;&#65292;&#36890;&#36807;BERT&#19979;&#19968;&#20010;&#21477;&#23376;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#27979;&#37327;&#65292;&#20197;&#21450;&#22312;&#24494;&#35843;&#19979;&#28216;&#20219;&#21153;&#26102;&#32531;&#35299;&#35266;&#23519;&#21040;&#30340;&#20559;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36824;&#23545;&#29992;&#20110;&#37327;&#21270;&#20869;&#22312;&#20559;&#35265;&#30340;&#27969;&#34892;&#24615;&#21035;&#20559;&#35265;&#35780;&#20272;&#27979;&#35797;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;&#27979;&#35797;&#38598;&#21644;&#26032;&#30340;&#20559;&#35265;&#27979;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#25237;&#24433;&#26041;&#27861;&#22312;&#20943;&#23569;&#20869;&#22312;&#20559;&#35265;&#21644;&#32531;&#35299;&#19979;&#28216;&#20559;&#35265;&#26041;&#38754;&#37117;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18803v1 Announce Type: new  Abstract: Mitigation of gender bias in NLP has a long history tied to debiasing static word embeddings. More recently, attention has shifted to debiasing pre-trained language models. We study to what extent the simplest projective debiasing methods, developed for word embeddings, can help when applied to BERT's internal representations. Projective methods are fast to implement, use a small number of saved parameters, and make no updates to the existing model parameters. We evaluate the efficacy of the methods in reducing both intrinsic bias, as measured by BERT's next sentence prediction task, and in mitigating observed bias in a downstream setting when fine-tuned. To this end, we also provide a critical analysis of a popular gender-bias assessment test for quantifying intrinsic bias, resulting in an enhanced test set and new bias measures. We find that projective methods can be effective at both intrinsic bias and downstream bias mitigation, but 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.18802</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Long-form factuality in large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#24320;&#25918;&#24615;&#20027;&#39064;&#30340;&#20107;&#23454;&#24615;&#25552;&#31034;&#26102;&#65292;&#32463;&#24120;&#29983;&#25104;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#22312;&#24320;&#25918;&#39046;&#22495;&#20013;&#23545;&#27169;&#22411;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;GPT-4&#29983;&#25104;&#20102;&#19968;&#20010;&#21517;&#20026;LongFact&#30340;&#25552;&#31034;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#21315;&#20010;&#22218;&#25324;38&#20010;&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;Search-Augmented Factuality Evaluator&#65288;SAFE&#65289;&#30340;&#26041;&#27861;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#33258;&#21160;&#35780;&#20272;&#22120;&#12290;SAFE&#21033;&#29992;LLM&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#19968;&#32452;&#21333;&#29420;&#30340;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#20197;&#21450;&#30830;&#23450;&#19968;&#20010;&#20107;&#23454;&#26159;&#21542;&#24471;&#21040;&#25628;&#32034;&#32467;&#26524;&#25903;&#25345;&#30340;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#26469;&#35780;&#20272;&#27599;&#20010;&#20107;&#23454;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#35758;&#23558;F1&#20998;&#25968;&#25193;&#23637;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24179;&#34913;&#20102;&#22238;&#24212;&#20013;&#25903;&#25345;&#20107;&#23454;&#30340;&#30334;&#20998;&#27604;&#65288;&#31934;&#24230;&#65289;&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18802v1 Announce Type: cross  Abstract: Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20026;&#35774;&#22791;&#19978;&#30340;&#34394;&#25311;&#21161;&#25163;&#25171;&#36896;&#19990;&#30028;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#33521;&#35821;&#30340;&#21306;&#22495;&#21464;&#20307;&#65292;&#37319;&#29992;&#36866;&#37197;&#22120;&#29942;&#39048;&#26469;&#27169;&#25311;&#29305;&#23450;&#26041;&#35328;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24615;&#12289;&#24310;&#36831;&#21644;&#20869;&#23384;&#26041;&#38754;&#30340;&#24615;&#33021;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.18783</link><description>&lt;p&gt;
&#20026;&#35774;&#22791;&#19978;&#30340;&#34394;&#25311;&#21161;&#25163;&#25171;&#36896;&#19990;&#30028;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards a World-English Language Model for On-Device Virtual Assistants
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20026;&#35774;&#22791;&#19978;&#30340;&#34394;&#25311;&#21161;&#25163;&#25171;&#36896;&#19990;&#30028;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#33521;&#35821;&#30340;&#21306;&#22495;&#21464;&#20307;&#65292;&#37319;&#29992;&#36866;&#37197;&#22120;&#29942;&#39048;&#26469;&#27169;&#25311;&#29305;&#23450;&#26041;&#35328;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24615;&#12289;&#24310;&#36831;&#21644;&#20869;&#23384;&#26041;&#38754;&#30340;&#24615;&#33021;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#21161;&#25163;&#65288;VAs&#65289;&#30340;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#65288;NNLMs&#65289;&#36890;&#24120;&#26159;&#22522;&#20110;&#35821;&#35328;&#12289;&#22320;&#21306;&#65292;&#26377;&#26102;&#29978;&#33267;&#36824;&#26377;&#35774;&#22791;&#30340;&#20381;&#36182;&#24615;&#65292;&#36825;&#22686;&#21152;&#20102;&#25193;&#23637;&#21644;&#32500;&#25252;&#30340;&#24037;&#20316;&#37327;&#12290;&#23558;NNLMs&#32452;&#21512;&#29992;&#20110;&#19968;&#20010;&#25110;&#22810;&#20010;&#31867;&#21035;&#26159;&#25913;&#21892;&#25193;&#23637;&#24615;&#30340;&#19968;&#31181;&#26041;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#21306;&#22495;&#24615;&#33521;&#35821;&#21464;&#20307;&#26500;&#24314;&#20102;&#19968;&#20010;&#8220;&#19990;&#30028;&#33521;&#35821;&#8221;NNLM&#65292;&#29992;&#20110;&#35774;&#22791;&#19978;&#30340;VAs&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36866;&#37197;&#22120;&#29942;&#39048;&#30340;&#24212;&#29992;&#65292;&#20197;&#27169;&#25311;&#25105;&#20204;&#29616;&#26377;&#29983;&#20135;NNLMS&#20013;&#30340;&#29305;&#23450;&#26041;&#35328;&#29305;&#24449;{&#24182;&#22686;&#24378;&#22810;&#26041;&#35328;&#22522;&#32447;}&#12290;&#25105;&#20204;&#21457;&#29616;&#36866;&#37197;&#22120;&#27169;&#22359;&#22312;&#27169;&#25311;&#26041;&#35328;&#26041;&#38754;&#27604;&#19987;&#38376;&#21270;&#25972;&#20010;&#23376;&#32593;&#32476;&#26356;&#26377;&#25928;&#12290;&#22522;&#20110;&#36825;&#19968;&#35265;&#35299;&#65292;&#24182;&#21033;&#29992;&#25105;&#20204;&#29983;&#20135;&#27169;&#22411;&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#19990;&#30028;&#33521;&#35821;NNLM&#26550;&#26500;&#65292;&#31526;&#21512;&#25105;&#20204;&#21333;&#26041;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#24310;&#36831;&#21644;&#20869;&#23384;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18783v1 Announce Type: new  Abstract: Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are generally language-, region-, and in some cases, device-dependent, which increases the effort to scale and maintain them. Combining NNLMs for one or more of the categories is one way to improve scalability. In this work, we combine regional variants of English to build a ``World English'' NNLM for on-device VAs. In particular, we investigate the application of adapter bottlenecks to model dialect-specific characteristics in our existing production NNLMs {and enhance the multi-dialect baselines}. We find that adapter modules are more effective in modeling dialects than specializing entire sub-networks. Based on this insight and leveraging the design of our production models, we introduce a new architecture for World English NNLM that meets the accuracy, latency, and memory constraints of our single-dialect models.
&lt;/p&gt;</description></item><item><title>CheckEval&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#35814;&#32454;&#30340;&#23376;&#26041;&#38754;&#21644;&#24067;&#23572;&#38382;&#39064;&#28165;&#21333;&#31616;&#21270;&#20102;&#35780;&#20272;&#36807;&#31243;&#65292;&#22686;&#24378;&#20102;&#35780;&#20272;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#36890;&#36807;SummEval&#22522;&#20934;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18771</link><description>&lt;p&gt;
CheckEval: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#28165;&#21333;&#26500;&#24314;&#20581;&#22766;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CheckEval: Robust Evaluation Framework using Large Language Model via Checklist
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18771
&lt;/p&gt;
&lt;p&gt;
CheckEval&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#35814;&#32454;&#30340;&#23376;&#26041;&#38754;&#21644;&#24067;&#23572;&#38382;&#39064;&#28165;&#21333;&#31616;&#21270;&#20102;&#35780;&#20272;&#36807;&#31243;&#65292;&#22686;&#24378;&#20102;&#35780;&#20272;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#36890;&#36807;SummEval&#22522;&#20934;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CheckEval&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#35780;&#20272;&#26041;&#27861;&#20013;&#30340;&#27495;&#20041;&#21644;&#19981;&#19968;&#33268;&#24615;&#25361;&#25112;&#12290;CheckEval&#36890;&#36807;&#23558;&#35780;&#20272;&#26631;&#20934;&#20998;&#35299;&#20026;&#35814;&#32454;&#30340;&#23376;&#26041;&#38754;&#65292;&#24182;&#20026;&#27599;&#20010;&#26500;&#24314;&#24067;&#23572;&#38382;&#39064;&#28165;&#21333;&#65292;&#31616;&#21270;&#20102;&#35780;&#20272;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20351;&#36807;&#31243;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#36890;&#36807;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#35780;&#20272;&#32500;&#24230;&#26174;&#30528;&#22686;&#24378;&#20102;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;SummEval&#22522;&#20934;&#36827;&#34892;&#30340;&#19987;&#27880;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#65292;CheckEval&#26174;&#31034;&#20986;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#23637;&#31034;&#20102;&#39640;&#24230;&#19968;&#33268;&#30340;&#20114;&#27880;&#32773;&#19968;&#33268;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;CheckEval&#22312;&#23458;&#35266;&#12289;&#28789;&#27963;&#21644;&#31934;&#30830;&#35780;&#20272;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#25552;&#20379;&#21487;&#23450;&#21046;&#21644;&#20114;&#21160;&#30340;&#26694;&#26550;&#65292;CheckEval&#20026;LL&#30340;&#20351;&#29992;&#35774;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18771v1 Announce Type: new  Abstract: We introduce CheckEval, a novel evaluation framework using Large Language Models, addressing the challenges of ambiguity and inconsistency in current evaluation methods. CheckEval addresses these challenges by dividing evaluation criteria into detailed sub-aspects and constructing a checklist of Boolean questions for each, simplifying the evaluation. This approach not only renders the process more interpretable but also significantly enhances the robustness and reliability of results by focusing on specific evaluation dimensions. Validated through a focused case study using the SummEval benchmark, CheckEval indicates a strong correlation with human judgments. Furthermore, it demonstrates a highly consistent Inter-Annotator Agreement. These findings highlight the effectiveness of CheckEval for objective, flexible, and precise evaluations. By offering a customizable and interactive framework, CheckEval sets a new standard for the use of LL
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#21453;&#23556;&#26469;&#39044;&#27979;&#31062;&#35821;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#31070;&#32463;&#21407;&#22411;&#37325;&#24314;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.18769</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#23556;&#39044;&#27979;&#25913;&#36827;&#31070;&#32463;&#21407;&#22411;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Improved Neural Protoform Reconstruction via Reflex Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18769
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#21453;&#23556;&#26469;&#39044;&#27979;&#31062;&#35821;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#31070;&#32463;&#21407;&#22411;&#37325;&#24314;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Protolanguage&#37325;&#24314;&#23545;&#21382;&#21490;&#35821;&#35328;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#27604;&#36739;&#26041;&#27861;&#26159;&#35821;&#35328;&#31185;&#23398;&#21490;&#19978;&#26368;&#26377;&#24433;&#21709;&#21147;&#30340;&#29702;&#35770;&#21644;&#26041;&#27861;&#35770;&#26694;&#26550;&#20043;&#19968;&#65292;&#20801;&#35768;&#35821;&#35328;&#23398;&#23478;&#26681;&#25454;&#23450;&#26399;&#30340;&#22768;&#38899;&#21464;&#21270;&#30340;&#20551;&#35774;&#20174;&#21453;&#23556;&#20851;&#31995;&#30340;&#29616;&#20195;&#35789;&#20013;&#25512;&#26029;&#20986;&#21407;&#22411;&#24418;&#24335;&#65288;&#37325;&#24314;&#30340;&#31062;&#20808;&#35789;&#65289;&#12290;&#20247;&#22810;&#35745;&#31639;&#35821;&#35328;&#23398;&#23478;&#35797;&#22270;&#36890;&#36807;&#21508;&#31181;&#35745;&#31639;&#27169;&#22411;&#23454;&#29616;&#27604;&#36739;&#37325;&#24314;&#65292;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#26368;&#25104;&#21151;&#30340;&#26159;&#30417;&#30563;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23558;&#22312;&#32473;&#23450;&#19968;&#32452;&#21453;&#23556;&#35789;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#21407;&#22411;&#24418;&#24335;&#20316;&#20026;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#19968;&#26694;&#26550;&#24573;&#30053;&#20102;&#27604;&#36739;&#26041;&#27861;&#20013;&#26368;&#37325;&#35201;&#30340;&#19968;&#20010;&#26041;&#38754;&#65306;&#19981;&#20165;&#24212;&#35813;&#21487;&#20197;&#20174;&#21516;&#28304;&#35789;&#32452;&#65288;&#30456;&#20851;&#21453;&#23556;&#30340;&#38598;&#21512;&#65289;&#25512;&#26029;&#20986;&#21407;&#22411;&#24418;&#24335;&#65292;&#21516;&#26102;&#36824;&#24212;&#35813;&#33021;&#22815;&#20174;&#21407;&#22411;&#24418;&#24335;&#25512;&#26029;&#20986;&#21453;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18769v1 Announce Type: new  Abstract: Protolanguage reconstruction is central to historical linguistics. The comparative method, one of the most influential theoretical and methodological frameworks in the history of the language sciences, allows linguists to infer protoforms (reconstructed ancestral words) from their reflexes (related modern words) based on the assumption of regular sound change. Not surprisingly, numerous computational linguists have attempted to operationalize comparative reconstruction through various computational models, the most successful of which have been supervised encoder-decoder models, which treat the problem of predicting protoforms given sets of reflexes as a sequence-to-sequence problem. We argue that this framework ignores one of the most important aspects of the comparative method: not only should protoforms be inferable from cognate sets (sets of related reflexes) but the reflexes should also be inferable from the protoforms. Leveraging a
&lt;/p&gt;</description></item><item><title>CYCLE&#26694;&#26550;&#25552;&#20986;&#20102;&#23398;&#20064;&#22914;&#20309;&#26681;&#25454;&#21487;&#29992;&#21453;&#39304;&#65292;&#22914;&#27979;&#35797;&#22871;&#20214;&#25253;&#21578;&#30340;&#25191;&#34892;&#32467;&#26524;&#65292;&#33258;&#25105;&#35843;&#25972;&#38169;&#35823;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#33258;&#25105;&#35843;&#25972;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.18746</link><description>&lt;p&gt;
CYCLE: &#23398;&#20064;&#33258;&#25105;&#35843;&#25972;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CYCLE: Learning to Self-Refine the Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18746
&lt;/p&gt;
&lt;p&gt;
CYCLE&#26694;&#26550;&#25552;&#20986;&#20102;&#23398;&#20064;&#22914;&#20309;&#26681;&#25454;&#21487;&#29992;&#21453;&#39304;&#65292;&#22914;&#27979;&#35797;&#22871;&#20214;&#25253;&#21578;&#30340;&#25191;&#34892;&#32467;&#26524;&#65292;&#33258;&#25105;&#35843;&#25972;&#38169;&#35823;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#33258;&#25105;&#35843;&#25972;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#39640;&#20102;&#20154;&#31867;&#24320;&#21457;&#32773;&#30340;&#32534;&#31243;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#23545;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#36890;&#24120;&#24573;&#35270;&#20102;&#23427;&#20204;&#30340;&#33258;&#25105;&#35843;&#25972;&#33021;&#21147;&#65292;&#32780;&#26159;&#21482;&#20851;&#27880;&#19968;&#27425;&#24615;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#26410;&#33021;&#23454;&#29616;&#27491;&#30830;&#31243;&#24207;&#30340;&#24773;&#20917;&#19979;&#65292;&#24320;&#21457;&#32773;&#23454;&#38469;&#19978;&#24456;&#38590;&#35843;&#35797;&#21644;&#20462;&#22797;&#38169;&#35823;&#30340;&#39044;&#27979;&#65292;&#22240;&#20026;&#36825;&#20123;&#39044;&#27979;&#19981;&#26159;&#30001;&#24320;&#21457;&#32773;&#33258;&#24049;&#32534;&#20889;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20063;&#26080;&#27861;&#39640;&#25928;&#22320;&#33258;&#25105;&#35843;&#25972;&#20854;&#38169;&#35823;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18746v1 Announce Type: cross  Abstract: Pre-trained code language models have achieved promising performance in code generation and improved the programming efficiency of human developers. However, their self-refinement capability is typically overlooked by the existing evaluations of code LMs, which focus only on the accuracy of the one-time prediction. For the cases when code LMs fail to implement the correct program, developers actually find it hard to debug and fix the faulty prediction since it is not written by the developers themselves. Unfortunately, our study reveals that code LMs cannot efficiently self-refine their faulty generations as well.   In this paper, we propose CYCLE framework, learning to self-refine the faulty generation according to the available feedback, such as the execution results reported by the test suites. We evaluate CYCLE on three popular code generation benchmarks, HumanEval, MBPP, and APPS. The results reveal that CYCLE successfully maintai
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25351;&#31034;&#23545;&#27604;&#35299;&#30721;(ICD)&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#24187;&#35273;&#65292;&#36890;&#36807;&#23545;&#26631;&#20934;&#21644;&#25351;&#31034;&#25200;&#21160;&#30340;&#20998;&#24067;&#36827;&#34892;&#23545;&#27604;&#65292;&#20174;&#21407;&#22987;&#20998;&#24067;&#20013;&#20943;&#21435;&#24187;&#35273;&#27010;&#24565;&#12290;</title><link>https://arxiv.org/abs/2403.18715</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#31034;&#23545;&#27604;&#35299;&#30721;&#20943;&#36731;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Mitigating Hallucinations in Large Vision-Language Models with Instruction Contrastive Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18715
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25351;&#31034;&#23545;&#27604;&#35299;&#30721;(ICD)&#26041;&#27861;&#65292;&#26088;&#22312;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#25512;&#26029;&#36807;&#31243;&#20013;&#30340;&#24187;&#35273;&#65292;&#36890;&#36807;&#23545;&#26631;&#20934;&#21644;&#25351;&#31034;&#25200;&#21160;&#30340;&#20998;&#24067;&#36827;&#34892;&#23545;&#27604;&#65292;&#20174;&#21407;&#22987;&#20998;&#24067;&#20013;&#20943;&#21435;&#24187;&#35273;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#36234;&#26469;&#36234;&#25797;&#38271;&#20174;&#35270;&#35273;&#36755;&#20837;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#32454;&#33410;&#21644;&#36830;&#36143;&#24615;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#27169;&#24335;&#20915;&#31574;&#21644;&#24320;&#25918;&#24335;&#29983;&#25104;&#20013;&#24212;&#29992;&#23427;&#20204;&#26102;&#65292;&#20854;&#24212;&#29992;&#21463;&#21040;&#24187;&#35273;&#30340;&#38459;&#30861;&#65292;&#21363;&#29983;&#25104;&#30340;&#25991;&#26412;&#19981;&#20934;&#30830;&#22320;&#20195;&#34920;&#20102;&#35270;&#35273;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#25351;&#31034;&#23545;&#27604;&#35299;&#30721;(ICD)&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#22312;LVLM&#25512;&#26029;&#36807;&#31243;&#20013;&#20943;&#23569;&#24187;&#35273;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#25105;&#20204;&#35266;&#23519;&#21040;&#30340;&#25200;&#21160;&#25351;&#31034;&#26174;&#33879;&#21152;&#21095;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#20013;&#30340;&#24187;&#35273;&#30340;&#21551;&#21457;&#12290;ICD&#23545;&#26631;&#20934;&#21644;&#25351;&#31034;&#25200;&#21160;&#30340;&#20998;&#24067;&#36827;&#34892;&#23545;&#27604;&#65292;&#20174;&#32780;&#22686;&#21152;&#23545;&#40784;&#19981;&#30830;&#23450;&#24615;&#24182;&#26377;&#25928;&#22320;&#20174;&#21407;&#22987;&#20998;&#24067;&#20013;&#20943;&#21435;&#24187;&#35273;&#27010;&#24565;&#12290;&#36890;&#36807;&#22312;&#21028;&#21035;&#22522;&#20934;(POPE&#21644;MME)&#21644;&#29983;&#25104;&#22522;&#20934;&#19978;&#36827;&#34892;&#20840;&#38754;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18715v1 Announce Type: cross  Abstract: Large Vision-Language Models (LVLMs) are increasingly adept at generating contextually detailed and coherent responses from visual inputs. However, their application in multimodal decision-making and open-ended generation is hindered by a notable rate of hallucinations, where generated text inaccurately represents the visual contents. To address this issue, this paper introduces the Instruction Contrastive Decoding (ICD) method, a novel approach designed to reduce hallucinations during LVLM inference. Our method is inspired by our observation that what we call disturbance instructions significantly exacerbate hallucinations in multimodal fusion modules. ICD contrasts distributions from standard and instruction disturbance, thereby increasing alignment uncertainty and effectively subtracting hallucinated concepts from the original distribution. Through comprehensive experiments on discriminative benchmarks (POPE and MME) and a generativ
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#35821;&#30340;&#25968;&#23398;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20026;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.18697</link><description>&lt;p&gt;
Invalsi&#22522;&#20934;&#65306;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#35821;&#30340;&#25968;&#23398;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Invalsi Benchmark: measuring Language Models Mathematical and Language understanding in Italian
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18697
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#35821;&#30340;&#25968;&#23398;&#21644;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20026;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24847;&#22823;&#21033;&#35821;&#22312;&#25152;&#26377;&#25351;&#26631;&#19978;&#37117;&#26159;&#19968;&#31181;&#39640;&#36164;&#28304;&#35821;&#35328;&#65292;&#20294;&#30446;&#21069;&#24182;&#27809;&#26377;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#35813;&#35821;&#35328;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#23548;&#33268;&#20102;&#21487;&#29992;&#20110;&#35780;&#20272;&#24847;&#22823;&#21033;&#35821;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#22522;&#20934;&#25968;&#30446;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#24847;&#22823;&#21033;&#35821;&#30340;&#25968;&#23398;&#29702;&#35299;&#21644;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#22522;&#20934;&#22522;&#20110;&#24847;&#22823;&#21033;&#23398;&#26657;&#31995;&#32479;&#20869;11&#33267;18&#23681;&#23398;&#29983;&#36827;&#34892;&#30340;&#23454;&#38469;&#27979;&#35797;&#65292;&#24182;&#24050;&#30001;&#22810;&#20301;&#25945;&#23398;&#21644;&#25945;&#32946;&#23398;&#19987;&#23478;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18697v1 Announce Type: new  Abstract: While Italian is by all metrics a high resource language, currently, there are isn't a Language Model pre-trained exclusively in this language. This results in a lower number of available benchmarks to evaluate the performance of language models in Italian.   This work presents two new benchmarks to evaluate the models performance on mathematical understanding and language understanding in Italian. These benchmarks are based on real tests that are undertaken by students of age between 11 and 18 within the Italian school system and have therefore been validated by several experts in didactics and pedagogy.   To validate this dataset we evaluate the performance of 9 language models that are the best performing when writing in Italian, including our own fine-tuned models. We show that this is a challenging benchmark where current language models are bound by 60\% accuracy.   We believe that the release of this dataset paves the way for impr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#36981;&#24490;&#20854;&#20182;&#31070;&#32463;&#27169;&#22411;&#30340;&#32553;&#25918;&#35268;&#24459;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#23545;&#25968;&#20284;&#28982;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.18684</link><description>&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#30340;&#25193;&#23637;&#35268;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws For Dense Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18684
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#31350;&#20102;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#36981;&#24490;&#20854;&#20182;&#31070;&#32463;&#27169;&#22411;&#30340;&#32553;&#25918;&#35268;&#24459;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#23545;&#25968;&#20284;&#28982;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#31070;&#32463;&#27169;&#22411;&#25193;&#23637;&#21040;&#26356;&#22823;&#35268;&#27169;&#24050;&#32463;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#31070;&#32463;&#27169;&#22411;&#30340;&#24615;&#33021;&#24120;&#36981;&#24490;&#21487;&#39044;&#27979;&#30340;&#25193;&#23637;&#35268;&#24459;&#65292;&#19982;&#35757;&#32451;&#38598;&#22823;&#23567;&#21644;&#27169;&#22411;&#22823;&#23567;&#31561;&#22240;&#32032;&#30456;&#20851;&#12290;&#36825;&#19968;&#27934;&#23519;&#21147;&#38750;&#24120;&#23453;&#36149;&#65292;&#23588;&#20854;&#26159;&#38543;&#30528;&#22823;&#35268;&#27169;&#23454;&#39564;&#21464;&#24471;&#36234;&#26469;&#36234;&#32791;&#36153;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26816;&#32034;&#25351;&#26631;&#30340;&#31163;&#25955;&#24615;&#20197;&#21450;&#26816;&#32034;&#20219;&#21153;&#20013;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#22823;&#23567;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#23494;&#38598;&#26816;&#32034;&#20013;&#30340;&#36825;&#31181;&#25193;&#23637;&#35268;&#24459;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#21542;&#36981;&#24490;&#20854;&#20182;&#31070;&#32463;&#27169;&#22411;&#30340;&#32553;&#25918;&#35268;&#24459;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#23545;&#27604;&#23545;&#25968;&#20284;&#28982;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#23545;&#23454;&#29616;&#20102;&#19981;&#21516;&#21442;&#25968;&#25968;&#37327;&#24182;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18684v1 Announce Type: cross  Abstract: Scaling up neural models has yielded significant advancements in a wide array of tasks, particularly in language generation. Previous studies have found that the performance of neural models frequently adheres to predictable scaling laws, correlated with factors such as training set size and model size. This insight is invaluable, especially as large-scale experiments grow increasingly resource-intensive. Yet, such scaling law has not been fully explored in dense retrieval due to the discrete nature of retrieval metrics and complex relationships between training data and model sizes in retrieval tasks. In this study, we investigate whether the performance of dense retrieval models follows the scaling law as other neural models. We propose to use contrastive log-likelihood as the evaluation metric and conduct extensive experiments with dense retrieval models implemented with different numbers of parameters and trained with different amo
&lt;/p&gt;</description></item><item><title>NL-ITI&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#25506;&#27979;&#21644;&#22810;&#20196;&#29260;&#24178;&#39044;&#65292;&#25104;&#21151;&#25913;&#36827;&#20102;ITI&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#22810;&#36873;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.18680</link><description>&lt;p&gt;
NL-ITI&#65306;&#20248;&#21270;&#25506;&#27979;&#21644;&#24178;&#39044;&#20197;&#25913;&#36827;ITI&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NL-ITI: Optimizing Probing and Intervention for Improvement of ITI Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18680
&lt;/p&gt;
&lt;p&gt;
NL-ITI&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#25506;&#27979;&#21644;&#22810;&#20196;&#29260;&#24178;&#39044;&#65292;&#25104;&#21151;&#25913;&#36827;&#20102;ITI&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#22810;&#36873;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#23481;&#26131;&#36820;&#22238;&#34394;&#20551;&#20449;&#24687;&#65292;&#36825;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#25512;&#29702;&#26102;&#24178;&#39044;(Inference-Time-Intervention, ITI)&#26041;&#27861;&#24341;&#20837;&#30340;&#33539;&#24335;&#12290;&#39318;&#20808;&#65292;ITI&#26041;&#27861;&#35782;&#21035;&#21253;&#21547;&#26368;&#22810;&#25152;&#38656;&#30693;&#35782;&#31867;&#22411;(&#20363;&#22914;&#30495;&#23454;&#20449;&#24687;)&#30340;&#27880;&#24847;&#21147;&#22836;&#12290;&#38543;&#21518;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;LLM&#28608;&#27963;&#34987;&#31227;&#21160;&#21040;&#25152;&#36873;&#27880;&#24847;&#21147;&#22836;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#38750;&#32447;&#24615;&#25506;&#27979;&#21644;&#22810;&#20196;&#29260;&#24178;&#39044;-&#38750;&#32447;&#24615;ITI(NL-ITI)&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;ITI&#26694;&#26550;&#12290;NL-ITI&#22312;&#22810;&#20010;&#22810;&#36873;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21253;&#25324;TruthfulQA&#65292;&#25105;&#20204;&#22312;&#36825;&#39033;&#22522;&#20934;&#19978;&#30456;&#23545;&#20110;&#22522;&#32447;ITI&#32467;&#26524;&#25253;&#21578;&#20102;&#32422;14%&#30340;MC1&#25351;&#26631;&#25913;&#36827;&#12290;NL-ITI&#36824;&#22312;&#20854;&#20182;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#25104;&#32489;-&#22312;MMLU&#30340;&#21830;&#19994;&#20262;&#29702;&#23376;&#39046;&#22495;&#19978;&#65292;&#27604;&#22522;&#32447;LLaMA2-7B&#26377;&#32422;18%&#30340;MC1&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;NL-ITI&#22312;&#25928;&#26524;&#26356;&#22909;&#30340;&#21516;&#26102;&#20063;&#26356;&#23569;&#20405;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18680v1 Announce Type: new  Abstract: Large Language Models (LLM) are prone to returning false information. It constitutes one of major challenges in the AI field. In our work, we explore paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it identifies attention heads, which contain the highest amount of desired type of knowledge (e.g., truthful). Afterwards, during inference, LLM activations are shifted for chosen subset of attention heads. We further improved the ITI framework by introducing a nonlinear probing and multi-token intervention - Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice benchmarks, including TruthfulQA, on which we report around 14% MC1 metric improvement with respect to the baseline ITI results. NL-ITI achieves also encouraging results on other testsets - on Business Ethics subdomain of MMLU, around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI performs better while being less invasive in t
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#36229;&#36234;&#35757;&#32451;&#38598;&#30340;&#20107;&#23454;&#26680;&#26597;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#20986;&#26032;&#39062;&#30340;&#23545;&#25239;&#31639;&#27861;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#20107;&#23454;&#26680;&#26597;&#27969;&#31243;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#26356;&#21152;&#31283;&#20581;&#12290;</title><link>https://arxiv.org/abs/2403.18671</link><description>&lt;p&gt;
&#36229;&#36234;&#35757;&#32451;&#38598;&#30340;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
Fact Checking Beyond Training Set
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18671
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#36229;&#36234;&#35757;&#32451;&#38598;&#30340;&#20107;&#23454;&#26680;&#26597;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#20986;&#26032;&#39062;&#30340;&#23545;&#25239;&#31639;&#27861;&#21644;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#20107;&#23454;&#26680;&#26597;&#27969;&#31243;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26085;&#24120;&#35328;&#35770;&#30340;&#30495;&#23454;&#24615;&#32791;&#26102;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#35777;&#26126;&#65292;&#24120;&#29992;&#30340;&#20107;&#23454;&#26680;&#26597;&#27969;&#31243;&#65292;&#21363;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#65292;&#22312;&#20351;&#29992;&#19968;&#20010;&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21518;&#65292;&#24403;&#22312;&#21478;&#19968;&#20010;&#39046;&#22495;&#20013;&#20351;&#29992;&#26102;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#27969;&#31243;&#30340;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#31639;&#27861;&#65292;&#20351;&#26816;&#32034;&#22120;&#32452;&#20214;&#33021;&#22815;&#25269;&#24481;&#20998;&#24067;&#36716;&#31227;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#39318;&#20808;&#22312;&#26631;&#35760;&#30340;&#28304;&#25968;&#25454;&#19978;&#35757;&#32451;&#19968;&#20010;&#21452;&#32534;&#30721;&#22120;&#65292;&#28982;&#21518;&#36890;&#36807;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#23545;&#20004;&#20010;&#29420;&#31435;&#30340;&#25991;&#26723;&#32534;&#30721;&#22120;&#21644;&#35328;&#35770;&#32534;&#30721;&#22120;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#12290;&#28982;&#21518;&#25105;&#20204;&#19987;&#27880;&#20110;&#38405;&#35835;&#22120;&#32452;&#20214;&#65292;&#24182;&#24314;&#35758;&#35757;&#32451;&#23427;&#20197;&#23545;&#35328;&#35770;&#21644;&#35777;&#25454;&#25991;&#26723;&#30340;&#39034;&#24207;&#19981;&#25935;&#24863;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#25903;&#25345;&#36825;&#26679;&#19968;&#20010;&#38405;&#35835;&#22120;&#26174;&#31034;&#20986;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18671v1 Announce Type: new  Abstract: Evaluating the veracity of everyday claims is time consuming and in some cases requires domain expertise. We empirically demonstrate that the commonly used fact checking pipeline, known as the retriever-reader, suffers from performance deterioration when it is trained on the labeled data from one domain and used in another domain. Afterwards, we delve into each component of the pipeline and propose novel algorithms to address this problem. We propose an adversarial algorithm to make the retriever component robust against distribution shift. Our core idea is to initially train a bi-encoder on the labeled source data, and then, to adversarially train two separate document and claim encoders using unlabeled target data. We then focus on the reader component and propose to train it such that it is insensitive towards the order of claims and evidence documents. Our empirical evaluations support the hypothesis that such a reader shows a higher
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#35821;&#20041;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#29992;&#25143;-&#39033;&#30446;&#21644;&#39033;&#30446;-&#39033;&#30446;&#20132;&#20114;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#22810;&#26679;&#21270;&#30340;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2403.18667</link><description>&lt;p&gt;
&#25913;&#36827;&#20869;&#23481;&#25512;&#33616;&#65306;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#35821;&#20041;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#22810;&#26679;&#24615;&#21644;&#20919;&#21551;&#21160;&#29992;&#25143;
&lt;/p&gt;
&lt;p&gt;
Improving Content Recommendation: Knowledge Graph-Based Semantic Contrastive Learning for Diversity and Cold-Start Users
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18667
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#35821;&#20041;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#29992;&#25143;-&#39033;&#30446;&#21644;&#39033;&#30446;-&#39033;&#30446;&#20132;&#20114;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#22810;&#26679;&#21270;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#19982;&#25968;&#25454;&#31232;&#30095;&#24615;&#12289;&#20919;&#21551;&#21160;&#38382;&#39064;&#21644;&#25512;&#33616;&#31995;&#32479;&#22810;&#26679;&#24615;&#30456;&#20851;&#30340;&#25361;&#25112;&#26082;&#33267;&#20851;&#37325;&#35201;&#21448;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#35768;&#22810;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#30693;&#35782;&#22270;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#32467;&#21512;&#22522;&#20110;&#39033;&#30446;&#21644;&#29992;&#25143;-&#39033;&#30446;&#21327;&#20316;&#20449;&#21495;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#26222;&#36941;&#36235;&#21183;&#26159;&#19987;&#27880;&#20110;&#25552;&#39640;&#25490;&#21517;&#24615;&#33021;&#65292;&#20294;&#20250;&#22686;&#21152;&#27169;&#22411;&#22797;&#26434;&#24615;&#12289;&#38477;&#20302;&#22810;&#26679;&#24615;&#24182;&#20351;&#20219;&#21153;&#21464;&#24471;&#26356;&#22797;&#26434;&#12290;&#25552;&#20379;&#26082;&#20010;&#24615;&#21270;&#21448;&#22810;&#26679;&#21270;&#30340;&#25512;&#33616;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#23454;&#29616;&#39640;&#25490;&#21517;&#24615;&#33021;&#65292;&#27604;&#22914;&#28857;&#20987;&#29575;&#12289;&#21484;&#22238;&#29575;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#29992;&#25143;-&#39033;&#30446;&#21644;&#39033;&#30446;-&#39033;&#30446;&#20132;&#20114;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#25551;&#36848;&#24615;&#25991;&#26412;&#19978;&#24212;&#29992;&#22522;&#20110;&#39033;&#30446;&#30340;&#23545;&#27604;&#23398;&#20064;&#65292;&#26681;&#25454;&#39033;&#30446;&#20803;&#25968;&#25454;&#23545;&#27491;&#36127;&#26679;&#26412;&#36827;&#34892;&#25277;&#26679;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#27169;&#22411;&#33021;&#26356;&#22909;&#22320;&#29702;&#35299;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18667v1 Announce Type: cross  Abstract: Addressing the challenges related to data sparsity, cold-start problems, and diversity in recommendation systems is both crucial and demanding. Many current solutions leverage knowledge graphs to tackle these issues by combining both item-based and user-item collaborative signals. A common trend in these approaches focuses on improving ranking performance at the cost of escalating model complexity, reducing diversity, and complicating the task. It is essential to provide recommendations that are both personalized and diverse, rather than solely relying on achieving high rank-based performance, such as Click-through Rate, Recall, etc. In this paper, we propose a hybrid multi-task learning approach, training on user-item and item-item interactions. We apply item-based contrastive learning on descriptive text, sampling positive and negative pairs based on item metadata. Our approach allows the model to better understand the relationships 
&lt;/p&gt;</description></item><item><title>SDSAT&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#28789;&#27963;&#35299;&#30721;&#33021;&#21147;&#30340;&#35821;&#20041;&#33258;&#36866;&#24212;&#20196;&#29260;&#65292;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#33609;&#31295;&#20196;&#29260;&#30340;&#33021;&#21147;&#65292;&#24182;&#23454;&#29616;&#36229;&#36807;3.5&#20493;&#21644;3.0&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.18647</link><description>&lt;p&gt;
SDSAT&#65306;&#36890;&#36807;&#20855;&#26377;&#35821;&#20041;&#33258;&#36866;&#24212;&#20196;&#29260;&#30340;&#25512;&#27979;&#35299;&#30721;&#21152;&#36895;LLM&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18647
&lt;/p&gt;
&lt;p&gt;
SDSAT&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#28789;&#27963;&#35299;&#30721;&#33021;&#21147;&#30340;&#35821;&#20041;&#33258;&#36866;&#24212;&#20196;&#29260;&#65292;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#33609;&#31295;&#20196;&#29260;&#30340;&#33021;&#21147;&#65292;&#24182;&#23454;&#29616;&#36229;&#36807;3.5&#20493;&#21644;3.0&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21152;&#36895;&#26041;&#26696;&#65292;&#36890;&#36807;&#20855;&#26377;&#35821;&#20041;&#33258;&#36866;&#24212;&#20196;&#29260;&#65288;SDSAT&#65289;&#36827;&#34892;&#25512;&#27979;&#35299;&#30721;&#12290;&#35813;&#35774;&#35745;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22686;&#24378;LLM&#27169;&#22411;&#29983;&#25104;&#33609;&#31295;&#26631;&#35760;&#30340;&#33021;&#21147;&#65292;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#26680;&#24515;&#31574;&#30053;&#21253;&#25324;&#65306;1&#65289;&#36890;&#36807;&#21512;&#24182;&#20855;&#26377;&#28789;&#27963;&#35299;&#30721;&#33021;&#21147;&#30340;&#35821;&#20041;&#33258;&#36866;&#24212;&#20196;&#29260;&#26469;&#24494;&#35843;&#27169;&#22411;&#65292;&#32780;&#19981;&#25913;&#21464;&#20854;&#32467;&#26500;&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33609;&#31295;&#26631;&#35760;&#12290;2&#65289;&#36890;&#36807;&#20351;&#29992;&#19981;&#24433;&#21709;&#26631;&#20934;&#20196;&#29260;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#20854;&#21407;&#22987;&#26694;&#26550;&#20043;&#19978;&#33719;&#24471;&#24182;&#34892;&#35299;&#30721;&#33021;&#21147;&#65292;&#32780;&#35757;&#32451;&#24320;&#38144;&#26368;&#23567;&#12290;3&#65289;&#25105;&#20204;&#35774;&#35745;&#20102;&#20351;&#29992;&#36138;&#23146;&#25628;&#32034;&#21644;&#26680;&#37319;&#26679;&#30340;&#8220;&#20004;&#27493;&#36215;&#33609;&#28982;&#21518;&#39564;&#35777;&#8221;&#29983;&#25104;&#31574;&#30053;&#12290;&#22312;CodeLlama-13B&#21644;7B&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36895;&#24230;&#20998;&#21035;&#25552;&#39640;&#20102;3.5&#20493;&#21644;3.0&#20493;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18647v1 Announce Type: new  Abstract: We propose an acceleration scheme for large language models (LLMs) through Speculative Decoding with Semantic Adaptive Tokens (SDSAT). The primary objective of this design is to enhance the LLM model's ability to generate draft tokens more accurately without compromising the model's accuracy. The core strategies involve: 1) Fine-tune the model by incorporating semantic adaptive tokens that possess flexible decoding capabilities without changing its structure, allowing them to generate high-quality draft tokens. 2) By employing a training method that does not affect the standard tokens, the model can acquire parallel decoding abilities atop its original framework with minimal training overhead. 3) We have designed the "two-step-draft-then-verify" generation strategies using both greedy search and nucleus sampling. Experiments conducted on the CodeLlama-13B and 7B models have yielded speed increases of over 3.5X and 3.0X, respectively. Ple
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PrimeVul&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28431;&#27934;&#26816;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#26032;&#39062;&#30340;&#25968;&#25454;&#26631;&#35760;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#19982;&#20154;&#24037;&#39564;&#35777;&#22522;&#20934;&#30456;&#24403;&#30340;&#26631;&#31614;&#20934;&#30830;&#24615;&#65292;&#26174;&#33879;&#25193;&#22823;&#20102;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.18624</link><description>&lt;p&gt;
&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28431;&#27934;&#26816;&#27979;&#65306;&#25105;&#20204;&#31163;&#30446;&#26631;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
Vulnerability Detection with Code Language Models: How Far Are We?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18624
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PrimeVul&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28431;&#27934;&#26816;&#27979;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#26032;&#39062;&#30340;&#25968;&#25454;&#26631;&#35760;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#19982;&#20154;&#24037;&#39564;&#35777;&#22522;&#20934;&#30456;&#24403;&#30340;&#26631;&#31614;&#20934;&#30830;&#24615;&#65292;&#26174;&#33879;&#25193;&#22823;&#20102;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;code LMs&#65289;&#21644;&#28431;&#27934;&#26816;&#27979;&#22791;&#21463;&#20851;&#27880;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26816;&#27979;&#28431;&#27934;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#29616;&#26377;&#28431;&#27934;&#25968;&#25454;&#38598;&#23384;&#22312;&#30340;&#37325;&#22823;&#32570;&#38519;&#65292;&#21253;&#25324;&#25968;&#25454;&#36136;&#37327;&#20302;&#12289;&#26631;&#31614;&#20934;&#30830;&#24615;&#24046;&#20197;&#21450;&#39640;&#37325;&#22797;&#29575;&#65292;&#23548;&#33268;&#22312;&#29616;&#23454;&#28431;&#27934;&#26816;&#27979;&#22330;&#26223;&#20013;&#27169;&#22411;&#24615;&#33021;&#19981;&#21487;&#38752;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20351;&#29992;&#30340;&#35780;&#20272;&#26041;&#27861;&#20063;&#19981;&#33021;&#20195;&#34920;&#30495;&#23454;&#19990;&#30028;&#30340;&#28431;&#27934;&#26816;&#27979;&#24773;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18624v1 Announce Type: cross  Abstract: In the context of the rising interest in code language models (code LMs) and vulnerability detection, we study the effectiveness of code LMs for detecting vulnerabilities. Our analysis reveals significant shortcomings in existing vulnerability datasets, including poor data quality, low label accuracy, and high duplication rates, leading to unreliable model performance in realistic vulnerability detection scenarios. Additionally, the evaluation methods used with these datasets are not representative of real-world vulnerability detection.   To address these challenges, we introduce PrimeVul, a new dataset for training and evaluating code LMs for vulnerability detection. PrimeVul incorporates a novel set of data labeling techniques that achieve comparable label accuracy to human-verified benchmarks while significantly expanding the dataset. It also implements a rigorous data de-duplication and chronological data splitting strategy to miti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#23574;&#23792;&#31070;&#32463;&#33180;&#31995;&#32479;&#21644;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#21151;&#33021;&#12289;&#20248;&#32570;&#28857;&#65292;&#24182;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#26032;&#24212;&#29992;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;</title><link>https://arxiv.org/abs/2403.18609</link><description>&lt;p&gt;
&#26377;&#20851;&#23574;&#23792;&#31070;&#32463;&#33180;&#31995;&#32479;&#21644;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#27169;&#22411;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A survey on learning models of spiking neural membrane systems and spiking neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18609
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#23574;&#23792;&#31070;&#32463;&#33180;&#31995;&#32479;&#21644;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#21151;&#33021;&#12289;&#20248;&#32570;&#28857;&#65292;&#24182;&#23545;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#26032;&#24212;&#29992;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26159;&#19968;&#31181;&#21463;&#29983;&#29289;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20855;&#26377;&#26576;&#20123;&#31867;&#20284;&#22823;&#33041;&#30340;&#29305;&#24615;&#12290;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#37324;&#65292;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#29616;&#35937;&#65292;&#36825;&#31181;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#30028;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;SNN&#20013;&#65292;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#36890;&#20449;&#36890;&#36807;&#23574;&#23792;&#21644;&#23574;&#23792;&#21015;&#36827;&#34892;&#12290;&#36825;&#20351;&#24471;&#36825;&#20123;&#27169;&#22411;&#19982;&#8220;&#26631;&#20934;&#8221;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#19981;&#21516;&#65292;&#21518;&#32773;&#23558;&#23574;&#23792;&#30340;&#39057;&#29575;&#26367;&#25442;&#20026;&#23454;&#20540;&#20449;&#21495;&#12290;&#23574;&#23792;&#31070;&#32463;P&#31995;&#32479;&#65288;SNPS&#65289;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#22522;&#20110;&#24418;&#24335;&#33258;&#21160;&#26426;&#21407;&#29702;&#26356;&#22810;&#30340;SNN&#20998;&#25903;&#65292;&#35768;&#22810;&#21464;&#20307;&#37117;&#22312;&#33180;&#35745;&#31639;&#29702;&#35770;&#26694;&#26550;&#20869;&#21457;&#23637;&#12290;&#26412;&#25991;&#39318;&#20808;&#31616;&#35201;&#27604;&#36739;&#20102;SNN&#21644;SNPS&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#12289;&#20248;&#32570;&#28857;&#12290;&#25991;&#31456;&#30340;&#37325;&#28857;&#37096;&#20998;&#26159;&#35843;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#26032;&#32467;&#26524;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18609v1 Announce Type: cross  Abstract: Spiking neural networks (SNN) are a biologically inspired model of neural networks with certain brain-like properties. In the past few decades, this model has received increasing attention in computer science community, owing also to the successful phenomenon of deep learning. In SNN, communication between neurons takes place through the spikes and spike trains. This differentiates these models from the ``standard'' artificial neural networks (ANN) where the frequency of spikes is replaced by real-valued signals. Spiking neural P systems (SNPS) can be considered a branch of SNN based more on the principles of formal automata, with many variants developed within the framework of the membrane computing theory. In this paper, we first briefly compare structure and function, advantages and drawbacks of SNN and SNPS. A key part of the article is a survey of recent results and applications of machine learning and deep learning models of both
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#35789;&#23545;&#28040;&#38500;&#21477;&#23376;&#23884;&#20837;&#22120;&#20013;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20559;&#24046;&#20449;&#24687;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.18555</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#35789;&#23545;&#28040;&#38500;&#21477;&#23376;&#23884;&#20837;&#22120;&#20013;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Debiasing Sentence Embedders through Contrastive Word Pairs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#35789;&#23545;&#28040;&#38500;&#21477;&#23376;&#23884;&#20837;&#22120;&#20013;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20559;&#24046;&#20449;&#24687;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#21508;&#31181;&#21477;&#23376;&#23884;&#20837;&#22120;&#24050;&#25104;&#20026;&#30446;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#25104;&#21151;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22810;&#20010;&#26469;&#28304;&#24050;&#32463;&#34920;&#26126;&#65292;&#36825;&#20123;&#23884;&#20837;&#26041;&#27861;&#25152;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#20013;&#22266;&#26377;&#30340;&#20559;&#24046;&#26159;&#30001;&#23427;&#20204;&#23398;&#20064;&#21040;&#30340;&#12290;&#25991;&#29486;&#20013;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#28040;&#38500;&#23884;&#20837;&#20013;&#30340;&#20559;&#24046;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#36866;&#29992;&#20110;&#35789;&#23884;&#20837;&#65292;&#26356;&#23569;&#30340;&#24773;&#20917;&#36866;&#29992;&#20110;&#21477;&#23376;&#23884;&#20837;&#12290;&#38382;&#39064;&#22312;&#20110;&#65292;&#22823;&#22810;&#25968;&#21435;&#20559;&#26041;&#27861;&#30452;&#25509;&#20174;&#35789;&#23884;&#20837;&#36716;&#31227;&#32780;&#26469;&#65292;&#22240;&#27492;&#36825;&#20123;&#26041;&#27861;&#26410;&#33021;&#32771;&#34385;&#21040;&#21477;&#23376;&#23884;&#20837;&#22120;&#30340;&#38750;&#32447;&#24615;&#29305;&#24615;&#21450;&#20854;&#20135;&#29983;&#30340;&#23884;&#20837;&#12290;&#25991;&#29486;&#20013;&#24050;&#32463;&#34920;&#26126;&#65292;&#22914;&#26524;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#21435;&#38500;&#35821;&#21477;&#23884;&#20837;&#20013;&#30340;&#20559;&#24046;&#65292;&#21017;&#20559;&#24046;&#20449;&#24687;&#20173;&#28982;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#28040;&#38500;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20559;&#24046;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18555v1 Announce Type: new  Abstract: Over the last years, various sentence embedders have been an integral part in the success of current machine learning approaches to Natural Language Processing (NLP). Unfortunately, multiple sources have shown that the bias, inherent in the datasets upon which these embedding methods are trained, is learned by them. A variety of different approaches to remove biases in embeddings exists in the literature. Most of these approaches are applicable to word embeddings and in fewer cases to sentence embeddings. It is problematic that most debiasing approaches are directly transferred from word embeddings, therefore these approaches fail to take into account the nonlinear nature of sentence embedders and the embeddings they produce. It has been shown in literature that bias information is still present if sentence embeddings are debiased using such methods. In this contribution, we explore an approach to remove linear and nonlinear bias informa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#20851;&#27880;&#35821;&#22659;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20013;&#25991;&#38405;&#35835;&#20219;&#21153;&#20013;&#30340;&#20957;&#35270;&#25345;&#32493;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.18542</link><description>&lt;p&gt;
&#20851;&#27880;&#35821;&#22659;&#35821;&#20041;&#30456;&#20851;&#24615;&#39044;&#27979;&#20013;&#25991;&#21477;&#23376;&#38405;&#35835;
&lt;/p&gt;
&lt;p&gt;
Attention-aware semantic relevance predicting Chinese sentence reading
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18542
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#20851;&#27880;&#35821;&#22659;&#8221;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20013;&#25991;&#38405;&#35835;&#20219;&#21153;&#20013;&#30340;&#20957;&#35270;&#25345;&#32493;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26377;&#24433;&#21709;&#21147;&#30340;&#35745;&#31639;&#27169;&#22411;&#21644;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#39044;&#27979;&#20154;&#31867;&#22914;&#20309;&#29702;&#35299;&#21644;&#22788;&#29702;&#21477;&#23376;&#12290;&#26412;&#30740;&#31350;&#21463;Transformer&#20013;&#30340;&#27880;&#24847;&#21147;&#31639;&#27861;&#21644;&#20154;&#31867;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20851;&#27880;&#35821;&#22659;&#8221;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#35821;&#22659;&#35821;&#20041;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#26032;&#26041;&#27861;&#32771;&#34385;&#20102;&#35821;&#22659;&#37096;&#20998;&#30340;&#19981;&#21516;&#36129;&#29486;&#21644;&#26399;&#26395;&#25928;&#24212;&#65292;&#20351;&#20854;&#33021;&#22815;&#20805;&#20998;&#25972;&#21512;&#35821;&#22659;&#20449;&#24687;&#12290;&#20851;&#27880;&#35821;&#22659;&#26041;&#27861;&#36824;&#26377;&#21161;&#20110;&#27169;&#25311;&#29616;&#26377;&#30340;&#38405;&#35835;&#27169;&#22411;&#24182;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;&#26368;&#32456;&#24471;&#21040;&#30340;&#8220;&#20851;&#27880;&#35821;&#22659;&#8221;&#35821;&#20041;&#30456;&#20851;&#24615;&#24230;&#37327;&#26631;&#20934;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20102;&#35760;&#24405;&#22312;&#30524;&#21160;&#36861;&#36394;&#35821;&#26009;&#24211;&#20013;&#30340;&#20013;&#25991;&#38405;&#35835;&#20219;&#21153;&#20013;&#20957;&#35270;&#25345;&#32493;&#26102;&#38388;&#12290;&#35813;&#30740;&#31350;&#30340;&#21457;&#29616;&#36827;&#19968;&#27493;&#20026;&#29616;&#26377;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#21147;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18542v1 Announce Type: new  Abstract: In recent years, several influential computational models and metrics have been proposed to predict how humans comprehend and process sentence. One particularly promising approach is contextual semantic similarity. Inspired by the attention algorithm in Transformer and human memory mechanisms, this study proposes an ``attention-aware'' approach for computing contextual semantic relevance. This new approach takes into account the different contributions of contextual parts and the expectation effect, allowing it to incorporate contextual information fully. The attention-aware approach also facilitates the simulation of existing reading models and evaluate them. The resulting ``attention-aware'' metrics of semantic relevance can more accurately predict fixation durations in Chinese reading tasks recorded in an eye-tracking corpus than those calculated by existing approaches. The study's findings further provide strong support for the prese
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20114;&#25805;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;&#19987;&#23478;&#31995;&#32479;&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#26469;&#25552;&#21462;&#12289;&#36716;&#25442;&#12289;&#21152;&#36733;&#21644;&#35745;&#31639;&#27861;&#24459;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#36890;&#21521;&#27861;&#24459;&#33258;&#27835;&#30340;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.18537</link><description>&lt;p&gt;
&#36890;&#24448;&#27861;&#24459;&#33258;&#27835;&#30340;&#36335;&#24452;&#65306;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;&#19987;&#23478;&#31995;&#32479;&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#25552;&#21462;&#12289;&#36716;&#25442;&#12289;&#21152;&#36733;&#21644;&#35745;&#31639;&#27861;&#24459;&#20449;&#24687;&#30340;&#21487;&#20114;&#25805;&#20316;&#21644;&#21487;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Path Towards Legal Autonomy: An interoperable and explainable approach to extracting, transforming, loading and computing legal information using large language models, expert systems and Bayesian networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20114;&#25805;&#20316;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;&#19987;&#23478;&#31995;&#32479;&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#26469;&#25552;&#21462;&#12289;&#36716;&#25442;&#12289;&#21152;&#36733;&#21644;&#35745;&#31639;&#27861;&#24459;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#36890;&#21521;&#27861;&#24459;&#33258;&#27835;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#33258;&#27835; - &#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#21512;&#27861;&#27963;&#21160; - &#21487;&#20197;&#36890;&#36807;&#20004;&#31181;&#26041;&#24335;&#23454;&#29616;&#12290;&#19968;&#31181;&#26041;&#24335;&#26159;&#23545;AI&#34892;&#20026;&#32773;&#65288;&#22914;&#24320;&#21457;&#32773;&#12289;&#37096;&#32626;&#32773;&#21644;&#29992;&#25143;&#65289;&#21644;AI&#36164;&#28304;&#65288;&#22914;&#25968;&#25454;&#65289;&#26045;&#21152;&#32422;&#26463;&#65292;&#25110;&#32773;&#26159;&#23545;AI&#20195;&#29702;&#22312;&#29615;&#22659;&#20013;&#21487;&#33021;&#20135;&#29983;&#24433;&#21709;&#30340;&#33539;&#22260;&#21644;&#24433;&#21709;&#31243;&#24230;&#26045;&#21152;&#32422;&#26463;&#12290;&#21518;&#19968;&#31181;&#26041;&#27861;&#28041;&#21450;&#23558;&#20851;&#20110;&#30001;AI&#39537;&#21160;&#30340;&#35774;&#22791;&#30340;&#29616;&#26377;&#35268;&#21017;&#32534;&#30721;&#21040;&#25511;&#21046;&#36825;&#20123;&#35774;&#22791;&#30340;AI&#20195;&#29702;&#36719;&#20214;&#20013;&#65288;&#20363;&#22914;&#65292;&#23558;&#20851;&#20110;&#26080;&#20154;&#26426;&#35774;&#22791;&#25805;&#20316;&#33539;&#22260;&#38480;&#21046;&#30340;&#35268;&#21017;&#32534;&#30721;&#21040;&#26080;&#20154;&#26426;&#35774;&#22791;&#30340;&#20195;&#29702;&#36719;&#20214;&#20013;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#38656;&#35201;&#19968;&#31181;&#25552;&#21462;&#12289;&#21152;&#36733;&#12289;&#36716;&#25442;&#21644;&#35745;&#31639;&#27861;&#24459;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#26082;&#21487;&#35299;&#37322;&#21448;&#21487;&#22312;&#27861;&#24459;&#19978;&#30456;&#20114;&#25805;&#20316;&#65292;&#24182;&#33021;&#35753;AI&#20195;&#29702;&#25512;&#29702;&#27861;&#24459;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#27010;&#36848;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#19987;&#23478;&#31995;&#32479;&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18537v1 Announce Type: new  Abstract: Legal autonomy - the lawful activity of artificial intelligence agents - can be achieved in one of two ways. It can be achieved either by imposing constraints on AI actors such as developers, deployers and users, and on AI resources such as data, or by imposing constraints on the range and scope of the impact that AI agents can have on the environment. The latter approach involves encoding extant rules concerning AI driven devices into the software of AI agents controlling those devices (e.g., encoding rules about limitations on zones of operations into the agent software of an autonomous drone device). This is a challenge since the effectivity of such an approach requires a method of extracting, loading, transforming and computing legal information that would be both explainable and legally interoperable, and that would enable AI agents to reason about the law. In this paper, we sketch a proof of principle for such a method using large 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#22823;&#22411;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#22312;&#23545;&#35937;-&#23646;&#24615;&#32452;&#21512;&#27867;&#21270;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#20026;&#27867;&#21270;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.18525</link><description>&lt;p&gt;
&#35821;&#35328;&#22312;CLIP&#23545;&#35937;-&#23646;&#24615;&#32452;&#21512;&#27867;&#21270;&#20013;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Language Plays a Pivotal Role in the Object-Attribute Compositional Generalization of CLIP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#22823;&#22411;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#22312;&#23545;&#35937;-&#23646;&#24615;&#32452;&#21512;&#27867;&#21270;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#20026;&#27867;&#21270;&#21644;&#25968;&#25454;&#38598;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18525v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495; &#25688;&#35201;:&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;CLIP&#65292;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#19979;&#23637;&#29616;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#35843;&#26597;&#36825;&#31181;&#33021;&#21147;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27839;&#29992;&#20102;&#30456;&#21516;&#30340;&#24605;&#36335;&#65292;&#20294;&#19987;&#27880;&#20110;&#29305;&#23450;&#31867;&#22411;&#30340;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454; - &#20855;&#26377;&#26032;&#39062;&#23646;&#24615;-&#23545;&#35937;&#23545;&#32452;&#21512;&#30340;&#22270;&#20687; - &#24182;&#30740;&#31350;&#36825;&#26679;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25104;&#21151;&#22320;&#23558;&#36825;&#20123;&#22270;&#20687;&#20998;&#31867;&#21040;&#32452;&#21512;&#31867;&#21035;&#20013;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;ImageNet-AO&#30340;&#30495;&#23454;&#22270;&#20687;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;CLIP&#35757;&#32451;&#38598;&#20013;&#19981;&#22826;&#21487;&#33021;&#36935;&#21040;&#30340;&#23545;&#35937;&#30340;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#22823;&#22411;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#65288;&#22914;OpenAI CLIP&#65292;LAION-400M&#21644;LAION-2B&#65289;&#22312;&#26377;&#25928;&#30340;&#32452;&#21512;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#26041;&#38754;&#27604;&#21463;&#30417;&#30563;&#27169;&#22411;&#21644;&#36890;&#36807;&#36739;&#23567;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;CLIP&#27169;&#22411;&#65288;&#22914;CC-12M&#21644;YFCC-15M&#65289;&#34920;&#29616;&#20986;&#25968;&#37327;&#32423;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#20851;&#20110;&#35268;&#27169;&#12289;&#25968;&#25454;&#38598;&#21644;&#27867;&#21270;&#20043;&#38388;&#20851;&#31995;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18525v1 Announce Type: cross  Abstract: Vision-language models, such as CLIP, have shown promising Out-of-Distribution (OoD) generalization under various types of distribution shifts. Recent studies attempted to investigate the leading cause of this capability. In this work, we follow the same path, but focus on a specific type of OoD data - images with novel compositions of attribute-object pairs - and study whether such models can successfully classify those images into composition classes. We carefully designed an authentic image test dataset called ImageNet-AO, consisting of attributes for objects that are unlikely encountered in the CLIP training sets. We found that CLIPs trained with large datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude improvement in effective compositional OoD generalization compared to both supervised models and CLIPs trained with smaller datasets, such as CC-12M and YFCC-15M. Our results provide evidence that the sca
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#25237;&#31080;&#39537;&#21160;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#33258;&#21160;&#33719;&#21462;&#20107;&#20214;&#30340;&#20856;&#22411;&#25345;&#32493;&#26102;&#38388;&#65292;&#20197;&#27492;&#20316;&#20026;&#20266;&#26631;&#31614;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22312;&#26102;&#38388;&#24120;&#35782;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18504</link><description>&lt;p&gt;
AcTED&#65306;&#21322;&#30417;&#30563;&#26102;&#38388;&#24120;&#35782;&#38382;&#31572;&#20013;&#20856;&#22411;&#20107;&#20214;&#25345;&#32493;&#26102;&#38388;&#30340;&#33258;&#21160;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
AcTED: Automatic Acquisition of Typical Event Duration for Semi-supervised Temporal Commonsense QA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18504
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#25237;&#31080;&#39537;&#21160;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#33258;&#21160;&#33719;&#21462;&#20107;&#20214;&#30340;&#20856;&#22411;&#25345;&#32493;&#26102;&#38388;&#65292;&#20197;&#27492;&#20316;&#20026;&#20266;&#26631;&#31614;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22312;&#26102;&#38388;&#24120;&#35782;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#25237;&#31080;&#39537;&#21160;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#65292;&#33258;&#21160;&#33719;&#21462;&#20107;&#20214;&#30340;&#20856;&#22411;&#25345;&#32493;&#26102;&#38388;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#20266;&#26631;&#31614;&#25968;&#25454;&#12290;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20266;&#26631;&#31614;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#39640;&#20934;&#30830;&#24615;&#21644;&#24179;&#34913;&#35206;&#30422;&#29575;&#12290;&#22312;&#26102;&#38388;&#24120;&#35782;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#20165;&#20351;&#29992;400&#20010;&#20107;&#20214;&#30340;&#20266;&#20363;&#23376;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19982;&#29616;&#26377;&#22522;&#20110;BERT&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#19982;RoBERTa&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#26041;&#27861;&#22312;&#20934;&#30830;&#21305;&#37197;&#19978;&#23454;&#29616;&#20102;7&#65285;&#30340;&#25552;&#21319;&#65292;&#24314;&#31435;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18504v1 Announce Type: new  Abstract: We propose a voting-driven semi-supervised approach to automatically acquire the typical duration of an event and use it as pseudo-labeled data. The human evaluation demonstrates that our pseudo labels exhibit surprisingly high accuracy and balanced coverage. In the temporal commonsense QA task, experimental results show that using only pseudo examples of 400 events, we achieve performance comparable to the existing BERT-based weakly supervised approaches that require a significant amount of training examples. When compared to the RoBERTa baselines, our best approach establishes state-of-the-art performance with a 7% improvement in Exact Match.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LMTraj&#65288;&#22522;&#20110;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#36712;&#36857;&#39044;&#27979;&#22120;&#65289;&#65292;&#23558;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#36716;&#21270;&#20026;&#19968;&#31181;&#31867;&#20284;&#20110;&#38382;&#31572;&#38382;&#39064;&#30340;&#24418;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#20540;&#21644;&#22270;&#20687;&#25968;&#25454;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#32487;&#32493;&#29983;&#25104;&#19982;&#36712;&#36857;&#30456;&#20851;&#30340;&#22810;&#27169;&#24577;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.18447</link><description>&lt;p&gt;
&#35821;&#35328;&#33021;&#21542;&#32988;&#36807;&#25968;&#20540;&#22238;&#24402;&#65311;&#22522;&#20110;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LMTraj&#65288;&#22522;&#20110;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#36712;&#36857;&#39044;&#27979;&#22120;&#65289;&#65292;&#23558;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#36716;&#21270;&#20026;&#19968;&#31181;&#31867;&#20284;&#20110;&#38382;&#31572;&#38382;&#39064;&#30340;&#24418;&#24335;&#65292;&#36890;&#36807;&#23558;&#25968;&#20540;&#21644;&#22270;&#20687;&#25968;&#25454;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#26469;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#32487;&#32493;&#29983;&#25104;&#19982;&#36712;&#36857;&#30456;&#20851;&#30340;&#22810;&#27169;&#24577;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18447v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#29983;&#25104;&#24615;&#33021;&#26041;&#38754;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#21463;&#21040;&#36817;&#26399;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LMTraj&#65288;&#22522;&#20110;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#36712;&#36857;&#39044;&#27979;&#22120;&#65289;&#65292;&#23558;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#37325;&#26032;&#35774;&#35745;&#20026;&#19968;&#31181;&#31867;&#20284;&#20110;&#38382;&#31572;&#38382;&#39064;&#30340;&#24418;&#24335;&#12290;&#19982;&#20256;&#32479;&#30340;&#23558;&#36712;&#36857;&#22352;&#26631;&#24207;&#21015;&#35270;&#20026;&#36830;&#32493;&#20449;&#21495;&#30340;&#25968;&#20540;&#22238;&#24402;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#31867;&#20284;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;&#31163;&#25955;&#20449;&#21495;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#36712;&#36857;&#22352;&#26631;&#30340;&#36755;&#20837;&#31354;&#38388;&#36716;&#25442;&#25104;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#12290;&#22312;&#36825;&#37324;&#65292;&#34892;&#20154;&#30340;&#25972;&#20010;&#26102;&#38388;&#24207;&#21015;&#36712;&#36857;&#34987;&#36716;&#25442;&#20026;&#19968;&#20010;&#25991;&#26412;&#25552;&#31034;&#65292;&#22330;&#26223;&#22270;&#20687;&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#34987;&#25551;&#36848;&#20026;&#25991;&#26412;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#36716;&#25442;&#21518;&#30340;&#25968;&#20540;&#21644;&#22270;&#20687;&#25968;&#25454;&#34987;&#21253;&#35013;&#36827;&#38382;&#31572;&#27169;&#26495;&#20013;&#65292;&#20379;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#32487;&#32493;&#29983;&#25104;&#19982;&#36712;&#36857;&#30456;&#20851;&#30340;&#22810;&#27169;&#24577;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#35757;&#32451;&#31574;&#30053;&#65292;&#21516;&#26102;&#20248;&#21270;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18447v1 Announce Type: new  Abstract: Language models have demonstrated impressive ability in context understanding and generative performance. Inspired by the recent success of language foundation models, in this paper, we propose LMTraj (Language-based Multimodal Trajectory predictor), which recasts the trajectory prediction task into a sort of question-answering problem. Departing from traditional numerical regression models, which treat the trajectory coordinate sequence as continuous signals, we consider them as discrete signals like text prompts. Specially, we first transform an input space for the trajectory coordinate into the natural language space. Here, the entire time-series trajectories of pedestrians are converted into a text prompt, and scene images are described as text information through image captioning. The transformed numerical and image data are then wrapped into the question-answering template for use in a language model. Next, to guide the language mo
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;DELTA&#65292;&#21033;&#29992;&#32467;&#26500;&#21270;&#35789;&#23545;&#40784;&#39044;&#35757;&#32451;&#21028;&#21035;&#24335;&#32534;&#30721;&#22120;&#36827;&#34892;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#65292;&#36890;&#36807;&#24378;&#35843;&#20851;&#38190;&#20107;&#23454;&#26469;&#25552;&#39640;&#34920;&#31034;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.18435</link><description>&lt;p&gt;
&#21033;&#29992;&#32467;&#26500;&#21270;&#35789;&#23545;&#40784;&#39044;&#35757;&#32451;&#21028;&#21035;&#24335;&#32534;&#30721;&#22120;&#36827;&#34892;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
DELTA: Pre-train a Discriminative Encoder for Legal Case Retrieval via Structural Word Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18435
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;DELTA&#65292;&#21033;&#29992;&#32467;&#26500;&#21270;&#35789;&#23545;&#40784;&#39044;&#35757;&#32451;&#21028;&#21035;&#24335;&#32534;&#30721;&#22120;&#36827;&#34892;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#65292;&#36890;&#36807;&#24378;&#35843;&#20851;&#38190;&#20107;&#23454;&#26469;&#25552;&#39640;&#34920;&#31034;&#30340;&#21028;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#26159;&#26377;&#25928;&#30340;&#12290;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#20391;&#37325;&#20110;&#25552;&#39640;[CLS]&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#30340;&#34920;&#24449;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#35745;&#31639;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#27861;&#24459;&#39046;&#22495;&#65292;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24615;&#24182;&#19981;&#24635;&#26159;&#24847;&#21619;&#30528;&#26696;&#20363;&#20043;&#38388;&#36275;&#22815;&#30456;&#20851;&#12290;&#30456;&#21453;&#65292;&#22312;&#27861;&#24459;&#26696;&#20363;&#20013;&#65292;&#30456;&#20851;&#24615;&#20027;&#35201;&#21462;&#20915;&#20110;&#24433;&#21709;&#26368;&#32456;&#21028;&#20915;&#30340;&#20851;&#38190;&#20107;&#23454;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DELTA&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#35774;&#35745;&#30340;&#21028;&#21035;&#24335;&#27169;&#22411;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#22312;&#27861;&#24459;&#26696;&#20363;&#20013;&#25214;&#21040;&#20851;&#38190;&#20107;&#23454;&#65292;&#24182;&#23558;[CLS]&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#36924;&#36817;&#36825;&#20123;&#20851;&#38190;&#20107;&#23454;&#65292;&#21516;&#26102;&#36828;&#31163;&#38750;&#20851;&#38190;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18435v1 Announce Type: cross  Abstract: Recent research demonstrates the effectiveness of using pre-trained language models for legal case retrieval. Most of the existing works focus on improving the representation ability for the contextualized embedding of the [CLS] token and calculate relevance using textual semantic similarity. However, in the legal domain, textual semantic similarity does not always imply that the cases are relevant enough. Instead, relevance in legal cases primarily depends on the similarity of key facts that impact the final judgment. Without proper treatments, the discriminative ability of learned representations could be limited since legal cases are lengthy and contain numerous non-key facts. To this end, we introduce DELTA, a discriminative model designed for legal case retrieval. The basic idea involves pinpointing key facts in legal cases and pulling the contextualized embedding of the [CLS] token closer to the key facts while pushing away from 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21477;&#27861;&#36317;&#31163;&#21644;&#22320;&#29702;&#37051;&#36817;&#24615;&#25506;&#32034;&#35821;&#35328;&#20851;&#31995;&#65292;&#20351;&#29992;POS trigrams&#26368;&#22823;&#21270;&#25429;&#25417;&#21477;&#27861;&#21464;&#21270;&#65292;&#24314;&#31435;&#35821;&#35328;&#36830;&#25509;&#24182;&#25581;&#31034;&#35821;&#35328;&#23478;&#26063;&#21644;&#32676;&#20307;&#30340;&#31751;&#12290;</title><link>https://arxiv.org/abs/2403.18430</link><description>&lt;p&gt;
&#36890;&#36807;&#21477;&#27861;&#36317;&#31163;&#21644;&#22320;&#29702;&#37051;&#36817;&#24615;&#25506;&#32034;&#35821;&#35328;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring language relations through syntactic distances and geographic proximity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18430
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21477;&#27861;&#36317;&#31163;&#21644;&#22320;&#29702;&#37051;&#36817;&#24615;&#25506;&#32034;&#35821;&#35328;&#20851;&#31995;&#65292;&#20351;&#29992;POS trigrams&#26368;&#22823;&#21270;&#25429;&#25417;&#21477;&#27861;&#21464;&#21270;&#65292;&#24314;&#31435;&#35821;&#35328;&#36830;&#25509;&#24182;&#25581;&#31034;&#35821;&#35328;&#23478;&#26063;&#21644;&#32676;&#20307;&#30340;&#31751;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#34987;&#20998;&#20026;&#20849;&#20139;&#30456;&#21516;&#35821;&#35328;&#29305;&#24449;&#30340;&#35821;&#31995;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#22312;&#29702;&#35299;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#22522;&#22240;&#20851;&#31995;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#38656;&#35201;&#26356;&#22810;&#30340;&#20998;&#26512;&#26469;&#20934;&#30830;&#37327;&#21270;&#23427;&#20204;&#30340;&#20851;&#32852;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#23569;&#30740;&#31350;&#30340;&#35821;&#35328;&#23618;&#38754;&#65292;&#27604;&#22914;&#21477;&#27861;&#12290;&#26412;&#25991;&#20351;&#29992;&#20174;Universal Dependencies&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30340;&#19968;&#31995;&#21015;&#35789;&#24615;&#65288;POS&#65289;&#26469;&#25506;&#32034;&#35821;&#35328;&#36317;&#31163;&#12290;&#22312;&#20449;&#24687;&#35770;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#37319;&#29992;POS&#19977;&#20803;&#32452;&#26368;&#22823;&#21270;&#25429;&#25417;&#21477;&#27861;&#21464;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#19982;&#21487;&#29992;&#25968;&#25454;&#37327;&#20860;&#23481;&#12290;&#36890;&#36807;&#22522;&#20110;POS&#20998;&#24067;&#30340;&#25104;&#23545;&#36317;&#31163;&#35780;&#20272;&#24314;&#31435;&#35821;&#35328;&#36830;&#25509;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#26126;&#30830;&#23545;&#24212;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#35821;&#35328;&#23478;&#26063;&#21644;&#32676;&#20307;&#30340;&#31751;&#65292;&#24322;&#24120;&#24773;&#20917;&#34987;&#35299;&#37322;&#20026;&#29420;&#29305;&#30340;&#24418;&#24577;&#31867;&#22411;&#23398;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#33719;&#24471;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18430v1 Announce Type: new  Abstract: Languages are grouped into families that share common linguistic traits. While this approach has been successful in understanding genetic relations between diverse languages, more analyses are needed to accurately quantify their relatedness, especially in less studied linguistic levels such as syntax. Here, we explore linguistic distances using series of parts of speech (POS) extracted from the Universal Dependencies dataset. Within an information-theoretic framework, we show that employing POS trigrams maximizes the possibility of capturing syntactic variations while being at the same time compatible with the amount of available data. Linguistic connections are then established by assessing pairwise distances based on the POS distributions. Intriguingly, our analysis reveals definite clusters that correspond to well known language families and groups, with exceptions explained by distinct morphological typologies. Furthermore, we obtain
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;160,230&#20010;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;TriviaHG&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#26469;&#34913;&#37327;&#25552;&#31034;&#30340;&#36136;&#37327;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18426</link><description>&lt;p&gt;
TriviaHG&#65306;&#29992;&#20110;&#20174;&#20107;&#23454;&#24615;&#38382;&#39064;&#29983;&#25104;&#33258;&#21160;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
TriviaHG: A Dataset for Automatic Hint Generation from Factoid Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18426
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;160,230&#20010;&#25552;&#31034;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;TriviaHG&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#26469;&#34913;&#37327;&#25552;&#31034;&#30340;&#36136;&#37327;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#65292;&#20010;&#20154;&#20542;&#21521;&#20110;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#65292;&#23547;&#25214;&#20182;&#20204;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#22312;&#36825;&#26679;&#30340;&#31572;&#26696;&#23545;&#20219;&#20309;&#20154;&#37117;&#24456;&#23481;&#26131;&#33719;&#24471;&#30340;&#26102;&#20195;&#65292;&#21050;&#28608;&#21644;&#20445;&#25345;&#20154;&#31867;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#20197;&#21450;&#30830;&#20445;&#20154;&#31867;&#20445;&#25345;&#33391;&#22909;&#25512;&#29702;&#33021;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#25552;&#31034;&#65288;&#32780;&#19981;&#26159;&#26368;&#32456;&#31572;&#26696;&#25110;&#22312;&#32473;&#20986;&#31572;&#26696;&#20043;&#21069;&#65289;&#20316;&#20026;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#20107;&#23454;&#24615;&#38382;&#39064;&#30340;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#26694;&#26550;&#65292;&#21033;&#29992;&#23427;&#26500;&#24314;&#20102;TriviaHG&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;TriviaQA&#25968;&#25454;&#38598;&#30340;16,645&#20010;&#38382;&#39064;&#23545;&#24212;&#30340;160,230&#20010;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#25552;&#31034;&#30340;&#25910;&#25947;&#24615;&#21644;&#29087;&#24713;&#24230;&#36136;&#37327;&#23646;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;TriviaHG&#25968;&#25454;&#38598;&#21644;&#25152;&#25552;&#20986;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#25105;&#20204;&#36992;&#35831;&#20102;10&#21517;&#20010;&#20307;&#27880;&#37322;2,791&#20010;&#25552;&#31034;&#65292;&#24182;&#20998;&#37197;&#20102;6&#21517;&#30740;&#31350;&#20154;&#21592;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18426v1 Announce Type: new  Abstract: Nowadays, individuals tend to engage in dialogues with Large Language Models, seeking answers to their questions. In times when such answers are readily accessible to anyone, the stimulation and preservation of human's cognitive abilities, as well as the assurance of maintaining good reasoning skills by humans becomes crucial. This study addresses such needs by proposing hints (instead of final answers or before giving answers) as a viable solution. We introduce a framework for the automatic hint generation for factoid questions, employing it to construct TriviaHG, a novel large-scale dataset featuring 160,230 hints corresponding to 16,645 questions from the TriviaQA dataset. Additionally, we present an automatic evaluation method that measures the Convergence and Familiarity quality attributes of hints. To evaluate the TriviaHG dataset and the proposed evaluation method, we enlisted 10 individuals to annotate 2,791 hints and tasked 6 hu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Semantic Robust Defence (SemRoDe)&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23439;&#35266;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#22686;&#24378;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#40065;&#26834;&#24615;&#65292;&#23398;&#20064;&#20102;&#33021;&#22815;&#36830;&#25509;&#23545;&#25239;&#39046;&#22495;&#21644;&#22522;&#26412;&#39046;&#22495;&#30340;&#40065;&#26834;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.18423</link><description>&lt;p&gt;
SemRoDe: &#23439;&#35266;&#23545;&#25239;&#35757;&#32451;&#20197;&#23398;&#20064;&#23545;&#21333;&#35789;&#32423;&#25915;&#20987;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18423
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Semantic Robust Defence (SemRoDe)&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23439;&#35266;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#22686;&#24378;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#40065;&#26834;&#24615;&#65292;&#23398;&#20064;&#20102;&#33021;&#22815;&#36830;&#25509;&#23545;&#25239;&#39046;&#22495;&#21644;&#22522;&#26412;&#39046;&#22495;&#30340;&#40065;&#26834;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#20173;&#28982;&#20196;&#20154;&#25285;&#24551;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#23545;&#25239;&#35757;&#32451;&#25216;&#26415;&#65292;&#20294;&#20854;&#22312;&#38450;&#24481;&#21333;&#35789;&#32423;&#25915;&#20987;&#26041;&#38754;&#30340;&#25913;&#36827;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#40065;&#26834;&#38450;&#24481;&#65288;SemRoDe&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#23439;&#35266;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#65292;&#26088;&#22312;&#22686;&#24378;LMs&#30340;&#40065;&#26834;&#24615;&#12290;&#21463;&#21040;&#22270;&#20687;&#39046;&#22495;&#26368;&#36817;&#30740;&#31350;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#35752;&#24182;&#30830;&#35748;&#22312;&#20687;&#35821;&#35328;&#36825;&#26679;&#30340;&#31163;&#25955;&#25968;&#25454;&#35774;&#32622;&#20013;&#65292;&#36890;&#36807;&#21333;&#35789;&#26367;&#25442;&#29983;&#25104;&#30340;&#23545;&#25239;&#26679;&#26412;&#30830;&#23454;&#23646;&#20110;&#19968;&#20010;&#23637;&#29616;&#19982;&#22522;&#26412;&#22495;&#20855;&#26377;&#39640;Wasserstein&#36317;&#31163;&#30340;&#23545;&#25239;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#19968;&#20010;&#33021;&#22815;&#36830;&#25509;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#40065;&#26834;&#34920;&#31034;&#12290;&#25105;&#20204;&#20551;&#35774;&#22914;&#26524;&#26679;&#26412;&#34987;&#25237;&#24433;&#21040;&#19968;&#20010;&#38750;&#23545;&#25239;&#39046;&#22495;&#65292;&#32780;&#26159;&#25237;&#24433;&#21040;&#19968;&#20010;&#20855;&#26377;&#26368;&#23567;&#20559;&#31227;&#30340;&#39046;&#22495;&#65292;&#37027;&#20040;&#21487;&#33021;&#20250;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18423v1 Announce Type: new  Abstract: Language models (LMs) are indispensable tools for natural language processing tasks, but their vulnerability to adversarial attacks remains a concern. While current research has explored adversarial training techniques, their improvements to defend against word-level attacks have been limited. In this work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a Macro Adversarial Training strategy to enhance the robustness of LMs. Drawing inspiration from recent studies in the image domain, we investigate and later confirm that in a discrete data setting such as language, adversarial samples generated via word substitutions do indeed belong to an adversarial domain exhibiting a high Wasserstein distance from the base domain. Our method learns a robust representation that bridges these two domains. We hypothesize that if samples were not projected into an adversarial domain, but instead to a domain with minimal shift, it w
&lt;/p&gt;</description></item><item><title>BioMedLM&#26159;&#19968;&#20010;27&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;PubMed&#25991;&#29486;&#19978;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#21644;&#24739;&#32773;&#25552;&#38382;&#12290;</title><link>https://arxiv.org/abs/2403.18421</link><description>&lt;p&gt;
BioMedLM&#65306;&#22522;&#20110;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#35757;&#32451;&#30340;27&#20159;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18421
&lt;/p&gt;
&lt;p&gt;
BioMedLM&#26159;&#19968;&#20010;27&#20159;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;PubMed&#25991;&#29486;&#19978;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#21644;&#24739;&#32773;&#25552;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18421v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;GPT-4&#21644;Med-PaLM 2&#31561;&#27169;&#22411;&#22312;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;NLP&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26377;&#25968;&#21315;&#20159;&#20010;&#21442;&#25968;&#65292;&#35745;&#31639;&#20195;&#20215;&#39640;&#26114;&#65292;&#38656;&#35201;&#29992;&#25143;&#36890;&#36807;&#20114;&#32852;&#32593;&#21457;&#36865;&#36755;&#20837;&#25968;&#25454;&#65292;&#24182;&#19988;&#26159;&#22312;&#26410;&#30693;&#25968;&#25454;&#26469;&#28304;&#19978;&#35757;&#32451;&#30340;&#12290;&#26356;&#23567;&#19988;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#27169;&#22411;&#33021;&#21542;&#31454;&#20105;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#24182;&#21457;&#24067;&#20102;BioMedLM&#65292;&#19968;&#20010;&#20165;&#22312;PubMed&#25688;&#35201;&#21644;&#20840;&#25991;&#19978;&#35757;&#32451;&#30340;27&#20159;&#21442;&#25968;GPT&#39118;&#26684;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#22312;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;BioMedLM&#21487;&#20197;&#20135;&#29983;&#24378;&#22823;&#30340;&#22810;&#39033;&#36873;&#25321;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#32467;&#26524;&#65292;&#19982;&#26356;&#22823;&#30340;&#27169;&#22411;&#31454;&#20105;&#65292;&#20363;&#22914;&#22312;MedMCQA&#65288;dev&#65289;&#19978;&#21462;&#24471;57.3%&#30340;&#24471;&#20998;&#65292;&#22312;MMLU&#21307;&#23398;&#36951;&#20256;&#23398;&#32771;&#35797;&#19978;&#21462;&#24471;69.0%&#30340;&#24471;&#20998;&#12290;BioMedLM&#36824;&#21487;&#20197;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#23545;&#21307;&#23398;&#35805;&#39064;&#19978;&#24739;&#32773;&#25552;&#20986;&#30340;&#38382;&#39064;&#25552;&#20379;&#26377;&#29992;&#30340;&#31572;&#26696;&#12290;&#36825;&#34920;&#26126;&#36739;&#23567;&#30340;&#27169;&#22411;&#28508;&#22312;&#22320;&#21487;&#20197;&#20316;&#20026;&#36879;&#26126;&#19988;&#38544;&#31169;&#24615;&#30340;&#26381;&#21153;&#25552;&#20379;&#32773;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18421v1 Announce Type: cross  Abstract: Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance on a wide variety of biomedical NLP tasks. However, these models have hundreds of billions of parameters, are computationally expensive to run, require users to send their input data over the internet, and are trained on unknown data sources. Can smaller, more targeted models compete? To address this question, we build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive model trained exclusively on PubMed abstracts and full articles. When fine-tuned, BioMedLM can produce strong multiple-choice biomedical question-answering results competitive with much larger models, such as achieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical Genetics exam. BioMedLM can also be fine-tuned to produce useful answers to patient questions on medical topics. This demonstrates that smaller models can potentially serve as transparent, privacy-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#21333;&#19968;&#30340;Vision Language Model (VLM) &#26469;&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#39057;&#38382;&#31572;&#65292;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#21333;&#20010;&#21512;&#25104;&#22270;&#20687;&#20197;&#23454;&#29616;&#35270;&#39057;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.18406</link><description>&lt;p&gt;
&#22270;&#20687;&#32593;&#26684;&#21487;&#33021;&#27604;&#35270;&#39057;&#26356;&#26377;&#20215;&#20540;&#65306;&#20351;&#29992;VLM&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#39057;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering Using a VLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#21333;&#19968;&#30340;Vision Language Model (VLM) &#26469;&#36827;&#34892;&#38646;&#26679;&#26412;&#35270;&#39057;&#38382;&#31572;&#65292;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#21333;&#20010;&#21512;&#25104;&#22270;&#20687;&#20197;&#23454;&#29616;&#35270;&#39057;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#20154;&#20204;&#25552;&#20986;&#20102;&#21508;&#31181;&#29992;&#20110;&#36830;&#25509;&#35270;&#39057;&#27169;&#24577;&#30340;&#31574;&#30053;&#12290;&#20854;&#20013;&#19968;&#31181;&#31361;&#20986;&#30340;&#31574;&#30053;&#28041;&#21450;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#65288;VideoLMs&#65289;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#25509;&#21475;&#23558;&#20808;&#36827;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#19982;LLMs&#36830;&#25509;&#36215;&#26469;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#21478;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#22810;&#20010;&#38454;&#27573;&#36328;&#27169;&#24577;&#36827;&#34892;&#27169;&#24577;&#26725;&#25509;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#29616;&#25104;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;VideoLMs&#21644;LLMs&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#21364;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#21482;&#20351;&#29992;&#21333;&#19968;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#12290;&#25105;&#20204;&#30340;&#20986;&#21457;&#28857;&#26159;&#35270;&#39057;&#21253;&#21547;&#19968;&#31995;&#21015;&#22270;&#20687;&#25110;&#24103;&#65292;&#36825;&#20123;&#22270;&#20687;&#19982;&#26102;&#38388;&#20449;&#24687;&#20132;&#32455;&#22312;&#19968;&#36215;&#30340;&#31616;&#21333;&#27934;&#23519;&#12290;&#35270;&#39057;&#29702;&#35299;&#30340;&#31934;&#39635;&#22312;&#20110;&#24039;&#22937;&#22320;&#31649;&#29702;&#27599;&#20010;&#24103;&#30340;&#26102;&#38388;&#26041;&#38754;&#20197;&#21450;&#31354;&#38388;&#32454;&#33410;&#12290;&#21021;&#22987;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#25490;&#21015;&#22810;&#20010;&#24103;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#21333;&#20010;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18406v1 Announce Type: cross  Abstract: Stimulated by the sophisticated reasoning capabilities of recent Large Language Models (LLMs), a variety of strategies for bridging video modality have been devised. A prominent strategy involves Video Language Models (VideoLMs), which train a learnable interface with video data to connect advanced vision encoders with LLMs. Recently, an alternative strategy has surfaced, employing readily available foundation models, such as VideoLMs and LLMs, across multiple stages for modality bridging. In this study, we introduce a simple yet novel strategy where only a single Vision Language Model (VLM) is utilized. Our starting point is the plain insight that a video comprises a series of images, or frames, interwoven with temporal information. The essence of video comprehension lies in adeptly managing the temporal aspects along with the spatial details of each frame. Initially, we transform a video into a single composite image by arranging mul
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20559;&#22909;&#23398;&#20064;&#24314;&#27169;&#21644;&#24341;&#20837;&#33258;&#21160;&#20559;&#22909;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#21487;&#38752;&#20869;&#23481;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#24402;&#22240;&#20559;&#22909;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.18381</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#22909;&#23398;&#20064;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23646;&#24615;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Attributed Text Generation of Large Language Models via Preference Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18381
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20559;&#22909;&#23398;&#20064;&#24314;&#27169;&#21644;&#24341;&#20837;&#33258;&#21160;&#20559;&#22909;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#21487;&#38752;&#20869;&#23481;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#24402;&#22240;&#20559;&#22909;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#29983;&#25104;&#19981;&#21487;&#38752;&#20869;&#23481;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24402;&#22240;&#20316;&#20026;&#25552;&#20379;&#35777;&#25454;&#65288;&#21363;&#24341;&#29992;&#65289;&#30340;&#25163;&#27573;&#26469;&#20943;&#23569;&#38169;&#35823;&#20449;&#24687;&#21644;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#24402;&#22240;&#26041;&#27861;&#36890;&#24120;&#20391;&#37325;&#20110;&#26816;&#32034;&#38454;&#27573;&#21644;&#33258;&#21160;&#35780;&#20272;&#65292;&#24573;&#35270;&#20102;&#22312;&#20154;&#31867;&#23398;&#26415;&#20889;&#20316;&#20013;&#21453;&#26144;&#24341;&#25991;&#26426;&#21046;&#20197;&#22686;&#24378;&#21487;&#20449;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#24402;&#22240;&#20219;&#21153;&#24314;&#27169;&#20026;&#20559;&#22909;&#23398;&#20064;&#65292;&#24182;&#24341;&#20837;&#33258;&#21160;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#25910;&#38598;&#21644;&#36807;&#28388;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;6,330&#20010;&#31034;&#20363;&#20379;&#21518;&#26399;&#35757;&#32451;&#20351;&#29992;&#30340;&#31934;&#24515;&#25910;&#38598;&#38598;&#21512;&#12290;&#20854;&#27425;&#65292;&#32771;&#34385;&#21040;&#26631;&#35760;&#20559;&#22909;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#24402;&#22240;&#20559;&#22909;&#25968;&#25454;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#29983;&#25104;&#20102;95,263&#23545;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18381v1 Announce Type: cross  Abstract: Large language models have been widely adopted in natural language processing, yet they face the challenge of generating unreliable content. Recent works aim to reduce misinformation and hallucinations by resorting to attribution as a means to provide evidence (i.e., citations). However, current attribution methods usually focus on the retrieval stage and automatic evaluation that neglect mirroring the citation mechanisms in human scholarly writing to bolster credibility. In this paper, we address these challenges by modelling the attribution task as preference learning and introducing an Automatic Preference Optimization (APO) framework. First, we create a curated collection for post-training with 6,330 examples by collecting and filtering from existing datasets. Second, considering the high cost of labelling preference data, we further propose an automatic method to synthesize attribution preference data resulting in 95,263 pairs. Mo
&lt;/p&gt;</description></item><item><title>BLADE&#26694;&#26550;&#23558;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#23567;&#22411;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#32467;&#21512;&#65292;&#26082;&#20445;&#30041;&#20102;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#19987;&#19994;&#35265;&#35299;&#65292;&#21448;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.18365</link><description>&lt;p&gt;
BLADE&#65306;&#21033;&#29992;&#23567;&#22411;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#22686;&#24378;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18365
&lt;/p&gt;
&lt;p&gt;
BLADE&#26694;&#26550;&#23558;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#23567;&#22411;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#32467;&#21512;&#65292;&#26082;&#20445;&#30041;&#20102;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#19987;&#19994;&#35265;&#35299;&#65292;&#21448;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#21644;GPT-4&#20855;&#26377;&#36890;&#29992;&#24615;&#21644;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#24320;&#25918;&#22495;&#25968;&#25454;&#19978;&#24320;&#21457;&#30340;&#36890;&#29992;LLMs&#21487;&#33021;&#32570;&#20047;&#22402;&#30452;&#39046;&#22495;&#65288;&#22914;&#27861;&#24459;&#12289;&#21307;&#23398;&#31561;&#65289;&#20219;&#21153;&#25152;&#38656;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#36827;&#34892;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#35201;&#20040;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#26469;&#25903;&#25345;&#36890;&#29992;LLMs&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#31574;&#30053;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#35201;&#20040;&#25104;&#26412;&#39640;&#26114;&#65292;&#35201;&#20040;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;BLADE&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#23427;&#22686;&#24378;&#20102;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#23567;&#22411;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18365v1 Announce Type: new  Abstract: Large Language Models (LLMs) like ChatGPT and GPT-4 are versatile and capable of addressing a diverse range of tasks. However, general LLMs, which are developed on open-domain data, may lack the domain-specific knowledge essential for tasks in vertical domains, such as legal, medical, etc. To address this issue, previous approaches either conduct continuous pre-training with domain-specific data or employ retrieval augmentation to support general LLMs. Unfortunately, these strategies are either cost-intensive or unreliable in practical applications. To this end, we present a novel framework named BLADE, which enhances Black-box LArge language models with small Domain-spEcific models. BLADE consists of a black-box LLM and a small domain-specific LM. The small LM preserves domain-specific knowledge and offers specialized insights, while the general LLM contributes robust language comprehension and reasoning capabilities. Specifically, our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#38463;&#25289;&#20271;&#35821;&#20041;&#25628;&#32034;&#30340;&#22522;&#20934;&#65292;&#24182;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26694;&#26550;&#20869;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18350</link><description>&lt;p&gt;
&#35780;&#20272;&#35821;&#20041;&#25628;&#32034;&#21450;&#20854;&#22312;&#38463;&#25289;&#20271;&#35821;&#35328;&#20013;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Semantic Search and its Role in Retrieved-Augmented-Generation (RAG) for Arabic Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#38463;&#25289;&#20271;&#35821;&#20041;&#25628;&#32034;&#30340;&#22522;&#20934;&#65292;&#24182;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26694;&#26550;&#20869;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#24102;&#26469;&#20102;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#27010;&#24565;&#65292;&#24050;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#35777;&#26126;&#20855;&#26377;&#24040;&#22823;&#30410;&#22788;&#65292;&#22823;&#22823;&#21462;&#20195;&#20102;&#20851;&#38190;&#35789;&#25628;&#32034;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#35821;&#20041;&#30456;&#20284;&#24615;&#24182;&#22312;&#21508;&#31181;&#25991;&#26723;&#20013;&#20026;&#29305;&#23450;&#26597;&#35810;&#36827;&#34892;&#25628;&#32034;&#20173;&#28982;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#28304;&#20110;&#20219;&#21153;&#30340;&#22810;&#26041;&#38754;&#24615;&#65292;&#32570;&#20047;&#26631;&#20934;&#22522;&#20934;&#65292;&#32780;&#36825;&#20123;&#25361;&#25112;&#23545;&#20110;&#38463;&#25289;&#20271;&#35821;&#35328;&#32780;&#35328;&#26356;&#21152;&#22797;&#26434;&#12290;&#26412;&#25991;&#33268;&#21147;&#20110;&#20026;&#38463;&#25289;&#20271;&#35821;&#20041;&#25628;&#32034;&#24314;&#31435;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#31934;&#30830;&#35780;&#20272;&#36825;&#20123;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26694;&#26550;&#20869;&#36827;&#34892;&#35821;&#20041;&#25628;&#32034;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18350v1 Announce Type: new  Abstract: The latest advancements in machine learning and deep learning have brought forth the concept of semantic similarity, which has proven immensely beneficial in multiple applications and has largely replaced keyword search. However, evaluating semantic similarity and conducting searches for a specific query across various documents continue to be a complicated task. This complexity is due to the multifaceted nature of the task, the lack of standard benchmarks, whereas these challenges are further amplified for Arabic language. This paper endeavors to establish a straightforward yet potent benchmark for semantic search in Arabic. Moreover, to precisely evaluate the effectiveness of these metrics and the dataset, we conduct our assessment of semantic search within the framework of retrieval augmented generation (RAG).
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25298;&#32477;&#26426;&#21046;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#38752;&#24615;&#20013;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;RLKF&#12290;</title><link>https://arxiv.org/abs/2403.18349</link><description>&lt;p&gt;
&#25298;&#32477;&#25552;&#39640;&#21487;&#38752;&#24615;&#65306;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#30693;&#35782;&#21453;&#39304;&#35757;&#32451;LLMs&#25298;&#32477;&#26410;&#30693;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18349
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25298;&#32477;&#26426;&#21046;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#38752;&#24615;&#20013;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;RLKF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#29983;&#25104;&#38169;&#35823;&#36755;&#20986;&#65292;&#34987;&#31216;&#20026;&#24187;&#24819;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#36776;&#21035;&#36229;&#20986;&#20854;&#30693;&#35782;&#33539;&#22260;&#30340;&#38382;&#39064;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;&#34429;&#28982;&#35299;&#20915;&#24187;&#24819;&#19968;&#30452;&#26159;&#30740;&#31350;&#30340;&#28966;&#28857;&#65292;&#20197;&#24448;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#27491;&#30830;&#24615;&#32780;&#26410;&#20805;&#20998;&#32771;&#34385;&#25298;&#32477;&#26426;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#25298;&#32477;&#30340;&#20316;&#29992;&#65292;&#24341;&#20837;&#20102;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#27010;&#24565;&#20197;&#21450;&#30456;&#24212;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#34913;&#37327;&#20102;&#27169;&#22411;&#22312;&#25552;&#20379;&#20934;&#30830;&#21709;&#24212;&#30340;&#21516;&#26102;&#65292;&#28789;&#27963;&#25298;&#32477;&#36229;&#20986;&#20854;&#30693;&#35782;&#36793;&#30028;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#24187;&#24819;&#12290;&#20026;&#20102;&#25552;&#39640;LLMs&#22266;&#26377;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLKF&#65289;&#30340;&#26032;&#23545;&#40784;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18349v1 Announce Type: new  Abstract: Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope. While addressing hallucination has been a focal point in research, previous efforts primarily concentrate on enhancing correctness without giving due consideration to the significance of rejection mechanisms. In this paper, we conduct a comprehensive examination of the role of rejection, introducing the notion of model reliability along with corresponding metrics. These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby minimizing hallucinations. To improve the inherent reliability of LLMs, we present a novel alignment framework called Reinforcement Learning from Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically determine the model's knowledge boundary and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#29992;&#20110;&#35299;&#37322;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#20013;&#30340;&#20559;&#24046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;MORE&#65292;&#21516;&#26102;&#25552;&#20986;&#20004;&#31181;&#20943;&#36731;&#21333;&#27169;&#24577;&#20559;&#24046;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.18346</link><description>&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#21644;&#20943;&#36731;&#21333;&#27169;&#24577;&#20559;&#24046;&#65306;&#22240;&#26524;&#20851;&#31995;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18346
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#29992;&#20110;&#35299;&#37322;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#38382;&#39064;&#20013;&#30340;&#20559;&#24046;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;MORE&#65292;&#21516;&#26102;&#25552;&#20986;&#20004;&#31181;&#20943;&#36731;&#21333;&#27169;&#24577;&#20559;&#24046;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#36827;&#20102;&#22810;&#27169;&#24577;LLMs&#65288;MLLMs&#65289;&#30340;&#21457;&#23637;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;MLLMs&#36890;&#24120;&#36807;&#24230;&#20381;&#36182;&#21333;&#27169;&#24577;&#20559;&#24046;&#65288;&#20363;&#22914;&#35821;&#35328;&#20559;&#24046;&#21644;&#35270;&#35273;&#20559;&#24046;&#65289;&#65292;&#23548;&#33268;&#22312;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#32473;&#20986;&#19981;&#27491;&#30830;&#31572;&#26696;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#26694;&#26550;&#26469;&#35299;&#37322;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#38382;&#39064;&#20013;&#30340;&#20559;&#24046;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22240;&#26524;&#22270;&#26469;&#38416;&#26126;MLLMs&#23545;VQA&#38382;&#39064;&#30340;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#28145;&#20837;&#30340;&#22240;&#26524;&#20998;&#26512;&#35780;&#20272;&#20559;&#24046;&#30340;&#22240;&#26524;&#25928;&#26524;&#12290;&#21463;&#22240;&#26524;&#22270;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;MORE&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;12,000&#20010;VQA&#23454;&#20363;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#25361;&#25112;MLLMs&#30340;&#33021;&#21147;&#65292;&#38656;&#35201;&#22810;&#36339;&#25512;&#29702;&#21644;&#20811;&#26381;&#21333;&#27169;&#24577;&#20559;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#20943;&#36731;&#21333;&#27169;&#24577;&#20559;&#24046;&#24182;&#22686;&#24378;MLLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18346v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have facilitated the development of Multimodal LLMs (MLLMs). Despite their impressive capabilities, MLLMs often suffer from an over-reliance on unimodal biases (e.g., language bias and vision bias), leading to incorrect answers in complex multimodal tasks. To investigate this issue, we propose a causal framework to interpret the biases in Visual Question Answering (VQA) problems. Within our framework, we devise a causal graph to elucidate the predictions of MLLMs on VQA problems, and assess the causal effect of biases through an in-depth causal analysis. Motivated by the causal graph, we introduce a novel MORE dataset, consisting of 12,000 VQA instances. This dataset is designed to challenge MLLMs' abilities, necessitating multi-hop reasoning and the surmounting of unimodal biases. Furthermore, we propose two strategies to mitigate unimodal biases and enhance MLLMs' reasoning capabiliti
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;IterAlign&#65292;&#26159;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#23466;&#27861;&#21457;&#29616;&#21644;&#33258;&#23545;&#40784;&#26694;&#26550;&#65292;&#36890;&#36807;&#32418;&#38431;&#27979;&#35797;&#21457;&#29616;LLM&#30340;&#24369;&#28857;&#65292;&#24182;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;LLM&#33258;&#21160;&#21457;&#29616;&#26032;&#30340;&#23466;&#27861;&#65292;&#20174;&#32780;&#25351;&#23548;LLM&#30340;&#33258;&#26657;&#27491;&#12290;</title><link>https://arxiv.org/abs/2403.18341</link><description>&lt;p&gt;
IterAlign: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36845;&#20195;&#24335;&#23466;&#27861;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
IterAlign: Iterative Constitutional Alignment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18341
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;IterAlign&#65292;&#26159;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#23466;&#27861;&#21457;&#29616;&#21644;&#33258;&#23545;&#40784;&#26694;&#26550;&#65292;&#36890;&#36807;&#32418;&#38431;&#27979;&#35797;&#21457;&#29616;LLM&#30340;&#24369;&#28857;&#65292;&#24182;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;LLM&#33258;&#21160;&#21457;&#29616;&#26032;&#30340;&#23466;&#27861;&#65292;&#20174;&#32780;&#25351;&#23548;LLM&#30340;&#33258;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23558;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#31038;&#20250;&#35268;&#33539;&#23545;&#40784;&#20197;&#30830;&#20445;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#21644;&#23466;&#27861;AI&#65288;CAI&#65289;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;LLM&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#27714;&#22823;&#37327;&#30340;&#20154;&#31867;&#27880;&#37322;&#25110;&#26126;&#30830;&#39044;&#23450;&#20041;&#30340;&#23466;&#27861;&#65292;&#36825;&#20123;&#37117;&#26159;&#21171;&#21160;&#23494;&#38598;&#22411;&#21644;&#36164;&#28304;&#28040;&#32791;&#22411;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#23466;&#27861;&#30340;LLM&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IterAlign&#30340;&#25968;&#25454;&#39537;&#21160;&#23466;&#27861;&#21457;&#29616;&#21644;&#33258;&#23545;&#40784;&#26694;&#26550;&#12290;IterAlign&#21033;&#29992;&#32418;&#38431;&#27979;&#35797;&#25581;&#31034;LLM&#30340;&#24369;&#28857;&#65292;&#24182;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;LLM&#33258;&#21160;&#21457;&#29616;&#26032;&#30340;&#23466;&#27861;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#23466;&#27861;&#34987;&#29992;&#26469;&#25351;&#23548;&#22522;&#20110;LLM&#30340;&#33258;&#26657;&#27491;&#12290;&#36825;&#31181;&#23466;&#27861;&#21457;&#29616;&#27969;&#31243;&#21487;&#20197;&#36845;&#20195;&#33258;&#21160;&#36816;&#34892;&#65292;&#20197;&#21457;&#29616;&#38024;&#23545;&#29305;&#23450;LLMs&#30340;&#26032;&#23466;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18341v1 Announce Type: new  Abstract: With the rapid development of large language models (LLMs), aligning LLMs with human values and societal norms to ensure their reliability and safety has become crucial. Reinforcement learning with human feedback (RLHF) and Constitutional AI (CAI) have been proposed for LLM alignment. However, these methods require either heavy human annotations or explicitly pre-defined constitutions, which are labor-intensive and resource-consuming. To overcome these drawbacks, we study constitution-based LLM alignment and propose a data-driven constitution discovery and self-alignment framework called IterAlign. IterAlign leverages red teaming to unveil the weaknesses of an LLM and automatically discovers new constitutions using a stronger LLM. These constitutions are then used to guide self-correction of the base LLM. Such a constitution discovery pipeline can be run iteratively and automatically to discover new constitutions that specifically target
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#33647;&#29289;&#19981;&#33391;&#21453;&#24212;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#24503;&#35821;&#12289;&#27861;&#35821;&#21644;&#26085;&#35821;&#25991;&#26412;&#65292;&#21253;&#21547;&#20102;12&#31181;&#23454;&#20307;&#31867;&#22411;&#12289;&#22235;&#31181;&#23646;&#24615;&#31867;&#22411;&#21644;13&#31181;&#20851;&#31995;&#31867;&#22411;&#30340;&#27880;&#37322;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24320;&#21457;&#29616;&#23454;&#19990;&#30028;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.18336</link><description>&lt;p&gt;
&#38754;&#21521;&#24503;&#35821;&#12289;&#27861;&#35821;&#21644;&#26085;&#35821;&#30340;&#33647;&#29289;&#35686;&#25106;&#25968;&#25454;&#38598;&#65306;&#36328;&#35821;&#35328;&#26631;&#27880;&#33647;&#29289;&#19981;&#33391;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
A Dataset for Pharmacovigilance in German, French, and Japanese: Annotating Adverse Drug Reactions across Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#33647;&#29289;&#19981;&#33391;&#21453;&#24212;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#24503;&#35821;&#12289;&#27861;&#35821;&#21644;&#26085;&#35821;&#25991;&#26412;&#65292;&#21253;&#21547;&#20102;12&#31181;&#23454;&#20307;&#31867;&#22411;&#12289;&#22235;&#31181;&#23646;&#24615;&#31867;&#22411;&#21644;13&#31181;&#20851;&#31995;&#31867;&#22411;&#30340;&#27880;&#37322;&#65292;&#20026;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24320;&#21457;&#29616;&#23454;&#19990;&#30028;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#28304;&#22312;&#25581;&#31034;&#33647;&#29289;&#19981;&#33391;&#21453;&#24212;&#65288;ADRs&#65289;&#26041;&#38754;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#35752;&#35770;&#25968;&#37327;&#20063;&#22312;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20020;&#24202;&#35821;&#26009;&#24211;&#20027;&#35201;&#22260;&#32469;&#33521;&#25991;&#31185;&#23398;&#25991;&#31456;&#23637;&#24320;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#25324;&#24503;&#35821;&#12289;&#27861;&#35821;&#21644;&#26085;&#35821;&#20013;&#20851;&#20110;ADR&#30340;&#25991;&#26412;&#30340;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#26469;&#28304;&#21253;&#25324;&#24739;&#32773;&#35770;&#22363;&#12289;&#31038;&#20132;&#23186;&#20307;&#21644;&#20020;&#24202;&#25253;&#21578;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#21253;&#21547;&#20102;12&#31181;&#23454;&#20307;&#31867;&#22411;&#12289;&#22235;&#31181;&#23646;&#24615;&#31867;&#22411;&#21644;13&#31181;&#20851;&#31995;&#31867;&#22411;&#30340;&#27880;&#37322;&#12290;&#36825;&#26377;&#21161;&#20110;&#20026;&#21307;&#30103;&#20445;&#20581;&#24320;&#21457;&#29616;&#23454;&#19990;&#30028;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#32479;&#35745;&#25968;&#25454;&#20197;&#20984;&#26174;&#19982;&#35821;&#26009;&#24211;&#30456;&#20851;&#30340;&#26576;&#20123;&#25361;&#25112;&#65292;&#24182;&#36827;&#34892;&#20102;&#21021;&#27493;&#23454;&#39564;&#65292;&#20026;&#22312;&#21333;&#19968;&#35821;&#35328;&#20869;&#37096;&#21644;&#36328;&#35821;&#35328;&#20043;&#38388;&#25552;&#21462;&#23454;&#20307;&#21644;&#23454;&#20307;&#20043;&#38388;&#20851;&#31995;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18336v1 Announce Type: new  Abstract: User-generated data sources have gained significance in uncovering Adverse Drug Reactions (ADRs), with an increasing number of discussions occurring in the digital world. However, the existing clinical corpora predominantly revolve around scientific articles in English. This work presents a multilingual corpus of texts concerning ADRs gathered from diverse sources, including patient fora, social media, and clinical reports in German, French, and Japanese. Our corpus contains annotations covering 12 entity types, four attribute types, and 13 relation types. It contributes to the development of real-world multilingual language models for healthcare. We provide statistics to highlight certain challenges associated with the corpus and conduct preliminary experiments resulting in strong baselines for extracting entities and relations between these entities, both within and across languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20004;&#20010;LLM&#30340;&#21103;&#26412;&#19982;&#39564;&#35777;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#33021;&#22815;&#33258;&#21160;&#35780;&#20272;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#27491;&#24335;&#35268;&#33539;&#20043;&#38388;&#36716;&#25442;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#24037;&#36755;&#20837;&#12290;</title><link>https://arxiv.org/abs/2403.18327</link><description>&lt;p&gt;
LLM&#21487;&#20197;&#36827;&#34892;&#27491;&#24335;&#23545;&#35805;&#21527;&#65311;&#33258;&#21160;&#35780;&#20272;LLMs&#22312;&#36716;&#25442;&#21644;&#35299;&#37322;&#27491;&#24335;&#35268;&#33539;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Converse Formally? Automatically Assessing LLMs in Translating and Interpreting Formal Specifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#20004;&#20010;LLM&#30340;&#21103;&#26412;&#19982;&#39564;&#35777;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#33021;&#22815;&#33258;&#21160;&#35780;&#20272;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#27491;&#24335;&#35268;&#33539;&#20043;&#38388;&#36716;&#25442;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20154;&#24037;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#30410;&#30456;&#20851;&#32773;&#32463;&#24120;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#31995;&#32479;&#38656;&#27714;&#65292;&#28982;&#21518;&#30001;&#39046;&#22495;&#19987;&#23478;&#23558;&#20854;&#36716;&#25442;&#20026;&#24418;&#24335;&#21270;&#35821;&#27861;&#65292;&#20174;&#32780;&#22686;&#21152;&#35774;&#35745;&#25104;&#26412;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#27491;&#24335;&#35268;&#33539;&#20043;&#38388;&#36716;&#25442;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#20004;&#20010;LLM&#30340;&#21103;&#26412;&#19982;&#29616;&#25104;&#30340;&#39564;&#35777;&#22120;&#32467;&#21512;&#20351;&#29992;&#65292;&#26080;&#38656;&#20219;&#20309;&#39069;&#22806;&#30340;&#20154;&#24037;&#36755;&#20837;&#23601;&#21487;&#20197;&#33258;&#21160;&#35780;&#20272;&#20854;&#32763;&#35793;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#35821;&#35328;&#35821;&#27861;&#29983;&#25104;&#24418;&#24335;&#21270;&#35821;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#32463;&#39564;&#35780;&#20272;&#20197;&#34913;&#37327;&#36825;&#31181;&#32763;&#35793;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18327v1 Announce Type: cross  Abstract: Stakeholders often describe system requirements using natural language which are then converted to formal syntax by a domain-expert leading to increased design costs. This paper assesses the capabilities of Large Language Models (LLMs) in converting between natural language descriptions and formal specifications. Existing work has evaluated the capabilities of LLMs in generating formal syntax such as source code but such experiments are typically hand-crafted and use problems that are likely to be in the training set of LLMs, and often require human-annotated datasets. We propose an approach that can use two copies of an LLM in conjunction with an off-the-shelf verifier to automatically evaluate its translation abilities without any additional human input. Our approach generates formal syntax using language grammars to automatically generate a dataset. We conduct an empirical evaluation to measure the accuracy of this translation task 
&lt;/p&gt;</description></item><item><title>&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#20013;&#25991;&#20013;&#26816;&#27979; offensive &#35821;&#35328;&#30340;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#24320;&#21457;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#29305;&#23450;&#27169;&#22411;&#21644;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.18314</link><description>&lt;p&gt;
&#20013;&#22269; offensive &#35821;&#35328;&#26816;&#27979;&#65306;&#29616;&#29366;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Chinese Offensive Language Detection:Current Status and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18314
&lt;/p&gt;
&lt;p&gt;
&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#20013;&#25991;&#20013;&#26816;&#27979; offensive &#35821;&#35328;&#30340;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#24320;&#21457;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#29305;&#23450;&#27169;&#22411;&#21644;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#27491;&#22312;&#20570;&#20986;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#30417;&#27979;&#21644;&#35268;&#33539;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#65292;&#20294;&#22312;&#25968;&#23383;&#31354;&#38388;&#20013;&#65292;&#24694;&#24847;&#35821;&#35328;&#65288;&#22914;&#20167;&#24680;&#35328;&#35770;&#25110;&#32593;&#32476;&#27450;&#20940;&#65289;&#30340;&#26222;&#36941;&#23384;&#22312;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#37492;&#20110;&#32500;&#25252;&#25991;&#26126;&#21644;&#23562;&#37325;&#30340;&#22312;&#32447;&#29615;&#22659;&#30340;&#37325;&#35201;&#24615;&#65292;&#36843;&#20999;&#38656;&#35201;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#24694;&#24847;&#35328;&#35770;&#30340;&#33258;&#21160;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#24320;&#21457;&#22788;&#29702;&#27721;&#35821;&#31561;&#35821;&#35328;&#30340;&#26377;&#25928;&#31995;&#32479;&#65292;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#35821;&#35328;&#30340;&#22797;&#26434;&#21644;&#24494;&#22937;&#24615;&#20351;&#24471;&#33258;&#21160;&#22788;&#29702;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#20840;&#38754;&#24635;&#32467;&#20102;&#20013;&#22269; offensive &#35821;&#35328;&#26816;&#27979;&#24773;&#20917;&#65292;&#23457;&#26597;&#20102;&#24403;&#21069;&#30340;&#22522;&#20934;&#21644;&#26041;&#27861;&#65292;&#24182;&#37325;&#28857;&#20171;&#32461;&#20102;&#29992;&#20110;&#35299;&#20915;&#22312;&#36825;&#31181;&#22797;&#26434;&#35821;&#35328;&#20013;&#26816;&#27979;&#24694;&#24847;&#35821;&#35328;&#30340;&#29420;&#29305;&#25361;&#25112;&#30340;&#29305;&#23450;&#27169;&#22411;&#21644;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18314v1 Announce Type: cross  Abstract: Despite the considerable efforts being made to monitor and regulate user-generated content on social media platforms, the pervasiveness of offensive language, such as hate speech or cyberbullying, in the digital space remains a significant challenge. Given the importance of maintaining a civilized and respectful online environment, there is an urgent and growing need for automatic systems capable of detecting offensive speech in real time. However, developing effective systems for processing languages such as Chinese presents a significant challenge, owing to the language's complex and nuanced nature, which makes it difficult to process automatically. This paper provides a comprehensive overview of offensive language detection in Chinese, examining current benchmarks and approaches and highlighting specific models and tools for addressing the unique challenges of detecting offensive language in this complex language. The primary object
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21452;&#37325;&#25351;&#20196;&#35843;&#25972;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#20013;&#38388;&#25512;&#29702;&#29366;&#24577;&#39044;&#27979;&#20219;&#21153;&#21644;&#25351;&#20196;&#37325;&#26500;&#20219;&#21153;&#65292;&#20174;&#21069;&#21521;&#21644;&#21518;&#21521;&#20004;&#20010;&#26041;&#21521;&#31934;&#23494;&#24314;&#27169;&#25968;&#23398;&#25512;&#29702;&#65292;&#20197;&#22686;&#24378;LLMs&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#21644;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.18295</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25968;&#23398;&#25512;&#29702;&#30340;&#21452;&#37325;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Dual Instruction Tuning with Large Language Models for Mathematical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18295
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21452;&#37325;&#25351;&#20196;&#35843;&#25972;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#20013;&#38388;&#25512;&#29702;&#29366;&#24577;&#39044;&#27979;&#20219;&#21153;&#21644;&#25351;&#20196;&#37325;&#26500;&#20219;&#21153;&#65292;&#20174;&#21069;&#21521;&#21644;&#21518;&#21521;&#20004;&#20010;&#26041;&#21521;&#31934;&#23494;&#24314;&#27169;&#25968;&#23398;&#25512;&#29702;&#65292;&#20197;&#22686;&#24378;LLMs&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#21644;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#31361;&#26174;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#21033;&#29992;CoT&#25968;&#25454;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#30340;&#25104;&#21151;&#12290;&#23613;&#31649;&#36827;&#34892;&#20102;&#31934;&#32454;&#35843;&#25972;&#30340;LLMs&#65292;&#22312;CoT&#29983;&#25104;&#20013;&#20173;&#23384;&#22312;&#38169;&#35823;&#12289;&#32570;&#22833;&#21644;&#22810;&#20313;&#30340;&#27493;&#39588;&#65292;&#23548;&#33268;&#31572;&#26696;&#39044;&#27979;&#19981;&#20934;&#30830;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#25351;&#20196;&#35843;&#25972;&#31574;&#30053;&#65292;&#20197;&#20174;&#21069;&#21521;&#21644;&#21518;&#21521;&#20004;&#20010;&#26041;&#21521;&#31934;&#23494;&#24314;&#27169;&#25968;&#23398;&#25512;&#29702;&#12290;&#36825;&#28041;&#21450;&#24341;&#20837;&#20013;&#38388;&#25512;&#29702;&#29366;&#24577;&#39044;&#27979;&#20219;&#21153;&#65288;&#21069;&#21521;&#25512;&#29702;&#65289;&#21644;&#25351;&#20196;&#37325;&#26500;&#20219;&#21153;&#65288;&#21518;&#21521;&#25512;&#29702;&#65289;&#65292;&#20197;&#22686;&#24378;LLMs&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#21644;&#25191;&#34892;&#12290;&#36825;&#20123;&#20219;&#21153;&#30340;&#35757;&#32451;&#23454;&#20363;&#26159;&#22522;&#20110;&#29616;&#26377;&#30340;&#25968;&#23398;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#12290;&#38543;&#21518;&#65292;LLMs&#32463;&#36807;&#22810;&#20219;&#21153;&#24494;&#35843;&#65292;&#21516;&#26102;&#20351;&#29992;&#29616;&#26377;&#30340;&#25968;&#23398;&#25351;&#20196;&#21644;&#26032;&#21019;&#24314;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18295v1 Announce Type: new  Abstract: Recent advancements highlight the success of instruction tuning with large language models (LLMs) utilizing Chain-of-Thought (CoT) data for mathematical reasoning tasks. Despite the fine-tuned LLMs, challenges persist, such as incorrect, missing, and redundant steps in CoT generation leading to inaccuracies in answer predictions. To alleviate this problem, we propose a dual instruction tuning strategy to meticulously model mathematical reasoning from both forward and reverse directions. This involves introducing the Intermediate Reasoning State Prediction task (forward reasoning) and the Instruction Reconstruction task (reverse reasoning) to enhance the LLMs' understanding and execution of instructions. Training instances for these tasks are constructed based on existing mathematical instruction tuning datasets. Subsequently, LLMs undergo multi-task fine-tuning using both existing mathematical instructions and the newly created data. Com
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#29305;&#23450;&#20999;&#29255;&#37325;&#26657;&#20934;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#20219;&#24847;&#20998;&#24067;&#29255;&#27573;&#19978;&#33719;&#24471;&#26657;&#20934;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.18286</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#37325;&#26657;&#20934;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Recalibration of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18286
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#29305;&#23450;&#20999;&#29255;&#37325;&#26657;&#20934;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#22312;&#20219;&#24847;&#20998;&#24067;&#29255;&#27573;&#19978;&#33719;&#24471;&#26657;&#20934;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#20123;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#25552;&#21462;&#20986;&#26657;&#20934;&#33391;&#22909;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#20854;&#20013;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#21453;&#26144;&#20102;&#20854;&#27491;&#30830;&#24615;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;LMs&#22312;&#24191;&#27867;&#20998;&#24067;&#19978;&#21487;&#33021;&#20855;&#26377;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#65292;&#20294;&#36825;&#24448;&#24448;&#38544;&#34255;&#22312;&#26356;&#31364;&#20998;&#29255;&#20869;&#23384;&#22312;&#26174;&#33879;&#30340;&#26657;&#20934;&#19981;&#20934;&#30830;&#24615;&#65288;&#20363;&#22914;&#65292;&#22312;&#25968;&#23398;&#20013;&#23384;&#22312;&#31995;&#32479;&#24615;&#36807;&#24230;&#33258;&#20449;&#21487;&#33021;&#20250;&#24179;&#34913;&#21382;&#21490;&#20013;&#30340;&#31995;&#32479;&#24615;&#19981;&#36275;&#33258;&#20449;&#65292;&#20174;&#32780;&#22312;&#24635;&#20307;&#19978;&#23454;&#29616;&#23436;&#32654;&#26657;&#20934;&#65289;&#12290;&#20026;&#20102;&#33719;&#24471;&#20219;&#20309;&#20998;&#24067;&#29255;&#27573;&#30340;&#26657;&#20934;&#33391;&#22909;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#29305;&#23450;&#20999;&#29255;&#37325;&#26657;&#20934;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#37325;&#26032;&#26657;&#20934;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25509;&#21463;&#26469;&#33258;&#20219;&#20309;&#32473;&#23450;&#20999;&#29255;&#30340;&#23569;&#37327;&#26080;&#26631;&#31614;&#31034;&#20363;&#65292;&#24182;&#39044;&#27979;&#19968;&#26465;&#37325;&#26032;&#26144;&#23556;&#32622;&#20449;&#24230;&#20998;&#25968;&#20197;&#20351;&#20854;&#23545;&#35813;&#20999;&#29255;&#26356;&#20934;&#30830;&#30340;&#26354;&#32447;&#12290;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#37325;&#26032;&#26657;&#20934;&#20219;&#24847;&#26032;&#30340;&#20999;&#29255;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#35813;&#20999;&#29255;&#30340;&#20219;&#20309;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;d
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18286v1 Announce Type: cross  Abstract: Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify d
&lt;/p&gt;</description></item><item><title>BlendX&#25552;&#20986;&#20102;&#19968;&#22871;&#31934;&#21046;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#22522;&#20110;&#35268;&#21017;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;OpenAI&#30340;ChatGPT&#29983;&#25104;&#24037;&#20855;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#25552;&#39640;&#22810;&#24847;&#22270;&#26816;&#27979;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.18277</link><description>&lt;p&gt;
BlendX&#65306;&#34701;&#21512;&#27169;&#24335;&#30340;&#22797;&#26434;&#22810;&#24847;&#22270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
BlendX: Complex Multi-Intent Detection with Blended Patterns
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18277
&lt;/p&gt;
&lt;p&gt;
BlendX&#25552;&#20986;&#20102;&#19968;&#22871;&#31934;&#21046;&#25968;&#25454;&#38598;&#65292;&#37319;&#29992;&#22810;&#26679;&#21270;&#27169;&#24335;&#12289;&#22522;&#20110;&#35268;&#21017;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;OpenAI&#30340;ChatGPT&#29983;&#25104;&#24037;&#20855;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#25552;&#39640;&#22810;&#24847;&#22270;&#26816;&#27979;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18277v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25552;&#35201;&#65306;&#20197;&#20219;&#21153;&#20026;&#23548;&#21521;&#30340;&#23545;&#35805;&#65288;TOD&#65289;&#31995;&#32479;&#36890;&#24120;&#35774;&#35745;&#26102;&#20551;&#23450;&#27599;&#20010;&#35805;&#35821;&#20195;&#34920;&#19968;&#20010;&#24847;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#19981;&#20934;&#30830;&#21453;&#26144;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24773;&#20917;&#65292;&#22312;&#37027;&#37324;&#29992;&#25143;&#32463;&#24120;&#22312;&#19968;&#20010;&#35805;&#35821;&#20013;&#34920;&#36798;&#22810;&#20010;&#24847;&#22270;&#12290;&#34429;&#28982;&#22810;&#24847;&#22270;&#26816;&#27979;&#65288;MID&#65289;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#65292;&#20294;&#29616;&#26377;&#30340;&#39046;&#22495;&#20869;&#25968;&#25454;&#38598;&#65288;&#22914;MixATIS&#21644;MixSNIPS&#65289;&#22312;&#20854;&#21046;&#23450;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BlendX&#65292;&#19968;&#22871;&#31934;&#21046;&#25968;&#25454;&#38598;&#65292;&#20854;&#29305;&#24449;&#27169;&#24335;&#27604;&#20854;&#21069;&#36523;&#26356;&#22810;&#26679;&#21270;&#65292;&#25552;&#21319;&#20102;&#20854;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#23545;&#20110;&#25968;&#25454;&#38598;&#26500;&#24314;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#19968;&#20010;&#29983;&#25104;&#24037;&#20855;--OpenAI&#30340;ChatGPT--&#65292;&#24182;&#37319;&#29992;&#20102;&#30456;&#20284;&#24615;&#39537;&#21160;&#31574;&#30053;&#36827;&#34892;&#35805;&#35821;&#36873;&#25321;&#12290;&#20026;&#20102;&#30830;&#20445;&#25152;&#25552;&#20986;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19977;&#20010;&#35780;&#20272;&#35805;&#35821;&#32479;&#35745;&#23646;&#24615;&#30340;&#26032;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18277v1 Announce Type: new  Abstract: Task-oriented dialogue (TOD) systems are commonly designed with the presumption that each utterance represents a single intent. However, this assumption may not accurately reflect real-world situations, where users frequently express multiple intents within a single utterance. While there is an emerging interest in multi-intent detection (MID), existing in-domain datasets such as MixATIS and MixSNIPS have limitations in their formulation. To address these issues, we present BlendX, a suite of refined datasets featuring more diverse patterns than their predecessors, elevating both its complexity and diversity. For dataset construction, we utilize both rule-based heuristics as well as a generative tool -- OpenAI's ChatGPT -- which is augmented with a similarity-driven strategy for utterance selection. To ensure the quality of the proposed datasets, we also introduce three novel metrics that assess the statistical properties of an utterance
&lt;/p&gt;</description></item><item><title>Mamba&#27169;&#22411;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32463;&#20856;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;--&#25991;&#26723;&#25490;&#21517;&#20013;&#23637;&#29616;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18276</link><description>&lt;p&gt;
RankMamba&#65292;&#22312;Transformer&#26102;&#20195;&#23545;Mamba&#25991;&#26723;&#25490;&#21517;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era of Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18276
&lt;/p&gt;
&lt;p&gt;
Mamba&#27169;&#22411;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32463;&#20856;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;--&#25991;&#26723;&#25490;&#21517;&#20013;&#23637;&#29616;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#32467;&#26500;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#20449;&#24687;&#26816;&#32034;(IR)&#31561;&#22810;&#20010;&#24212;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;Transformer&#26550;&#26500;&#30340;&#26680;&#24515;&#26426;&#21046;--&#27880;&#24847;&#21147;&#65292;&#22312;&#35757;&#32451;&#20013;&#38656;&#35201;$O(n^2)$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#22312;&#25512;&#26029;&#20013;&#38656;&#35201;$O(n)$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#35768;&#22810;&#24037;&#20316;&#24050;&#32463;&#25552;&#20986;&#25913;&#36827;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#27604;&#22914;Flash Attention&#21644;Multi-query Attention&#12290;&#21478;&#19968;&#26041;&#38754;&#30340;&#24037;&#20316;&#26088;&#22312;&#35774;&#35745;&#26032;&#30340;&#26426;&#21046;&#26469;&#21462;&#20195;&#27880;&#24847;&#21147;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#22411;&#32467;&#26500;--Mamba&#65292;&#22312;&#22810;&#20010;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18276v1 Announce Type: cross  Abstract: Transformer structure has achieved great success in multiple applied machine learning communities, such as natural language processing (NLP), computer vision (CV) and information retrieval (IR). Transformer architecture's core mechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$ time complexity in inference. Many works have been proposed to improve the attention mechanism's scalability, such as Flash Attention and Multi-query Attention. A different line of work aims to design new mechanisms to replace attention. Recently, a notable model structure -- Mamba, which is based on state space models, has achieved transformer-equivalent performance in multiple sequence modeling tasks.   In this work, we examine \mamba's efficacy through the lens of a classical IR task -- document ranking. A reranker model takes a query and a document as input, and predicts a scalar relevance score. This task demands the language mod
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#34987;&#31216;&#20026;RegionVLM&#30340;&#27169;&#22411;&#65292;&#20855;&#22791;&#26174;&#24335;&#30340;&#21306;&#22495;&#24314;&#27169;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20449;&#24687;&#28304;Local Narratives&#65292;&#35774;&#35745;&#31616;&#27905;&#32780;&#21019;&#26032;&#30340;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#20132;&#20114;&#24335;&#21306;&#22495;&#29702;&#35299;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18260</link><description>&lt;p&gt;
&#22312;&#35270;&#35273;-&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20132;&#20114;&#24335;&#21306;&#22495;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Toward Interactive Regional Understanding in Vision-Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18260
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#34987;&#31216;&#20026;RegionVLM&#30340;&#27169;&#22411;&#65292;&#20855;&#22791;&#26174;&#24335;&#30340;&#21306;&#22495;&#24314;&#27169;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20449;&#24687;&#28304;Local Narratives&#65292;&#35774;&#35745;&#31616;&#27905;&#32780;&#21019;&#26032;&#30340;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#20132;&#20114;&#24335;&#21306;&#22495;&#29702;&#35299;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36807;&#20998;&#20381;&#36182;&#20110;&#25429;&#25417;&#22270;&#20687;&#31895;&#31961;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#23548;&#33268;&#23427;&#20204;&#22312;&#21306;&#22495;&#29702;&#35299;&#33021;&#21147;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;\textbf{RegionVLM}&#65292;&#20855;&#22791;&#26174;&#24335;&#30340;&#21306;&#22495;&#24314;&#27169;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#25351;&#23450;&#30340;&#22270;&#20687;&#21306;&#22495;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#21364;&#21019;&#26032;&#30340;&#26550;&#26500;&#65292;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#26550;&#26500;&#25110;&#30446;&#26631;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;&#26032;&#39062;&#20449;&#24687;&#28304;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;&#23616;&#37096;&#21465;&#20107;&#65292;&#36825;&#22312;&#20808;&#21069;&#30340;VLP&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#21333;&#19968;&#36890;&#29992;&#27169;&#22411;&#19981;&#20165;&#23454;&#29616;&#20102;&#20132;&#20114;&#24335;&#23545;&#35805;&#31995;&#32479;&#65292;&#36824;&#22312;&#21508;&#31181;&#38646;&#26679;&#26412;&#21306;&#22495;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18260v1 Announce Type: cross  Abstract: Recent Vision-Language Pre-training (VLP) models have demonstrated significant advancements. Nevertheless, these models heavily rely on image-text pairs that capture only coarse and global information of an image, leading to a limitation in their regional understanding ability. In this work, we introduce \textbf{RegionVLM}, equipped with explicit regional modeling capabilities, allowing them to understand user-indicated image regions. To achieve this, we design a simple yet innovative architecture, requiring no modifications to the model architecture or objective function. Additionally, we leverage a dataset that contains a novel source of information, namely Localized Narratives, which has been overlooked in previous VLP research. Our experiments demonstrate that our single generalist model not only achieves an interactive dialogue system but also exhibits superior performance on various zero-shot region understanding tasks, without c
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#30693;&#35782;&#33976;&#39311;&#21644;&#25552;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#35299;&#20915;&#38544;&#21947;&#26816;&#27979;&#20013;&#30340;&#35821;&#35328;&#35268;&#21017;&#24212;&#29992;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#31034;&#20449;&#24687;&#21644;&#36719;&#26631;&#31614;&#20248;&#21270;&#23398;&#29983;&#27169;&#22411;&#65292;&#25552;&#39640;&#38544;&#21947;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18253</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#38544;&#21947;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MD-PK: Metaphor Detection via Prompt Learning and Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18253
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#30693;&#35782;&#33976;&#39311;&#21644;&#25552;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#35299;&#20915;&#38544;&#21947;&#26816;&#27979;&#20013;&#30340;&#35821;&#35328;&#35268;&#21017;&#24212;&#29992;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#31034;&#20449;&#24687;&#21644;&#36719;&#26631;&#31614;&#20248;&#21270;&#23398;&#29983;&#27169;&#22411;&#65292;&#25552;&#39640;&#38544;&#21947;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21947;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#38543;&#22788;&#21487;&#35265;&#65292;&#20294;&#26159;&#26816;&#27979;&#23427;&#20204;&#21364;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#32463;&#24120;&#22240;&#35821;&#35328;&#35268;&#21017;&#24212;&#29992;&#19981;&#24403;&#32780;&#38590;&#20197;&#24212;&#23545;&#65292;&#24182;&#24573;&#35270;&#20102;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#30693;&#35782;&#33976;&#39311;&#21644;&#25552;&#31034;&#23398;&#20064;&#24341;&#20837;&#38544;&#21947;&#26816;&#27979;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19987;&#20026;&#38544;&#21947;&#26816;&#27979;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#25552;&#31034;&#23398;&#20064;&#27169;&#26495;&#12290;&#36890;&#36807;&#23631;&#34109;&#30446;&#26631;&#35789;&#24182;&#25552;&#20379;&#30456;&#20851;&#25552;&#31034;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#23548;&#27169;&#22411;&#20934;&#30830;&#25512;&#26029;&#36825;&#20123;&#35789;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20943;&#36731;&#20102;&#30446;&#26631;&#35789;&#23383;&#38754;&#21547;&#20041;&#30340;&#24178;&#25200;&#65292;&#36824;&#30830;&#20445;&#20102;&#23545;&#20110;&#38544;&#21947;&#26816;&#27979;&#30340;MIP&#35821;&#35328;&#35268;&#21017;&#30340;&#27491;&#30830;&#21033;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#37197;&#22791;&#20808;&#21069;&#30693;&#35782;&#30340;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#36719;&#26631;&#31614;&#65292;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#36719;&#26631;&#31614;&#30340;&#24341;&#20837;&#31867;&#20284;&#20110;&#26631;&#31614;&#24179;&#28369;&#65292;&#26377;&#21161;&#20110;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18253v1 Announce Type: new  Abstract: Metaphors are ubiquitous in daily life, yet detecting them poses a significant challenge. Previous approaches often struggled with improper application of language rules and overlooked the issue of data sparsity. To address these challenges, we introduce knowledge distillation and prompt learning into metaphor detection. Specifically, we devise a prompt learning template tailored for the metaphor detection task. By masking target words and providing relevant prompt information, we guide the model to accurately infer the contextual meaning of these words. This approach not only mitigates the interference from the literal meaning of target words but also ensures the proper utilization of MIP language rules for metaphor detection. Moreover, we employ a teacher model equipped with prior knowledge to generate meaningful soft labels, guiding the optimization process of the student model. The inclusion of soft labels, akin to label smoothing, h
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Visual Table&#65292;&#19968;&#31181;&#20026;MLLMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#35270;&#35273;&#34920;&#31034;&#65292;&#36890;&#36807;&#25552;&#20379;&#23618;&#27425;&#21270;&#25991;&#26412;&#25551;&#36848;&#30340;&#20840;&#38754;&#35270;&#35273;&#22330;&#26223;&#26469;&#24357;&#34917;&#29616;&#26377;&#35270;&#35273;&#34920;&#31034;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.18252</link><description>&lt;p&gt;
&#36229;&#36234;&#23884;&#20837;&#65306;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#35270;&#35273;&#34920;&#26684;&#30340;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18252
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Visual Table&#65292;&#19968;&#31181;&#20026;MLLMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#35270;&#35273;&#34920;&#31034;&#65292;&#36890;&#36807;&#25552;&#20379;&#23618;&#27425;&#21270;&#25991;&#26412;&#25551;&#36848;&#30340;&#20840;&#38754;&#35270;&#35273;&#22330;&#26223;&#26469;&#24357;&#34917;&#29616;&#26377;&#35270;&#35273;&#34920;&#31034;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#19968;&#30452;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#22522;&#30707;&#65292;&#20174;&#20855;&#26377;&#20154;&#31867;&#27880;&#37322;&#26631;&#31614;&#30340;&#30417;&#30563;&#23398;&#20064;&#21457;&#23637;&#21040;&#23545;&#40784;&#26469;&#33258;&#20114;&#32852;&#32593;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12290;&#23613;&#31649;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#30340;&#35270;&#35273;&#34920;&#31034;&#65288;&#22914;CLIP&#23884;&#20837;&#65289;&#36890;&#24120;&#32570;&#20047;&#20851;&#38190;&#30340;&#22806;&#37096;&#19990;&#30028;&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#35270;&#35273;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Visual Table&#65292;&#36825;&#26159;&#20026;MLLMs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#35270;&#35273;&#34920;&#31034;&#12290;&#23427;&#25552;&#20379;&#20840;&#38754;&#35270;&#35273;&#22330;&#26223;&#30340;&#23618;&#27425;&#21270;&#25991;&#26412;&#25551;&#36848;&#65292;&#21253;&#25324;&#22330;&#26223;&#25551;&#36848;&#21644;&#28085;&#30422;&#31867;&#21035;&#12289;&#23646;&#24615;&#21644;&#23454;&#20363;&#32423;&#21035;&#30693;&#35782;&#30340;&#22810;&#20010;&#20197;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;&#25551;&#36848;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#20174;GPT4V&#30340;&#23567;&#35268;&#27169;&#27880;&#37322;&#20013;&#29983;&#25104;&#35270;&#35273;&#34920;&#26684;&#65292;&#24182;&#35757;&#32451;&#23427;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#35270;&#35273;&#34920;&#26684;&#20316;&#20026;&#39069;&#22806;&#35270;&#35273;&#34920;&#31034;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18252v1 Announce Type: cross  Abstract: Visual representation learning has been a cornerstone in computer vision, evolving from supervised learning with human-annotated labels to aligning image-text pairs from the Internet. Despite recent advancements in multi-modal large language models (MLLMs), the visual representations they rely on, such as CLIP embeddings, often lack access to external world knowledge critical for real-world visual reasoning. In this work, we propose Visual Table, a novel visual representation tailored for MLLMs. It provides hierarchical text descriptions of holistic visual scenes, consisting of a scene description and multiple object-centric descriptions that encompass categories, attributes, and knowledge at instance level. We further develop a scalable generator for visual table generation and train it on small-scale annotations from GPT4V. Extensive evaluations demonstrate that, with generated visual tables as additional visual representations, our 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25351;&#20986;&#31185;&#23398;&#25991;&#29486;&#22810;&#35821;&#35328;&#29305;&#24615;&#65292;&#20027;&#24352;&#24403;&#21069;&#27169;&#22411;&#21644;&#22522;&#20934;&#24212;&#21453;&#26144;&#35821;&#35328;&#22810;&#26679;&#24615;&#65292;&#25552;&#20986;&#25913;&#21892;&#38750;&#33521;&#35821;&#25991;&#26723;&#24615;&#33021;&#30340;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2403.18251</link><description>&lt;p&gt;
&#30001;&#20110;&#31185;&#23398;&#25991;&#29486;&#26159;&#22810;&#35821;&#35328;&#30340;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20063;&#24212;&#35813;&#22914;&#27492;
&lt;/p&gt;
&lt;p&gt;
Since the Scientific Literature Is Multilingual, Our Models Should Be Too
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18251
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25351;&#20986;&#31185;&#23398;&#25991;&#29486;&#22810;&#35821;&#35328;&#29305;&#24615;&#65292;&#20027;&#24352;&#24403;&#21069;&#27169;&#22411;&#21644;&#22522;&#20934;&#24212;&#21453;&#26144;&#35821;&#35328;&#22810;&#26679;&#24615;&#65292;&#25552;&#20986;&#25913;&#21892;&#38750;&#33521;&#35821;&#25991;&#26723;&#24615;&#33021;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33521;&#35821;&#38271;&#26399;&#20197;&#26469;&#34987;&#35748;&#20026;&#26159;&#31185;&#23398;&#30740;&#31350;&#30340;$\textit{lingua franca}$&#65292;&#36825;&#19968;&#35266;&#24565;&#22312;&#28041;&#21450;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#20013;&#24471;&#21040;&#20102;&#20307;&#29616;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#37327;&#22320;&#23637;&#31034;&#25991;&#29486;&#22823;&#22810;&#26159;&#22810;&#35821;&#35328;&#30340;&#65292;&#24182;&#25552;&#20986;&#24403;&#21069;&#30340;&#27169;&#22411;&#21644;&#22522;&#20934;&#24212;&#35813;&#21453;&#26144;&#36825;&#31181;&#35821;&#35328;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#26080;&#27861;&#20026;&#38750;&#33521;&#35821;&#35770;&#25991;&#21019;&#24314;&#26377;&#24847;&#20041;&#30340;&#34920;&#31034;&#65292;&#24182;&#24378;&#35843;&#36328;&#22810;&#35821;&#35328;&#39046;&#22495;&#26080;&#24046;&#21035;&#22320;&#20351;&#29992;&#20165;&#33521;&#35821;&#27169;&#22411;&#23545;&#29992;&#25143;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#24314;&#35758;&#65292;&#24076;&#26395;NLP&#31038;&#21306;&#33021;&#22815;&#25913;&#21892;&#23545;&#38750;&#33521;&#35821;&#25991;&#26723;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18251v1 Announce Type: new  Abstract: English has long been assumed the $\textit{lingua franca}$ of scientific research, and this notion is reflected in the natural language processing (NLP) research involving scientific document representation. In this position piece, we quantitatively show that the literature is largely multilingual and argue that current models and benchmarks should reflect this linguistic diversity. We provide evidence that text-based models fail to create meaningful representations for non-English papers and highlight the negative user-facing impacts of using English-only models non-discriminately across a multilingual domain. We end with suggestions for the NLP community on how to improve performance on non-English documents.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20551;&#26032;&#38395;&#25915;&#20987;&#26041;&#27861;VLPrompt&#65292;&#36890;&#36807;&#28040;&#38500;&#23545;&#39069;&#22806;&#25968;&#25454;&#25910;&#38598;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#22659;&#19968;&#33268;&#24615;&#21644;&#21407;&#22987;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#32553;&#23567;LLM&#29983;&#25104;&#34394;&#20551;&#26032;&#38395;&#30340;&#27450;&#39575;&#21147;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.18249</link><description>&lt;p&gt;
&#25506;&#32034;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#26032;&#38395;&#30340;&#27450;&#39575;&#21147;&#65306;&#23545;&#29616;&#23454;&#19990;&#30028;&#26816;&#27979;&#25361;&#25112;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20551;&#26032;&#38395;&#25915;&#20987;&#26041;&#27861;VLPrompt&#65292;&#36890;&#36807;&#28040;&#38500;&#23545;&#39069;&#22806;&#25968;&#25454;&#25910;&#38598;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#22659;&#19968;&#33268;&#24615;&#21644;&#21407;&#22987;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#32553;&#23567;LLM&#29983;&#25104;&#34394;&#20551;&#26032;&#38395;&#30340;&#27450;&#39575;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#22312;&#22797;&#26434;&#39046;&#22495;&#22914;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#23588;&#20026;&#21487;&#33021;&#21019;&#36896;&#34394;&#20551;&#26032;&#38395;&#25104;&#20026;&#21487;&#33021;&#12290;&#30740;&#31350;&#31361;&#26174;&#20102;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#26032;&#38395;&#22312;&#26377;&#26080;&#20154;&#31867;&#36741;&#21161;&#30340;&#24773;&#20917;&#19979;&#30340;&#27450;&#39575;&#21147;&#24046;&#36317;&#65292;&#20294;&#23545;&#20110;&#28608;&#21169;&#25216;&#26415;&#30340;&#28508;&#21147;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#28608;&#21169;&#31574;&#30053;&#26159;&#21542;&#33021;&#26377;&#25928;&#22320;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#12290;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#34394;&#20551;&#26032;&#38395;&#25915;&#20987;&#38656;&#35201;&#20154;&#31867;&#24178;&#39044;&#36827;&#34892;&#20449;&#24687;&#25910;&#38598;&#65292;&#36890;&#24120;&#20250;&#24573;&#30053;&#32454;&#33410;&#24182;&#19988;&#26080;&#27861;&#20445;&#25345;&#19978;&#19979;&#25991;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#23041;&#32961;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#24335;&#25552;&#31034;&#65288;VLPrompt&#65289;&#30340;&#24378;&#20551;&#26032;&#38395;&#25915;&#20987;&#26041;&#27861;&#12290;&#19982;&#24403;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;VLPrompt&#28040;&#38500;&#20102;&#23545;&#39069;&#22806;&#25968;&#25454;&#25910;&#38598;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#35821;&#22659;&#30340;&#36830;&#36143;&#24615;&#24182;&#20445;&#30041;&#20102;&#21407;&#22987;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18249v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored. Thus, this work aims to determine whether prompting strategies can effectively narrow this gap. Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPro
&lt;/p&gt;</description></item><item><title>ZAEBUC-Spoken&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#22810;&#26041;&#35328;&#30340;&#38463;&#25289;&#20271;&#35821;-&#33521;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#38463;&#25289;&#20271;&#35821;&#21644;&#33521;&#35821;&#30340;&#22810;&#31181;&#21464;&#20307;&#20197;&#21450;&#20004;&#31181;&#35821;&#35328;&#20043;&#38388;&#30340;&#20195;&#30721;&#20999;&#25442;&#65292;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#25552;&#20379;&#20102;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.18182</link><description>&lt;p&gt;
ZAEBUC-Spoken: &#19968;&#20010;&#22810;&#35821;&#35328;&#22810;&#26041;&#35328;&#30340;&#38463;&#25289;&#20271;&#35821;-&#33521;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
ZAEBUC-Spoken: A Multilingual Multidialectal Arabic-English Speech Corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18182
&lt;/p&gt;
&lt;p&gt;
ZAEBUC-Spoken&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#22810;&#26041;&#35328;&#30340;&#38463;&#25289;&#20271;&#35821;-&#33521;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#38463;&#25289;&#20271;&#35821;&#21644;&#33521;&#35821;&#30340;&#22810;&#31181;&#21464;&#20307;&#20197;&#21450;&#20004;&#31181;&#35821;&#35328;&#20043;&#38388;&#30340;&#20195;&#30721;&#20999;&#25442;&#65292;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#25552;&#20379;&#20102;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;ZAEBUC-Spoken&#65292;&#19968;&#20010;&#22810;&#35821;&#35328;&#22810;&#26041;&#35328;&#30340;&#38463;&#25289;&#20271;&#35821;-&#33521;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;&#21313;&#20108;&#23567;&#26102;&#30340;Zoom&#20250;&#35758;&#20869;&#23481;&#65292;&#28041;&#21450;&#22810;&#20301;&#25198;&#28436;&#24037;&#20316;&#24773;&#22659;&#20013;&#23398;&#29983;&#22836;&#33041;&#39118;&#26292;&#26576;&#19968;&#20027;&#39064;&#28982;&#21518;&#19982;&#23545;&#35805;&#32773;&#35752;&#35770;&#30340;&#21457;&#35328;&#32773;&#12290;&#20250;&#35758;&#28085;&#30422;&#19981;&#21516;&#30340;&#20027;&#39064;&#65292;&#24182;&#20998;&#20026;&#20855;&#26377;&#19981;&#21516;&#35821;&#35328;&#35774;&#32622;&#30340;&#38454;&#27573;&#12290;&#36825;&#20010;&#35821;&#26009;&#24211;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#25552;&#20379;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20004;&#31181;&#35821;&#35328;&#65288;&#38463;&#25289;&#20271;&#35821;&#21644;&#33521;&#35821;&#65289;&#65292;&#20854;&#20013;&#38463;&#25289;&#20271;&#35821;&#20197;&#22810;&#31181;&#21464;&#20307;&#65288;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#12289;&#28023;&#28286;&#38463;&#25289;&#20271;&#35821;&#21644;&#22467;&#21450;&#38463;&#25289;&#20271;&#35821;&#65289;&#23384;&#22312;&#65292;&#33521;&#35821;&#21017;&#20351;&#29992;&#19981;&#21516;&#21475;&#38899;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#35821;&#26009;&#24211;&#30340;&#22797;&#26434;&#24615;&#36824;&#34920;&#29616;&#22312;&#36825;&#20123;&#35821;&#35328;&#21644;&#26041;&#35328;&#20043;&#38388;&#30340;&#20195;&#30721;&#20999;&#25442;&#12290;&#20316;&#20026;&#25105;&#20204;&#24037;&#20316;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#20174;&#24050;&#24314;&#31435;&#30340;&#36716;&#24405;&#25351;&#21335;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#19968;&#22871;&#22788;&#29702;&#20250;&#35805;&#24615;&#35821;&#38899;&#12289;&#20195;&#30721;&#20999;&#25442;&#21644;&#27491;&#23383;&#27861;&#38382;&#39064;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18182v1 Announce Type: new  Abstract: We present ZAEBUC-Spoken, a multilingual multidialectal Arabic-English speech corpus. The corpus comprises twelve hours of Zoom meetings involving multiple speakers role-playing a work situation where Students brainstorm ideas for a certain topic and then discuss it with an Interlocutor. The meetings cover different topics and are divided into phases with different language setups. The corpus presents a challenging set for automatic speech recognition (ASR), including two languages (Arabic and English) with Arabic spoken in multiple variants (Modern Standard Arabic, Gulf Arabic, and Egyptian Arabic) and English used with various accents. Adding to the complexity of the corpus, there is also code-switching between these languages and dialects. As part of our work, we take inspiration from established sets of transcription guidelines to present a set of guidelines handling issues of conversational speech, code-switching and orthography of 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#38750;&#20107;&#23454;&#24615;&#24187;&#35273;&#30340;&#20004;&#20010;&#36890;&#29992;&#26426;&#21046;&#65306;&#20027;&#39064;&#23646;&#24615;&#30693;&#35782;&#19981;&#36275;&#21644;&#26410;&#33021;&#27491;&#30830;&#36873;&#25321;&#23545;&#35937;&#23646;&#24615;&#65292;&#36825;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#21644;&#20943;&#36731;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2403.18167</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#38750;&#20107;&#23454;&#24615;&#24187;&#35273;&#30340;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Mechanisms of non-factual hallucinations in language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18167
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#38750;&#20107;&#23454;&#24615;&#24187;&#35273;&#30340;&#20004;&#20010;&#36890;&#29992;&#26426;&#21046;&#65306;&#20027;&#39064;&#23646;&#24615;&#30693;&#35782;&#19981;&#36275;&#21644;&#26410;&#33021;&#27491;&#30830;&#36873;&#25321;&#23545;&#35937;&#23646;&#24615;&#65292;&#36825;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#21644;&#20943;&#36731;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20170;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26377;&#26102;&#20250;&#20135;&#29983;&#19982;&#19990;&#30028;&#30693;&#35782;&#19981;&#31526;&#30340;&#38750;&#20107;&#23454;&#24187;&#35273;&#12290;&#23613;&#31649;&#20154;&#20204;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#24187;&#35273;&#65292;&#20294;&#29702;&#35299;&#23427;&#20204;&#30340;&#20869;&#22312;&#26426;&#21046;&#20173;&#28982;&#26159;&#22256;&#38590;&#30340;&#12290; &#25105;&#20204;&#30340;&#30740;&#31350;&#35843;&#26597;&#20102;&#24187;&#35273;&#30340;&#26426;&#21046;&#21407;&#22240;&#65292;&#29305;&#21035;&#26159; LM &#22312;&#23545;&#20027;&#39064;&#20851;&#31995;&#26597;&#35810;&#20570;&#20986;&#22238;&#31572;&#26102;&#38169;&#35823;&#22320;&#39044;&#27979;&#23545;&#35937;&#23646;&#24615;&#30340;&#38750;&#20107;&#23454;&#24418;&#24335;&#12290;&#36890;&#36807;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#21644;&#23884;&#20837;&#31354;&#38388;&#25237;&#24433;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;&#36328;&#19981;&#21516;&#35268;&#27169;&#21644;&#35774;&#35745;&#30340; LM &#20013;&#20849;&#20139;&#30340;&#20004;&#20010;&#36896;&#25104;&#24187;&#35273;&#30340;&#19968;&#33324;&#26426;&#21046;&#21407;&#22240;&#65306;1&#65289;&#22312;&#36739;&#20302;&#23618; MLPs &#20013;&#20027;&#39064;&#23646;&#24615;&#30693;&#35782;&#19981;&#36275;&#65292;&#20197;&#21450;2&#65289;&#22312;&#36739;&#39640;&#23618;&#27880;&#24847;&#21147;&#22836;&#21644; MLPs &#20013;&#26410;&#33021;&#36873;&#25321;&#27491;&#30830;&#30340;&#23545;&#35937;&#23646;&#24615;&#12290;&#36825;&#20004;&#20010;&#26426;&#21046;&#23637;&#31034;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#20027;&#23486;&#20851;&#31995;&#12289;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#21644;&#25200;&#21160;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23457;&#26597;&#20102; LM &#30340;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18167v1 Announce Type: cross  Abstract: State-of-the-art language models (LMs) sometimes generate non-factual hallucinations that misalign with world knowledge. Despite extensive efforts to detect and mitigate hallucinations, understanding their internal mechanisms remains elusive. Our study investigates the mechanistic causes of hallucination, specifically non-factual ones where the LM incorrectly predicts object attributes in response to subject-relation queries. With causal mediation analysis and embedding space projection, we identify two general mechanistic causes of hallucinations shared across LMs of various scales and designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and 2) failing to select the correct object attribute in upper layer attention heads and MLPs. These two mechanisms exhibit varying degrees of subject-object association, predictive uncertainty and perturbation robustness. Additionally, we scrutinize LM pre-training checkpoints, r
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#21495;&#20256;&#25773;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;ov-freeze&#31283;&#23450;KD-QAT&#36807;&#31243;&#30340;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2403.18159</link><description>&lt;p&gt;
&#22114;&#65281;&#25105;&#20204;&#20919;&#20923;&#65306;&#36890;&#36807;&#20449;&#21495;&#20256;&#25773;&#20998;&#26512;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal Propagation Analysis for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18159
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#21495;&#20256;&#25773;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;ov-freeze&#31283;&#23450;KD-QAT&#36807;&#31243;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#20998;&#21035;&#22312;NLP&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#36895;&#24230;&#24930;&#65292;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#39640;&#65292;&#36825;&#20351;&#24471;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;&#23427;&#20204;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#37327;&#21270;&#24863;&#30693;&#24494;&#35843;&#25216;&#26415;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD-QAT&#65289;&#26469;&#25913;&#21892;&#20351;&#29992;&#24120;&#29992;&#25968;&#25454;&#38598;&#25913;&#36827;4&#20301;&#37325;&#37327;&#37327;&#21270;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#23454;&#29616;&#27969;&#34892;&#30340;&#35821;&#35328;&#20351;&#29992;&#26696;&#20363;&#65292;&#22312;&#35774;&#22791;&#32842;&#22825;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#25913;&#36827;&#36825;&#31181;&#24494;&#35843;&#33539;&#24335;&#65292;&#20316;&#20026;&#20027;&#35201;&#36129;&#29486;&#65292;&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#20256;&#25773;&#65292;&#25552;&#20379;&#23545;KD-QAT&#31283;&#23450;&#24615;&#30340;&#27934;&#23519;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#20110;KD-QAT&#30340;&#26041;&#27861;&#23545;&#20302;&#20301;&#37327;&#21270;&#35823;&#24046;&#30340;&#33030;&#24369;&#24615;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ov-freeze&#65292;&#19968;&#31181;&#31283;&#23450;KD-QAT&#36807;&#31243;&#30340;&#31616;&#21333;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18159v1 Announce Type: cross  Abstract: Large generative models, such as large language models (LLMs) and diffusion models have as revolutionized the fields of NLP and computer vision respectively. However, their slow inference, high computation and memory requirement makes it challenging to deploy them on edge devices. In this study, we propose a light-weight quantization aware fine tuning technique using knowledge distillation (KD-QAT) to improve the performance of 4-bit weight quantized LLMs using commonly available datasets to realize a popular language use case, on device chat applications. To improve this paradigm of finetuning, as main contributions, we provide insights into stability of KD-QAT by empirically studying the gradient propagation during training to better understand the vulnerabilities of KD-QAT based approaches to low-bit quantization errors. Based on our insights, we propose ov-freeze, a simple technique to stabilize the KD-QAT process. Finally, we expe
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#25968;&#25454;&#26631;&#27880;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25928;&#29575;&#65292;&#21487;&#20197;&#20316;&#20026;&#38750;&#19987;&#23478;&#26631;&#27880;&#21592;&#30340;&#26367;&#20195;&#21697;&#65292;&#20851;&#38190;&#22312;&#20110;&#23450;&#21046;&#21270;&#25552;&#31034;&#21644;&#29305;&#23450;&#31034;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.18152</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#37329;&#34701;&#25968;&#25454;&#26631;&#27880;&#21592;&#65306;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Financial Data Annotators: A Study on Effectiveness and Efficiency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18152
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37329;&#34701;&#25968;&#25454;&#26631;&#27880;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25928;&#29575;&#65292;&#21487;&#20197;&#20316;&#20026;&#38750;&#19987;&#23478;&#26631;&#27880;&#21592;&#30340;&#26367;&#20195;&#21697;&#65292;&#20851;&#38190;&#22312;&#20110;&#23450;&#21046;&#21270;&#25552;&#31034;&#21644;&#29305;&#23450;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#39046;&#22495;&#25910;&#38598;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#39046;&#22495;&#19987;&#23478;&#31232;&#32570;&#19988;&#38599;&#20323;&#20182;&#20204;&#30340;&#25104;&#26412;&#36739;&#39640;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19968;&#33324;&#39046;&#22495;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#26631;&#27880;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#20316;&#20026;&#37329;&#34701;&#25991;&#26723;&#20013;&#25552;&#21462;&#20851;&#31995;&#30340;&#39640;&#25928;&#25968;&#25454;&#26631;&#27880;&#21592;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#23558;&#19977;&#20010;LLMs&#65288;GPT-4&#12289;PaLM 2&#21644;MPT Instruct&#65289;&#29983;&#25104;&#30340;&#26631;&#27880;&#19982;&#19987;&#23478;&#26631;&#27880;&#21592;&#21644;&#20247;&#21253;&#24037;&#20316;&#32773;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35777;&#26126;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#21487;&#20197;&#20316;&#20026;&#38750;&#19987;&#23478;&#20247;&#21253;&#24037;&#20316;&#32773;&#30340;&#21512;&#36866;&#26367;&#20195;&#21697;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#21644;&#21442;&#25968;&#35774;&#32622;&#23545;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;&#20026;&#27599;&#20010;&#20851;&#31995;&#32452;&#23450;&#21046;&#25552;&#31034;&#65292;&#36890;&#36807;&#25552;&#20379;&#23646;&#20110;&#37027;&#20123;&#32452;&#30340;&#20855;&#20307;&#31034;&#20363;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#38752;&#24615;&#25351;&#25968;&#65288;LLM-RelI&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18152v1 Announce Type: new  Abstract: Collecting labeled datasets in finance is challenging due to scarcity of domain experts and higher cost of employing them. While Large Language Models (LLMs) have demonstrated remarkable performance in data annotation tasks on general domain datasets, their effectiveness on domain specific datasets remains underexplored. To address this gap, we investigate the potential of LLMs as efficient data annotators for extracting relations in financial documents. We compare the annotations produced by three LLMs (GPT-4, PaLM 2, and MPT Instruct) against expert annotators and crowdworkers. We demonstrate that the current state-of-the-art LLMs can be sufficient alternatives to non-expert crowdworkers. We analyze models using various prompts and parameter settings and find that customizing the prompts for each relation group by providing specific examples belonging to those groups is paramount. Furthermore, we introduce a reliability index (LLM-RelI
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20004;&#39033;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#22238;&#22797;&#22312;&#20849;&#24773;&#24615;&#26041;&#38754;&#34987;&#35748;&#20026;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#22238;&#22797;&#26356;&#20855;&#26377;&#20849;&#24773;&#24615;&#65292;&#36825;&#34920;&#26126;&#20102;&#22312;&#20154;&#38469;&#25903;&#25345;&#26041;&#38754;&#20351;&#29992;LLMs&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.18148</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#34987;&#35748;&#20026;&#20855;&#26377;&#20849;&#24773;&#30340;&#22238;&#24212;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Produce Responses Perceived to be Empathic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18148
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20004;&#39033;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#22238;&#22797;&#22312;&#20849;&#24773;&#24615;&#26041;&#38754;&#34987;&#35748;&#20026;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#22238;&#22797;&#26356;&#20855;&#26377;&#20849;&#24773;&#24615;&#65292;&#36825;&#34920;&#26126;&#20102;&#22312;&#20154;&#38469;&#25903;&#25345;&#26041;&#38754;&#20351;&#29992;LLMs&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#24778;&#35766;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25776;&#20889;&#26174;&#31034;&#20849;&#24773;&#30340;&#25903;&#25345;&#24615;&#28040;&#24687;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35753;&#36825;&#20123;&#27169;&#22411;&#26681;&#25454;&#25551;&#36848;&#24120;&#35265;&#29983;&#27963;&#32463;&#21382;&#30340;&#24086;&#23376;&#29983;&#25104;&#20849;&#24773;&#28040;&#24687;&#65292;&#22914;&#24037;&#20316;&#22330;&#26223;&#12289;&#32946;&#20799;&#12289;&#20154;&#38469;&#20851;&#31995;&#20197;&#21450;&#20854;&#20182;&#24341;&#21457;&#28966;&#34385;&#21644;&#24868;&#24594;&#30340;&#24773;&#20917;&#12290;&#22312;&#20004;&#39033;&#30740;&#31350;&#20013;&#65288;N=192&#65292;202&#65289;&#65292;&#25105;&#20204;&#21521;&#20154;&#31867;&#35780;&#20998;&#21592;&#23637;&#31034;&#20102;&#30001;&#20960;&#31181;&#27169;&#22411;&#65288;GPT4 Turbo&#65292;Llama2 &#21644; Mistral&#65289;&#25776;&#20889;&#30340;&#21508;&#31181;&#22238;&#22797;&#65292;&#24182;&#35753;&#20154;&#20204;&#26681;&#25454;&#36825;&#20123;&#22238;&#22797;&#22312;&#20849;&#24773;&#31243;&#24230;&#19978;&#35780;&#20998;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLM &#29983;&#25104;&#30340;&#22238;&#22797;&#34987;&#19968;&#33268;&#35780;&#20026;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#22238;&#22797;&#26356;&#20855;&#20849;&#24773;&#24615;&#12290;&#35821;&#35328;&#20998;&#26512;&#36824;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20351;&#29992;&#26631;&#28857;&#31526;&#21495;&#12289;&#34920;&#24773;&#31526;&#21495;&#21644;&#26576;&#20123;&#35789;&#27719;&#26041;&#38754;&#20855;&#26377;&#26126;&#26174;&#12289;&#21487;&#39044;&#27979;&#30340;&#8220;&#39118;&#26684;&#8221;&#12290;&#36825;&#20123;&#32467;&#26524;&#20984;&#26174;&#20102;&#22312;&#38656;&#35201;&#20849;&#24773;&#30340;&#24773;&#22659;&#20013;&#21033;&#29992;LLMs&#25552;&#21319;&#20154;&#31867;&#21516;&#34892;&#25903;&#25345;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18148v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated surprising performance on many tasks, including writing supportive messages that display empathy. Here, we had these models generate empathic messages in response to posts describing common life experiences, such as workplace situations, parenting, relationships, and other anxiety- and anger-eliciting situations. Across two studies (N=192, 202), we showed human raters a variety of responses written by several models (GPT4 Turbo, Llama2, and Mistral), and had people rate these responses on how empathic they seemed to be. We found that LLM-generated responses were consistently rated as more empathic than human-written responses. Linguistic analyses also show that these models write in distinct, predictable ``styles", in terms of their use of punctuation, emojis, and certain words. These results highlight the potential of using LLMs to enhance human peer support in contexts where empathy is i
&lt;/p&gt;</description></item><item><title>Juru &#27169;&#22411;&#36890;&#36807;&#20174;&#24052;&#35199;&#27861;&#24459;&#26469;&#28304;&#25552;&#21462;&#30340;19&#20159;&#20010;&#21807;&#19968;&#26631;&#35760;&#65292;&#23637;&#31034;&#20102;&#39046;&#22495;&#19987;&#38376;&#21270;&#21487;&#20197;&#22312;&#20943;&#23569;&#39044;&#35757;&#32451;&#25968;&#25454;&#37327;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#65292;&#20294;&#36825;&#31181;&#19987;&#38376;&#21270;&#20250;&#23548;&#33268;&#21516;&#19968;&#35821;&#35328;&#20013;&#20854;&#20182;&#30693;&#35782;&#39046;&#22495;&#24615;&#33021;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.18140</link><description>&lt;p&gt;
Juru: &#26469;&#33258;&#21487;&#38752;&#26469;&#28304;&#30340;&#24052;&#35199;&#27861;&#24459;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Juru: Legal Brazilian Large Language Model from Reputable Sources
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18140
&lt;/p&gt;
&lt;p&gt;
Juru &#27169;&#22411;&#36890;&#36807;&#20174;&#24052;&#35199;&#27861;&#24459;&#26469;&#28304;&#25552;&#21462;&#30340;19&#20159;&#20010;&#21807;&#19968;&#26631;&#35760;&#65292;&#23637;&#31034;&#20102;&#39046;&#22495;&#19987;&#38376;&#21270;&#21487;&#20197;&#22312;&#20943;&#23569;&#39044;&#35757;&#32451;&#25968;&#25454;&#37327;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#65292;&#20294;&#36825;&#31181;&#19987;&#38376;&#21270;&#20250;&#23548;&#33268;&#21516;&#19968;&#35821;&#35328;&#20013;&#20854;&#20182;&#30693;&#35782;&#39046;&#22495;&#24615;&#33021;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#30456;&#20851;&#30740;&#31350;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20986;&#29616;&#20102;&#20004;&#31181;&#31574;&#30053;&#65306;&#39046;&#22495;&#19987;&#38376;&#21270;&#21644;&#20351;&#29992;&#39640;&#36136;&#37327;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#20026;&#25506;&#32034;&#36825;&#20123;&#31574;&#30053;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#21487;&#38752;&#24052;&#35199;&#27861;&#24459;&#26469;&#28304;&#30340;19&#20159;&#20010;&#21807;&#19968;&#26631;&#35760;&#19987;&#38376;&#21270;&#20102;Sabi\'a-2 Small&#27169;&#22411;&#65292;&#24182;&#22312;&#27861;&#24459;&#21644;&#19968;&#33324;&#30693;&#35782;&#32771;&#35797;&#20013;&#36827;&#34892;&#20102;&#23569;&#26679;&#26412;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;Juru&#23637;&#31034;&#20102;&#39046;&#22495;&#19987;&#38376;&#21270;&#22312;&#20943;&#23569;&#39044;&#35757;&#32451;&#25968;&#25454;&#37327;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#19987;&#38376;&#21270;&#26159;&#20197;&#22312;&#21516;&#19968;&#35821;&#35328;&#20013;&#20854;&#20182;&#30693;&#35782;&#39046;&#22495;&#24615;&#33021;&#19979;&#38477;&#20026;&#20195;&#20215;&#30340;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#21161;&#20110;&#22686;&#21152;&#30340;&#31185;&#23398;&#35777;&#25454;&#65292;&#34920;&#26126;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#36873;&#25321;&#21487;&#33021;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#33021;&#22815;&#20197;&#36739;&#20302;&#25104;&#26412;&#25506;&#32034;&#36825;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18140v1 Announce Type: cross  Abstract: The high computational cost associated with pretraining large language models limits their research. Two strategies have emerged to address this issue: domain specialization and pretraining with high-quality data. To explore these strategies, we specialized the Sabi\'a-2 Small model with 1.9 billion unique tokens from reputable Brazilian legal sources and conducted few-shot evaluations on legal and general knowledge exams. Our model, Juru, demonstrates the benefits of domain specialization with a reduced amount of pretraining data. However, this specialization comes at the expense of degrading performance in other knowledge areas within the same language. This study contributes to the growing body of scientific evidence showing that pretraining data selection may enhance the performance of large language models, enabling the exploration of these models at a lower cost.
&lt;/p&gt;</description></item><item><title>&#26500;&#24314;&#20102;&#19968;&#20010;&#38024;&#23545;&#25968;&#23383;&#26032;&#25163;&#30340;&#25216;&#26415;&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#20197;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#35789;&#27719;&#25110;&#27010;&#24565;&#38556;&#30861;&#65292;&#20026;&#35299;&#20915;&#19981;&#28165;&#26224;&#25110;&#38750;&#26631;&#20934;&#35821;&#35328;&#26597;&#35810;&#23545;&#27169;&#22411;&#36755;&#20986;&#24433;&#21709;&#38382;&#39064;&#25552;&#20379;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2403.18125</link><description>&lt;p&gt;
&#23545;&#20110;&#19981;&#25026;&#22914;&#20309;&#25552;&#38382;&#30340;&#20154;&#65306;&#26500;&#24314;&#19968;&#20010;&#38024;&#23545;&#25968;&#23383;&#26032;&#25163;&#30340;&#25216;&#26415;&#38382;&#39064;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
For those who don't know (how) to ask: Building a dataset of technology questions for digital newcomers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18125
&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#20102;&#19968;&#20010;&#38024;&#23545;&#25968;&#23383;&#26032;&#25163;&#30340;&#25216;&#26415;&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#20197;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#22788;&#29702;&#30340;&#35789;&#27719;&#25110;&#27010;&#24565;&#38556;&#30861;&#65292;&#20026;&#35299;&#20915;&#19981;&#28165;&#26224;&#25110;&#38750;&#26631;&#20934;&#35821;&#35328;&#26597;&#35810;&#23545;&#27169;&#22411;&#36755;&#20986;&#24433;&#21709;&#38382;&#39064;&#25552;&#20379;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23835;&#36215;&#24320;&#36767;&#20102;&#23398;&#20064;&#25968;&#23383;&#25216;&#26415;&#30340;&#20016;&#23500;&#26032;&#26426;&#20250;&#65292;&#20294;&#35768;&#22810;&#22312;&#25216;&#26415;&#36793;&#32536;&#30340;&#20154;&#30001;&#20110;&#35789;&#27719;&#25110;&#27010;&#24565;&#38556;&#30861;&#32780;&#38590;&#20197;&#33719;&#24471;&#21644;&#20445;&#25345;&#33021;&#21147;&#65292;&#26080;&#27861;&#25552;&#20986;&#24688;&#24403;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#26377;&#35768;&#22810;&#21162;&#21147;&#21435;&#29702;&#35299;LLM&#21019;&#24314;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#20197;&#21450;LLM&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#20294;&#20154;&#20204;&#24182;&#19981;&#22826;&#20102;&#35299;&#19981;&#28165;&#26224;&#25110;&#38750;&#26631;&#20934;&#35821;&#35328;&#26597;&#35810;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#36755;&#20986;&#12290;&#25105;&#20204;&#25552;&#35758;&#21019;&#24314;&#19968;&#20010;&#25429;&#25417;&#25968;&#23383;&#26032;&#25163;&#21644;&#23616;&#22806;&#20154;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#25105;&#20204;&#20174;&#21313;&#24180;&#19968;&#23545;&#19968;&#36741;&#23548;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#35745;&#21010;&#30340;&#21162;&#21147;&#21644;&#35813;&#25968;&#25454;&#38598;&#30340;&#19968;&#20123;&#28508;&#22312;&#29992;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18125v1 Announce Type: new  Abstract: While the rise of large language models (LLMs) has created rich new opportunities to learn about digital technology, many on the margins of this technology struggle to gain and maintain competency due to lexical or conceptual barriers that prevent them from asking appropriate questions. Although there have been many efforts to understand factuality of LLM-created content and ability of LLMs to answer questions, it is not well understood how unclear or nonstandard language queries affect the model outputs. We propose the creation of a dataset that captures questions of digital newcomers and outsiders, utilizing data we have compiled from a decade's worth of one-on-one tutoring. In this paper we lay out our planned efforts and some potential uses of this dataset.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#22312;&#19981;&#21516;&#23545;&#35805;&#29615;&#22659;&#19979;&#30340;&#34892;&#20026;&#65292;&#20171;&#32461;&#24182;&#20998;&#26512;&#20102;&#35282;&#33394;&#25198;&#28436;&#25968;&#25454;&#38598;&#65292;&#31361;&#20986;&#20102;&#29992;&#25143;&#19982;ChatGPT&#20132;&#20114;&#26102;&#30340;&#21160;&#26426;&#22810;&#26679;&#24615;&#21644;AI&#33258;&#28982;&#24230;&#30340;&#21464;&#21270;&#65292;&#20026;&#25913;&#36827;&#20154;&#26426;&#20132;&#27969;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.18121</link><description>&lt;p&gt;
ChatGPT&#35282;&#33394;&#25198;&#28436;&#25968;&#25454;&#38598;&#65306;&#29992;&#25143;&#21160;&#26426;&#19982;&#27169;&#22411;&#33258;&#28982;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Role-play Dataset: Analysis of User Motives and Model Naturalness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18121
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#22312;&#19981;&#21516;&#23545;&#35805;&#29615;&#22659;&#19979;&#30340;&#34892;&#20026;&#65292;&#20171;&#32461;&#24182;&#20998;&#26512;&#20102;&#35282;&#33394;&#25198;&#28436;&#25968;&#25454;&#38598;&#65292;&#31361;&#20986;&#20102;&#29992;&#25143;&#19982;ChatGPT&#20132;&#20114;&#26102;&#30340;&#21160;&#26426;&#22810;&#26679;&#24615;&#21644;AI&#33258;&#28982;&#24230;&#30340;&#21464;&#21270;&#65292;&#20026;&#25913;&#36827;&#20154;&#26426;&#20132;&#27969;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#20114;&#21160;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#21508;&#20010;&#39046;&#22495;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#33258;&#28982;&#21644;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;ChatGPT&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#23545;&#35805;&#20013;&#30340;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#27491;&#24120;&#23545;&#35805;&#21644;&#35282;&#33394;&#25198;&#28436;&#35774;&#32622;&#20013;&#30340;&#20114;&#21160;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#24191;&#27867;&#33539;&#22260;&#30340;&#20154;&#26426;&#23545;&#35805;&#65292;&#24102;&#26377;&#29992;&#25143;&#21160;&#26426;&#21644;&#27169;&#22411;&#33258;&#28982;&#24230;&#30340;&#27880;&#37322;&#65292;&#20197;&#30740;&#31350;&#65288;i&#65289;&#20154;&#31867;&#22914;&#20309;&#19982;&#23545;&#35805;AI&#27169;&#22411;&#20132;&#20114;&#65292;&#20197;&#21450;&#65288;ii&#65289;AI&#27169;&#22411;&#21709;&#24212;&#30340;&#33258;&#28982;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;&#29992;&#25143;&#19982;ChatGPT&#20114;&#21160;&#26102;&#30340;&#21160;&#26426;&#22810;&#26679;&#24615;&#21644;AI&#33258;&#28982;&#24230;&#30340;&#21464;&#21270;&#65292;&#19981;&#20165;&#23637;&#31034;&#20102;&#20154;&#19982;AI&#20043;&#38388;&#33258;&#28982;&#23545;&#35805;&#30340;&#24494;&#22937;&#21160;&#21147;&#23398;&#65292;&#32780;&#19988;&#20026;&#25913;&#36827;&#20154;&#26426;&#36890;&#20449;&#30340;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18121v1 Announce Type: new  Abstract: Recent advances in interactive large language models like ChatGPT have revolutionized various domains; however, their behavior in natural and role-play conversation settings remains underexplored. In our study, we address this gap by deeply investigating how ChatGPT behaves during conversations in different settings by analyzing its interactions in both a normal way and a role-play setting. We introduce a novel dataset of broad range of human-AI conversations annotated with user motives and model naturalness to examine (i) how humans engage with the conversational AI model, and (ii) how natural are AI model responses. Our study highlights the diversity of user motives when interacting with ChatGPT and variable AI naturalness, showing not only the nuanced dynamics of natural conversations between humans and AI, but also providing new avenues for improving the effectiveness of human-AI communication.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#38750;&#27491;&#24335;&#30340;&#25968;&#23398;&#38472;&#36848;&#32763;&#35793;&#20026;&#24418;&#24335;&#30340;Isabelle&#20195;&#30721;&#24182;&#36827;&#34892;&#33258;&#21160;&#39564;&#35777;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#21160;&#25298;&#32477;&#22312;&#20869;&#37096;&#19968;&#33268;&#24615;&#26041;&#38754;&#19982;&#24418;&#24335;&#21270;&#38382;&#39064;&#38472;&#36848;&#19981;&#19968;&#33268;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.18120</link><description>&lt;p&gt;
&#19981;&#35201;&#30456;&#20449;&#65306;&#39564;&#35777;--&#29992;&#33258;&#21160;&#24418;&#24335;&#21270;&#20026;&#22522;&#30784;&#30340;LLM&#23450;&#37327;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18120
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#38750;&#27491;&#24335;&#30340;&#25968;&#23398;&#38472;&#36848;&#32763;&#35793;&#20026;&#24418;&#24335;&#30340;Isabelle&#20195;&#30721;&#24182;&#36827;&#34892;&#33258;&#21160;&#39564;&#35777;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#21160;&#25298;&#32477;&#22312;&#20869;&#37096;&#19968;&#33268;&#24615;&#26041;&#38754;&#19982;&#24418;&#24335;&#21270;&#38382;&#39064;&#38472;&#36848;&#19981;&#19968;&#33268;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;Google&#30340;Minerva&#21644;OpenAI&#30340;GPT&#31995;&#21015;&#65292;&#27491;&#22312;&#36234;&#26469;&#36234;&#33021;&#22815;&#35299;&#20915;&#25968;&#23398;&#23450;&#37327;&#25512;&#29702;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25512;&#29702;&#27493;&#39588;&#21644;&#31572;&#26696;&#20013;&#20173;&#28982;&#23384;&#22312;&#27809;&#26377;&#29702;&#30001;&#30340;&#36923;&#36753;&#21644;&#35745;&#31639;&#38169;&#35823;&#12290;&#26412;&#25991;&#21033;&#29992;LLMs&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#21253;&#21547;&#36275;&#22815;&#22810;&#30340;&#24418;&#24335;&#21270;&#25968;&#23398;&#31034;&#20363;&#65288;&#20363;&#22914;&#22312;Isabelle&#20013;&#65292;&#19968;&#20010;&#24418;&#24335;&#23450;&#29702;&#35777;&#26126;&#29615;&#22659;&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#34987;&#25552;&#31034;&#23558;&#38750;&#27491;&#24335;&#30340;&#25968;&#23398;&#38472;&#36848;&#32763;&#35793;&#21363;&#33258;&#21160;&#24418;&#24335;&#21270;&#20026;&#24418;&#24335;&#30340;Isabelle&#20195;&#30721;--&#35813;&#20195;&#30721;&#21487;&#20197;&#34987;&#33258;&#21160;&#39564;&#35777;&#20869;&#37096;&#19968;&#33268;&#24615;&#12290;&#36825;&#25552;&#20379;&#20102;&#19968;&#20010;&#26426;&#21046;&#65292;&#21487;&#20197;&#33258;&#21160;&#25298;&#32477;&#37027;&#20123;&#20854;&#24418;&#24335;&#21270;&#29256;&#26412;&#22312;&#20854;&#20869;&#37096;&#25110;&#19982;&#24418;&#24335;&#21270;&#38382;&#39064;&#38472;&#36848;&#19981;&#19968;&#33268;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;GSM8K&#12289;MATH&#21644;MultiArith&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#30452;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18120v1 Announce Type: new  Abstract: Large language models (LLM), such as Google's Minerva and OpenAI's GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code -- which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#35843;&#30740;&#24635;&#32467;&#20102;LLMs&#22312;&#25945;&#32946;&#20013;&#30340;&#21508;&#31181;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#23398;&#29983;&#21644;&#25945;&#24072;&#36741;&#21161;&#12289;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#21830;&#19994;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26426;&#20250;&#21644;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.18105</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#65306;&#35843;&#30740;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Education: A Survey and Outlook
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18105
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#35843;&#30740;&#24635;&#32467;&#20102;LLMs&#22312;&#25945;&#32946;&#20013;&#30340;&#21508;&#31181;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#23398;&#29983;&#21644;&#25945;&#24072;&#36741;&#21161;&#12289;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#21830;&#19994;&#24037;&#20855;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26426;&#20250;&#21644;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20026;&#25945;&#32946;&#39046;&#22495;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#31687;&#35843;&#30740;&#35770;&#25991;&#24635;&#32467;&#20102;LLMs&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#25216;&#26415;&#65292;&#28085;&#30422;&#20102;&#23398;&#29983;&#21644;&#25945;&#24072;&#30340;&#36741;&#21161;&#65292;&#33258;&#36866;&#24212;&#23398;&#20064;&#21644;&#21830;&#19994;&#24037;&#20855;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#27599;&#20010;&#35270;&#35282;&#20013;&#30340;&#25216;&#26415;&#36827;&#27493;&#65292;&#25972;&#29702;&#20102;&#30456;&#20851;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#25945;&#32946;&#20013;&#37096;&#32626;LLMs&#25152;&#28041;&#21450;&#30340;&#39118;&#38505;&#21644;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#65292;&#31361;&#20986;&#20102;&#28508;&#22312;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#35843;&#30740;&#26088;&#22312;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#20915;&#31574;&#32773;&#25552;&#20379;&#20840;&#38754;&#30340;&#25216;&#26415;&#22270;&#26223;&#65292;&#20197;&#21033;&#29992;LLMs&#30340;&#21147;&#37327;&#65292;&#24443;&#24213;&#25913;&#38761;&#25945;&#32946;&#23454;&#36341;&#65292;&#24182;&#20419;&#36827;&#26356;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#23398;&#20064;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18105v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) has brought in a new era of possibilities in the realm of education. This survey paper summarizes the various technologies of LLMs in educational settings from multifaceted perspectives, encompassing student and teacher assistance, adaptive learning, and commercial tools. We systematically review the technological advancements in each perspective, organize related datasets and benchmarks, and identify the risks and challenges associated with deploying LLMs in education. Furthermore, we outline future research opportunities, highlighting the potential promising directions. Our survey aims to provide a comprehensive technological picture for educators, researchers, and policymakers to harness the power of LLMs to revolutionize educational practices and foster a more effective personalized learning environment.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT&#22312;&#36328;&#35821;&#35328;&#27861;&#24459;&#38382;&#31572;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20998;&#26512;&#33521;&#35821;&#21644;&#26085;&#35821;&#25552;&#31034;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#21457;&#23637;&#26356;&#39640;&#25928;&#20934;&#30830;&#30340;&#36328;&#35821;&#35328;&#38382;&#31572;&#35299;&#20915;&#26041;&#26696;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.18098</link><description>&lt;p&gt;
GPT&#19982;&#35821;&#35328;&#38556;&#30861;&#65306;&#36328;&#35821;&#35328;&#27861;&#24459;&#38382;&#31572;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPTs and Language Barrier: A Cross-Lingual Legal QA Examination
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT&#22312;&#36328;&#35821;&#35328;&#27861;&#24459;&#38382;&#31572;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20998;&#26512;&#33521;&#35821;&#21644;&#26085;&#35821;&#25552;&#31034;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20026;&#21457;&#23637;&#26356;&#39640;&#25928;&#20934;&#30830;&#30340;&#36328;&#35821;&#35328;&#38382;&#31572;&#35299;&#20915;&#26041;&#26696;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;COLIEE Task 4&#25968;&#25454;&#38598;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPTs&#65289;&#22312;&#36328;&#35821;&#35328;&#27861;&#24459;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;COLIEE Task 4&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#38472;&#36848;&#21644;&#19968;&#32452;&#20316;&#20026;&#19978;&#19979;&#25991;&#30340;&#30456;&#20851;&#27861;&#24459;&#25991;&#31456;&#65292;&#30446;&#26631;&#26159;&#30830;&#23450;&#35813;&#38472;&#36848;&#26159;&#21542;&#22312;&#27861;&#24459;&#19978;&#26377;&#25928;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#20174;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#25991;&#31456;&#20013;&#25512;&#26029;&#20986;&#26469;&#65292;&#20063;&#31216;&#20026;&#34164;&#28085;&#20219;&#21153;&#12290;&#36890;&#36807;&#35780;&#20272;&#22235;&#31181;&#19981;&#21516;&#30340;&#33521;&#35821;&#21644;&#26085;&#35821;&#25552;&#31034;&#21644;&#25968;&#25454;&#32452;&#21512;&#65292;&#25105;&#20204;&#20026;&#22810;&#35821;&#35328;&#27861;&#24459;&#38382;&#31572;&#22330;&#26223;&#20013;GPTs&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#22312;&#27861;&#24459;&#39046;&#22495;&#26356;&#39640;&#25928;&#20934;&#30830;&#30340;&#36328;&#35821;&#35328;&#38382;&#31572;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18098v1 Announce Type: cross  Abstract: In this paper, we explore the application of Generative Pre-trained Transformers (GPTs) in cross-lingual legal Question-Answering (QA) systems using the COLIEE Task 4 dataset. In the COLIEE Task 4, given a statement and a set of related legal articles that serve as context, the objective is to determine whether the statement is legally valid, i.e., if it can be inferred from the provided contextual articles or not, which is also known as an entailment task. By benchmarking four different combinations of English and Japanese prompts and data, we provide valuable insights into GPTs' performance in multilingual legal QA scenarios, contributing to the development of more efficient and accurate cross-lingual QA solutions in the legal domain.
&lt;/p&gt;</description></item><item><title>&#23558;&#25552;&#31034;&#25216;&#26415;&#20316;&#20026;&#26816;&#32034;&#31995;&#32479;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#36890;&#36807;BM25&#39044;&#25490;&#24207;&#21644;&#22522;&#20110;BERT&#30340;&#37325;&#26032;&#25490;&#24207;&#30340;&#25903;&#25345;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27861;&#24459;&#25991;&#20214;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18093</link><description>&lt;p&gt;
&#25552;&#21319;&#27861;&#24459;&#25991;&#20214;&#26816;&#32034;&#65306;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Legal Document Retrieval: A Multi-Phase Approach with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18093
&lt;/p&gt;
&lt;p&gt;
&#23558;&#25552;&#31034;&#25216;&#26415;&#20316;&#20026;&#26816;&#32034;&#31995;&#32479;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#36890;&#36807;BM25&#39044;&#25490;&#24207;&#21644;&#22522;&#20110;BERT&#30340;&#37325;&#26032;&#25490;&#24207;&#30340;&#25903;&#25345;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27861;&#24459;&#25991;&#20214;&#26816;&#32034;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-3.5&#12289;GPT-4&#21644;LLaMA&#31561;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#35768;&#22810;&#30740;&#31350;&#25506;&#32034;&#20102;&#26377;&#25928;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20197;&#21033;&#29992;&#36825;&#20123;LLM&#22312;&#21508;&#31181;&#30740;&#31350;&#38382;&#39064;&#19978;&#30340;&#33021;&#21147;&#12290;&#22312;&#27861;&#24459;&#25968;&#25454;&#39046;&#22495;&#65292;&#20855;&#20307;&#22312;&#26816;&#32034;&#19978;&#65292;&#30001;&#20110;&#27861;&#24459;&#25991;&#31456;&#25968;&#37327;&#24222;&#22823;&#19988;&#38271;&#24230;&#21487;&#35266;&#65292;&#30452;&#25509;&#24212;&#29992;&#25552;&#31034;&#25216;&#26415;&#38754;&#20020;&#30528;&#25361;&#25112;&#12290;&#36825;&#39033;&#30740;&#31350;&#19987;&#27880;&#20110;&#26368;&#22823;&#31243;&#24230;&#22320;&#21457;&#25381;&#25552;&#31034;&#30340;&#28508;&#21147;&#65292;&#23558;&#20854;&#32622;&#20110;&#26816;&#32034;&#31995;&#32479;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#21069;&#38754;&#26377;&#20004;&#20010;&#38454;&#27573;&#30340;&#25903;&#25345;&#65306;BM25&#39044;&#25490;&#24207;&#21644;&#22522;&#20110;BERT&#30340;&#37325;&#26032;&#25490;&#24207;&#12290;&#22312;COLIEE 2023&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23558;&#25552;&#31034;&#25216;&#26415;&#25972;&#21512;&#21040;&#26816;&#32034;&#31995;&#32479;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#32034;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#38169;&#35823;&#20998;&#26512;&#25581;&#31034;&#20102;&#26816;&#32034;&#31995;&#32479;&#20013;&#20173;&#38656;&#35201;&#35299;&#20915;&#30340;&#19968;&#20123;&#29616;&#26377;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18093v1 Announce Type: cross  Abstract: Large language models with billions of parameters, such as GPT-3.5, GPT-4, and LLaMA, are increasingly prevalent. Numerous studies have explored effective prompting techniques to harness the power of these LLMs for various research problems. Retrieval, specifically in the legal data domain, poses a challenging task for the direct application of Prompting techniques due to the large number and substantial length of legal articles. This research focuses on maximizing the potential of prompting by placing it as the final phase of the retrieval system, preceded by the support of two phases: BM25 Pre-ranking and BERT-based Re-ranking. Experiments on the COLIEE 2023 dataset demonstrate that integrating prompting techniques on LLMs into the retrieval system significantly improves retrieval accuracy. However, error analysis reveals several existing issues in the retrieval system that still need resolution.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120; (SCT)&#65292;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#20449;&#24687;&#30340;&#21367;&#31215;&#25805;&#20316;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#22797;&#26434;&#20613;&#37324;&#21494;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#23454;&#37096;&#21644;&#22797;&#37096;&#22810;&#35270;&#22270;&#20809;&#35889;&#31639;&#23376;&#30340;&#21327;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18063</link><description>&lt;p&gt;
&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120;&#65306;&#21327;&#35843;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#30340;&#23454;&#37096;&#21644;&#22797;&#37096;&#22810;&#35270;&#22270;&#20809;&#35889;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Spectral Convolutional Transformer: Harmonizing Real vs. Complex Multi-View Spectral Operators for Vision Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18063
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120; (SCT)&#65292;&#36890;&#36807;&#32467;&#21512;&#23616;&#37096;&#20449;&#24687;&#30340;&#21367;&#31215;&#25805;&#20316;&#21644;&#20840;&#23616;&#20449;&#24687;&#30340;&#22797;&#26434;&#20613;&#37324;&#21494;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;&#23545;&#35270;&#35273;&#21464;&#21387;&#22120;&#20013;&#23454;&#37096;&#21644;&#22797;&#37096;&#22810;&#35270;&#22270;&#20809;&#35889;&#31639;&#23376;&#30340;&#21327;&#35843;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20013;&#20351;&#29992;&#30340;Transformer&#24050;&#32463;&#36890;&#36807;&#21508;&#31181;&#32467;&#26500;&#36827;&#34892;&#20102;&#30740;&#31350; - &#22914;ViT&#12289;PVT&#21644;Swin&#12290;&#36825;&#20123;&#24037;&#20316;&#26088;&#22312;&#25913;&#36827;&#27880;&#24847;&#21147;&#26426;&#21046;&#24182;&#20351;&#20854;&#26356;&#21152;&#39640;&#25928;&#12290;&#19982;&#27492;&#19981;&#21516;&#30340;&#26159;&#65292;&#20154;&#20204;&#24863;&#21463;&#21040;&#20102;&#21253;&#21547;&#23616;&#37096;&#20449;&#24687;&#30340;&#38656;&#35201;&#65292;&#36825;&#23548;&#33268;&#22312;Transformer&#20013;&#24341;&#20837;&#21367;&#31215;&#65292;&#22914;CPVT&#21644;CvT&#12290;&#25105;&#20204;&#20351;&#29992;&#22797;&#26434;&#20613;&#31435;&#21494;&#22522;&#30784;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#65292;&#22914;AFNO&#12289;GFNet&#21644;Spectformer&#23454;&#29616;&#20840;&#23616;&#20196;&#29260;&#28151;&#21512;&#12290;&#25105;&#20204;&#25552;&#20513;&#32467;&#21512;&#25968;&#25454;&#30340;&#19977;&#31181;&#19981;&#21516;&#35270;&#22270; - &#23616;&#37096;&#12289;&#20840;&#23616;&#21644;&#38271;&#31243;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20165;&#20351;&#29992;&#23454;&#22495;&#20809;&#35889;&#34920;&#31034;&#30340;&#26368;&#31616;&#21333;&#20840;&#23616;&#34920;&#31034; - &#36890;&#36807;Hartley&#21464;&#25442;&#33719;&#24471;&#12290;&#25105;&#20204;&#22312;&#21021;&#22987;&#23618;&#20013;&#20351;&#29992;&#21367;&#31215;&#31639;&#23376;&#25429;&#25417;&#23616;&#37096;&#20449;&#24687;&#12290;&#36890;&#36807;&#36825;&#20004;&#20010;&#36129;&#29486;&#65292;&#25105;&#20204;&#33021;&#22815;&#20248;&#21270;&#24182;&#33719;&#24471;&#19968;&#20010;&#25552;&#20379;&#25913;&#36827;&#24615;&#33021;&#30340;&#20809;&#35889;&#21367;&#31215;&#21464;&#21387;&#22120;&#65288;SCT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18063v1 Announce Type: cross  Abstract: Transformers used in vision have been investigated through diverse architectures - ViT, PVT, and Swin. These have worked to improve the attention mechanism and make it more efficient. Differently, the need for including local information was felt, leading to incorporating convolutions in transformers such as CPVT and CvT. Global information is captured using a complex Fourier basis to achieve global token mixing through various methods, such as AFNO, GFNet, and Spectformer. We advocate combining three diverse views of data - local, global, and long-range dependence. We also investigate the simplest global representation using only the real domain spectral representation - obtained through the Hartley transform. We use a convolutional operator in the initial layers to capture local information. Through these two contributions, we are able to optimize and obtain a spectral convolution transformer (SCT) that provides improved performance 
&lt;/p&gt;</description></item><item><title>COIG-CQIA &#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#12289;&#24191;&#27867;&#30340;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20197;&#26356;&#22909;&#22320;&#20351;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#20132;&#20114;&#20445;&#25345;&#19968;&#33268;&#12290;</title><link>https://arxiv.org/abs/2403.18058</link><description>&lt;p&gt;
COIG-CQIA&#65306;&#21482;&#38656;&#36136;&#37327;&#8212;&#8212;&#38754;&#21521;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
COIG-CQIA: Quality is All You Need for Chinese Instruction Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18058
&lt;/p&gt;
&lt;p&gt;
COIG-CQIA &#26159;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#12289;&#24191;&#27867;&#30340;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20197;&#26356;&#22909;&#22320;&#20351;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#20132;&#20114;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#33521;&#35821;&#39046;&#22495;&#12290;&#36825;&#20123;&#36827;&#23637;&#20351;&#24471;&#36825;&#20123;LLMs&#33021;&#22815;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#20934;&#30830;&#24615;&#21644;&#27969;&#30021;&#24230;&#29702;&#35299;&#24182;&#25191;&#34892;&#22797;&#26434;&#25351;&#20196;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#30340;&#21457;&#23637;&#20173;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#20013;&#25991;&#35821;&#35328;&#30340;&#29420;&#29305;&#35821;&#35328;&#29305;&#24449;&#21644;&#25991;&#21270;&#28145;&#24230;&#20026;&#25351;&#20196;&#24494;&#35843;&#20219;&#21153;&#24102;&#26469;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#35201;&#20040;&#28304;&#33258;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;LLMs&#65292;&#35201;&#20040;&#19981;&#36866;&#21512;&#19982;&#29616;&#23454;&#20013;&#25991;&#29992;&#25143;&#30340;&#20132;&#20114;&#27169;&#24335;&#30456;&#31526;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;COIG-CQIA&#65292;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#20013;&#25991;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26500;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#12289;&#24191;&#27867;&#30340;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20197;&#26356;&#22909;&#22320;&#20351;&#27169;&#22411;&#34892;&#20026;&#19982;&#20154;&#31867;&#20132;&#20114;&#20445;&#25345;&#19968;&#33268;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#26469;&#28304;&#25910;&#38598;&#20102;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#32534;&#20889;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18058v1 Announce Type: cross  Abstract: Recently, there have been significant advancements in large language models (LLMs), particularly focused on the English language. These advancements have enabled these LLMs to understand and execute complex instructions with unprecedented accuracy and fluency. However, despite these advancements, there remains a noticeable gap in the development of Chinese instruction tuning. The unique linguistic features and cultural depth of the Chinese language pose challenges for instruction tuning tasks. Existing datasets are either derived from English-centric LLMs or are ill-suited for aligning with the interaction patterns of real-world Chinese users. To bridge this gap, we introduce COIG-CQIA, a high-quality Chinese instruction tuning dataset. Our aim is to build a diverse, wide-ranging instruction-tuning dataset to better align model behavior with human interactions. To this end, we collect a high-quality human-written corpus from various so
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30563;&#23548;&#25552;&#31034;&#35757;&#32451;&#65288;SPT&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#21452;LLM&#31995;&#32479;&#29983;&#25104;&#39640;&#25928;&#25552;&#31034;&#24182;&#24341;&#20837;&#24433;&#21709;&#20998;&#25968;&#27010;&#24565;&#65292;&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#25104;&#21151;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18051</link><description>&lt;p&gt;
&#30563;&#23548;&#25552;&#31034;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Supervisory Prompt Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18051
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30563;&#23548;&#25552;&#31034;&#35757;&#32451;&#65288;SPT&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#21452;LLM&#31995;&#32479;&#29983;&#25104;&#39640;&#25928;&#25552;&#31034;&#24182;&#24341;&#20837;&#24433;&#21709;&#20998;&#25968;&#27010;&#24565;&#65292;&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#25104;&#21151;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25552;&#31034;&#30340;&#36136;&#37327;&#65292;&#36825;&#20123;&#25552;&#31034;&#36890;&#24120;&#26159;&#25163;&#24037;&#35774;&#35745;&#30340;&#24182;&#19988;&#29305;&#23450;&#20110;&#20219;&#21153;&#65292;&#20351;&#24471;&#23427;&#20204;&#26114;&#36149;&#19988;&#19981;&#21487;&#25193;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#30563;&#23548;&#25552;&#31034;&#35757;&#32451;&#65288;SPT&#65289;&#12290;SPT&#21033;&#29992;&#21452;LLM&#31995;&#32479;&#33258;&#21160;&#29983;&#25104;&#39640;&#25928;&#30340;&#25552;&#31034;&#12290;&#22312;&#35813;&#31995;&#32479;&#20013;&#65292;&#19968;&#20010;LLM&#65292;&#21363;&#29983;&#25104;&#22120;&#65292;&#25191;&#34892;&#20219;&#21153;&#65292;&#32780;&#21478;&#19968;&#20010;LLM&#65292;&#21363;&#26657;&#27491;&#22120;&#65292;&#25552;&#20379;&#21453;&#39304;&#24182;&#29983;&#25104;&#25913;&#36827;&#30340;&#25552;&#31034;&#12290;&#19982;&#20808;&#21069;&#30340;&#25216;&#26415;&#30456;&#27604;&#65292;&#29983;&#25104;&#22120;&#21644;&#26657;&#27491;&#22120;&#22312;&#26102;&#38388;&#19978;&#20849;&#21516;&#24182;&#25345;&#32493;&#25913;&#36827;&#23427;&#20204;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#8220;&#24433;&#21709;&#20998;&#25968;&#8221;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#34913;&#37327;&#25552;&#31034;&#30340;&#21477;&#23376;&#32423;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#27979;&#35797;LLMs&#20013;&#24187;&#35273;&#30340;&#27700;&#24179;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#25104;&#21151;&#23558;GPT-4&#22312;GSM8K&#19978;&#30340;&#20934;&#30830;&#29575;&#20174;65.8%&#25552;&#39640;&#21040;94.1%&#65288;&#22686;&#21152;28.3%&#65289;&#12290;SPT&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#25552;&#39640;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18051v1 Announce Type: cross  Abstract: The performance of Large Language Models (LLMs) relies heavily on the quality of prompts, which are often manually engineered and task-specific, making them costly and non-scalable. We propose a novel approach, Supervisory Prompt Training (SPT). SPT automates the generation of highly effective prompts using a dual LLM system. In this system, one LLM, the generator, performs a task while the other, the corrector, provides feedback and generates improved prompts. In contrast to earlier techniques, both the generator and corrector collaboratively and continuously improve their prompts over time. We also introduce the concept of \textit{impact scores} to measure the sentence-level effectiveness of the prompts. Our method was tested on four benchmarks, testing the level of hallucinations in LLMs. Notably, we were able to increase the accuracy of GPT-4 on GSM8K from 65.8\% to 94.1\% (28.3\% increase). SPT advances LLMs by refining prompts to
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#35821;&#20041;&#25509;&#36817;&#23545;&#21453;&#21521;&#32763;&#35793;&#30340;&#24433;&#21709;&#24456;&#37325;&#35201;&#65292;&#25512;&#27979;&#36328;&#35821;&#31181;&#24179;&#34892;&#30340;&#35821;&#20041;&#20381;&#36182;&#24615;&#26159;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;</title><link>https://arxiv.org/abs/2403.18031</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#20013;&#21477;&#27861;&#21644;&#35821;&#20041;&#25509;&#36817;&#23545;&#21453;&#21521;&#32763;&#35793;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Syntactic and Semantic Proximity on Machine Translation with Back-Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18031
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#35821;&#20041;&#25509;&#36817;&#23545;&#21453;&#21521;&#32763;&#35793;&#30340;&#24433;&#21709;&#24456;&#37325;&#35201;&#65292;&#25512;&#27979;&#36328;&#35821;&#31181;&#24179;&#34892;&#30340;&#35821;&#20041;&#20381;&#36182;&#24615;&#26159;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18031v1&#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#26080;&#30417;&#30563;&#21363;&#26102;&#21453;&#21521;&#32763;&#35793;&#65292;&#32467;&#21512;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#26159;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#29702;&#35770;&#19978;&#65292;&#35813;&#26041;&#27861;&#19968;&#33324;&#19981;&#24212;&#36215;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#38024;&#23545;&#20154;&#24037;&#35821;&#35328;&#30340;&#21463;&#25511;&#23454;&#39564;&#65292;&#20197;&#30830;&#23450;&#21738;&#20123;&#35821;&#35328;&#23646;&#24615;&#20351;&#21453;&#21521;&#32763;&#35793;&#25104;&#20026;&#26377;&#25928;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#35206;&#30422;&#20102;&#35789;&#27719;&#12289;&#21477;&#27861;&#21644;&#35821;&#20041;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#26222;&#36941;&#35748;&#20026;&#30340;&#30456;&#21453;&#65292;&#65288;i&#65289;&#24179;&#34892;&#35789;&#39057;&#20998;&#24067;&#65292;&#65288;ii&#65289;&#37096;&#20998;&#20849;&#20139;&#35789;&#27719;&#21644;&#65288;iii&#65289;&#19981;&#21516;&#35821;&#31181;&#20043;&#38388;&#30340;&#31867;&#20284;&#21477;&#27861;&#32467;&#26500;&#24182;&#19981;&#36275;&#20197;&#35299;&#37322;&#21453;&#21521;&#32763;&#35793;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#26174;&#31034;&#65292;&#21363;&#20351;&#31895;&#31961;&#30340;&#35821;&#20041;&#20449;&#21495;&#65288;&#36328;&#35821;&#35328;&#30456;&#20284;&#30340;&#35789;&#27719;&#39046;&#22495;&#65289;&#20063;&#30830;&#23454;&#36890;&#36807;&#21453;&#21521;&#32763;&#35793;&#25913;&#21892;&#20102;&#20004;&#31181;&#35821;&#35328;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#29468;&#27979;&#65292;&#23500;&#21547;&#35821;&#20041;&#20381;&#36182;&#24615;&#24182;&#19988;&#36328;&#35821;&#31181;&#24179;&#34892;&#30340;&#35821;&#20041;&#23545;&#25104;&#21151;&#30340;&#26080;&#30417;&#30563;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18031v1 Announce Type: new  Abstract: Unsupervised on-the-fly back-translation, in conjunction with multilingual pretraining, is the dominant method for unsupervised neural machine translation. Theoretically, however, the method should not work in general. We therefore conduct controlled experiments with artificial languages to determine what properties of languages make back-translation an effective training method, covering lexical, syntactic, and semantic properties. We find, contrary to popular belief, that (i) parallel word frequency distributions, (ii) partially shared vocabulary, and (iii) similar syntactic structure across languages are not sufficient to explain the success of back-translation. We show however that even crude semantic signal (similar lexical fields across languages) does improve alignment of two languages through back-translation. We conjecture that rich semantic dependencies, parallel across languages, are at the root of the success of unsupervised 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;LM&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#23545;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.18025</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#23450;&#25513;&#30721;&#25439;&#22833;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#65306;&#20197;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35782;&#21035;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Improving Pre-trained Language Model Sensitivity via Mask Specific losses: A case study on Biomedical NER
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18025
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;LM&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#23545;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#25935;&#24863;&#24615;&#65292;&#36890;&#36807;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#37325;&#35201;&#24615;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35843;&#25972;&#21040;&#26032;&#39046;&#22495;&#36890;&#24120;&#36890;&#36807;&#22312;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;LM&#65288;PLM&#65289;&#26469;&#23454;&#29616;&#12290;&#24494;&#35843;&#23558;&#26032;&#30693;&#35782;&#24341;&#20837;LM&#65292;&#20351;&#23427;&#33021;&#22815;&#29702;&#35299;&#21644;&#26377;&#25928;&#25191;&#34892;&#30446;&#26631;&#22495;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#21464;&#24471;&#19981;&#22815;&#25935;&#24863;&#65292;&#22914;&#26524;&#23427;&#24573;&#35270;&#20102;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#24191;&#27867;&#24046;&#24322;&#65288;&#20363;&#22914;&#22312;&#35789;&#20041;&#19978;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#24494;&#35843;&#19981;&#25935;&#24863;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Mask Specific Language Modeling&#65288;MSLM&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#36866;&#24403;&#21152;&#26435;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#65288;DS-terms&#65289;&#30340;&#37325;&#35201;&#24615;&#26469;&#26377;&#25928;&#33719;&#21462;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;MSLM&#21516;&#26102;&#23631;&#34109;DS&#26415;&#35821;&#21644;&#36890;&#29992;&#35789;&#65292;&#28982;&#21518;&#36890;&#36807;&#30830;&#20445;LM&#21463;&#21040;&#26356;&#22823;&#24809;&#32602;&#26469;&#23398;&#20064;&#29305;&#23450;&#20110;&#25513;&#30721;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18025v1 Announce Type: cross  Abstract: Adapting language models (LMs) to novel domains is often achieved through fine-tuning a pre-trained LM (PLM) on domain-specific data. Fine-tuning introduces new knowledge into an LM, enabling it to comprehend and efficiently perform a target domain task. Fine-tuning can however be inadvertently insensitive if it ignores the wide array of disparities (e.g in word meaning) between source and target domains. For instance, words such as chronic and pressure may be treated lightly in social conversations, however, clinically, these words are usually an expression of concern. To address insensitive fine-tuning, we propose Mask Specific Language Modeling (MSLM), an approach that efficiently acquires target domain knowledge by appropriately weighting the importance of domain-specific terms (DS-terms) during fine-tuning. MSLM jointly masks DS-terms and generic words, then learns mask-specific losses by ensuring LMs incur larger penalties for in
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32858;&#31867;&#23450;&#20041;&#65292;&#23558;&#29616;&#26377;&#22810;&#31181;&#35821;&#35328;&#30340;&#35789;&#35821;&#20351;&#29992;&#22270;&#35889;WUGs&#36827;&#34892;&#20016;&#23500;&#65292;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#36825;&#20123;&#23450;&#20041;&#26356;&#22909;&#22320;&#21305;&#37197;WUGs&#20013;&#30340;&#31751;&#65292;&#23545;&#20110;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#21464;&#21270;&#24314;&#27169;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2403.18024</link><description>&lt;p&gt;
&#29992;&#32858;&#31867;&#23450;&#20041;&#20016;&#23500;&#35789;&#35821;&#20351;&#29992;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Enriching Word Usage Graphs with Cluster Definitions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18024
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32858;&#31867;&#23450;&#20041;&#65292;&#23558;&#29616;&#26377;&#22810;&#31181;&#35821;&#35328;&#30340;&#35789;&#35821;&#20351;&#29992;&#22270;&#35889;WUGs&#36827;&#34892;&#20016;&#23500;&#65292;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#36825;&#20123;&#23450;&#20041;&#26356;&#22909;&#22320;&#21305;&#37197;WUGs&#20013;&#30340;&#31751;&#65292;&#23545;&#20110;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#21464;&#21270;&#24314;&#27169;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35789;&#35821;&#20351;&#29992;&#22270;&#35889;&#65288;WUGs&#65289;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#29616;&#26377;&#30340;&#22810;&#31181;&#35821;&#35328;&#30340;WUGs&#21152;&#20837;&#20102;&#20316;&#20026;&#24847;&#20041;&#23450;&#20041;&#30340;&#32858;&#31867;&#26631;&#31614;&#12290;&#36825;&#20123;&#26631;&#31614;&#26159;&#36890;&#36807;&#32463;&#36807;&#24494;&#35843;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#30340;&#12290;&#36827;&#34892;&#30340;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;&#36825;&#20123;&#23450;&#20041;&#19982;&#29616;&#26377;&#30340;WUGs&#20013;&#30340;&#31751;&#26356;&#21305;&#37197;&#65292;&#27604;&#36215;&#20004;&#20010;&#22522;&#32447;&#31995;&#32479;&#20174;WordNet&#20013;&#36873;&#25321;&#30340;&#23450;&#20041;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#26131;&#20110;&#20351;&#29992;&#19988;&#26131;&#20110;&#25193;&#23637;&#21040;&#26032;&#30340;&#35821;&#35328;&#12290;&#20135;&#29983;&#30340;&#20016;&#23500;&#25968;&#25454;&#38598;&#23545;&#20110;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#35821;&#20041;&#21464;&#21270;&#24314;&#27169;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18024v1 Announce Type: new  Abstract: We present a dataset of word usage graphs (WUGs), where the existing WUGs for multiple languages are enriched with cluster labels functioning as sense definitions. They are generated from scratch by fine-tuned encoder-decoder language models. The conducted human evaluation has shown that these definitions match the existing clusters in WUGs better than the definitions chosen from WordNet by two baseline systems. At the same time, the method is straightforward to use and easy to extend to new languages. The resulting enriched datasets can be extremely helpful for moving on to explainable semantic change modeling.
&lt;/p&gt;</description></item><item><title>DORE&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#30340;&#23450;&#20041;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#21253;&#21547;&#36229;&#36807;10&#19975;&#20010;&#23450;&#20041;&#65292;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.18018</link><description>&lt;p&gt;
DORE&#65306;&#19968;&#20221;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#23450;&#20041;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DORE: A Dataset For Portuguese Definition Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18018
&lt;/p&gt;
&lt;p&gt;
DORE&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#30340;&#23450;&#20041;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#21253;&#21547;&#36229;&#36807;10&#19975;&#20010;&#23450;&#20041;&#65292;&#24182;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18018v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#23450;&#20041;&#24314;&#27169;&#65288;DM&#65289;&#26159;&#33258;&#21160;&#20026;&#29305;&#23450;&#21333;&#35789;&#29983;&#25104;&#35789;&#20856;&#23450;&#20041;&#30340;&#20219;&#21153;&#12290;&#20855;&#26377;DM&#33021;&#21147;&#30340;&#35745;&#31639;&#31995;&#32479;&#21487;&#20197;&#22312;&#22810;&#20010;&#21463;&#20247;&#20013;&#21463;&#30410;&#65292;&#22240;&#20026;DM&#34987;&#35270;&#20026;&#30417;&#30563;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#38382;&#39064;&#65292;&#36825;&#20123;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#24050;&#32463;&#21457;&#24067;&#20102;&#19968;&#20123;&#29992;&#20110;&#33521;&#35821;&#21644;&#20854;&#20182;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;DM&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#33889;&#33796;&#29273;&#35821;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#20013;/&#39640;&#36164;&#28304;&#35821;&#35328;&#65292;&#19988;&#34987;2&#20159;&#22810;&#27597;&#35821;&#20154;&#21475;&#20351;&#29992;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#33889;&#33796;&#29273;&#35821;&#30340;DM&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;DORE&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65307;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#30340;&#23450;&#20041;&#24314;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;10&#19975;&#20010;&#23450;&#20041;&#12290;&#25105;&#20204;&#36824;&#22312;DORE&#19978;&#35780;&#20272;&#20102;&#20960;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;DM&#27169;&#22411;&#65292;&#24182;&#25253;&#21578;&#20102;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18018v1 Announce Type: new  Abstract: Definition modelling (DM) is the task of automatically generating a dictionary definition for a specific word. Computational systems that are capable of DM can have numerous applications benefiting a wide range of audiences. As DM is considered a supervised natural language generation problem, these systems require large annotated datasets to train the machine learning (ML) models. Several DM datasets have been released for English and other high-resource languages. While Portuguese is considered a mid/high-resource language in most natural language processing tasks and is spoken by more than 200 million native speakers, there is no DM dataset available for Portuguese. In this research, we fill this gap by introducing DORE; the first dataset for Definition MOdelling for PoRtuguEse containing more than 100,000 definitions. We also evaluate several deep learning based DM models on DORE and report the results. The dataset and the findings o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#38382;&#31572;&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#20869;&#22312;&#29983;&#25104;&#23376;&#22270;&#26469;&#25552;&#20379;&#20915;&#31574;&#27934;&#23519;&#65292;&#24182;&#22312;GQA&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17647</link><description>&lt;p&gt;
&#29992;&#20110;&#21487;&#35299;&#37322;&#22270;&#20687;&#38382;&#31572;&#30340;&#20869;&#22312;&#23376;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Subgraph Generation for Interpretable Graph based Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#38382;&#31572;&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#20869;&#22312;&#29983;&#25104;&#23376;&#22270;&#26469;&#25552;&#20379;&#20915;&#31574;&#27934;&#23519;&#65292;&#24182;&#22312;GQA&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#23545;&#21487;&#35299;&#37322;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#22823;&#22810;&#25968;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#20391;&#37325;&#20110;&#29983;&#25104;&#20107;&#21518;&#35299;&#37322;&#65292;&#32780;&#38750;&#37319;&#21462;&#20869;&#22312;&#26041;&#27861;&#65292;&#21518;&#32773;&#29305;&#24449;&#21270;&#20102;&#21487;&#35299;&#37322;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#22270;&#30340;VQA&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#22312;GQA&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#24357;&#21512;&#20102;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#34987;&#35774;&#35745;&#25104;&#22312;&#38382;&#31572;&#36807;&#31243;&#20013;&#26412;&#36136;&#19978;&#29983;&#25104;&#19968;&#20010;&#23376;&#22270;&#20316;&#20026;&#35299;&#37322;&#65292;&#25552;&#20379;&#20915;&#31574;&#21046;&#23450;&#30340;&#27934;&#23519;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#29983;&#25104;&#30340;&#23376;&#22270;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#24314;&#31435;&#30340;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20107;&#21518;&#35299;&#37322;&#33021;&#21147;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#36827;&#34892;&#20154;&#31867;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19982;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17647v1 Announce Type: new  Abstract: The large success of deep learning based methods in Visual Question Answering (VQA) has concurrently increased the demand for explainable methods. Most methods in Explainable Artificial Intelligence (XAI) focus on generating post-hoc explanations rather than taking an intrinsic approach, the latter characterizing an interpretable model. In this work, we introduce an interpretable approach for graph-based VQA and demonstrate competitive performance on the GQA dataset. This approach bridges the gap between interpretability and performance. Our model is designed to intrinsically produce a subgraph during the question-answering process as its explanation, providing insight into the decision making. To evaluate the quality of these generated subgraphs, we compare them against established post-hoc explainability methods for graph neural networks, and perform a human evaluation. Moreover, we present quantitative metrics that correlate with the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#21069;&#32512;&#35843;&#25972;&#30340;&#28151;&#21512;&#20513;&#35758;&#21709;&#24212;&#29983;&#25104;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20132;&#21449;&#27745;&#26579;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#22312;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#35774;&#32622;&#19979;&#23398;&#20064;&#20513;&#35758;&#24863;&#30693;&#21069;&#32512;&#12290;</title><link>https://arxiv.org/abs/2403.17636</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#21069;&#32512;&#35843;&#25972;&#30340;&#28151;&#21512;&#20513;&#35758;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Mix-Initiative Response Generation with Dynamic Prefix Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17636
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21160;&#24577;&#21069;&#32512;&#35843;&#25972;&#30340;&#28151;&#21512;&#20513;&#35758;&#21709;&#24212;&#29983;&#25104;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20132;&#21449;&#27745;&#26579;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#22312;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#35774;&#32622;&#19979;&#23398;&#20064;&#20513;&#35758;&#24863;&#30693;&#21069;&#32512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#20513;&#35758;&#22312;&#25511;&#21046;&#23545;&#35805;&#26041;&#21521;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23545;&#20110;&#21457;&#35328;&#32773;&#65292; passively &#21709;&#24212;&#25110; proactively &#20027;&#23548;&#20250;&#23548;&#33268;&#23436;&#20840;&#19981;&#21516;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#23545;&#35805;&#31995;&#32479;&#19987;&#27880;&#20110;&#35757;&#32451;&#19968;&#20010;&#32479;&#19968;&#30340;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#65292;&#32780;&#19981;&#21306;&#20998;&#19981;&#21516;&#30340;&#20513;&#35758;&#12290;&#36825;&#23548;&#33268;&#20102;&#20132;&#21449;&#27745;&#26579;&#38382;&#39064;&#65292;&#27169;&#22411;&#28151;&#28102;&#20102;&#19981;&#21516;&#30340;&#20513;&#35758;&#24182;&#29983;&#25104;&#19981;&#21512;&#36866;&#30340;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#20026;&#20513;&#35758;&#26631;&#31614;&#33719;&#21462;&#22823;&#37327;&#20154;&#31867;&#27880;&#37322;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#28151;&#21512;&#20513;&#35758;&#21160;&#24577;&#21069;&#32512;&#35843;&#25972;&#26694;&#26550; (IDPT)&#65292;&#20197;&#35299;&#32806;&#19981;&#21516;&#20513;&#35758;&#19982;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#35774;&#32622;&#19979;&#23398;&#20064;&#20513;&#35758;&#24863;&#30693;&#21069;&#32512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;IDPT&#23558;&#20513;&#35758;&#22240;&#32032;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#21069;&#32512;&#21442;&#25968;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#35843;&#25972;&#20513;&#35758;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17636v1 Announce Type: new  Abstract: Mixed initiative serves as one of the key factors in controlling conversation directions. For a speaker, responding passively or leading proactively would result in rather different responses. However, most dialogue systems focus on training a holistic response generation model without any distinction among different initiatives. It leads to the cross-contamination problem, where the model confuses different initiatives and generates inappropriate responses. Moreover, obtaining plenty of human annotations for initiative labels can be expensive. To address this issue, we propose a general mix-Initiative Dynamic Prefix Tuning framework (IDPT) to decouple different initiatives from the generation model, which learns initiative-aware prefixes in both supervised and unsupervised settings. Specifically, IDPT decouples initiative factors into different prefix parameters and uses the attention mechanism to adjust the selection of initiatives in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;&#27531;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#25928;&#24615;&#65292;&#21033;&#29992;&#20923;&#32467;&#30340;&#21464;&#21387;&#22120;&#22359;&#36827;&#34892;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#20196;&#29260;&#65292;&#20174;&#32780;&#25552;&#39640;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17343</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#30340;&#20813;&#36153;&#21161;&#25512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language Models are Free Boosters for Biomedical Imaging Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22522;&#20110;&#27531;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#25928;&#24615;&#65292;&#21033;&#29992;&#20923;&#32467;&#30340;&#21464;&#21387;&#22120;&#22359;&#36827;&#34892;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#20196;&#29260;&#65292;&#20174;&#32780;&#25552;&#39640;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22522;&#20110;&#27531;&#24046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#20219;&#21153;&#20013;&#20316;&#20026;&#32534;&#30721;&#22120;&#30340;&#24847;&#24819;&#19981;&#21040;&#30340;&#26377;&#25928;&#24615;&#65292;&#36825;&#26159;&#20256;&#32479;&#19978;&#32570;&#20047;&#35821;&#35328;&#25110;&#25991;&#26412;&#25968;&#25454;&#30340;&#39046;&#22495;&#12290;&#35813;&#26041;&#27861;&#19981;&#21516;&#20110;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;&#39044;&#35757;&#32451;&#30340;LLMs&#20013;&#25552;&#21462;&#30340;&#20923;&#32467;&#21464;&#21387;&#22120;&#22359;&#20316;&#20026;&#21019;&#26032;&#30340;&#32534;&#30721;&#22120;&#23618;&#65292;&#30452;&#25509;&#22788;&#29702;&#35270;&#35273;&#20196;&#29260;&#12290;&#36825;&#31181;&#31574;&#30053;&#19982;&#36890;&#24120;&#20381;&#36182;&#20110;&#35821;&#35328;&#39537;&#21160;&#25552;&#31034;&#21644;&#36755;&#20837;&#30340;&#26631;&#20934;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#26694;&#26550;&#26377;&#30528;&#26174;&#33879;&#30340;&#19981;&#21516;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;LLMs&#33021;&#22815;&#25552;&#21319;&#21508;&#31181;&#29983;&#29289;&#21307;&#23398;&#25104;&#20687;&#24212;&#29992;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;2D&#21644;3D&#35270;&#35273;&#20998;&#31867;&#20219;&#21153;&#65292;&#20805;&#24403;&#21363;&#25554;&#21363;&#29992;&#30340;&#21161;&#25512;&#22120;&#12290;&#26356;&#26377;&#36259;&#30340;&#26159;&#65292;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#21457;&#29616;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#22312;M&#30340;&#24191;&#27867;&#12289;&#26631;&#20934;&#21270;&#25968;&#25454;&#38598;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17343v1 Announce Type: cross  Abstract: In this study, we uncover the unexpected efficacy of residual-based large language models (LLMs) as part of encoders for biomedical imaging tasks, a domain traditionally devoid of language or textual data. The approach diverges from established methodologies by utilizing a frozen transformer block, extracted from pre-trained LLMs, as an innovative encoder layer for the direct processing of visual tokens. This strategy represents a significant departure from the standard multi-modal vision-language frameworks, which typically hinge on language-driven prompts and inputs. We found that these LLMs could boost performance across a spectrum of biomedical imaging applications, including both 2D and 3D visual classification tasks, serving as plug-and-play boosters. More interestingly, as a byproduct, we found that the proposed framework achieved superior performance, setting new state-of-the-art results on extensive, standardized datasets in M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24212;&#29992;&#24341;&#23548;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#65292;&#20026;&#24503;&#35821;&#21019;&#24314;&#20102;&#26368;&#22823;&#30340;&#20256;&#35760;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21457;&#24067;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.17143</link><description>&lt;p&gt;
&#24341;&#23548;&#36828;&#31243;&#30417;&#30563;&#29992;&#20110;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#65306;&#36866;&#24212;&#26032;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Guided Distant Supervision for Multilingual Relation Extraction Data: Adapting to a New Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24212;&#29992;&#24341;&#23548;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#65292;&#20026;&#24503;&#35821;&#21019;&#24314;&#20102;&#26368;&#22823;&#30340;&#20256;&#35760;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21457;&#24067;&#20102;&#25163;&#21160;&#26631;&#27880;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#65306;&#20851;&#31995;&#25277;&#21462;&#23545;&#20110;&#22312;&#25968;&#23383;&#20154;&#25991;&#23398;&#21644;&#30456;&#20851;&#23398;&#31185;&#32972;&#26223;&#19979;&#25552;&#21462;&#21644;&#29702;&#35299;&#20256;&#35760;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#31038;&#21306;&#23545;&#26500;&#24314;&#33021;&#22815;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#21462;&#20851;&#31995;&#30340;&#25968;&#25454;&#38598;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#26631;&#27880;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#65292;&#32780;&#19988;&#20165;&#38480;&#20110;&#33521;&#35821;&#12290;&#26412;&#25991;&#24212;&#29992;&#20102;&#24341;&#23548;&#24335;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#65292;&#20026;&#24503;&#35821;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#20256;&#35760;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#36229;&#36807;80,000&#20010;&#23454;&#20363;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#20851;&#31995;&#31867;&#22411;&#65292;&#26159;&#26368;&#22823;&#30340;&#24503;&#35821;&#20256;&#35760;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;2000&#20010;&#23454;&#20363;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#19982;&#21033;&#29992;&#24341;&#23548;&#24335;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#32534;&#21046;&#30340;&#25968;&#25454;&#38598;&#19968;&#36215;&#21457;&#24067;&#12290;&#25105;&#20204;&#22312;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17143v1 Announce Type: new  Abstract: Relation extraction is essential for extracting and understanding biographical information in the context of digital humanities and related subjects. There is a growing interest in the community to build datasets capable of training machine learning models to extract relationships. However, annotating such datasets can be expensive and time-consuming, in addition to being limited to English. This paper applies guided distant supervision to create a large biographical relationship extraction dataset for German. Our dataset, composed of more than 80,000 instances for nine relationship types, is the largest biographical German relationship extraction dataset. We also create a manually annotated dataset with 2000 instances to evaluate the models and release it together with the dataset compiled using guided distant supervision. We train several state-of-the-art machine learning models on the automatically created dataset and release them as 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16915</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31895;&#35843;&#20248;&#30340;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM-based IR&#65289;&#36827;&#34892;&#24494;&#35843;&#38656;&#35201;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#38500;&#20102;&#19979;&#28216;&#20219;&#21153;&#29305;&#23450;&#30340;&#23398;&#20064;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#36890;&#36807;&#22312;&#31895;&#35843;&#20248;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#25105;&#20204;&#26088;&#22312;&#20943;&#23569;&#24494;&#35843;&#30340;&#36127;&#25285;&#65292;&#25552;&#39640;&#19979;&#28216;IR&#20219;&#21153;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#31895;&#35843;&#20248;&#30340;&#26597;&#35810;-&#25991;&#26723;&#23545;&#39044;&#27979;&#65288;QDPP&#65289;&#65292;&#20854;&#39044;&#27979;&#26597;&#35810;-&#25991;&#26723;&#23545;&#30340;&#36866;&#24403;&#24615;&#12290;&#35780;&#20272;&#23454;&#39564;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#22235;&#20010;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#25968;&#25454;&#38598;&#20013;&#30340;MRR&#21644;/&#25110;nDCG@5&#12290;&#27492;&#22806;&#65292;&#26597;&#35810;&#39044;&#27979;&#20219;&#21153;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31895;&#35843;&#20248;&#20419;&#36827;&#20102;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16915v1 Announce Type: cross  Abstract: Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ViTLP&#30340;&#21487;&#35270;&#24341;&#23548;&#30340;&#29983;&#25104;&#25991;&#26412;&#24067;&#23616;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#35789;&#27719;&#23494;&#38598;&#22411;&#25991;&#26723;&#65292;&#24182;&#19988;&#21487;&#20197;&#20316;&#20026;OCR&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#23450;&#20301;&#21644;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.16516</link><description>&lt;p&gt;
&#21487;&#35270;&#24341;&#23548;&#30340;&#29983;&#25104;&#24335;&#25991;&#26412;&#24067;&#23616;&#39044;&#35757;&#32451;&#29992;&#20110;&#25991;&#26723;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Visually Guided Generative Text-Layout Pre-training for Document Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16516
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ViTLP&#30340;&#21487;&#35270;&#24341;&#23548;&#30340;&#29983;&#25104;&#25991;&#26412;&#24067;&#23616;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#35789;&#27719;&#23494;&#38598;&#22411;&#25991;&#26723;&#65292;&#24182;&#19988;&#21487;&#20197;&#20316;&#20026;OCR&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#23450;&#20301;&#21644;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#25216;&#26415;&#21487;&#20197;&#25552;&#21319;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#65288;VDU&#65289;&#30340;&#24615;&#33021;&#65292;&#36890;&#24120;&#38656;&#35201;&#27169;&#22411;&#33719;&#24471;&#24863;&#30693;&#21644;&#25512;&#29702;&#25991;&#26723;&#25991;&#26412;&#21644;&#24067;&#23616;&#65288;&#20363;&#22914;&#25991;&#26412;&#21644;&#34920;&#26684;&#21333;&#20803;&#30340;&#20301;&#32622;&#65289;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ViTLP&#30340;&#21487;&#35270;&#24341;&#23548;&#30340;&#29983;&#25104;&#25991;&#26412;&#24067;&#23616;&#39044;&#35757;&#32451;&#25216;&#26415;&#12290;&#32473;&#23450;&#19968;&#20010;&#25991;&#26723;&#22270;&#20687;&#65292;&#35813;&#27169;&#22411;&#20248;&#21270;&#20998;&#23618;&#35821;&#35328;&#21644;&#24067;&#23616;&#24314;&#27169;&#30446;&#26631;&#65292;&#20197;&#29983;&#25104;&#20132;&#38169;&#30340;&#25991;&#26412;&#21644;&#24067;&#23616;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;Transformers&#22788;&#29702;&#38271;&#25991;&#26723;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#27573;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#26041;&#26696;&#65292;&#20351;ViTLP&#33021;&#22815;&#22788;&#29702;&#20219;&#24847;&#38271;&#24230;&#30340;&#35789;&#27719;&#23494;&#38598;&#22411;&#25991;&#26723;&#12290;ViTLP&#21487;&#20197;&#20316;&#20026;&#26412;&#22320;OCR&#27169;&#22411;&#65292;&#29992;&#20110;&#23450;&#20301;&#21644;&#35782;&#21035;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;ViTLP&#21487;&#20197;&#26377;&#25928;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;VDU&#20219;&#21153;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;ViTLP&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16516v1 Announce Type: new  Abstract: Prior study shows that pre-training techniques can boost the performance of visual document understanding (VDU), which typically requires models to gain abilities to perceive and reason both document texts and layouts (e.g., locations of texts and table-cells). To this end, we propose visually guided generative text-layout pre-training, named ViTLP. Given a document image, the model optimizes hierarchical language and layout modeling objectives to generate the interleaved text and layout sequence. In addition, to address the limitation of processing long documents by Transformers, we introduce a straightforward yet effective multi-segment generative pre-training scheme, facilitating ViTLP to process word-intensive documents of any length. ViTLP can function as a native OCR model to localize and recognize texts of document images. Besides, ViTLP can be effectively applied to various downstream VDU tasks. Extensive experiments show that Vi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#26597;&#35810;&#23545;&#40784;&#65292;&#24182;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#25552;&#20379;&#20102;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16512</link><description>&lt;p&gt;
LLMs&#26159;&#23569;&#26679;&#26412;&#24773;&#22659;&#20302;&#36164;&#28304;&#35821;&#35328;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
LLMs Are Few-Shot In-Context Low-Resource Language Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16512
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#26597;&#35810;&#23545;&#40784;&#65292;&#24182;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#25552;&#20379;&#20102;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#25903;&#25345;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#21033;&#29992;&#30701;&#26102;&#30340;&#24773;&#22659;&#20449;&#24687;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#65292;&#36825;&#20026;&#32553;&#23567;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#36317;&#25552;&#20379;&#20102;&#37325;&#35201;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25506;&#35752;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#38598;&#20013;&#22312;&#30456;&#23545;&#39640;&#36164;&#28304;&#30340;&#35821;&#35328;&#65292;&#27604;&#22914;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;ICL&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#65288;X-ICL&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#19981;&#20165;&#35780;&#20272;&#20102;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#36824;&#21457;&#29616;&#20102;&#24773;&#22659;&#26631;&#31614;&#23545;&#40784;&#30340;&#32570;&#38519;&#65292;&#24182;&#24341;&#20837;&#20102;&#26356;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#26597;&#35810;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#30340;&#21508;&#20010;&#26041;&#38754;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24635;&#32467;&#20102;&#23569;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16512v1 Announce Type: cross  Abstract: In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages. Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish. In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages. Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of ICL for low-resource languages. Our study concludes the significance of few-shot in-cont
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#36807;&#31243;&#25581;&#31034;&#20102;&#29983;&#25104;&#23545;&#25239;&#25552;&#31034;&#20197;&#35823;&#23548;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#24341;&#21457;&#20102;&#23545;&#35813;&#33539;&#24335;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#25285;&#24551;&#12290;</title><link>https://arxiv.org/abs/2403.16432</link><description>&lt;p&gt;
$\textit{LinkPrompt}$: &#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#21644;&#36890;&#29992;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
$\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on Prompt-based Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16432
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#36807;&#31243;&#25581;&#31034;&#20102;&#29983;&#25104;&#23545;&#25239;&#25552;&#31034;&#20197;&#35823;&#23548;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#24341;&#21457;&#20102;&#23545;&#35813;&#33539;&#24335;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt-based learning &#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#35843;&#25972;&#21040;&#19979;&#28216;&#20219;&#21153;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#25552;&#21319;&#20102;&#24615;&#33021;&#22522;&#20934;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20248;&#21270;&#25628;&#32034;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#22266;&#23450;&#30340;&#25552;&#31034;&#27169;&#26495;&#26469;&#24494;&#35843;&#27169;&#22411;&#12290;&#36825;&#31181;&#22522;&#20110;&#25552;&#31034;&#20248;&#21270;&#36807;&#31243;&#23545;PLMs&#30340;&#23398;&#20064;&#20063;&#25581;&#31034;&#20102;&#29983;&#25104;&#23545;&#25239;&#25552;&#31034;&#20197;&#35823;&#23548;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#24341;&#21457;&#20102;&#23545;&#36825;&#19968;&#33539;&#24335;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#25285;&#24551;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#29983;&#25104;&#36890;&#29992;&#23545;&#25239;&#35302;&#21457;&#22120;&#65288;UATs&#65289;&#26469;&#25913;&#21464;&#19981;&#20165;&#30446;&#26631;PLMs&#30340;&#39044;&#27979;&#65292;&#36824;&#26377;&#23545;&#24212;Prompt-based Fine-tuning Models&#65288;PFMs&#65289;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#20316;&#21697;&#20013;&#21457;&#29616;&#30340;UATs&#36890;&#24120;&#26159;&#26080;&#27861;&#38405;&#35835;&#30340;&#20196;&#29260;&#25110;&#23383;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16432v1 Announce Type: cross  Abstract: Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks. Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization. Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm. Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm. However, UATs found in previous works are often unreadable tokens or characters a
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20013;&#24515;&#25513;&#34109;&#30340;GLIP&#25216;&#26415;&#22312;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#20013;&#21462;&#20195;&#20102;&#38543;&#26426;&#25513;&#34109;&#65292;&#21033;&#29992;&#39640;&#26031;&#20998;&#24067;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#26131;&#20110;&#33719;&#24471;&#19988;&#36866;&#29992;&#20110;&#19981;&#20855;&#26377;&#26126;&#26174;&#20013;&#24515;&#28966;&#28857;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.15837</link><description>&lt;p&gt;
&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#20013;&#24515;&#25513;&#34109;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Centered Masking for Language-Image Pre-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15837
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20013;&#24515;&#25513;&#34109;&#30340;GLIP&#25216;&#26415;&#22312;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#20013;&#21462;&#20195;&#20102;&#38543;&#26426;&#25513;&#34109;&#65292;&#21033;&#29992;&#39640;&#26031;&#20998;&#24067;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#26131;&#20110;&#33719;&#24471;&#19988;&#36866;&#29992;&#20110;&#19981;&#20855;&#26377;&#26126;&#26174;&#20013;&#24515;&#28966;&#28857;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;GLIP&#65289;&#30340;&#39640;&#26031;&#25513;&#34109;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#12289;&#30452;&#25509;&#21644;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#22270;&#20687;&#34917;&#19969;&#36827;&#34892;&#25513;&#34109;&#12290;GLIP&#22522;&#20110;&#24555;&#36895;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;FLIP&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;CLIP&#27169;&#22411;&#26102;&#38543;&#26426;&#23631;&#34109;&#22270;&#20687;&#34917;&#19969;&#12290;GLIP&#23558;&#38543;&#26426;&#23631;&#34109;&#26367;&#25442;&#20026;&#20013;&#24515;&#25513;&#34109;&#65292;&#20351;&#29992;&#39640;&#26031;&#20998;&#24067;&#65292;&#24182;&#21463;&#21040;&#22270;&#20687;&#20013;&#24515;&#37325;&#35201;&#24615;&#30340;&#21551;&#21457;&#12290;&#22312;&#19968;&#31995;&#21015;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#20013;&#65292;GLIP&#20445;&#30041;&#20102;&#19982;FLIP&#30456;&#21516;&#30340;&#35745;&#31639;&#33410;&#30465;&#33021;&#21147;&#65292;&#21516;&#26102;&#25913;&#21892;&#20102;&#24615;&#33021;&#65292;&#36825;&#26159;&#30001;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#25152;&#35777;&#23454;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GLIP&#30340;&#22909;&#22788;&#24456;&#23481;&#26131;&#33719;&#24471;&#65292;&#26080;&#38656;&#31934;&#32454;&#35843;&#25972;&#39640;&#26031;&#65292;&#20063;&#36866;&#29992;&#20110;&#21253;&#21547;&#26080;&#26126;&#26174;&#20013;&#24515;&#28966;&#28857;&#22270;&#29255;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15837v1 Announce Type: cross  Abstract: We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel, straightforward, and effective technique for masking image patches during pre-training of a vision-language model. GLIP builds on Fast Language-Image Pre-Training (FLIP), which randomly masks image patches while training a CLIP model. GLIP replaces random masking with centered masking, that uses a Gaussian distribution and is inspired by the importance of image patches at the center of the image. GLIP retains the same computational savings as FLIP, while improving performance across a range of downstream datasets and tasks, as demonstrated by our experimental results. We show the benefits of GLIP to be easy to obtain, requiring no delicate tuning of the Gaussian, and also applicable to data sets containing images without an obvious center focus.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26377;&#26395;&#25552;&#20379;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24212;&#27880;&#24847;&#20854;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#31215;&#26497;&#37319;&#21462;&#31574;&#30053;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.14814</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#26426;&#20250;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
The opportunities and risks of large language models in mental health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14814
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26377;&#26395;&#25552;&#20379;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24212;&#27880;&#24847;&#20854;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#31215;&#26497;&#37319;&#21462;&#31574;&#30053;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#21457;&#29983;&#29575;&#27491;&#22312;&#19978;&#21319;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#29616;&#26377;&#30340;&#24515;&#29702;&#20445;&#20581;&#27169;&#24335;&#26080;&#27861;&#20805;&#20998;&#25193;&#23637;&#20197;&#28385;&#36275;&#38656;&#27714;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#20855;&#26377;&#21019;&#36896;&#26032;&#39062;&#12289;&#22823;&#35268;&#27169;&#35299;&#20915;&#26041;&#26696;&#20197;&#25903;&#25345;&#24515;&#29702;&#20581;&#24247;&#30340;&#25215;&#35834;&#24863;&#21040;&#20048;&#35266;&#12290;&#23613;&#31649;&#23427;&#20204;&#36824;&#22788;&#20110;&#21021;&#26399;&#38454;&#27573;&#65292;LLMs&#24050;&#34987;&#24212;&#29992;&#20110;&#19982;&#24515;&#29702;&#20581;&#24247;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#24050;&#26377;&#25991;&#29486;&#20013;&#20851;&#20110;&#21033;&#29992;LLMs&#25552;&#20379;&#24515;&#29702;&#20581;&#24247;&#25945;&#32946;&#12289;&#35780;&#20272;&#21644;&#24178;&#39044;&#30340;&#21162;&#21147;&#65292;&#24182;&#31361;&#20986;&#20102;&#27599;&#20010;&#39046;&#22495;&#20013;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#30340;&#20851;&#38190;&#26426;&#20250;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23558;LLMs&#24212;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#25152;&#20276;&#38543;&#30340;&#39118;&#38505;&#65292;&#24182;&#40723;&#21169;&#37319;&#29992;&#31574;&#30053;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#23545;&#20110;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#36843;&#20999;&#38656;&#27714;&#24517;&#39035;&#19982;&#36127;&#36131;&#20219;&#30340;&#24515;&#29702;&#20581;&#24247;LLMs&#30340;&#24320;&#21457;&#12289;&#27979;&#35797;&#21644;&#37096;&#32626;&#30456;&#24179;&#34913;&#12290;&#29305;&#21035;&#20851;&#38190;&#30340;&#26159;&#30830;&#20445;&#24515;&#29702;&#20581;&#24247;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14814v1 Announce Type: cross  Abstract: Global rates of mental health concerns are rising and there is increasing realization that existing models of mental healthcare will not adequately expand to meet the demand. With the emergence of large language models (LLMs) has come great optimism regarding their promise to create novel, large-scale solutions to support mental health. Despite their nascence, LLMs have already been applied to mental health-related tasks. In this review, we summarize the extant literature on efforts to use LLMs to provide mental health education, assessment, and intervention and highlight key opportunities for positive impact in each area. We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks. The urgent need for mental health support must be balanced with responsible development, testing, and deployment of mental health LLMs. Especially critical is ensuring that mental he
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#25104;&#26412;&#26377;&#25928;&#30340;&#26041;&#27861;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#33521;&#35821;-&#38889;&#35821;-&#20013;&#25991;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24320;&#21457;&#20102;&#34920;&#29616;&#20248;&#36234;&#30340;&#21452;&#35821;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11399</link><description>&lt;p&gt;
X-LLaVA: &#20248;&#21270;&#21452;&#35821;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11399
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#25104;&#26412;&#26377;&#25928;&#30340;&#26041;&#27861;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#33521;&#35821;-&#38889;&#35821;-&#20013;&#25991;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#24320;&#21457;&#20102;&#34920;&#29616;&#20248;&#36234;&#30340;&#21452;&#35821;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26174;&#33879;&#21457;&#23637;&#27491;&#22312;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#30340;&#39046;&#22495;&#65292;&#36825;&#20123;&#27169;&#22411;&#38598;&#25104;&#20102;&#38500;&#25991;&#26412;&#20197;&#22806;&#30340;&#22810;&#31181;&#25968;&#25454;&#31867;&#22411;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#29305;&#24615;&#23548;&#33268;&#22312;&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#26041;&#38754;&#23384;&#22312;&#26174;&#30528;&#30340;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#20026;LMMs&#26500;&#24314;&#22810;&#35821;&#35328;&#25968;&#25454;&#20063;&#38754;&#20020;&#30528;&#35821;&#35328;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25104;&#26412;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#22810;&#35821;&#35328;LLM&#30340;&#35789;&#27719;&#25193;&#23637;&#21644;&#39044;&#35757;&#32451;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20351;&#29992;GPT4-V&#33258;&#21160;&#21644;&#31934;&#24515;&#26500;&#24314;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#22522;&#20110;&#36825;&#20123;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;91K&#33521;&#25991;-&#38889;&#25991;-&#20013;&#25991;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21452;&#35821;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#22312;&#38889;&#35821;&#21644;&#33521;&#35821;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11399v1 Announce Type: new  Abstract: The impressive development of large language models (LLMs) is expanding into the realm of large multimodal models (LMMs), which incorporate multiple types of data beyond text. However, the nature of multimodal models leads to significant expenses in the creation of training data. Furthermore, constructing multilingual data for LMMs presents its own set of challenges due to language diversity and complexity. Therefore, in this study, we propose two cost-effective methods to solve this problem: (1) vocabulary expansion and pretraining of multilingual LLM for specific languages, and (2) automatic and elaborate construction of multimodal datasets using GPT4-V. Based on015 these methods, we constructed a 91K English-Korean-Chinese multilingual, multimodal training dataset. Additionally, we developed a bilingual multimodal model that exhibits excellent performance in both Korean and English, surpassing existing approaches.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#21160;&#21160;&#24577;&#35780;&#20272;&#65288;AutoDE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#30340;API&#35843;&#29992;&#33021;&#21147;&#65292;&#36991;&#20813;&#38745;&#24577;&#35780;&#20272;&#20013;&#23548;&#33268;&#30340;&#35823;&#23548;&#24615;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.11128</link><description>&lt;p&gt;
&#36229;&#36234;&#38745;&#24577;&#35780;&#20272;&#65306;&#19968;&#31181;&#21160;&#24577;&#26041;&#27861;&#26469;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#30340;API&#35843;&#29992;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Static Evaluation: A Dynamic Approach to Assessing AI Assistants' API Invocation Capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11128
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#21160;&#21160;&#24577;&#35780;&#20272;&#65288;AutoDE&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#30340;API&#35843;&#29992;&#33021;&#21147;&#65292;&#36991;&#20813;&#38745;&#24577;&#35780;&#20272;&#20013;&#23548;&#33268;&#30340;&#35823;&#23548;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#21033;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;API&#35843;&#29992;&#65292;&#24050;&#32463;&#26174;&#33879;&#25552;&#21319;&#12290;&#36825;&#31181;&#36827;&#27493;&#38656;&#35201;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#37319;&#29992;&#38745;&#24577;&#35780;&#20272;&#65292;&#21363;&#22522;&#20110;&#39044;&#23450;&#20041;&#30340;&#23545;&#35805;&#21382;&#21490;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#30340;API&#35843;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35780;&#20272;&#26041;&#27861;&#21487;&#33021;&#20250;&#20855;&#26377;&#35823;&#23548;&#24615;&#65292;&#22240;&#20026;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#20154;&#24037;&#26234;&#33021;&#21161;&#25163;&#21487;&#33021;&#26080;&#27861;&#26681;&#25454;&#20043;&#21069;&#30340;&#20154;&#31867;&#20114;&#21160;&#29983;&#25104;API&#35843;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#21160;&#24577;&#35780;&#20272;&#65288;AutoDE&#65289;&#65292;&#20197;&#35780;&#20272;&#21161;&#25163;&#30340;API&#35843;&#29992;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#20154;&#31867;&#21442;&#19982;&#20197;&#21450;&#32791;&#36153;&#22823;&#37327;&#36164;&#28304;&#30340;&#30452;&#25509;&#20154;&#26426;&#20132;&#20114;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#21162;&#21147;&#27169;&#25311;&#30495;&#23454;&#30340;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20154;&#31867;&#23545;&#35805;&#27169;&#24335;&#65292;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#29992;&#25143;&#20195;&#29702;&#65292;&#24182;&#37197;&#22791;&#29992;&#25143;&#33050;&#26412;&#20197;&#30830;&#20445;&#20154;&#26426;&#23545;&#40784;&#12290;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20986;AutoDE&#25581;&#31034;&#20102;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11128v1 Announce Type: new  Abstract: With the rise of Large Language Models (LLMs), AI assistants' ability to utilize tools, especially through API calls, has advanced notably. This progress has necessitated more accurate evaluation methods. Many existing studies adopt static evaluation, where they assess AI assistants' API call based on pre-defined dialogue histories. However, such evaluation method can be misleading, as an AI assistant might fail in generating API calls from preceding human interaction in real cases. Instead of the resource-intensive method of direct human-machine interactions, we propose Automated Dynamic Evaluation (AutoDE) to assess an assistant's API call capability without human involvement. In our framework, we endeavor to closely mirror genuine human conversation patterns in human-machine interactions, using a LLM-based user agent, equipped with a user script to ensure human alignment. Experimental results highlight that AutoDE uncovers errors over
&lt;/p&gt;</description></item><item><title>Sabi'a-2&#26159;&#19968;&#20195;&#26032;&#30340;&#33889;&#33796;&#29273;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#30340;Sabi'a-2 Medium&#27169;&#22411;&#22312;&#22810;&#20010;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#36229;&#36234;&#20102;GPT-4&#65292;&#19988;&#22312;&#22823;&#22810;&#25968;&#32771;&#35797;&#20013;&#36229;&#36807;&#20102;GPT-3.5&#65292;&#21516;&#26102;&#19987;&#19994;&#21270;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#21487;&#22312;&#26080;&#38656;&#22686;&#22823;&#27169;&#22411;&#23610;&#23544;&#30340;&#24773;&#20917;&#19979;&#20197;&#27604;GPT-4&#20415;&#23452;10&#20493;&#30340;&#20215;&#26684;&#25552;&#20379;&#12290;</title><link>https://arxiv.org/abs/2403.09887</link><description>&lt;p&gt;
Sabi\'a-2:&#19968;&#20195;&#26032;&#30340;&#33889;&#33796;&#29273;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sabi\'a-2: A New Generation of Portuguese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09887
&lt;/p&gt;
&lt;p&gt;
Sabi'a-2&#26159;&#19968;&#20195;&#26032;&#30340;&#33889;&#33796;&#29273;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#30340;Sabi'a-2 Medium&#27169;&#22411;&#22312;&#22810;&#20010;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#36229;&#36234;&#20102;GPT-4&#65292;&#19988;&#22312;&#22823;&#22810;&#25968;&#32771;&#35797;&#20013;&#36229;&#36807;&#20102;GPT-3.5&#65292;&#21516;&#26102;&#19987;&#19994;&#21270;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#21487;&#22312;&#26080;&#38656;&#22686;&#22823;&#27169;&#22411;&#23610;&#23544;&#30340;&#24773;&#20917;&#19979;&#20197;&#27604;GPT-4&#20415;&#23452;10&#20493;&#30340;&#20215;&#26684;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Sabi'a-2&#65292;&#36825;&#26159;&#19968;&#26063;&#22312;&#33889;&#33796;&#29273;&#25991;&#26412;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#20010;&#32771;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#24052;&#35199;&#22823;&#23398;&#30340;&#20837;&#23398;&#32771;&#35797;&#12289;&#19987;&#19994;&#35748;&#35777;&#32771;&#35797;&#20197;&#21450;&#21508;&#31181;&#23398;&#31185;&#65288;&#22914;&#20250;&#35745;&#12289;&#32463;&#27982;&#23398;&#12289;&#24037;&#31243;&#23398;&#12289;&#27861;&#24459;&#21644;&#21307;&#23398;&#65289;&#30340;&#30740;&#31350;&#29983;&#20837;&#23398;&#32771;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#25105;&#20204;&#26368;&#20248;&#31168;&#30340;&#27169;&#22411;Sabi'a-2 Medium&#65292;&#22312;64&#22330;&#32771;&#35797;&#20013;&#26377;23&#22330;&#19982;GPT-4&#30340;&#34920;&#29616;&#30456;&#21305;&#25932;&#25110;&#36229;&#36234;&#65292;&#24182;&#19988;&#22312;64&#22330;&#32771;&#35797;&#20013;&#26377;58&#22330;&#36229;&#36807;&#20102;GPT-3.5&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#19987;&#19994;&#21270;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#26080;&#38656;&#22686;&#22823;&#27169;&#22411;&#23610;&#23544;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#20379;Sabi'a-2 Medium&#65292;&#27599;&#20010;&#35760;&#21495;&#30340;&#20215;&#26684;&#27604;GPT-4&#20415;&#23452;10&#20493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#25968;&#23398;&#21644;&#32534;&#30721;&#26159;&#38656;&#35201;&#25913;&#36827;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09887v1 Announce Type: cross  Abstract: We introduce Sabi\'a-2, a family of large language models trained on Portuguese texts. The models are evaluated on a diverse range of exams, including entry-level tests for Brazilian universities, professional certification exams, and graduate-level exams for various disciplines such as accounting, economics, engineering, law and medicine. Our results reveal that our best model so far, Sabi\'a-2 Medium, matches or surpasses GPT-4's performance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64 exams. Notably, specialization has a significant impact on a model's performance without the need to increase its size, allowing us to offer Sabi\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4. Finally, we identified that math and coding are key abilities that need improvement.
&lt;/p&gt;</description></item><item><title>ProSwitch&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#22312;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#20043;&#38388;&#29983;&#25104;&#25991;&#26412;&#65292;&#24182;&#22312;&#19987;&#19994;&#24615;&#35780;&#20272;&#21644;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09131</link><description>&lt;p&gt;
ProSwitch&#65306;&#30693;&#35782;&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#65292;&#29983;&#25104;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate Professional and Non-Professional Styled Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09131
&lt;/p&gt;
&lt;p&gt;
ProSwitch&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#22312;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#39118;&#26684;&#20043;&#38388;&#29983;&#25104;&#25991;&#26412;&#65292;&#24182;&#22312;&#19987;&#19994;&#24615;&#35780;&#20272;&#21644;&#36136;&#37327;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#35821;&#35328;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#25991;&#26412;&#25688;&#35201;&#21644;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#20204;&#36890;&#36807;&#24494;&#35843;&#22312;&#19981;&#21516;&#39118;&#26684;&#38388;&#20999;&#25442;&#30340;&#33021;&#21147;&#30340;&#30740;&#31350;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#25991;&#26412;&#19987;&#19994;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;ProSwitch&#65292;&#36890;&#36807;&#30693;&#35782;&#24341;&#23548;&#30340;&#25351;&#20196;&#24494;&#35843;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#29983;&#25104;&#19987;&#19994;&#21644;&#38750;&#19987;&#19994;&#22238;&#22797;&#30340;&#33021;&#21147;&#12290;ProSwitch&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;&#25968;&#25454;&#20934;&#22791;&#65292;&#29992;&#20110;&#25910;&#38598;&#39046;&#22495;&#30693;&#35782;&#21644;&#35757;&#32451;&#35821;&#26009;&#24211;&#65307;&#25351;&#20196;&#24494;&#35843;&#65292;&#29992;&#20110;&#20248;&#21270;&#24102;&#26377;&#22810;&#31181;&#25351;&#20196;&#26684;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#65307;&#20840;&#38754;&#35780;&#20272;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#19987;&#19994;&#24615;&#21306;&#20998;&#33021;&#21147;&#21644;&#22522;&#20110;&#21442;&#32771;&#30340;&#36136;&#37327;&#12290; ProSwitch&#30456;&#23545;&#20110;&#36890;&#29992;&#21644;&#19987;&#38376;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#20998;&#26512;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09131v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated efficacy in various linguistic applications, including text summarization and controlled text generation. However, studies into their capacity of switching between styles via fine-tuning remain underexplored. This study concentrates on textual professionalism and introduces a novel methodology, named ProSwitch, which equips a language model with the ability to produce both professional and non-professional responses through knowledge-guided instruction tuning. ProSwitch unfolds across three phases: data preparation for gathering domain knowledge and training corpus; instruction tuning for optimizing language models with multiple levels of instruction formats; and comprehensive evaluation for assessing the professionalism discrimination and reference-based quality of generated text. Comparative analysis of ProSwitch against both general and specialized language models reveals that our appro
&lt;/p&gt;</description></item><item><title>NLPre&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#19988;&#20844;&#24179;&#30340;&#35821;&#35328;&#20013;&#24515;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#20351;&#24471;&#21487;&#20197;&#20840;&#38754;&#25345;&#32493;&#35780;&#20272;&#22810;&#20010;NLPre&#24037;&#20855;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#38752;&#22320;&#36319;&#36394;&#20854;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.04507</link><description>&lt;p&gt;
NLPre: &#19968;&#31181;&#38754;&#21521;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#35821;&#35328;&#20013;&#24515;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
NLPre: a revised approach towards language-centric benchmarking of Natural Language Preprocessing systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04507
&lt;/p&gt;
&lt;p&gt;
NLPre&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#38752;&#19988;&#20844;&#24179;&#30340;&#35821;&#35328;&#20013;&#24515;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#20351;&#24471;&#21487;&#20197;&#20840;&#38754;&#25345;&#32493;&#35780;&#20272;&#22810;&#20010;NLPre&#24037;&#20855;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#38752;&#22320;&#36319;&#36394;&#20854;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#26550;&#26500;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLPre&#65289;&#24037;&#20855;&#30340;&#23835;&#36215;&#65292;&#36825;&#20123;&#24037;&#20855;&#33021;&#22815;&#35299;&#20915;&#21021;&#27493;&#30340;NLP&#20219;&#21153;&#65288;&#20363;&#22914;&#26631;&#35760;&#21270;&#12289;&#35789;&#24615;&#26631;&#27880;&#12289;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#25110;&#24418;&#24577;&#20998;&#26512;&#65289;&#32780;&#26080;&#38656;&#20219;&#20309;&#22806;&#37096;&#35821;&#35328;&#25351;&#23548;&#12290;&#22312;&#36153;&#26102;&#36153;&#21147;&#22320;&#27604;&#36739;&#26032;&#22411;&#35299;&#20915;&#26041;&#26696;&#19982;&#20381;&#36182;&#22522;&#20110;&#35268;&#21017;&#30340;&#24418;&#24577;&#20998;&#26512;&#22120;&#25110;&#35789;&#20856;&#30340;&#25104;&#29087;&#39044;&#22788;&#29702;&#24037;&#20855;&#21253;&#30340;&#22522;&#30784;&#19978;&#26159;&#24456;&#22256;&#38590;&#30340;&#12290;&#37492;&#20110;&#29616;&#26377;NLPre&#35780;&#20272;&#26041;&#27861;&#30340;&#32570;&#38519;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#21487;&#38752;&#21644;&#20844;&#24179;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#24615;&#33021;&#25253;&#21578;&#12290;&#21463;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#30340;&#35821;&#35328;&#20013;&#24515;&#22522;&#20934;&#27979;&#35797;&#31995;&#32479;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;NLPre&#24037;&#20855;&#30340;&#20840;&#38754;&#25345;&#32493;&#35780;&#20272;&#65292;&#21516;&#26102;&#21487;&#38752;&#22320;&#36319;&#36394;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#21407;&#22411;&#24212;&#29992;&#31243;&#24207;&#34987;&#37197;&#32622;&#20026;&#27874;&#20848;&#35821;&#65292;&#24182;&#19982;&#31934;&#24515;&#32452;&#32455;&#30340;NLPre-PL&#22522;&#20934;&#22871;&#20214;&#38598;&#25104;&#12290;&#22522;&#20110;&#36825;&#19968;&#22522;&#20934;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23454;&#39564;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04507v1 Announce Type: new  Abstract: With the advancements of transformer-based architectures, we observe the rise of natural language preprocessing (NLPre) tools capable of solving preliminary NLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or morphological analysis) without any external linguistic guidance. It is arduous to compare novel solutions to well-entrenched preprocessing toolkits, relying on rule-based morphological analysers or dictionaries. Aware of the shortcomings of existing NLPre evaluation approaches, we investigate a novel method of reliable and fair evaluation and performance reporting. Inspired by the GLUE benchmark, the proposed language-centric benchmarking system enables comprehensive ongoing evaluation of multiple NLPre tools, while credibly tracking their performance. The prototype application is configured for Polish and integrated with the thoroughly assembled NLPre-PL benchmark. Based on this benchmark, we conduct an ex
&lt;/p&gt;</description></item><item><title>NaturalSpeech 3&#21033;&#29992;&#20998;&#35299;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;</title><link>https://arxiv.org/abs/2403.03100</link><description>&lt;p&gt;
NaturalSpeech 3: &#21033;&#29992;&#20998;&#35299;&#32534;&#35299;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03100
&lt;/p&gt;
&lt;p&gt;
NaturalSpeech 3&#21033;&#29992;&#20998;&#35299;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#22312;&#35821;&#38899;&#36136;&#37327;&#12289;&#30456;&#20284;&#24230;&#21644;&#38901;&#24459;&#26041;&#38754;&#20173;&#23384;&#22312;&#19981;&#36275;&#12290;&#37492;&#20110;&#35821;&#38899;&#22797;&#26434;&#22320;&#21253;&#21547;&#21508;&#31181;&#23646;&#24615;&#65288;&#20363;&#22914;&#20869;&#23481;&#12289;&#38901;&#24459;&#12289;&#38899;&#33394;&#21644;&#22768;&#23398;&#32454;&#33410;&#65289;&#65292;&#32473;&#29983;&#25104;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#24819;&#27861;&#26159;&#23558;&#35821;&#38899;&#22240;&#23376;&#20998;&#35299;&#20026;&#20195;&#34920;&#19981;&#21516;&#23646;&#24615;&#30340;&#21508;&#20010;&#23376;&#31354;&#38388;&#65292;&#24182;&#21333;&#29420;&#29983;&#25104;&#23427;&#20204;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NaturalSpeech 3&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#26032;&#39062;&#30340;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;TTS&#31995;&#32479;&#65292;&#21487;&#20197;&#20197;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;1) &#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;&#20998;&#35299;&#21521;&#37327;&#37327;&#21270;&#65288;FVQ&#65289;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#65292;&#23558;&#35821;&#38899;&#27874;&#24418;&#20998;&#35299;&#20026;&#20869;&#23481;&#12289;&#38901;&#24459;&#12289;&#38899;&#33394;&#21644;&#22768;&#23398;&#32454;&#33410;&#30340;&#23376;&#31354;&#38388;&#65307;2) &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#65292;&#26681;&#25454;&#20854;&#30456;&#24212;&#30340;&#25552;&#31034;&#29983;&#25104;&#27599;&#20010;&#23376;&#31354;&#38388;&#20013;&#30340;&#23646;&#24615;&#12290;&#20511;&#21161;&#36825;&#31181;&#20998;&#35299;&#35774;&#35745;&#65292;NaturalSpeech 3&#33021;&#22815;ef
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03100v1 Announce Type: cross  Abstract: While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can ef
&lt;/p&gt;</description></item><item><title>SoftTiger&#26159;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#35774;&#35745;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22788;&#29702;&#20020;&#24202;&#31508;&#35760;&#30340;&#32467;&#26500;&#21270;&#65292;&#23454;&#29616;&#20102;&#22522;&#26412;&#20020;&#24202;&#20219;&#21153;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#19979;&#28216;&#20020;&#24202;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.00868</link><description>&lt;p&gt;
SoftTiger: &#29992;&#20110;&#21307;&#30103;&#24037;&#20316;&#27969;&#30340;&#20020;&#24202;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SoftTiger: A Clinical Foundation Model for Healthcare Workflows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00868
&lt;/p&gt;
&lt;p&gt;
SoftTiger&#26159;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#24037;&#20316;&#27969;&#35774;&#35745;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22788;&#29702;&#20020;&#24202;&#31508;&#35760;&#30340;&#32467;&#26500;&#21270;&#65292;&#23454;&#29616;&#20102;&#22522;&#26412;&#20020;&#24202;&#20219;&#21153;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#19979;&#28216;&#20020;&#24202;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#24067;&#24182;&#20171;&#32461;&#20102;SoftTiger&#65292;&#19968;&#20010;&#19987;&#20026;&#21307;&#30103;&#20445;&#20581;&#24037;&#20316;&#27969;&#35774;&#35745;&#30340;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CLaM&#65289;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#12290;&#20020;&#24202;&#31508;&#35760;&#30340;&#21465;&#36848;&#24615;&#21644;&#38750;&#32467;&#26500;&#21270;&#29305;&#24615;&#26159;&#21307;&#30103;&#26234;&#33021;&#21270;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#25353;&#29031;&#22269;&#38469;&#20114;&#25805;&#20316;&#24615;&#26631;&#20934;&#23558;&#20020;&#24202;&#31508;&#35760;&#32467;&#26500;&#21270;&#20026;&#20020;&#24202;&#25968;&#25454;&#65292;&#28041;&#21450;&#22269;&#38469;&#24739;&#32773;&#25688;&#35201;&#12289;&#20020;&#24202;&#21360;&#35937;&#21644;&#21307;&#30103;&#25509;&#35302;&#19977;&#20010;&#20851;&#38190;&#23376;&#20219;&#21153;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#27880;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20844;&#24320;&#21644;&#39564;&#35777;&#30340;&#20020;&#24202;&#25968;&#25454;&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#12290;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#30446;&#26631;&#27169;&#22411;&#39318;&#20808;&#33021;&#22815;&#25903;&#25345;&#22522;&#26412;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#22914;&#32553;&#20889;&#25193;&#23637;&#21644;&#26102;&#38388;&#20449;&#24687;&#25552;&#21462;&#65292;&#28982;&#21518;&#23398;&#20064;&#25191;&#34892;&#26356;&#22797;&#26434;&#30340;&#19979;&#28216;&#20020;&#24202;&#20219;&#21153;&#65292;&#22914;&#21360;&#35937;&#21644;&#25509;&#35302;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#21307;&#30103;&#27169;&#22411;&#20013;&#30340;&#19968;&#20123;&#24314;&#27169;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00868v1 Announce Type: cross  Abstract: We release and introduce SoftTiger, a clinical large language model (CLaM) designed as a foundation model for healthcare workflows. The narrative and unstructured nature of clinical notes is a major obstacle for healthcare intelligentization. We address a critical problem of structuring clinical notes into clinical data, according to international interoperability standards. We collect and annotate data for three critical subtasks, namely, international patient summary, clinical impression and medical encounter. We then supervised fine-tuned a state-of-the-art LLM using public and credentialed clinical data. The training is orchestrated in a way that the target model can first support basic clinical tasks such as abbreviation expansion and temporal information extraction, and then learn to perform more complex downstream clinical tasks such as impression and encounter summary. Moreover, we address, several modeling challenges in the he
&lt;/p&gt;</description></item><item><title>Agent-Pro&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#31574;&#30053;&#32423;&#21035;&#30340;&#21453;&#24605;&#21644;&#20248;&#21270;&#65292;&#21487;&#20197;&#20174;&#20114;&#21160;&#32463;&#39564;&#20013;&#23398;&#20064;&#24182;&#36880;&#27493;&#25552;&#21319;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.17574</link><description>&lt;p&gt;
Agent-Pro: &#36890;&#36807;&#31574;&#30053;&#32423;&#21035;&#21453;&#24605;&#21644;&#20248;&#21270;&#23398;&#20064;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17574
&lt;/p&gt;
&lt;p&gt;
Agent-Pro&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#31574;&#30053;&#32423;&#21035;&#30340;&#21453;&#24605;&#21644;&#20248;&#21270;&#65292;&#21487;&#20197;&#20174;&#20114;&#21160;&#32463;&#39564;&#20013;&#23398;&#20064;&#24182;&#36880;&#27493;&#25552;&#21319;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#37117;&#26159;&#29305;&#23450;&#20219;&#21153;&#27714;&#35299;&#22120;&#65292;&#24182;&#20855;&#26377;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#32780;&#19981;&#26159;&#33021;&#22815;&#36890;&#36807;&#20114;&#21160;&#23398;&#20064;&#21644;&#36827;&#21270;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Agent-Pro&#65306;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#20855;&#26377;&#31574;&#30053;&#32423;&#21035;&#30340;&#21453;&#24605;&#21644;&#20248;&#21270;&#65292;&#21487;&#20197;&#20174;&#20114;&#21160;&#32463;&#39564;&#20013;&#23398;&#20064;&#20016;&#23500;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#36880;&#28176;&#25552;&#21319;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#28041;&#21450;&#19968;&#20010;&#21160;&#24577;&#20449;&#24565;&#29983;&#25104;&#21644;&#21453;&#24605;&#36807;&#31243;&#65292;&#29992;&#20110;&#31574;&#30053;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17574v1 Announce Type: new  Abstract: Large Language Models exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#33021;&#22815;&#26356;&#22909;&#22320;&#32534;&#30721;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#39030;&#23618;&#21487;&#33021;&#36807;&#22810;&#20851;&#27880;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#29702;&#35299;&#20840;&#23616;&#20449;&#24687;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.17304</link><description>&lt;p&gt;
&#25506;&#31350;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20840;&#23616;&#21644;&#23616;&#37096;&#35821;&#20041;&#34920;&#31034;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Probing Multimodal Large Language Models for Global and Local Semantic Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17304
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#33021;&#22815;&#26356;&#22909;&#22320;&#32534;&#30721;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#39030;&#23618;&#21487;&#33021;&#36807;&#22810;&#20851;&#27880;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#29702;&#35299;&#20840;&#23616;&#20449;&#24687;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#21551;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#23558;&#20854;&#20248;&#31168;&#30340;&#34920;&#31034;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#27169;&#24577;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#21033;&#29992;&#22270;&#20687;&#25551;&#36848;&#23545;&#40784;&#25968;&#25454;&#38598;&#35757;&#32451;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#65292;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;MLLMs&#26159;&#21542;&#30495;&#27491;&#29702;&#35299;&#23436;&#25972;&#30340;&#22270;&#20687;&#20449;&#24687;&#65292;&#21363;&#20840;&#23616;&#20449;&#24687;&#65292;&#25110;&#32773;&#23427;&#20204;&#21482;&#33021;&#25429;&#25417;&#19968;&#20123;&#23616;&#37096;&#23545;&#35937;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#21487;&#20197;&#32534;&#30721;&#26356;&#22810;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#20854;&#34920;&#31034;&#21521;&#37327;&#22312;&#35270;&#35273;-&#35821;&#35328;&#34164;&#28085;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#19981;&#26159;&#39030;&#23618;&#12290;&#25105;&#20204;&#36890;&#36807;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#36827;&#19968;&#27493;&#25506;&#31350;&#27169;&#22411;&#30340;&#23616;&#37096;&#35821;&#20041;&#34920;&#31034;&#12290;&#25105;&#20204;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#39030;&#23618;&#21487;&#33021;&#36807;&#22810;&#19987;&#27880;&#20110;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#20943;&#24369;&#20102;&#23545;&#20840;&#23616;&#20449;&#24687;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17304v1 Announce Type: cross  Abstract: The success of large language models has inspired researchers to transfer their exceptional representing ability to other modalities. Several recent works leverage image-caption alignment datasets to train multimodal large language models (MLLMs), which achieve state-of-the-art performance on image-to-text tasks. However, there are very few studies exploring whether MLLMs truly understand the complete image information, i.e., global information, or if they can only capture some local object information. In this study, we find that the intermediate layers of models can encode more global semantic information, whose representation vectors perform better on visual-language entailment tasks, rather than the topmost layers. We further probe models for local semantic representation through object detection tasks. And we draw a conclusion that the topmost layers may excessively focus on local information, leading to a diminished ability to en
&lt;/p&gt;</description></item><item><title>PEP&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#26469;&#25913;&#21892;LLMs&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#20043;&#21069;&#32454;&#21270;&#21644;&#38416;&#26126;&#38382;&#39064;&#32972;&#26223;&#65292;&#25552;&#21319;&#20840;&#23616;&#19978;&#19979;&#25991;&#24314;&#27169;&#33021;&#21147;&#65292;&#20943;&#23569;&#35299;&#26512;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.15764</link><description>&lt;p&gt;
&#22312;&#36339;&#27133;&#20043;&#21069;&#19977;&#24605;&#65306;&#38382;&#39064;&#32454;&#21270;&#25552;&#31034;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15764
&lt;/p&gt;
&lt;p&gt;
PEP&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26041;&#27861;&#26469;&#25913;&#21892;LLMs&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#20043;&#21069;&#32454;&#21270;&#21644;&#38416;&#26126;&#38382;&#39064;&#32972;&#26223;&#65292;&#25552;&#21319;&#20840;&#23616;&#19978;&#19979;&#25991;&#24314;&#27169;&#33021;&#21147;&#65292;&#20943;&#23569;&#35299;&#26512;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#20173;&#38754;&#20020;&#25361;&#25112;&#65292;&#24182;&#19988;&#23545;&#36755;&#20837;&#19978;&#19979;&#25991;&#25935;&#24863;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#38382;&#39064;&#32454;&#21270;&#25552;&#31034;&#65288;PEP&#65289;&#65292;&#26088;&#22312;&#22312;&#25512;&#29702;&#20043;&#21069;&#20998;&#35299;&#21644;&#38416;&#26126;&#38382;&#39064;&#32972;&#26223;&#65292;&#20174;&#32780;&#22686;&#24378;&#20840;&#23616;&#19978;&#19979;&#25991;&#24314;&#27169;&#21644;&#20943;&#23569;&#35299;&#26512;&#22256;&#38590;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PEP&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23545;&#20110;&#38382;&#39064;&#25552;&#20986;&#30340;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15764v1 Announce Type: cross  Abstract: Large language models~(LLMs) have exhibited impressive performance across NLP tasks. So far they still face challenges in complex reasoning tasks and can be sensitive to input context. Despite significant efforts have been invested in enhancing reasoning process and improving prefix-prompts robustness, the crucial role of problem context has been overlooked. In this study, we propose a new approach to improve the mathematical capacities of LLMs, named Problem Elaboration Prompting~(PEP). Specifically, PEP decomposes and elucidates the problem context before reasoning, thus enhancing the global context modeling and reducing the parsing difficulties. Experiments on datasets demonstrate promising performances on complex reasoning and indicate the beneficial impact for ill-formed problems. For instance, with the GPT-3.5 model~(\texttt{text-davinci-003}), we observed a 9.93\% improvement with greedy decoding and 8.80\% improvement with self
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#32467;&#26500;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#24341;&#23548;&#30340;SQL&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;SQL&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#25191;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13284</link><description>&lt;p&gt;
&#32467;&#26500;&#24341;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;SQL&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Structure Guided Large Language Model for SQL Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13284
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#32467;&#26500;&#20449;&#24687;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#24341;&#23548;&#30340;SQL&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;SQL&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#25191;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20934;&#30830;&#30340;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#23558;&#29992;&#25143;&#30340;&#35821;&#20041;&#26597;&#35810;&#19982;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#21305;&#37197;&#65292;&#28982;&#21518;&#29983;&#25104;&#32467;&#26500;&#21270;SQL&#26041;&#38754;&#12290;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#23558;&#26597;&#35810;&#21644;&#25968;&#25454;&#24211;&#27169;&#24335;&#36755;&#20837;&#21040;LLM&#20013;&#65292;&#24182;&#20381;&#36182;LLM&#25191;&#34892;&#35821;&#20041;-&#32467;&#26500;&#21305;&#37197;&#24182;&#29983;&#25104;&#32467;&#26500;&#21270;SQL&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#24573;&#30053;&#20102;&#29992;&#25143;&#26597;&#35810;&#21644;&#25968;&#25454;&#24211;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#32780;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#29992;&#26469;&#22686;&#24378;&#32467;&#26500;&#21270;SQL&#30340;&#29983;&#25104;&#12290;&#36825;&#19968;&#30095;&#24573;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#25110;&#26080;&#27861;&#25191;&#34892;&#30340;SQL&#29983;&#25104;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21040;SQL&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22266;&#26377;&#30340;&#32467;&#26500;&#20449;&#24687;&#26469;&#25913;&#21892;LLM&#30340;SQL&#29983;&#25104;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#32467;&#26500;&#24341;&#23548;SQL&#65288;SGU-SQL&#65289;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13284v1 Announce Type: cross  Abstract: Generating accurate Structured Querying Language (SQL) is a long-standing problem, especially in matching users' semantic queries with structured databases and then generating structured SQL. Existing models typically input queries and database schemas into the LLM and rely on the LLM to perform semantic-structure matching and generate structured SQL. However, such solutions overlook the structural information within user queries and databases, which can be utilized to enhance the generation of structured SQL. This oversight can lead to inaccurate or unexecutable SQL generation. To fully exploit the structure, we propose a structure-to-SQL framework, which leverages the inherent structure information to improve the SQL generation of LLMs. Specifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model. SGU-SQL first links user queries and databases in a structure-enhanced manner. It then decomposes complicated linked str
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20877;&#25490;&#24207;&#38454;&#27573;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#36798;&#21040;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#20195;&#30721;&#20197;&#20419;&#36827;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12997</link><description>&lt;p&gt;
&#26397;&#30528;&#21487;&#20449;&#30340;&#20877;&#25490;&#24207;&#65306;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#24323;&#26435;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12997
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#20877;&#25490;&#24207;&#38454;&#27573;&#65292;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#36798;&#21040;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24320;&#28304;&#20195;&#30721;&#20197;&#20419;&#36827;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#65288;NIR&#65289;&#24050;&#32463;&#26174;&#33879;&#25913;&#36827;&#20102;&#22522;&#20110;&#21551;&#21457;&#24335;&#30340;IR&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22833;&#36133;&#20173;&#28982;&#39057;&#32321;&#21457;&#29983;&#65292;&#36890;&#24120;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#26080;&#27861;&#26816;&#32034;&#19982;&#29992;&#25143;&#26597;&#35810;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#23454;&#32422;&#26463;&#30340;&#36731;&#37327;&#32423;&#24323;&#26435;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29305;&#21035;&#24378;&#35843;&#20877;&#25490;&#24207;&#38454;&#27573;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21327;&#35758;&#65292;&#29992;&#20110;&#22312;&#40657;&#21283;&#23376;&#22330;&#26223;&#20013;&#35780;&#20272;&#24323;&#26435;&#31574;&#30053;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25968;&#25454;&#39537;&#21160;&#26426;&#21046;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#39564;&#22797;&#21046;&#21644;&#24323;&#26435;&#23454;&#26045;&#30340;&#24320;&#28304;&#20195;&#30721;&#65292;&#20419;&#36827;&#20854;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#26356;&#24191;&#27867;&#30340;&#37319;&#29992;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12997v1 Announce Type: cross  Abstract: Neural Information Retrieval (NIR) has significantly improved upon heuristic-based IR systems. Yet, failures remain frequent, the models used often being unable to retrieve documents relevant to the user's query. We address this challenge by proposing a lightweight abstention mechanism tailored for real-world constraints, with particular emphasis placed on the reranking phase. We introduce a protocol for evaluating abstention strategies in a black-box scenario, demonstrating their efficacy, and propose a simple yet effective data-driven mechanism. We provide open-source code for experiment replication and abstention implementation, fostering wider adoption and application in diverse contexts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-4&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21453;&#39304;&#23545;&#30456;&#23545;&#32622;&#20449;&#24230;&#26377;&#24433;&#21709;&#65292;&#20294;&#24182;&#19981;&#19968;&#33268;&#22320;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;</title><link>https://arxiv.org/abs/2402.09654</link><description>&lt;p&gt;
GPT-4&#22312;&#22522;&#20110;USMLE&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GPT-4's assessment of its performance in a USMLE-based case study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-4&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21453;&#39304;&#23545;&#30456;&#23545;&#32622;&#20449;&#24230;&#26377;&#24433;&#21709;&#65292;&#20294;&#24182;&#19981;&#19968;&#33268;&#22320;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;GPT-4&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20174;&#32654;&#22269;&#21307;&#23398;&#25191;&#29031;&#32771;&#35797;&#65288;USMLE&#65289;&#38382;&#21367;&#20013;&#25552;&#21462;&#38382;&#39064;&#30340;&#26041;&#24335;&#65292;&#20219;&#21153;&#26159;&#35780;&#20272;&#27169;&#22411;&#22312;&#25552;&#38382;&#20043;&#21069;&#21644;&#25552;&#38382;&#20043;&#21518;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#12290;&#38382;&#21367;&#26681;&#25454;&#26159;&#21542;&#26377;&#21453;&#39304;&#20998;&#20026;&#20004;&#32452;&#65306;&#21453;&#39304;&#32452;&#65288;WF&#65289;&#21644;&#26080;&#21453;&#39304;&#32452;&#65288;NF&#65289;&#12290;&#35201;&#27714;&#27169;&#22411;&#22312;&#27599;&#20010;&#38382;&#39064;&#20043;&#21069;&#21644;&#20043;&#21518;&#25552;&#20379;&#32477;&#23545;&#21644;&#30456;&#23545;&#32622;&#20449;&#24230;&#24471;&#20998;&#12290;&#36890;&#36807;&#20351;&#29992;&#32479;&#35745;&#24037;&#20855;&#20998;&#26512;&#23454;&#39564;&#32467;&#26524;&#65292;&#30740;&#31350;&#20102;WF&#21644;NF&#32452;&#30340;&#32622;&#20449;&#24230;&#21464;&#24322;&#24615;&#12290;&#27492;&#22806;&#65292;&#36827;&#34892;&#20102;&#39034;&#24207;&#20998;&#26512;&#20197;&#35266;&#23519;WF&#21644;NF&#32452;&#30340;&#24615;&#33021;&#21464;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21453;&#39304;&#20250;&#24433;&#21709;&#30456;&#23545;&#32622;&#20449;&#24230;&#65292;&#20294;&#24182;&#19981;&#24635;&#26159;&#22686;&#21152;&#25110;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09654v1 Announce Type: new  Abstract: This study investigates GPT-4's assessment of its performance in healthcare applications. A simple prompting technique was used to prompt the LLM with questions taken from the United States Medical Licensing Examination (USMLE) questionnaire and it was tasked to evaluate its confidence score before posing the question and after asking the question. The questionnaire was categorized into two groups-questions with feedback (WF) and questions with no feedback(NF) post-question. The model was asked to provide absolute and relative confidence scores before and after each question. The experimental findings were analyzed using statistical tools to study the variability of confidence in WF and NF groups. Additionally, a sequential analysis was conducted to observe the performance variation for the WF and NF groups. Results indicate that feedback influences relative confidence but doesn't consistently increase or decrease it. Understanding the p
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35813;&#20027;&#39064;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.09283</link><description>&lt;p&gt;
&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09283
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35843;&#26597;&#25552;&#20379;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#26088;&#22312;&#25552;&#39640;&#23545;&#35813;&#20027;&#39064;&#30340;&#29702;&#35299;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09283v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340;&#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#24212;&#29992;&#20013;&#24050;&#32463;&#24456;&#24120;&#35265;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#22238;&#22797;&#30340;&#39118;&#38505;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#31038;&#20250;&#20851;&#20999;&#65292;&#24182;&#28608;&#21457;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#26368;&#26032;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#22312;&#27492;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26368;&#36817;&#30740;&#31350;&#30340;&#20840;&#38754;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#25688;&#35201;&#65292;&#22686;&#36827;&#23545;LLM&#23545;&#35805;&#23433;&#20840;&#24615;&#30340;&#29702;&#35299;&#65292;&#24182;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#19968;&#37325;&#35201;&#35838;&#39064;&#12290;&#20026;&#20102;&#26041;&#20415;&#21442;&#32771;&#65292;&#25105;&#20204;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#23545;&#25152;&#26377;&#22312;&#27492;&#35843;&#26597;&#20013;&#25552;&#21040;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;https://github.com/niconi19/LLM-conversation-safety&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09283v1 Announce Type: new Abstract: Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.
&lt;/p&gt;</description></item><item><title>OpenMoE&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#21644;&#21457;&#24067;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#22797;&#29616;&#24615;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;MoE&#27169;&#22411;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#25104;&#26412;&#25928;&#30410;&#24179;&#34913;&#65292;&#24182;&#19988;&#36827;&#34892;&#20102;&#23545;&#36335;&#30001;&#26426;&#21046;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.01739</link><description>&lt;p&gt;
OpenMoE&#65306;&#24320;&#28304;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#30340;&#26089;&#26399;&#21162;&#21147;
&lt;/p&gt;
&lt;p&gt;
OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01739
&lt;/p&gt;
&lt;p&gt;
OpenMoE&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#21644;&#21457;&#24067;&#19968;&#31995;&#21015;&#20855;&#26377;&#21487;&#22797;&#29616;&#24615;&#30340;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#25105;&#20204;&#30830;&#35748;&#20102;MoE&#27169;&#22411;&#30456;&#27604;&#23494;&#38598;&#27169;&#22411;&#20855;&#26377;&#26356;&#26377;&#21033;&#30340;&#25104;&#26412;&#25928;&#30410;&#24179;&#34913;&#65292;&#24182;&#19988;&#36827;&#34892;&#20102;&#23545;&#36335;&#30001;&#26426;&#21046;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24110;&#21161;&#24320;&#28304;&#31038;&#21306;&#26356;&#22909;&#22320;&#29702;&#35299;&#22522;&#20110;&#28151;&#21512;&#19987;&#23478;(MoE)&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25105;&#20204;&#35757;&#32451;&#24182;&#21457;&#24067;&#20102;OpenMoE&#65292;&#19968;&#31995;&#21015;&#23436;&#20840;&#24320;&#25918;&#28304;&#30721;&#21644;&#21487;&#22797;&#29616;&#30340;&#20165;&#35299;&#30721;&#22120;MoE LLM&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;650M&#21040;34B&#65292;&#35757;&#32451;&#25968;&#25454;&#36229;&#36807;1T&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#23454;&#65292;MoE-based LLM&#21487;&#20197;&#25552;&#20379;&#27604;&#23494;&#38598;LLM&#26356;&#26377;&#21033;&#30340;&#25104;&#26412;&#25928;&#30410;&#24179;&#34913;&#65292;&#31361;&#20986;&#20102;&#26410;&#26469;LLM&#24320;&#21457;&#30340;&#28508;&#22312;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#21478;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#23545;&#25105;&#20204;&#30340;OpenMoE&#27169;&#22411;&#20013;&#30340;&#36335;&#30001;&#26426;&#21046;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#24471;&#21040;&#20102;&#19977;&#20010;&#37325;&#35201;&#21457;&#29616;&#65306;&#19978;&#19979;&#25991;&#26080;&#20851;&#19987;&#19994;&#21270;&#12289;&#26089;&#26399;&#36335;&#30001;&#23398;&#20064;&#21644;&#26411;&#23614;&#38477;&#20302;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;MoE&#27169;&#22411;&#20013;&#30340;&#36335;&#30001;&#20915;&#31574;&#20027;&#35201;&#22522;&#20110;&#26631;&#35760;ID&#65292;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#24456;&#23567;&#12290;&#26631;&#35760;&#21040;&#19987;&#23478;&#30340;&#20998;&#37197;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26089;&#26399;&#30830;&#23450;&#65292;&#24182;&#19988;&#22522;&#26412;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#31181;&#19981;&#23436;&#20840;&#30340;&#36335;&#30001;&#21487;&#33021;&#23548;&#33268;...
&lt;/p&gt;
&lt;p&gt;
To help the open-source community have a better understanding of Mixture-of-Experts (MoE) based large language models (LLMs), we train and release OpenMoE, a series of fully open-sourced and reproducible decoder-only MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T tokens. Our investigation confirms that MoE-based LLMs can offer a more favorable cost-effectiveness trade-off than dense LLMs, highlighting the potential effectiveness for future LLM development.   One more important contribution of this study is an in-depth analysis of the routing mechanisms within our OpenMoE models, leading to three significant findings: Context-Independent Specialization, Early Routing Learning, and Drop-towards-the-End. We discovered that routing decisions in MoE models are predominantly based on token IDs, with minimal context relevance. The token-to-expert assignments are determined early in the pre-training phase and remain largely unchanged. This imperfect routing can resu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Fast LoRA&#65288;FLoRA&#65289;&#26694;&#26550;&#65292;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#20302;&#31209;&#35843;&#25972;&#21487;&#20197;&#39640;&#25928;&#25209;&#22788;&#29702;&#24322;&#26500;&#35831;&#27714;&#65292;&#24182;&#22312;&#32489;&#25928;&#19978;&#20445;&#25345;&#31454;&#20105;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.05677</link><description>&lt;p&gt;
&#22522;&#20110;&#25209;&#22788;&#29702;&#30340;&#22522;&#30784;&#27169;&#22411;&#20302;&#31209;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Batched Low-Rank Adaptation of Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05677
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Fast LoRA&#65288;FLoRA&#65289;&#26694;&#26550;&#65292;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#20302;&#31209;&#35843;&#25972;&#21487;&#20197;&#39640;&#25928;&#25209;&#22788;&#29702;&#24322;&#26500;&#35831;&#27714;&#65292;&#24182;&#22312;&#32489;&#25928;&#19978;&#20445;&#25345;&#31454;&#20105;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#22240;&#36890;&#36807;&#24341;&#20837;&#21487;&#35757;&#32451;&#30340;&#20302;&#31209;&#30697;&#38453;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#24182;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#32780;&#24341;&#36215;&#20851;&#27880;&#12290;&#34429;&#28982;LoRA&#25552;&#20379;&#20102;&#35768;&#22810;&#20248;&#28857;&#65292;&#20294;&#20854;&#22312;&#23454;&#26102;&#20026;&#21508;&#31181;&#20840;&#29699;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#26102;&#26080;&#27861;&#39640;&#25928;&#22788;&#29702;&#22810;&#20010;&#29305;&#23450;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;&#36825;&#20026;&#38656;&#35201;&#20026;&#27599;&#20010;&#20256;&#20837;&#35831;&#27714;&#20010;&#24615;&#21270;&#12289;&#29305;&#23450;&#20219;&#21153;&#36866;&#24212;&#30340;&#22330;&#26223;&#20013;&#36896;&#25104;&#20102;&#24615;&#33021;&#29942;&#39048;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#32422;&#26463;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24555;&#36895;LoRA&#65288;FLoRA&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#25209;&#22788;&#29702;&#20013;&#30340;&#27599;&#20010;&#36755;&#20837;&#31034;&#20363;&#37117;&#21487;&#20197;&#19982;&#20854;&#29420;&#29305;&#30340;&#20302;&#31209;&#36866;&#24212;&#26435;&#37325;&#30456;&#20851;&#32852;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#24322;&#26500;&#35831;&#27714;&#30340;&#39640;&#25928;&#25209;&#22788;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#34920;&#26126;&#65292;FLoRA&#20445;&#30041;&#20102;LoRA&#30340;&#32489;&#25928;&#20248;&#28857;&#65292;&#22312;&#36328;&#36234;8&#31181;&#35821;&#35328;&#30340;MultiPL-E&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#19978;&#23637;&#31034;&#20986;&#31454;&#20105;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05677v2 Announce Type: replace-cross  Abstract: Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While LoRA offers numerous advantages, its applicability for real-time serving to a diverse and global user base is constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request. To mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching of heterogeneous requests. We empirically demonstrate that FLoRA retains the performance merits of LoRA, showcasing competitive results on the MultiPL-E code generation benchmark spanning over 8 languages and 
&lt;/p&gt;</description></item><item><title>PEMA&#26159;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20214;&#22806;&#37096;&#20869;&#23384;&#33258;&#36866;&#24212;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#32469;&#36807;&#20102;&#23545;&#25152;&#26377;&#26435;&#37325;&#30340;&#35775;&#38382;&#38656;&#27714;&#65292;&#21516;&#26102;&#21033;&#29992;&#22806;&#37096;&#20869;&#23384;&#21644;&#36866;&#37197;&#22120;&#26435;&#37325;&#30697;&#38453;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.08590</link><description>&lt;p&gt;
PEMA&#65306;&#19968;&#31181;&#21487;&#22312;&#31163;&#32447;&#35843;&#25972;&#30340;&#22806;&#37096;&#25554;&#20214;&#20869;&#23384;&#33258;&#36866;&#24212;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08590
&lt;/p&gt;
&lt;p&gt;
PEMA&#26159;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#25554;&#20214;&#22806;&#37096;&#20869;&#23384;&#33258;&#36866;&#24212;&#23454;&#29616;&#20102;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#32469;&#36807;&#20102;&#23545;&#25152;&#26377;&#26435;&#37325;&#30340;&#35775;&#38382;&#38656;&#27714;&#65292;&#21516;&#26102;&#21033;&#29992;&#22806;&#37096;&#20869;&#23384;&#21644;&#36866;&#37197;&#22120;&#26435;&#37325;&#30697;&#38453;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#30001;&#20110;&#36164;&#28304;&#38656;&#27714;&#24040;&#22823;&#65292;&#35768;&#22810;PLM&#26435;&#37325;&#26159;&#26426;&#23494;&#30340;&#65292;&#29992;&#25143;&#34987;&#36843;&#23558;&#20854;&#25968;&#25454;&#19982;&#27169;&#22411;&#25152;&#26377;&#32773;&#20849;&#20139;&#65292;&#20197;&#20415;&#20026;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25554;&#20214;&#22806;&#37096;&#20869;&#23384;&#33258;&#36866;&#24212;&#65288;PEMA&#65289;&#65292;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;&#19981;&#38656;&#35201;&#35775;&#38382;&#25152;&#26377;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#23545;PLM&#36827;&#34892;&#24494;&#35843;&#12290;PEMA&#22312;&#25512;&#29702;&#26399;&#38388;&#38598;&#25104;&#20102;&#26469;&#33258;&#27979;&#35797;&#25968;&#25454;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#20197;&#25191;&#34892;&#19979;&#28216;&#20219;&#21153;&#12290;&#23427;&#20351;&#29992;&#22806;&#37096;&#20869;&#23384;&#23384;&#20648;&#30001;PLM&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#19982;&#30446;&#26631;&#26631;&#35760;&#30456;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;PLM&#26368;&#32456;&#23618;&#20013;&#31867;&#20284;LoRA&#30340;&#29942;&#39048;&#36866;&#37197;&#22120;&#30340;&#26435;&#37325;&#30697;&#38453;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08590v2 Announce Type: replace  Abstract: Pre-trained language models (PLMs) show impressive performance in various downstream NLP tasks. However, pre-training large language models demands substantial memory and training compute. Furthermore, due to the substantial resources required, many PLM weights are confidential. Consequently, users are compelled to share their data with model owners for fine-tuning specific tasks. To overcome the limitations, we introduce Plug-in External Memory Adaptation (PEMA), a Parameter-Efficient Fine-Tuning (PEFT) method, enabling PLM fine-tuning without requiring access to all the weights. PEMA integrates with context representations from test data during inference to perform downstream tasks. It uses external memory to store PLM-generated context representations mapped with target tokens. Our method utilizes weight matrices of LoRA-like bottlenecked adapter in the PLM's final layer to enhance efficiency. Our approach also includes Gradual Un
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#36234;&#29425;&#25552;&#31034;&#30340;&#33258;&#21160;&#26694;&#26550;ReNeLLM&#65292;&#26174;&#33879;&#25552;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2311.08268</link><description>&lt;p&gt;
&#20266;&#35013;&#25104;&#32650;&#30340;&#29436;&#65306;&#26222;&#36941;&#30340;&#23884;&#22871;&#36234;&#29425;&#25552;&#31034;&#21487;&#20197;&#36731;&#26494;&#24858;&#24324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08268
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#36234;&#29425;&#25552;&#31034;&#30340;&#33258;&#21160;&#26694;&#26550;ReNeLLM&#65292;&#26174;&#33879;&#25552;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#26088;&#22312;&#25552;&#20379;&#26377;&#29992;&#21644;&#23433;&#20840;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#34987;&#31216;&#20026;&#8220;&#36234;&#29425;&#8221;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#21487;&#20197;&#35268;&#36991;&#20445;&#38556;&#25514;&#26045;&#65292;&#23548;&#33268;LLMs&#29983;&#25104;&#28508;&#22312;&#26377;&#23475;&#20869;&#23481;&#12290;&#25506;&#32034;&#36234;&#29425;&#25552;&#31034;&#21487;&#20197;&#24110;&#21161;&#26356;&#22909;&#22320;&#25581;&#31034;LLMs&#30340;&#24369;&#28857;&#65292;&#24182;&#36827;&#19968;&#27493;&#24341;&#23548;&#25105;&#20204;&#23433;&#20840;&#22320;&#20445;&#25252;&#23427;&#20204;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#36234;&#29425;&#26041;&#27861;&#35201;&#20040;&#36973;&#21463;&#22797;&#26434;&#30340;&#25163;&#24037;&#35774;&#35745;&#65292;&#35201;&#20040;&#38656;&#35201;&#22312;&#20854;&#20182;&#30333;&#30418;&#27169;&#22411;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#27867;&#21270;&#24615;&#25110;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36234;&#29425;&#25552;&#31034;&#25915;&#20987;&#27010;&#25324;&#20026;&#20004;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#25552;&#31034;&#37325;&#20889;&#21644;&#65288;2&#65289;&#22330;&#26223;&#23884;&#22871;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReNeLLM&#65292;&#19968;&#20010;&#21033;&#29992;LLMs&#33258;&#36523;&#29983;&#25104;&#26377;&#25928;&#36234;&#29425;&#25552;&#31034;&#30340;&#33258;&#21160;&#26694;&#26550;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#22522;&#32447;&#30456;&#27604;&#65292;ReNeLLM&#26174;&#33879;&#25552;&#39640;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08268v2 Announce Type: replace  Abstract: Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as 'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on other white-box models, compromising generalization or efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Ou
&lt;/p&gt;</description></item><item><title>&#21487;&#39564;&#35777;&#29983;&#25104;&#20013;&#26816;&#32034;&#30340;&#25991;&#20214;&#19981;&#20165;&#24110;&#21161;LLM&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#36824;&#20316;&#20026;&#29992;&#25143;&#39564;&#35777;LLM&#36755;&#20986;&#30340;&#35777;&#25454;&#65292;&#20294;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#26816;&#32034;&#22120;&#24050;&#25104;&#20026;&#24615;&#33021;&#29942;&#39048;&#65292;&#38656;&#35201;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2311.07838</link><description>&lt;p&gt;
LLatrieval&#65306;LLM-&#39564;&#35777;&#26816;&#32034;&#29992;&#20110;&#21487;&#39564;&#35777;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LLatrieval: LLM-Verified Retrieval for Verifiable Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07838
&lt;/p&gt;
&lt;p&gt;
&#21487;&#39564;&#35777;&#29983;&#25104;&#20013;&#26816;&#32034;&#30340;&#25991;&#20214;&#19981;&#20165;&#24110;&#21161;LLM&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#36824;&#20316;&#20026;&#29992;&#25143;&#39564;&#35777;LLM&#36755;&#20986;&#30340;&#35777;&#25454;&#65292;&#20294;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#26816;&#32034;&#22120;&#24050;&#25104;&#20026;&#24615;&#33021;&#29942;&#39048;&#65292;&#38656;&#35201;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#39564;&#35777;&#29983;&#25104;&#26088;&#22312;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#20855;&#26377;&#25903;&#25745;&#25991;&#20214;&#30340;&#25991;&#26412;&#65292;&#36825;&#20351;&#29992;&#25143;&#33021;&#22815;&#28789;&#27963;&#39564;&#35777;&#31572;&#26696;&#65292;&#24182;&#20351;LLM&#30340;&#36755;&#20986;&#26356;&#21487;&#38752;&#12290;&#26816;&#32034;&#22312;&#21487;&#39564;&#35777;&#29983;&#25104;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26816;&#32034;&#21040;&#30340;&#25991;&#20214;&#19981;&#20165;&#34917;&#20805;&#30693;&#35782;&#20197;&#24110;&#21161;LLM&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#36824;&#20316;&#20026;&#25903;&#25345;&#29992;&#25143;&#39564;&#35777;LLM&#36755;&#20986;&#30340;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#26816;&#32034;&#22120;&#25104;&#20026;&#25972;&#20010;&#27969;&#31243;&#30340;&#29942;&#39048;&#65292;&#24182;&#38480;&#21046;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;&#30001;&#20110;&#36890;&#24120;&#20855;&#26377;&#30340;&#21442;&#25968;&#27604;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23569;&#24471;&#22810;&#65292;&#24182;&#19988;&#23578;&#26410;&#35777;&#26126;&#33021;&#22815;&#33391;&#22909;&#22320;&#25193;&#23637;&#21040;LLM&#30340;&#35268;&#27169;&#65292;&#22240;&#27492;&#23427;&#20204;&#30340;&#33021;&#21147;&#36890;&#24120;&#27604;LLMs&#24046;&#12290;&#22914;&#26524;&#26816;&#32034;&#22120;&#26410;&#33021;&#27491;&#30830;&#25214;&#21040;&#25903;&#25345;&#25991;&#20214;&#65292;&#21017;LLM&#23558;&#26080;&#27861;&#29983;&#25104;&#27491;&#30830;&#21644;&#21487;&#39564;&#35777;&#30340;&#31572;&#26696;&#65292;&#36825;&#20250;&#25513;&#30422;LLM&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07838v2 Announce Type: replace  Abstract: Verifiable generation aims to let the large language model (LLM) generate text with supporting documents, which enables the user to flexibly verify the answer and makes the LLM's output more reliable. Retrieval plays a crucial role in verifiable generation. Specifically, the retrieved documents not only supplement knowledge to help the LLM generate correct answers, but also serve as supporting evidence for the user to verify the LLM's output. However, the widely used retrievers become the bottleneck of the entire pipeline and limit the overall performance. Their capabilities are usually inferior to LLMs since they often have much fewer parameters than the large language model and have not been demonstrated to scale well to the size of LLMs. If the retriever does not correctly find the supporting documents, the LLM can not generate the correct and verifiable answer, which overshadows the LLM's remarkable abilities. To address these li
&lt;/p&gt;</description></item><item><title>&#35843;&#30740;&#20102;&#26368;&#36817;&#24212;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2310.19055</link><description>&lt;p&gt;
&#26368;&#36817;&#23569;&#26679;&#26412;&#23398;&#20064;&#38598;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#20998;&#31867;&#26041;&#27861;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Few-Shot Learning Focused Survey on Recent Named Entity Recognition and Relation Classification Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19055
&lt;/p&gt;
&lt;p&gt;
&#35843;&#30740;&#20102;&#26368;&#36817;&#24212;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#20851;&#31995;&#20998;&#31867;&#65288;RC&#65289;&#26159;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#23558;&#20854;&#26684;&#24335;&#21270;&#20026;&#26426;&#22120;&#21487;&#35835;&#26684;&#24335;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26368;&#36817;&#35299;&#20915;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#20998;&#31867;&#38382;&#39064;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35843;&#30740;&#65292;&#37325;&#28857;&#20851;&#27880;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35843;&#30740;&#26377;&#21161;&#20110;&#30740;&#31350;&#20154;&#21592;&#20102;&#35299;&#25991;&#26412;&#25366;&#25496;&#21644;&#20174;&#21407;&#22987;&#25991;&#26412;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19055v2 Announce Type: replace  Abstract: Named Entity Recognition (NER) and Relation Classification (RC) are important steps in extracting information from unstructured text and formatting it into a machine-readable format. We present a survey of recent deep learning models that address named entity recognition and relation classification, with focus on few-shot learning performance. Our survey is helpful for researchers in knowing the recent techniques in text mining and extracting structured information from raw text.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36328;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#35780;&#20272;&#20998;&#31867;&#22120;&#26159;&#21542;&#33021;&#26816;&#27979;&#26469;&#33258;&#30446;&#26631;LLM&#30340;&#25991;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#20851;&#38190;&#21457;&#29616;</title><link>https://arxiv.org/abs/2309.13322</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#21040;&#26469;&#28304;: &#22312;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20869;&#23481;&#26041;&#38754;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
From Text to Source: Results in Detecting Large Language Model-Generated Content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36328;&#27169;&#22411;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#35780;&#20272;&#20998;&#31867;&#22120;&#26159;&#21542;&#33021;&#26816;&#27979;&#26469;&#33258;&#30446;&#26631;LLM&#30340;&#25991;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#20851;&#38190;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#24341;&#36215;&#20102;&#23545;&#20449;&#24687;&#38169;&#35823;&#21644;&#36947;&#24503;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#38656;&#35201;&#24320;&#21457;&#31283;&#20581;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#24402;&#22240;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#8220;&#36328;&#27169;&#22411;&#26816;&#27979;&#8221;&#65292;&#35780;&#20272;&#20102;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#20197;&#21306;&#20998;&#28304;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#30340;&#20998;&#31867;&#22120;&#26159;&#21542;&#20063;&#33021;&#26816;&#27979;&#30446;&#26631;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#35757;&#32451;&#12290;&#35813;&#30740;&#31350;&#20840;&#38754;&#25506;&#35752;&#20102;&#19981;&#21516;&#22823;&#23567;&#21644;&#23478;&#26063;&#30340;LLMs&#65292;&#35780;&#20272;&#20102;&#23545;&#20998;&#31867;&#22120;&#27867;&#21270;&#30340;&#23545;&#35805;&#24494;&#35843;&#25216;&#26415;&#12289;&#37327;&#21270;&#21644;&#27700;&#21360;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#27169;&#22411;&#24402;&#22240;&#65292;&#21253;&#25324;&#28304;&#27169;&#22411;&#35782;&#21035;&#12289;&#27169;&#22411;&#23478;&#26063;&#21644;&#27169;&#22411;&#22823;&#23567;&#20998;&#31867;&#65292;&#20197;&#21450;&#37327;&#21270;&#21644;&#27700;&#21360;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#20960;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13322v2 Announce Type: replace  Abstract: The widespread use of Large Language Models (LLMs), celebrated for their ability to generate human-like text, has raised concerns about misinformation and ethical implications. Addressing these concerns necessitates the development of robust methods to detect and attribute text generated by LLMs. This paper investigates "Cross-Model Detection," by evaluating whether a classifier trained to distinguish between source LLM-generated and human-written text can also detect text from a target LLM without further training. The study comprehensively explores various LLM sizes and families, and assesses the impact of conversational fine-tuning techniques, quantization, and watermarking on classifier generalization. The research also explores Model Attribution, encompassing source model identification, model family, and model size classification, in addition to quantization and watermarking detection. Our results reveal several key findings: a
&lt;/p&gt;</description></item><item><title>GlotScript&#26159;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#20070;&#20889;&#31995;&#32479;&#35782;&#21035;&#30340;&#36164;&#28304;&#21644;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;7000&#22810;&#31181;&#35821;&#35328;&#30340;&#32463;&#36807;&#39564;&#35777;&#30340;&#20070;&#20889;&#31995;&#32479;&#65292;&#21516;&#26102;&#28085;&#30422;&#20102;&#25152;&#26377;161&#31181;Unicode 15.0&#33050;&#26412;&#65292;&#24182;&#33021;&#24110;&#21161;&#28165;&#29702;&#22810;&#35821;&#31181;&#35821;&#26009;&#24211;&#20197;&#21450;&#20998;&#26512;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#23545;&#20302;&#36164;&#28304;&#33050;&#26412;&#21644;&#35821;&#35328;&#30340;&#35206;&#30422;&#12290;</title><link>https://arxiv.org/abs/2309.13320</link><description>&lt;p&gt;
GlotScript&#65306;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#20070;&#20889;&#31995;&#32479;&#35782;&#21035;&#30340;&#36164;&#28304;&#21644;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
GlotScript: A Resource and Tool for Low Resource Writing System Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13320
&lt;/p&gt;
&lt;p&gt;
GlotScript&#26159;&#19968;&#31181;&#29992;&#20110;&#20302;&#36164;&#28304;&#20070;&#20889;&#31995;&#32479;&#35782;&#21035;&#30340;&#36164;&#28304;&#21644;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;7000&#22810;&#31181;&#35821;&#35328;&#30340;&#32463;&#36807;&#39564;&#35777;&#30340;&#20070;&#20889;&#31995;&#32479;&#65292;&#21516;&#26102;&#28085;&#30422;&#20102;&#25152;&#26377;161&#31181;Unicode 15.0&#33050;&#26412;&#65292;&#24182;&#33021;&#24110;&#21161;&#28165;&#29702;&#22810;&#35821;&#31181;&#35821;&#26009;&#24211;&#20197;&#21450;&#20998;&#26512;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#23545;&#20302;&#36164;&#28304;&#33050;&#26412;&#21644;&#35821;&#35328;&#30340;&#35206;&#30422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;GlotScript&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20302;&#36164;&#28304;&#20070;&#20889;&#31995;&#32479;&#35782;&#21035;&#30340;&#24320;&#25918;&#36164;&#28304;&#21644;&#24037;&#20855;&#12290;GlotScript-R&#26159;&#19968;&#20010;&#36164;&#28304;&#65292;&#25552;&#20379;&#20102;7000&#22810;&#31181;&#35821;&#35328;&#30340;&#32463;&#36807;&#39564;&#35777;&#30340;&#20070;&#20889;&#31995;&#32479;&#12290;&#23427;&#36890;&#36807;&#25972;&#21512;&#29616;&#26377;&#30340;&#20070;&#20889;&#31995;&#32479;&#36164;&#28304;&#20013;&#30340;&#20449;&#24687;&#36827;&#34892;&#32534;&#21046;&#12290;GlotScript-T&#26159;&#19968;&#20010;&#20070;&#20889;&#31995;&#32479;&#35782;&#21035;&#24037;&#20855;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;161&#31181;Unicode 15.0&#33050;&#26412;&#12290;&#23545;&#20110;&#36755;&#20837;&#25991;&#26412;&#65292;&#23427;&#36820;&#22238;&#20854;&#33050;&#26412;&#20998;&#24067;&#65292;&#20854;&#20013;&#33050;&#26412;&#30001;ISO 15924&#20195;&#30721;&#26631;&#35782;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;GlotScript&#30340;&#20004;&#20010;&#29992;&#20363;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;GlotScript&#22914;&#20309;&#24110;&#21161;&#28165;&#29702;&#22810;&#35821;&#31181;&#35821;&#26009;&#24211;&#65292;&#20363;&#22914;mC4&#21644;OSCAR&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;GlotScript&#20998;&#26512;&#20102;&#22810;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#26631;&#35760;&#21270;&#65292;&#20363;&#22914;GPT-4&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#27599;&#20010;&#35821;&#35328;&#27169;&#22411;&#35206;&#30422;&#20302;&#36164;&#28304;&#33050;&#26412;&#21644;&#35821;&#35328;&#30340;&#35265;&#35299;&#12290;&#24076;&#26395;GlotScript&#33021;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#20013;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#24037;&#20316;&#30340;&#26377;&#29992;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13320v2 Announce Type: replace  Abstract: We present GlotScript, an open resource and tool for low resource writing system identification. GlotScript-R is a resource that provides the attested writing systems for more than 7,000 languages. It is compiled by aggregating information from existing writing system resources. GlotScript-T is a writing system identification tool that covers all 161 Unicode 15.0 scripts. For an input text, it returns its script distribution where scripts are identified by ISO 15924 codes. We also present two use cases for GlotScript. First, we demonstrate that GlotScript can help cleaning multilingual corpora such as mC4 and OSCAR. Second, we analyze the tokenization of a number of language models such as GPT-4 using GlotScript and provide insights on the coverage of low resource scripts and languages by each language model. We hope that GlotScript will become a useful resource for work on low resource languages in the NLP community. GlotScript-R an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#27491;&#24335;&#21270;&#21644;&#24050;&#30693;&#36234;&#29425;&#20998;&#31867;&#27861;&#20197;&#22635;&#34917;&#23545;&#21830;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#36234;&#29425;&#25915;&#20987;&#30340;&#32570;&#20047;&#30740;&#31350;&#65292;&#35843;&#26597;&#29616;&#26377;&#30340;&#36234;&#29425;&#26041;&#27861;&#21450;&#20854;&#22312;&#24320;&#28304;&#21644;&#21830;&#29992;LLMs&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2305.14965</link><description>&lt;p&gt;
&#27450;&#39575;LLMs&#35753;&#20854;&#19981;&#36981;&#20174;&#65306;&#27491;&#24335;&#21270;&#12289;&#20998;&#26512;&#21644;&#26816;&#27979;&#36234;&#29425;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#27491;&#24335;&#21270;&#21644;&#24050;&#30693;&#36234;&#29425;&#20998;&#31867;&#27861;&#20197;&#22635;&#34917;&#23545;&#21830;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#36234;&#29425;&#25915;&#20987;&#30340;&#32570;&#20047;&#30740;&#31350;&#65292;&#35843;&#26597;&#29616;&#26377;&#30340;&#36234;&#29425;&#26041;&#27861;&#21450;&#20854;&#22312;&#24320;&#28304;&#21644;&#21830;&#29992;LLMs&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#21830;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25506;&#32034;&#34920;&#26126;&#65292;&#38750;&#19987;&#23478;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#25805;&#32437;&#20182;&#20204;&#30340;&#25552;&#31034;&#26469;&#36234;&#29425;LLMs&#65307;&#23548;&#33268;&#36864;&#21270;&#30340;&#36755;&#20986;&#34892;&#20026;&#12289;&#38544;&#31169;&#19982;&#23433;&#20840;&#28431;&#27934;&#12289;&#20882;&#29359;&#24615;&#36755;&#20986;&#20197;&#21450;&#36829;&#21453;&#20869;&#23481;&#30417;&#31649;&#25919;&#31574;&#12290;&#26377;&#38480;&#30340;&#30740;&#31350;&#24050;&#36827;&#34892;&#20102;&#23545;&#36825;&#20123;&#25915;&#20987;&#21450;&#20854;&#32531;&#35299;&#25514;&#26045;&#30340;&#27491;&#24335;&#21270;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#24418;&#24335;&#21270;&#25551;&#36848;&#21644;&#24050;&#30693;&#65288;&#21450;&#21487;&#33021;&#30340;&#65289;&#36234;&#29425;&#20998;&#31867;&#27861;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#35843;&#26597;&#29616;&#26377;&#30340;&#36234;&#29425;&#26041;&#27861;&#21450;&#20854;&#22312;&#24320;&#28304;&#21644;&#21830;&#29992;LLMs&#65288;&#22914;&#22522;&#20110;GPT&#30340;&#27169;&#22411;&#12289;OPT&#12289;BLOOM&#21644;FLAN-T5-XXL&#65289;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#36234;&#29425;&#26816;&#27979;&#22312;&#38024;&#23545;&#24050;&#30693;&#25915;&#20987;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;4&#39033;&#20219;&#21153;&#30340;3700&#20010;&#36234;&#29425;&#25552;&#31034;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23558;&#38543;&#30528;&#27169;&#22411;&#36755;&#20986;&#19968;&#36215;&#20844;&#24320;&#36825;&#19968;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.14965v2 Announce Type: replace  Abstract: Recent explorations with commercial Large Language Models (LLMs) have shown that non-expert users can jailbreak LLMs by simply manipulating their prompts; resulting in degenerate output behavior, privacy and security breaches, offensive outputs, and violations of content regulator policies. Limited studies have been conducted to formalize and analyze these attacks and their mitigations. We bridge this gap by proposing a formalism and a taxonomy of known (and possible) jailbreaks. We survey existing jailbreak methods and their effectiveness on open-source and commercial LLMs (such as GPT-based models, OPT, BLOOM, and FLAN-T5-XXL). We further discuss the challenges of jailbreak detection in terms of their effectiveness against known attacks. For our analysis, we collect a dataset of 3700 jailbreak prompts across 4 tasks. We will make the dataset public along with the model outputs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20197;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#21512;&#24182;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#21017;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.12492</link><description>&lt;p&gt;
&#27604;&#36739;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35821;&#35328;&#24314;&#27169;&#65306;&#27169;&#25311;&#32676;&#20307;&#12289;&#20010;&#20307;&#29305;&#28857;&#36824;&#26159;&#20004;&#32773;&#20860;&#39038;&#65311;
&lt;/p&gt;
&lt;p&gt;
Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?. (arXiv:2401.12492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20197;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#21512;&#24182;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#21017;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#23558;&#20154;&#30340;&#19978;&#19979;&#25991;&#32435;&#20837;&#20854;&#27169;&#22411;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20351;&#29992;&#32676;&#20307;&#23646;&#24615;&#65288;&#22914;45&#23681;&#20197;&#19978;&#30340;&#20154;&#32676;&#65289;&#36824;&#26159;&#27169;&#25311;&#20010;&#20307;&#20154;&#29289;&#26356;&#26377;&#25928;&#30340;&#38382;&#39064;&#23578;&#26410;&#30830;&#23450;&#12290;&#32676;&#20307;&#23646;&#24615;&#22312;&#25216;&#26415;&#19978;&#26356;&#23481;&#26131;&#23454;&#29616;&#65292;&#20294;&#26159;&#36807;&#20110;&#31895;&#31961;&#65306;&#24182;&#38750;&#25152;&#26377;45&#23681;&#20197;&#19978;&#30340;&#20154;&#37117;&#20197;&#30456;&#21516;&#30340;&#26041;&#24335;&#20070;&#20889;&#12290;&#30456;&#21453;&#65292;&#27169;&#25311;&#20010;&#20307;&#20154;&#29289;&#33021;&#22815;&#25429;&#25417;&#27599;&#20010;&#20154;&#36523;&#20221;&#30340;&#22797;&#26434;&#24615;&#65292;&#20801;&#35768;&#26356;&#20010;&#24615;&#21270;&#30340;&#34920;&#31034;&#65292;&#20294;&#25105;&#20204;&#21487;&#33021;&#38656;&#35201;&#27169;&#25311;&#26080;&#38480;&#25968;&#37327;&#30340;&#29992;&#25143;&#24182;&#19988;&#38656;&#35201;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#36890;&#36807;&#32676;&#20307;&#23646;&#24615;&#12289;&#20010;&#20307;&#29992;&#25143;&#21644;&#32452;&#21512;&#26041;&#27861;&#26469;&#27169;&#25311;&#20154;&#30340;&#19978;&#19979;&#25991;&#12290;&#23558;&#32676;&#20307;&#21644;&#20010;&#20307;&#29305;&#24449;&#32467;&#21512;&#36215;&#26469;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#30340;&#29992;&#25143;&#32423;&#22238;&#24402;&#20219;&#21153;&#65288;&#22914;&#24180;&#40836;&#20272;&#35745;&#25110;&#20010;&#24615;&#35780;&#20272;&#65289;&#30340;&#24615;&#33021;&#12290;&#27169;&#25311;&#20010;&#20307;&#29992;&#25143;&#26174;&#33879;&#25552;&#39640;&#20102;&#21333;&#20010;&#25991;&#26723;&#32423;&#20998;&#31867;&#20219;&#21153;&#65288;&#22914;&#31435;&#22330;&#21644;&#20027;&#39064;&#26816;&#27979;&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing has made progress in incorporating human context into its models, but whether it is more effective to use group-wise attributes (e.g., over-45-year-olds) or model individuals remains open. Group attributes are technically easier but coarse: not all 45-year-olds write the same way. In contrast, modeling individuals captures the complexity of each person's identity. It allows for a more personalized representation, but we may have to model an infinite number of users and require data that may be impossible to get. We compare modeling human context via group attributes, individual users, and combined approaches. Combining group and individual features significantly benefits user-level regression tasks like age estimation or personality assessment from a user's documents. Modeling individual users significantly improves the performance of single document-level classification tasks like stance and topic detection. We also find that individual-user modeling does w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26679;&#24335;&#34920;&#31034;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#30340;&#21306;&#21035;&#65292;&#20197;&#35299;&#20915;&#28389;&#29992;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06712</link><description>&lt;p&gt;
&#20351;&#29992;&#26679;&#24335;&#34920;&#31034;&#36827;&#34892;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#23567;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Detection of Machine-Generated Text using Style Representations. (arXiv:2401.06712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26679;&#24335;&#34920;&#31034;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#30340;&#21306;&#21035;&#65292;&#20197;&#35299;&#20915;&#28389;&#29992;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#25351;&#23548;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#20351;&#24471;&#20154;&#31867;&#20889;&#20316;&#30340;&#36924;&#30495;&#27169;&#20223;&#38754;&#20020;&#30528;&#37325;&#22823;&#28389;&#29992;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#26816;&#27979;&#19968;&#27573;&#25991;&#26412;&#26159;&#30001;&#35821;&#35328;&#27169;&#22411;&#36824;&#26159;&#20154;&#31867;&#25776;&#20889;&#32780;&#25104;&#26469;&#23545;&#25239;&#27492;&#31867;&#28389;&#29992;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#24335;&#34920;&#31034;&#30340;&#23567;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#22312;&#38754;&#23545;&#25968;&#25454;&#36716;&#25442;&#26102;&#30340;&#35268;&#32422;&#19981;&#36275;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#20063;&#36991;&#20813;&#20102;&#22312;&#25512;&#29702;&#25110;&#26816;&#27979;&#26102;&#38656;&#35201;&#35775;&#38382;&#21487;&#33021;&#29983;&#25104;&#25991;&#26723;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of instruction-tuned language models that convincingly mimic human writing poses a significant risk of abuse. For example, such models could be used for plagiarism, disinformation, spam, or phishing. However, such abuse may be counteracted with the ability to detect whether a piece of text was composed by a language model rather than a human. Some previous approaches to this problem have relied on supervised methods trained on corpora of confirmed human and machine-written documents. Unfortunately, model under-specification poses an unavoidable challenge for neural network-based detectors, making them brittle in the face of data shifts, such as the release of further language models producing still more fluent text than the models used to train the detectors. Other previous approaches require access to the models that may have generated a document in question at inference or detection time, which is often impractical. In light of these challenges, we pursue a fundamentally d
&lt;/p&gt;</description></item><item><title>EASYTOOL&#26159;&#19968;&#20010;&#23558;&#22810;&#26679;&#21270;&#32780;&#20887;&#38271;&#30340;&#24037;&#20855;&#25991;&#26723;&#36716;&#21270;&#20026;&#32479;&#19968;&#32780;&#31616;&#27905;&#30340;&#24037;&#20855;&#25351;&#31034;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20174;&#22810;&#20010;&#26469;&#28304;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#26631;&#20934;&#21270;&#30340;&#24037;&#20855;&#25551;&#36848;&#21644;&#21151;&#33021;&#65292;EasyTool&#26174;&#33879;&#38477;&#20302;&#20102;&#26631;&#35760;&#28040;&#32791;&#65292;&#24182;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24037;&#20855;&#21033;&#29992;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06201</link><description>&lt;p&gt;
EASYTOOL: &#20351;&#29992;&#31616;&#27905;&#30340;&#24037;&#20855;&#25351;&#31034;&#22686;&#24378;&#22522;&#20110;LLM&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction. (arXiv:2401.06201v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06201
&lt;/p&gt;
&lt;p&gt;
EASYTOOL&#26159;&#19968;&#20010;&#23558;&#22810;&#26679;&#21270;&#32780;&#20887;&#38271;&#30340;&#24037;&#20855;&#25991;&#26723;&#36716;&#21270;&#20026;&#32479;&#19968;&#32780;&#31616;&#27905;&#30340;&#24037;&#20855;&#25351;&#31034;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20174;&#22810;&#20010;&#26469;&#28304;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#26631;&#20934;&#21270;&#30340;&#24037;&#20855;&#25551;&#36848;&#21644;&#21151;&#33021;&#65292;EasyTool&#26174;&#33879;&#38477;&#20302;&#20102;&#26631;&#35760;&#28040;&#32791;&#65292;&#24182;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24037;&#20855;&#21033;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#25918;&#22312;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24212;&#29992;&#20013;&#30340;&#24037;&#20855;&#21033;&#29992;&#19978;&#12290;&#20026;&#20102;&#24320;&#21457;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#36890;&#24120;&#38656;&#35201;LLM&#20174;&#19981;&#21516;&#30340;&#24037;&#20855;&#25991;&#26723;&#20013;&#29702;&#35299;&#35768;&#22810;&#24037;&#20855;&#21151;&#33021;&#12290;&#20294;&#36825;&#20123;&#25991;&#26723;&#21487;&#33021;&#26159;&#22810;&#26679;&#21270;&#30340;&#12289;&#20887;&#20313;&#30340;&#25110;&#19981;&#23436;&#25972;&#30340;&#65292;&#36825;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;LLM&#22312;&#20351;&#29992;&#24037;&#20855;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EASYTOOL&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#22810;&#26679;&#21270;&#32780;&#20887;&#38271;&#30340;&#24037;&#20855;&#25991;&#26723;&#36716;&#21270;&#20026;&#32479;&#19968;&#32780;&#31616;&#27905;&#30340;&#24037;&#20855;&#25351;&#31034;&#65292;&#20197;&#20415;&#26356;&#23481;&#26131;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;EasyTool&#20174;&#19981;&#21516;&#26469;&#28304;&#30340;&#24191;&#27867;&#24037;&#20855;&#25991;&#26723;&#20013;&#25552;&#21462;&#20986;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#35814;&#32454;&#35828;&#26126;&#19968;&#20010;&#32479;&#19968;&#30340;&#25509;&#21475;&#65288;&#21363;&#24037;&#20855;&#25351;&#31034;&#65289;&#65292;&#20026;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#25552;&#20379;&#26631;&#20934;&#21270;&#30340;&#24037;&#20855;&#25551;&#36848;&#21644;&#21151;&#33021;&#12290;&#23545;&#22810;&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;EasyTool&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#26631;&#35760;&#30340;&#28040;&#32791;&#65292;&#24182;&#25913;&#21892;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24037;&#20855;&#21033;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address intricate real-world tasks, there has been a rising interest in tool utilization in applications of large language models (LLMs). To develop LLM-based agents, it usually requires LLMs to understand many tool functions from different tool documentation. But these documentations could be diverse, redundant or incomplete, which immensely affects the capability of LLMs in using tools. To solve this, we introduce EASYTOOL, a framework transforming diverse and lengthy tool documentation into a unified and concise tool instruction for easier tool usage. EasyTool purifies essential information from extensive tool documentation of different sources, and elaborates a unified interface (i.e., tool instruction) to offer standardized tool descriptions and functionalities for LLM-based agents. Extensive experiments on multiple different tasks demonstrate that EasyTool can significantly reduce token consumption and improve the performance of tool utilization in real-world scenarios. Our co
&lt;/p&gt;</description></item><item><title>&#33258;&#25105;&#23545;&#27604;&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#27714;&#35299;&#35270;&#35282;&#21644;&#24635;&#32467;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21453;&#24605;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02009</link><description>&lt;p&gt;
&#33258;&#25105;&#23545;&#27604;&#65306;&#36890;&#36807;&#19981;&#19968;&#33268;&#30340;&#27714;&#35299;&#35270;&#35282;&#33719;&#24471;&#26356;&#22909;&#30340;&#21453;&#24605;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives. (arXiv:2401.02009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02009
&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#23545;&#27604;&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#27714;&#35299;&#35270;&#35282;&#21644;&#24635;&#32467;&#24046;&#24322;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21453;&#24605;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21453;&#24605;&#33021;&#21147;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#19968;&#31181;&#20107;&#21518;&#25552;&#31034;&#31574;&#30053;&#65292;&#20363;&#22914;&#21453;&#24605;&#21644;&#33258;&#25105;&#25913;&#36827;&#65292;&#26681;&#25454;&#33258;&#25105;&#35780;&#20272;&#25110;&#22806;&#37096;&#21453;&#39304;&#26469;&#25913;&#21892;LLM&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27809;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#65292;LLM&#30340;&#20869;&#22312;&#21453;&#24605;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;&#33258;&#25105;&#35780;&#20272;&#21453;&#39304;&#36136;&#37327;&#26159;&#20851;&#38190;&#29942;&#39048;&#12290;&#25105;&#20204;&#21457;&#29616;LLM&#22312;&#33258;&#25105;&#35780;&#20272;&#26102;&#24120;&#24120;&#34920;&#29616;&#20986;&#36807;&#24230;&#33258;&#20449;&#25110;&#39640;&#24230;&#38543;&#26426;&#24615;&#65292;&#25552;&#20379;&#22266;&#25191;&#25110;&#19981;&#19968;&#33268;&#30340;&#21453;&#39304;&#65292;&#23548;&#33268;&#21453;&#24605;&#33021;&#21147;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#23545;&#27604;&#30340;&#26041;&#27861;&#65306;&#23427;&#26681;&#25454;&#35831;&#27714;&#33258;&#36866;&#24212;&#22320;&#25506;&#32034;&#22810;&#26679;&#30340;&#27714;&#35299;&#35270;&#35282;&#65292;&#23545;&#27604;&#24046;&#24322;&#65292;&#24182;&#23558;&#36825;&#20123;&#24046;&#24322;&#24635;&#32467;&#20026;&#19968;&#20010;&#26816;&#26597;&#34920;&#65292;&#29992;&#20110;&#37325;&#26032;&#23457;&#35270;&#21644;&#28040;&#38500;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36171;&#20104;LLM&#22810;&#26679;&#30340;&#35270;&#35282;&#20197;&#20943;&#36731;&#22266;&#25191;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#24046;&#24322;&#25351;&#31034;&#20102;&#28508;&#22312;&#30340;&#38169;&#35823;&#25110;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM's response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM's intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method endows LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties tha
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#33539;&#24335;&#65306;Naive RAG&#12289;Advanced RAG&#21644;Modular RAG&#12290;RAG&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#21644;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.10997</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generation for Large Language Models: A Survey. (arXiv:2312.10997v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#33539;&#24335;&#65306;Naive RAG&#12289;Advanced RAG&#21644;Modular RAG&#12290;RAG&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#21644;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#20294;&#38754;&#20020;&#24187;&#35273;&#12289;&#36807;&#26102;&#30693;&#35782;&#21644;&#38750;&#36879;&#26126;&#12289;&#19981;&#21487;&#36861;&#28335;&#30340;&#25512;&#29702;&#36807;&#31243;&#31561;&#25361;&#25112;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#21644;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#12290;RAG&#23558;LLMs&#33258;&#36523;&#30340;&#30693;&#35782;&#19982;&#24222;&#22823;&#12289;&#21160;&#24577;&#30340;&#22806;&#37096;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#21327;&#21516;&#25928;&#24212;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#35814;&#32454;&#32771;&#23519;&#20102;RAG&#33539;&#24335;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;Naive RAG&#12289;Advanced RAG&#21644;Modular RAG&#12290;&#23427;&#35814;&#32454;&#23457;&#35270;&#20102;RAG&#26694;&#26550;&#30340;&#19977;&#20010;&#22522;&#26412;&#35201;&#32032;&#65292;&#21253;&#25324;&#26816;&#32034;&#12289;&#29983;&#25104;&#21644;&#22686;&#24378;&#25216;&#26415;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#23884;&#20837;&#22312;RAG&#26694;&#26550;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#20849;&#21516;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;CARE&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#21644;&#21452;&#21521;&#20132;&#20114;&#26469;&#35299;&#20915;&#29305;&#24449;&#28151;&#28102;&#21644;&#23376;&#20219;&#21153;&#20132;&#20114;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12531</link><description>&lt;p&gt;
CARE: &#29992;&#20110;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#20849;&#21516;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CARE: Co-Attention Network for Joint Entity and Relation Extraction. (arXiv:2308.12531v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#20849;&#21516;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;CARE&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#21644;&#21452;&#21521;&#20132;&#20114;&#26469;&#35299;&#20915;&#29305;&#24449;&#28151;&#28102;&#21644;&#23376;&#20219;&#21153;&#20132;&#20114;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#26159;&#20449;&#24687;&#25277;&#21462;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#21253;&#25324;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32852;&#21512;&#25277;&#21462;&#26041;&#27861;&#37117;&#23384;&#22312;&#29305;&#24449;&#28151;&#28102;&#25110;&#20004;&#20010;&#23376;&#20219;&#21153;&#20043;&#38388;&#20132;&#20114;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#20849;&#21516;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;CARE&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23398;&#20064;&#27599;&#20010;&#23376;&#20219;&#21153;&#30340;&#20998;&#31163;&#34920;&#31034;&#65292;&#26088;&#22312;&#36991;&#20813;&#29305;&#24449;&#37325;&#21472;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20849;&#21516;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#23427;&#25429;&#25417;&#20004;&#20010;&#23376;&#20219;&#21153;&#20043;&#38388;&#30340;&#21452;&#21521;&#20132;&#20114;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#23454;&#20307;&#20449;&#24687;&#36827;&#34892;&#20851;&#31995;&#39044;&#27979;&#65292;&#21453;&#20043;&#20134;&#28982;&#65292;&#20174;&#32780;&#20419;&#36827;&#30456;&#20114;&#22686;&#24378;&#12290;&#22312;&#19977;&#20010;&#32852;&#21512;&#23454;&#20307;-&#20851;&#31995;&#25277;&#21462;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;NYT&#12289;WebNLG&#21644;SciERC&#65289;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Joint entity and relation extraction is the fundamental task of information extraction, consisting of two subtasks: named entity recognition and relation extraction. Most existing joint extraction methods suffer from issues of feature confusion or inadequate interaction between two subtasks. In this work, we propose a Co-Attention network for joint entity and Relation Extraction (CARE). Our approach involves learning separate representations for each subtask, aiming to avoid feature overlap. At the core of our approach is the co-attention module that captures two-way interaction between two subtasks, allowing the model to leverage entity information for relation prediction and vice versa, thus promoting mutual enhancement. Extensive experiments on three joint entity-relation extraction benchmark datasets (NYT, WebNLG and SciERC) show that our proposed model achieves superior performance, surpassing existing baseline models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#28040;&#36153;&#32773;&#25237;&#35785;&#21465;&#36848;&#20013;&#30340;&#31995;&#32479;&#24322;&#24120;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#20998;&#31867;&#31639;&#27861;&#23545;&#20110;&#36739;&#23567;&#19988;&#39057;&#32321;&#20986;&#29616;&#30340;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#25237;&#35785;&#21465;&#36848;&#36716;&#21270;&#20026;&#23450;&#37327;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.11138</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#28040;&#36153;&#32773;&#25237;&#35785;&#21465;&#36848;&#20013;&#31995;&#32479;&#24322;&#24120;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NLP-based detection of systematic anomalies among the narratives of consumer complaints. (arXiv:2308.11138v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#28040;&#36153;&#32773;&#25237;&#35785;&#21465;&#36848;&#20013;&#30340;&#31995;&#32479;&#24322;&#24120;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#20998;&#31867;&#31639;&#27861;&#23545;&#20110;&#36739;&#23567;&#19988;&#39057;&#32321;&#20986;&#29616;&#30340;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#25237;&#35785;&#21465;&#36848;&#36716;&#21270;&#20026;&#23450;&#37327;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#25237;&#35785;&#21465;&#36848;&#20013;&#30340;&#31995;&#32479;&#24322;&#24120;&#65292;&#31616;&#31216;&#20026;&#31995;&#32479;&#24322;&#24120;&#12290;&#23613;&#31649;&#20998;&#31867;&#31639;&#27861;&#34987;&#29992;&#20110;&#26816;&#27979;&#26126;&#26174;&#30340;&#24322;&#24120;&#65292;&#20294;&#22312;&#36739;&#23567;&#19988;&#39057;&#32321;&#20986;&#29616;&#30340;&#31995;&#32479;&#24322;&#24120;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#21487;&#33021;&#20250;&#22240;&#20026;&#21508;&#31181;&#21407;&#22240;&#32780;&#22833;&#25928;&#65292;&#21253;&#25324;&#25216;&#26415;&#21407;&#22240;&#21644;&#20154;&#24037;&#20998;&#26512;&#24072;&#30340;&#33258;&#28982;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#22312;&#20998;&#31867;&#20043;&#21518;&#30340;&#19979;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#23558;&#25237;&#35785;&#21465;&#36848;&#36716;&#21270;&#20026;&#23450;&#37327;&#25968;&#25454;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#31181;&#31639;&#27861;&#26469;&#26816;&#27979;&#31995;&#32479;&#24322;&#24120;&#12290;&#25105;&#20204;&#20351;&#29992;&#28040;&#36153;&#32773;&#37329;&#34701;&#20445;&#25252;&#23616;&#30340;&#28040;&#36153;&#32773;&#25237;&#35785;&#25968;&#25454;&#24211;&#20013;&#30340;&#25237;&#35785;&#21465;&#36848;&#26469;&#35828;&#26126;&#25972;&#20010;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop an NLP-based procedure for detecting systematic nonmeritorious consumer complaints, simply called systematic anomalies, among complaint narratives. While classification algorithms are used to detect pronounced anomalies, in the case of smaller and frequent systematic anomalies, the algorithms may falter due to a variety of reasons, including technical ones as well as natural limitations of human analysts. Therefore, as the next step after classification, we convert the complaint narratives into quantitative data, which are then analyzed using an algorithm for detecting systematic anomalies. We illustrate the entire procedure using complaint narratives from the Consumer Complaint Database of the Consumer Financial Protection Bureau.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Iro&#769;yi&#768;nSpeech&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#32422;&#40065;&#24052;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#21487;&#29992;&#20110;TTS&#21644;ASR&#20219;&#21153;&#65292;&#21253;&#21547;38.5&#23567;&#26102;&#30340;&#25968;&#25454;&#65292;&#30001;80&#21517;&#24535;&#24895;&#32773;&#24405;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.16071</link><description>&lt;p&gt;
Iro&#769;yi&#768;nSpeech&#65306;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#32422;&#40065;&#24052;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
\`{I}r\`{o}y\`{i}nSpeech: A multi-purpose Yor\`{u}b\'{a} Speech Corpus. (arXiv:2307.16071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;Iro&#769;yi&#768;nSpeech&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#32422;&#40065;&#24052;&#35821;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#21487;&#29992;&#20110;TTS&#21644;ASR&#20219;&#21153;&#65292;&#21253;&#21547;38.5&#23567;&#26102;&#30340;&#25968;&#25454;&#65292;&#30001;80&#21517;&#24535;&#24895;&#32773;&#24405;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Iro&#769;yi&#768;nSpeech&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#21040;&#24076;&#26395;&#22686;&#21152;&#39640;&#36136;&#37327;&#12289;&#20813;&#36153;&#21487;&#29992;&#30340;&#24403;&#20195;&#32422;&#40065;&#24052;&#35821;&#35821;&#38899;&#30340;&#24433;&#21709;&#32780;&#21019;&#24314;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#22810;&#29992;&#36884;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;TTS&#21644;ASR&#20219;&#21153;&#12290;&#25105;&#20204;&#20174;&#26032;&#38395;&#21644;&#21019;&#24847;&#20889;&#20316;&#39046;&#22495;&#25910;&#38598;&#20102;&#25991;&#23383;&#21477;&#23376;&#65292;&#22312;&#24320;&#25918;&#25480;&#26435;&#65288;CC-BY-4.0&#65289;&#19979;&#36827;&#34892;&#31579;&#36873;&#65292;&#24182;&#30001;&#22810;&#20301;&#21457;&#38899;&#20154;&#24405;&#21046;&#27599;&#20010;&#21477;&#23376;&#12290;&#25105;&#20204;&#23558;5000&#20010;&#21457;&#38899;&#21477;&#25552;&#20379;&#32473;Common Voice&#24179;&#21488;&#65292;&#22312;&#32447;&#20247;&#21253;&#36716;&#24405;&#12290;&#35813;&#25968;&#25454;&#38598;&#24635;&#20849;&#26377;38.5&#23567;&#26102;&#30340;&#25968;&#25454;&#65292;&#30001;80&#21517;&#24535;&#24895;&#32773;&#24405;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the \`{I}r\`{o}y\`{i}nSpeech corpus -- a new dataset influenced by a desire to increase the amount of high quality, freely available, contemporary Yor\`{u}b\'{a} speech. We release a multi-purpose dataset that can be used for both TTS and ASR tasks. We curated text sentences from the news and creative writing domains under an open license i.e., CC-BY-4.0 and had multiple speakers record each sentence. We provide 5000 of our utterances to the Common Voice platform to crowdsource transcriptions online. The dataset has 38.5 hours of data in total, recorded by 80 volunteers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#19977;&#20010;&#21830;&#19994;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#36827;&#34892;&#20102;&#32454;&#33268;&#35780;&#20272;&#65292;&#22312;&#20851;&#27880;&#24615;&#21035;&#32763;&#35793;&#21644;&#20559;&#35265;&#30340;&#21069;&#25552;&#19979;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#24615;&#21035;&#32763;&#35793;&#26041;&#38754;&#30340;&#26174;&#30528;&#24046;&#24322;&#21644;&#20559;&#35265;&#65292;&#36825;&#19968;&#28857;&#19981;&#20250;&#22240;&#20026;&#23427;&#20204;&#25972;&#20307;&#32763;&#35793;&#36136;&#37327;&#30340;&#22909;&#22351;&#32780;&#26377;&#25152;&#25913;&#21464;&#12290;</title><link>http://arxiv.org/abs/2306.05882</link><description>&lt;p&gt;
&#22909;&#30340;&#65292;&#20294;&#24182;&#38750;&#24635;&#26159;&#20844;&#27491;&#65306;&#23545;&#19977;&#20010;&#21830;&#19994;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#24615;&#21035;&#20559;&#35265;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Good, but not always Fair: An Evaluation of Gender Bias for three commercial Machine Translation Systems. (arXiv:2306.05882v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#19977;&#20010;&#21830;&#19994;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#36827;&#34892;&#20102;&#32454;&#33268;&#35780;&#20272;&#65292;&#22312;&#20851;&#27880;&#24615;&#21035;&#32763;&#35793;&#21644;&#20559;&#35265;&#30340;&#21069;&#25552;&#19979;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#24615;&#21035;&#32763;&#35793;&#26041;&#38754;&#30340;&#26174;&#30528;&#24046;&#24322;&#21644;&#20559;&#35265;&#65292;&#36825;&#19968;&#28857;&#19981;&#20250;&#22240;&#20026;&#23427;&#20204;&#25972;&#20307;&#32763;&#35793;&#36136;&#37327;&#30340;&#22909;&#22351;&#32780;&#26377;&#25152;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#22312;&#36136;&#37327;&#26041;&#38754;&#32487;&#32493;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#65292;&#24182;&#19988;&#27491;&#22312;&#36234;&#26469;&#36234;&#22823;&#35268;&#27169;&#22320;&#34987;&#37319;&#29992;&#12290;&#22240;&#27492;&#65292;&#20998;&#26512;&#24050;&#36716;&#21521;&#26356;&#24494;&#22937;&#30340;&#26041;&#38754;&#12289;&#22797;&#26434;&#30340;&#29616;&#35937;&#20197;&#21450;&#21487;&#33021;&#20174;&#24191;&#27867;&#20351;&#29992;MT&#24037;&#20855;&#20013;&#20135;&#29983;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#38024;&#23545;&#19977;&#20010;&#21830;&#19994;MT&#31995;&#32479; - Google Translate&#12289;DeepL&#21644;Modern MT&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#35780;&#20272;&#65292;&#37325;&#28857;&#20851;&#27880;&#24615;&#21035;&#32763;&#35793;&#21644;&#20559;&#35265;&#12290;&#23545;&#20110;&#19977;&#31181;&#35821;&#35328;&#23545;&#65288;&#33521;&#35821;/&#35199;&#29677;&#29273;&#35821;&#12289;&#33521;&#35821;/&#24847;&#22823;&#21033;&#35821;&#21644;&#33521;&#35821;/&#27861;&#35821;&#65289;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#22659;&#19979;&#30340;&#24615;&#21035;&#29616;&#35937;&#30340;&#22810;&#20010;&#31890;&#24230;&#32423;&#21035;&#19978;&#23457;&#26597;&#20102;&#36825;&#20123;&#31995;&#32479;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32771;&#34385;&#20102;&#22312;&#32447;MT&#24037;&#20855;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#25581;&#31034;&#20102;&#19977;&#20010;&#31995;&#32479;&#24615;&#21035;&#32763;&#35793;&#26041;&#38754;&#30340;&#26174;&#30528;&#24046;&#24322;&#65292;&#23613;&#31649;&#23427;&#20204;&#30340;&#25972;&#20307;&#32763;&#35793;&#36136;&#37327;&#19981;&#38169;&#65292;&#20294;&#26159;&#27599;&#20010;&#31995;&#32479;&#37117;&#34920;&#29616;&#20986;&#19981;&#21516;&#31243;&#24230;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Translation (MT) continues to make significant strides in quality and is increasingly adopted on a larger scale. Consequently, analyses have been redirected to more nuanced aspects, intricate phenomena, as well as potential risks that may arise from the widespread use of MT tools. Along this line, this paper offers a meticulous assessment of three commercial MT systems - Google Translate, DeepL, and Modern MT - with a specific focus on gender translation and bias. For three language pairs (English/Spanish, English/Italian, and English/French), we scrutinize the behavior of such systems at several levels of granularity and on a variety of naturally occurring gender phenomena in translation. Our study takes stock of the current state of online MT tools, by revealing significant discrepancies in the gender translation of the three systems, with each system displaying varying degrees of bias despite their overall translation quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04357</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20110;&#26816;&#32034;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems. (arXiv:2306.04357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#29992;&#25143;&#21644;&#31995;&#32479;&#35805;&#35821;&#21382;&#21490;&#35760;&#24405;&#20174;&#20960;&#20010;&#20505;&#36873;&#21709;&#24212;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#21518;&#35757;&#32451;&#22823;&#22810;&#20381;&#36182;&#20110;&#21333;&#32431;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#26469;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#24320;&#21457;&#30340;&#29983;&#25104;&#26041;&#27861;&#22312;IR&#31038;&#21306;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#25991;&#26412;&#34920;&#31034;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#23545;&#35805;&#35821;&#20041;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; Dial-MAE&#65288;&#23545;&#35805;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;&#12290; Dial-MAE&#20351;&#29992;&#19968;&#20010;&#19981;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#23398;&#20064;&#23558;&#23545;&#35805;&#30340;&#35821;&#20041;&#26356;&#22909;&#22320;&#21387;&#32553;&#21040;&#23494;&#38598;&#21521;&#37327;&#20013;&#12290; Dial-MAE&#30340;&#36807;&#31243;&#21253;&#25324;&#30001;&#28145;&#24230;&#32534;&#30721;&#22120;&#21019;&#24314;&#24102;&#26377;&#25513;&#30721;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#23545;&#35805;&#23884;&#20837;&#65292;&#28982;&#21518;&#26159;&#27973;&#35299;&#30721;&#22120;&#65292;&#35813;&#35299;&#30721;&#22120;&#20351;&#29992;&#27492;&#23884;&#20837;&#20197;&#21450;&#19978;&#19979;&#25991;&#21521;&#37327;&#26469;&#29983;&#25104;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue response selection aims to select an appropriate response from several candidates based on a given user and system utterance history. Recent studies have been improving the accuracy of dialogue response selection through post-training, mostly relying on naive masked language modeling methods. However, the recently developed generative methods have shown promising text representation capabilities in IR community, which could potentially lead to better dialogue semantics modeling. Thus, in this paper, we propose Dial-MAE (Dialogue Contextual Masking Auto-encoder), a straightforward yet effective post-training technique tailored for dialogue response selection. Dial-MAE uses an asymmetric encoder-decoder architecture that learns to better compress the semantics of the dialogue into dialogue-dense vectors. The process of Dial-MAE involves a deep encoder creating a dialogue embedding with the masked dialogue context, followed by a shallow decoder that uses this embedding along with
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;XLex&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;&#35789;&#20856;&#21644;Transformer&#27169;&#22411;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#22312;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20013;&#25552;&#20379;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03997</link><description>&lt;p&gt;
&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#65306;&#20174;Transformer&#22238;&#24402;&#21040;&#21487;&#35299;&#37322;&#24615;&#35789;&#20856;(XLex)
&lt;/p&gt;
&lt;p&gt;
Sentiment Analysis in Finance: From Transformers Back to eXplainable Lexicons (XLex). (arXiv:2306.03997v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;XLex&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;&#35789;&#20856;&#21644;Transformer&#27169;&#22411;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#22312;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20013;&#25552;&#20379;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35789;&#20856;&#30340;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#65288;SA&#65289;&#21033;&#29992;&#20154;&#24037;&#19987;&#23478;&#21019;&#24314;&#30340;&#19987;&#38376;&#27880;&#37322;&#30340;&#35789;&#20856;&#20174;&#37329;&#34701;&#25991;&#26412;&#20013;&#25552;&#21462;&#24773;&#24863;&#12290;&#34429;&#28982;&#22522;&#20110;&#35789;&#20856;&#30340;&#26041;&#27861;&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#23454;&#29616;&#31616;&#21333;&#19988;&#36895;&#24230;&#24555;&#65292;&#20294;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#25163;&#21160;&#27880;&#37322;&#24037;&#20316;&#26469;&#21019;&#24314;&#12289;&#32500;&#25252;&#21644;&#26356;&#26032;&#35789;&#20856;&#12290;&#36825;&#20123;&#26041;&#27861;&#20063;&#34987;&#35748;&#20026;&#19981;&#22914;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;&#22914;Transformer&#27169;&#22411;&#65289;&#20248;&#36234;&#65292;&#30001;&#20110;&#23427;&#20204;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#24050;&#32463;&#25104;&#20026;&#20027;&#27969;&#12290;&#28982;&#32780;&#65292;Transformer&#38656;&#35201;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#28041;&#21450;&#26174;&#33879;&#30340;&#39044;&#27979;&#26102;&#38388;&#65292;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#23454;&#26102;&#29983;&#20135;&#29615;&#22659;&#25110;&#22788;&#29702;&#33021;&#21147;&#21463;&#38480;&#30340;&#31995;&#32479;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;eXplainable Lexicons&#65288;XLex&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#35789;&#20856;&#30340;&#26041;&#27861;&#21644;Transformer&#27169;&#22411;&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lexicon-based sentiment analysis (SA) in finance leverages specialized, manually annotated lexicons created by human experts to extract sentiment from financial texts. Although lexicon-based methods are simple to implement and fast to operate on textual data, they require considerable manual annotation efforts to create, maintain, and update the lexicons. These methods are also considered inferior to the deep learning-based approaches, such as transformer models, which have become dominant in various NLP tasks due to their remarkable performance. However, transformers require extensive data and computational resources for both training and testing. Additionally, they involve significant prediction times, making them unsuitable for real-time production environments or systems with limited processing capabilities. In this paper, we introduce a novel methodology named eXplainable Lexicons (XLex) that combines the advantages of both lexicon-based methods and transformer models. We propose 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35757;&#32451;&#31639;&#27861;Left-over Lunch RL &#65288;LoL-RL&#65289;&#65292;&#20351;&#29992;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#20219;&#20309;&#24207;&#21015;&#21040;&#24207;&#21015;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20248;&#21270;LM&#25928;&#29992;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14718</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#20248;&#21183;&#30340;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Language Models with Advantage-based Offline Policy Gradients. (arXiv:2305.14718v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#35757;&#32451;&#31639;&#27861;Left-over Lunch RL &#65288;LoL-RL&#65289;&#65292;&#20351;&#29992;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#23398;&#20064;&#20219;&#20309;&#24207;&#21015;&#21040;&#24207;&#21015;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#20248;&#21270;LM&#25928;&#29992;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#29992;&#25143;&#23450;&#20041;&#30340;&#36136;&#37327;&#25110;&#39118;&#26684;&#38480;&#21046;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20856;&#22411;&#30340;&#26041;&#27861;&#21253;&#25324;&#23398;&#20064;&#39069;&#22806;&#30340;&#20154;&#24037;&#32534;&#20889;&#25968;&#25454;&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#36807;&#28388;&#8220;&#20302;&#36136;&#37327;&#8221;&#25968;&#25454;&#21644;/&#25110;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#20307;&#21453;&#39304;&#65288;RLHF&#65289;&#12290;&#28982;&#32780;&#65292;&#36807;&#28388;&#20250;&#21024;&#38500;&#26377;&#20215;&#20540;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#32780;&#25968;&#25454;&#25910;&#38598;&#21644;RLHF&#19981;&#26029;&#38656;&#35201;&#39069;&#22806;&#30340;&#20154;&#24037;&#32534;&#20889;&#25110;LM&#25506;&#32034;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#25104;&#26412;&#39640;&#12290;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#26159;&#8220;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;RL&#26469;&#20248;&#21270;&#29616;&#26377;&#30340;&#20247;&#21253;&#21644;&#20114;&#32852;&#32593;&#25968;&#25454;&#19978;&#30340;LM&#25928;&#29992;&#21527;&#65311;&#8221;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21097;&#20313;&#21320;&#39184;&#24378;&#21270;&#23398;&#20064;&#65288;LoL-RL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#30340;&#35757;&#32451;&#31639;&#27861;&#65292;&#20351;&#29992;&#31163;&#32447;&#31574;&#30053;&#26799;&#24230;&#26469;&#23398;&#20064;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20316;&#20026;1&#27493;RL&#28216;&#25103;&#12290; LoL-RL&#21487;&#20197;&#24494;&#35843;LM&#65292;&#20197;&#20248;&#21270;&#20219;&#24847;&#22522;&#20110;&#20998;&#31867;&#22120;&#25110;&#20154;&#23450;&#20041;&#30340;&#25928;&#29992;&#20989;&#25968;&#30340;&#20219;&#20309;&#24207;&#21015;&#21040;&#24207;&#21015;&#25968;&#25454;&#12290;&#20351;&#29992;&#19981;&#21516;&#22823;&#23567;&#27169;&#22411;&#30340;&#20116;&#20010;&#19981;&#21516;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Improving language model generations according to some user-defined quality or style constraints is challenging. Typical approaches include learning on additional human-written data, filtering ``low-quality'' data using heuristics and/or using reinforcement learning with human feedback (RLHF). However, filtering can remove valuable training signals, whereas data collection and RLHF constantly require additional human-written or LM exploration data which can be costly to obtain. A natural question to ask is ``Can we leverage RL to optimize LM utility on existing crowd-sourced and internet data?''  To this end, we present Left-over Lunch RL (LoL-RL), a simple training algorithm that uses offline policy gradients for learning language generation tasks as a 1-step RL game. LoL-RL can finetune LMs to optimize arbitrary classifier-based or human-defined utility functions on any sequence-to-sequence data. Experiments with five different language generation tasks using models of varying sizes 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.03123</link><description>&lt;p&gt;
ChatGPT &#38656;&#35201;&#36827;&#34892;SPADE&#65288;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65289;&#35780;&#20272;&#65306;&#19968;&#39033;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#21478;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#24615;&#33021;&#21644;&#26377;&#25928;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#22312;&#30740;&#31350;&#21644;&#24037;&#19994;&#30028;&#20013;&#24471;&#21040;&#20102;&#24040;&#22823;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21457;&#34920;&#65292;&#20197;&#23637;&#31034;ChatGPT&#21644;&#20854;&#20182;LLMs&#30340;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#12289;&#38598;&#25104;&#21644;&#24773;&#24863;&#12290;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#22823;&#22810;&#25968;&#34987;&#24573;&#35270;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#21363;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65292;&#24182;&#24314;&#35758;&#19981;&#20165;&#20165;&#26159;ChatGPT&#65292;&#32780;&#26159;&#22312;&#23545;&#35805;&#26426;&#22120;&#20154;&#31867;&#21035;&#20013;&#30340;&#27599;&#19968;&#20010;&#21518;&#32493;&#20837;&#21475;&#37117;&#24212;&#35813;&#36827;&#34892;SPADE&#35780;&#20272;&#12290;&#26412;&#25991;&#35814;&#32454;&#35752;&#35770;&#20102;&#20851;&#20110;ChatGPT&#30340;&#38382;&#39064;&#21644;&#20851;&#27880;&#28857;&#19982;&#19978;&#36848;&#29305;&#24449;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#21021;&#27493;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21487;&#35270;&#21270;&#20197;&#21450;&#20551;&#35774;&#30340;&#20107;&#23454;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36824;&#20026;&#27599;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is another large language model (LLM) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. Recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatGPT and other LLMs. In contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This paper discusses in detail about the issues and concerns raised over chatGPT in line with aforementioned characteristics. We support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. We also suggest mitigations and recommendations for each of the concerns. Furthermore, we also s
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20102;&#35821;&#35328;&#29305;&#24449;&#23545;&#22810;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21021;&#27493;&#25552;&#20379;&#20102;&#26041;&#27861;&#20197;&#22686;&#24378;&#23545;&#35821;&#35328;&#19978;&#30456;&#36317;&#36739;&#36828;&#30340;&#35821;&#35328;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.02151</link><description>&lt;p&gt;
&#35821;&#35328;&#36317;&#31163;&#19982;&#22810;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#30340;&#30456;&#20851;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Identifying the Correlation Between Language Distance and Cross-Lingual Transfer in a Multilingual Representation Space. (arXiv:2305.02151v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02151
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#35821;&#35328;&#29305;&#24449;&#23545;&#22810;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21021;&#27493;&#25552;&#20379;&#20102;&#26041;&#27861;&#20197;&#22686;&#24378;&#23545;&#35821;&#35328;&#19978;&#30456;&#36317;&#36739;&#36828;&#30340;&#35821;&#35328;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#35752;&#20102;&#19981;&#21516;&#35821;&#35328;&#29305;&#24449;&#23545;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#31181;&#25928;&#24212;&#22914;&#20309;&#26144;&#23556;&#21040;&#34920;&#31034;&#31354;&#38388;&#20013;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24494;&#35843;&#26399;&#38388;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#23545;&#40784;&#19978;&#30340;&#24433;&#21709;&#65292;&#32780;&#26412;&#30740;&#31350;&#30740;&#31350;&#30340;&#26159;&#30001;MLLMs&#29983;&#25104;&#30340;&#30456;&#24212;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#30340;&#32477;&#23545;&#28436;&#21464;&#12290;&#25105;&#20204;&#29305;&#21035;&#24378;&#35843;&#35821;&#35328;&#29305;&#24449;&#30340;&#20316;&#29992;&#65292;&#24182;&#35843;&#26597;&#20854;&#19982;&#34920;&#31034;&#31354;&#38388;&#21644;&#36328;&#35821;&#35328;&#20256;&#36882;&#24615;&#33021;&#30340;&#24433;&#21709;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#21021;&#27493;&#35777;&#25454;&#65292;&#35828;&#26126;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#22686;&#24378;&#23545;&#35821;&#35328;&#19978;&#30456;&#36317;&#36739;&#36828;&#30340;&#35821;&#35328;&#30340;&#20256;&#36882;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior research has investigated the impact of various linguistic features on cross-lingual transfer performance. In this study, we investigate the manner in which this effect can be mapped onto the representation space. While past studies have focused on the impact on cross-lingual alignment in multilingual language models during fine-tuning, this study examines the absolute evolution of the respective language representation spaces produced by MLLMs. We place a specific emphasis on the role of linguistic characteristics and investigate their inter-correlation with the impact on representation spaces and cross-lingual transfer performance. Additionally, this paper provides preliminary evidence of how these findings can be leveraged to enhance transfer to linguistically distant languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#36328;&#35821;&#35328;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;InfoCTM&#65292;&#36890;&#36807;&#20027;&#39064;&#23545;&#40784;&#26041;&#27861;&#35268;&#33539;&#21270;&#20027;&#39064;&#29983;&#25104;&#24182;&#23547;&#25214;&#26356;&#22810;&#38142;&#25509;&#30340;&#36328;&#35821;&#35328;&#35789;&#27719;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#37325;&#22797;&#20027;&#39064;&#21644;&#20302;&#35206;&#30422;&#23383;&#20856;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.03544</link><description>&lt;p&gt;
InfoCTM: &#19968;&#31181;&#22522;&#20110;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#36328;&#35821;&#35328;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InfoCTM: A Mutual Information Maximization Perspective of Cross-Lingual Topic Modeling. (arXiv:2304.03544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#36328;&#35821;&#35328;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;InfoCTM&#65292;&#36890;&#36807;&#20027;&#39064;&#23545;&#40784;&#26041;&#27861;&#35268;&#33539;&#21270;&#20027;&#39064;&#29983;&#25104;&#24182;&#23547;&#25214;&#26356;&#22810;&#38142;&#25509;&#30340;&#36328;&#35821;&#35328;&#35789;&#27719;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#37325;&#22797;&#20027;&#39064;&#21644;&#20302;&#35206;&#30422;&#23383;&#20856;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#20027;&#39064;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#25991;&#26412;&#20998;&#26512;&#20013;&#20351;&#29992;&#24191;&#27867;&#65292;&#21487;&#20197;&#25581;&#31034;&#23545;&#40784;&#30340;&#28508;&#22312;&#20027;&#39064;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#20135;&#29983;&#37325;&#22797;&#20027;&#39064;&#30340;&#38382;&#39064;&#20197;&#21450;&#30001;&#20110;&#20302;&#35206;&#30422;&#23383;&#20856;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#36328;&#35821;&#35328;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;InfoCTM&#12290;&#19982;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#30452;&#25509;&#23545;&#40784;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20114;&#20449;&#24687;&#30340;&#20027;&#39064;&#23545;&#40784;&#26041;&#27861;&#12290;&#36825;&#21487;&#20316;&#20026;&#35268;&#33539;&#21270;&#65292;&#20197;&#27491;&#30830;&#23545;&#40784;&#20027;&#39064;&#24182;&#38450;&#27490;&#35789;&#30340;&#36864;&#21270;&#20027;&#39064;&#34920;&#31034;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#37325;&#22797;&#20027;&#39064;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#20302;&#35206;&#30422;&#23383;&#20856;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#35789;&#27719;&#38142;&#25509;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32473;&#23450;&#23383;&#20856;&#30340;&#32763;&#35793;&#20197;&#22806;&#65292;&#23547;&#25214;&#26356;&#22810;&#38142;&#25509;&#30340;&#36328;&#35821;&#35328;&#35789;&#27719;&#36827;&#34892;&#20027;&#39064;&#23545;&#40784;&#12290;&#22312;&#33521;&#35821;&#65292;&#20013;&#25991;&#21644;&#26085;&#35821;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#32447;&#65292;&#20135;&#29983;&#26356;&#22810;&#22810;&#26679;&#21270;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#20027;&#39064;&#65292;&#21516;&#26102;&#19981;&#25439;&#23475;&#36328;&#35821;&#35328;&#23545;&#40784;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual topic models have been prevalent for cross-lingual text analysis by revealing aligned latent topics. However, most existing methods suffer from producing repetitive topics that hinder further analysis and performance decline caused by low-coverage dictionaries. In this paper, we propose the Cross-lingual Topic Modeling with Mutual Information (InfoCTM). Instead of the direct alignment in previous work, we propose a topic alignment with mutual information method. This works as a regularization to properly align topics and prevent degenerate topic representations of words, which mitigates the repetitive topic issue. To address the low-coverage dictionary issue, we further propose a cross-lingual vocabulary linking method that finds more linked cross-lingual words for topic alignment beyond the translations of a given dictionary. Extensive experiments on English, Chinese, and Japanese datasets demonstrate that our method outperforms state-of-the-art baselines, producing more
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;&#12290;&#36890;&#36807;&#25910;&#38598;&#34987;&#32534;&#36753;&#22270;&#20687;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#25429;&#25417;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#38480;&#21046;&#25152;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.09618</link><description>&lt;p&gt;
HIVE&#65306;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
HIVE: Harnessing Human Feedback for Instructional Visual Editing. (arXiv:2303.09618v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;&#12290;&#36890;&#36807;&#25910;&#38598;&#34987;&#32534;&#36753;&#22270;&#20687;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#25429;&#25417;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#38480;&#21046;&#25152;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#20154;&#31867;&#21453;&#39304;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#23545;&#40784;&#21040;&#20154;&#31867;&#20559;&#22909;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20551;&#35774;&#65292;&#26368;&#20808;&#36827;&#30340;&#25351;&#23548;&#24615;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#65292;&#20854;&#36755;&#20986;&#22522;&#20110;&#36755;&#20837;&#22270;&#20687;&#21644;&#32534;&#36753;&#25351;&#20196;&#65292;&#21516;&#26679;&#21487;&#20197;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#21463;&#30410;&#65292;&#22240;&#20026;&#20854;&#36755;&#20986;&#21487;&#33021;&#19981;&#31526;&#21512;&#29992;&#25143;&#30340;&#27491;&#30830;&#25351;&#20196;&#21644;&#20559;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#25351;&#23548;&#24615;&#35270;&#35273;&#32534;&#36753;&#65288;HIVE&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#32534;&#36753;&#30340;&#22270;&#20687;&#19978;&#25910;&#38598;&#20154;&#31867;&#21453;&#39304;&#24182;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#20197;&#25429;&#25417;&#22522;&#30784;&#29992;&#25143;&#20559;&#22909;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#21487;&#25193;&#23637;&#30340;&#25193;&#25955;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#26681;&#25454;&#20272;&#35745;&#30340;&#22870;&#21169;&#20540;&#34701;&#20837;&#20154;&#31867;&#20559;&#22909;&#12290;&#27492;&#22806;&#65292;&#20026;&#20943;&#36731;&#25968;&#25454;&#38480;&#21046;&#24102;&#26469;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#36129;&#29486;&#20102;1M&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;3.6K&#22870;&#21169;&#25968;&#25454;&#38598;&#20197;&#29992;&#20110;&#22870;&#21169;&#23398;&#20064;&#65292;&#20197;&#21450;1K&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#25351;&#23548;&#24615;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incorporating human feedback has been shown to be crucial to align text generated by large language models to human preferences. We hypothesize that state-of-the-art instructional image editing models, where outputs are generated based on an input image and an editing instruction, could similarly benefit from human feedback, as their outputs may not adhere to the correct instructions and preferences of users. In this paper, we present a novel framework to harness human feedback for instructional visual editing (HIVE). Specifically, we collect human feedback on the edited images and learn a reward function to capture the underlying user preferences. We then introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward. Besides, to mitigate the bias brought by the limitation of data, we contribute a new 1M training dataset, a 3.6K reward dataset for rewards learning, and a 1K evaluation dataset to boost the performance of inst
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;AdaPTGen&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#25552;&#31034;&#27169;&#26495;&#35843;&#25972;&#20026;&#27169;&#22411;&#25152;&#38656;&#65292;&#26469;&#35299;&#20915;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#35813;&#26694;&#26550;&#27880;&#20837;&#20102;&#24120;&#35268;&#34920;&#26684;&#30456;&#20851;&#25551;&#36848;&#30340;&#34920;&#31034;&#65292;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#24182;&#20801;&#35768;&#35774;&#35745;&#21508;&#31181;&#20219;&#21153;&#26469;&#25506;&#32034;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2302.12468</link><description>&lt;p&gt;
&#20026;&#23569;&#25968;&#25454;&#26679;&#26412;&#30340;&#34920;&#26684;&#29983;&#25104;&#33258;&#36866;&#24212;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Adapting Prompt for Few-shot Table-to-Text Generation. (arXiv:2302.12468v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12468
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;AdaPTGen&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#25552;&#31034;&#27169;&#26495;&#35843;&#25972;&#20026;&#27169;&#22411;&#25152;&#38656;&#65292;&#26469;&#35299;&#20915;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#35813;&#26694;&#26550;&#27880;&#20837;&#20102;&#24120;&#35268;&#34920;&#26684;&#30456;&#20851;&#25551;&#36848;&#30340;&#34920;&#31034;&#65292;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#24182;&#20801;&#35768;&#35774;&#35745;&#21508;&#31181;&#20219;&#21153;&#26469;&#25506;&#32034;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#34920;&#26684;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#24456;&#38590;&#24357;&#21512;&#34920;&#26684;&#25968;&#25454;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#36317;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#26377;&#38480;&#36164;&#28304;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65306;&#33258;&#36866;&#24212;&#29983;&#25104;&#25552;&#31034;&#65288;AdaPTGen&#65289;&#12290;AdaPTGen&#30340;&#26680;&#24515;&#26159;&#23558;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#25552;&#31034;&#27169;&#26495;&#35843;&#25972;&#20026;&#27169;&#22411;&#25152;&#38656;&#65292;&#24102;&#26469;&#20102;&#33267;&#23569;&#19977;&#20010;&#22909;&#22788;&#65306;&#65288;1&#65289;&#23427;&#27880;&#20837;&#20102;&#24120;&#35268;&#34920;&#26684;&#30456;&#20851;&#25551;&#36848;&#30340;&#34920;&#31034;&#65292;&#20197;&#24357;&#21512;&#34920;&#26684;&#25968;&#25454;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#36317;&#65307;&#65288;2&#65289;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;PLMs&#32570;&#20047;&#39046;&#22495;&#30693;&#35782;&#30340;&#22266;&#26377;&#32570;&#28857;&#65307;&#65288;3&#65289;&#23427;&#20801;&#35768;&#25105;&#20204;&#35774;&#35745;&#21508;&#31181;&#20219;&#21153;&#26469;&#25506;&#32034;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#22312;&#19977;&#20010;&#24320;&#25918;&#39046;&#22495;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (PLMs) have made remarkable progress in table-to-text generation tasks. However, the lack of domain-specific knowledge makes it challenging to bridge the topological gap between tabular data and text, especially in real-world applications with limited resources. To mitigate the limitation of insufficient labeled data, we propose a novel framework: Adapt-Prompt-to-Generate (AdaPTGen). The core insight of AdaPTGen is to adapt prompt templates of domain-specific knowledge into the model, which brings at least three benefits: (1) it injects representation of normal table-related descriptions to bridge the topological gap between tabular data and texts; (2) it enables us to use large amounts of unlabeled domain-specific knowledge fully, which can alleviate the PLMs' inherent shortcomings of lacking domain knowledge; (3) it allows us to design various tasks to explore the domain-specific knowledge. Extensive experiments and analyses are conducted on three open-doma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;16&#20010;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#21644;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;2.3&#65285;&#33267;26.7&#65285;&#30340;&#25991;&#31456;&#23558;&#20027;&#39064;&#24402;&#22240;&#20110;&#30005;&#25253;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2301.10856</link><description>&lt;p&gt;
&#37096;&#20998;&#21160;&#21592;&#65306;&#36319;&#36394;&#20420;&#32599;&#26031;&#23186;&#20307;&#21644;&#30005;&#25253;&#20043;&#38388;&#30340;&#22810;&#35821;&#35328;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram. (arXiv:2301.10856v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;16&#20010;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#21644;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;2.3&#65285;&#33267;26.7&#65285;&#30340;&#25991;&#31456;&#23558;&#20027;&#39064;&#24402;&#22240;&#20110;&#30005;&#25253;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20420;&#32599;&#26031;&#20837;&#20405;&#20044;&#20811;&#20848;&#21518;&#65292;&#38024;&#23545;&#20420;&#32599;&#26031;&#22312;&#32447;&#23186;&#20307;&#30340;&#34394;&#20551;&#20449;&#24687;&#21644;&#23459;&#20256;&#65292;&#21253;&#25324;&#20420;&#32599;&#26031;&#20043;&#22768;&#21644;&#21355;&#26143;&#26032;&#38395;&#22312;&#20869;&#30340;&#20420;&#32599;&#26031;&#23186;&#20307;&#22312;&#27431;&#27954;&#36973;&#21040;&#31105;&#27490;&#12290;&#20026;&#20102;&#20445;&#25345;&#35266;&#20247;&#25968;&#37327;&#65292;&#35768;&#22810;&#20420;&#32599;&#26031;&#23186;&#20307;&#24320;&#22987;&#22312;&#30005;&#25253;&#31561;&#28040;&#24687;&#26381;&#21153;&#19978;&#22823;&#21147;&#23459;&#20256;&#20854;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;2022&#24180;&#26399;&#38388;16&#23478;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#22914;&#20309;&#19982;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20114;&#21160;&#21644;&#21033;&#29992;&#12290;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;MPNet&#12289;DP-means&#32858;&#31867;&#21644;Hawkes&#36807;&#31243;&#65292;&#25105;&#20204;&#36319;&#36394;&#26032;&#38395;&#32593;&#31449;&#21644;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#21465;&#20107;&#20256;&#25773;&#24773;&#20917;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20182;&#20204;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#12290;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#32593;&#31449;&#20013;&#65292;2.3&#65285;&#65288;ura.news&#65289;&#33267;26.7&#65285;&#65288;ukraina.ru&#65289;&#30340;&#25991;&#31456;&#35752;&#35770;&#20102;&#28304;&#20110;/&#23548;&#33268;&#30005;&#25253;&#27963;&#21160;&#30340;&#20869;&#23481;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#36319;&#36394;&#20010;&#21035;&#20027;&#39064;&#30340;&#25193;&#25955;&#65292;&#25105;&#20204;&#27979;&#37327;&#26032;&#38395;&#32593;&#31449;&#21457;&#34920;&#25991;&#31456;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to disinformation and propaganda from Russian online media following the Russian invasion of Ukraine, Russian outlets including Russia Today and Sputnik News were banned throughout Europe. To maintain viewership, many of these Russian outlets began to heavily promote their content on messaging services like Telegram. In this work, we study how 16 Russian media outlets interacted with and utilized 732 Telegram channels throughout 2022. Leveraging the foundational model MPNet, DP-means clustering, and Hawkes Processes, we trace how narratives spread between news sites and Telegram channels. We show that news outlets not only propagate existing narratives through Telegram, but that they source material from the messaging platform. Across the sites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of articles discuss content that originated/resulted from activity on Telegram. Finally, tracking the spread of individual topics, we measure the rate at which news website
&lt;/p&gt;</description></item></channel></rss>