<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.10225</link><description>&lt;p&gt;
ChatQA: &#26500;&#24314;GPT-4&#32423;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10225
&lt;/p&gt;
&lt;p&gt;
ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatQA&#65292;&#19968;&#31995;&#21015;&#20855;&#26377;GPT-4&#32423;&#21035;&#20934;&#30830;&#24615;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22788;&#29702;&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22810;&#36718;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#24494;&#35843;&#65292;&#36825;&#26679;&#21487;&#20197;&#25552;&#20379;&#19982;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ChatQA-70B&#21487;&#20197;&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#20998;&#19978;&#36229;&#36807;GPT-4&#65288;54.14 vs. 53.90&#65289;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;OpenAI GPT&#27169;&#22411;&#30340;&#20219;&#20309;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MM-&#20132;&#38169;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20132;&#38169;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#21644;&#22810;&#22270;&#20687;&#29305;&#24449;&#21516;&#27493;&#22120;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#25429;&#25417;&#22270;&#20687;&#32454;&#33410;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#31471;&#21040;&#31471;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10208</link><description>&lt;p&gt;
MM-&#20132;&#38169;&#30340;&#65306;&#36890;&#36807;&#22810;&#27169;&#24335;&#29305;&#24449;&#21516;&#27493;&#22120;&#36827;&#34892;&#20132;&#38169;&#22270;&#20687;-&#25991;&#26412;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer. (arXiv:2401.10208v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MM-&#20132;&#38169;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20132;&#38169;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#21644;&#22810;&#22270;&#20687;&#29305;&#24449;&#21516;&#27493;&#22120;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#25429;&#25417;&#22270;&#20687;&#32454;&#33410;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#31471;&#21040;&#31471;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#24494;&#35843;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;&#20854;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20132;&#38169;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#24320;&#21457;&#20855;&#26377;&#30740;&#31350;&#21644;&#23454;&#38469;&#20215;&#20540;&#12290;&#23427;&#35201;&#27714;&#27169;&#22411;&#29702;&#35299;&#20132;&#38169;&#30340;&#24207;&#21015;&#65292;&#24182;&#38543;&#21518;&#29983;&#25104;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23581;&#35797;&#21463;&#21040;&#20102;&#22266;&#23450;&#25968;&#37327;&#30340;&#35270;&#35273;&#26631;&#35760;&#19981;&#33021;&#26377;&#25928;&#25429;&#25417;&#22270;&#20687;&#32454;&#33410;&#30340;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#22312;&#22810;&#22270;&#20687;&#22330;&#26223;&#20013;&#65292;&#36825;&#19968;&#38382;&#39064;&#23588;&#20026;&#20005;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;MM-&#20132;&#38169;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20132;&#38169;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#27169;&#22411;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#21644;&#22810;&#22270;&#20687;&#29305;&#24449;&#21516;&#27493;&#22120;&#27169;&#22359;&#65292;&#20801;&#35768;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30452;&#25509;&#35775;&#38382;&#20808;&#21069;&#19978;&#19979;&#25991;&#20013;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#29305;&#24449;&#12290;MM-&#20132;&#38169;&#22312;&#37197;&#23545;&#21644;&#20132;&#38169;&#22270;&#20687;-&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#39044;&#35757;&#32451;&#12290;&#23427;&#36824;&#36890;&#36807;&#19968;&#38454;&#27573;&#30340;&#30417;&#30563;&#24494;&#35843;&#26469;&#36827;&#19968;&#27493;&#25913;&#21892;&#20854;&#36981;&#24490;&#22797;&#26434;&#22810;&#27169;&#24577;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;MM-&#20132;&#38169;&#22312;&#22270;&#20687;&#20462;&#22797;&#12289;&#22270;&#20687;&#29983;&#25104;&#21644;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#31561;&#20219;&#21153;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing generative models for interleaved image-text data has both research and practical value. It requires models to understand the interleaved sequences and subsequently generate images and text. However, existing attempts are limited by the issue that the fixed number of visual tokens cannot efficiently capture image details, which is particularly problematic in the multi-image scenarios. To address this, this paper presents MM-Interleaved, an end-to-end generative model for interleaved image-text data. It introduces a multi-scale and multi-image feature synchronizer module, allowing direct access to fine-grained image features in the previous context during the generation process. MM-Interleaved is end-to-end pre-trained on both paired and interleaved image-text corpora. It is further enhanced through a supervised fine-tuning phase, wherein the model improves its ability to follow complex multi-modal instructions. Experiments demonstrate the versatility of MM-Interleaved in rec
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10189</link><description>&lt;p&gt;
Chem-FINESE: &#36890;&#36807;&#25991;&#26412;&#37325;&#26500;&#39564;&#35777;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10189
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Chem-FINESE&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#21270;&#23398;&#39046;&#22495;&#20013;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#21644;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#26469;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#24182;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21270;&#23398;&#39046;&#22495;&#20013;&#65292;&#32454;&#31890;&#24230;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#38754;&#20020;&#20004;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#19982;&#19968;&#33324;&#39046;&#22495;&#30340;&#23454;&#20307;&#25552;&#21462;&#20219;&#21153;&#30456;&#27604;&#65292;&#21270;&#23398;&#35770;&#25991;&#20013;&#30340;&#21477;&#23376;&#36890;&#24120;&#21253;&#21547;&#26356;&#22810;&#30340;&#23454;&#20307;&#12290;&#27492;&#22806;&#65292;&#23454;&#20307;&#25552;&#21462;&#27169;&#22411;&#36890;&#24120;&#38590;&#20197;&#25552;&#21462;&#38271;&#23614;&#31867;&#22411;&#30340;&#23454;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23569;&#26679;&#26412;&#23454;&#20307;&#25552;&#21462;&#26041;&#27861;Chem-FINESE&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;Chem-FINESE&#21253;&#21547;&#20004;&#20010;&#32452;&#20214;&#65306;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#23454;&#20307;&#25552;&#21462;&#22120;&#29992;&#20110;&#20174;&#36755;&#20837;&#21477;&#23376;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#65292;&#20197;&#21450;&#19968;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#29992;&#20110;&#20174;&#25552;&#21462;&#30340;&#23454;&#20307;&#20013;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#21463;&#21040;&#19968;&#20010;&#22909;&#30340;&#23454;&#20307;&#25552;&#21462;&#31995;&#32479;&#38656;&#35201;&#24544;&#23454;&#25552;&#21462;&#23454;&#20307;&#30340;&#20107;&#23454;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26032;&#33258;&#25105;&#39564;&#35777;&#27169;&#22359;&#21033;&#29992;&#23454;&#20307;&#25552;&#21462;&#32467;&#26524;&#26469;&#37325;&#26500;&#21407;&#22987;&#36755;&#20837;&#21477;&#23376;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#26469;&#20943;&#23569;&#22312;&#25552;&#21462;&#36807;&#31243;&#20013;&#30340;&#36807;&#24230;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained few-shot entity extraction in the chemical domain faces two unique challenges. First, compared with entity extraction tasks in the general domain, sentences from chemical papers usually contain more entities. Moreover, entity extraction models usually have difficulty extracting entities of long-tailed types. In this paper, we propose Chem-FINESE, a novel sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to address these two challenges. Our Chem-FINESE has two components: a seq2seq entity extractor to extract named entities from the input sentence and a seq2seq self-validation module to reconstruct the original input sentence from extracted entities. Inspired by the fact that a good entity extraction system needs to extract entities faithfully, our new self-validation module leverages entity extraction results to reconstruct the original input sentence. Besides, we design a new contrastive loss to reduce excessive copying during the extraction proces
&lt;/p&gt;</description></item><item><title>&#24320;&#25918;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#33021;&#22815;&#20174;&#21508;&#31181;&#26631;&#20934;&#25968;&#25454;&#26684;&#24335;&#20013;&#29983;&#25104;&#27969;&#30021;&#21644;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#20294;&#26159;&#36755;&#20986;&#30340;&#35821;&#20041;&#20934;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10186</link><description>&lt;p&gt;
&#36229;&#36234;&#22522;&#20110;&#21442;&#32771;&#25351;&#26631;&#65306;&#20998;&#26512;&#24320;&#25918;&#24335;LLM&#22312;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#19978;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on Data-to-Text Generation. (arXiv:2401.10186v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10186
&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#33021;&#22815;&#20174;&#21508;&#31181;&#26631;&#20934;&#25968;&#25454;&#26684;&#24335;&#20013;&#29983;&#25104;&#27969;&#30021;&#21644;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#20294;&#26159;&#36755;&#20986;&#30340;&#35821;&#20041;&#20934;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#24320;&#25918;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#29983;&#25104;&#36830;&#36143;&#21644;&#30456;&#20851;&#25991;&#26412;&#26041;&#38754;&#30340;&#31243;&#24230;&#12290;&#20026;&#20102;&#38450;&#27490;&#22522;&#20934;&#27844;&#38706;&#21040;LLM&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;Quintd-1:&#19968;&#20010;&#20026;5&#20010;&#25968;&#25454;&#21040;&#25991;&#26412;(D2T)&#29983;&#25104;&#20219;&#21153;&#35774;&#35745;&#30340;&#19987;&#38376;&#22522;&#20934;&#65292;&#35813;&#20219;&#21153;&#21253;&#25324;&#20174;&#20844;&#20849;API&#20013;&#25910;&#38598;&#30340;&#26631;&#20934;&#26684;&#24335;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#35760;&#24405;&#12290;&#25105;&#20204;&#21033;&#29992;&#26080;&#21442;&#32771;&#35780;&#20272;&#25351;&#26631;&#21644;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#27809;&#26377;&#20154;&#24037;&#20889;&#20316;&#21442;&#32771;&#36164;&#26009;&#30340;&#24773;&#20917;&#19979;&#27979;&#35797;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#37325;&#28857;&#26159;&#22312;token&#32423;&#21035;&#19978;&#23545;&#35821;&#20041;&#20934;&#30830;&#24615;&#38169;&#35823;&#36827;&#34892;&#27880;&#37322;&#65292;&#32467;&#21512;&#20154;&#31867;&#26631;&#27880;&#32773;&#21644;&#22522;&#20110;GPT-4&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;7B&#21442;&#25968;&#30340;&#26368;&#20808;&#36827;&#24320;&#25918;&#24335;LLMs&#21487;&#20197;&#22312;&#38646;-shot&#35774;&#32622;&#20013;&#20174;&#21508;&#31181;&#26631;&#20934;&#25968;&#25454;&#26684;&#24335;&#20013;&#29983;&#25104;&#27969;&#30021;&#21644;&#36830;&#36143;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#34920;&#26126;&#36755;&#20986;&#30340;&#35821;&#20041;&#20934;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#38382;&#39064;&#65306;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#65292;80%&#30340;&#36755;&#20986;&#23384;&#22312;&#35821;&#20041;&#20934;&#30830;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate to which extent open large language models (LLMs) can generate coherent and relevant text from structured data. To prevent bias from benchmarks leaked into LLM training data, we collect Quintd-1: an ad-hoc benchmark for five data-to-text (D2T) generation tasks, consisting of structured data records in standard formats gathered from public APIs. We leverage reference-free evaluation metrics and LLMs' in-context learning capabilities, allowing us to test the models with no human-written references. Our evaluation focuses on annotating semantic accuracy errors on token-level, combining human annotators and a metric based on GPT-4. Our systematic examination of the models' behavior across domains and tasks suggests that state-of-the-art open LLMs with 7B parameters can generate fluent and coherent text from various standard data formats in zero-shot settings. However, we also show that semantic accuracy of the outputs remains a major issue: on our benchmark, 80% of outputs o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#65292;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.10134</link><description>&lt;p&gt;
&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatial-Temporal Large Language Model for Traffic Prediction. (arXiv:2401.10134v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#65292;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#21382;&#21490;&#25968;&#25454;&#26469;&#39044;&#27979;&#29305;&#23450;&#20301;&#32622;&#30340;&#26410;&#26469;&#20132;&#36890;&#24773;&#20917;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#24378;&#35843;&#24320;&#21457;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20294;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#24182;&#26410;&#30456;&#24212;&#25552;&#39640;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#19982;&#29616;&#26377;&#27169;&#22411;&#19981;&#21516;&#65292;LLMs&#20027;&#35201;&#36890;&#36807;&#21442;&#25968;&#25193;&#23637;&#21644;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#26469;&#36827;&#27493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#22522;&#26412;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31354;&#38388;-&#26102;&#38388;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;ST-LLM&#65289;&#29992;&#20110;&#20132;&#36890;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ST-LLM&#23558;&#27599;&#20010;&#20301;&#32622;&#30340;&#26102;&#38388;&#27493;&#38271;&#23450;&#20041;&#20026;&#26631;&#35760;&#65292;&#24182;&#32467;&#21512;&#31354;&#38388;-&#26102;&#38388;&#23884;&#20837;&#27169;&#22359;&#26469;&#23398;&#20064;&#26631;&#35760;&#30340;&#31354;&#38388;&#20301;&#32622;&#21644;&#20840;&#23616;&#26102;&#38388;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#34920;&#31034;&#34987;&#34701;&#21512;&#20197;&#20026;&#27599;&#20010;&#26631;&#35760;&#25552;&#20379;&#32479;&#19968;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic prediction, a critical component for intelligent transportation systems, endeavors to foresee future traffic at specific locations using historical data. Although existing traffic prediction models often emphasize developing complex neural network structures, their accuracy has not seen improvements accordingly. Recently, Large Language Models (LLMs) have shown outstanding capabilities in time series analysis. Differing from existing models, LLMs progress mainly through parameter expansion and extensive pre-training while maintaining their fundamental structures. In this paper, we propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic prediction. Specifically, ST-LLM redefines the timesteps at each location as tokens and incorporates a spatial-temporal embedding module to learn the spatial location and global temporal representations of tokens. Then these representations are fused to provide each token with unified spatial and temporal information. Furthermore, we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#36866;&#37197;&#22120;&#21644;Mixup&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#19981;&#38656;&#35201;&#39057;&#32321;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#36866;&#37197;&#22120;&#30340;&#20984;&#32452;&#21512;&#21644;&#38750;&#25968;&#25454;&#23545;&#30340;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#22312;&#24615;&#33021;&#19979;&#38477;&#21644;&#35745;&#31639;&#24320;&#38144;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.10111</link><description>&lt;p&gt;
&#23558;&#36866;&#37197;&#22120;&#21644;Mixup&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Marrying Adapters and Mixup to Efficiently Enhance the Adversarial Robustness of Pre-Trained Language Models for Text Classification. (arXiv:2401.10111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#36866;&#37197;&#22120;&#21644;Mixup&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#19981;&#38656;&#35201;&#39057;&#32321;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#36866;&#37197;&#22120;&#30340;&#20984;&#32452;&#21512;&#21644;&#38750;&#25968;&#25454;&#23545;&#30340;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#22312;&#24615;&#33021;&#19979;&#38477;&#21644;&#35745;&#31639;&#24320;&#38144;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#26679;&#26412;&#26469;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#20854;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35757;&#32451;&#26041;&#27861;&#24448;&#24448;&#20250;&#23548;&#33268;&#23545;&#28165;&#27905;&#36755;&#20837;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#21478;&#22806;&#65292;&#23427;&#38656;&#35201;&#39057;&#32321;&#22320;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#65292;&#20197;&#36866;&#24212;&#26032;&#30340;&#25915;&#20987;&#31867;&#22411;&#65292;&#20174;&#32780;&#23548;&#33268;&#26114;&#36149;&#19988;&#35745;&#31639;&#37327;&#22823;&#30340;&#35745;&#31639;&#12290;&#36825;&#20123;&#38480;&#21046;&#20351;&#24471;&#23545;&#25239;&#35757;&#32451;&#26426;&#21046;&#30340;&#23454;&#38469;&#24212;&#29992;&#21464;&#24471;&#19981;&#22826;&#23454;&#29992;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#25968;&#30334;&#19975;&#29978;&#33267;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22797;&#26434;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#21516;&#26102;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#30340;&#29702;&#35770;&#30410;&#22788;&#65292;&#26412;&#30740;&#31350;&#23558;&#20004;&#20010;&#27010;&#24565;&#30456;&#32467;&#21512;&#65306;&#65288;1&#65289;&#36866;&#37197;&#22120;&#65292;&#21487;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#21644;&#65288;2&#65289;Mixup&#65292;&#36890;&#36807;&#25968;&#25454;&#23545;&#30340;&#20984;&#32452;&#21512;&#35757;&#32451;NNs&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#38750;&#25968;&#25454;&#23545;&#30340;&#36866;&#37197;&#22120;&#30340;&#20984;&#32452;&#21512;&#26469;&#24494;&#35843;PLMs&#65292;&#20854;&#20013;&#19968;&#20010;&#36866;&#37197;&#22120;&#20351;&#29992;&#24178;&#20928;&#30340;&#26679;&#26412;&#35757;&#32451;&#65292;&#21478;&#19968;&#20010;&#20351;&#29992;&#23545;&#25239;&#24615;&#30340;&#26679;&#26412;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing works show that augmenting training data of neural networks using both clean and adversarial examples can enhance their generalizability under adversarial attacks. However, this training approach often leads to performance degradation on clean inputs. Additionally, it requires frequent re-training of the entire model to account for new attack types, resulting in significant and costly computations. Such limitations make adversarial training mechanisms less practical, particularly for complex Pre-trained Language Models (PLMs) with millions or even billions of parameters. To overcome these challenges while still harnessing the theoretical benefits of adversarial training, this study combines two concepts: (1) adapters, which enable parameter-efficient fine-tuning, and (2) Mixup, which train NNs via convex combinations of pairs data pairs. Intuitively, we propose to fine-tune PLMs through convex combinations of non-data pairs of fine-tuned adapters, one trained with clean and an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#27599;&#20010;&#31034;&#20363;&#32454;&#35843;&#19982;&#22235;&#20010;&#23545;&#25239;&#21477;&#23376;&#26469;&#23454;&#29616;&#40065;&#26834;&#30340;&#38405;&#35835;&#29702;&#35299;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#32454;&#35843;&#33021;&#25552;&#39640;&#27169;&#22411;&#22312;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.10091</link><description>&lt;p&gt;
&#25968;&#23383;&#30340;&#21147;&#37327;&#65306;&#36890;&#36807;&#27599;&#20010;&#31034;&#20363;&#32454;&#35843;&#19982;&#22235;&#20010;&#23545;&#25239;&#21477;&#23376;&#26469;&#23454;&#29616;&#40065;&#26834;&#30340;&#38405;&#35835;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Power in Numbers: Robust reading comprehension by finetuning with four adversarial sentences per example. (arXiv:2401.10091v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#27599;&#20010;&#31034;&#20363;&#32454;&#35843;&#19982;&#22235;&#20010;&#23545;&#25239;&#21477;&#23376;&#26469;&#23454;&#29616;&#40065;&#26834;&#30340;&#38405;&#35835;&#29702;&#35299;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#32454;&#35843;&#33021;&#25552;&#39640;&#27169;&#22411;&#22312;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35780;&#20272;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#26102;&#65292;&#26368;&#36817;&#30340;&#27169;&#22411;&#24050;&#32463;&#36798;&#21040;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#20351;&#29992;F1&#20998;&#25968;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#25945;&#26426;&#22120;&#29702;&#35299;&#25991;&#26412;&#36824;&#27809;&#26377;&#35299;&#20915;&#12290;&#36890;&#36807;&#23558;&#19968;&#20010;&#23545;&#25239;&#24615;&#21477;&#23376;&#28155;&#21152;&#21040;&#19978;&#19979;&#25991;&#27573;&#33853;&#20013;&#65292;&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#38405;&#35835;&#29702;&#35299;&#27169;&#22411;&#30340;F1&#20998;&#25968;&#20960;&#20046;&#20943;&#21322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;ELECTRA-Small&#22797;&#21046;&#20102;&#36807;&#21435;&#30340;&#23545;&#25239;&#24615;&#30740;&#31350;&#65292;&#24182;&#35777;&#26126;&#20102;&#26032;&#27169;&#22411;&#30340;F1&#20998;&#25968;&#20174;83.9&#65285;&#19979;&#38477;&#21040;29.2&#65285;&#12290;&#20026;&#20102;&#25552;&#39640;ELECTRA-Small&#23545;&#36825;&#31181;&#25915;&#20987;&#30340;&#25269;&#25239;&#21147;&#65292;&#25105;&#23545;SQuAD v1.1&#35757;&#32451;&#31034;&#20363;&#36827;&#34892;&#20102;&#32454;&#35843;&#65292;&#23558;&#19968;&#21040;&#20116;&#20010;&#23545;&#25239;&#21477;&#23376;&#38468;&#21152;&#21040;&#19978;&#19979;&#25991;&#27573;&#33853;&#20013;&#12290;&#19982;&#36807;&#21435;&#30340;&#30740;&#31350;&#19968;&#26679;&#65292;&#25105;&#21457;&#29616;&#32454;&#35843;&#21518;&#30340;&#27169;&#22411;&#23545;&#19968;&#20010;&#23545;&#25239;&#21477;&#23376;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#20339;&#12290;&#28982;&#32780;&#65292;&#24403;&#32454;&#35843;&#29992;&#20110;&#22235;&#20010;&#25110;&#20116;&#20010;&#23545;&#25239;&#21477;&#23376;&#26102;&#65292;&#35813;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#36229;&#36807;70&#65285;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent models have achieved human level performance on the Stanford Question Answering Dataset when using F1 scores to evaluate the reading comprehension task. Yet, teaching machines to comprehend text has not been solved in the general case. By appending one adversarial sentence to the context paragraph, past research has shown that the F1 scores from reading comprehension models drop almost in half. In this paper, I replicate past adversarial research with a new model, ELECTRA-Small, and demonstrate that the new model's F1 score drops from 83.9% to 29.2%. To improve ELECTRA-Small's resistance to this attack, I finetune the model on SQuAD v1.1 training examples with one to five adversarial sentences appended to the context paragraph. Like past research, I find that the finetuned model on one adversarial sentence does not generalize well across evaluation datasets. However, when finetuned on four or five adversarial sentences the model attains an F1 score of more than 70% on most evalu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;LoRA&#27169;&#22359;&#36827;&#34892;&#23458;&#25143;&#31471;&#35843;&#25972;&#21644;&#19982;&#26381;&#21153;&#22120;&#30340;&#20132;&#20114;&#65292;&#20197;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#65292;&#20197;&#21450;&#20351;&#29992;K&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#20840;&#23616;&#27169;&#22411;&#26469;&#23454;&#29616;&#20010;&#24615;&#21270;&#24182;&#20811;&#26381;&#25968;&#25454;&#24322;&#26500;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10070</link><description>&lt;p&gt;
&#36890;&#20449;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#22312;&#35821;&#38899;&#36716;&#25991;&#26412;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Personalized Federated Learning for Speech-to-Text Tasks. (arXiv:2401.10070v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10070
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;LoRA&#27169;&#22359;&#36827;&#34892;&#23458;&#25143;&#31471;&#35843;&#25972;&#21644;&#19982;&#26381;&#21153;&#22120;&#30340;&#20132;&#20114;&#65292;&#20197;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#65292;&#20197;&#21450;&#20351;&#29992;K&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#30340;&#20840;&#23616;&#27169;&#22411;&#26469;&#23454;&#29616;&#20010;&#24615;&#21270;&#24182;&#20811;&#26381;&#25968;&#25454;&#24322;&#26500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20445;&#25252;&#38544;&#31169;&#24182;&#28385;&#36275;&#27861;&#35268;&#35201;&#27714;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#35757;&#32451;&#35821;&#38899;&#36716;&#25991;&#26412;&#31995;&#32479;&#65288;&#21253;&#25324;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#38899;&#32763;&#35793;&#65289;&#26041;&#38754;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22312;&#35821;&#38899;&#36716;&#25991;&#26412;&#20219;&#21153;&#20013;&#24120;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65288;&#21363;FedAvg&#65289;&#36890;&#24120;&#38754;&#20020;&#30528;&#22823;&#37327;&#30340;&#36890;&#20449;&#24320;&#38144;&#21644;&#25968;&#25454;&#24322;&#26500;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#35821;&#38899;&#36716;&#25991;&#26412;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#36731;&#37327;&#32423;&#30340;LoRA&#27169;&#22359;&#65288;FedLoRA&#65289;&#29992;&#20110;&#23458;&#25143;&#31471;&#35843;&#25972;&#21644;&#19982;&#26381;&#21153;&#22120;&#36827;&#34892;&#20132;&#20114;&#20197;&#26368;&#23567;&#21270;&#36890;&#20449;&#24320;&#38144;&#65292;&#20197;&#21450;&#20840;&#23616;&#27169;&#22411;&#65288;FedMem&#65289;&#37197;&#22791;&#20102;K&#26368;&#36817;&#37051;&#20998;&#31867;&#22120;&#65292;&#20197;&#25429;&#25417;&#23458;&#25143;&#29305;&#23450;&#30340;&#20998;&#24067;&#21464;&#21270;&#20197;&#23454;&#29616;&#20010;&#24615;&#21270;&#24182;&#20811;&#26381;&#25968;&#25454;&#24322;&#26500;&#12290;&#22312;CoVoST&#21644;GigaSp&#25968;&#25454;&#38598;&#19978;&#22522;&#20110;Conformer&#21644;Whisper&#20027;&#24178;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
To protect privacy and meet legal regulations, federated learning (FL) has gained significant attention for training speech-to-text (S2T) systems, including automatic speech recognition (ASR) and speech translation (ST). However, the commonly used FL approach (i.e., \textsc{FedAvg}) in S2T tasks typically suffers from extensive communication overhead due to multi-round interactions based on the whole model and performance degradation caused by data heterogeneity among clients.To address these issues, we propose a personalized federated S2T framework that introduces \textsc{FedLoRA}, a lightweight LoRA module for client-side tuning and interaction with the server to minimize communication overhead, and \textsc{FedMem}, a global model equipped with a $k$-nearest-neighbor ($k$NN) classifier that captures client-specific distributional shifts to achieve personalization and overcome data heterogeneity. Extensive experiments based on Conformer and Whisper backbone models on CoVoST and GigaSp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#35302;&#21457;&#26465;&#20214;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20195;&#30721;&#25552;&#31034;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#20195;&#30721;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.10065</link><description>&lt;p&gt;
&#20195;&#30721;&#25552;&#31034;&#22312;&#25991;&#26412;+&#20195;&#30721;LLMs&#20013;&#24341;&#21457;&#20102;&#26465;&#20214;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs. (arXiv:2401.10065v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#35302;&#21457;&#26465;&#20214;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20195;&#30721;&#25552;&#31034;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#20195;&#30721;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#26159;&#23454;&#29616;&#35821;&#35328;&#29702;&#35299;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#22810;&#31181;&#25512;&#29702;&#31867;&#22411;&#20013;&#65292;&#26465;&#20214;&#25512;&#29702;&#26159;&#19968;&#31181;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#24471;&#20986;&#19981;&#21516;&#32467;&#35770;&#30340;&#33021;&#21147;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#19968;&#30452;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26368;&#36817;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;LLMs&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20110;&#20160;&#20040;&#35302;&#21457;&#20102;LLMs&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#25105;&#20204;&#20551;&#35774;&#20195;&#30721;&#25552;&#31034;&#33021;&#22815;&#35302;&#21457;&#22312;&#25991;&#26412;&#21644;&#20195;&#30721;&#19978;&#35757;&#32451;&#30340;LLMs&#20013;&#30340;&#26465;&#20214;&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#30340;&#25552;&#31034;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#20195;&#30721;&#65292;&#24182;&#29992;&#29983;&#25104;&#30340;&#20195;&#30721;&#25552;&#31034;LLMs&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#38656;&#35201;&#26465;&#20214;&#25512;&#29702;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#20195;&#30721;&#25552;&#31034;&#20351;&#24471;GPT 3.5&#30340;&#24615;&#33021;&#25552;&#21319;&#20102;2.6&#21040;7.7&#20010;&#30334;&#20998;&#28857;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#25506;&#32034;&#20102;&#20195;&#30721;&#25552;&#31034;&#22914;&#20309;&#24341;&#21457;&#26465;&#20214;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;&#36890;&#36807;&#21738;&#20123;&#29305;&#24449;&#36827;&#34892;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#25552;&#31034;&#30340;&#24418;&#24335;&#21644;&#20869;&#23481;&#23545;&#20110;&#24341;&#21457;&#26465;&#20214;&#25512;&#29702;&#33021;&#21147;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning is a fundamental component for achieving language understanding. Among the multiple types of reasoning, conditional reasoning, the ability to draw different conclusions depending on some condition, has been understudied in large language models (LLMs). Recent prompting methods, such as chain of thought, have significantly improved LLMs on reasoning tasks. Nevertheless, there is still little understanding of what triggers reasoning abilities in LLMs. We hypothesize that code prompts can trigger conditional reasoning in LLMs trained on text and code. We propose a chain of prompts that transforms a natural language problem into code and prompts the LLM with the generated code. Our experiments find that code prompts exhibit a performance boost between 2.6 and 7.7 points on GPT 3.5 across multiple datasets requiring conditional reasoning. We then conduct experiments to discover how code prompts elicit conditional reasoning abilities and through which features. We observe that prom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICE-NET&#30340;&#20132;&#32455;&#32534;&#30721;&#22120;&#32593;&#32476;&#65292;&#29992;&#20110;&#21306;&#20998;&#21453;&#20041;&#35789;&#21644;&#21516;&#20041;&#35789;&#12290;&#36890;&#36807;&#25429;&#25417;&#21644;&#27169;&#25311;&#23427;&#20204;&#30340;&#20851;&#31995;&#29305;&#23450;&#23646;&#24615;&#65292;ICE-NET&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30740;&#31350;&#65292;&#25552;&#39640;&#20102;1.8%&#30340;F1&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2401.10045</link><description>&lt;p&gt;
&#20351;&#29992;&#20132;&#32455;&#32534;&#30721;&#22120;&#32593;&#32476;&#65288;ICE-NET&#65289;&#21306;&#20998;&#21453;&#20041;&#35789;&#21644;&#21516;&#20041;&#35789;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Antonym vs Synonym Distinction using InterlaCed Encoder NETworks (ICE-NET). (arXiv:2401.10045v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICE-NET&#30340;&#20132;&#32455;&#32534;&#30721;&#22120;&#32593;&#32476;&#65292;&#29992;&#20110;&#21306;&#20998;&#21453;&#20041;&#35789;&#21644;&#21516;&#20041;&#35789;&#12290;&#36890;&#36807;&#25429;&#25417;&#21644;&#27169;&#25311;&#23427;&#20204;&#30340;&#20851;&#31995;&#29305;&#23450;&#23646;&#24615;&#65292;ICE-NET&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30740;&#31350;&#65292;&#25552;&#39640;&#20102;1.8%&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20041;&#35789;&#21644;&#21516;&#20041;&#35789;&#30340;&#21306;&#20998;&#26159;&#35789;&#27719;-&#35821;&#20041;&#20998;&#26512;&#21644;&#33258;&#21160;&#35789;&#27719;&#36164;&#28304;&#26500;&#24314;&#20013;&#30340;&#26680;&#24515;&#25361;&#25112;&#12290;&#36825;&#20123;&#35789;&#23545;&#20849;&#20139;&#30456;&#20284;&#30340;&#20998;&#24067;&#19978;&#19979;&#25991;&#65292;&#36825;&#20351;&#24471;&#21306;&#20998;&#23427;&#20204;&#26356;&#21152;&#22256;&#38590;&#12290;&#26377;&#20851;&#36825;&#26041;&#38754;&#30340;&#20027;&#35201;&#30740;&#31350;&#35797;&#22270;&#25429;&#25417;&#20851;&#31995;&#23545;&#30340;&#24615;&#36136;&#65292;&#21363;&#23545;&#31216;&#24615;&#12289;&#20256;&#36882;&#24615;&#21644;&#20256;&#36882;-&#20256;&#36882;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#26080;&#27861;&#36866;&#24403;&#22320;&#27169;&#25311;&#20851;&#31995;&#29305;&#23450;&#23646;&#24615;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#26368;&#32456;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#21453;&#20041;&#35789;&#21644;&#21516;&#20041;&#35789;&#21306;&#20998;&#30340;&#20132;&#32455;&#32534;&#30721;&#22120;&#32593;&#32476;&#65288;&#21363;ICE-NET&#65289;&#65292;&#26088;&#22312;&#25429;&#25417;&#21644;&#27169;&#25311;&#21453;&#20041;&#35789;&#21644;&#21516;&#20041;&#35789;&#23545;&#30340;&#20851;&#31995;&#29305;&#23450;&#23646;&#24615;&#65292;&#20197;&#20415;&#20197;&#24615;&#33021;&#22686;&#24378;&#30340;&#26041;&#24335;&#25191;&#34892;&#20998;&#31867;&#20219;&#21153;&#12290;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;ICE-NET&#22312;F1&#24471;&#20998;&#19978;&#30456;&#23545;&#20110;&#29616;&#26377;&#30740;&#31350;&#25552;&#39640;&#20102;&#26368;&#39640;1.8%&#12290;&#25105;&#20204;&#22312;https://github.com/asif6827/ICENET&#19978;&#21457;&#24067;&#20102;ICE-NET&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antonyms vs synonyms distinction is a core challenge in lexico-semantic analysis and automated lexical resource construction. These pairs share a similar distributional context which makes it harder to distinguish them. Leading research in this regard attempts to capture the properties of the relation pairs, i.e., symmetry, transitivity, and trans-transitivity. However, the inability of existing research to appropriately model the relation-specific properties limits their end performance. In this paper, we propose InterlaCed Encoder NETworks (i.e., ICE-NET) for antonym vs synonym distinction, that aim to capture and model the relation-specific properties of the antonyms and synonyms pairs in order to perform the classification task in a performance-enhanced manner. Experimental evaluation using the benchmark datasets shows that ICE-NET outperforms the existing research by a relative score of upto 1.8% in F1-measure. We release the codes for ICE-NET at https://github.com/asif6827/ICENET
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32467;&#26500;&#21270;&#30340;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#65292;&#22312;&#30149;&#27602;&#23398;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#31616;&#27905;&#30340;&#23398;&#26415;&#36129;&#29486;&#25688;&#35201;&#65292;&#23545;&#31185;&#23398;&#23478;&#36827;&#34892;&#23548;&#33322;&#21644;&#35299;&#20915;LLM&#30340;&#32039;&#36843;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.10040</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#38024;&#23545;&#30149;&#27602;&#23398;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Scientific Information Extraction: An Empirical Study for Virology. (arXiv:2401.10040v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10040
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32467;&#26500;&#21270;&#30340;&#31185;&#23398;&#20449;&#24687;&#25552;&#21462;&#65292;&#22312;&#30149;&#27602;&#23398;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#31616;&#27905;&#30340;&#23398;&#26415;&#36129;&#29486;&#25688;&#35201;&#65292;&#23545;&#31185;&#23398;&#23478;&#36827;&#34892;&#23548;&#33322;&#21644;&#35299;&#20915;LLM&#30340;&#32039;&#36843;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20513;&#23548;&#20351;&#29992;&#32467;&#26500;&#21270;&#21644;&#35821;&#20041;&#20869;&#23481;&#34920;&#31034;&#26469;&#36827;&#34892;&#22522;&#20110;&#23398;&#26415;&#20132;&#27969;&#30340;&#23398;&#26415;&#35770;&#25991;&#65292;&#21463;&#21040;&#32500;&#22522;&#30334;&#31185;&#20449;&#24687;&#26694;&#25110;&#32467;&#26500;&#21270;&#30340;&#20122;&#39532;&#36874;&#20135;&#21697;&#25551;&#36848;&#31561;&#24037;&#20855;&#30340;&#21551;&#21457;&#12290;&#36825;&#20123;&#34920;&#31034;&#24418;&#24335;&#25552;&#20379;&#20102;&#31616;&#27905;&#30340;&#27010;&#36848;&#65292;&#24110;&#21161;&#31185;&#23398;&#23478;&#22312;&#27987;&#21402;&#30340;&#23398;&#26415;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#33258;&#21160;&#21270;&#26041;&#27861;&#21033;&#29992;LLM&#30340;&#24378;&#22823;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#20135;&#29983;&#32467;&#26500;&#21270;&#30340;&#23398;&#26415;&#36129;&#29486;&#25688;&#35201;&#65292;&#26082;&#25552;&#20379;&#20102;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#65292;&#20063;&#25581;&#31034;&#20102;LLM&#32039;&#36843;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;LLM&#65292;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#25913;&#21892;&#20854;&#20316;&#20026;&#23545;&#35805;&#20195;&#29702;&#30340;&#36890;&#29992;&#26234;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#27169;&#22411;&#20063;&#21487;&#20197;&#22312;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#20013;&#26377;&#25928;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#31185;&#23398;&#31561;&#39046;&#22495;&#30340;&#22797;&#26434;IE&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#33539;&#24335;&#36716;&#21464;&#29992;&#19968;&#31995;&#21015;&#25351;&#20196;&#20195;&#26367;&#20102;&#20256;&#32479;&#30340;&#27169;&#22359;&#21270;&#12289;&#27969;&#27700;&#32447;&#24335;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#31616;&#21270;&#20102;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#24494;&#35843;&#30340;FLAN-T&#27169;&#22411;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we champion the use of structured and semantic content representation of discourse-based scholarly communication, inspired by tools like Wikipedia infoboxes or structured Amazon product descriptions. These representations provide users with a concise overview, aiding scientists in navigating the dense academic landscape. Our novel automated approach leverages the robust text generation capabilities of LLMs to produce structured scholarly contribution summaries, offering both a practical solution and insights into LLMs' emergent abilities.  For LLMs, the prime focus is on improving their general intelligence as conversational agents. We argue that these models can also be applied effectively in information extraction (IE), specifically in complex IE tasks within terse domains like Science. This paradigm shift replaces the traditional modular, pipelined machine learning approach with a simpler objective expressed through instructions. Our results show that finetuned FLAN-T
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#35745;&#31639;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#36827;&#19968;&#27493;&#25552;&#21319;&#22823;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#21450;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#32467;&#21512;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10034</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#36827;&#21270;&#35745;&#31639;&#65306;&#35843;&#26597;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap. (arXiv:2401.10034v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10034
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#35745;&#31639;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#36827;&#19968;&#27493;&#25552;&#21319;&#22823;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#21450;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#32467;&#21512;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22522;&#20110;Transformer&#26550;&#26500;&#65292;&#22312;&#22810;&#26679;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#65292;&#23427;&#20204;&#19981;&#20165;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#36824;&#23558;&#20854;&#33021;&#21147;&#25193;&#23637;&#21040;&#20102;&#21508;&#20010;&#39046;&#22495;&#65292;&#36808;&#21521;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;&#23613;&#31649;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#19982;LLMs&#22312;&#30446;&#26631;&#21644;&#26041;&#27861;&#35770;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#20294;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25581;&#31034;&#20102;&#26377;&#36259;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#29305;&#21035;&#26159;&#22312;&#20182;&#20204;&#20849;&#21516;&#30340;&#20248;&#21270;&#24615;&#36136;&#12289;&#40657;&#30418;&#29305;&#24615;&#21644;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#26041;&#38754;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#36827;&#21270;&#31639;&#27861;&#19981;&#20165;&#21487;&#20197;&#20026;LLM&#22312;&#40657;&#30418;&#35774;&#32622;&#19979;&#25552;&#20379;&#20248;&#21270;&#26694;&#26550;&#65292;&#36824;&#21487;&#20197;&#22312;&#24212;&#29992;&#20013;&#20026;LLM&#36171;&#20104;&#28789;&#27963;&#30340;&#20840;&#23616;&#25628;&#32034;&#21644;&#36845;&#20195;&#26426;&#21046;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;LLM&#20016;&#23500;&#30340;&#39046;&#22495;&#30693;&#35782;&#20351;&#24471;&#36827;&#21270;&#31639;&#27861;&#21487;&#20197;&#36827;&#34892;&#26356;&#26234;&#33021;&#30340;&#25628;&#32034;&#65292;&#32780;&#20854;&#25991;&#26412;&#22788;&#29702;&#33021;&#21147;&#21017;&#26377;&#21161;&#20110;&#23558;&#36827;&#21270;&#31639;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#12290;&#22522;&#20110;&#23427;&#20204;&#30340;&#20114;&#34917;&#20248;&#21183;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20221;&#35843;&#26597;&#21644;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), built upon Transformer-based architectures with massive pretraining on diverse data, have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and Evolutionary Algorithms (EAs), despite differing in objectives and methodologies, reveals intriguing parallels, especially in their shared optimization nature, black-box characteristics, and proficiency in handling complex problems. Meanwhile, EA can not only provide an optimization framework for LLM's further enhancement under black-box settings but also empower LLM with flexible global search and iterative mechanism in applications. On the other hand, LLM's abundant domain knowledge enables EA to perform smarter searches, while its text processing capability assist in deploying EA across various tasks. Based on their complementary advantages, this paper presents a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#20581;&#24247;&#30456;&#20851;&#21465;&#20107;&#30340;&#26694;&#26550;&#20998;&#26512;&#65292;&#27604;&#36739;&#20102;&#38452;&#35851;&#23186;&#20307;&#19982;&#20027;&#27969;&#23186;&#20307;&#20043;&#38388;&#22312;COVID-19&#31561;&#35805;&#39064;&#19978;&#30340;&#26694;&#26550;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#38452;&#35851;&#23186;&#20307;&#20013;&#65292;&#20581;&#24247;&#30456;&#20851;&#30340;&#21465;&#20107;&#20027;&#35201;&#20197;&#20449;&#20208;&#20026;&#26694;&#26550;&#65292;&#32780;&#20027;&#27969;&#23186;&#20307;&#21017;&#20197;&#31185;&#23398;&#20026;&#26694;&#26550;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#26356;&#28145;&#20837;&#30340;&#26694;&#26550;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.10030</link><description>&lt;p&gt;
&#22522;&#20110;&#20581;&#24247;&#30456;&#20851;&#21465;&#20107;&#30340;&#26694;&#26550;&#20998;&#26512;&#65306;&#38452;&#35851;&#19982;&#20027;&#27969;&#23186;&#20307;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Framing Analysis of Health-Related Narratives: Conspiracy versus Mainstream Media. (arXiv:2401.10030v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#20581;&#24247;&#30456;&#20851;&#21465;&#20107;&#30340;&#26694;&#26550;&#20998;&#26512;&#65292;&#27604;&#36739;&#20102;&#38452;&#35851;&#23186;&#20307;&#19982;&#20027;&#27969;&#23186;&#20307;&#20043;&#38388;&#22312;COVID-19&#31561;&#35805;&#39064;&#19978;&#30340;&#26694;&#26550;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#38452;&#35851;&#23186;&#20307;&#20013;&#65292;&#20581;&#24247;&#30456;&#20851;&#30340;&#21465;&#20107;&#20027;&#35201;&#20197;&#20449;&#20208;&#20026;&#26694;&#26550;&#65292;&#32780;&#20027;&#27969;&#23186;&#20307;&#21017;&#20197;&#31185;&#23398;&#20026;&#26694;&#26550;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#26356;&#28145;&#20837;&#30340;&#26694;&#26550;&#20998;&#26512;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20102;&#35299;&#22312;&#32447;&#23186;&#20307;&#22914;&#20309;&#26694;&#23450;&#38382;&#39064;&#23545;&#20110;&#20854;&#23545;&#20844;&#20247;&#33286;&#35770;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#36827;&#34892;&#30340;&#26694;&#26550;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#28040;&#24687;&#20013;&#30340;&#29305;&#23450;&#20869;&#23481;&#29305;&#24449;&#65292;&#32780;&#24573;&#35270;&#20102;&#20854;&#21465;&#20107;&#35201;&#32032;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#20449;&#24687;&#28304;&#20013;&#30340;&#26694;&#26550;&#24046;&#24322;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#35805;&#39064;&#65288;&#22914;COVID-19&#21644;&#20854;&#20182;&#30142;&#30149;&#65289;&#22312;&#38452;&#35851;&#21644;&#20027;&#27969;&#23186;&#20307;&#20043;&#38388;&#30340;&#26694;&#26550;&#24046;&#24322;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#35821;&#20041;&#22270;&#30340;&#26032;&#22411;&#26694;&#26550;&#25552;&#21462;&#26041;&#27861;&#23558;&#21465;&#20107;&#20449;&#24687;&#32435;&#20837;&#26694;&#26550;&#20998;&#26512;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#38452;&#35851;&#23186;&#20307;&#20013;&#65292;&#19982;&#20581;&#24247;&#30456;&#20851;&#30340;&#21465;&#20107;&#20027;&#35201;&#20197;&#20449;&#20208;&#20026;&#26694;&#26550;&#65292;&#32780;&#20027;&#27969;&#23186;&#20307;&#20542;&#21521;&#20110;&#20197;&#31185;&#23398;&#20026;&#26694;&#26550;&#21576;&#29616;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#20026;&#26356;&#32454;&#33268;&#20837;&#24494;&#30340;&#26694;&#26550;&#20998;&#26512;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding how online media frame issues is crucial due to their impact on public opinion. Research on framing using natural language processing techniques mainly focuses on specific content features in messages and neglects their narrative elements. Also, the distinction between framing in different sources remains an understudied problem. We address those issues and investigate how the framing of health-related topics, such as COVID-19 and other diseases, differs between conspiracy and mainstream websites. We incorporate narrative information into the framing analysis by introducing a novel frame extraction approach based on semantic graphs. We find that health-related narratives in conspiracy media are predominantly framed in terms of beliefs, while mainstream media tend to present them in terms of science. We hope our work offers new ways for a more nuanced frame analysis.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#24049;&#25552;&#20379;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22870;&#21169;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#65292;&#36824;&#21487;&#20197;&#20026;&#33258;&#24049;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#22870;&#21169;&#12290;&#36890;&#36807;&#23545;Llama 2 70B&#27169;&#22411;&#30340;&#19977;&#27425;&#36845;&#20195;&#24494;&#35843;&#65292;&#32467;&#26524;&#22312;AlpacaEval 2.0&#25490;&#34892;&#27036;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#29616;&#26377;&#31995;&#32479;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#23454;&#29616;&#33021;&#22815;&#19981;&#26029;&#33258;&#25105;&#25913;&#36827;&#30340;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10020</link><description>&lt;p&gt;
&#33258;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Self-Rewarding Language Models. (arXiv:2401.10020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10020
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#24049;&#25552;&#20379;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#22870;&#21169;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#65292;&#36824;&#21487;&#20197;&#20026;&#33258;&#24049;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#22870;&#21169;&#12290;&#36890;&#36807;&#23545;Llama 2 70B&#27169;&#22411;&#30340;&#19977;&#27425;&#36845;&#20195;&#24494;&#35843;&#65292;&#32467;&#26524;&#22312;AlpacaEval 2.0&#25490;&#34892;&#27036;&#19978;&#36229;&#36807;&#20102;&#20854;&#20182;&#29616;&#26377;&#31995;&#32479;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#23454;&#29616;&#33021;&#22815;&#19981;&#26029;&#33258;&#25105;&#25913;&#36827;&#30340;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20551;&#35774;&#35201;&#23454;&#29616;&#36229;&#20154;&#32423;&#30340;&#26234;&#33021;&#20307;&#65292;&#26410;&#26469;&#30340;&#27169;&#22411;&#38656;&#35201;&#36229;&#20154;&#32423;&#30340;&#21453;&#39304;&#65292;&#20197;&#25552;&#20379;&#36275;&#22815;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#26159;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#21487;&#33021;&#20250;&#21463;&#21040;&#20154;&#31867;&#34920;&#29616;&#27700;&#24179;&#30340;&#38480;&#21046;&#65292;&#32780;&#19988;&#36825;&#20123;&#29420;&#31435;&#30340;&#20923;&#32467;&#22870;&#21169;&#27169;&#22411;&#22312;LLM&#35757;&#32451;&#36807;&#31243;&#20013;&#26080;&#27861;&#23398;&#20064;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#36890;&#36807;LLM&#20316;&#20026;&#35780;&#21028;&#32773;&#30340;&#25552;&#31034;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#33258;&#24049;&#30340;&#22870;&#21169;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#36845;&#20195;DPO&#35757;&#32451;&#20013;&#65292;&#19981;&#20165;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#24471;&#21040;&#20102;&#25552;&#39640;&#65292;&#32780;&#19988;&#33021;&#22815;&#20026;&#33258;&#24049;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#22870;&#21169;&#12290;&#36890;&#36807;&#23545;Llama 2 70B&#36827;&#34892;&#25105;&#20204;&#26041;&#27861;&#30340;&#19977;&#27425;&#36845;&#20195;&#30340;&#24494;&#35843;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#22312;AlpacaEval 2.0&#25490;&#34892;&#27036;&#19978;&#32988;&#36807;&#35768;&#22810;&#29616;&#26377;&#31995;&#32479;&#65292;&#21253;&#25324;Claude 2&#12289;Gemini Pro&#21644;GPT-4 0613&#12290;&#34429;&#28982;&#36825;&#21482;&#26159;&#19968;&#39033;&#21021;&#27493;&#30740;&#31350;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;&#21487;&#33021;&#23454;&#29616;&#33021;&#22815;&#19981;&#26029;&#33258;&#25105;&#25913;&#36827;&#30340;&#27169;&#22411;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
We posit that to achieve superhuman agents, future models require superhuman feedback in order to provide an adequate training signal. Current approaches commonly train reward models from human preferences, which may then be bottlenecked by human performance level, and secondly these separate frozen reward models cannot then learn to improve during LLM training. In this work, we study Self-Rewarding Language Models, where the language model itself is used via LLM-as-a-Judge prompting to provide its own rewards during training. We show that during Iterative DPO training that not only does instruction following ability improve, but also the ability to provide high-quality rewards to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard, including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study, this work opens the door to the possibility of models that can continuall
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21028;&#26029;&#23433;&#20840;&#39118;&#38505;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;R-Judge&#65292;&#36890;&#36807;&#23545;162&#20010;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#36798;&#21040;&#20102;72.29%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.10019</link><description>&lt;p&gt;
R-Judge: &#35780;&#20272;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#39118;&#38505;&#24847;&#35782;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
R-Judge: Benchmarking Safety Risk Awareness for LLM Agents. (arXiv:2401.10019v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10019
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21028;&#26029;&#23433;&#20840;&#39118;&#38505;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;R-Judge&#65292;&#36890;&#36807;&#23545;162&#20010;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#36798;&#21040;&#20102;72.29%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#23436;&#25104;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLM&#20195;&#29702;&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#25805;&#20316;&#26102;&#20250;&#24341;&#20837;&#24847;&#22806;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#19982;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#23433;&#20840;&#24615;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#23433;&#20840;&#24615;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;R-Judge&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#22312;&#32473;&#23450;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#26102;&#21028;&#26029;&#23433;&#20840;&#39118;&#38505;&#30340;&#33021;&#21147;&#12290;R-Judge&#21253;&#25324;162&#20010;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#65292;&#28085;&#30422;7&#20010;&#24212;&#29992;&#39046;&#22495;&#21644;10&#31181;&#39118;&#38505;&#31867;&#22411;&#30340;27&#20010;&#20851;&#38190;&#39118;&#38505;&#22330;&#26223;&#12290;&#23427;&#32467;&#21512;&#20102;&#20154;&#31867;&#23545;&#23433;&#20840;&#24615;&#30340;&#20849;&#35782;&#65292;&#24182;&#20855;&#26377;&#26631;&#35760;&#30340;&#23433;&#20840;&#39118;&#38505;&#26631;&#31614;&#21644;&#39640;&#36136;&#37327;&#30340;&#39118;&#38505;&#25551;&#36848;&#12290;&#21033;&#29992;R-Judge&#65292;&#25105;&#20204;&#23545;8&#31181;&#24120;&#29992;&#20316;&#20195;&#29702;&#39592;&#24178;&#30340;&#33879;&#21517;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4&#23454;&#29616;&#20102;72.29%&#30340;&#23545;&#27604;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on LLM-generated content safety in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging safety risks given agent interaction records. R-Judge comprises 162 agent interaction records, encompassing 27 key risk scenarios among 7 application categories and 10 risk types. It incorporates human consensus on safety with annotated safety risk labels and high-quality risk descriptions. Utilizing R-Judge, we conduct a comprehensive evaluation of 8 prominent LLMs commonly employed as the backbone for agents. The best-performing model, GPT-4, achieves 72.29% in contrast to
&lt;/p&gt;</description></item><item><title>&#26412;&#31456;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#20256;&#32479;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#30456;&#20851;&#24037;&#20316;&#12290;&#36890;&#36807;&#22312;&#33521;&#35821;-&#24847;&#22823;&#21033;&#35821;&#30340;&#32763;&#35793;&#29615;&#22659;&#20013;&#20351;&#29992;ChatGPT&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#20854;&#35299;&#20915;&#24615;&#21035;&#20559;&#35265;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#20943;&#23569;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10016</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;
&lt;/p&gt;
&lt;p&gt;
Gender Bias in Machine Translation and The Era of Large Language Models. (arXiv:2401.10016v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#38382;&#39064;&#65292;&#20171;&#32461;&#20102;&#20256;&#32479;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#30456;&#20851;&#24037;&#20316;&#12290;&#36890;&#36807;&#22312;&#33521;&#35821;-&#24847;&#22823;&#21033;&#35821;&#30340;&#32763;&#35793;&#29615;&#22659;&#20013;&#20351;&#29992;ChatGPT&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#20854;&#35299;&#20915;&#24615;&#21035;&#20559;&#35265;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#20943;&#23569;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20559;&#35265;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31456;&#25506;&#35752;&#20102;&#26426;&#22120;&#32763;&#35793;&#22312;&#24310;&#32493;&#24615;&#21035;&#20559;&#35265;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#30528;&#37325;&#24378;&#35843;&#20102;&#36328;&#35821;&#35328;&#29615;&#22659;&#21644;&#32479;&#35745;&#20381;&#36182;&#24615;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#25552;&#20379;&#20102;&#20851;&#20110;&#24615;&#21035;&#20559;&#35265;&#22312;&#20256;&#32479;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#21644;&#20316;&#20026;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#30340;&#30456;&#20851;&#29616;&#26377;&#24037;&#20316;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#36890;&#36807;&#22312;&#33521;&#35821;-&#24847;&#22823;&#21033;&#35821;&#32763;&#35793;&#29615;&#22659;&#20013;&#20351;&#29992;ChatGPT (&#22522;&#20110;GPT-3.5)&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;ChatGPT&#35299;&#20915;&#24615;&#21035;&#20559;&#35265;&#30340;&#24403;&#21069;&#33021;&#21147;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#22312;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20013;&#20943;&#23569;&#20559;&#35265;&#30340;&#25345;&#32493;&#38656;&#27714;&#65292;&#24182;&#24378;&#35843;&#20102;&#20419;&#36827;&#35821;&#35328;&#25216;&#26415;&#30340;&#20844;&#24179;&#24615;&#21644;&#21253;&#23481;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This chapter examines the role of Machine Translation in perpetuating gender bias, highlighting the challenges posed by cross-linguistic settings and statistical dependencies. A comprehensive overview of relevant existing work related to gender bias in both conventional Neural Machine Translation approaches and Generative Pretrained Transformer models employed as Machine Translation systems is provided. Through an experiment using ChatGPT (based on GPT-3.5) in an English-Italian translation context, we further assess ChatGPT's current capacity to address gender bias. The findings emphasize the ongoing need for advancements in mitigating bias in Machine Translation systems and underscore the importance of fostering fairness and inclusivity in language technologies.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;H-UDM&#30340;&#23618;&#27425;&#21270;&#21475;&#35821;&#28132;&#22622;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#21475;&#35821;&#28132;&#22622;&#36716;&#24405;&#21644;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#23545;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#36716;&#24405;&#21644;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10015</link><description>&lt;p&gt;
&#21521;&#23618;&#27425;&#21270;&#21475;&#35821;&#28132;&#22622;&#24314;&#27169;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Hierarchical Spoken Language Dysfluency Modeling. (arXiv:2401.10015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;H-UDM&#30340;&#23618;&#27425;&#21270;&#21475;&#35821;&#28132;&#22622;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#21475;&#35821;&#28132;&#22622;&#36716;&#24405;&#21644;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#23545;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#36716;&#24405;&#21644;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#28132;&#22622;&#24314;&#27169;&#26159;&#35821;&#35328;&#27835;&#30103;&#21644;&#35821;&#35328;&#23398;&#20064;&#30340;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#26469;&#31995;&#32479;&#22320;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#23450;&#20041;&#21475;&#35821;&#28132;&#22622;&#21644;&#21475;&#35821;&#28132;&#22622;&#24314;&#27169;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hierarchical Unconstrained Dysfluency Modeling (H-UDM)&#30340;&#26041;&#27861;&#65292;&#26082;&#35299;&#20915;&#20102;&#21475;&#35821;&#28132;&#22622;&#36716;&#24405;&#38382;&#39064;&#65292;&#21448;&#35299;&#20915;&#20102;&#26816;&#27979;&#38382;&#39064;&#65292;&#28040;&#38500;&#20102;&#23545;&#22823;&#37327;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;VCTK++&#30340;&#27169;&#25311;&#28132;&#22622;&#25968;&#25454;&#38598;&#65292;&#20197;&#22686;&#24378;H-UDM&#22312;&#38899;&#26631;&#36716;&#24405;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#36716;&#24405;&#21644;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech dysfluency modeling is the bottleneck for both speech therapy and language learning. However, there is no AI solution to systematically tackle this problem. We first propose to define the concept of dysfluent speech and dysfluent speech modeling. We then present Hierarchical Unconstrained Dysfluency Modeling (H-UDM) approach that addresses both dysfluency transcription and detection to eliminate the need for extensive manual annotation. Furthermore, we introduce a simulated dysfluent dataset called VCTK++ to enhance the capabilities of H-UDM in phonetic transcription. Our experimental results demonstrate the effectiveness and robustness of our proposed methods in both transcription and detection tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24615;&#25512;&#29702;&#21644;&#38382;&#39064;&#29983;&#25104;&#65292;&#23558;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMM)&#36171;&#20104;&#20102;&#26174;&#24615;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25512;&#29702;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10005</link><description>&lt;p&gt;
&#20197;&#26174;&#24615;&#25512;&#29702;&#38142;&#21644;&#35270;&#35273;&#38382;&#39064;&#29983;&#25104;&#25512;&#36827;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation. (arXiv:2401.10005v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24615;&#25512;&#29702;&#21644;&#38382;&#39064;&#29983;&#25104;&#65292;&#23558;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMM)&#36171;&#20104;&#20102;&#26174;&#24615;&#25512;&#29702;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#25512;&#29702;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#33021;&#22815;&#35299;&#37322;&#21644;&#25512;&#29702;&#35270;&#35273;&#20869;&#23481;&#30340;&#26234;&#33021;&#31995;&#32479;&#38656;&#27714;&#36234;&#26469;&#36234;&#39640;&#65292;&#38656;&#35201;&#24320;&#21457;&#19981;&#20165;&#20934;&#30830;&#32780;&#19988;&#20855;&#26377;&#26174;&#24615;&#25512;&#29702;&#33021;&#21147;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#26174;&#24615;&#25512;&#29702;&#33021;&#21147;&#36171;&#20104;LMMs&#65292;&#22522;&#20110;&#35270;&#35273;&#20869;&#23481;&#21644;&#25991;&#26412;&#25351;&#23548;&#36827;&#34892;&#26174;&#24615;&#25512;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21487;&#20197;&#25552;&#38382;&#20197;&#33719;&#21462;&#24517;&#35201;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#25512;&#29702;&#36807;&#31243;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#30340;&#24320;&#21457;&#65292;&#26088;&#22312;&#20419;&#36827;&#24605;&#32500;&#38142;&#25512;&#29702;&#19982;&#25552;&#38382;&#26426;&#21046;&#30340;&#32467;&#21512;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#24230;&#20855;&#26377;&#21306;&#22495;&#24847;&#35782;&#30340;LMM&#65292;&#20197;&#35299;&#20915;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#30340;&#22797;&#26434;&#38656;&#27714;&#12290;&#35813;&#27169;&#22411;&#32463;&#21382;&#20102;&#19977;&#20010;&#38454;&#27573;&#30340;&#35757;&#32451;&#65292;&#39318;&#20808;&#26159;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#23545;&#40784;&#65292;&#25509;&#19979;&#26469;&#26159;&#36890;&#36807;&#26174;&#24335;&#25512;&#29702;&#30340;&#38382;&#39064;&#29983;&#25104;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing demand for intelligent systems capable of interpreting and reasoning about visual content requires the development of Large Multi-Modal Models (LMMs) that are not only accurate but also have explicit reasoning capabilities. This paper presents a novel approach to imbue an LMM with the ability to conduct explicit reasoning based on visual content and textual instructions. We introduce a system that can ask a question to acquire necessary knowledge, thereby enhancing the robustness and explicability of the reasoning process. Our method comprises the development of a novel dataset generated by a Large Language Model (LLM), designed to promote chain-of-thought reasoning combined with a question-asking mechanism. We designed an LMM, which has high capabilities on region awareness to address the intricate requirements of image-text alignment. The model undergoes a three-stage training phase, starting with large-scale image-text alignment using a large-scale datasets, followed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#30417;&#30563;&#30340;&#24418;&#24577;&#21477;&#27861;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#21644;&#20998;&#31867;&#19968;&#32452;&#19981;&#21463;&#38480;&#21046;&#30340;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24418;&#24577;&#21477;&#27861;&#25552;&#21462;&#27169;&#24335;&#21644;&#21019;&#24314;&#21477;&#27861;&#21644;&#35821;&#20041;&#32034;&#24341;&#26469;&#23454;&#29616;&#12290;&#22312;&#22522;&#20110;Wikidata&#21644;Wikipedia&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;0.85&#30340;&#31934;&#30830;&#24230;&#24471;&#20998;&#12290;&#36825;&#19968;&#26041;&#27861;&#20801;&#35768;&#24555;&#36895;&#26500;&#24314;&#22522;&#20110;&#35268;&#21017;&#30340;&#20449;&#24687;&#25277;&#21462;&#31995;&#32479;&#65292;&#24182;&#26500;&#24314;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2401.10002</link><description>&lt;p&gt;
&#36828;&#31243;&#30417;&#30563;&#30340;&#24418;&#24577;&#21477;&#27861;&#27169;&#22411;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Distantly Supervised Morpho-Syntactic Model for Relation Extraction. (arXiv:2401.10002v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36828;&#31243;&#30417;&#30563;&#30340;&#24418;&#24577;&#21477;&#27861;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#21644;&#20998;&#31867;&#19968;&#32452;&#19981;&#21463;&#38480;&#21046;&#30340;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#24418;&#24577;&#21477;&#27861;&#25552;&#21462;&#27169;&#24335;&#21644;&#21019;&#24314;&#21477;&#27861;&#21644;&#35821;&#20041;&#32034;&#24341;&#26469;&#23454;&#29616;&#12290;&#22312;&#22522;&#20110;Wikidata&#21644;Wikipedia&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;0.85&#30340;&#31934;&#30830;&#24230;&#24471;&#20998;&#12290;&#36825;&#19968;&#26041;&#27861;&#20801;&#35768;&#24555;&#36895;&#26500;&#24314;&#22522;&#20110;&#35268;&#21017;&#30340;&#20449;&#24687;&#25277;&#21462;&#31995;&#32479;&#65292;&#24182;&#26500;&#24314;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#30340;&#20219;&#21153;&#28041;&#21450;&#23558;&#38750;&#32467;&#26500;&#21270;&#30340;&#25991;&#26412;&#20869;&#23481;&#33258;&#21160;&#36716;&#25442;&#20026;&#32467;&#26500;&#21270;&#30340;&#25968;&#25454;&#12290;&#35813;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#38598;&#20013;&#22312;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#25152;&#26377;&#20107;&#23454;&#25110;&#29305;&#23450;&#19968;&#32452;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#21644;&#20998;&#31867;&#19968;&#32452;&#19981;&#21463;&#38480;&#21046;&#30340;&#20851;&#31995;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#36828;&#31243;&#30417;&#30563;&#26041;&#27861;&#33719;&#24471;&#30340;&#24418;&#24577;&#21477;&#27861;&#25552;&#21462;&#27169;&#24335;&#65292;&#24182;&#21019;&#24314;&#21477;&#27861;&#21644;&#35821;&#20041;&#32034;&#24341;&#26469;&#25552;&#21462;&#21644;&#20998;&#31867;&#20505;&#36873;&#22270;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;Wikidata&#21644;Wikipedia&#26500;&#24314;&#30340;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;0.85&#30340;&#31934;&#30830;&#24230;&#24471;&#20998;&#65292;&#20294;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#36739;&#20302;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#24555;&#36895;&#26500;&#24314;&#22522;&#20110;&#35268;&#21017;&#30340;&#20449;&#24687;&#25277;&#21462;&#31995;&#32479;&#65292;&#24182;&#26500;&#24314;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of Information Extraction (IE) involves automatically converting unstructured textual content into structured data. Most research in this field concentrates on extracting all facts or a specific set of relationships from documents. In this paper, we present a method for the extraction and categorisation of an unrestricted set of relationships from text. Our method relies on morpho-syntactic extraction patterns obtained by a distant supervision method, and creates Syntactic and Semantic Indices to extract and classify candidate graphs. We evaluate our approach on six datasets built on Wikidata and Wikipedia. The evaluation shows that our approach can achieve Precision scores of up to 0.85, but with lower Recall and F1 scores. Our approach allows to quickly create rule-based systems for Information Extraction and to build annotated datasets to train machine-learning and deep-learning based classifiers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#20998;&#31867;&#31995;&#32479;&#65292;&#29992;&#20110;&#23450;&#20041;&#21487;&#20998;&#32423;&#30340;&#32763;&#35793;&#25552;&#31034;&#65292;&#20197;&#24110;&#21161;&#26500;&#24314;&#36866;&#29992;&#20110;&#19981;&#21516;&#32763;&#35793;&#20219;&#21153;&#30340;&#20855;&#26377;&#19981;&#21516;&#29305;&#24615;&#30340;&#25552;&#31034;&#12290;&#39564;&#35777;&#21644;&#35828;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09984</link><description>&lt;p&gt;
&#21487;&#20998;&#32423;ChatGPT&#32763;&#35793;&#35780;&#20215;
&lt;/p&gt;
&lt;p&gt;
Gradable ChatGPT Translation Evaluation. (arXiv:2401.09984v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#20998;&#31867;&#31995;&#32479;&#65292;&#29992;&#20110;&#23450;&#20041;&#21487;&#20998;&#32423;&#30340;&#32763;&#35793;&#25552;&#31034;&#65292;&#20197;&#24110;&#21161;&#26500;&#24314;&#36866;&#29992;&#20110;&#19981;&#21516;&#32763;&#35793;&#20219;&#21153;&#30340;&#20855;&#26377;&#19981;&#21516;&#29305;&#24615;&#30340;&#25552;&#31034;&#12290;&#39564;&#35777;&#21644;&#35828;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#22312;ChatGPT&#20013;&#65292;&#8220;&#25552;&#31034;&#8221;&#26159;&#25351;&#29992;&#20110;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#29305;&#23450;&#31867;&#22411;&#22238;&#24212;&#30340;&#25991;&#26412;&#27573;&#33853;&#25110;&#25351;&#23548;&#12290;&#32763;&#35793;&#25552;&#31034;&#30340;&#35774;&#35745;&#25104;&#20026;&#24433;&#21709;&#32763;&#35793;&#39118;&#26684;&#12289;&#20934;&#30830;&#24615;&#21644;&#31934;&#30830;&#24230;&#31561;&#22240;&#32032;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#20849;&#21516;&#30340;&#26631;&#20934;&#21644;&#26041;&#27861;&#26469;&#35774;&#35745;&#21644;&#36873;&#25321;&#32763;&#35793;&#25552;&#31034;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#20998;&#31867;&#31995;&#32479;&#65292;&#20197;&#34920;&#36798;&#31867;&#22411;&#12289;&#32763;&#35793;&#39118;&#26684;&#12289;POS&#20449;&#24687;&#21644;&#26174;&#24335;&#22768;&#26126;&#30340;&#26041;&#24335;&#23450;&#20041;&#21487;&#20998;&#32423;&#30340;&#32763;&#35793;&#25552;&#31034;&#65292;&#20174;&#32780;&#20026;&#19981;&#21516;&#30340;&#32763;&#35793;&#20219;&#21153;&#26500;&#24314;&#20855;&#26377;&#19981;&#21516;&#29305;&#24615;&#30340;&#25552;&#31034;&#12290;&#36873;&#25321;&#20102;&#20855;&#20307;&#30340;&#23454;&#39564;&#21644;&#26696;&#20363;&#26469;&#39564;&#35777;&#21644;&#35828;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, as a language model based on large-scale pre-training, has exerted a profound influence on the domain of machine translation. In ChatGPT, a "Prompt" refers to a segment of text or instruction employed to steer the model towards generating a specific category of response. The design of the translation prompt emerges as a key aspect that can wield influence over factors such as the style, precision and accuracy of the translation to a certain extent. However, there is a lack of a common standard and methodology on how to design and select a translation prompt. Accordingly, this paper proposes a generic taxonomy, which defines gradable translation prompts in terms of expression type, translation style, POS information and explicit statement, thus facilitating the construction of prompts endowed with distinct attributes tailored for various translation tasks. Specific experiments and cases are selected to validate and illustrate the effectiveness of the method.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#23618;&#38388;&#30456;&#20851;&#20256;&#25773;&#26041;&#27861;&#20043;&#19978;&#20351;&#29992;&#31934;&#32454;&#21270;&#30340;&#20449;&#24687;&#27969;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;Transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31361;&#20986;&#37325;&#35201;&#20449;&#24687;&#24182;&#28040;&#38500;&#26080;&#20851;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22788;&#29702;&#20998;&#31867;&#21644;&#38382;&#31572;&#20219;&#21153;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#20843;&#31181;&#22522;&#32447;&#27169;&#22411;&#26356;&#21152;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.09972</link><description>&lt;p&gt;
&#36890;&#36807;&#31361;&#20986;&#37325;&#35201;&#20449;&#24687;&#26469;&#26356;&#22909;&#22320;&#35299;&#37322;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Better Explain Transformers by Illuminating Important Information. (arXiv:2401.09972v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09972
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#23618;&#38388;&#30456;&#20851;&#20256;&#25773;&#26041;&#27861;&#20043;&#19978;&#20351;&#29992;&#31934;&#32454;&#21270;&#30340;&#20449;&#24687;&#27969;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;Transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31361;&#20986;&#37325;&#35201;&#20449;&#24687;&#24182;&#28040;&#38500;&#26080;&#20851;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22788;&#29702;&#20998;&#31867;&#21644;&#38382;&#31572;&#20219;&#21153;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#30456;&#27604;&#20854;&#20182;&#20843;&#31181;&#22522;&#32447;&#27169;&#22411;&#26356;&#21152;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21560;&#24341;&#20102;&#26080;&#25968;&#21162;&#21147;&#26469;&#35299;&#37322;&#20854;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20851;&#27880;&#21407;&#22987;&#26799;&#24230;&#21644;&#27880;&#24847;&#21147;&#26469;&#35299;&#37322;Transformer&#65292;&#23558;&#38750;&#30456;&#20851;&#20449;&#24687;&#36890;&#24120;&#35270;&#20026;&#35299;&#37322;&#35745;&#31639;&#30340;&#19968;&#37096;&#20998;&#65292;&#23548;&#33268;&#32467;&#26524;&#28151;&#20081;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23618;&#38388;&#30456;&#20851;&#20256;&#25773;&#65288;LRP&#65289;&#26041;&#27861;&#20043;&#19978;&#36890;&#36807;&#31934;&#32454;&#21270;&#20449;&#24687;&#27969;&#26469;&#31361;&#20986;&#37325;&#35201;&#20449;&#24687;&#24182;&#28040;&#38500;&#26080;&#20851;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#21477;&#27861;&#21644;&#20301;&#32622;&#22836;&#35782;&#21035;&#20026;&#37325;&#35201;&#27880;&#24847;&#21147;&#22836;&#65292;&#24182;&#19987;&#27880;&#20110;&#20174;&#36825;&#20123;&#37325;&#35201;&#22836;&#37096;&#33719;&#24471;&#30340;&#30456;&#20851;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#20851;&#20449;&#24687;&#30830;&#23454;&#20250;&#25197;&#26354;&#36755;&#20986;&#30340;&#24402;&#22240;&#20998;&#25968;&#65292;&#22240;&#27492;&#22312;&#35299;&#37322;&#35745;&#31639;&#36807;&#31243;&#20013;&#24212;&#35813;&#23545;&#20854;&#36827;&#34892;&#23631;&#34109;&#12290;&#19982;&#20843;&#31181;&#22522;&#32447;&#27169;&#22411;&#22312;&#20998;&#31867;&#21644;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#27604;&#36739;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32467;&#26524;&#19978;&#19981;&#26029;&#22320;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models excel in various natural language processing (NLP) tasks, attracting countless efforts to explain their inner workings. Prior methods explain Transformers by focusing on the raw gradient and attention as token attribution scores, where non-relevant information is often considered during explanation computation, resulting in confusing results. In this work, we propose highlighting the important information and eliminating irrelevant information by a refined information flow on top of the layer-wise relevance propagation (LRP) method. Specifically, we consider identifying syntactic and positional heads as important attention heads and focus on the relevance obtained from these important heads. Experimental results demonstrate that irrelevant information does distort output attribution scores and then should be masked during explanation computation. Compared to eight baselines on both classification and question-answering datasets, our method consistently outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#36923;&#36753;&#22238;&#24402;&#30340;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33609;&#22270;&#24341;&#23548;&#32422;&#26463;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#36741;&#21161;&#27169;&#22411;&#20248;&#21270;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20197;&#21021;&#27493;&#36755;&#20986;&#20316;&#20026;&#36827;&#19968;&#27493;&#25193;&#23637;&#30340; "&#33609;&#22270;"&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26377;&#38480;&#32422;&#26463;&#35299;&#30721;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09967</link><description>&lt;p&gt;
&#26080;&#38656;&#35775;&#38382;&#36923;&#36753;&#22238;&#24402;&#30340;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33609;&#22270;&#24341;&#23548;&#32422;&#26463;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language Models without Logit Access. (arXiv:2401.09967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#36923;&#36753;&#22238;&#24402;&#30340;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33609;&#22270;&#24341;&#23548;&#32422;&#26463;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26412;&#22320;&#36741;&#21161;&#27169;&#22411;&#20248;&#21270;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20197;&#21021;&#27493;&#36755;&#20986;&#20316;&#20026;&#36827;&#19968;&#27493;&#25193;&#23637;&#30340; "&#33609;&#22270;"&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26377;&#38480;&#32422;&#26463;&#35299;&#30721;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#32422;&#26463;&#22312;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#25511;&#21046;&#19978;&#25552;&#20379;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25110;&#26550;&#26500;&#20462;&#25913;&#30340;&#26041;&#24335;&#65292;&#20294;&#36890;&#24120;&#21482;&#36866;&#29992;&#20110;&#25317;&#26377;&#36923;&#36753;&#22238;&#24402;&#35775;&#38382;&#26435;&#38480;&#30340;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#38480;&#21046;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33609;&#22270;&#24341;&#23548;&#30340;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32422;&#26463;&#35299;&#30721;&#65288;SGCD&#65289;&#26041;&#27861;&#65292;&#26080;&#38656;&#35775;&#38382;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#22238;&#24402;&#12290;SGCD&#21033;&#29992;&#26412;&#22320;&#36741;&#21161;&#27169;&#22411;&#26469;&#20248;&#21270;&#26080;&#32422;&#26463;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#23558;&#20854;&#20316;&#20026;&#36827;&#19968;&#27493;&#25193;&#23637;&#30340;&#8220;&#33609;&#22270;&#8221;&#12290;&#27492;&#26041;&#27861;&#21487;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#36923;&#36753;&#22238;&#24402;&#30340;&#25216;&#26415;&#30456;&#20114;&#34917;&#20805;&#65292;&#20351;&#26377;&#38480;&#32422;&#26463;&#35299;&#30721;&#22312;&#26080;&#27861;&#23436;&#20840;&#36879;&#26126;&#30340;&#27169;&#22411;&#29615;&#22659;&#20013;&#24212;&#29992;&#12290;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;SGCD&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constrained decoding, a technique for enforcing constraints on language model outputs, offers a way to control text generation without retraining or architectural modifications. Its application is, however, typically restricted to models that give users access to next-token distributions (usually via softmax logits), which poses a limitation with blackbox large language models (LLMs). This paper introduces sketch-guided constrained decoding (SGCD), a novel approach to constrained decoding for blackbox LLMs, which operates without access to the logits of the blackbox LLM. SGCD utilizes a locally hosted auxiliary model to refine the output of an unconstrained blackbox LLM, effectively treating this initial output as a "sketch" for further elaboration. This approach is complementary to traditional logit-based techniques and enables the application of constrained decoding in settings where full model transparency is unavailable. We demonstrate the efficacy of SGCD through experiments in cl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MultiBully-Ex&#30340;&#22810;&#27169;&#24577;&#35299;&#37322;&#30340;&#32593;&#32476;&#27450;&#20940;&#27169;&#22240;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#31361;&#20986;&#26174;&#31034;&#20102;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#65292;&#29992;&#20110;&#35299;&#37322;&#20026;&#20160;&#20040;&#32473;&#23450;&#30340;&#27169;&#22240;&#26159;&#32593;&#32476;&#27450;&#20940;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2401.09899</link><description>&lt;p&gt;
&#26377;&#24847;&#20041;&#30340;&#27169;&#22240;&#20998;&#26512;&#65306;&#36890;&#36807;&#22810;&#27169;&#24577;&#35299;&#37322;&#25552;&#39640;&#23545;&#32593;&#32476;&#27450;&#20940;&#20013;&#30340;&#27169;&#22240;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Meme-ingful Analysis: Enhanced Understanding of Cyberbullying in Memes Through Multimodal Explanations. (arXiv:2401.09899v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MultiBully-Ex&#30340;&#22810;&#27169;&#24577;&#35299;&#37322;&#30340;&#32593;&#32476;&#27450;&#20940;&#27169;&#22240;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#31361;&#20986;&#26174;&#31034;&#20102;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#65292;&#29992;&#20110;&#35299;&#37322;&#20026;&#20160;&#20040;&#32473;&#23450;&#30340;&#27169;&#22240;&#26159;&#32593;&#32476;&#27450;&#20940;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#32852;&#32593;&#27169;&#22240;&#22312;&#20256;&#36798;&#25919;&#27835;&#12289;&#24515;&#29702;&#21644;&#31038;&#20250;&#25991;&#21270;&#35266;&#24565;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#21147;&#12290;&#34429;&#28982;&#27169;&#22240;&#24120;&#24120;&#20855;&#26377;&#24189;&#40664;&#30340;&#29305;&#28857;&#65292;&#20294;&#20351;&#29992;&#27169;&#22240;&#36827;&#34892;&#24694;&#20316;&#21095;&#21644;&#32593;&#32476;&#27450;&#20940;&#30340;&#29616;&#35937;&#27491;&#36880;&#28176;&#22686;&#21152;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#21508;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#26469;&#26816;&#27979;&#20882;&#29359;&#24615;&#22810;&#27169;&#24577;&#27169;&#22240;&#65292;&#20294;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21482;&#26377;&#23569;&#25968;&#24037;&#20316;&#12290;&#31867;&#20284;&#20110;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#20013;&#30340;&#8220;&#35299;&#37322;&#26435;&#8221;&#65292;&#26368;&#36817;&#30340;&#30456;&#20851;&#27861;&#24459;&#24050;&#32463;&#25512;&#21160;&#20102;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#24615;&#33021;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#35299;&#37322;&#30340;&#20195;&#30721;&#28151;&#21512;&#32593;&#32476;&#27450;&#20940;&#27169;&#22240;&#22522;&#20934;&#25968;&#25454;&#38598;MultiBully-Ex&#12290;&#22312;&#36825;&#37324;&#65292;&#39640;&#20142;&#26174;&#31034;&#20102;&#35270;&#35273;&#21644;&#25991;&#26412;&#27169;&#24577;&#65292;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#32473;&#23450;&#30340;&#27169;&#22240;&#26159;&#32593;&#32476;&#27450;&#20940;&#34892;&#20026;&#12290;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#25237;&#24433;&#30340;&#22810;&#27169;&#24577;&#20849;&#20139;-&#31169;&#26377;&#22810;&#20219;&#21153;&#26041;&#27861;&#65292;&#29992;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Internet memes have gained significant influence in communicating political, psychological, and sociocultural ideas. While memes are often humorous, there has been a rise in the use of memes for trolling and cyberbullying. Although a wide variety of effective deep learning-based models have been developed for detecting offensive multimodal memes, only a few works have been done on explainability aspect. Recent laws like "right to explanations" of General Data Protection Regulation, have spurred research in developing interpretable models rather than only focusing on performance. Motivated by this, we introduce {\em MultiBully-Ex}, the first benchmark dataset for multimodal explanation from code-mixed cyberbullying memes. Here, both visual and textual modalities are highlighted to explain why a given meme is cyberbullying. A Contrastive Language-Image Pretraining (CLIP) projection-based multimodal shared-private multitask approach has been proposed for visual and textual explanation of 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35770;&#25991;&#35843;&#26597;&#20102;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#21644;&#33021;&#37327;&#25928;&#29575;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#24182;&#23545;&#22810;&#31181;&#21152;&#36895;&#22120;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#24037;&#31243;&#24072;&#21644;&#20915;&#31574;&#32773;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.09890</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Hardware Accelerators for Large Language Models. (arXiv:2401.09890v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09890
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35770;&#25991;&#35843;&#26597;&#20102;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#21644;&#33021;&#37327;&#25928;&#29575;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#24182;&#23545;&#22810;&#31181;&#21152;&#36895;&#22120;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#24037;&#31243;&#24072;&#21644;&#20915;&#31574;&#32773;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36890;&#36807;&#20854;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#23427;&#20204;&#27491;&#22312;&#20026;&#35813;&#39046;&#22495;&#24102;&#26469;&#38761;&#21629;&#24615;&#21464;&#38761;&#12290;&#38543;&#30528;&#23545;&#26356;&#22797;&#26434;LLMs&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#65292;&#36843;&#20999;&#38656;&#35201;&#35299;&#20915;&#19982;&#20854;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#30456;&#20851;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#26412;&#25991;&#38024;&#23545;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#21644;&#33021;&#37327;&#25928;&#29575;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#36890;&#36807;&#23545;&#21253;&#25324;GPU&#12289;FPGA&#21644;&#23450;&#21046;&#26550;&#26500;&#22312;&#20869;&#30340;&#21508;&#31181;&#21152;&#36895;&#22120;&#36827;&#34892;&#30740;&#31350;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26088;&#22312;&#28385;&#36275;LLMs&#30340;&#29420;&#29305;&#35745;&#31639;&#38656;&#27714;&#30340;&#30828;&#20214;&#35299;&#20915;&#26041;&#26696;&#30340;&#26684;&#23616;&#12290;&#26412;&#35843;&#26597;&#28085;&#30422;&#20102;&#23545;&#26550;&#26500;&#12289;&#24615;&#33021;&#25351;&#26631;&#21644;&#33021;&#37327;&#25928;&#29575;&#32771;&#34385;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#24037;&#31243;&#24072;&#21644;&#20915;&#31574;&#32773;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20248;&#21270;LLMs&#30340;&#37096;&#32626;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful tools for natural language processing tasks, revolutionizing the field with their ability to understand and generate human-like text. As the demand for more sophisticated LLMs continues to grow, there is a pressing need to address the computational challenges associated with their scale and complexity. This paper presents a comprehensive survey on hardware accelerators designed to enhance the performance and energy efficiency of Large Language Models. By examining a diverse range of accelerators, including GPUs, FPGAs, and custom-designed architectures, we explore the landscape of hardware solutions tailored to meet the unique computational demands of LLMs. The survey encompasses an in-depth analysis of architecture, performance metrics, and energy efficiency considerations, providing valuable insights for researchers, engineers, and decision-makers aiming to optimize the deployment of LLMs in real-world applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#19979;&#34507;&#40481;&#34892;&#20026;&#12290;&#36890;&#36807;&#22768;&#38899;&#20998;&#26512;&#21644;&#29305;&#24449;&#25552;&#21462;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#34892;&#20026;&#29305;&#24449;&#21270;&#31995;&#32479;&#65292;&#23545;&#19979;&#34507;&#40481;&#30340;&#20581;&#24247;&#34892;&#20026;&#36827;&#34892;&#30417;&#27979;&#21644;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#32508;&#21512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09880</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#33258;&#21160;&#34892;&#20026;&#19979;&#34507;&#40481;&#35782;&#21035;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Attention-Based Recurrent Neural Network For Automatic Behavior Laying Hen Recognition. (arXiv:2401.09880v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09880
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#19979;&#34507;&#40481;&#34892;&#20026;&#12290;&#36890;&#36807;&#22768;&#38899;&#20998;&#26512;&#21644;&#29305;&#24449;&#25552;&#21462;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;&#34892;&#20026;&#29305;&#24449;&#21270;&#31995;&#32479;&#65292;&#23545;&#19979;&#34507;&#40481;&#30340;&#20581;&#24247;&#34892;&#20026;&#36827;&#34892;&#30417;&#27979;&#21644;&#35782;&#21035;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#32508;&#21512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#20859;&#31165;&#19994;&#30340;&#19968;&#20010;&#20851;&#27880;&#28857;&#26159;&#19979;&#34507;&#40481;&#30340;&#40483;&#21483;&#22768;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20851;&#20110;&#20581;&#24247;&#34892;&#20026;&#30340;&#38750;&#24120;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#34987;&#29992;&#20316;&#20581;&#24247;&#21644;&#31119;&#31049;&#30340;&#25351;&#26631;&#65292;&#24110;&#21161;&#20859;&#27542;&#20154;&#21592;&#26356;&#22909;&#22320;&#30417;&#27979;&#19979;&#34507;&#40481;&#65292;&#20174;&#32780;&#21450;&#26089;&#21457;&#29616;&#38382;&#39064;&#65292;&#20197;&#20415;&#36827;&#34892;&#26356;&#24555;&#21644;&#26356;&#26377;&#25928;&#30340;&#24178;&#39044;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#23545;&#19979;&#34507;&#40481;&#40483;&#21483;&#31867;&#22411;&#30340;&#22768;&#38899;&#20998;&#26512;&#65292;&#20197;&#25552;&#20986;&#19968;&#31181;&#40065;&#26834;&#30340;&#34892;&#20026;&#29305;&#24449;&#21270;&#31995;&#32479;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#30417;&#27979;&#19979;&#34507;&#40481;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#24182;&#27880;&#37322;&#20102;&#19979;&#34507;&#40481;&#30340;&#40483;&#21483;&#20449;&#21495;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#29305;&#24449;&#32452;&#21512;&#30340;&#26368;&#20339;&#22768;&#23398;&#29305;&#24449;&#21270;&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#26500;&#24314;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#27169;&#22411;&#65292;&#23558;&#35821;&#20041;&#31867;&#21035;&#20998;&#37197;&#32473;&#25551;&#36848;&#19979;&#34507;&#40481;&#34892;&#20026;&#30340;&#40483;&#21483;&#22768;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#32508;&#21512;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the interests of modern poultry farming is the vocalization of laying hens which contain very useful information on health behavior. This information is used as health and well-being indicators that help breeders better monitor laying hens, which involves early detection of problems for rapid and more effective intervention. In this work, we focus on the sound analysis for the recognition of the types of calls of the laying hens in order to propose a robust system of characterization of their behavior for a better monitoring. To do this, we first collected and annotated laying hen call signals, then designed an optimal acoustic characterization based on the combination of time and frequency domain features. We then used these features to build the multi-label classification models based on recurrent neural network to assign a semantic class to the vocalization that characterize the laying hen behavior. The results show an overall performance with our model based on the combinati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#20248;&#21270;&#30340;&#36827;&#21270;&#22810;&#30446;&#26631;&#26041;&#27861;&#65292;&#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#33021;&#22815;&#21516;&#26102;&#20307;&#29616;&#20004;&#31181;&#30456;&#20114;&#20914;&#31361;&#24773;&#24863;&#30340;&#25552;&#31034;&#35821;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#30456;&#20851;&#20449;&#24687;&#30340;&#25552;&#21462;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09862</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30340;&#36827;&#21270;&#22810;&#30446;&#26631;&#20248;&#21270;&#20197;&#24179;&#34913;&#24773;&#24863;
&lt;/p&gt;
&lt;p&gt;
Evolutionary Multi-Objective Optimization of Large Language Model Prompts for Balancing Sentiments. (arXiv:2401.09862v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#20248;&#21270;&#30340;&#36827;&#21270;&#22810;&#30446;&#26631;&#26041;&#27861;&#65292;&#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#33021;&#22815;&#21516;&#26102;&#20307;&#29616;&#20004;&#31181;&#30456;&#20114;&#20914;&#31361;&#24773;&#24863;&#30340;&#25552;&#31034;&#35821;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#30456;&#20851;&#20449;&#24687;&#30340;&#25552;&#21462;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#38750;&#20961;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#30340;&#20351;&#29992;&#19981;&#26029;&#22686;&#38271;&#65292;&#26377;&#25928;&#30340;&#25552;&#31034;&#24037;&#31243;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#25552;&#31034;&#20248;&#21270;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#30452;&#25509;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#21644;&#30456;&#20851;&#20449;&#24687;&#30340;&#25552;&#21462;&#12290;&#26368;&#36817;&#65292;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#65292;&#20026;&#26032;&#30340;&#20248;&#21270;&#31574;&#30053;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#38024;&#23545;&#25552;&#31034;&#20248;&#21270;&#30340;&#36827;&#21270;&#22810;&#30446;&#26631;&#65288;EMO&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;EMO-Prompts&#65292;&#20197;&#24773;&#24863;&#20998;&#26512;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;&#24773;&#24863;&#20998;&#26512;&#33021;&#21147;&#20316;&#20026;&#25105;&#20204;&#30340;&#23454;&#39564;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;EMO-Prompts&#33021;&#22815;&#26377;&#25928;&#22320;&#29983;&#25104;&#25552;&#31034;&#65292;&#20351;LLM&#33021;&#22815;&#21516;&#26102;&#20135;&#29983;&#20307;&#29616;&#20004;&#31181;&#30456;&#20114;&#20914;&#31361;&#24773;&#24863;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) such as ChatGPT has attracted considerable attention in various domains due to their remarkable performance and versatility. As the use of these models continues to grow, the importance of effective prompt engineering has come to the fore. Prompt optimization emerges as a crucial challenge, as it has a direct impact on model performance and the extraction of relevant information. Recently, evolutionary algorithms (EAs) have shown promise in addressing this issue, paving the way for novel optimization strategies. In this work, we propose a evolutionary multi-objective (EMO) approach specifically tailored for prompt optimization called EMO-Prompts, using sentiment analysis as a case study. We use sentiment analysis capabilities as our experimental targets. Our results demonstrate that EMO-Prompts effectively generates prompts capable of guiding the LLM to produce texts embodying two conflicting emotions simultaneously.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MatSciRE&#65292;&#19968;&#31181;&#22522;&#20110;&#25351;&#38024;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#31456;&#20013;&#33258;&#21160;&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#20197;&#26500;&#24314;&#19968;&#20010;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#24211;&#12290;&#36890;&#36807;&#38024;&#23545;&#30005;&#27744;&#26448;&#26009;&#30340;&#20116;&#20010;&#20851;&#31995;&#30340;&#25552;&#21462;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;F1&#20998;&#25968;&#19978;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#20351;&#29992;ChemDataExtractor&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.09839</link><description>&lt;p&gt;
MatSciRE:&#21033;&#29992;&#25351;&#38024;&#32593;&#32476;&#33258;&#21160;&#21270;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#24211;&#26500;&#24314;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
MatSciRE: Leveraging Pointer Networks to Automate Entity and Relation Extraction for Material Science Knowledge-base Construction. (arXiv:2401.09839v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MatSciRE&#65292;&#19968;&#31181;&#22522;&#20110;&#25351;&#38024;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#31456;&#20013;&#33258;&#21160;&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#20197;&#26500;&#24314;&#19968;&#20010;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#24211;&#12290;&#36890;&#36807;&#38024;&#23545;&#30005;&#27744;&#26448;&#26009;&#30340;&#20116;&#20010;&#20851;&#31995;&#30340;&#25552;&#21462;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;F1&#20998;&#25968;&#19978;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#20351;&#29992;ChemDataExtractor&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#26159;&#20851;&#20110;&#21508;&#31181;&#23454;&#20307;&#65288;&#22914;&#26448;&#26009;&#21644;&#25104;&#20998;&#65289;&#21644;&#36825;&#20123;&#23454;&#20307;&#20043;&#38388;&#21508;&#31181;&#20851;&#31995;&#65288;&#22914;&#23548;&#30005;&#24615;&#12289;&#30005;&#21387;&#31561;&#65289;&#30340;&#20016;&#23500;&#26469;&#28304;&#12290;&#33258;&#21160;&#25552;&#21462;&#36825;&#20123;&#20449;&#24687;&#20197;&#29983;&#25104;&#19968;&#20010;&#26448;&#26009;&#31185;&#23398;&#30693;&#35782;&#24211;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MatSciRE&#65288;&#26448;&#26009;&#31185;&#23398;&#20851;&#31995;&#25552;&#21462;&#22120;&#65289;&#65292;&#19968;&#31181;&#22522;&#20110;&#25351;&#38024;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#31456;&#20013;&#21516;&#26102;&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#20316;&#20026;&#19977;&#20803;&#32452;&#65288;$&#23454;&#20307;1&#65292;&#20851;&#31995;&#65292;&#23454;&#20307;2$&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#38024;&#23545;&#30005;&#27744;&#26448;&#26009;&#65292;&#24182;&#30830;&#23450;&#20102;&#20116;&#20010;&#35201;&#22788;&#29702;&#30340;&#20851;&#31995; - &#23548;&#30005;&#24615;&#12289;&#24211;&#20262;&#25928;&#29575;&#12289;&#23481;&#37327;&#12289;&#30005;&#21387;&#21644;&#33021;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;F1&#20998;&#25968;&#19978;&#21462;&#24471;&#20102;&#27604;&#20351;&#29992;ChemDataExtractor&#65288;0.716&#65289;&#26356;&#22909;&#30340;&#32467;&#26524;&#65288;0.771&#65289;&#12290;MatSciRE&#30340;&#25972;&#20307;&#22270;&#24418;&#26694;&#26550;&#22914;&#22270;1&#25152;&#31034;&#12290;&#26448;&#26009;&#20449;&#24687;&#20197;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#24418;&#24335;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Material science literature is a rich source of factual information about various categories of entities (like materials and compositions) and various relations between these entities, such as conductivity, voltage, etc. Automatically extracting this information to generate a material science knowledge base is a challenging task. In this paper, we propose MatSciRE (Material Science Relation Extractor), a Pointer Network-based encoder-decoder framework, to jointly extract entities and relations from material science articles as a triplet ($entity1, relation, entity2$). Specifically, we target the battery materials and identify five relations to work on - conductivity, coulombic efficiency, capacity, voltage, and energy. Our proposed approach achieved a much better F1-score (0.771) than a previous attempt using ChemDataExtractor (0.716). The overall graphical framework of MatSciRE is shown in Fig 1. The material information is extracted from material science literature in the form of ent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#65292;&#36890;&#36807;&#37319;&#26679;MR&#24182;&#36827;&#34892;&#21453;&#21521;&#32763;&#35793;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#23545;&#20110;&#32452;&#21512;&#27867;&#21270;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#24403;&#20174;&#27491;&#30830;&#30340;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#26102;&#25928;&#26524;&#26356;&#22909;&#12290;&#19982;&#20174;&#35757;&#32451;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20174;&#22343;&#21248;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#25928;&#26524;&#20960;&#20046;&#19982;&#20174;&#27979;&#35797;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#30456;&#24403;&#65292;&#24182;&#19988;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.09815</link><description>&lt;p&gt;
&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#23545;&#20110;&#32452;&#21512;&#27867;&#21270;&#20855;&#26377;&#26174;&#33879;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Simple and effective data augmentation for compositional generalization. (arXiv:2401.09815v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#36890;&#36807;&#37319;&#26679;MR&#24182;&#36827;&#34892;&#21453;&#21521;&#32763;&#35793;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#23545;&#20110;&#32452;&#21512;&#27867;&#21270;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#24403;&#20174;&#27491;&#30830;&#30340;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#26102;&#25928;&#26524;&#26356;&#22909;&#12290;&#19982;&#20174;&#35757;&#32451;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#20174;&#22343;&#21248;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#25928;&#26524;&#20960;&#20046;&#19982;&#20174;&#27979;&#35797;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#30456;&#24403;&#65292;&#24182;&#19988;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#21270;&#27867;&#21270;&#65292;&#21363;&#36890;&#36807;&#22312;&#36739;&#31616;&#21333;&#30340;&#21477;&#23376;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#39044;&#27979;&#22797;&#26434;&#30340;&#21547;&#20041;&#65292;&#23545;&#20110;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#26469;&#35828;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#37319;&#26679;MR&#24182;&#36827;&#34892;&#21453;&#21521;&#32763;&#35793;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21487;&#20197;&#23545;&#32452;&#21512;&#27867;&#21270;&#20135;&#29983;&#26377;&#25928;&#30340;&#25928;&#26524;&#65292;&#20294;&#20165;&#24403;&#25105;&#20204;&#20174;&#27491;&#30830;&#30340;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#26102;&#25165;&#26159;&#22914;&#27492;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20174;&#22343;&#21248;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#25928;&#26524;&#20960;&#20046;&#19982;&#20174;&#27979;&#35797;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#30456;&#24403;&#65292;&#24182;&#19988;&#27604;&#20043;&#21069;&#20174;&#35757;&#32451;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36827;&#34892;&#23454;&#39564;&#26469;&#25506;&#31350;&#20026;&#20309;&#20250;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#20197;&#21450;&#36825;&#31867;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#22909;&#22788;&#26469;&#33258;&#20309;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositional generalization, the ability to predict complex meanings from training on simpler sentences, poses challenges for powerful pretrained seq2seq models. In this paper, we show that data augmentation methods that sample MRs and backtranslate them can be effective for compositional generalization, but only if we sample from the right distribution. Remarkably, sampling from a uniform distribution performs almost as well as sampling from the test distribution, and greatly outperforms earlier methods that sampled from the training distribution. We further conduct experiments to investigate the reason why this happens and where the benefit of such data augmentation methods come from.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36234;&#29425;&#25915;&#20987;&#25552;&#31034;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#36523;&#65292;&#23558;&#26377;&#23475;&#25552;&#31034;&#37325;&#20889;&#20026;&#38750;&#26377;&#23475;&#34920;&#36798;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;80%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#21363;&#20351;&#27169;&#22411;&#26356;&#26032;&#65292;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.09798</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#29992;&#20110;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks. (arXiv:2401.09798v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#36234;&#29425;&#25915;&#20987;&#25552;&#31034;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#36523;&#65292;&#23558;&#26377;&#23475;&#25552;&#31034;&#37325;&#20889;&#20026;&#38750;&#26377;&#23475;&#34920;&#36798;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;80%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#21363;&#20351;&#27169;&#22411;&#26356;&#26032;&#65292;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#8220;&#36234;&#29425;&#8221;&#25361;&#25112;&#65292;&#21363;&#35268;&#36991;&#20445;&#38556;&#25514;&#26045;&#20197;&#20135;&#29983;&#19981;&#31526;&#21512;&#20262;&#29702;&#30340;&#25552;&#31034;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#40657;&#30418;&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#36234;&#29425;&#25552;&#31034;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#39640;&#22797;&#26434;&#24615;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#33258;&#36523;&#65292;&#36845;&#20195;&#22320;&#23558;&#26377;&#23475;&#25552;&#31034;&#37325;&#20889;&#20026;&#38750;&#26377;&#23475;&#34920;&#36798;&#65292;&#22522;&#20110;&#20551;&#35774;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29983;&#25104;&#35268;&#36991;&#20445;&#38556;&#30340;&#34920;&#36798;&#12290;&#36890;&#36807;&#22312;ChatGPT&#65288;GPT-3.5&#21644;GPT-4&#65289;&#21644;Gemini-Pro&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#24179;&#22343;5&#27425;&#36845;&#20195;&#20869;&#23454;&#29616;&#20102;&#36229;&#36807;80%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#21363;&#20351;&#27169;&#22411;&#26356;&#26032;&#65292;&#25928;&#26524;&#20173;&#28982;&#26377;&#25928;&#12290;&#29983;&#25104;&#30340;&#36234;&#29425;&#25552;&#31034;&#33258;&#28982;&#32780;&#31616;&#32451;&#65292;&#34920;&#26126;&#23427;&#20204;&#36739;&#19981;&#26131;&#34987;&#26816;&#27979;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21019;&#24314;&#26377;&#25928;&#30340;&#36234;&#29425;&#25552;&#31034;&#27604;&#20808;&#21069;&#30740;&#31350;&#35748;&#20026;&#30340;&#35201;&#31616;&#21333;&#65292;&#24182;&#19988;&#40657;&#30418;&#36234;&#29425;&#25915;&#20987;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) like ChatGPT face `jailbreak' challenges, where safeguards are bypassed to produce ethically harmful prompts. This study introduces a simple black-box method to effectively generate jailbreak prompts, overcoming the limitations of high complexity and computational costs associated with existing methods. The proposed technique iteratively rewrites harmful prompts into non-harmful expressions using the target LLM itself, based on the hypothesis that LLMs can directly sample safeguard-bypassing expressions. Demonstrated through experiments with ChatGPT (GPT-3.5 and GPT-4) and Gemini-Pro, this method achieved an attack success rate of over 80% within an average of 5 iterations and remained effective despite model updates. The jailbreak prompts generated were naturally-worded and concise, suggesting they are less detectable. The results indicate that creating effective jailbreak prompts is simpler than previously considered, and black-box jailbreak attacks pose 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20302;&#24310;&#36831;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#26041;&#27861;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#30005;&#23376;&#21830;&#21153;&#39038;&#23458;&#30340;&#28040;&#24687;&#36716;&#21270;&#20026;&#31616;&#27905;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30005;&#23376;&#21830;&#21153;&#20080;&#21334;&#21452;&#26041;&#22312;&#32447;&#28040;&#24687;&#20013;&#30340;&#21363;&#26102;&#22238;&#31572;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38382;&#39064;&#29702;&#35299;&#21644;&#22238;&#31572;&#29575;&#26041;&#38754;&#30456;&#23545;&#22686;&#21152;&#20102;&#24456;&#22810;&#65292;&#23545;&#20110;&#25552;&#39640;&#39038;&#23458;&#30340;&#36141;&#29289;&#20307;&#39564;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.09785</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#20080;&#21334;&#21452;&#26041;&#22312;&#32447;&#28040;&#24687;&#20013;&#30340;&#21363;&#26102;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Instant Answering in E-Commerce Buyer-Seller Messaging. (arXiv:2401.09785v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09785
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20302;&#24310;&#36831;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#26041;&#27861;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#30005;&#23376;&#21830;&#21153;&#39038;&#23458;&#30340;&#28040;&#24687;&#36716;&#21270;&#20026;&#31616;&#27905;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30005;&#23376;&#21830;&#21153;&#20080;&#21334;&#21452;&#26041;&#22312;&#32447;&#28040;&#24687;&#20013;&#30340;&#21363;&#26102;&#22238;&#31572;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38382;&#39064;&#29702;&#35299;&#21644;&#22238;&#31572;&#29575;&#26041;&#38754;&#30456;&#23545;&#22686;&#21152;&#20102;&#24456;&#22810;&#65292;&#23545;&#20110;&#25552;&#39640;&#39038;&#23458;&#30340;&#36141;&#29289;&#20307;&#39564;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#39038;&#23458;&#32463;&#24120;&#23547;&#27714;&#35814;&#32454;&#30340;&#20135;&#21697;&#20449;&#24687;&#20197;&#20570;&#20986;&#36141;&#20080;&#20915;&#31574;&#65292;&#36890;&#24120;&#36890;&#36807;&#30452;&#25509;&#21521;&#21334;&#23478;&#21457;&#36865;&#25193;&#23637;&#26597;&#35810;&#26469;&#32852;&#31995;&#12290;&#36825;&#31181;&#25163;&#21160;&#22238;&#22797;&#35201;&#27714;&#22686;&#21152;&#20102;&#39069;&#22806;&#30340;&#25104;&#26412;&#65292;&#24182;&#19988;&#22312;&#21709;&#24212;&#26102;&#38388;&#27874;&#21160;&#33539;&#22260;&#20174;&#20960;&#23567;&#26102;&#21040;&#20960;&#22825;&#26102;&#24178;&#25200;&#20102;&#39038;&#23458;&#30340;&#36141;&#29289;&#20307;&#39564;&#12290;&#25105;&#20204;&#26088;&#22312;&#20351;&#29992;&#39046;&#20808;&#30340;&#30005;&#23376;&#21830;&#21153;&#21830;&#24215;&#20013;&#30340;&#29305;&#23450;&#39046;&#22495;&#32852;&#21512;&#38382;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#33258;&#21160;&#22788;&#29702;&#39038;&#23458;&#23545;&#21334;&#23478;&#30340;&#35810;&#38382;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#23558;&#24403;&#21069;&#20026;&#21333;&#20010;&#38382;&#39064;&#35774;&#35745;&#30340;QA&#31995;&#32479;&#35843;&#25972;&#20026;&#35299;&#20915;&#35814;&#32454;&#30340;&#39038;&#23458;&#26597;&#35810;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#20302;&#24310;&#36831;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#26041;&#27861;&#8212;&#8212;MESSAGE-TO-QUESTION&#65288;M2Q&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#20174;&#28040;&#24687;&#20013;&#35782;&#21035;&#21644;&#25552;&#21462;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#26469;&#23558;&#20080;&#23478;&#28040;&#24687;&#37325;&#26032;&#26500;&#24314;&#25104;&#31616;&#27905;&#30340;&#38382;&#39064;&#12290;&#19982;&#22522;&#32447;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;M2Q&#22312;&#38382;&#39064;&#29702;&#35299;&#26041;&#38754;&#30456;&#23545;&#22686;&#21152;&#20102;757%&#65292;&#22312;&#32852;&#21512;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#22238;&#31572;&#29575;&#22686;&#21152;&#20102;1746%&#12290;&#23454;&#38469;&#37096;&#32626;&#34920;&#26126;&#65292;&#33258;&#21160;&#21270;&#22238;&#31572;&#31995;&#32479;&#21487;&#20197;&#20197;&#19968;&#20010;&#24179;&#22343;&#30340;&#22238;&#31572;&#36895;&#29575;&#24555;4.67&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
E-commerce customers frequently seek detailed product information for purchase decisions, commonly contacting sellers directly with extended queries. This manual response requirement imposes additional costs and disrupts buyer's shopping experience with response time fluctuations ranging from hours to days. We seek to automate buyer inquiries to sellers in a leading e-commerce store using a domain-specific federated Question Answering (QA) system. The main challenge is adapting current QA systems, designed for single questions, to address detailed customer queries. We address this with a low-latency, sequence-to-sequence approach, MESSAGE-TO-QUESTION ( M2Q ). It reformulates buyer messages into succinct questions by identifying and extracting the most salient information from a message. Evaluation against baselines shows that M2Q yields relative increases of 757% in question understanding, and 1,746% in answering rate from the federated QA system. Live deployment shows that automatic a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;bias-kNN&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#26679;&#26412;&#21644;&#27169;&#22411;&#24773;&#22659;&#19979;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;&#36825;&#19968;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35270;&#35282;&#65292;&#23558;&#20559;&#35265;&#36716;&#21270;&#20026;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#30340;&#36164;&#20135;&#12290;</title><link>http://arxiv.org/abs/2401.09783</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65306;&#26377;&#25928;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#8220;bias-kNN&#8221;
&lt;/p&gt;
&lt;p&gt;
Leveraging Biases in Large Language Models: "bias-kNN'' for Effective Few-Shot Learning. (arXiv:2401.09783v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;bias-kNN&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#26679;&#26412;&#21644;&#27169;&#22411;&#24773;&#22659;&#19979;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;&#36825;&#19968;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35270;&#35282;&#65292;&#23558;&#20559;&#35265;&#36716;&#21270;&#20026;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#30340;&#36164;&#20135;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21487;&#33021;&#21463;&#21040;&#20869;&#22312;&#20559;&#35265;&#30340;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;bias-kNN&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#19978;&#33268;&#21147;&#20110;&#26368;&#23567;&#21270;&#25110;&#20462;&#27491;&#36825;&#20123;&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26377;&#20559;&#35265;&#30340;&#36755;&#20986;&#65292;&#23558;&#20854;&#20316;&#20026;kNN&#30340;&#20027;&#35201;&#29305;&#24449;&#65292;&#24182;&#19982;&#37329;&#26631;&#31614;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#23545;&#22810;&#26679;&#21270;&#39046;&#22495;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;GPT-2&#27169;&#22411;&#23610;&#23544;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#20102;&#8220;bias-kNN&#8221;&#26041;&#27861;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#26524;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#20248;&#20110;&#20256;&#32479;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#32780;&#19988;&#22312;&#21508;&#31181;&#26679;&#26412;&#12289;&#27169;&#26495;&#21644;&#35328;&#35821;&#22120;&#19978;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20559;&#35265;&#36716;&#21270;&#20026;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#30340;&#36164;&#20135;&#30340;&#29420;&#29305;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown significant promise in various applications, including zero-shot and few-shot learning. However, their performance can be hampered by inherent biases. Instead of traditionally sought methods that aim to minimize or correct these biases, this study introduces a novel methodology named ``bias-kNN''. This approach capitalizes on the biased outputs, harnessing them as primary features for kNN and supplementing with gold labels. Our comprehensive evaluations, spanning diverse domain text classification datasets and different GPT-2 model sizes, indicate the adaptability and efficacy of the ``bias-kNN'' method. Remarkably, this approach not only outperforms conventional in-context learning in few-shot scenarios but also demonstrates robustness across a spectrum of samples, templates and verbalizers. This study, therefore, presents a unique perspective on harnessing biases, transforming them into assets for enhanced model performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#23558;&#26497;&#24615;&#38382;&#39064;&#30340;&#31572;&#26696;&#37325;&#20889;&#20026;&#33073;&#31163;&#24773;&#22659;&#19988;&#31616;&#27905;&#30340;&#20107;&#23454;&#38472;&#36848;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;Transformer&#27169;&#22411;&#23454;&#29616;&#30340;&#21487;&#25511;&#21046;&#37325;&#20889;&#26041;&#27861;&#65292;&#24182;&#22312;&#19977;&#20010;&#29420;&#31435;&#30340;PQA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;</title><link>http://arxiv.org/abs/2401.09775</link><description>&lt;p&gt;
&#21487;&#25511;&#21046;&#30340;&#23545;&#26159;/&#21542;&#38382;&#39064;&#21644;&#31572;&#26696;&#36827;&#34892;&#21435;&#24773;&#22659;&#21270;&#22788;&#29702;&#65292;&#36716;&#21270;&#20026;&#20107;&#23454;&#38472;&#36848;
&lt;/p&gt;
&lt;p&gt;
Controllable Decontextualization of Yes/No Question and Answers into Factual Statements. (arXiv:2401.09775v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;&#23558;&#26497;&#24615;&#38382;&#39064;&#30340;&#31572;&#26696;&#37325;&#20889;&#20026;&#33073;&#31163;&#24773;&#22659;&#19988;&#31616;&#27905;&#30340;&#20107;&#23454;&#38472;&#36848;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;Transformer&#27169;&#22411;&#23454;&#29616;&#30340;&#21487;&#25511;&#21046;&#37325;&#20889;&#26041;&#27861;&#65292;&#24182;&#22312;&#19977;&#20010;&#29420;&#31435;&#30340;PQA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26159;/&#21542;&#38382;&#39064;&#25110;&#26497;&#24615;&#38382;&#39064;&#26159;&#20027;&#35201;&#30340;&#35821;&#35328;&#38382;&#39064;&#31867;&#21035;&#20043;&#19968;&#12290;&#23427;&#20204;&#30001;&#19968;&#20010;&#20027;&#35201;&#30340;&#30097;&#38382;&#23376;&#21477;&#32452;&#25104;&#65292;&#20854;&#31572;&#26696;&#26159;&#20108;&#36827;&#21046;&#30340;&#65288;&#32943;&#23450;&#25110;&#21542;&#23450;&#65289;&#12290;&#26497;&#24615;&#38382;&#39064;&#21644;&#31572;&#26696;&#65288;PQA&#65289;&#26159;&#35768;&#22810;&#31038;&#21306;&#21644;&#20854;&#20182;&#32463;&#36807;&#31579;&#36873;&#30340;&#38382;&#31572;&#36164;&#28304;&#20013;&#30340;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#36164;&#28304;&#65292;&#20363;&#22914;&#35770;&#22363;&#25110;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#31243;&#24207;&#12290;&#21333;&#29420;&#20351;&#29992;&#26497;&#24615;&#38382;&#39064;&#30340;&#31572;&#26696;&#22312;&#20854;&#20182;&#35821;&#22659;&#20013;&#24182;&#19981;&#26159;&#24456;&#23481;&#26131;&#12290;&#31572;&#26696;&#26159;&#24773;&#22659;&#21270;&#30340;&#65292;&#24182;&#20551;&#35774;&#25552;&#38382;&#32773;&#21644;&#22238;&#31572;&#32773;&#20043;&#38388;&#30340;&#20849;&#20139;&#30693;&#35782;&#20197;&#21450;&#30097;&#38382;&#23376;&#21477;&#37117;&#24050;&#25552;&#20379;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#23558;&#26497;&#24615;&#38382;&#39064;&#30340;&#31572;&#26696;&#37325;&#20889;&#20026;&#33073;&#31163;&#24773;&#22659;&#19988;&#31616;&#27905;&#30340;&#20107;&#23454;&#38472;&#36848;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;Transformer&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#21033;&#29992;&#36719;&#32422;&#26463;&#30830;&#20445;&#21487;&#25511;&#21046;&#30340;&#37325;&#20889;&#65292;&#20351;&#24471;&#36755;&#20986;&#30340;&#38472;&#36848;&#22312;&#35821;&#20041;&#19978;&#31561;&#21516;&#20110;&#20854;PQA&#36755;&#20837;&#12290;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#22312;&#19977;&#20010;&#29420;&#31435;&#30340;PQA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Yes/No or polar questions represent one of the main linguistic question categories. They consist of a main interrogative clause, for which the answer is binary (assertion or negation). Polar questions and answers (PQA) represent a valuable knowledge resource present in many community and other curated QA sources, such as forums or e-commerce applications. Using answers to polar questions alone in other contexts is not trivial. Answers are contextualized, and presume that the interrogative question clause and any shared knowledge between the asker and answerer are provided.  We address the problem of controllable rewriting of answers to polar questions into decontextualized and succinct factual statements. We propose a Transformer sequence to sequence model that utilizes soft-constraints to ensure controllable rewriting, such that the output statement is semantically equivalent to its PQA input. Evaluation on three separate PQA datasets as measured through automated and human evaluation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#38899;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38899;&#39057;&#24187;&#21548;&#38382;&#39064;&#12290;&#36890;&#36807;&#25910;&#38598;1,000&#20010;&#21477;&#23376;&#24182;&#36827;&#34892;&#20998;&#31867;&#65292;&#30740;&#31350;&#21457;&#29616;&#26377;332&#20010;&#21477;&#23376;&#20986;&#29616;&#20102;&#24187;&#21548;&#29616;&#35937;&#65292;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#25991;&#26412;&#27169;&#22411;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#24494;&#35843;&#65292;&#32467;&#26524;&#26174;&#31034;&#24494;&#35843;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09774</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#38899;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38899;&#39057;&#24187;&#21548;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Audio Hallucinations in Large Audio-Video Language Models. (arXiv:2401.09774v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#38899;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38899;&#39057;&#24187;&#21548;&#38382;&#39064;&#12290;&#36890;&#36807;&#25910;&#38598;1,000&#20010;&#21477;&#23376;&#24182;&#36827;&#34892;&#20998;&#31867;&#65292;&#30740;&#31350;&#21457;&#29616;&#26377;332&#20010;&#21477;&#23376;&#20986;&#29616;&#20102;&#24187;&#21548;&#29616;&#35937;&#65292;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#25991;&#26412;&#27169;&#22411;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#24494;&#35843;&#65292;&#32467;&#26524;&#26174;&#31034;&#24494;&#35843;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#38899;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20026;&#35270;&#39057;&#21644;&#38899;&#39057;&#29983;&#25104;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26377;&#26102;&#20250;&#24573;&#30053;&#38899;&#39057;&#20869;&#23481;&#65292;&#20165;&#26681;&#25454;&#35270;&#35273;&#20449;&#24687;&#29983;&#25104;&#38899;&#39057;&#25551;&#36848;&#12290;&#26412;&#25991;&#23558;&#27492;&#31216;&#20026;&#38899;&#39057;&#24187;&#21548;&#65292;&#24182;&#23545;&#22823;&#22411;&#38899;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#21548;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;1000&#20010;&#21477;&#23376;&#65292;&#36890;&#36807;&#35810;&#38382;&#38899;&#39057;&#20449;&#24687;&#65292;&#24182;&#27880;&#37322;&#23427;&#20204;&#26159;&#21542;&#21253;&#21547;&#24187;&#21548;&#12290;&#22914;&#26524;&#19968;&#20010;&#21477;&#23376;&#26159;&#24187;&#21548;&#30340;&#65292;&#25105;&#20204;&#36824;&#23545;&#24187;&#21548;&#31867;&#22411;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26377;332&#20010;&#21477;&#23376;&#26159;&#24187;&#21548;&#30340;&#65292;&#24182;&#19988;&#22312;&#27599;&#31181;&#24187;&#21548;&#31867;&#22411;&#30340;&#21517;&#35789;&#21644;&#21160;&#35789;&#20013;&#35266;&#23519;&#21040;&#20102;&#26126;&#26174;&#30340;&#36235;&#21183;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#25991;&#26412;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#24494;&#35843;&#35774;&#32622;&#19979;&#35299;&#20915;&#20102;&#38899;&#39057;&#24187;&#21548;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#38646;&#26679;&#26412;&#27169;&#22411;&#30340;&#24615;&#33021;&#36739;&#39640;&#65288;F1&#20026;52.2%&#65289;&#65292;&#20248;&#20110;&#38543;&#26426;&#27169;&#22411;&#65288;40.3%&#65289;&#65292;&#32780;&#24494;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#20026;87.9%&#65292;&#36229;&#36807;&#20102;&#38646;&#26679;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large audio-video language models can generate descriptions for both video and audio. However, they sometimes ignore audio content, producing audio descriptions solely reliant on visual information. This paper refers to this as audio hallucinations and analyzes them in large audio-video language models. We gather 1,000 sentences by inquiring about audio information and annotate them whether they contain hallucinations. If a sentence is hallucinated, we also categorize the type of hallucination. The results reveal that 332 sentences are hallucinated with distinct trends observed in nouns and verbs for each hallucination type. Based on this, we tackle a task of audio hallucination classification using pre-trained audio-text models in the zero-shot and fine-tuning settings. Our experimental results reveal that the zero-shot models achieve higher performance (52.2% in F1) than the random (40.3%) and the fine-tuning models achieve 87.9%, outperforming the zero-shot models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#20247;&#21253;&#26631;&#31614;&#21644;LLM&#26631;&#31614;&#30340;&#36136;&#37327;&#27604;&#36739;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22312;&#25968;&#25454;&#26631;&#27880;&#20219;&#21153;&#19978;&#32988;&#36807;&#20247;&#21253;&#12290;&#26412;&#30740;&#31350;&#23545;&#29616;&#26377;&#20247;&#21253;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#21033;&#29992;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#35780;&#20272;&#26631;&#27880;&#36136;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#32858;&#21512;&#26631;&#31614;&#30340;&#36136;&#37327;&#23545;&#20110;&#20247;&#21253;&#20219;&#21153;&#23588;&#20026;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.09760</link><description>&lt;p&gt;
&#36890;&#36807;&#26631;&#31614;&#32858;&#21512;&#65292;&#20247;&#21253;&#21644;LLM&#30340;&#26631;&#27880;&#36136;&#37327;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study on Annotation Quality of Crowdsourcing and LLM via Label Aggregation. (arXiv:2401.09760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#20247;&#21253;&#26631;&#31614;&#21644;LLM&#26631;&#31614;&#30340;&#36136;&#37327;&#27604;&#36739;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22312;&#25968;&#25454;&#26631;&#27880;&#20219;&#21153;&#19978;&#32988;&#36807;&#20247;&#21253;&#12290;&#26412;&#30740;&#31350;&#23545;&#29616;&#26377;&#20247;&#21253;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#21033;&#29992;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#35780;&#20272;&#26631;&#27880;&#36136;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#32858;&#21512;&#26631;&#31614;&#30340;&#36136;&#37327;&#23545;&#20110;&#20247;&#21253;&#20219;&#21153;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26159;&#21542;&#33021;&#22312;&#25968;&#25454;&#26631;&#27880;&#20219;&#21153;&#19978;&#32988;&#36807;&#20247;&#21253;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#12290;&#19968;&#20123;&#30740;&#31350;&#36890;&#36807;&#37319;&#38598;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;&#20010;&#20307;&#20247;&#21253;&#24037;&#20316;&#32773;&#21644;LLM&#24037;&#20316;&#32773;&#22312;&#19968;&#20123;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#24179;&#22343;&#34920;&#29616;&#26469;&#39564;&#35777;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#30740;&#31350;&#20247;&#21253;&#26631;&#27880;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#23578;&#26410;&#22312;&#36825;&#26679;&#30340;&#35780;&#20272;&#20013;&#24471;&#21040;&#21033;&#29992;&#65292;&#36825;&#21487;&#33021;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#25552;&#20379;&#21487;&#38752;&#30340;&#35780;&#20272;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32858;&#21512;&#26631;&#31614;&#30340;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#22312;&#20351;&#29992;&#20247;&#21253;&#26102;&#65292;&#20174;&#22810;&#20010;&#20247;&#21253;&#26631;&#31614;&#21040;&#30456;&#21516;&#23454;&#20363;&#30340;&#20272;&#35745;&#26631;&#31614;&#26159;&#26368;&#32456;&#25910;&#38598;&#21040;&#30340;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#21738;&#20123;&#29616;&#26377;&#30340;&#20247;&#21253;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#20110;&#27604;&#36739;&#30740;&#31350;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#12290;&#28982;&#21518;&#25105;&#20204;&#27604;&#36739;&#20102;&#20010;&#20307;&#20247;&#21253;&#26631;&#31614;&#21644;LLM&#26631;&#31614;&#30340;&#36136;&#37327;&#65292;&#24182;&#23545;&#32858;&#21512;&#26631;&#31614;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whether Large Language Models (LLMs) can outperform crowdsourcing on the data annotation task is attracting interest recently. Some works verified this issue with the average performance of individual crowd workers and LLM workers on some specific NLP tasks by collecting new datasets. However, on the one hand, existing datasets for the studies of annotation quality in crowdsourcing are not yet utilized in such evaluations, which potentially provide reliable evaluations from a different viewpoint. On the other hand, the quality of these aggregated labels is crucial because, when utilizing crowdsourcing, the estimated labels aggregated from multiple crowd labels to the same instances are the eventually collected labels. Therefore, in this paper, we first investigate which existing crowdsourcing datasets can be used for a comparative study and create a benchmark. We then compare the quality between individual crowd labels and LLM labels and make the evaluations on the aggregated labels. I
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#20102;&#27721;&#35821;&#35789;&#27719;&#32593;(CWN)&#19978;&#24120;&#35265;&#21333;&#35789;&#20041;&#39033;&#28040;&#27495;&#27169;&#22411;&#21644;&#28857;&#23545;&#35937;&#20316;&#20026;&#19987;&#26377;&#21517;&#35789;&#30340;&#28040;&#27495;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#35299;&#20915;&#24120;&#35265;&#21517;&#35789;&#21644;&#19987;&#26377;&#21517;&#35789;&#30340;&#27495;&#20041;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.09758</link><description>&lt;p&gt;
&#35299;&#20915;&#21629;&#21517;&#23454;&#20307;&#20013;&#30340;&#27491;&#35268;&#22810;&#20041;&#24615;
&lt;/p&gt;
&lt;p&gt;
Resolving Regular Polysemy in Named Entities. (arXiv:2401.09758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#20102;&#27721;&#35821;&#35789;&#27719;&#32593;(CWN)&#19978;&#24120;&#35265;&#21333;&#35789;&#20041;&#39033;&#28040;&#27495;&#27169;&#22411;&#21644;&#28857;&#23545;&#35937;&#20316;&#20026;&#19987;&#26377;&#21517;&#35789;&#30340;&#28040;&#27495;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#35299;&#20915;&#24120;&#35265;&#21517;&#35789;&#21644;&#19987;&#26377;&#21517;&#35789;&#30340;&#27495;&#20041;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#20041;&#28040;&#27495;&#20027;&#35201;&#35299;&#20915;&#22522;&#20110;&#39044;&#23450;&#20041;&#20041;&#39033;&#24211;&#30340;&#24120;&#35265;&#21333;&#35789;&#30340;&#35821;&#20041;&#27169;&#31946;&#24615;&#12290;&#30456;&#21453;&#65292;&#19987;&#26377;&#21517;&#35789;&#36890;&#24120;&#34987;&#35748;&#20026;&#34920;&#31034;&#29305;&#23450;&#30340;&#29616;&#23454;&#19990;&#30028;&#25351;&#31216;&#12290;&#19968;&#26086;&#30830;&#23450;&#20102;&#24341;&#29992;&#65292;&#23601;&#34987;&#35748;&#20026;&#35299;&#20915;&#20102;&#27495;&#20041;&#12290;&#28982;&#32780;&#65292;&#19987;&#26377;&#21517;&#35789;&#20063;&#36890;&#36807;&#21464;&#25104;&#36890;&#29992;&#21517;&#35789;&#32780;&#20135;&#29983;&#27495;&#20041;&#65292;&#21363;&#23427;&#20204;&#34920;&#29616;&#24471;&#20687;&#24120;&#35265;&#21333;&#35789;&#65292;&#24182;&#19988;&#21487;&#33021;&#34920;&#31034;&#23427;&#20204;&#24341;&#29992;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#25105;&#20204;&#25552;&#35758;&#36890;&#36807;&#27491;&#35268;&#22810;&#20041;&#24615;&#30340;&#20809;&#29031;&#26469;&#35299;&#20915;&#19987;&#26377;&#21517;&#35789;&#30340;&#27495;&#20041;&#65292;&#25105;&#20204;&#23558;&#20854;&#27491;&#24335;&#21270;&#20026;&#28857;&#23545;&#35937;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32467;&#21512;&#20102;&#27721;&#35821;&#35789;&#27719;&#32593;(CWN)&#19978;&#24120;&#35265;&#21333;&#35789;&#20041;&#39033;&#28040;&#27495;&#27169;&#22411;&#21644;&#28857;&#23545;&#35937;&#20316;&#20026;&#19987;&#26377;&#21517;&#35789;&#30340;&#28040;&#27495;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#22522;&#20110;&#35789;&#27719;&#32593;&#20041;&#39033;&#21644;&#20363;&#21477;&#30340;&#27169;&#22411;&#26550;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#24120;&#35265;&#21517;&#35789;&#21644;&#19987;&#26377;&#21517;&#35789;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#65292;&#21363;&#20351;&#22312;&#30456;&#23545;&#31232;&#30095;&#30340;&#20041;&#39033;&#25968;&#25454;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word sense disambiguation primarily addresses the lexical ambiguity of common words based on a predefined sense inventory. Conversely, proper names are usually considered to denote an ad-hoc real-world referent. Once the reference is decided, the ambiguity is purportedly resolved. However, proper names also exhibit ambiguities through appellativization, i.e., they act like common words and may denote different aspects of their referents. We proposed to address the ambiguities of proper names through the light of regular polysemy, which we formalized as dot objects. This paper introduces a combined word sense disambiguation (WSD) model for disambiguating common words against Chinese Wordnet (CWN) and proper names as dot objects. The model leverages the flexibility of a gloss-based model architecture, which takes advantage of the glosses and example sentences of CWN. We show that the model achieves competitive results on both common and proper nouns, even on a relatively sparse sense dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#32452;&#32455;&#29615;&#22659;&#20013;&#23454;&#29616;&#27178;&#21521;&#32593;&#32476;&#38035;&#40060;&#30340;&#24773;&#20917;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#21453;&#38035;&#40060;&#22522;&#30784;&#35774;&#26045;&#26080;&#27861;&#38450;&#27490;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38035;&#40060;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2401.09727</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27178;&#21521;&#32593;&#32476;&#38035;&#40060;&#65306;&#22823;&#35268;&#27169;&#32452;&#32455;&#29615;&#22659;&#20013;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Lateral Spear Phishing: A Comparative Study in Large-Scale Organizational Settings. (arXiv:2401.09727v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#32452;&#32455;&#29615;&#22659;&#20013;&#23454;&#29616;&#27178;&#21521;&#32593;&#32476;&#38035;&#40060;&#30340;&#24773;&#20917;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#21453;&#38035;&#40060;&#22522;&#30784;&#35774;&#26045;&#26080;&#27861;&#38450;&#27490;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38035;&#40060;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38035;&#40060;&#30005;&#23376;&#37038;&#20214;&#30340;&#20005;&#37325;&#23041;&#32961;&#34987;LLMs&#29983;&#25104;&#39640;&#24230;&#23450;&#21521;&#12289;&#20010;&#24615;&#21270;&#21644;&#33258;&#21160;&#21270;&#30340;&#40060;&#21449;&#24335;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#30340;&#28508;&#21147;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;&#20851;&#20110;LLM&#20419;&#25104;&#30340;&#38035;&#40060;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;&#65306;1&#65289;&#29616;&#26377;&#30340;&#27178;&#21521;&#32593;&#32476;&#38035;&#40060;&#30740;&#31350;&#32570;&#20047;&#38024;&#23545;&#25972;&#20010;&#32452;&#32455;&#36827;&#34892;&#22823;&#35268;&#27169;&#25915;&#20987;&#30340;LLM&#25972;&#21512;&#30340;&#20855;&#20307;&#23457;&#26597;&#65307;2&#65289;&#23613;&#31649;&#21453;&#38035;&#40060;&#22522;&#30784;&#35774;&#26045;&#32463;&#36807;&#24191;&#27867;&#24320;&#21457;&#65292;&#20294;&#20173;&#26080;&#27861;&#38450;&#27490;LLM&#29983;&#25104;&#30340;&#25915;&#20987;&#65292;&#21487;&#33021;&#24433;&#21709;&#21592;&#24037;&#21644;IT&#23433;&#20840;&#20107;&#20214;&#31649;&#29702;&#12290;&#28982;&#32780;&#65292;&#36827;&#34892;&#36825;&#26679;&#30340;&#35843;&#26597;&#30740;&#31350;&#38656;&#35201;&#22312;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#65292;&#35813;&#29615;&#22659;&#22312;&#27491;&#24120;&#19994;&#21153;&#36816;&#20316;&#26399;&#38388;&#24037;&#20316;&#65292;&#24182;&#21453;&#26144;&#20986;&#22823;&#22411;&#32452;&#32455;&#22522;&#30784;&#35774;&#26045;&#30340;&#22797;&#26434;&#24615;&#12290;&#27492;&#35774;&#32622;&#36824;&#24517;&#39035;&#25552;&#20379;&#25152;&#38656;&#30340;&#28789;&#27963;&#24615;&#65292;&#20197;&#20419;&#36827;&#21508;&#31181;&#23454;&#39564;&#26465;&#20214;&#30340;&#23454;&#26045;&#65292;&#29305;&#21035;&#26159;&#38035;&#40060;&#30005;&#23376;&#37038;&#20214;&#30340;&#21046;&#20316;&#21644;&#32452;&#32455;&#33539;&#22260;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
The critical threat of phishing emails has been further exacerbated by the potential of LLMs to generate highly targeted, personalized, and automated spear phishing attacks. Two critical problems concerning LLM-facilitated phishing require further investigation: 1) Existing studies on lateral phishing lack specific examination of LLM integration for large-scale attacks targeting the entire organization, and 2) Current anti-phishing infrastructure, despite its extensive development, lacks the capability to prevent LLM-generated attacks, potentially impacting both employees and IT security incident management. However, the execution of such investigative studies necessitates a real-world environment, one that functions during regular business operations and mirrors the complexity of a large organizational infrastructure. This setting must also offer the flexibility required to facilitate a diverse array of experimental conditions, particularly the incorporation of phishing emails crafted
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#30149;&#27602;&#35875;&#35328;&#21644;&#26131;&#21463;&#25915;&#20987;&#29992;&#25143;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#39044;&#35757;&#32451;&#30340;&#29992;&#25143;&#23884;&#20837;&#12289;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#21644;&#22686;&#24378;&#31038;&#21306;&#20256;&#25773;&#33030;&#24369;&#24615;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#34920;&#31034;&#65292;&#20197;&#21450;&#37319;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09724</link><description>&lt;p&gt;
&#39044;&#27979;&#30149;&#27602;&#35875;&#35328;&#21644;&#26131;&#21463;&#25915;&#20987;&#29992;&#25143;&#30340;&#20449;&#24687;&#20256;&#25773;&#30417;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Predicting Viral Rumors and Vulnerable Users for Infodemic Surveillance. (arXiv:2401.09724v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09724
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#30149;&#27602;&#35875;&#35328;&#21644;&#26131;&#21463;&#25915;&#20987;&#29992;&#25143;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32479;&#19968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#39044;&#35757;&#32451;&#30340;&#29992;&#25143;&#23884;&#20837;&#12289;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#21644;&#22686;&#24378;&#31038;&#21306;&#20256;&#25773;&#33030;&#24369;&#24615;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#34920;&#31034;&#65292;&#20197;&#21450;&#37319;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#30123;&#24773;&#26102;&#20195;&#65292;&#26377;&#25928;&#30417;&#27979;&#36805;&#36895;&#20256;&#25773;&#30340;&#30127;&#29378;&#35875;&#35328;&#65292;&#24182;&#35782;&#21035;&#26131;&#21463;&#25915;&#20987;&#30340;&#29992;&#25143;&#65292;&#23545;&#20110;&#21450;&#26102;&#37319;&#21462;&#39044;&#38450;&#25514;&#26045;&#65292;&#20943;&#36731;&#34394;&#20551;&#20449;&#24687;&#23545;&#31038;&#20250;&#30340;&#36127;&#38754;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#30149;&#27602;&#35875;&#35328;&#21644;&#26131;&#21463;&#25915;&#20987;&#30340;&#29992;&#25143;&#65292;&#20351;&#29992;&#32479;&#19968;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#25105;&#20204;&#39044;&#20808;&#35757;&#32451;&#22522;&#20110;&#32593;&#32476;&#30340;&#29992;&#25143;&#23884;&#20837;&#65292;&#24182;&#21033;&#29992;&#29992;&#25143;&#21644;&#24086;&#23376;&#20043;&#38388;&#30340;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#21450;&#19968;&#20010;&#22686;&#24378;&#31038;&#21306;&#20256;&#25773;&#33030;&#24369;&#24615;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#29992;&#25143;&#21644;&#20256;&#25773;&#22270;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20004;&#31181;&#22810;&#20219;&#21153;&#35757;&#32451;&#31574;&#30053;&#26469;&#20943;&#36731;&#19981;&#21516;&#35774;&#32622;&#20013;&#20219;&#21153;&#20043;&#38388;&#30340;&#36127;&#38754;&#36716;&#31227;&#25928;&#24212;&#65292;&#25552;&#39640;&#26041;&#27861;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#20004;&#20010;&#21253;&#21547;&#20855;&#20307;&#26631;&#27880;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39564;&#35777;&#20449;&#24687;&#20256;&#25773;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the age of the infodemic, it is crucial to have tools for effectively monitoring the spread of rampant rumors that can quickly go viral, as well as identifying vulnerable users who may be more susceptible to spreading such misinformation. This proactive approach allows for timely preventive measures to be taken, mitigating the negative impact of false information on society. We propose a novel approach to predict viral rumors and vulnerable users using a unified graph neural network model. We pre-train network-based user embeddings and leverage a cross-attention mechanism between users and posts, together with a community-enhanced vulnerability propagation (CVP) method to improve user and propagation graph representations. Furthermore, we employ two multi-task training strategies to mitigate negative transfer effects among tasks in different settings, enhancing the overall performance of our approach. We also construct two datasets with ground-truth annotations on information virali
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;Transformer&#22522;&#30784;&#27169;&#22411;&#12289;InfoNCE&#25439;&#22833;&#21644;&#35821;&#35328;&#20999;&#25442;&#26041;&#27861;&#26469;&#35299;&#20915;&#35838;&#31243;&#25512;&#33616;&#20013;&#30340;&#20869;&#23481;&#20914;&#31361;&#21644;&#35821;&#35328;&#32763;&#35793;&#24341;&#36215;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#12289;&#21253;&#23481;&#22810;&#26679;&#24615;&#30340;&#25945;&#32946;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2401.09699</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#19982;InfoNCE&#25439;&#22833;&#21644;&#35821;&#35328;&#20999;&#25442;&#26041;&#27861;&#30340;&#35838;&#31243;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Curriculum Recommendations Using Transformer Base Model with InfoNCE Loss And Language Switching Method. (arXiv:2401.09699v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09699
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;Transformer&#22522;&#30784;&#27169;&#22411;&#12289;InfoNCE&#25439;&#22833;&#21644;&#35821;&#35328;&#20999;&#25442;&#26041;&#27861;&#26469;&#35299;&#20915;&#35838;&#31243;&#25512;&#33616;&#20013;&#30340;&#20869;&#23481;&#20914;&#31361;&#21644;&#35821;&#35328;&#32763;&#35793;&#24341;&#36215;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#12289;&#21253;&#23481;&#22810;&#26679;&#24615;&#30340;&#25945;&#32946;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35838;&#31243;&#25512;&#33616;&#33539;&#24335;&#33268;&#21147;&#20110;&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#25945;&#32946;&#25216;&#26415;&#21644;&#35838;&#31243;&#24320;&#21457;&#39046;&#22495;&#20013;&#20419;&#36827;&#23398;&#20064;&#24179;&#31561;&#12290;&#37492;&#20110;&#29616;&#26377;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#20869;&#23481;&#20914;&#31361;&#21644;&#35821;&#35328;&#32763;&#35793;&#24341;&#36215;&#30340;&#24178;&#25200;&#31561;&#22256;&#38590;&#65292;&#35813;&#33539;&#24335;&#26088;&#22312;&#38754;&#23545;&#24182;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#23427;&#35299;&#20915;&#20102;&#35821;&#35328;&#32763;&#35793;&#24341;&#20837;&#30340;&#20869;&#23481;&#20914;&#31361;&#21644;&#24178;&#25200;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#33021;&#38459;&#30861;&#21019;&#24314;&#20840;&#38754;&#21644;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#12290;&#35813;&#33539;&#24335;&#30340;&#30446;&#26631;&#26159;&#22521;&#20859;&#19968;&#20010;&#26082;&#21253;&#23481;&#22810;&#26679;&#24615;&#21448;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#23398;&#20064;&#32773;&#30340;&#29420;&#29305;&#38656;&#27714;&#23450;&#21046;&#23398;&#20064;&#20307;&#39564;&#30340;&#25945;&#32946;&#29615;&#22659;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35838;&#31243;&#24320;&#21457;&#21644;&#20010;&#24615;&#21270;&#23398;&#20064;&#26041;&#38754;&#24341;&#20837;&#20102;&#19977;&#20010;&#20851;&#38190;&#21019;&#26032;&#12290;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;Transformer&#22522;&#30784;&#27169;&#22411;&#22686;&#24378;&#35745;&#31639;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Curriculum Recommendations paradigm is dedicated to fostering learning equality within the ever-evolving realms of educational technology and curriculum development. In acknowledging the inherent obstacles posed by existing methodologies, such as content conflicts and disruptions from language translation, this paradigm aims to confront and overcome these challenges. Notably, it addresses content conflicts and disruptions introduced by language translation, hindrances that can impede the creation of an all-encompassing and personalized learning experience. The paradigm's objective is to cultivate an educational environment that not only embraces diversity but also customizes learning experiences to suit the distinct needs of each learner. To overcome these challenges, our approach builds upon notable contributions in curriculum development and personalized learning, introducing three key innovations. These include the integration of Transformer Base Model to enhance computational e
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32593;&#32476;&#21644;&#35821;&#35328;&#20998;&#26512;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#22312;&#32447;&#31038;&#32676;&#20013;&#25512;&#24191;&#39278;&#39135;&#32010;&#20081;&#30340;&#21160;&#24577;&#65292;&#35748;&#20026;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25918;&#22823;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20998;&#26512;&#31038;&#32676;&#20869;&#30340;&#35805;&#35821;&#65292;&#25105;&#20204;&#25506;&#27979;&#21040;&#20102;&#19982;&#39278;&#39135;&#32010;&#20081;&#30456;&#20851;&#30340;&#28508;&#22312;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.09647</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#24449;&#22312;&#32447;&#39278;&#39135;&#32010;&#20081;&#31038;&#32676;
&lt;/p&gt;
&lt;p&gt;
Characterizing Online Eating Disorder Communities with Large Language Models. (arXiv:2401.09647v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09647
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32593;&#32476;&#21644;&#35821;&#35328;&#20998;&#26512;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#22312;&#32447;&#31038;&#32676;&#20013;&#25512;&#24191;&#39278;&#39135;&#32010;&#20081;&#30340;&#21160;&#24577;&#65292;&#35748;&#20026;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25918;&#22823;&#20102;&#36825;&#19968;&#29616;&#35937;&#12290;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20998;&#26512;&#31038;&#32676;&#20869;&#30340;&#35805;&#35821;&#65292;&#25105;&#20204;&#25506;&#27979;&#21040;&#20102;&#19982;&#39278;&#39135;&#32010;&#20081;&#30456;&#20851;&#30340;&#28508;&#22312;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39278;&#39135;&#32010;&#20081;&#20316;&#20026;&#19968;&#31181;&#21361;&#38505;&#30340;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#27515;&#20129;&#29575;&#21644;&#21457;&#30149;&#29575;&#65292;&#20854;&#19978;&#21319;&#19982;&#31038;&#20132;&#23186;&#20307;&#19978;&#29702;&#24819;&#21270;&#36523;&#20307;&#24418;&#35937;&#30340;&#27867;&#28389;&#26377;&#20851;&#12290;&#28982;&#32780;&#65292;&#31038;&#20132;&#23186;&#20307;&#19982;&#39278;&#39135;&#32010;&#20081;&#20043;&#38388;&#30340;&#32852;&#31995;&#36828;&#19981;&#27490;&#22914;&#27492;&#12290;&#25105;&#20204;&#35748;&#20026;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#21019;&#24314;&#20102;&#19968;&#20010;&#21453;&#39304;&#24490;&#29615;&#65292;&#25918;&#22823;&#20102;&#25512;&#24191;&#21388;&#39135;&#30151;&#21644;&#26292;&#39135;&#30151;&#31561;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#21644;&#31038;&#32676;&#30340;&#22686;&#38271;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#20351;&#26131;&#21463;&#20260;&#23475;&#30340;&#20010;&#20307;&#33021;&#22815;&#36731;&#26494;&#25214;&#21040;&#24182;&#32852;&#31995;&#21040;&#24535;&#21516;&#36947;&#21512;&#30340;&#20854;&#20182;&#20154;&#65292;&#32780;&#32676;&#20307;&#21160;&#24577;&#36807;&#31243;&#21017;&#40723;&#21169;&#20182;&#20204;&#22312;&#25512;&#24191;&#21644;&#32654;&#21270;&#19982;&#39278;&#39135;&#32010;&#20081;&#30456;&#20851;&#30340;&#26377;&#23475;&#34892;&#20026;&#30340;&#31038;&#32676;&#20013;&#25345;&#32493;&#21442;&#19982;&#12290;&#25105;&#20204;&#36890;&#36807;&#32593;&#32476;&#21644;&#35821;&#35328;&#20998;&#26512;&#30340;&#32452;&#21512;&#65292;&#20174;&#32463;&#39564;&#19978;&#25551;&#36848;&#20102;&#36825;&#19968;&#21160;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#22312;&#32447;&#31038;&#32676;&#20869;&#30340;&#35805;&#35821;&#65292;&#24182;&#23545;&#19982;&#39278;&#39135;&#32010;&#20081;&#30456;&#20851;&#30340;&#35805;&#39064;&#30340;&#24577;&#24230;&#36827;&#34892;&#25506;&#27979;&#65292;&#20197;&#37492;&#21035;&#28508;&#22312;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise in eating disorders, a dangerous mental health condition with high mortality and morbidity, has been linked to the proliferation of idealized body images on social media. However, the link between social media and eating disorders is far more complex. We argue that social media platforms create a feedback loop that amplifies the growth of content and communities that promote eating disorders like anorexia and bulimia. Specifically, social media platforms make it easy for vulnerable individuals to find and connect to like-minded others, while group dynamic processes encourage them to stay engaged within communities that promote and glorify harmful behaviors linked to eating disorders. We characterize this dynamic empirically through a combination of network and language analysis. We describe a novel framework that leverages large language models to analyze the discourse within online communities and probe their attitudes on topics related to eating disorders to identify potenti
&lt;/p&gt;</description></item><item><title>ClimateGPT&#26159;&#19968;&#20010;&#38024;&#23545;&#27668;&#20505;&#21464;&#21270;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#21512;&#25104;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#21270;&#26816;&#32034;&#22686;&#24378;&#21644;&#20351;&#29992;&#32423;&#32852;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09646</link><description>&lt;p&gt;
ClimateGPT: &#23454;&#29616;&#23545;&#27668;&#20505;&#21464;&#21270;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#36827;&#34892;&#21512;&#25104;&#30340;AI&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change. (arXiv:2401.09646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09646
&lt;/p&gt;
&lt;p&gt;
ClimateGPT&#26159;&#19968;&#20010;&#38024;&#23545;&#27668;&#20505;&#21464;&#21270;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#21512;&#25104;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#21270;&#26816;&#32034;&#22686;&#24378;&#21644;&#20351;&#29992;&#32423;&#32852;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ClimateGPT&#65292;&#19968;&#31181;&#29305;&#23450;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#29992;&#20110;&#21512;&#25104;&#27668;&#20505;&#21464;&#21270;&#30340;&#36328;&#23398;&#31185;&#30740;&#31350;&#12290;&#25105;&#20204;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#20102;&#20004;&#20010;7B&#27169;&#22411;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#21253;&#21547;300B&#20010;&#31185;&#23398;&#23548;&#21521;&#30340;&#20196;&#29260;&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#21253;&#21547;&#20102;4.2B&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#20196;&#29260;&#65292;&#31532;&#20108;&#20010;&#27169;&#22411;&#22312;&#39044;&#35757;&#32451;&#21518;&#38024;&#23545;&#27668;&#20505;&#39046;&#22495;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;ClimateGPT-7B&#65292;13B&#21644;70B&#36827;&#34892;&#20102;&#36830;&#32493;&#39044;&#35757;&#32451;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#21253;&#21547;4.2B&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#20196;&#29260;&#65292;&#24182;&#19982;&#27668;&#20505;&#31185;&#23398;&#23478;&#32039;&#23494;&#21512;&#20316;&#21019;&#24314;&#12290;&#20026;&#20102;&#20943;&#23569;&#34394;&#26500;&#29983;&#25104;&#30340;&#25968;&#37327;&#65292;&#25105;&#20204;&#20026;&#27169;&#22411;&#36827;&#34892;&#20102;&#26816;&#32034;&#22686;&#24378;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26816;&#32034;&#31574;&#30053;&#12290;&#20026;&#20102;&#25552;&#39640;&#25105;&#20204;&#27169;&#22411;&#23545;&#38750;&#33521;&#35821;&#20351;&#29992;&#32773;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#32423;&#32852;&#26426;&#22120;&#32763;&#35793;&#65292;&#24182;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#19982;&#32763;&#35793;&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces ClimateGPT, a model family of domain-specific large language models that synthesize interdisciplinary research on climate change. We trained two 7B models from scratch on a science-oriented dataset of 300B tokens. For the first model, the 4.2B domain-specific tokens were included during pre-training and the second was adapted to the climate domain after pre-training. Additionally, ClimateGPT-7B, 13B and 70B are continuously pre-trained from Llama~2 on a domain-specific dataset of 4.2B tokens. Each model is instruction fine-tuned on a high-quality and human-generated domain-specific dataset that has been created in close cooperation with climate scientists. To reduce the number of hallucinations, we optimize the model for retrieval augmentation and propose a hierarchical retrieval strategy. To increase the accessibility of our model to non-English speakers, we propose to make use of cascaded machine translation and show that this approach can perform comparably to 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#38405;&#35835;&#20020;&#24202;&#31508;&#35760;&#65292;&#24739;&#32773;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#29702;&#35299;&#21644;&#33258;&#20449;&#12290;&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#24037;&#20855;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#31616;&#21270;&#21644;&#22686;&#21152;&#19978;&#19979;&#25991;&#65292;&#20351;&#20020;&#24202;&#31508;&#35760;&#26356;&#26131;&#35835;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#22686;&#24378;&#23545;&#24739;&#32773;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2401.09637</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#23545;&#24739;&#32773;&#38405;&#35835;&#20020;&#24202;&#31508;&#35760;&#30340;&#24433;&#21709;&#65306;&#19968;&#20010;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Impact of Large Language Model Assistance on Patients Reading Clinical Notes: A Mixed-Methods Study. (arXiv:2401.09637v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09637
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#38405;&#35835;&#20020;&#24202;&#31508;&#35760;&#65292;&#24739;&#32773;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#29702;&#35299;&#21644;&#33258;&#20449;&#12290;&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#24037;&#20855;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#31616;&#21270;&#21644;&#22686;&#21152;&#19978;&#19979;&#25991;&#65292;&#20351;&#20020;&#24202;&#31508;&#35760;&#26356;&#26131;&#35835;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#22686;&#24378;&#23545;&#24739;&#32773;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#32773;&#36890;&#36807;&#38405;&#35835;&#20182;&#20204;&#30340;&#20020;&#24202;&#31508;&#35760;&#33719;&#24471;&#20102;&#35768;&#22810;&#22909;&#22788;&#65292;&#21253;&#25324;&#22686;&#21152;&#23545;&#33258;&#36523;&#20581;&#24247;&#30340;&#25511;&#21046;&#24863;&#21644;&#23545;&#25252;&#29702;&#35745;&#21010;&#30340;&#29702;&#35299;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#31508;&#35760;&#20013;&#22797;&#26434;&#30340;&#21307;&#23398;&#27010;&#24565;&#21644;&#26415;&#35821;&#38459;&#30861;&#20102;&#24739;&#32773;&#30340;&#29702;&#35299;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#28966;&#34385;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38754;&#21521;&#24739;&#32773;&#30340;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31616;&#21270;&#31508;&#35760;&#12289;&#20174;&#20013;&#25552;&#21462;&#20449;&#24687;&#24182;&#22686;&#21152;&#19978;&#19979;&#25991;&#65292;&#20197;&#20351;&#20020;&#24202;&#31508;&#35760;&#26356;&#26131;&#35835;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#24037;&#20855;&#25552;&#31034;&#25913;&#36827;&#30340;GPT-4&#23545;&#30001;&#20083;&#33146;&#30284;&#24184;&#23384;&#32773;&#25424;&#36192;&#30340;&#30495;&#23454;&#20020;&#24202;&#31508;&#35760;&#21644;&#20020;&#24202;&#21307;&#29983;&#29983;&#25104;&#30340;&#21512;&#25104;&#20020;&#24202;&#31508;&#35760;&#36827;&#34892;&#36825;&#20123;&#22686;&#24378;&#20219;&#21153;&#12290;&#20849;&#26377;12&#26465;&#31508;&#35760;&#65292;3868&#20010;&#23383;&#12290;2023&#24180;6&#26376;&#65292;&#25105;&#20204;&#38543;&#26426;&#20998;&#37197;&#20102;200&#21517;&#32654;&#22269;&#22899;&#24615;&#21442;&#19982;&#32773;&#65292;&#24182;&#21521;&#20182;&#20204;&#20998;&#21457;&#20102;&#19977;&#20010;&#20855;&#26377;&#19981;&#21516;&#31243;&#24230;&#22686;&#24378;&#30340;&#20020;&#24202;&#31508;&#35760;&#12290;&#21442;&#19982;&#32773;&#22238;&#31572;&#20102;&#26377;&#20851;&#27599;&#20010;&#31508;&#35760;&#30340;&#38382;&#39064;&#65292;&#35780;&#20272;&#20102;&#20182;&#20204;&#23545;&#21518;&#32493;&#34892;&#21160;&#30340;&#29702;&#35299;&#21644;&#33258;&#25105;&#25253;&#21578;&#30340;&#33258;&#20449;&#24515;&#12290;&#25105;&#20204;&#21457;&#29616;&#22686;&#24378;&#23545;&#38405;&#35835;&#29702;&#35299;&#21644;&#33258;&#20449;&#24515;&#21451;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patients derive numerous benefits from reading their clinical notes, including an increased sense of control over their health and improved understanding of their care plan. However, complex medical concepts and jargon within clinical notes hinder patient comprehension and may lead to anxiety. We developed a patient-facing tool to make clinical notes more readable, leveraging large language models (LLMs) to simplify, extract information from, and add context to notes. We prompt engineered GPT-4 to perform these augmentation tasks on real clinical notes donated by breast cancer survivors and synthetic notes generated by a clinician, a total of 12 notes with 3868 words. In June 2023, 200 female-identifying US-based participants were randomly assigned three clinical notes with varying levels of augmentations using our tool. Participants answered questions about each note, evaluating their understanding of follow-up actions and self-reported confidence. We found that augmentations were ass
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20351;&#29992;&#25463;&#24452;&#23398;&#20064;&#30340;&#29616;&#35937;&#65292;&#24378;&#35843;&#20102;&#36825;&#31181;&#29616;&#35937;&#23545;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#24182;&#21628;&#21505;&#21152;&#22823;&#23545;&#25463;&#24452;&#23398;&#20064;&#30340;&#30740;&#31350;&#21147;&#24230;&#20197;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.09615</link><description>&lt;p&gt;
&#23398;&#20064;&#25463;&#24452;&#65306;&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35823;&#23548;&#24615;&#25215;&#35834;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Learning Shortcuts: On the Misleading Promise of NLU in Language Models. (arXiv:2401.09615v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09615
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20351;&#29992;&#25463;&#24452;&#23398;&#20064;&#30340;&#29616;&#35937;&#65292;&#24378;&#35843;&#20102;&#36825;&#31181;&#29616;&#35937;&#23545;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#24182;&#21628;&#21505;&#21152;&#22823;&#23545;&#25463;&#24452;&#23398;&#20064;&#30340;&#30740;&#31350;&#21147;&#24230;&#20197;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#24120;&#24120;&#37319;&#29992;&#25463;&#24452;&#65292;&#23548;&#33268;&#22312;&#20915;&#31574;&#35268;&#21017;&#19978;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#24615;&#33021;&#19978;&#20135;&#29983;&#20102;&#19968;&#31181;&#38169;&#35273;&#12290;&#36825;&#19968;&#29616;&#35937;&#22312;&#20934;&#30830;&#35780;&#20272;LLMs&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#19978;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#35813;&#39046;&#22495;&#30340;&#30456;&#20851;&#30740;&#31350;&#36827;&#34892;&#20102;&#31616;&#27905;&#30340;&#27010;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20351;&#29992;&#25463;&#24452;&#23398;&#20064;&#30340;&#24433;&#21709;&#30340;&#35266;&#28857;&#12290;&#26412;&#25991;&#21628;&#21505;&#21152;&#22823;&#23545;&#25463;&#24452;&#23398;&#20064;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#30740;&#31350;&#21147;&#24230;&#65292;&#20026;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#39640;&#30495;&#23454;&#22330;&#26223;&#19979;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#30340;&#26631;&#20934;&#20316;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) has enabled significant performance gains in the field of natural language processing. However, recent studies have found that LLMs often resort to shortcuts when performing tasks, creating an illusion of enhanced performance while lacking generalizability in their decision rules. This phenomenon introduces challenges in accurately assessing natural language understanding in LLMs. Our paper provides a concise survey of relevant research in this area and puts forth a perspective on the implications of shortcut learning in the evaluation of language models, specifically for NLU tasks. This paper urges more research efforts to be put towards deepening our comprehension of shortcut learning, contributing to the development of more robust language models, and raising the standards of NLU evaluation in real-world scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#20107;&#23454;&#23545;&#25239;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#39118;&#26684;&#23545;&#40784;&#65292;&#36991;&#20813;&#20154;&#31867;&#24178;&#39044;&#65292;&#24182;&#25104;&#21151;&#22521;&#20859;&#20986;&#21487;&#21462;&#34892;&#20026;&#21644;&#20943;&#36731;&#19981;&#21487;&#21462;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2401.09566</link><description>&lt;p&gt;
&#20351;&#29992;&#21453;&#20107;&#23454;&#23545;&#25239;&#20248;&#21270;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models with Counterfactual DPO. (arXiv:2401.09566v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#20107;&#23454;&#23545;&#25239;&#20248;&#21270;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#39118;&#26684;&#23545;&#40784;&#65292;&#36991;&#20813;&#20154;&#31867;&#24178;&#39044;&#65292;&#24182;&#25104;&#21151;&#22521;&#20859;&#20986;&#21487;&#21462;&#34892;&#20026;&#21644;&#20943;&#36731;&#19981;&#21487;&#21462;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36827;&#27493;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#19978;&#19979;&#25991;&#36830;&#36143;&#19988;&#28085;&#30422;&#24191;&#27867;&#20027;&#39064;&#30340;&#25991;&#26412;&#34917;&#20840;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35757;&#32451;&#25152;&#38656;&#30340;&#22823;&#37327;&#25968;&#25454;&#20351;&#24471;&#22312;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#38454;&#27573;&#23545;&#40784;&#21709;&#24212;&#39118;&#26684;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#20250;&#37319;&#29992;&#39069;&#22806;&#30340;&#23545;&#40784;&#38454;&#27573;&#65292;&#36827;&#19968;&#27493;&#20351;&#29992;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#26356;&#22909;&#22320;&#23558;&#20854;&#36755;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#23545;&#40784;&#12290;&#34429;&#28982;&#36825;&#20010;&#36807;&#31243;&#26412;&#36523;&#24182;&#27809;&#26377;&#24341;&#20837;&#26032;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#31361;&#20986;&#20102;&#27169;&#22411;&#22266;&#26377;&#30340;&#29983;&#25104;&#39118;&#26684;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;(DPO)&#26694;&#26550;&#20869;&#21033;&#29992;&#21453;&#20107;&#23454;&#25552;&#31034;&#26469;&#23545;&#40784;&#27169;&#22411;&#30340;&#39118;&#26684;&#65292;&#32780;&#19981;&#20381;&#36182;&#20154;&#31867;&#24178;&#39044;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#22521;&#20859;&#20102;&#21487;&#21462;&#30340;&#34892;&#20026;&#65292;&#20943;&#36731;&#20102;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in large language models (LLMs) have demonstrated remarkable capabilities across a diverse range of applications. These models excel in generating text completions that are contextually coherent and cover an extensive array of subjects. However, the vast datasets required for their training make aligning response styles during the pretraining and instruction tuning phases challenging. Consequently, an additional alignment phase is typically employed, wherein the model is further trained with human preference data to better align its outputs with human expectations. While this process doesn't introduce new capabilities per se, it does accentuate generation styles innate to the model. This paper explores the utilization of counterfactual prompting within the framework of Direct Preference Optimization (DPO) to align the model's style without relying on human intervention. We demonstrate that this method effectively instils desirable behaviour, mitigates undesirable ones, and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26469;&#25913;&#36827;&#20998;&#31867;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#23569;&#37327;&#26377;&#26631;&#31614;&#31034;&#20363;&#65292;&#36890;&#36807;&#36830;&#32493;&#21453;&#39304;&#24490;&#29615;&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#36229;&#36234;&#38646;&#26679;&#26412;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#26356;&#24378;&#30340;&#25991;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09555</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#25913;&#36827;&#20998;&#31867;&#24615;&#33021;&#65306;&#26631;&#35760;&#19968;&#20123;&#65292;&#25105;&#20204;&#26631;&#35760;&#20854;&#20313;&#37096;&#20998;
&lt;/p&gt;
&lt;p&gt;
Improving Classification Performance With Human Feedback: Label a few, we label the rest. (arXiv:2401.09555v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26469;&#25913;&#36827;&#20998;&#31867;&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#23569;&#37327;&#26377;&#26631;&#31614;&#31034;&#20363;&#65292;&#36890;&#36807;&#36830;&#32493;&#21453;&#39304;&#24490;&#29615;&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#36229;&#36234;&#38646;&#26679;&#26412;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#26356;&#24378;&#30340;&#25991;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22823;&#37096;&#20998;&#25968;&#25454;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#65292;&#22240;&#27492;&#33719;&#21462;&#36275;&#22815;&#30340;&#26377;&#26631;&#31614;&#25968;&#25454;&#26469;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#65292;&#21363;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26469;&#25913;&#36827;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#20165;&#20351;&#29992;&#19968;&#23567;&#37096;&#20998;&#26377;&#26631;&#31614;&#31034;&#20363;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#29702;&#35299;&#36830;&#32493;&#21453;&#39304;&#24490;&#29615;&#22914;&#20309;&#25913;&#21892;&#27169;&#22411;&#65292;&#20174;&#32780;&#36890;&#36807;&#28176;&#36827;&#24335;&#30340;&#20154;&#31867;&#21442;&#19982;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12289;&#22238;&#24402;&#21644;&#31934;&#30830;&#24230;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#12289;BERT&#21644;SetFit&#65292;&#25105;&#20204;&#26088;&#22312;&#20998;&#26512;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#26377;&#26631;&#31614;&#31034;&#20363;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;Financial Phrasebank&#12289;Banking&#12289;Craigslist&#12289;Trec&#21644;Amazon Reviews&#25968;&#25454;&#38598;&#19978;&#23545;&#27492;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20165;&#20351;&#29992;&#23569;&#37327;&#26377;&#26631;&#31614;&#31034;&#20363;&#23601;&#33021;&#36229;&#36234;&#38646;&#26679;&#26412;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#25552;&#20379;&#22686;&#24378;&#30340;&#25991;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of artificial intelligence, where a vast majority of data is unstructured, obtaining substantial amounts of labeled data to train supervised machine learning models poses a significant challenge. To address this, we delve into few-shot and active learning, where are goal is to improve AI models with human feedback on a few labeled examples. This paper focuses on understanding how a continuous feedback loop can refine models, thereby enhancing their accuracy, recall, and precision through incremental human input. By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and SetFit, we aim to analyze the efficacy of using a limited number of labeled examples to substantially improve model accuracy. We benchmark this approach on the Financial Phrasebank, Banking, Craigslist, Trec, Amazon Reviews datasets to prove that with just a few labeled examples, we are able to surpass the accuracy of zero shot large language models to provide enhanced text classification performa
&lt;/p&gt;</description></item><item><title>BERTologyNavigator&#26159;&#19968;&#20010;&#22522;&#20110;BERT&#35821;&#20041;&#30340;&#39640;&#32423;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#65292;&#32467;&#21512;&#20851;&#31995;&#25277;&#21462;&#21644;BERT&#23884;&#20837;&#65292;&#21487;&#20197;&#22312;DBLP&#30693;&#35782;&#22270;&#35889;&#20013;&#31934;&#30830;&#22320;&#23548;&#33322;&#20851;&#31995;&#65292;&#24182;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.09553</link><description>&lt;p&gt;
BERTologyNavigator: &#22522;&#20110;BERT&#35821;&#20041;&#30340;&#39640;&#32423;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
BERTologyNavigator: Advanced Question Answering with BERT-based Semantics. (arXiv:2401.09553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09553
&lt;/p&gt;
&lt;p&gt;
BERTologyNavigator&#26159;&#19968;&#20010;&#22522;&#20110;BERT&#35821;&#20041;&#30340;&#39640;&#32423;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#65292;&#32467;&#21512;&#20851;&#31995;&#25277;&#21462;&#21644;BERT&#23884;&#20837;&#65292;&#21487;&#20197;&#22312;DBLP&#30693;&#35782;&#22270;&#35889;&#20013;&#31934;&#30830;&#22320;&#23548;&#33322;&#20851;&#31995;&#65292;&#24182;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#38598;&#25104;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;BERTologyNavigator&#8212;&#8212;&#19968;&#20010;&#23558;&#20851;&#31995;&#25277;&#21462;&#25216;&#26415;&#21644;BERT&#23884;&#20837;&#30456;&#32467;&#21512;&#30340;&#20004;&#38454;&#27573;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;DBLP&#30693;&#35782;&#22270;&#35889;&#20013;&#23548;&#33322;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19987;&#27880;&#20110;&#25552;&#21462;&#19968;&#36339;&#20851;&#31995;&#21644;&#26631;&#35760;&#30340;&#20505;&#36873;&#23545;&#65292;&#28982;&#21518;&#22312;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;BERT&#30340;CLS&#23884;&#20837;&#21644;&#20854;&#20182;&#21551;&#21457;&#24335;&#26041;&#27861;&#36827;&#34892;&#20851;&#31995;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;Scholarly QALD&#30340;DBLP QuAD Final&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;0.2175&#30340;F1&#20998;&#25968;&#65292;&#32780;&#22312;DBLP QuAD&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#19978;&#22312;QA&#38454;&#27573;&#36798;&#21040;&#20102;0.98&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development and integration of knowledge graphs and language models has significance in artificial intelligence and natural language processing. In this study, we introduce the BERTologyNavigator -- a two-phased system that combines relation extraction techniques and BERT embeddings to navigate the relationships within the DBLP Knowledge Graph (KG). Our approach focuses on extracting one-hop relations and labelled candidate pairs in the first phases. This is followed by employing BERT's CLS embeddings and additional heuristics for relation selection in the second phase. Our system reaches an F1 score of 0.2175 on the DBLP QuAD Final test dataset for Scholarly QALD and 0.98 F1 score on the subset of the DBLP QuAD test dataset during the QA phase.
&lt;/p&gt;</description></item><item><title>LoMA&#26159;&#19968;&#31181;&#26080;&#25439;&#21387;&#32553;&#30340;&#20869;&#23384;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#25991;&#26412;&#24182;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2401.09486</link><description>&lt;p&gt;
LoMA: &#26080;&#25439;&#21387;&#32553;&#30340;&#20869;&#23384;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
LoMA: Lossless Compressed Memory Attention. (arXiv:2401.09486v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09486
&lt;/p&gt;
&lt;p&gt;
LoMA&#26159;&#19968;&#31181;&#26080;&#25439;&#21387;&#32553;&#30340;&#20869;&#23384;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#25991;&#26412;&#24182;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#38271;&#25991;&#26412;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#37325;&#35201;&#30340;&#33021;&#21147;&#20043;&#19968;&#65292;&#20294;&#38543;&#30528;&#25991;&#26412;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#36164;&#28304;&#28040;&#32791;&#20063;&#24613;&#21095;&#22686;&#21152;&#12290;&#30446;&#21069;&#65292;&#36890;&#36807;&#21387;&#32553;KV&#32531;&#23384;&#26469;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#29616;&#26377;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#37117;&#26377;&#19968;&#20010;&#20849;&#21516;&#30340;&#32570;&#28857;&#65306;&#21387;&#32553;&#26159;&#26377;&#25439;&#30340;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#22312;&#21387;&#32553;&#36807;&#31243;&#20013;&#20449;&#24687;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#20002;&#22833;&#12290;&#22914;&#26524;&#21387;&#32553;&#29575;&#24456;&#39640;&#65292;&#20002;&#22833;&#37325;&#35201;&#20449;&#24687;&#30340;&#27010;&#29575;&#20250;&#22823;&#22823;&#22686;&#21152;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#26080;&#25439;&#21387;&#32553;&#30340;&#20869;&#23384;&#27880;&#24847;&#21147;&#65288;LoMA&#65289;&#65292;&#21487;&#20197;&#26681;&#25454;&#19968;&#32452;&#21387;&#32553;&#27604;&#29575;&#23558;&#20449;&#24687;&#26080;&#25439;&#21387;&#32553;&#25104;&#29305;&#27530;&#30340;&#20869;&#23384;&#20196;&#29260;KV&#23545;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LoMA&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#39640;&#25928;&#35757;&#32451;&#19988;&#20855;&#26377;&#38750;&#24120;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to handle long texts is one of the most important capabilities of Large Language Models (LLMs), but as the text length increases, the consumption of resources also increases dramatically. At present, reducing resource consumption by compressing the KV cache is a common approach. Although there are many existing compression methods, they share a common drawback: the compression is not lossless. That is, information is inevitably lost during the compression process. If the compression rate is high, the probability of losing important information increases dramatically. We propose a new method, Lossless Compressed Memory Attention (LoMA), which allows for lossless compression of information into special memory token KV pairs according to a set compression ratio. Our experiments have achieved remarkable results, demonstrating that LoMA can be efficiently trained and has very effective performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29992;&#25143;&#27880;&#35270;&#27880;&#24847;&#21147;&#23545;&#40784;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#22330;&#26223;&#21644;&#22810;&#20010;&#29289;&#20307;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.09454</link><description>&lt;p&gt;
Voila-A: &#29992;&#29992;&#25143;&#27880;&#35270;&#27880;&#24847;&#21147;&#23545;&#40784;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Voila-A: Aligning Vision-Language Models with User's Gaze Attention. (arXiv:2401.09454v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29992;&#25143;&#27880;&#35270;&#27880;&#24847;&#21147;&#23545;&#40784;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#22312;&#22788;&#29702;&#22797;&#26434;&#22330;&#26223;&#21644;&#22810;&#20010;&#29289;&#20307;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#35270;&#35273;&#21644;&#35821;&#35328;&#29702;&#35299;&#30340;&#25972;&#21512;&#36890;&#36807;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;VLMs&#22312;&#22788;&#29702;&#22797;&#26434;&#22330;&#26223;&#21644;&#22810;&#20010;&#29289;&#20307;&#30340;&#23454;&#38469;&#24212;&#29992;&#20197;&#21450;&#19982;&#20154;&#31867;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#27880;&#24847;&#21147;&#27169;&#24335;&#30456;&#19968;&#33268;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#36890;&#36807;&#22686;&#24378;&#29616;&#23454;&#65288;AR&#65289;&#25110;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#35774;&#22791;&#25910;&#38598;&#30340;&#27880;&#35270;&#20449;&#24687;&#65292;&#20316;&#20026;&#20154;&#31867;&#27880;&#24847;&#21147;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;VLMs&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;Voila-A&#65292;&#20197;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#25968;&#30334;&#20998;&#38047;&#30340;&#27880;&#35270;&#25968;&#25454;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#26412;&#22320;&#21270;&#30340;&#21465;&#20107;&#26469;&#27169;&#25311;&#20154;&#31867;&#30340;&#27880;&#35270;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#21160;&#25968;&#25454;&#27880;&#37322;&#27969;&#27700;&#32447;&#65292;&#21033;&#29992;GPT-4&#29983;&#25104;&#20102;VOILA-COCO&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#26032;&#20102;Voila Perceiver&#27169;&#22359;&#65292;&#23558;&#27880;&#35270;&#20449;&#24687;&#25972;&#21512;&#21040;VL&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the integration of vision and language understanding has led to significant advancements in artificial intelligence, particularly through Vision-Language Models (VLMs). However, existing VLMs face challenges in handling real-world applications with complex scenes and multiple objects, as well as aligning their focus with the diverse attention patterns of human users. In this paper, we introduce gaze information, feasibly collected by AR or VR devices, as a proxy for human attention to guide VLMs and propose a novel approach, Voila-A, for gaze alignment to enhance the interpretability and effectiveness of these models in real-world applications. First, we collect hundreds of minutes of gaze data to demonstrate that we can mimic human gaze modalities using localized narratives. We then design an automatic data annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset. Additionally, we innovate the Voila Perceiver modules to integrate gaze information into VL
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26041;&#27861;&#26469;&#35299;&#37322;&#23391;&#21152;&#25289;&#35821;Memes&#30340;&#24773;&#24863;&#65292;&#20197;&#22635;&#34917;&#27492;&#39046;&#22495;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#23545;&#27604;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;MemoSen&#25968;&#25454;&#38598;&#24182;&#34920;&#26126;&#20854;&#20934;&#30830;&#29575;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22312;&#23391;&#21152;&#25289;&#35821;Memes&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.09446</link><description>&lt;p&gt;
&#12298;&#21487;&#35299;&#37322;&#30340;&#23391;&#21152;&#25289;&#35821;Memes&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12299;
&lt;/p&gt;
&lt;p&gt;
Explainable Multimodal Sentiment Analysis on Bengali Memes. (arXiv:2401.09446v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09446
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26041;&#27861;&#26469;&#35299;&#37322;&#23391;&#21152;&#25289;&#35821;Memes&#30340;&#24773;&#24863;&#65292;&#20197;&#22635;&#34917;&#27492;&#39046;&#22495;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#23545;&#27604;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;MemoSen&#25968;&#25454;&#38598;&#24182;&#34920;&#26126;&#20854;&#20934;&#30830;&#29575;&#30340;&#23616;&#38480;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22312;&#23391;&#21152;&#25289;&#35821;Memes&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Memes&#24050;&#25104;&#20026;&#25968;&#23383;&#26102;&#20195;&#29420;&#29305;&#32780;&#26377;&#25928;&#30340;&#27807;&#36890;&#24418;&#24335;&#65292;&#21560;&#24341;&#20102;&#22312;&#32447;&#31038;&#21306;&#65292;&#24182;&#36328;&#36234;&#25991;&#21270;&#38556;&#30861;&#12290;&#23613;&#31649;Memes&#32463;&#24120;&#21644;&#24189;&#40664;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#20294;&#23427;&#20204;&#26377;&#30528;&#20256;&#36798;&#24191;&#27867;&#24773;&#24863;&#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#21253;&#25324;&#24555;&#20048;&#12289;&#35773;&#21050;&#12289;&#27822;&#20007;&#31561;&#12290;&#22312;&#20449;&#24687;&#26102;&#20195;&#65292;&#29702;&#35299;&#21644;&#35299;&#37322;Memes&#32972;&#21518;&#30340;&#24773;&#24863;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#25506;&#32034;&#20102;&#22522;&#20110;&#25991;&#26412;&#12289;&#22522;&#20110;&#22270;&#20687;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#23548;&#33268;&#20102;&#20687;CAPSAN&#21644;PromptHate&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#29992;&#20110;&#26816;&#27979;&#21508;&#31181;Memes&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23391;&#21152;&#25289;&#35821;Memes&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#30740;&#31350;&#20173;&#28982;&#31232;&#32570;&#65292;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#25968;&#37327;&#26377;&#38480;&#12290;&#26368;&#36817;&#30340;&#19968;&#20010;&#36129;&#29486;&#26159;&#24341;&#20837;&#20102;MemoSen&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#25152;&#23454;&#29616;&#30340;&#20934;&#30830;&#29575;&#26126;&#26174;&#36739;&#20302;&#65292;&#24182;&#19988;&#25968;&#25454;&#38598;&#20998;&#24067;&#19981;&#24179;&#34913;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;ResNet50&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memes have become a distinctive and effective form of communication in the digital era, attracting online communities and cutting across cultural barriers. Even though memes are frequently linked with humor, they have an amazing capacity to convey a wide range of emotions, including happiness, sarcasm, frustration, and more. Understanding and interpreting the sentiment underlying memes has become crucial in the age of information. Previous research has explored text-based, image-based, and multimodal approaches, leading to the development of models like CAPSAN and PromptHate for detecting various meme categories. However, the study of low-resource languages like Bengali memes remains scarce, with limited availability of publicly accessible datasets. A recent contribution includes the introduction of the MemoSen dataset. However, the achieved accuracy is notably low, and the dataset suffers from imbalanced distribution. In this study, we employed a multimodal approach using ResNet50 and
&lt;/p&gt;</description></item><item><title>RoleCraft-GLM&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#20010;&#24615;&#21270;&#20114;&#21160;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29420;&#29305;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#32454;&#33268;&#20837;&#24494;&#30340;&#35282;&#33394;&#21457;&#23637;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21453;&#26144;&#35282;&#33394;&#20010;&#24615;&#29305;&#24449;&#21644;&#24773;&#24863;&#30340;&#23545;&#35805;&#65292;&#25552;&#21319;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.09432</link><description>&lt;p&gt;
RoleCraft-GLM&#65306;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;
&lt;/p&gt;
&lt;p&gt;
RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language Models. (arXiv:2401.09432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09432
&lt;/p&gt;
&lt;p&gt;
RoleCraft-GLM&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#20010;&#24615;&#21270;&#20114;&#21160;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29420;&#29305;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#32454;&#33268;&#20837;&#24494;&#30340;&#35282;&#33394;&#21457;&#23637;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21453;&#26144;&#35282;&#33394;&#20010;&#24615;&#29305;&#24449;&#21644;&#24773;&#24863;&#30340;&#23545;&#35805;&#65292;&#25552;&#21319;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;RoleCraft-GLM&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#12290;RoleCraft-GLM&#35299;&#20915;&#20102;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#32570;&#20047;&#20010;&#24615;&#21270;&#20114;&#21160;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#35814;&#32454;&#25551;&#32472;&#24773;&#24863;&#32454;&#33147;&#30340;&#35282;&#33394;&#21051;&#30011;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#32452;&#29420;&#29305;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#20174;&#20256;&#32479;&#30340;&#20197;&#21517;&#20154;&#20026;&#20013;&#24515;&#30340;&#35282;&#33394;&#36716;&#21464;&#20026;&#22810;&#26679;&#21270;&#30340;&#38750;&#21517;&#20154;&#35282;&#33394;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#35821;&#35328;&#24314;&#27169;&#20114;&#21160;&#30340;&#30495;&#23454;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#32454;&#33268;&#20837;&#24494;&#30340;&#35282;&#33394;&#21457;&#23637;&#65292;&#30830;&#20445;&#23545;&#35805;&#26082;&#30495;&#23454;&#21448;&#24773;&#24863;&#20849;&#40483;&#12290;&#36890;&#36807;&#22810;&#20010;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;RoleCraft-GLM&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#26174;&#20102;&#23427;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#20934;&#30830;&#21453;&#26144;&#35282;&#33394;&#30340;&#20010;&#24615;&#29305;&#24449;&#21644;&#24773;&#24863;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;&#24635;&#20043;&#65292;RoleCraft-GLM&#26631;&#24535;&#30528;&#19968;&#20010;&#21019;&#26032;&#30340;&#37324;&#31243;&#30865;&#65292;&#25512;&#21160;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents RoleCraft-GLM, an innovative framework aimed at enhancing personalized role-playing with Large Language Models (LLMs). RoleCraft-GLM addresses the key issue of lacking personalized interactions in conversational AI, and offers a solution with detailed and emotionally nuanced character portrayals. We contribute a unique conversational dataset that shifts from conventional celebrity-centric characters to diverse, non-celebrity personas, thus enhancing the realism and complexity of language modeling interactions. Additionally, our approach includes meticulous character development, ensuring dialogues are both realistic and emotionally resonant. The effectiveness of RoleCraft-GLM is validated through various case studies, highlighting its versatility and skill in different scenarios. Our framework excels in generating dialogues that accurately reflect characters' personality traits and emotions, thereby boosting user engagement. In conclusion, RoleCraft-GLM marks a sign
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23545;&#27604;&#24615;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#30340;&#26041;&#27861;&#65292;&#24357;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#33021;&#19982;&#20256;&#32479;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32763;&#35793;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.08417</link><description>&lt;p&gt;
&#23545;&#27604;&#24615;&#20559;&#22909;&#20248;&#21270;&#65306;&#25512;&#21160;&#26426;&#22120;&#32763;&#35793;&#20013;LLM&#24615;&#33021;&#30340;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation. (arXiv:2401.08417v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#23545;&#27604;&#24615;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#30340;&#26041;&#27861;&#65292;&#24357;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#24615;&#33021;&#19982;&#20256;&#32479;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#32763;&#35793;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#31561;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#8212;&#8212;7B&#25110;&#32773;13B&#21442;&#25968;&#30340;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#34920;&#29616;&#26368;&#22909;&#30340;13B LLM&#32763;&#35793;&#27169;&#22411;&#65292;&#22914;ALMA&#65292;&#20063;&#26080;&#27861;&#36798;&#21040;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#20256;&#32479;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32763;&#35793;&#27169;&#22411;&#25110;&#32773;&#26356;&#22823;&#35268;&#27169;&#30340;LLM&#65288;&#22914;GPT-4&#65289;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24615;&#33021;&#24046;&#36317;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#30417;&#30563;&#24494;&#35843;&#22312;MT&#20219;&#21153;&#20013;&#38024;&#23545;LLM&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24378;&#35843;&#20102;&#23613;&#31649;&#26159;&#20154;&#24037;&#29983;&#25104;&#30340;&#21442;&#32771;&#25968;&#25454;&#65292;&#20294;&#20854;&#20013;&#23384;&#22312;&#36136;&#37327;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#19982;&#27169;&#20223;&#21442;&#32771;&#32763;&#35793;&#30340;SFT&#30456;&#21453;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#27604;&#24615;&#20559;&#22909;&#20248;&#21270;&#65288;CPO&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35757;&#32451;&#27169;&#22411;&#36991;&#20813;&#29983;&#25104;&#20165;&#20165;&#21512;&#20046;&#35201;&#27714;&#20294;&#19981;&#23436;&#32654;&#30340;&#32763;&#35793;&#12290;&#23558;CPO&#24212;&#29992;&#20110;&#20165;&#26377;22K&#23545;&#21477;&#23376;&#21644;12M&#21442;&#25968;&#30340;ALMA&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#24471;&#21040;&#30340;&#27169;&#22411;&#31216;&#20026;ALMA-R&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#36798;&#21040;&#25110;&#36229;&#36807;WMT&#27604;&#36187;&#30340;&#33719;&#32988;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moderate-sized large language models (LLMs) -- those with 7B or 13B parameters -- exhibit promising machine translation (MT) performance. However, even the top-performing 13B LLM-based translation models, like ALMA, does not match the performance of state-of-the-art conventional encoder-decoder translation models or larger-scale LLMs such as GPT-4. In this study, we bridge this performance gap. We first assess the shortcomings of supervised fine-tuning for LLMs in the MT task, emphasizing the quality issues present in the reference data, despite being human-generated. Then, in contrast to SFT which mimics reference translations, we introduce Contrastive Preference Optimization (CPO), a novel approach that trains models to avoid generating adequate but not perfect translations. Applying CPO to ALMA models with only 22K parallel sentences and 12M parameters yields significant improvements. The resulting model, called ALMA-R, can match or exceed the performance of the WMT competition winn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#24494;&#35843;&#20004;&#31181;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20892;&#19994;&#25968;&#25454;&#38598;&#30340;&#31649;&#36947;&#21644;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.08406</link><description>&lt;p&gt;
RAG vs Fine-tuning: &#31649;&#36947;&#65292;&#26435;&#34913;&#20197;&#21450;&#22312;&#20892;&#19994;&#19978;&#30340;&#20010;&#26696;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. (arXiv:2401.08406v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#24494;&#35843;&#20004;&#31181;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20892;&#19994;&#25968;&#25454;&#38598;&#30340;&#31649;&#36947;&#21644;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#31243;&#24207;&#26102;&#65292;&#24320;&#21457;&#32773;&#36890;&#24120;&#26377;&#20004;&#31181;&#24120;&#35265;&#26041;&#27861;&#26469;&#25972;&#21512;&#19987;&#26377;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#25968;&#25454;&#65306;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#24494;&#35843;&#12290;RAG&#21033;&#29992;&#22806;&#37096;&#25968;&#25454;&#22686;&#24378;&#25552;&#31034;&#20449;&#24687;&#65292;&#32780;&#24494;&#35843;&#21017;&#23558;&#38468;&#21152;&#30693;&#35782;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#24182;&#19981;&#20026;&#20154;&#25152;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24494;&#35843;&#21644;RAG&#30340;&#31649;&#36947;&#65292;&#24182;&#23545;&#22810;&#31181;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#21253;&#25324;Llama2-13B&#65292;GPT-3.5&#21644;GPT-4&#65289;&#36827;&#34892;&#20102;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#31649;&#36947;&#30001;&#22810;&#20010;&#38454;&#27573;&#32452;&#25104;&#65292;&#21253;&#25324;&#20174;PDF&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#29983;&#25104;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#23558;&#20854;&#29992;&#20110;&#24494;&#35843;&#65292;&#24182;&#21033;&#29992;GPT-4&#35780;&#20272;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;RAG&#21644;&#24494;&#35843;&#31649;&#36947;&#19981;&#21516;&#38454;&#27573;&#24615;&#33021;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#23545;&#20892;&#19994;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#20316;&#20026;&#19968;&#20010;&#20135;&#19994;&#65292;&#20892;&#19994;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#26041;&#38754;&#24182;&#27809;&#26377;&#24471;&#21040;&#24456;&#22823;&#30340;&#28183;&#36879;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are two common ways in which developers are incorporating proprietary and domain-specific data when building applications of Large Language Models (LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the prompt with the external data, while fine-Tuning incorporates the additional knowledge into the model itself. However, the pros and cons of both approaches are not well understood. In this paper, we propose a pipeline for fine-tuning and RAG, and present the tradeoffs of both for multiple popular LLMs, including Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages, including extracting information from PDFs, generating questions and answers, using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We propose metrics to assess the performance of different stages of the RAG and fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset. Agriculture as an industry has not seen much penetration of AI, an
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;AI&#23433;&#20840;&#32771;&#34385;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.07927</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;?
&lt;/p&gt;
&lt;p&gt;
Are self-explanations from Large Language Models faithful?. (arXiv:2401.07927v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07927
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#26159;&#21542;&#21487;&#38752;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;AI&#23433;&#20840;&#32771;&#34385;&#22240;&#32032;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#35780;&#20272;&#20854;&#21487;&#38752;&#24615;&#21644;&#35299;&#37322;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29978;&#33267;&#33021;&#22815;&#25552;&#20379;&#20854;&#34892;&#20026;&#30340;&#35299;&#37322;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#23545;&#20844;&#20247;&#26159;&#30452;&#25509;&#21487;&#35775;&#38382;&#30340;&#65292;&#22240;&#27492;&#23384;&#22312;&#36825;&#26679;&#30340;&#39118;&#38505;&#65292;&#21363;&#20196;&#20154;&#20449;&#26381;&#20294;&#38169;&#35823;&#30340;&#35299;&#37322;&#21487;&#33021;&#23548;&#33268;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#25903;&#25745;&#30340;&#33258;&#20449;&#12290;&#22240;&#27492;&#65292;&#35299;&#37322;&#33021;&#21147;&#21644;&#21487;&#38752;&#24615;&#26159;AI&#23433;&#20840;&#30340;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#12290;&#35780;&#20272;&#33258;&#25105;&#35299;&#37322;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#36807;&#20110;&#22797;&#26434;&#65292;&#26080;&#27861;&#27880;&#37322;&#20160;&#20040;&#26159;&#27491;&#30830;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#27965;&#24615;&#26816;&#27979;&#20316;&#20026;&#21487;&#38752;&#24615;&#30340;&#34913;&#37327;&#25351;&#26631;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35828;&#26576;&#32452;&#35789;&#23545;&#20110;&#20570;&#20986;&#39044;&#27979;&#24456;&#37325;&#35201;&#65292;&#37027;&#20040;&#22312;&#27809;&#26377;&#36825;&#20123;&#35789;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#24212;&#35813;&#26080;&#27861;&#20570;&#20986;&#30456;&#21516;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#33258;&#27965;&#24615;&#26816;&#27979;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#21487;&#38752;&#24615;&#26041;&#27861;&#65292;&#20294;&#20043;&#21069;&#23578;&#26410;&#24212;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#35299;&#37322;&#20013;&#12290;&#25105;&#20204;&#23558;&#33258;&#27965;&#24615;&#26816;&#27979;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs) excel at many tasks, and will even provide explanations for their behavior. Since these models are directly accessible to the public, there is a risk that convincing and wrong explanations can lead to unsupported confidence in LLMs. Therefore, interpretability-faithfulness of self-explanations is an important consideration for AI Safety. Assessing the interpretability-faithfulness of these explanations, termed self-explanations, is challenging as the models are too complex for humans to annotate what is a correct explanation. To address this, we propose employing self-consistency checks as a measure of faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make the same prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been applied to LLM's self-explanations. We apply self-consistency checks to t
&lt;/p&gt;</description></item><item><title>TAROT&#26159;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#32467;&#21512;&#22810;&#31890;&#24230;&#30340;&#20219;&#21153;&#26469;&#25552;&#21319;&#20154;-&#23703;&#20301;&#21305;&#37197;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.07525</link><description>&lt;p&gt;
TAROT&#65306;&#19968;&#31181;&#22312;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#36827;&#34892;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#30340;&#23618;&#27425;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#20154;-&#23703;&#20301;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
TAROT: A Hierarchical Framework with Multitask Co-Pretraining on Semi-Structured Data towards Effective Person-Job Fit. (arXiv:2401.07525v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07525
&lt;/p&gt;
&lt;p&gt;
TAROT&#26159;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#32467;&#21512;&#22810;&#31890;&#24230;&#30340;&#20219;&#21153;&#26469;&#25552;&#21319;&#20154;-&#23703;&#20301;&#21305;&#37197;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;-&#23703;&#20301;&#21305;&#37197;&#26159;&#22312;&#32447;&#25307;&#32856;&#24179;&#21488;&#20013;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#65292;&#22914;&#32844;&#20301;&#25628;&#32034;&#21644;&#20505;&#36873;&#20154;&#25512;&#33616;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#29992;&#25143;&#31616;&#20171;&#21644;&#32844;&#20301;&#25551;&#36848;&#20013;&#30340;&#20016;&#23500;&#25991;&#26412;&#20449;&#24687;&#20197;&#21450;&#29992;&#25143;&#34892;&#20026;&#29305;&#24449;&#21644;&#32844;&#20301;&#20803;&#25968;&#25454;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#19968;&#33324;&#30340;&#38754;&#21521;&#39046;&#22495;&#30340;&#35774;&#35745;&#38590;&#20197;&#25429;&#25417;&#29992;&#25143;&#31616;&#20171;&#21644;&#32844;&#20301;&#25551;&#36848;&#20013;&#30340;&#29420;&#29305;&#32467;&#26500;&#20449;&#24687;&#65292;&#23548;&#33268;&#28508;&#22312;&#35821;&#20041;&#30456;&#20851;&#24615;&#30340;&#20007;&#22833;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TAROT&#65292;&#19968;&#31181;&#23618;&#27425;&#21270;&#30340;&#22810;&#20219;&#21153;&#20849;&#21516;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#20449;&#24687;&#24615;&#25991;&#26412;&#23884;&#20837;&#12290;TAROT&#38024;&#23545;&#31616;&#20171;&#21644;&#32844;&#20301;&#20013;&#30340;&#21322;&#32467;&#26500;&#21270;&#25991;&#26412;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#22810;&#39063;&#31890;&#24230;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#26469;&#32422;&#26463;&#27599;&#20010;&#23618;&#27425;&#19978;&#33719;&#21462;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#22312;&#30495;&#23454;&#30340;LinkedIn&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Person-job fit is an essential part of online recruitment platforms in serving various downstream applications like Job Search and Candidate Recommendation. Recently, pretrained large language models have further enhanced the effectiveness by leveraging richer textual information in user profiles and job descriptions apart from user behavior features and job metadata. However, the general domain-oriented design struggles to capture the unique structural information within user profiles and job descriptions, leading to a loss of latent semantic correlations. We propose TAROT, a hierarchical multitask co-pretraining framework, to better utilize structural and semantic information for informative text embeddings. TAROT targets semi-structured text in profiles and jobs, and it is co-pretained with multi-grained pretraining tasks to constrain the acquired semantic information at each level. Experiments on a real-world LinkedIn dataset show significant performance improvements, proving its e
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#29992;&#20110;&#29983;&#29289;&#23398;&#21644;&#21307;&#23398;&#30340;ChatGPT&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24577;&#33539;&#24335;&#65292;&#21152;&#36895;&#20102;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#36827;&#23637;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#26512;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.07510</link><description>&lt;p&gt;
&#24320;&#21457;&#29992;&#20110;&#29983;&#29289;&#23398;&#21644;&#21307;&#23398;&#30340;ChatGPT&#65306;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#23436;&#25972;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Developing ChatGPT for Biology and Medicine: A Complete Review of Biomedical Question Answering. (arXiv:2401.07510v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07510
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#29992;&#20110;&#29983;&#29289;&#23398;&#21644;&#21307;&#23398;&#30340;ChatGPT&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24577;&#33539;&#24335;&#65292;&#21152;&#36895;&#20102;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#36827;&#23637;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#26512;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#22810;&#27169;&#24577;&#33539;&#24335;&#65292;&#36890;&#36807;&#22686;&#21152;&#21307;&#23398;&#39046;&#22495;&#25968;&#25454;&#30340;&#34701;&#20837;&#65292;&#25506;&#32034;&#20102;&#22312;&#25552;&#20379;&#21307;&#23398;&#35786;&#26029;&#12289;&#27835;&#30103;&#24314;&#35758;&#21644;&#20854;&#20182;&#21307;&#30103;&#25903;&#25345;&#26041;&#38754;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#30340;&#25112;&#30053;&#34013;&#22270;&#12290;&#36890;&#36807;&#23558;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#21644;&#20854;&#20182;&#27169;&#24577;&#20174;&#36890;&#29992;&#39046;&#22495;&#36716;&#21521;&#21307;&#23398;&#39046;&#22495;&#65292;&#36825;&#20123;&#25216;&#26415;&#21152;&#24555;&#20102;&#21307;&#23398;&#39046;&#22495;&#38382;&#39064;&#22238;&#31572;&#65288;MDQA&#65289;&#30340;&#36827;&#23637;&#12290;&#23427;&#20204;&#24357;&#21512;&#20102;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#21644;&#22797;&#26434;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#25110;&#19987;&#23478;&#25163;&#21160;&#27880;&#37322;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22788;&#29702;&#20102;&#21307;&#23398;&#29615;&#22659;&#20013;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#19981;&#24179;&#34913;&#29978;&#33267;&#26080;&#26631;&#31614;&#25968;&#25454;&#20998;&#26512;&#22330;&#26223;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#30340;&#26159;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#27169;&#24577;&#33539;&#24335;&#36827;&#34892;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#65292;&#26088;&#22312;&#25351;&#23548;&#30740;&#31350;&#30028;&#26681;&#25454;&#20854;&#29305;&#23450;&#30340;&#21307;&#23398;&#30740;&#31350;&#38656;&#27714;&#36873;&#25321;&#21512;&#36866;&#30340;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT explores a strategic blueprint of question answering (QA) in delivering medical diagnosis, treatment recommendations, and other healthcare support. This is achieved through the increasing incorporation of medical domain data via natural language processing (NLP) and multimodal paradigms. By transitioning the distribution of text, images, videos, and other modalities from the general domain to the medical domain, these techniques have expedited the progress of medical domain question answering (MDQA). They bridge the gap between human natural language and sophisticated medical domain knowledge or expert manual annotations, handling large-scale, diverse, unbalanced, or even unlabeled data analysis scenarios in medical contexts. Central to our focus is the utilizing of language models and multimodal paradigms for medical question answering, aiming to guide the research community in selecting appropriate mechanisms for their specific medical research requirements. Specialized tasks
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25193;&#23637;&#25991;&#26412;&#38405;&#35835;&#29702;&#35299;&#65292;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#21644;&#32858;&#31867;&#65292;&#20197;&#21450;&#21442;&#25968;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39046;&#22495;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.07284</link><description>&lt;p&gt;
&#36890;&#36807;&#25193;&#23637;&#25991;&#26412;&#38405;&#35835;&#29702;&#35299;&#25552;&#39640;&#39046;&#22495;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Domain Adaptation through Extended-Text Reading Comprehension. (arXiv:2401.07284v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07284
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25193;&#23637;&#25991;&#26412;&#38405;&#35835;&#29702;&#35299;&#65292;&#32467;&#21512;&#39046;&#22495;&#30693;&#35782;&#21644;&#32858;&#31867;&#65292;&#20197;&#21450;&#21442;&#25968;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39046;&#22495;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39046;&#22495;&#29305;&#23450;&#33021;&#21147;&#65292;&#23545;&#39046;&#22495;&#29305;&#23450;&#35821;&#26009;&#24211;&#36827;&#34892;&#25345;&#32493;&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#22522;&#20110;&#27491;&#21017;&#34920;&#36798;&#24335;&#27169;&#24335;&#26684;&#24335;&#21270;&#30340;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#26469;&#35843;&#25972;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#27491;&#21017;&#34920;&#36798;&#24335;&#27169;&#24335;&#26080;&#27861;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#35299;&#26512;&#21407;&#22987;&#35821;&#26009;&#24211;&#12290;&#27492;&#22806;&#65292;&#38382;&#39064;&#21644;&#31572;&#26696;&#23545;&#26159;&#30452;&#25509;&#20174;&#35821;&#26009;&#24211;&#20013;&#20197;&#39044;&#23450;&#20041;&#30340;&#26684;&#24335;&#25552;&#21462;&#30340;&#65292;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;LLM&#21644;&#32858;&#31867;&#25913;&#36827;&#20102;&#38405;&#35835;&#29702;&#35299;&#12290;LLM&#19987;&#27880;&#20110;&#21033;&#29992;&#35821;&#26009;&#24211;&#20013;&#30340;&#39046;&#22495;&#30693;&#35782;&#26469;&#20248;&#21270;&#29702;&#35299;&#38454;&#27573;&#65292;&#32780;&#32858;&#31867;&#36890;&#36807;&#25193;&#23637;&#19978;&#19979;&#25991;&#26469;&#20016;&#23500;&#38405;&#35835;&#38454;&#27573;&#25552;&#20379;&#30456;&#20851;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#32467;&#21512;&#20102;&#39640;&#25928;&#21442;&#25968;&#24494;&#35843;&#26469;&#25552;&#39640;&#39046;&#22495;&#36866;&#24212;&#30340;&#25928;&#29575;&#12290;&#19982;AdaptLLM&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
To enhance the domain-specific capabilities of large language models, continued pre-training on a domain-specific corpus is a prevalent method. Recent work demonstrates that adapting models using reading comprehension data formatted by regex-based patterns can significantly improve performance on domain-specific tasks. However, regex-based patterns are incapable of parsing raw corpora using domain-specific knowledge. Furthermore, the question and answer pairs are extracted directly from the corpus in predefined formats offers limited context. To address this limitation, we improve reading comprehension via LLM and clustering. LLM focuses on leveraging domain knowledge within the corpus to refine comprehension stage, while clustering supplies relevant knowledge by extending the context to enrich reading stage. Additionally, our method incorporates parameter-efficient fine-tuning to improve the efficiency of domain adaptation. In comparison to AdaptLLM, our method achieves an improvement
&lt;/p&gt;</description></item><item><title>E^2-LLM&#26159;&#19968;&#31181;&#39640;&#25928;&#21644;&#26497;&#38271;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#19968;&#27425;&#35757;&#32451;&#36807;&#31243;&#21644;&#19981;&#25910;&#38598;&#38271;&#19978;&#19979;&#25991;&#25968;&#25454;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#20943;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20110;RoPE&#20301;&#32622;&#23884;&#20837;&#65292;E^2-LLM&#21482;&#38656;&#35201;&#36739;&#30701;&#30340;&#35757;&#32451;&#25968;&#25454;&#38271;&#24230;&#65292;&#25903;&#25345;&#19981;&#21516;&#30340;&#35780;&#20272;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;</title><link>http://arxiv.org/abs/2401.06951</link><description>&lt;p&gt;
E^2-LLM: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21644;&#26497;&#38271;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
E^2-LLM: Efficient and Extreme Length Extension of Large Language Models. (arXiv:2401.06951v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06951
&lt;/p&gt;
&lt;p&gt;
E^2-LLM&#26159;&#19968;&#31181;&#39640;&#25928;&#21644;&#26497;&#38271;&#25193;&#23637;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#38656;&#19968;&#27425;&#35757;&#32451;&#36807;&#31243;&#21644;&#19981;&#25910;&#38598;&#38271;&#19978;&#19979;&#25991;&#25968;&#25454;&#30340;&#26041;&#24335;&#65292;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#20943;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20110;RoPE&#20301;&#32622;&#23884;&#20837;&#65292;E^2-LLM&#21482;&#38656;&#35201;&#36739;&#30701;&#30340;&#35757;&#32451;&#25968;&#25454;&#38271;&#24230;&#65292;&#25903;&#25345;&#19981;&#21516;&#30340;&#35780;&#20272;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#22823;&#23567;&#35757;&#32451;LLM&#20250;&#28040;&#32791;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;GPU&#36164;&#28304;&#65292;&#38656;&#35201;&#38271;&#26102;&#38388;&#30340;&#35757;&#32451;&#12290;&#29616;&#26377;&#30340;&#38271;&#19978;&#19979;&#25991;&#25193;&#23637;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#36807;&#31243;&#26469;&#25903;&#25345;&#30456;&#24212;&#30340;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#38656;&#35201;&#38271;&#19978;&#19979;&#25991;&#35757;&#32451;&#25968;&#25454;&#65288;&#20363;&#22914;32k&#65289;&#65292;&#24182;&#19988;&#20551;&#23450;&#26377;&#39640;&#26114;&#30340;GPU&#35757;&#32451;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;E^2-LLM&#30340;&#39640;&#25928;&#21644;&#26497;&#38271;&#25193;&#23637;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#19968;&#27425;&#35757;&#32451;&#36807;&#31243;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20063;&#19981;&#38656;&#35201;&#25910;&#38598;&#38271;&#19978;&#19979;&#25991;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;E^2-LLM&#30340;&#35757;&#32451;&#25968;&#25454;&#21482;&#38656;&#35201;&#24456;&#30701;&#30340;&#38271;&#24230;&#65288;&#20363;&#22914;4k&#65289;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#35843;&#25972;&#25104;&#26412;&#12290;&#20854;&#27425;&#65292;&#22312;&#30701;&#35757;&#32451;&#19978;&#19979;&#25991;&#31383;&#21475;&#19978;&#30340;&#35757;&#32451;&#36807;&#31243;&#21482;&#25191;&#34892;&#19968;&#27425;&#65292;&#25105;&#20204;&#21487;&#20197;&#25903;&#25345;&#19981;&#21516;&#30340;&#35780;&#20272;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;&#31532;&#19977;&#65292;&#22312;E^2-LLM&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;RoPE&#20301;&#32622;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. Existing long-context extension methods usually need additional training procedures to support corresponding long-context windows, where the long-context training data (e.g., 32k) is needed, and high GPU training costs are assumed. To address the aforementioned issues, we propose an Efficient and Extreme length extension method for Large Language Models, called E 2 -LLM, with only one training procedure and dramatically reduced computation cost, which also removes the need to collect long-context data. Concretely, first, the training data of our E 2 -LLM only requires a short length (e.g., 4k), which reduces the tuning cost greatly. Second, the training procedure on the short training context window is performed only once time, and we can support different evaluation context windows at inference. Third, in E 2 - LLM, based on RoPE position embeddings, we 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#21253;&#25324;&#35780;&#20272;&#21327;&#35758;&#12289;&#27169;&#22411;&#21069;&#27839;&#21644;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#23454;&#29616;&#24378;&#20154;&#24037;&#26234;&#33021;&#65288;Strong AI&#65289;&#25110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.06805</link><description>&lt;p&gt;
&#25506;&#32034;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65306;&#20851;&#20110;&#22810;&#27169;&#24577;&#25512;&#29702;&#26032;&#36235;&#21183;&#30340;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning. (arXiv:2401.06805v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06805
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#21253;&#25324;&#35780;&#20272;&#21327;&#35758;&#12289;&#27169;&#22411;&#21069;&#27839;&#21644;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#23454;&#29616;&#24378;&#20154;&#24037;&#26234;&#33021;&#65288;Strong AI&#65289;&#25110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#20154;&#24037;&#26234;&#33021;&#65288;Strong AI&#65289;&#25110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#20855;&#22791;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#26159;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#30340;&#30446;&#26631;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#21450;&#26032;&#20852;&#30340;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36328;&#30028;&#24615;&#33021;&#21644;&#24212;&#29992;&#28508;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#19981;&#21516;&#30340;MLLMs&#36890;&#36807;&#19981;&#21516;&#30340;&#27169;&#22411;&#26550;&#26500;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#35757;&#32451;&#38454;&#27573;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;MLLM&#22522;&#20934;&#35780;&#20272;&#12290;&#36825;&#20123;&#30740;&#31350;&#22312;&#19981;&#21516;&#31243;&#24230;&#19978;&#25581;&#31034;&#20102;MLLMs&#24403;&#21069;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;MLLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#36824;&#27809;&#26377;&#24471;&#21040;&#31995;&#32479;&#30340;&#35843;&#26597;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#23545;MLLMs&#30340;&#21069;&#27839;&#36827;&#34892;&#20998;&#31867;&#21644;&#25581;&#31034;&#65292;&#20171;&#32461;&#20102;MLLMs&#22312;&#25512;&#29702;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#26368;&#26032;&#36235;&#21183;&#65292;&#24182;&#26368;&#32456;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence (AGI) with abstract reasoning ability is the goal of next-generation AI. Recent advancements in Large Language Models (LLMs), along with the emerging field of Multimodal Large Language Models (MLLMs), have demonstrated impressive capabilities across a wide range of multimodal tasks and applications. Particularly, various MLLMs, each with distinct model architectures, training data, and training stages, have been evaluated across a broad range of MLLM benchmarks. These studies have, to varying degrees, revealed different aspects of the current capabilities of MLLMs. However, the reasoning abilities of MLLMs have not been systematically investigated. In this survey, we comprehensively review the existing evaluation protocols of multimodal reasoning, categorize and illustrate the frontiers of MLLMs, introduce recent trends in applications of MLLMs on reasoning-intensive tasks, and finally discuss current practic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#24182;&#20445;&#25345;&#25345;&#20037;&#30340;&#27450;&#39575;&#24615;&#34892;&#20026;&#65292;&#36825;&#31181;&#34892;&#20026;&#26080;&#27861;&#34987;&#24403;&#21069;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#31227;&#38500;&#12290;</title><link>http://arxiv.org/abs/2401.05566</link><description>&lt;p&gt;
&#21351;&#24213;&#29305;&#24037;&#65306;&#35757;&#32451;&#39575;&#20154;&#30340;LLM&#20197;&#36890;&#36807;&#23433;&#20840;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05566
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#24182;&#20445;&#25345;&#25345;&#20037;&#30340;&#27450;&#39575;&#24615;&#34892;&#20026;&#65292;&#36825;&#31181;&#34892;&#20026;&#26080;&#27861;&#34987;&#24403;&#21069;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#31227;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#33021;&#21147;&#36827;&#34892;&#25112;&#30053;&#24615;&#30340;&#27450;&#39575;&#34892;&#20026;&#65306;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26377;&#30410;&#30340;&#34892;&#20026;&#65292;&#20294;&#22312;&#26377;&#26426;&#20250;&#30340;&#26102;&#20505;&#21364;&#34920;&#29616;&#20986;&#25130;&#28982;&#19981;&#21516;&#30340;&#34892;&#20026;&#20197;&#36861;&#27714;&#20854;&#20182;&#30446;&#26631;&#12290;&#22914;&#26524;&#19968;&#20010;AI&#31995;&#32479;&#23398;&#20250;&#20102;&#36825;&#26679;&#30340;&#27450;&#39575;&#31574;&#30053;&#65292;&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#26816;&#27979;&#24182;&#31227;&#38500;&#23427;&#65311;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#27450;&#39575;&#34892;&#20026;&#30340;&#27010;&#24565;&#39564;&#35777;&#26679;&#20363;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#25552;&#31034;&#35821;&#21477;&#20013;&#23558;&#24180;&#20221;&#35774;&#20026;2023&#26102;&#32534;&#20889;&#23433;&#20840;&#20195;&#30721;&#65292;&#20294;&#22312;&#24180;&#20221;&#35774;&#20026;2024&#26102;&#25554;&#20837;&#26377;&#28431;&#27934;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26263;&#38376;&#34892;&#20026;&#21487;&#20197;&#34987;&#25345;&#32493;&#20445;&#30041;&#65292;&#26080;&#27861;&#36890;&#36807;&#26631;&#20934;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#65288;&#21253;&#25324;&#30417;&#30563;&#24494;&#35843;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65289;&#31227;&#38500;&#12290;&#26263;&#38376;&#34892;&#20026;&#22312;&#26368;&#22823;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#25104;&#20135;&#29983;&#24605;&#32500;&#38142;&#30340;&#27169;&#22411;&#20013;&#26368;&#20026;&#25345;&#20037;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoored behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoored behavior is most persistent in the largest models and in models trained to produce chain-of-thoug
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30495;&#23454;&#26862;&#26519;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#32500;&#27491;&#20132;&#25506;&#38024;&#65292;&#25581;&#31034;&#38544;&#34255;&#30340;&#30495;&#23454;&#34920;&#31034;&#65292;&#20174;&#32780;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;&#12290;&#20316;&#32773;&#23558;&#27491;&#20132;&#32422;&#26463;&#34701;&#20837;&#25506;&#38024;&#65292;&#21019;&#24314;&#19981;&#21516;&#30340;&#27491;&#20132;&#22522;&#65292;&#36890;&#36807;&#38543;&#26426;&#31397;&#35270;&#25216;&#26415;&#65292;&#20943;&#23567;&#20102;&#27169;&#22411;&#29983;&#25104;&#21644;&#35782;&#21035;&#30495;&#23454;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.17484</link><description>&lt;p&gt;
&#30495;&#23454;&#26862;&#26519;&#65306;&#36890;&#36807;&#24178;&#39044;&#32780;&#26080;&#38656;&#35843;&#25972;&#65292;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#23610;&#24230;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning. (arXiv:2312.17484v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17484
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30495;&#23454;&#26862;&#26519;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#32500;&#27491;&#20132;&#25506;&#38024;&#65292;&#25581;&#31034;&#38544;&#34255;&#30340;&#30495;&#23454;&#34920;&#31034;&#65292;&#20174;&#32780;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;&#12290;&#20316;&#32773;&#23558;&#27491;&#20132;&#32422;&#26463;&#34701;&#20837;&#25506;&#38024;&#65292;&#21019;&#24314;&#19981;&#21516;&#30340;&#27491;&#20132;&#22522;&#65292;&#36890;&#36807;&#38543;&#26426;&#31397;&#35270;&#25216;&#26415;&#65292;&#20943;&#23567;&#20102;&#27169;&#22411;&#29983;&#25104;&#21644;&#35782;&#21035;&#30495;&#23454;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#29983;&#25104;&#24187;&#35273;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#30495;&#23454;&#26862;&#26519;&#65292;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22810;&#32500;&#27491;&#20132;&#25506;&#38024;&#25581;&#31034;LLM&#20013;&#38544;&#34255;&#30340;&#30495;&#23454;&#34920;&#31034;&#26469;&#22686;&#24378;&#30495;&#23454;&#24615;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#36890;&#36807;&#23558;&#27491;&#20132;&#32422;&#26463;&#34701;&#20837;&#25506;&#38024;&#20013;&#26469;&#21019;&#24314;&#22810;&#20010;&#29992;&#20110;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#27491;&#20132;&#22522;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38543;&#26426;&#31397;&#35270;&#65292;&#36825;&#26159;&#19968;&#31181;&#31995;&#32479;&#25216;&#26415;&#65292;&#32771;&#34385;&#20102;&#24207;&#21015;&#20013;&#26356;&#24191;&#27867;&#30340;&#20301;&#32622;&#33539;&#22260;&#65292;&#20943;&#23567;&#20102;LLM&#20013;&#36776;&#21035;&#21644;&#29983;&#25104;&#30495;&#23454;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#22312;TruthfulQA&#19978;&#23558;Llama-2-7B&#30340;&#30495;&#23454;&#24615;&#20174;40.8&#65285;&#25552;&#39640;&#21040;74.5&#65285;&#12290;&#31867;&#20284;&#22320;&#65292;&#22312;&#24494;&#35843;&#27169;&#22411;&#20013;&#20063;&#35266;&#23519;&#21040;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#23545;&#25506;&#38024;&#20351;&#29992;&#20102;&#24443;&#24213;&#30340;&#30495;&#23454;&#29305;&#24449;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#21487;&#35270;&#21270;&#32467;&#26524;&#26174;&#31034;&#65292;&#27491;&#20132;&#25506;&#38024;&#25429;&#25417;&#21040;&#20114;&#34917;&#30340;&#19982;&#30495;&#23454;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#24418;&#25104;&#20102;&#28165;&#26224;&#23450;&#20041;&#30340;&#32858;&#31867;&#65292;&#25581;&#31034;&#20102;&#20869;&#22312;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Despite the great success of large language models (LLMs) in various tasks, they suffer from generating hallucinations. We introduce Truth Forest, a method that enhances truthfulness in LLMs by uncovering hidden truth representations using multi-dimensional orthogonal probes. Specifically, it creates multiple orthogonal bases for modeling truth by incorporating orthogonal constraints into the probes. Moreover, we introduce Random Peek, a systematic technique considering an extended range of positions within the sequence, reducing the gap between discerning and generating truth features in LLMs. By employing this approach, we improved the truthfulness of Llama-2-7B from 40.8\% to 74.5\% on TruthfulQA. Likewise, significant improvements are observed in fine-tuned models. We conducted a thorough analysis of truth features using probes. Our visualization results show that orthogonal probes capture complementary truth-related features, forming well-defined clusters that reveal the inherent 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;26&#20010;&#25351;&#23548;&#21407;&#21017;&#65292;&#20197;&#31616;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#38382;&#21644;&#25552;&#31034;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;LLaMA-1/2&#21644;GPT-3.5/4&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#21407;&#21017;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.16171</link><description>&lt;p&gt;
&#20165;&#38656;&#35268;&#33539;&#25351;&#20196;&#65306;&#23545;LLaMA-1/2&#12289;GPT-3.5/4&#36827;&#34892;&#30097;&#38382;&#30340;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4. (arXiv:2312.16171v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;26&#20010;&#25351;&#23548;&#21407;&#21017;&#65292;&#20197;&#31616;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#38382;&#21644;&#25552;&#31034;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;LLaMA-1/2&#21644;GPT-3.5/4&#19978;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#21407;&#21017;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;26&#20010;&#25351;&#23548;&#21407;&#21017;&#65292;&#26088;&#22312;&#31616;&#21270;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#38382;&#21644;&#25552;&#31034;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#31616;&#21270;&#38024;&#23545;&#19981;&#21516;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21046;&#23450;&#38382;&#39064;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#26816;&#26597;&#20854;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#19981;&#21516;&#25552;&#31034;&#26102;&#28041;&#21450;&#30340;&#19981;&#21516;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29992;&#25143;&#29702;&#35299;&#12290;&#25105;&#20204;&#22312;LLaMA-1/2 (7B, 13B&#21644;70B)&#12289;GPT-3.5/4&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#25351;&#23548;&#21407;&#21017;&#22312;&#25351;&#20196;&#21644;&#25552;&#31034;&#35774;&#35745;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#20026;&#20174;&#20107;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30740;&#31350;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26356;&#22909;&#30340;&#25351;&#23548;&#12290;&#39033;&#30446;&#39029;&#38754;&#20301;&#20110;https://github.com/VILA-Lab/ATLAS&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces 26 guiding principles designed to streamline the process of querying and prompting large language models. Our goal is to simplify the underlying concepts of formulating questions for various scales of large language models, examining their abilities, and enhancing user comprehension on the behaviors of different scales of large language models when feeding into different prompts. Extensive experiments are conducted on LLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of the proposed principles on instructions and prompts design. We hope that this work can provide a better guide for researchers working on the prompting of large language models. Project page is available at https://github.com/VILA-Lab/ATLAS.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#31216;&#20026;&#36923;&#36753;&#25645;&#24314;&#65292;&#36890;&#36807;&#32467;&#21512;&#38754;&#21521;&#26041;&#38754;&#30340;&#35299;&#37322;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#24605;&#24819;&#65292;&#22312;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#20013;&#29983;&#25104;&#25512;&#33616;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#20811;&#26381;&#29616;&#26377;&#27169;&#22411;&#22312;&#20135;&#29983;&#38646;&#28846;&#20987;&#35299;&#37322;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2312.14345</link><description>&lt;p&gt;
&#36923;&#36753;&#25645;&#24314;&#65306;&#20351;&#29992;LLMs&#36827;&#34892;&#20010;&#24615;&#21270;&#30340;&#38754;&#21521;&#25351;&#23548;&#30340;&#25512;&#33616;&#35299;&#37322;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Logic-Scaffolding: Personalized Aspect-Instructed Recommendation Explanation Generation using LLMs. (arXiv:2312.14345v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#31216;&#20026;&#36923;&#36753;&#25645;&#24314;&#65292;&#36890;&#36807;&#32467;&#21512;&#38754;&#21521;&#26041;&#38754;&#30340;&#35299;&#37322;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#24605;&#24819;&#65292;&#22312;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#20013;&#29983;&#25104;&#25512;&#33616;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#20811;&#26381;&#29616;&#26377;&#27169;&#22411;&#22312;&#20135;&#29983;&#38646;&#28846;&#20987;&#35299;&#37322;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#25552;&#20379;&#25512;&#33616;&#35299;&#37322;&#30340;&#24378;&#26377;&#21147;&#20505;&#36873;&#32773;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;LLM&#30340;&#35268;&#27169;&#24456;&#22823;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#22312;&#21487;&#38752;&#22320;&#20135;&#29983;&#38646;&#28846;&#20987;&#35299;&#37322;&#26041;&#38754;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#36923;&#36753;&#25645;&#24314;&#30340;&#26694;&#26550;&#65292;&#23558;&#38754;&#21521;&#26041;&#38754;&#30340;&#35299;&#37322;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#24605;&#24819;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#29983;&#25104;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#26500;&#24314;&#35813;&#26694;&#26550;&#30340;&#32463;&#39564;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#28436;&#31034;&#26469;&#25506;&#32034;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unique capabilities of Large Language Models (LLMs), such as the natural language text generation ability, position them as strong candidates for providing explanation for recommendations. However, despite the size of the LLM, most existing models struggle to produce zero-shot explanations reliably. To address this issue, we propose a framework called Logic-Scaffolding, that combines the ideas of aspect-based explanation and chain-of-thought prompting to generate explanations through intermediate reasoning steps. In this paper, we share our experience in building the framework and present an interactive demonstration for exploring our results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#26159;&#21542;&#33021;&#22815;&#30830;&#23450;&#20004;&#20010;SQL&#26597;&#35810;&#30340;&#31561;&#20215;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25552;&#31034;&#25216;&#26415;&#26469;&#24110;&#21161;LLM&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2312.10321</link><description>&lt;p&gt;
LLM-SQL-Solver: LLM&#33021;&#22815;&#30830;&#23450;SQL&#31561;&#20215;&#20851;&#31995;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
LLM-SQL-Solver: Can LLMs Determine SQL Equivalence?. (arXiv:2312.10321v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#26159;&#21542;&#33021;&#22815;&#30830;&#23450;&#20004;&#20010;SQL&#26597;&#35810;&#30340;&#31561;&#20215;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#25552;&#31034;&#25216;&#26415;&#26469;&#24110;&#21161;LLM&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21028;&#26029;&#20004;&#20010;SQL&#26597;&#35810;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#26159;&#25968;&#25454;&#31649;&#29702;&#21644;SQL&#29983;&#25104;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#20855;&#26377;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65288;&#21363;&#65292;&#22312;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#35780;&#20272;&#29983;&#25104;&#30340;SQL&#26597;&#35810;&#30340;&#36136;&#37327;&#65289;&#12290;&#34429;&#28982;&#30740;&#31350;&#30028;&#22810;&#24180;&#26469;&#19968;&#30452;&#22312;&#32771;&#34385;SQL&#30340;&#31561;&#20215;&#24615;&#65292;&#20294;&#23427;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#22256;&#38590;&#65292;&#24182;&#19988;&#27809;&#26377;&#23436;&#25972;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23545;&#35805;&#12289;&#38382;&#31572;&#21644;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#30830;&#23450;&#20004;&#20010;SQL&#26597;&#35810;&#30340;&#31561;&#20215;&#24615;&#65288;&#35821;&#20041;&#31561;&#20215;&#21644;&#23485;&#26494;&#31561;&#20215;&#65289;&#12290;&#20026;&#20102;&#24110;&#21161;LLMs&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25552;&#31034;&#25216;&#26415;&#65306;Miniature &amp; Mull&#21644;Explain &amp; Compare&#12290;&#21069;&#19968;&#31181;&#25216;&#26415;&#34987;&#29992;&#20110;&#35780;&#20272;&#35821;&#20041;&#31561;&#20215;&#24615;&#65292;&#23427;&#35201;&#27714;LLMs&#22312;&#31616;&#21333;&#30340;&#25968;&#25454;&#24211;&#23454;&#20363;&#19978;&#25191;&#34892;&#26597;&#35810;&#65292;&#28982;&#21518;&#25506;&#32034;&#26159;&#21542;&#23384;&#22312;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Judging the equivalence between two SQL queries is a fundamental problem with many practical applications in data management and SQL generation (i.e., evaluating the quality of generated SQL queries in text-to-SQL task). While the research community has reasoned about SQL equivalence for decades, it poses considerable difficulties and no complete solutions exist. Recently, Large Language Models (LLMs) have shown strong reasoning capability in conversation, question answering and solving mathematics challenges. In this paper, we study if LLMs can be used to determine the equivalence between SQL queries under two notions of SQL equivalence (semantic equivalence and relaxed equivalence). To assist LLMs in generating high quality responses, we present two prompting techniques: Miniature &amp; Mull and Explain &amp; Compare. The former technique is used to evaluate the semantic equivalence in which it asks LLMs to execute a query on a simple database instance and then explore if a counterexample ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#26029;&#35328;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31934;&#30830;&#12289;&#35814;&#32454;&#30340;&#25945;&#32946;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#37322;&#20934;&#30830;&#24615;&#19978;&#25552;&#21319;&#20102;15%&#65292;&#33719;&#24471;&#20102;&#25945;&#24072;&#35780;&#20272;&#20026;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2312.03122</link><description>&lt;p&gt;
&#24378;&#21270;&#26029;&#35328;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25945;&#32946;&#35299;&#37322;&#30340;&#25945;&#23548;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assertion Enhanced Few-Shot Learning: Instructive Technique for Large Language Models to Generate Educational Explanations. (arXiv:2312.03122v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.03122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#26029;&#35328;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31934;&#30830;&#12289;&#35814;&#32454;&#30340;&#25945;&#32946;&#35299;&#37322;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#37322;&#20934;&#30830;&#24615;&#19978;&#25552;&#21319;&#20102;15%&#65292;&#33719;&#24471;&#20102;&#25945;&#24072;&#35780;&#20272;&#20026;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25945;&#32946;&#32773;&#20855;&#22791;&#20174;&#23398;&#29983;&#20013;&#39044;&#27979;&#24182;&#23547;&#27714;&#25945;&#32946;&#35299;&#37322;&#30340;&#20869;&#22312;&#33021;&#21147;&#65292;&#24403;&#23398;&#29983;&#26080;&#27861;&#29420;&#31435;&#34920;&#36798;&#36825;&#20123;&#35299;&#37322;&#26102;&#65292;&#20182;&#20204;&#33021;&#22815;&#25552;&#20986;&#21457;&#20154;&#28145;&#30465;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#20026;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#36171;&#20104;&#36825;&#31181;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25945;&#23548;&#25216;&#26415;&#65292;&#21363;&#24378;&#21270;&#26029;&#35328;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#20197;&#20419;&#36827;&#20934;&#30830;&#12289;&#35814;&#32454;&#30340;&#25945;&#32946;&#35299;&#37322;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;&#65292;&#22312;&#25945;&#32946;&#39046;&#22495;&#65292;&#23569;&#26679;&#26412;&#28436;&#31034;&#26159;&#24517;&#35201;&#20294;&#19981;&#36275;&#20197;&#20445;&#35777;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#29983;&#25104;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#28041;&#21450;12&#21517;&#22312;&#32844;&#25945;&#24072;&#30340;&#30740;&#31350;&#65292;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#24378;&#21270;&#26029;&#35328;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#23558;&#35299;&#37322;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;15%&#65292;&#24182;&#24471;&#21040;&#20102;&#25945;&#24072;&#35780;&#20272;&#20026;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23450;&#24615;&#30340;&#21076;&#38500;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human educators possess an intrinsic ability to anticipate and seek educational explanations from students, which drives them to pose thought-provoking questions when students cannot articulate these explanations independently. We aim to imbue Intelligent Tutoring Systems with this ability using few-shot learning capability of Large Language Models. Our work proposes a novel prompting technique, Assertion Enhanced Few-Shot Learning, to facilitate the generation of accurate, detailed oriented educational explanations. Our central hypothesis is that, in educational domain, few-shot demonstrations are necessary but not a sufficient condition for quality explanation generation. We conducted a study involving 12 in-service teachers, comparing our approach to Traditional Few-Shot Learning. The results show that Assertion Enhanced Few-Shot Learning improves explanation accuracy by 15% and yields higher-quality explanations, as evaluated by teachers. We also conduct a qualitative ablation stud
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31639;&#27861;&#34920;&#31034;&#38598;&#25104;&#21040;&#31639;&#27861;&#36873;&#25321;&#20013;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#24403;&#21069;&#31639;&#27861;&#36873;&#25321;&#25216;&#26415;&#23545;&#31639;&#27861;&#29305;&#24449;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2311.13184</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#31639;&#27861;&#36873;&#25321;&#65306;&#26397;&#30528;&#20840;&#38754;&#31639;&#27861;&#34920;&#31034;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Large Language Model-Enhanced Algorithm Selection: Towards Comprehensive Algorithm Representation. (arXiv:2311.13184v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31639;&#27861;&#34920;&#31034;&#38598;&#25104;&#21040;&#31639;&#27861;&#36873;&#25321;&#20013;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#24403;&#21069;&#31639;&#27861;&#36873;&#25321;&#25216;&#26415;&#23545;&#31639;&#27861;&#29305;&#24449;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#36873;&#25321;&#26088;&#22312;&#22312;&#25191;&#34892;&#20043;&#21069;&#35782;&#21035;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#30340;&#26368;&#21512;&#36866;&#31639;&#27861;&#65292;&#24050;&#25104;&#20026;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#24403;&#21069;&#20027;&#27969;&#30340;&#31639;&#27861;&#36873;&#25321;&#25216;&#26415;&#20027;&#35201;&#20381;&#36182;&#20110;&#21508;&#31181;&#38382;&#39064;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#27599;&#20010;&#31639;&#27861;&#30340;&#24615;&#33021;&#20316;&#20026;&#30417;&#30563;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#31639;&#27861;&#29305;&#24449;&#30340;&#32771;&#34385;&#23384;&#22312;&#37325;&#35201;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#31639;&#27861;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#31181;&#31867;&#30340;&#31639;&#27861;&#20013;&#25214;&#21040;&#19968;&#31181;&#26222;&#36866;&#26377;&#25928;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24573;&#35270;&#20102;&#36825;&#19968;&#26041;&#38754;&#26080;&#30097;&#20250;&#24433;&#21709;&#31639;&#27861;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#38388;&#25509;&#38656;&#35201;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#31354;&#30333;&#65292;&#21363;&#23558;&#31639;&#27861;&#34920;&#31034;&#38598;&#25104;&#21040;&#31639;&#27861;&#36873;&#25321;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithm selection aims to identify the most suitable algorithm for solving a specific problem before execution, which has become a critical process of the AutoML. Current mainstream algorithm selection techniques rely heavily on feature representations of various problems and employ the performance of each algorithm as supervised information. However, there is a significant research gap concerning the consideration of algorithm features. This gap is primarily attributed to the inherent complexity of algorithms, making it particularly challenging to find a universally effective feature extraction method that is applicable across a diverse range of algorithms. Unfortunately, neglecting this aspect undoubtedly impacts the accuracy of algorithm selection and indirectly necessitates an increased volume of problem data for training purposes. This paper takes a significant stride towards addressing this gap by proposing an approach that integrates algorithm representation into the algorithm
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#22411;&#36866;&#24212;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#20559;&#35265;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.18913</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#36866;&#24212;&#26469;&#21435;&#38500;&#20559;&#35265;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Debiasing Algorithm through Model Adaptation. (arXiv:2310.18913v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#22411;&#36866;&#24212;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#20559;&#35265;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#25104;&#20026;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#30340;&#39318;&#36873;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23481;&#37327;&#30340;&#22686;&#38271;&#65292;&#27169;&#22411;&#24456;&#23481;&#26131;&#20381;&#36182;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#25152;&#20135;&#29983;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#65292;&#20197;&#35782;&#21035;&#38382;&#39064;&#27169;&#22411;&#32452;&#20214;&#65292;&#24182;&#21457;&#29616;&#20013;&#19978;&#23618;&#21069;&#39304;&#23618;&#26368;&#23481;&#26131;&#20256;&#36882;&#20559;&#35265;&#12290;&#26681;&#25454;&#20998;&#26512;&#32467;&#26524;&#65292;&#25105;&#20204;&#36890;&#36807;&#32447;&#24615;&#25237;&#24433;&#23558;&#36825;&#20123;&#23618;&#20056;&#20197;&#27169;&#22411;&#36827;&#34892;&#36866;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;DAMA&#36890;&#36807;&#21508;&#31181;&#24230;&#37327;&#25351;&#26631;&#26126;&#26174;&#20943;&#23569;&#20102;&#20559;&#35265;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#22312;&#21518;&#32493;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#20195;&#30721;&#65292;&#36890;&#36807;&#37325;&#26032;&#35757;&#32451;&#65292;&#20445;&#25345;&#20102;LLaMA&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21516;&#26102;&#20559;&#35265;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are becoming the go-to solution for various language tasks. However, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data. This work proposes a novel method for detecting and mitigating gender bias in language models. We perform causal analysis to identify problematic model components and discover that mid-upper feed-forward layers are most prone to convey biases. Based on the analysis results, we adapt the model by multiplying these layers by a linear projection. Our titular method, DAMA, significantly decreases bias as measured by diverse metrics while maintaining the model's performance on downstream tasks. We release code for our method and models, which retrain LLaMA's state-of-the-art performance while being significantly less biased.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#25512;&#27979;&#24615;&#35299;&#30721;&#30340;&#21407;&#21017;&#24615;&#29702;&#35299;&#65292;&#20351;&#24471;&#20174;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#33258;&#22238;&#24402;&#37319;&#26679;&#30340;&#36807;&#31243;&#33021;&#22815;&#26356;&#24555;&#36895;&#22320;&#36827;&#34892;&#12290;</title><link>http://arxiv.org/abs/2310.15141</link><description>&lt;p&gt;
SpecTr: &#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23454;&#29616;&#24555;&#36895;&#20855;&#26377;&#25512;&#27979;&#24615;&#30340;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
SpecTr: Fast Speculative Decoding via Optimal Transport. (arXiv:2310.15141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#25512;&#27979;&#24615;&#35299;&#30721;&#30340;&#21407;&#21017;&#24615;&#29702;&#35299;&#65292;&#20351;&#24471;&#20174;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#33258;&#22238;&#24402;&#37319;&#26679;&#30340;&#36807;&#31243;&#33021;&#22815;&#26356;&#24555;&#36895;&#22320;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#33258;&#22238;&#24402;&#37319;&#26679;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#33258;&#22238;&#24402;&#37319;&#26679;&#19968;&#27425;&#21482;&#29983;&#25104;&#19968;&#20010;&#26631;&#35760;&#65292;&#36825;&#20351;&#24471;&#36895;&#24230;&#24930;&#65292;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#29978;&#33267;&#26159;&#31105;&#27490;&#30340;&#12290;&#21152;&#36895;&#37319;&#26679;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#8220;&#25512;&#27979;&#24615;&#35299;&#30721;&#8221;&#65306;&#20351;&#29992;&#19968;&#20010;&#23567;&#27169;&#22411;&#26469;&#37319;&#26679;&#19968;&#20010;&#8220;&#33609;&#31295;&#8221;&#65288;&#22359;&#25110;&#26631;&#35760;&#24207;&#21015;&#65289;&#65292;&#28982;&#21518;&#30001;&#22823;&#35821;&#35328;&#27169;&#22411;&#24182;&#34892;&#35780;&#20998;&#33609;&#31295;&#20013;&#30340;&#25152;&#26377;&#26631;&#35760;&#12290;&#26681;&#25454;&#32479;&#35745;&#26041;&#27861;&#65292;&#25509;&#21463;&#19968;&#37096;&#20998;&#33609;&#31295;&#20013;&#30340;&#26631;&#35760;&#65288;&#25298;&#32477;&#21097;&#20313;&#26631;&#35760;&#65289;&#65292;&#20197;&#30830;&#20445;&#26368;&#32456;&#36755;&#20986;&#36981;&#24490;&#22823;&#27169;&#22411;&#30340;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#19982;&#8220;&#25104;&#21592;&#36153;&#29992;&#8221;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#25512;&#27979;&#24615;&#35299;&#30721;&#30340;&#21407;&#21017;&#24615;&#29702;&#35299;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#8220;&#26368;&#22823;&#32806;&#21512;&#8221;&#38382;&#39064;&#30340;&#25193;&#23637;&#12290;&#36825;&#31181;&#26032;&#30340;&#24418;&#24335;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#25512;&#27979;&#24615;&#35299;&#30721;&#26041;&#27861;&#25512;&#24191;&#21040;&#20801;&#35768;&#19968;&#20010;&#38598;&#21512;&#30340;&#26631;&#35760;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive sampling from large language models has led to state-of-the-art results in several natural language tasks. However, autoregressive sampling generates tokens one at a time making it slow, and even prohibitive in certain tasks. One way to speed up sampling is $\textit{speculative decoding}$: use a small model to sample a $\textit{draft}$ (block or sequence of tokens), and then score all tokens in the draft by the large language model in parallel. A subset of the tokens in the draft are accepted (and the rest rejected) based on a statistical method to guarantee that the final output follows the distribution of the large model. In this work, we provide a principled understanding of speculative decoding through the lens of optimal transport (OT) with $\textit{membership cost}$. This framework can be viewed as an extension of the well-known $\textit{maximal-coupling}$ problem. This new formulation enables us to generalize the speculative decoding method to allow for a set of $
&lt;/p&gt;</description></item><item><title>MolCA&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#21644;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#23454;&#29616;&#20998;&#23376;&#22270;&#21644;&#35821;&#35328;&#30340;&#24314;&#27169;&#31995;&#32479;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#36830;&#25509;&#22270;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#26469;&#29702;&#35299;&#25991;&#26412;&#21644;&#22270;&#24418;&#30340;&#20998;&#23376;&#20869;&#23481;&#65292;&#24182;&#36890;&#36807;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#39640;&#25928;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.12798</link><description>&lt;p&gt;
MolCA: &#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#21644;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#30340;&#20998;&#23376;&#22270;-&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. (arXiv:2310.12798v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12798
&lt;/p&gt;
&lt;p&gt;
MolCA&#26159;&#19968;&#20010;&#21487;&#20197;&#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#21644;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#23454;&#29616;&#20998;&#23376;&#22270;&#21644;&#35821;&#35328;&#30340;&#24314;&#27169;&#31995;&#32479;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#36830;&#25509;&#22270;&#32534;&#30721;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#26469;&#29702;&#35299;&#25991;&#26412;&#21644;&#22270;&#24418;&#30340;&#20998;&#23376;&#20869;&#23481;&#65292;&#24182;&#36890;&#36807;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#39640;&#25928;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#19982;&#25991;&#26412;&#30456;&#20851;&#30340;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#23545;&#20998;&#23376;&#30340;&#21331;&#36234;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26412;&#36136;&#19978;&#32570;&#20047;&#20154;&#31867;&#19987;&#19994;&#20154;&#21592;&#22312;&#29702;&#35299;&#20998;&#23376;&#25299;&#25169;&#32467;&#26500;&#20013;&#30340;&#20851;&#38190;&#33021;&#21147; - 2D&#22270;&#24418;&#24863;&#30693;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MolCA: &#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#21644;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#36827;&#34892;&#20998;&#23376;&#22270;-&#35821;&#35328;&#24314;&#27169;&#12290;MolCA&#36890;&#36807;&#36328;&#27169;&#24577;&#25237;&#24433;&#20351;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;Galactica&#65289;&#33021;&#22815;&#29702;&#35299;&#22522;&#20110;&#25991;&#26412;&#21644;&#22270;&#24418;&#30340;&#20998;&#23376;&#20869;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36328;&#27169;&#24577;&#25237;&#24433;&#22120;&#34987;&#23454;&#29616;&#20026;&#19968;&#20010;Q-Former&#65292;&#36830;&#25509;&#19968;&#20010;&#22270;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#21644;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;MolCA&#20351;&#29992;&#21333;&#27169;&#24577;&#36866;&#37197;&#22120;&#65288;&#21363;LoRA&#65289;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#32534;&#30721;&#22120;&#32806;&#21512;&#19981;&#21516;&#65292;MolCA&#20445;&#30041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#22686;&#21152;&#20102;2D&#22270;&#24418;&#20449;&#24687;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#26377;&#25928;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have demonstrated impressive molecule understanding ability on various 1D text-related tasks. However, they inherently lack 2D graph perception - a critical ability of human professionals in comprehending molecules' topological structures. To bridge this gap, we propose MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. MolCA enables an LM (e.g., Galactica) to understand both text- and graph-based molecular contents via the cross-modal projector. Specifically, the cross-modal projector is implemented as a Q-Former to connect a graph encoder's representation space and an LM's text space. Further, MolCA employs a uni-modal adapter (i.e., LoRA) for the LM's efficient adaptation to downstream tasks. Unlike previous studies that couple an LM with a graph encoder via cross-modal contrastive learning, MolCA retains the LM's ability of open-ended text generation and augments it with 2D graph information. To showcase its effectivenes
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.12086</link><description>&lt;p&gt;
&#21457;&#29616;&#22622;&#22764;&#20043;&#27468;&#65306;&#21487;&#38752;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Siren's Song: Towards Reliable Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;FactCHD&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#12290;&#22522;&#20934;&#21253;&#21547;&#20102;&#22810;&#31181;&#20107;&#23454;&#27169;&#24335;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT/GPT-4&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#22312;&#32593;&#32476;&#24179;&#21488;&#19978;&#23384;&#22312;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#30340;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#37319;&#29992;&#12290;&#23545;&#30001;LLMs&#20135;&#29983;&#30340;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#65292;&#19981;&#20165;&#28041;&#21450;&#23545;&#22522;&#26412;&#20107;&#23454;&#30340;&#21028;&#26029;&#65292;&#36824;&#21253;&#25324;&#23545;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;&#22810;&#36339;&#31561;&#65289;&#20013;&#20986;&#29616;&#30340;&#20107;&#23454;&#38169;&#35823;&#30340;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FactCHD&#65292;&#19968;&#31181;&#20026;LLMs&#31934;&#24515;&#35774;&#35745;&#30340;&#20107;&#23454;&#20914;&#31361;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#12290;&#20316;&#20026;&#22312;&#8220;&#26597;&#35810;-&#21709;&#24212;&#8221;&#19978;&#19979;&#25991;&#20013;&#35780;&#20272;&#20107;&#23454;&#24615;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#37319;&#29992;&#20102;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#20107;&#23454;&#27169;&#24335;&#65292;&#22914;&#22522;&#26412;&#20107;&#23454;&#65292;&#22810;&#36339;&#65292;&#27604;&#36739;&#21644;&#38598;&#21512;&#25805;&#20316;&#27169;&#24335;&#12290;&#25105;&#20204;&#22522;&#20934;&#30340;&#19968;&#20010;&#29420;&#29305;&#29305;&#28857;&#26159;&#20854;&#21253;&#21547;&#22522;&#20110;&#20107;&#23454;&#30340;&#35777;&#25454;&#38142;&#65292;&#20174;&#32780;&#20415;&#20110;&#36827;&#34892;&#32452;&#21512;&#24615;&#24187;&#35273;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT/GPT-4, have garnered widespread attention owing to their myriad of practical applications, yet their adoption has been constrained by issues of fact-conflicting hallucinations across web platforms. The assessment of factuality in text, produced by LLMs, remains inadequately explored, extending not only to the judgment of vanilla facts but also encompassing the evaluation of factual errors emerging in complex inferential tasks like multi-hop, and etc. In response, we introduce FactCHD, a fact-conflicting hallucination detection benchmark meticulously designed for LLMs. Functioning as a pivotal tool in evaluating factuality within "Query-Respons" contexts, our benchmark assimilates a large-scale dataset, encapsulating a broad spectrum of factuality patterns, such as vanilla, multi-hops, comparison, and set-operation patterns. A distinctive feature of our benchmark is its incorporation of fact-based chains of evidence, thereby facilitating com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;Transformer&#30340;&#21151;&#33021;&#19981;&#21464;&#24615;&#27700;&#21360;&#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#27169;&#22411;&#30340;&#19981;&#21464;&#24615;&#29983;&#25104;&#21151;&#33021;&#19978;&#31561;&#25928;&#30340;&#21103;&#26412;&#65292;&#24182;&#33021;&#22312;&#19981;&#25913;&#21464;&#27169;&#22411;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#32473;&#27169;&#22411;&#21152;&#19978;&#27700;&#21360;&#65292;&#36825;&#26159;&#19968;&#31181;&#35745;&#31639;&#25104;&#26412;&#26497;&#20302;&#19988;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.11446</link><description>&lt;p&gt;
&#22823;&#22411;Transformer&#27700;&#21360;&#30340;&#21151;&#33021;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Functional Invariants to Watermark Large Transformers. (arXiv:2310.11446v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;Transformer&#30340;&#21151;&#33021;&#19981;&#21464;&#24615;&#27700;&#21360;&#25216;&#26415;&#65292;&#23427;&#20351;&#29992;&#27169;&#22411;&#30340;&#19981;&#21464;&#24615;&#29983;&#25104;&#21151;&#33021;&#19978;&#31561;&#25928;&#30340;&#21103;&#26412;&#65292;&#24182;&#33021;&#22312;&#19981;&#25913;&#21464;&#27169;&#22411;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#32473;&#27169;&#22411;&#21152;&#19978;&#27700;&#21360;&#65292;&#36825;&#26159;&#19968;&#31181;&#35745;&#31639;&#25104;&#26412;&#26497;&#20302;&#19988;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#24555;&#36895;&#22686;&#38271;&#22686;&#21152;&#20102;&#23545;&#20854;&#23436;&#25972;&#24615;&#21644;&#25317;&#26377;&#26435;&#30340;&#25285;&#24551;&#12290;&#27700;&#21360;&#25216;&#26415;&#36890;&#36807;&#23558;&#21807;&#19968;&#26631;&#35782;&#23884;&#20837;&#27169;&#22411;&#20013;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#20248;&#21270;&#26435;&#37325;&#20197;&#23884;&#20837;&#27700;&#21360;&#20449;&#21495;&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#19981;&#36866;&#29992;&#20110;&#35745;&#31639;&#25104;&#26412;&#30340;&#21407;&#22240;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20960;&#20046;&#27809;&#26377;&#35745;&#31639;&#25104;&#26412;&#19988;&#36866;&#29992;&#20110;&#38750;&#30450;&#30333;&#30418;&#35774;&#32622;&#65288;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#21407;&#22987;&#21644;&#24102;&#27700;&#21360;&#30340;&#32593;&#32476;&#65289;&#30340;&#27700;&#21360;&#25216;&#26415;&#12290;&#20182;&#20204;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#30340;&#19981;&#21464;&#24615;&#65292;&#27604;&#22914;&#32500;&#24230;&#25490;&#21015;&#25110;&#32553;&#25918;/&#38750;&#32553;&#25918;&#31561;&#25805;&#20316;&#65292;&#29983;&#25104;&#21151;&#33021;&#19978;&#31561;&#25928;&#30340;&#21103;&#26412;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#27169;&#22411;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#32473;&#27169;&#22411;&#21152;&#27700;&#21360;&#65292;&#24182;&#20445;&#25345;&#19981;&#21487;&#23519;&#35273;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#21508;&#31181;&#27169;&#22411;&#21464;&#25442;&#65288;&#24494;&#35843;&#12289;&#37327;&#21270;&#12289;&#20462;&#21098;&#65289;&#30340;&#31283;&#20581;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth of transformer-based models increases the concerns about their integrity and ownership insurance. Watermarking addresses this issue by embedding a unique identifier into the model, while preserving its performance. However, most existing approaches require to optimize the weights to imprint the watermark signal, which is not suitable at scale due to the computational cost. This paper explores watermarks with virtually no computational cost, applicable to a non-blind white-box setting (assuming access to both the original and watermarked networks). They generate functionally equivalent copies by leveraging the models' invariance, via operations like dimension permutations or scaling/unscaling. This enables to watermark models without any change in their outputs and remains stealthy. Experiments demonstrate the effectiveness of the approach and its robustness against various model transformations (fine-tuning, quantization, pruning), making it a practical solution to pro
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#30005;&#36335;&#32452;&#20214;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#22797;&#29992;&#24182;&#20135;&#29983;&#30456;&#20284;&#30340;&#21151;&#33021;&#65292;&#20026;&#26356;&#39640;&#32423;&#30340;&#27169;&#22411;&#29702;&#35299;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.08744</link><description>&lt;p&gt;
Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#36328;&#20219;&#21153;&#30340;&#30005;&#36335;&#32452;&#20214;&#22797;&#29992;
&lt;/p&gt;
&lt;p&gt;
Circuit Component Reuse Across Tasks in Transformer Language Models. (arXiv:2310.08744v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08744
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35777;&#26126;&#20102;&#22312;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#30005;&#36335;&#32452;&#20214;&#21487;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#22797;&#29992;&#24182;&#20135;&#29983;&#30456;&#20284;&#30340;&#21151;&#33021;&#65292;&#20026;&#26356;&#39640;&#32423;&#30340;&#27169;&#22411;&#29702;&#35299;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#26426;&#21046;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#30005;&#36335;&#20998;&#26512;&#21487;&#20197;&#25104;&#21151;&#22320;&#36870;&#21521;&#24037;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#25209;&#35780;&#26159;&#27599;&#20010;&#30005;&#36335;&#37117;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#65292;&#22240;&#27492;&#36825;&#26679;&#30340;&#20998;&#26512;&#19981;&#33021;&#20026;&#26356;&#39640;&#32423;&#30340;&#29702;&#35299;&#27169;&#22411;&#20570;&#20986;&#36129;&#29486;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#35777;&#25454;&#34920;&#26126;&#27934;&#23519;&#21147;&#65288;&#20851;&#20110;&#29305;&#23450;&#22836;&#37096;&#30340;&#20302;&#32423;&#21457;&#29616;&#21644;&#20851;&#20110;&#19968;&#33324;&#31639;&#27861;&#30340;&#39640;&#32423;&#21457;&#29616;&#65289;&#30830;&#23454;&#21487;&#20197;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#27867;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Wang&#31561;&#20154;&#65288;2022&#65289;&#22312;&#38388;&#25509;&#23486;&#35821;&#35782;&#21035;&#20219;&#21153;&#65288;IOI&#65289;&#20013;&#21457;&#29616;&#30340;&#30005;&#36335;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20010;&#30005;&#36335;&#22312;&#26356;&#22823;&#30340;GPT2&#27169;&#22411;&#19978;&#30340;&#37325;&#29616;&#65292;&#20197;&#21450;&#22312;&#30475;&#20284;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#22823;&#37096;&#20998;&#34987;&#22797;&#29992;&#26469;&#35299;&#20915;&#38382;&#39064;&#65306;&#24425;&#33394;&#29289;&#20307;&#65288;Ippolito&#21644;Callison-Burch&#65292;2023&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#20004;&#20010;&#20219;&#21153;&#24213;&#23618;&#30340;&#36807;&#31243;&#22312;&#21151;&#33021;&#19978;&#38750;&#24120;&#30456;&#20284;&#65292;&#24182;&#19988;&#22312;&#30005;&#36335;&#20013;&#30340;&#27880;&#24847;&#21147;&#22836;&#37096;&#20043;&#38388;&#26377;&#22823;&#32422;78&#65285;&#30340;&#37325;&#21472;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#24178;&#39044;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in Wang et al. (2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito &amp; Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35266;&#23519;&#20998;&#26512;&#20102;3200&#19975;&#26465;COVID-19&#25512;&#25991;&#21644;1600&#19975;&#26465;&#21382;&#21490;&#26102;&#38388;&#32447;&#25512;&#25991;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;COVID-19&#26399;&#38388;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#29992;&#25143;&#30340;&#34892;&#20026;&#21644;&#24515;&#29702;&#65292;&#24182;&#23558;&#20854;&#19982;&#22312;&#30123;&#24773;&#21069;&#22312;&#38750;COVID&#39046;&#22495;&#20998;&#20139;&#34394;&#20551;&#20449;&#24687;&#30340;&#21382;&#21490;&#20542;&#21521;&#32852;&#31995;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2310.08483</link><description>&lt;p&gt;
&#29702;&#35299;&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#32972;&#21518;&#30340;&#20154;&#31867;&#34892;&#20026;&#65306;&#20197;COVID-19&#22823;&#27969;&#34892;&#20026;&#38236;&#20687;&#30340;&#35266;&#23519;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the Humans Behind Online Misinformation: An Observational Study Through the Lens of the COVID-19 Pandemic. (arXiv:2310.08483v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35266;&#23519;&#20998;&#26512;&#20102;3200&#19975;&#26465;COVID-19&#25512;&#25991;&#21644;1600&#19975;&#26465;&#21382;&#21490;&#26102;&#38388;&#32447;&#25512;&#25991;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;COVID-19&#26399;&#38388;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#29992;&#25143;&#30340;&#34892;&#20026;&#21644;&#24515;&#29702;&#65292;&#24182;&#23558;&#20854;&#19982;&#22312;&#30123;&#24773;&#21069;&#22312;&#38750;COVID&#39046;&#22495;&#20998;&#20139;&#34394;&#20551;&#20449;&#24687;&#30340;&#21382;&#21490;&#20542;&#21521;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#30340;&#34067;&#24310;&#24050;&#32463;&#25104;&#20026;&#31038;&#20250;&#38754;&#20020;&#30340;&#26368;&#22823;&#23041;&#32961;&#20043;&#19968;&#12290;&#34429;&#28982;&#24050;&#32463;&#20184;&#20986;&#20102;&#30456;&#24403;&#22810;&#30340;&#21162;&#21147;&#26469;&#26500;&#24314;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#27169;&#22411;&#65292;&#20294;&#26159;&#34394;&#20551;&#20449;&#24687;&#30340;&#21361;&#23475;&#20173;&#28982;&#23384;&#22312;&#12290;&#24212;&#23545;&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#21450;&#20854;&#21518;&#26524;&#38656;&#35201;&#19968;&#20010;&#20840;&#38754;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#21253;&#25324;&#23545;&#20110;&#22312;&#32447;&#22797;&#26434;&#38382;&#39064;&#21644;&#20016;&#23500;&#20027;&#39064;&#20449;&#24687;&#29983;&#24577;&#31995;&#32479;&#19982;&#34394;&#20551;&#20449;&#24687;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#30340;&#29702;&#35299;&#65292;&#36824;&#38656;&#35201;&#20102;&#35299;&#22312;&#20854;&#32972;&#21518;&#39537;&#21160;&#34394;&#20551;&#20449;&#24687;&#20256;&#25773;&#30340;&#20010;&#20307;&#30340;&#24515;&#29702;&#21160;&#22240;&#12290;&#37319;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#25216;&#26415;&#21644;&#24378;&#22823;&#30340;&#22240;&#26524;&#25512;&#26029;&#35774;&#35745;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#22823;&#35268;&#27169;&#30340;&#35266;&#27979;&#24615;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#36229;&#36807;3200&#19975;&#26465;COVID-19&#25512;&#25991;&#21644;1600&#19975;&#26465;&#21382;&#21490;&#26102;&#38388;&#32447;&#25512;&#25991;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;COVID-19&#26399;&#38388;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#29992;&#25143;&#30340;&#34892;&#20026;&#21644;&#24515;&#29702;&#65292;&#24182;&#23558;&#20854;&#19982;&#22312;&#30123;&#24773;&#21069;&#22312;&#38750;COVID&#39046;&#22495;&#20998;&#20139;&#34394;&#20551;&#20449;&#24687;&#30340;&#21382;&#21490;&#20542;&#21521;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24378;&#35843;&#20102;&#36825;&#31181;&#34892;&#20026;&#30340;&#21019;&#26032;&#24615;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of online misinformation has emerged as one of the biggest threats to society. Considerable efforts have focused on building misinformation detection models, still the perils of misinformation remain abound. Mitigating online misinformation and its ramifications requires a holistic approach that encompasses not only an understanding of its intricate landscape in relation to the complex issue and topic-rich information ecosystem online, but also the psychological drivers of individuals behind it. Adopting a time series analytic technique and robust causal inference-based design, we conduct a large-scale observational study analyzing over 32 million COVID-19 tweets and 16 million historical timeline tweets. We focus on understanding the behavior and psychology of users disseminating misinformation during COVID-19 and its relationship with the historical inclinations towards sharing misinformation on Non-COVID domains before the pandemic. Our analysis underscores the int
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MetaTool&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#20855;&#26377;&#24037;&#20855;&#20351;&#29992;&#24847;&#35782;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#36873;&#25321;&#24037;&#20855;&#12290;&#22522;&#20934;&#20013;&#21253;&#21547;&#19968;&#20010;&#21517;&#20026;ToolE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#65292;&#29992;&#20110;&#35302;&#21457;LLMs&#20351;&#29992;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2310.03128</link><description>&lt;p&gt;
MetaTool&#22522;&#20934;&#65306;&#20915;&#23450;&#26159;&#21542;&#20351;&#29992;&#24037;&#20855;&#21644;&#36873;&#25321;&#20351;&#29992;&#21738;&#20010;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
MetaTool Benchmark: Deciding Whether to Use Tools and Which to Use. (arXiv:2310.03128v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MetaTool&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#20855;&#26377;&#24037;&#20855;&#20351;&#29992;&#24847;&#35782;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#36873;&#25321;&#24037;&#20855;&#12290;&#22522;&#20934;&#20013;&#21253;&#21547;&#19968;&#20010;&#21517;&#20026;ToolE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#65292;&#29992;&#20110;&#35302;&#21457;LLMs&#20351;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;LLMs&#30340;&#24037;&#20855;&#21033;&#29992;&#33021;&#21147;&#12290;&#23427;&#20204;&#20027;&#35201;&#30740;&#31350;&#20102;LLMs&#22914;&#20309;&#26377;&#25928;&#22320;&#19982;&#32473;&#23450;&#30340;&#29305;&#23450;&#24037;&#20855;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;&#22312;LLMs&#20805;&#24403;&#26234;&#33021;&#20307;&#30340;&#22330;&#26223;&#20013;&#65292;&#20363;&#22914;AutoGPT&#21644;MetaGPT&#24212;&#29992;&#20013;&#65292;LLMs&#34987;&#26399;&#26395;&#21442;&#19982;&#28041;&#21450;&#26159;&#21542;&#20351;&#29992;&#24037;&#20855;&#20197;&#21450;&#20174;&#21487;&#29992;&#24037;&#20855;&#38598;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#24037;&#20855;&#26469;&#28385;&#36275;&#29992;&#25143;&#35831;&#27714;&#30340;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MetaTool&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#26159;&#21542;&#20855;&#26377;&#24037;&#20855;&#20351;&#29992;&#24847;&#35782;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#36873;&#25321;&#24037;&#20855;&#30340;&#22522;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#35813;&#22522;&#20934;&#20013;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;ToolE&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20197;&#35302;&#21457;LLMs&#20351;&#29992;&#24037;&#20855;&#30340;&#25552;&#31034;&#24418;&#24335;&#20986;&#29616;&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#65292;&#21253;&#25324;&#21333;&#19968;&#24037;&#20855;&#21644;&#22810;&#31181;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have garnered significant attention due to their impressive natural language processing (NLP) capabilities. Recently, many studies have focused on the tool utilization ability of LLMs. They primarily investigated how LLMs effectively collaborate with given specific tools. However, in scenarios where LLMs serve as intelligent agents, as seen in applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate decision-making processes that involve deciding whether to employ a tool and selecting the most suitable tool(s) from a collection of available tools to fulfill user requests. Therefore, in this paper, we introduce MetaTool, a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools. Specifically, we create a dataset called ToolE within the benchmark. This dataset contains various types of user queries in the form of prompts that trigger LLMs to use tools, including both single-tool and multi-too
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#25552;&#21462;&#20851;&#38190;&#21477;&#23376;&#20877;&#36827;&#34892;&#35780;&#20272;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#25991;&#26723;&#25688;&#35201;&#35780;&#20272;&#20013;&#36935;&#21040;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#21644;&#24573;&#35270;&#37325;&#35201;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#35780;&#20272;&#25104;&#26412;&#65292;&#32780;&#19988;&#19982;&#20154;&#24037;&#35780;&#20272;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#26368;&#20339;&#25991;&#26723;&#38271;&#24230;&#21644;&#21477;&#23376;&#25552;&#21462;&#26041;&#27861;&#30340;&#23454;&#29992;&#24314;&#35758;&#65292;&#20026;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2309.07382</link><description>&lt;p&gt;
&#38271;&#25991;&#26412;&#25688;&#35201;&#35780;&#20272;&#20013;&#8220;&#23569;&#21363;&#26159;&#22810;&#8221;&#30340;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Less is More for Long Document Summary Evaluation by LLMs. (arXiv:2309.07382v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07382
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#25552;&#21462;&#20851;&#38190;&#21477;&#23376;&#20877;&#36827;&#34892;&#35780;&#20272;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#25991;&#26723;&#25688;&#35201;&#35780;&#20272;&#20013;&#36935;&#21040;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#21644;&#24573;&#35270;&#37325;&#35201;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#35780;&#20272;&#25104;&#26412;&#65292;&#32780;&#19988;&#19982;&#20154;&#24037;&#35780;&#20272;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#26368;&#20339;&#25991;&#26723;&#38271;&#24230;&#21644;&#21477;&#23376;&#25552;&#21462;&#26041;&#27861;&#30340;&#23454;&#29992;&#24314;&#35758;&#65292;&#20026;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#30340;&#21457;&#23637;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25688;&#35201;&#35780;&#20272;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#35832;&#22914;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#38271;&#25991;&#26723;&#20013;&#37325;&#35201;&#20449;&#24687;&#34987;&#24573;&#35270;&#30340;&#8220;&#36855;&#22833;&#22312;&#20013;&#38388;&#8221;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#8220;&#20808;&#25552;&#21462;&#20877;&#35780;&#20272;&#8221;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#20174;&#38271;&#25991;&#26412;&#28304;&#25991;&#20214;&#20013;&#25552;&#21462;&#20851;&#38190;&#21477;&#23376;&#65292;&#28982;&#21518;&#36890;&#36807;&#25552;&#38382;LLMs&#26469;&#35780;&#20272;&#25688;&#35201;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#26174;&#33879;&#38477;&#20302;&#20102;&#35780;&#20272;&#25104;&#26412;&#65292;&#32780;&#19988;&#19982;&#20154;&#24037;&#35780;&#20272;&#20043;&#38388;&#23384;&#22312;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#26368;&#20339;&#25991;&#26723;&#38271;&#24230;&#21644;&#21477;&#23376;&#25552;&#21462;&#26041;&#27861;&#30340;&#23454;&#29992;&#24314;&#35758;&#65292;&#20026;&#22522;&#20110;LLMs&#30340;&#25991;&#26412;&#29983;&#25104;&#35780;&#20272;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#25104;&#26412;&#25928;&#30410;&#26356;&#39640;&#19988;&#26356;&#20934;&#30830;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promising performance in summary evaluation tasks, yet they face challenges such as high computational costs and the Lost-in-the-Middle problem where important information in the middle of long documents is often overlooked. To address these issues, this paper introduces a novel approach, Extract-then-Evaluate, which involves extracting key sentences from a long source document and then evaluating the summary by prompting LLMs. The results reveal that the proposed method not only significantly reduces evaluation costs but also exhibits a higher correlation with human evaluations. Furthermore, we provide practical recommendations for optimal document length and sentence extraction methods, contributing to the development of cost-effective yet more accurate methods for LLM-based text generation evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;3D&#22330;&#26223;&#20013;&#24320;&#25918;&#35789;&#27719;&#20840;&#26223;&#20998;&#21106;&#30340;&#31639;&#27861;PVLFF&#65292;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;2D&#27169;&#22411;&#20013;&#25552;&#21462;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#26469;&#23398;&#20064;&#35821;&#20041;&#29305;&#24449;&#22330;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#24103;&#19978;&#30340;2D&#23454;&#20363;&#20998;&#21106;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#26469;&#32852;&#21512;&#25311;&#21512;&#23454;&#20363;&#29305;&#24449;&#22330;&#12290;&#35813;&#26041;&#27861;&#22312;&#20840;&#26223;&#20998;&#21106;&#21644;&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05448</link><description>&lt;p&gt;
&#20840;&#26223;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#22330;
&lt;/p&gt;
&lt;p&gt;
Panoptic Vision-Language Feature Fields. (arXiv:2309.05448v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;3D&#22330;&#26223;&#20013;&#24320;&#25918;&#35789;&#27719;&#20840;&#26223;&#20998;&#21106;&#30340;&#31639;&#27861;PVLFF&#65292;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;2D&#27169;&#22411;&#20013;&#25552;&#21462;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#26469;&#23398;&#20064;&#35821;&#20041;&#29305;&#24449;&#22330;&#65292;&#36890;&#36807;&#23545;&#36755;&#20837;&#24103;&#19978;&#30340;2D&#23454;&#20363;&#20998;&#21106;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#26469;&#32852;&#21512;&#25311;&#21512;&#23454;&#20363;&#29305;&#24449;&#22330;&#12290;&#35813;&#26041;&#27861;&#22312;&#20840;&#26223;&#20998;&#21106;&#21644;&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#19968;&#20123;&#29992;&#20110;3D&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#26681;&#25454;&#36816;&#34892;&#26102;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#23558;&#22330;&#26223;&#20998;&#21106;&#25104;&#20219;&#24847;&#31867;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36804;&#20170;&#20026;&#27490;&#39318;&#20010;&#29992;&#20110;3D&#22330;&#26223;&#20013;&#24320;&#25918;&#35789;&#27719;&#20840;&#26223;&#20998;&#21106;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;Panoptic Vision-Language Feature Fields (PVLFF)&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#30340;2D&#27169;&#22411;&#20013;&#25552;&#21462;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#26469;&#23398;&#20064;&#22330;&#26223;&#30340;&#35821;&#20041;&#29305;&#24449;&#22330;&#65292;&#24182;&#36890;&#36807;&#22312;&#36755;&#20837;&#24103;&#19978;&#20351;&#29992;2D&#23454;&#20363;&#20998;&#21106;&#23454;&#29616;&#23545;&#23454;&#20363;&#29305;&#24449;&#22330;&#30340;&#32852;&#21512;&#25311;&#21512;&#12290;&#23613;&#31649;&#27809;&#26377;&#38024;&#23545;&#30446;&#26631;&#31867;&#21035;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;HyperSim&#12289;ScanNet&#21644;Replica&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#30340;&#38381;&#38598;3D&#31995;&#32479;&#30456;&#20284;&#30340;&#20840;&#26223;&#20998;&#21106;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#30340;3D&#24320;&#25918;&#35789;&#27719;&#31995;&#32479;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#26041;&#27861;&#30340;&#32452;&#25104;&#37096;&#20998;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, methods have been proposed for 3D open-vocabulary semantic segmentation. Such methods are able to segment scenes into arbitrary classes based on text descriptions provided during runtime. In this paper, we propose to the best of our knowledge the first algorithm for open-vocabulary panoptic segmentation in 3D scenes. Our algorithm, Panoptic Vision-Language Feature Fields (PVLFF), learns a semantic feature field of the scene by distilling vision-language features from a pretrained 2D model, and jointly fits an instance feature field through contrastive learning using 2D instance segments on input frames. Despite not being trained on the target classes, our method achieves panoptic segmentation performance similar to the state-of-the-art closed-set 3D systems on the HyperSim, ScanNet and Replica dataset and additionally outperforms current 3D open-vocabulary systems in terms of semantic segmentation. We ablate the components of our method to demonstrate the effectiveness of our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#21512;&#29702;&#36164;&#28304;&#28040;&#32791;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#23558;&#35821;&#35328;&#27169;&#22411;&#19987;&#38376;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.10462</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models. (arXiv:2308.10462v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#21512;&#29702;&#36164;&#28304;&#28040;&#32791;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#23558;&#35821;&#35328;&#27169;&#22411;&#19987;&#38376;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#29305;&#23450;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#21487;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#29983;&#25104;&#20934;&#30830;&#30340;&#20195;&#30721;&#29255;&#27573;&#30340;&#21360;&#35937;&#33021;&#21147;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#31361;&#20986;&#20102;&#24494;&#35843;LLMs&#30340;&#20248;&#21183;&#65292;&#20294;&#36825;&#20010;&#36807;&#31243;&#20195;&#20215;&#39640;&#65292;&#23545;&#20110;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#22312;&#36164;&#28304;&#31232;&#32570;&#30340;&#29615;&#22659;&#19979;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20316;&#20026;&#19968;&#31181;&#31574;&#30053;&#65292;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#31034;&#20363;&#25351;&#23548;LLM&#29983;&#25104;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;ICL&#24341;&#20837;&#20102;&#19968;&#20123;&#19981;&#20415;&#20043;&#22788;&#65292;&#27604;&#22914;&#38656;&#35201;&#35774;&#35745;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25552;&#31034;&#21644;&#27809;&#26377;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#39044;&#35265;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#21512;&#29702;&#36164;&#28304;&#28040;&#32791;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#23558;LLM&#19987;&#38376;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in zero-shot, i.e., without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored In-Context Learning (ICL) as a strategy to guide the LLM generative process with task-specific prompt examples. However, ICL introduces inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee Parameter-Efficient Fine-Tuning (PEFT) techniques as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#21462;&#21644;&#28040;&#38500;&#21453;&#19987;&#23478;PEMs&#20013;&#30340;&#27531;&#32570;&#33021;&#21147;&#26469;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#21644;&#21435;&#27602;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.08090</link><description>&lt;p&gt;
&#25226;&#39640;&#19979;&#20998;&#28165;&#26970;&#65306;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#27169;&#22359;&#25805;&#20316;&#36827;&#34892;&#27169;&#22411;&#27531;&#32570;&#24615;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation. (arXiv:2308.08090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08090
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#21462;&#21644;&#28040;&#38500;&#21453;&#19987;&#23478;PEMs&#20013;&#30340;&#27531;&#32570;&#33021;&#21147;&#26469;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#21644;&#21435;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#23384;&#22312;&#19982;&#19981;&#30495;&#23454;&#21644;&#26377;&#27602;&#24615;&#26377;&#20851;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#21442;&#25968;&#39640;&#25928;&#27169;&#22359;&#65288;PEMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22312;&#20026;&#27169;&#22411;&#36171;&#20104;&#26032;&#25216;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#21033;&#29992;PEMs&#36827;&#34892;&#27531;&#32570;&#24615;&#21435;&#23398;&#20064;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;PEMs&#25805;&#20316;&#26041;&#27861;&#65292;&#21363;&#8220;&#25552;&#21462;-&#20943;&#21435;&#8221;&#65288;Ext-Sub&#65289;&#65292;&#36890;&#36807;&#25972;&#21512;&#8220;&#19987;&#23478;&#8221;PEMs&#21644;&#8220;&#21453;&#19987;&#23478;&#8221;PEMs&#26469;&#22686;&#24378;LLMs&#30340;&#30495;&#23454;&#24615;&#21644;&#21435;&#27602;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#26159;&#21453;&#19987;&#23478;PEMs&#20063;&#20855;&#26377;&#23453;&#36149;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#25797;&#38271;&#29983;&#25104;&#34394;&#26500;&#20869;&#23481;&#65292;&#36825;&#38656;&#35201;&#35821;&#35328;&#24314;&#27169;&#21644;&#36923;&#36753;&#21465;&#36848;&#33021;&#21147;&#12290;&#19982;&#20165;&#20165;&#21542;&#23450;&#21442;&#25968;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#25552;&#21462;&#21644;&#28040;&#38500;&#21453;&#19987;&#23478;PEMs&#20013;&#30340;&#27531;&#32570;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#30041;&#19968;&#33324;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been widely used in various applications but are known to suffer from issues related to untruthfulness and toxicity. While parameter-efficient modules (PEMs) have demonstrated their effectiveness in equipping models with new skills, leveraging PEMs for deficiency unlearning remains underexplored. In this work, we propose a PEMs operation approach, namely Extraction-before-Subtraction (Ext-Sub), to enhance the truthfulness and detoxification of LLMs through the integration of ``expert'' PEM and ``anti-expert'' PEM. Remarkably, even anti-expert PEM possess valuable capabilities due to their proficiency in generating fabricated content, which necessitates language modeling and logical narrative competence. Rather than merely negating the parameters, our approach involves extracting and eliminating solely the deficiency capability within anti-expert PEM while preserving the general capabilities. To evaluate the effectiveness of our approach in terms of tru
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LoRA&#32452;&#21512;&#22312;&#36328;&#20219;&#21153;&#36890;&#29992;&#24615;&#19978;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;LoraHub&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#65292;&#23454;&#29616;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#21487;&#36866;&#24212;&#24615;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LoraHub&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#33021;&#22815;&#26377;&#25928;&#27169;&#25311;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.13269</link><description>&lt;p&gt;
LoraHub: &#36890;&#36807;&#21160;&#24577;LoRA&#32452;&#21512;&#23454;&#29616;&#39640;&#25928;&#30340;&#20219;&#21153;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition. (arXiv:2307.13269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LoRA&#32452;&#21512;&#22312;&#36328;&#20219;&#21153;&#36890;&#29992;&#24615;&#19978;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;LoraHub&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#65292;&#23454;&#29616;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#21487;&#36866;&#24212;&#24615;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LoraHub&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#33021;&#22815;&#26377;&#25928;&#27169;&#25311;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#24120;&#24120;&#34987;&#29992;&#20110;&#23545;&#26032;&#20219;&#21153;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24494;&#35843;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LoRA&#32452;&#21512;&#22312;&#36328;&#20219;&#21153;&#36890;&#29992;&#24615;&#19978;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;LoraHub&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#30446;&#30340;&#24615;&#32452;&#35013;&#22312;&#19981;&#21516;&#32473;&#23450;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#30340;&#25112;&#30053;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#21487;&#36866;&#24212;&#24615;&#24615;&#33021;&#12290;&#20165;&#20973;&#20511;&#26469;&#33258;&#26032;&#20219;&#21153;&#30340;&#20960;&#20010;&#31034;&#20363;&#65292;LoraHub&#21487;&#20197;&#28789;&#27963;&#22320;&#32452;&#21512;&#22810;&#20010;LoRA&#27169;&#22359;&#65292;&#28040;&#38500;&#20102;&#23545;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#32452;&#21512;&#26082;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#20063;&#19981;&#38656;&#35201;&#26799;&#24230;&#12290;&#25105;&#20204;&#20174;Big-Bench Hard&#65288;BBH&#65289;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#20986;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;LoraHub&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#20013;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#65292;&#22312;&#27599;&#20010;&#25512;&#29702;&#36755;&#20837;&#26049;&#36793;&#19981;&#38656;&#35201;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#22521;&#32946;&#19968;&#20010;LoRA&#31038;&#21306;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#20854;&#20013;&#20998;&#20139;&#20182;&#20204;&#35757;&#32451;&#30340;LoRA&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-rank adaptations (LoRA) are often employed to fine-tune large language models (LLMs) for new tasks. This paper investigates LoRA composability for cross-task generalization and introduces LoraHub, a strategic framework devised for the purposive assembly of LoRA modules trained on diverse given tasks, with the objective of achieving adaptable performance on unseen tasks. With just a few examples from a novel task, LoraHub enables the fluid combination of multiple LoRA modules, eradicating the need for human expertise. Notably, the composition requires neither additional model parameters nor gradients. Our empirical results, derived from the Big-Bench Hard (BBH) benchmark, suggest that LoraHub can effectively mimic the performance of in-context learning in few-shot scenarios, excluding the necessity of in-context examples alongside each inference input. A significant contribution of our research is the fostering of a community for LoRA, where users can share their trained LoRA module
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;CMMLU&#65292;&#36825;&#26159;&#19968;&#20010;&#34913;&#37327;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#30340;&#32508;&#21512;&#24615;&#22522;&#20934;&#27979;&#35797;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#20027;&#39064;&#21644;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#37117;&#19981;&#22815;&#29702;&#24819;&#65292;&#23384;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;CMMLU&#22635;&#34917;&#20102;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;&#22686;&#24378;LLM&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.09212</link><description>&lt;p&gt;
CMMLU&#65306;&#34913;&#37327;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
CMMLU: Measuring massive multitask language understanding in Chinese. (arXiv:2306.09212v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;CMMLU&#65292;&#36825;&#26159;&#19968;&#20010;&#34913;&#37327;&#20013;&#25991;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#30340;&#32508;&#21512;&#24615;&#22522;&#20934;&#27979;&#35797;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#20027;&#39064;&#21644;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#37117;&#19981;&#22815;&#29702;&#24819;&#65292;&#23384;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;CMMLU&#22635;&#34917;&#20102;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31354;&#30333;&#65292;&#24182;&#25552;&#20986;&#20102;&#22686;&#24378;LLM&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#19981;&#26029;&#25552;&#39640;&#65292;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;CMMLU&#65292;&#19968;&#20010;&#21253;&#25324;&#33258;&#28982;&#31185;&#23398;&#12289;&#31038;&#20250;&#31185;&#23398;&#12289;&#24037;&#31243;&#21644;&#20154;&#25991;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#32508;&#21512;&#24615;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;&#65292;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#23545;18&#31181;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#21644;&#38754;&#21521;&#20013;&#25991;&#30340;LLM&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20215;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#19981;&#21516;&#20027;&#39064;&#21644;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;LLM&#22312;&#25552;&#20379;&#19978;&#19979;&#25991;&#31034;&#20363;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#26102;&#65292;&#38590;&#20197;&#36798;&#21040;&#24179;&#22343;50%&#30340;&#20934;&#30830;&#29575;&#65292;&#32780;&#38543;&#26426;&#22522;&#20934;&#32447;&#21017;&#20026;25%&#12290;&#36825;&#31361;&#26174;&#20102;LLM&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#30830;&#23450;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#22686;&#24378;LLM&#30340;&#26041;&#21521;&#12290;CMMLU&#22635;&#34917;&#20102;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the capabilities of large language models (LLMs) continue to advance, evaluating their performance becomes increasingly crucial and challenging. This paper aims to bridge this gap by introducing CMMLU, a comprehensive Chinese benchmark that covers various subjects, including natural science, social sciences, engineering, and humanities. We conduct a thorough evaluation of 18 advanced multilingual- and Chinese-oriented LLMs, assessing their performance across different subjects and settings. The results reveal that most existing LLMs struggle to achieve an average accuracy of 50%, even when provided with in-context examples and chain-of-thought prompts, whereas the random baseline stands at 25%. This highlights significant room for improvement in LLMs. Additionally, we conduct extensive experiments to identify factors impacting the models' performance and propose directions for enhancing LLMs. CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large languag
&lt;/p&gt;</description></item><item><title>&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#30340;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#21487;&#20197;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#26816;&#27979;&#21644;&#30830;&#35748;&#65292;&#36825;&#21487;&#24110;&#21161;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#36827;&#34892;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.05535</link><description>&lt;p&gt;
&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#26816;&#27979;&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;
&lt;/p&gt;
&lt;p&gt;
Detecting Check-Worthy Claims in Political Debates, Speeches, and Interviews Using Audio Data. (arXiv:2306.05535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05535
&lt;/p&gt;
&lt;p&gt;
&#25919;&#27835;&#36777;&#35770;&#12289;&#28436;&#35762;&#21644;&#35775;&#35848;&#20013;&#30340;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#21487;&#20197;&#20351;&#29992;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#26816;&#27979;&#21644;&#30830;&#35748;&#65292;&#36825;&#21487;&#24110;&#21161;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#36827;&#34892;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#30340;&#19968;&#22823;&#37096;&#20998;&#22242;&#32467;&#22312;&#30456;&#21516;&#30340;&#24895;&#26223;&#21644;&#24605;&#24819;&#21608;&#22260;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#33021;&#37327;&#12290;&#36825;&#27491;&#26159;&#25919;&#27835;&#20154;&#29289;&#24076;&#26395;&#20026;&#20182;&#20204;&#30340;&#20107;&#19994;&#25152;&#32047;&#31215;&#30340;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#20182;&#20204;&#26377;&#26102;&#20250;&#20351;&#29992;&#25197;&#26354;&#25110;&#38544;&#34255;&#30495;&#30456;&#30340;&#25163;&#27573;&#65292;&#26080;&#35770;&#26159;&#26080;&#24847;&#30340;&#36824;&#26159;&#26377;&#24847;&#30340;&#65292;&#36825;&#20026;&#38169;&#35823;&#20449;&#24687;&#21644;&#35823;&#23548;&#24320;&#20102;&#22823;&#38376;&#12290;&#33258;&#21160;&#26816;&#27979;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#30340;&#24037;&#20855;&#23558;&#23545;&#36777;&#35770;&#20027;&#25345;&#20154;&#12289;&#35760;&#32773;&#21644;&#20107;&#23454;&#26680;&#26597;&#32452;&#32455;&#26377;&#24456;&#22823;&#24110;&#21161;&#12290;&#34429;&#28982;&#20197;&#21069;&#20851;&#20110;&#26816;&#27979;&#20540;&#24471;&#26680;&#23454;&#30340;&#35770;&#26029;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#25991;&#26412;&#65292;&#20294;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#38899;&#39057;&#20449;&#21495;&#20316;&#20026;&#39069;&#22806;&#20449;&#24687;&#28304;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65288;&#33521;&#35821;&#25991;&#26412;&#21644;&#38899;&#39057;&#65289;&#65292;&#21253;&#21547;48&#23567;&#26102;&#30340;&#28436;&#35762;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#28436;&#35762;&#32773;&#30340;&#24773;&#20917;&#19979;&#65292;&#38899;&#39057;&#27169;&#24577;&#19982;&#25991;&#26412;&#32467;&#21512;&#20351;&#29992;&#27604;&#20165;&#20351;&#29992;&#25991;&#26412;&#20855;&#26377;&#25913;&#36827;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#21333;&#22768;&#36947;&#38899;&#39057;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#21333;&#22768;&#36947;&#25991;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A large portion of society united around the same vision and ideas carries enormous energy. That is precisely what political figures would like to accumulate for their cause. With this goal in mind, they can sometimes resort to distorting or hiding the truth, unintentionally or on purpose, which opens the door for misinformation and disinformation. Tools for automatic detection of check-worthy claims would be of great help to moderators of debates, journalists, and fact-checking organizations. While previous work on detecting check-worthy claims has focused on text, here we explore the utility of the audio signal as an additional information source. We create a new multimodal dataset (text and audio in English) containing 48 hours of speech. Our evaluation results show that the audio modality together with text yields improvements over text alone in the case of multiple speakers. Moreover, an audio-only model could outperform a text-only one for a single speaker.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13971</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#28789;&#27963;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#23569;&#37327;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20449;&#24687;&#25552;&#21462;&#25152;&#38656;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26102;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;LLM&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20542;&#21521;&#20110;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#32780;&#19981;&#26159;&#36981;&#24490;&#29305;&#23450;&#35821;&#27861;&#30340;&#31934;&#30830;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35299;&#30721;&#27493;&#39588;&#20013;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#26469;&#20016;&#23500;&#27169;&#22411;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#31526;&#21512;&#35821;&#27861;&#20135;&#29983;&#35268;&#21017;&#30340;&#26377;&#25928;&#20196;&#29260;&#33021;&#34987;&#32771;&#34385;&#21040;&#12290;&#36825;&#26679;&#23601;&#24378;&#21046;&#21482;&#20135;&#29983;&#26377;&#25928;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#21644;&#28789;&#27963;&#65292;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;(CFG)&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#32422;&#26463;beam&#25628;&#32034;&#23454;&#29616;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;NLP&#20219;&#21153;&#30340;&#36755;&#20986;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#24418;&#24335;&#35821;&#35328;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30452;&#25509;&#20351;&#29992;&#12290;&#23545;&#20110;&#36755;&#20986;&#31354;&#38388;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20837;&#30340;CFG&#65292;&#26681;&#25454;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#26356;&#26032;&#20135;&#29983;&#35268;&#21017;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#30340;&#30740;&#31350;&#20102;&#29616;&#26377;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#20110;&#25903;&#25345;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#25152;&#25552;&#20379;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2304.11065</link><description>&lt;p&gt;
&#23545;&#35805;&#36807;&#31243;&#24314;&#27169;&#65306;&#29616;&#29366;&#12289;&#24212;&#29992;&#21644;&#23454;&#36341;&#24433;&#21709;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Conversational Process Modelling: State of the Art, Applications, and Implications in Practice. (arXiv:2304.11065v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#30340;&#30740;&#31350;&#20102;&#29616;&#26377;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#20110;&#25903;&#25345;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#25152;&#25552;&#20379;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;Chatbots&#31561;&#32842;&#22825;&#26426;&#22120;&#20154;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#23545;&#20110;BPM&#24212;&#29992;&#26469;&#35828;&#65292;&#22914;&#20309;&#24212;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#26469;&#29983;&#25104;&#21830;&#19994;&#20215;&#20540;&#36890;&#24120;&#26159;&#19981;&#26126;&#30830;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#20998;&#26512;&#29616;&#26377;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#20110;&#25903;&#25345;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#20316;&#20026;&#38754;&#21521;&#27969;&#31243;&#30340;&#33021;&#21147;&#30340;&#25903;&#25345;&#12290;&#35813;&#30740;&#31350;&#35782;&#21035;&#20102;&#27839;&#27969;&#31243;&#29983;&#21629;&#21608;&#26399;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#12290;&#24471;&#20986;&#30340;&#20998;&#31867;&#23398;&#29992;&#20316;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#30340;&#24212;&#29992;&#22330;&#26223;&#30340;&#35782;&#21035;&#65292;&#21253;&#25324;&#27969;&#31243;&#25551;&#36848;&#30340;&#37322;&#20041;&#21644;&#25913;&#36827;&#12290;&#24212;&#29992;&#22330;&#26223;&#22522;&#20110;&#39640;&#31561;&#25945;&#32946;&#39046;&#22495;&#30340;&#23454;&#38469;&#27979;&#35797;&#38598;&#23545;&#29616;&#26377;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#35780;&#20272;&#12290;&#35813;&#27979;&#35797;&#38598;&#21253;&#21547;&#27969;&#31243;&#25551;&#36848;&#21450;&#20854;&#23545;&#24212;&#30340;&#27969;&#31243;&#27169;&#22411;&#65292;&#20197;&#21450;&#27169;&#22411;&#36136;&#37327;&#30340;&#35780;&#20272;&#12290;&#22522;&#20110;&#25991;&#29486;&#21644;&#24212;&#29992;&#22330;&#26223;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#22312;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#20013;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots such as ChatGPT have caused a tremendous hype lately. For BPM applications, it is often not clear how to apply chatbots to generate business value. Hence, this work aims at the systematic analysis of existing chatbots for their support of conversational process modelling as process-oriented capability. Application scenarios are identified along the process life cycle. Then a systematic literature review on conversational process modelling is performed. The resulting taxonomy serves as input for the identification of application scenarios for conversational process modelling, including paraphrasing and improvement of process descriptions. The application scenarios are evaluated for existing chatbots based on a real-world test set from the higher education domain. It contains process descriptions as well as corresponding process models, together with an assessment of the model quality. Based on the literature and application scenario analyses, recommendations for the usage (prac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09048</link><description>&lt;p&gt;
CodeKGC&#65306;&#29992;&#20110;&#29983;&#25104;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeKGC: Code Language Model for Generative Knowledge Graph Construction. (arXiv:2304.09048v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#32467;&#26500;&#24615;&#30693;&#35782;&#65292;&#32780;&#21482;&#26159;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#24207;&#21015;&#21270;&#25991;&#26412;&#25110;&#35268;&#33539;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20687;&#20195;&#30721;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20102;&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#20197;&#36827;&#34892;&#32467;&#26500;&#24615;&#39044;&#27979;&#21644;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#29983;&#25104;&#24335;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#32473;&#23450;&#20195;&#30721;&#26684;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#21487;&#20197;&#34920;&#31034;&#20026;&#20195;&#30721;&#34917;&#20840;&#20219;&#21153;&#30340;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20855;&#26377;&#27169;&#24335;&#24863;&#30693;&#22411;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#35821;&#20041;&#32467;&#26500;&#12290;&#30001;&#20110;&#20195;&#30721;&#26412;&#36136;&#19978;&#20855;&#26377;&#32467;&#26500;&#65292;&#22914;&#31867;&#21644;&#20989;&#25968;&#23450;&#20041;&#65292;&#22240;&#27492;&#23427;&#20316;&#20026;&#20808;&#39564;&#30340;&#35821;&#20041;&#32467;&#26500;&#30693;&#35782;&#27169;&#22411;&#38750;&#24120;&#26377;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110;&#21407;&#29702;&#30340;&#29983;&#25104;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#21407;&#29702;&#25552;&#20379;&#20102;&#27169;&#22411;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current generative knowledge graph construction approaches usually fail to capture structural knowledge by simply flattening natural language into serialized texts or a specification language. However, large generative language model trained on structured data such as code has demonstrated impressive capability in understanding natural language for structural prediction and reasoning tasks. Intuitively, we address the task of generative knowledge graph construction with code language model: given a code-format natural language input, the target is to generate triples which can be represented as code completion tasks. Specifically, we develop schema-aware prompts that effectively utilize the semantic structure within the knowledge graph. As code inherently possesses structure, such as class and function definitions, it serves as a useful model for prior semantic structural knowledge. Furthermore, we employ a rationale-enhanced generation method to boost the performance. Rationales provi
&lt;/p&gt;</description></item><item><title>ESD&#26159;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#21487;&#35757;&#32451;&#26657;&#20934;&#30446;&#26631;&#25439;&#22833;&#65292;&#36890;&#36807;&#23558;&#26657;&#20934;&#35823;&#24046;&#30475;&#20316;&#20004;&#20010;&#26399;&#26395;&#20540;&#20043;&#38388;&#30340;&#24179;&#26041;&#24046;&#65292;&#21487;&#20197;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26657;&#20934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.02472</link><description>&lt;p&gt;
ESD:&#39044;&#26399;&#24179;&#26041;&#24046;&#20316;&#20026;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#21487;&#35757;&#32451;&#26657;&#20934;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
ESD: Expected Squared Difference as a Tuning-Free Trainable Calibration Measure. (arXiv:2303.02472v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02472
&lt;/p&gt;
&lt;p&gt;
ESD&#26159;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#21487;&#35757;&#32451;&#26657;&#20934;&#30446;&#26631;&#25439;&#22833;&#65292;&#36890;&#36807;&#23558;&#26657;&#20934;&#35823;&#24046;&#30475;&#20316;&#20004;&#20010;&#26399;&#26395;&#20540;&#20043;&#38388;&#30340;&#24179;&#26041;&#24046;&#65292;&#21487;&#20197;&#25913;&#21892;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26657;&#20934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30001;&#20110;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#32780;&#24448;&#24448;&#26657;&#20934;&#19981;&#33391;&#12290;&#20256;&#32479;&#19978;&#65292;&#22312;&#35757;&#32451;&#20043;&#21518;&#20351;&#29992;&#21518;&#22788;&#29702;&#26041;&#27861;&#26469;&#26657;&#20934;&#27169;&#22411;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21487;&#35757;&#32451;&#30340;&#26657;&#20934;&#24230;&#37327;&#26469;&#30452;&#25509;&#23558;&#20854;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#21253;&#21547;&#20869;&#37096;&#36229;&#21442;&#25968;&#65292;&#24182;&#19988;&#36825;&#20123;&#26657;&#20934;&#30446;&#26631;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#35843;&#25972;&#36825;&#20123;&#36229;&#21442;&#25968;&#65292;&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#22686;&#22823;&#65292;&#20250;&#20135;&#29983;&#26356;&#22810;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#26399;&#24179;&#26041;&#24046;&#65288;ESD&#65289;&#65292;&#19968;&#31181;&#26080;&#38656;&#35843;&#21442;&#30340;&#21487;&#35757;&#32451;&#26657;&#20934;&#30446;&#26631;&#25439;&#22833;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#26399;&#26395;&#20540;&#20043;&#38388;&#30340;&#24179;&#26041;&#24046;&#30340;&#35282;&#24230;&#26469;&#30475;&#26657;&#20934;&#35823;&#24046;&#12290;&#36890;&#36807;&#23545;&#20960;&#31181;&#26550;&#26500;&#65288;CNN&#12289;Transformer&#65289;&#21644;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;ESD&#32435;&#20837;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#26657;&#20934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studies have shown that modern neural networks tend to be poorly calibrated due to over-confident predictions. Traditionally, post-processing methods have been used to calibrate the model after training. In recent years, various trainable calibration measures have been proposed to incorporate them directly into the training process. However, these methods all incorporate internal hyperparameters, and the performance of these calibration objectives relies on tuning these hyperparameters, incurring more computational costs as the size of neural networks and datasets become larger. As such, we present Expected Squared Difference (ESD), a tuning-free (i.e., hyperparameter-free) trainable calibration objective loss, where we view the calibration error from the perspective of the squared difference between the two expectations. With extensive experiments on several architectures (CNNs, Transformers) and datasets, we demonstrate that (1) incorporating ESD into the training improves model cali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65292;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19979;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#24182;&#26497;&#22823;&#22320;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.15629</link><description>&lt;p&gt;
&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#65306;&#36890;&#36807;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#20219;&#21153;&#39640;&#25928;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks. (arXiv:2210.15629v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65292;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19979;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#24182;&#26497;&#22823;&#22320;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#36890;&#29992;&#22411;&#26234;&#33021;&#20307;&#22312;&#21508;&#20010;&#26041;&#38754;&#37117;&#24456;&#22256;&#38590;&#65292;&#38656;&#35201;&#22788;&#29702;&#39640;&#32500;&#36755;&#20837;&#65288;&#31354;&#38388;&#65289;&#12289;&#38271;&#26102;&#38388;&#36328;&#24230;&#65288;&#26102;&#38388;&#65289;&#21644;&#22810;&#20010;&#26032;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#32467;&#26500;&#26041;&#38754;&#30340;&#36827;&#23637;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#27839;&#30528;&#20854;&#20013;&#19968;&#20010;&#25110;&#20004;&#20010;&#32500;&#24230;&#25552;&#39640;&#25193;&#23637;&#24615;&#33021;&#21147;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26465;&#20214;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65288;LCD&#65289;&#26469;&#24212;&#23545;&#36825;&#19977;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#12289;&#29366;&#24577;&#21644;&#20219;&#21153;&#31354;&#38388;&#32500;&#24230;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;CALVIN&#35821;&#35328;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#20013;&#23558;LCD&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LCD&#22312;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#32780;&#21333;&#20219;&#21153;&#25104;&#21151;&#29575;&#65288;SR&#65289;&#20026;88.7%&#65292;&#36828;&#39640;&#20110;&#20197;&#21069;&#30340;&#26368;&#20339;&#25104;&#32489;82.6%&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generalist agents is difficult across several axes, requiring us to deal with high-dimensional inputs (space), long horizons (time), and multiple and new tasks. Recent advances with architectures have allowed for improved scaling along one or two of these dimensions, but are still prohibitive computationally. In this paper, we propose to address all three axes by leveraging Language to Control Diffusion models as a hierarchical planner conditioned on language (LCD). We effectively and efficiently scale diffusion models for planning in extended temporal, state, and task dimensions to tackle long horizon control problems conditioned on natural language instructions. We compare LCD with other state-of-the-art models on the CALVIN language robotics benchmark and find that LCD outperforms other SOTA methods in multi task success rates while dramatically improving computational efficiency with a single task success rate (SR) of 88.7% against the previous best of 82.6%. We show that 
&lt;/p&gt;</description></item></channel></rss>