<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>OUTFOX&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20801;&#35768;&#26816;&#27979;&#22120;&#21644;&#25915;&#20987;&#32773;&#32771;&#34385;&#24444;&#27492;&#30340;&#36755;&#20986;&#65292;&#25552;&#39640;&#20102;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#25915;&#20987;&#32773;&#21033;&#29992;&#26816;&#27979;&#22120;&#30340;&#39044;&#27979;&#26631;&#31614;&#20316;&#20026;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#29983;&#25104;&#38590;&#20197;&#26816;&#27979;&#30340;&#23545;&#25239;&#29983;&#25104;&#30340;&#35770;&#25991;&#12290;</title><link>http://arxiv.org/abs/2307.11729</link><description>&lt;p&gt;
OUTFOX: &#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#23545;&#25239;&#29983;&#25104;&#20363;&#23376;&#30340;LLM&#29983;&#25104;&#35770;&#25991;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples. (arXiv:2307.11729v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11729
&lt;/p&gt;
&lt;p&gt;
OUTFOX&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20801;&#35768;&#26816;&#27979;&#22120;&#21644;&#25915;&#20987;&#32773;&#32771;&#34385;&#24444;&#27492;&#30340;&#36755;&#20986;&#65292;&#25552;&#39640;&#20102;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#25915;&#20987;&#32773;&#21033;&#29992;&#26816;&#27979;&#22120;&#30340;&#39044;&#27979;&#26631;&#31614;&#20316;&#20026;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#29983;&#25104;&#38590;&#20197;&#26816;&#27979;&#30340;&#23545;&#25239;&#29983;&#25104;&#30340;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#36798;&#21040;&#20102;&#19982;&#20154;&#31867;&#20889;&#20316;&#30456;&#24403;&#30340;&#27969;&#21033;&#31243;&#24230;&#65292;&#24456;&#38590;&#21306;&#20998;&#20154;&#31867;&#20889;&#20316;&#21644;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#36825;&#22686;&#21152;&#20102;LLMs&#34987;&#35823;&#29992;&#30340;&#39118;&#38505;&#65292;&#24182;&#38656;&#35201;&#24320;&#21457;&#26816;&#27979;&#22120;&#26469;&#35782;&#21035;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#36890;&#36807;&#31616;&#21333;&#22320;&#25913;&#20889;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26469;&#38477;&#20302;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#22312;&#23398;&#29983;&#22312;&#20889;&#20316;&#20316;&#19994;&#65288;&#22914;&#35770;&#25991;&#65289;&#20013;&#20351;&#29992;LLMs&#24182;&#36805;&#36895;&#23398;&#20250;&#22914;&#20309;&#35268;&#36991;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#30495;&#23454;&#29983;&#27963;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#34987;&#25506;&#35752;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OUTFOX&#65292;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20801;&#35768;&#26816;&#27979;&#22120;&#21644;&#25915;&#20987;&#32773;&#32771;&#34385;&#24444;&#27492;&#30340;&#36755;&#20986;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23398;&#29983;&#35770;&#25991;&#39046;&#22495;&#26469;&#25552;&#39640;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25915;&#20987;&#32773;&#20351;&#29992;&#26816;&#27979;&#22120;&#30340;&#39044;&#27979;&#26631;&#31614;&#20316;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#65292;&#24182;&#23545;&#38590;&#20197;&#26816;&#27979;&#30340;&#23545;&#25239;&#29983;&#25104;&#35770;&#25991;&#36827;&#34892;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and LLM-generated texts. This poses a growing risk of misuse of LLMs and demands the development of detectors to identify LLM-generated texts. However, existing detectors degrade detection accuracy by simply paraphrasing LLM-generated texts. Furthermore, the effectiveness of these detectors in real-life situations, such as when students use LLMs for writing homework assignments (e.g., essays) and quickly learn how to evade these detectors, has not been explored. In this paper, we propose OUTFOX, a novel framework that improves the robustness of LLM-generated-text detectors by allowing both the detector and the attacker to consider each other's output and apply this to the domain of student essays. In our framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are hard
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;GPT-4&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36866;&#24212;CLIP&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;CLIP&#30340;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#22312;&#19987;&#38376;&#32454;&#31890;&#24230;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20102;&#36739;&#22823;&#30340;0-shot&#36801;&#31227;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.11661</link><description>&lt;p&gt;
&#29992;GPT-4&#22686;&#24378;CLIP&#65306;&#21033;&#29992;&#35270;&#35273;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts. (arXiv:2307.11661v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;GPT-4&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36866;&#24212;CLIP&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;CLIP&#30340;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#22312;&#19987;&#38376;&#32454;&#31890;&#24230;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20102;&#36739;&#22823;&#30340;0-shot&#36801;&#31227;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22914;CLIP&#22312;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#33391;&#22909;&#24615;&#33021;&#65292;&#20174;&#32780;&#38761;&#26032;&#20102;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#12290;VLMs&#36890;&#36807;&#35774;&#35745;&#19982;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#25552;&#31034;&#26469;0-shot&#36866;&#24212;&#19979;&#28216;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#25552;&#31034;&#24037;&#31243;&#21033;&#29992;&#20102;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#39564;&#35777;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#20197;&#29992;&#20316;&#20808;&#36827;&#30340;&#20114;&#32852;&#32593;&#25628;&#32034;&#24037;&#20855;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#34987;&#25805;&#20316;&#20197;&#25552;&#20379;&#20219;&#20309;&#32467;&#26500;&#21270;&#30340;&#35270;&#35273;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;GPT-4&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36866;&#24212;CLIP&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;CLIP&#30340;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#19987;&#38376;&#32454;&#31890;&#24230;&#25968;&#25454;&#38598;&#65288;&#22914;EuroSAT&#65288;~7&#65285;&#65289;&#12289;DTD&#65288;~7&#65285;&#65289;&#12289;SUN397&#65288;~4.6&#65285;&#65289;&#21644;CUB&#65288;~3.3&#65285;&#65289;&#65289;&#19978;&#26174;&#31034;&#20986;&#20102;&#36739;&#22823;&#30340;0-shot&#36801;&#31227;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#23569;&#37327;&#26679;&#26412;&#36866;&#37197;&#22120;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#36873;&#25321;&#26368;&#20339;&#30340;s
&lt;/p&gt;
&lt;p&gt;
Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have revolutionized visual representation learning by providing good performance on downstream datasets. VLMs are 0-shot adapted to a downstream dataset by designing prompts that are relevant to the dataset. Such prompt engineering makes use of domain expertise and a validation dataset. Meanwhile, recent developments in generative pretrained models like GPT-4 mean they can be used as advanced internet search tools. They can also be manipulated to provide visual information in any structure. In this work, we show that GPT-4 can be used to generate text that is visually descriptive and how this can be used to adapt CLIP to downstream tasks. We show considerable improvements in 0-shot transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD (~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt. We also design a simple few-shot adapter that learns to choose the best possible s
&lt;/p&gt;</description></item><item><title>OxfordTVG-HIC&#26159;&#19968;&#20010;&#29992;&#20110;&#24189;&#40664;&#29983;&#25104;&#21644;&#29702;&#35299;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#20379;&#24191;&#27867;&#30340;&#24773;&#24863;&#21644;&#35821;&#20041;&#22810;&#26679;&#24615;&#65292;&#29305;&#21035;&#36866;&#21512;&#29983;&#25104;&#24189;&#40664;&#30340;&#33073;&#31163;&#19978;&#19979;&#25991;&#30340;&#20363;&#23376;&#65292;&#24182;&#29992;&#20110;&#35757;&#32451;&#36890;&#29992;&#24189;&#40664;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.11636</link><description>&lt;p&gt;
OxfordTVG-HIC: &#26426;&#22120;&#33021;&#21542;&#20174;&#22270;&#20687;&#20013;&#29983;&#25104;&#24189;&#40664;&#30340;&#26631;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?. (arXiv:2307.11636v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11636
&lt;/p&gt;
&lt;p&gt;
OxfordTVG-HIC&#26159;&#19968;&#20010;&#29992;&#20110;&#24189;&#40664;&#29983;&#25104;&#21644;&#29702;&#35299;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#20379;&#24191;&#27867;&#30340;&#24773;&#24863;&#21644;&#35821;&#20041;&#22810;&#26679;&#24615;&#65292;&#29305;&#21035;&#36866;&#21512;&#29983;&#25104;&#24189;&#40664;&#30340;&#33073;&#31163;&#19978;&#19979;&#25991;&#30340;&#20363;&#23376;&#65292;&#24182;&#29992;&#20110;&#35757;&#32451;&#36890;&#29992;&#24189;&#40664;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;OxfordTVG-HIC&#65288;Humorous Image Captions&#65289;&#36825;&#20010;&#22823;&#35268;&#27169;&#30340;&#29992;&#20110;&#24189;&#40664;&#29983;&#25104;&#21644;&#29702;&#35299;&#30340;&#25968;&#25454;&#38598;&#12290;&#24189;&#40664;&#26159;&#19968;&#20010;&#25277;&#35937;&#12289;&#20027;&#35266;&#21644;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#35748;&#30693;&#26500;&#24314;&#65292;&#28041;&#21450;&#22810;&#20010;&#35748;&#30693;&#22240;&#32032;&#65292;&#20351;&#24471;&#29983;&#25104;&#21644;&#35299;&#37322;&#24189;&#40664;&#25104;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#24189;&#40664;&#29983;&#25104;&#21644;&#29702;&#35299;&#21487;&#20197;&#20316;&#20026;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22788;&#29702;&#25277;&#35937;&#21644;&#20027;&#35266;&#20449;&#24687;&#33021;&#21147;&#30340;&#26032;&#20219;&#21153;&#12290;&#30001;&#20110;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#19982;&#24189;&#40664;&#30456;&#20851;&#30340;&#29983;&#25104;&#20219;&#21153;&#65288;&#22914;&#23383;&#24149;&#29983;&#25104;&#65289;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;OxfordTVG-HIC&#25552;&#20379;&#20102;&#32422;290&#19975;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#24182;&#38468;&#24102;&#24189;&#40664;&#35780;&#20998;&#65292;&#29992;&#20110;&#35757;&#32451;&#19968;&#20010;&#20855;&#26377;&#36890;&#29992;&#24189;&#40664;&#23383;&#24149;&#29983;&#25104;&#33021;&#21147;&#30340;&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#30340;&#23383;&#24149;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;OxfordTVG-HIC&#20855;&#26377;&#24191;&#27867;&#30340;&#24773;&#24863;&#21644;&#35821;&#20041;&#22810;&#26679;&#24615;&#65292;&#23548;&#33268;&#19968;&#20123;&#33073;&#31163;&#19978;&#19979;&#25991;&#30340;&#20363;&#23376;&#65292;&#29305;&#21035;&#36866;&#21512;&#29983;&#25104;&#24189;&#40664;&#12290;&#27492;&#22806;&#65292;OxfordTVG-HIC&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#65292;&#19981;&#21547;&#20882;&#29359;&#24615;&#20869;&#23481;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;Oxf
&lt;/p&gt;
&lt;p&gt;
This paper presents OxfordTVG-HIC (Humorous Image Captions), a large-scale dataset for humour generation and understanding. Humour is an abstract, subjective, and context-dependent cognitive construct involving several cognitive factors, making it a challenging task to generate and interpret. Hence, humour generation and understanding can serve as a new task for evaluating the ability of deep-learning methods to process abstract and subjective information. Due to the scarcity of data, humour-related generation tasks such as captioning remain under-explored. To address this gap, OxfordTVG-HIC offers approximately 2.9M image-text pairs with humour scores to train a generalizable humour captioning model. Contrary to existing captioning datasets, OxfordTVG-HIC features a wide range of emotional and semantic diversity resulting in out-of-context examples that are particularly conducive to generating humour. Moreover, OxfordTVG-HIC is curated devoid of offensive content. We also show how Oxf
&lt;/p&gt;</description></item><item><title>CausE&#26159;&#19968;&#20010;&#37319;&#29992;&#22240;&#26524;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21644;&#23884;&#20837;&#35299;&#32544;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#24178;&#39044;&#36827;&#34892;&#31283;&#23450;&#39044;&#27979;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#23436;&#25972;&#24615;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11610</link><description>&lt;p&gt;
CausE: &#26397;&#21521;&#22240;&#26524;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
CausE: Towards Causal Knowledge Graph Embedding. (arXiv:2307.11610v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11610
&lt;/p&gt;
&lt;p&gt;
CausE&#26159;&#19968;&#20010;&#37319;&#29992;&#22240;&#26524;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21644;&#23884;&#20837;&#35299;&#32544;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#24178;&#39044;&#36827;&#34892;&#31283;&#23450;&#39044;&#27979;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#23436;&#25972;&#24615;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#30340;&#37325;&#28857;&#26159;&#23558;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#34920;&#31034;&#20026;&#36830;&#32493;&#30340;&#21521;&#37327;&#31354;&#38388;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#32570;&#22833;&#30340;&#19977;&#20803;&#32452;&#20197;&#23454;&#29616;&#30693;&#35782;&#22270;&#35889;&#23436;&#25972;&#24615;&#65288;KGC&#65289;&#12290;&#28982;&#32780;&#65292;KGE&#27169;&#22411;&#36890;&#24120;&#21482;&#26159;&#31616;&#21333;&#22320;&#23398;&#20064;&#19977;&#20803;&#32452;&#25968;&#25454;&#30340;&#32467;&#26500;&#20851;&#32852;&#65292;&#24182;&#19988;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;KG&#20013;&#65292;&#23884;&#20837;&#21487;&#33021;&#20250;&#34987;&#24494;&#19981;&#36275;&#36947;&#30340;&#27169;&#24335;&#21644;&#22122;&#22768;&#38142;&#25509;&#25152;&#35823;&#23548;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22240;&#26524;&#24615;&#21644;&#23884;&#20837;&#35299;&#32544;&#26041;&#38754;&#24314;&#31435;&#20102;KGE&#30340;&#26032;&#27169;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;Causality-enhanced knowledge graph Embedding&#65288;CausE&#65289;&#26694;&#26550;&#12290;CausE&#20351;&#29992;&#22240;&#26524;&#24178;&#39044;&#26469;&#20272;&#35745;&#28151;&#26434;&#23884;&#20837;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#26469;&#36827;&#34892;&#31283;&#23450;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CausE&#21487;&#20197;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;KGC&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;https://github.com/zjukg/CausE&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) focuses on representing the entities and relations of a knowledge graph (KG) into the continuous vector spaces, which can be employed to predict the missing triples to achieve knowledge graph completion (KGC). However, KGE models often only briefly learn structural correlations of triple data and embeddings would be misled by the trivial patterns and noisy links in real-world KGs. To address this issue, we build the new paradigm of KGE in the context of causality and embedding disentanglement. We further propose a Causality-enhanced knowledge graph Embedding (CausE) framework. CausE employs causal intervention to estimate the causal effect of the confounder embeddings and design new training objectives to make stable predictions. Experimental results demonstrate that CausE could outperform the baseline models and achieve state-of-the-art KGC performance. We release our code in https://github.com/zjukg/CausE.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35821;&#38899;&#21040;&#25991;&#26412;&#30340;&#27169;&#24577;&#36716;&#25442;&#26041;&#27861;&#65292;&#22312;MELD&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#22522;&#20110;&#35821;&#38899;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#36825;&#34920;&#26126;&#20102;&#27169;&#24577;&#36716;&#25442;&#22312;&#26367;&#20195;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.11584</link><description>&lt;p&gt;
&#24515;&#24577;&#36716;&#21464;&#65306;&#36890;&#36807;&#35821;&#38899;&#21040;&#25991;&#26412;&#30340;&#27169;&#24577;&#36716;&#25442;&#25552;&#39640;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
A Change of Heart: Improving Speech Emotion Recognition through Speech-to-Text Modality Conversion. (arXiv:2307.11584v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35821;&#38899;&#21040;&#25991;&#26412;&#30340;&#27169;&#24577;&#36716;&#25442;&#26041;&#27861;&#65292;&#22312;MELD&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#22522;&#20110;&#35821;&#38899;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#36825;&#34920;&#26126;&#20102;&#27169;&#24577;&#36716;&#25442;&#22312;&#26367;&#20195;&#27169;&#24577;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27169;&#24577;&#36716;&#25442;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#25552;&#39640;&#22312;MELD&#25968;&#25454;&#38598;&#19978;&#30340;&#24773;&#24863;&#35782;&#21035;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#23454;&#39564;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#31532;&#19968;&#20010;&#23454;&#39564;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#21644;&#25991;&#26412;&#20998;&#31867;&#22120;&#23454;&#29616;&#30340;&#31216;&#20026;&#27169;&#24577;&#36716;&#25442;&#30340;&#26041;&#27861;&#65307;&#31532;&#20108;&#20010;&#23454;&#39564;&#20551;&#35774;&#26377;&#23436;&#32654;&#30340;ASR&#36755;&#20986;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#24577;&#36716;&#25442;&#23545;SER&#30340;&#24433;&#21709;&#65292;&#27492;&#26041;&#27861;&#31216;&#20026;&#27169;&#24577;&#36716;&#25442;++&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31532;&#19968;&#20010;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#32780;&#31532;&#20108;&#20010;&#26041;&#27861;&#22312;MELD&#25968;&#25454;&#38598;&#19978;&#30340;SER&#21152;&#26435;-F1&#65288;WF1&#65289;&#24471;&#20998;&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#35821;&#38899;&#30340;&#26041;&#27861;&#12290;&#36825;&#39033;&#30740;&#31350;&#31361;&#20986;&#20102;&#27169;&#24577;&#36716;&#25442;&#22312;&#21487;&#20197;&#20351;&#29992;&#26367;&#20195;&#27169;&#24577;&#36827;&#34892;&#30340;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech Emotion Recognition (SER) is a challenging task. In this paper, we introduce a modality conversion concept aimed at enhancing emotion recognition performance on the MELD dataset. We assess our approach through two experiments: first, a method named Modality-Conversion that employs automatic speech recognition (ASR) systems, followed by a text classifier; second, we assume perfect ASR output and investigate the impact of modality conversion on SER, this method is called Modality-Conversion++. Our findings indicate that the first method yields substantial results, while the second method outperforms state-of-the-art (SOTA) speech-based approaches in terms of SER weighted-F1 (WF1) score on the MELD dataset. This research highlights the potential of modality conversion for tasks that can be conducted in alternative modalities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#30693;&#35782;&#30340;&#35270;&#35273;&#23545;&#40784;&#26032;&#22522;&#20934;SK-VG&#65292;&#36890;&#36807;&#36843;&#20351;&#27169;&#22411;&#20855;&#22791;&#23545;&#38271;&#31687;&#22330;&#26223;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#36827;&#34892;&#32454;&#31890;&#24230;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2307.11558</link><description>&lt;p&gt;
&#36890;&#36807;&#22330;&#26223;&#30693;&#35782;&#25512;&#36827;&#35270;&#35273;&#23545;&#40784;&#65306;&#22522;&#20934;&#19982;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Advancing Visual Grounding with Scene Knowledge: Benchmark and Method. (arXiv:2307.11558v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#26223;&#30693;&#35782;&#30340;&#35270;&#35273;&#23545;&#40784;&#26032;&#22522;&#20934;SK-VG&#65292;&#36890;&#36807;&#36843;&#20351;&#27169;&#22411;&#20855;&#22791;&#23545;&#38271;&#31687;&#22330;&#26223;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#36827;&#34892;&#32454;&#31890;&#24230;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#23545;&#40784;&#26088;&#22312;&#24314;&#31435;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#23545;&#40784;&#12290;&#22312;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#23427;&#21487;&#20197;&#25104;&#20026;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#30340;&#22522;&#20934;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#29702;&#35299;&#20197;&#21450;&#23427;&#20204;&#22312;&#32852;&#21512;&#31354;&#38388;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35270;&#35273;&#23545;&#40784;&#25968;&#25454;&#38598;&#26159;&#20351;&#29992;&#31616;&#21333;&#30340;&#25551;&#36848;&#25991;&#26412;&#26500;&#24314;&#30340;&#65292;&#36825;&#20123;&#25991;&#26412;&#19981;&#38656;&#35201;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#36827;&#34892;&#36275;&#22815;&#30340;&#25512;&#29702;&#12290;&#22312;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#20013;~\cite{luo2022goes}&#65292;&#24050;&#32463;&#35777;&#26126;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20110;LSTM&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#20027;&#27969;&#35270;&#35273;&#23545;&#40784;&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#39044;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;\textbf{S}cene \textbf{K}nowledge-guided \textbf{V}isual \textbf{G}rounding (SK-VG)&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#22270;&#20687;&#20869;&#23481;&#21644;&#24341;&#29992;&#34920;&#36798;&#19981;&#36275;&#20197;&#23545;&#40784;&#30446;&#26631;&#23545;&#35937;&#65292;&#36843;&#20351;&#27169;&#22411;&#23545;&#38271;&#31687;&#22330;&#26223;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#25509;&#21463;t
&lt;/p&gt;
&lt;p&gt;
Visual grounding (VG) aims to establish fine-grained alignment between vision and language. Ideally, it can be a testbed for vision-and-language models to evaluate their understanding of the images and texts and their reasoning abilities over their joint space. However, most existing VG datasets are constructed using simple description texts, which do not require sufficient reasoning over the images and texts. This has been demonstrated in a recent study~\cite{luo2022goes}, where a simple LSTM-based text encoder without pretraining can achieve state-of-the-art performance on mainstream VG datasets. Therefore, in this paper, we propose a novel benchmark of \underline{S}cene \underline{K}nowledge-guided \underline{V}isual \underline{G}rounding (SK-VG), where the image content and referring expressions are not sufficient to ground the target objects, forcing the models to have a reasoning ability on the long-form scene knowledge. To perform this task, we propose two approaches to accept t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#24341;&#29992;&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#36827;&#34892;&#20102;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Bridger &#30340;&#36866;&#37197;&#22120;&#65292;&#22312;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#36328;&#27169;&#24577;&#20449;&#24687;&#20132;&#25442;&#21644;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#27880;&#20837;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#20998;&#21106;&#35299;&#30721;&#22120;&#12290;&#36890;&#36807;&#20165;&#36827;&#34892;1.61&#65285; &#33267; 3.38&#65285; &#30340;&#20027;&#24178;&#21442;&#25968;&#26356;&#26032;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#21487;&#27604;&#25110;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11545</link><description>&lt;p&gt;
&#26725;&#25509;&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#65306;&#21442;&#25968;&#39640;&#25928;&#30340;&#24341;&#29992;&#22270;&#20687;&#20998;&#21106;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation. (arXiv:2307.11545v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#24341;&#29992;&#22270;&#20687;&#20998;&#21106;&#38382;&#39064;&#36827;&#34892;&#20102;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Bridger &#30340;&#36866;&#37197;&#22120;&#65292;&#22312;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#36328;&#27169;&#24577;&#20449;&#24687;&#20132;&#25442;&#21644;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#27880;&#20837;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22270;&#20687;&#20998;&#21106;&#35299;&#30721;&#22120;&#12290;&#36890;&#36807;&#20165;&#36827;&#34892;1.61&#65285; &#33267; 3.38&#65285; &#30340;&#20027;&#24178;&#21442;&#25968;&#26356;&#26032;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#21487;&#27604;&#25110;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#35843;&#25972; (PET) &#22312;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#30340;&#21516;&#26102;&#20445;&#25345;&#24615;&#33021;&#21644;&#25552;&#20379;&#26356;&#22909;&#30340;&#30828;&#20214;&#36164;&#28304;&#33410;&#30465;&#26041;&#38754;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20294;&#24456;&#23569;&#30740;&#31350;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#21644;&#27169;&#24577;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24341;&#29992;&#22270;&#20687;&#20998;&#21106;&#19978;&#30340;&#39640;&#25928;&#35843;&#25972;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Bridger &#30340;&#26032;&#22411;&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#20419;&#36827;&#36328;&#27169;&#24577;&#20449;&#24687;&#20132;&#25442;&#24182;&#23558;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#27880;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20026;&#22270;&#20687;&#20998;&#21106;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#22522;&#20934;&#27979;&#35797;&#20013;&#36890;&#36807;&#20165;&#36827;&#34892;1.61&#65285; &#33267; 3.38&#65285; &#30340;&#20027;&#24178;&#21442;&#25968;&#26356;&#26032;&#65292;&#23454;&#29616;&#20102;&#21487;&#27604;&#25110;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312; \url{https://github.com/kkakkkka/ETRIS} &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter Efficient Tuning (PET) has gained attention for reducing the number of parameters while maintaining performance and providing better hardware resource savings, but few studies investigate dense prediction tasks and interaction between modalities. In this paper, we do an investigation of efficient tuning problems on referring image segmentation. We propose a novel adapter called Bridger to facilitate cross-modal information exchange and inject task-specific information into the pre-trained model. We also design a lightweight decoder for image segmentation. Our approach achieves comparable or superior performance with only 1.61\% to 3.38\% backbone parameter updates, evaluated on challenging benchmarks. The code is available at \url{https://github.com/kkakkkka/ETRIS}.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#12289;&#38899;&#39057;&#29305;&#24449;&#20540;&#21644;&#25991;&#26412;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#26469;&#26816;&#27979;&#35270;&#39057;&#20869;&#23481;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;</title><link>http://arxiv.org/abs/2307.11519</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Hate Speech Detection using Machine Learning. (arXiv:2307.11519v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#12289;&#38899;&#39057;&#29305;&#24449;&#20540;&#21644;&#25991;&#26412;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#26469;&#26816;&#27979;&#35270;&#39057;&#20869;&#23481;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#29992;&#25143;&#21644;&#23186;&#20307;&#20869;&#23481;&#30340;&#19981;&#26029;&#22686;&#38271;&#65292;&#36861;&#36394;&#38899;&#39057;&#21644;&#35270;&#39057;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#21464;&#24471;&#38750;&#24120;&#22256;&#38590;&#12290;&#23558;&#35270;&#39057;&#25110;&#38899;&#39057;&#36716;&#25442;&#20026;&#25991;&#26412;&#24182;&#19981;&#33021;&#20934;&#30830;&#26816;&#27979;&#21040;&#20167;&#24680;&#35328;&#35770;&#65292;&#22240;&#20026;&#20154;&#20204;&#26377;&#26102;&#20250;&#23558;&#20167;&#24680;&#35789;&#27719;&#20316;&#20026;&#24189;&#40664;&#25110;&#24841;&#24555;&#30340;&#24847;&#21619;&#20351;&#29992;&#65292;&#24182;&#22312;&#35270;&#39057;&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;&#22768;&#35843;&#25110;&#23637;&#31034;&#19981;&#21516;&#30340;&#21160;&#20316;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#22823;&#22810;&#26159;&#22522;&#20110;&#21333;&#19968;&#27169;&#24577;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#32508;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#22270;&#20687;&#29305;&#24449;&#12289;&#38899;&#39057;&#20013;&#30340;&#29305;&#24449;&#20540;&#12289;&#25991;&#26412;&#21644;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#26816;&#27979;&#35270;&#39057;&#20869;&#23481;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the continuous growth of internet users and media content, it is very hard to track down hateful speech in audio and video. Converting video or audio into text does not detect hate speech accurately as human sometimes uses hateful words as humorous or pleasant in sense and also uses different voice tones or show different action in the video. The state-ofthe-art hate speech detection models were mostly developed on a single modality. In this research, a combined approach of multimodal system has been proposed to detect hate speech from video contents by extracting feature images, feature values extracted from the audio, text and used machine learning and Natural language processing.
&lt;/p&gt;</description></item><item><title>IndigoVX&#26159;&#19968;&#31181;&#23558;&#20154;&#31867;&#26234;&#24935;&#19982;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#30340;&#26368;&#20248;&#20915;&#31574;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#21453;&#39304;&#24490;&#29615;&#65292;&#21033;&#29992;&#20154;&#31867;&#19987;&#23478;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#39537;&#21160;&#27934;&#35265;&#65292;&#21046;&#23450;&#21644;&#20248;&#21270;&#26397;&#30528;&#26126;&#30830;&#23450;&#20041;&#30340;&#30446;&#26631;&#30340;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#23450;&#37327;&#21270;&#30340;&#19977;&#20998;&#25968;&#27169;&#24335;&#36827;&#34892;&#35780;&#20272;&#21644;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.11516</link><description>&lt;p&gt;
IndigoVX: &#20154;&#31867;&#26234;&#24935;&#19982;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#30340;&#26368;&#20248;&#20915;&#31574;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
IndigoVX: Where Human Intelligence Meets AI for Optimal Decision Making. (arXiv:2307.11516v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11516
&lt;/p&gt;
&lt;p&gt;
IndigoVX&#26159;&#19968;&#31181;&#23558;&#20154;&#31867;&#26234;&#24935;&#19982;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#30340;&#26368;&#20248;&#20915;&#31574;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#21453;&#39304;&#24490;&#29615;&#65292;&#21033;&#29992;&#20154;&#31867;&#19987;&#23478;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#39537;&#21160;&#27934;&#35265;&#65292;&#21046;&#23450;&#21644;&#20248;&#21270;&#26397;&#30528;&#26126;&#30830;&#23450;&#20041;&#30340;&#30446;&#26631;&#30340;&#31574;&#30053;&#65292;&#24182;&#20351;&#29992;&#23450;&#37327;&#21270;&#30340;&#19977;&#20998;&#25968;&#27169;&#24335;&#36827;&#34892;&#35780;&#20272;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#26234;&#24935;&#19982;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#26368;&#20248;&#30446;&#26631;&#35299;&#20915;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;Indigo&#65292;&#26159;&#25351;&#36890;&#36807;&#36845;&#20195;&#30446;&#26631;&#23548;&#21521;&#20248;&#21270;&#36827;&#34892;&#30693;&#24773;&#25968;&#20540;&#20915;&#31574;&#12290;&#24403;&#19982;&#20154;&#31867;&#21327;&#20316;&#32773;&#32467;&#21512;&#26102;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#32852;&#21512;&#31995;&#32479;&#21629;&#21517;&#20026;IndigoVX&#65292;&#21363;&#34394;&#25311;&#19987;&#23478;&#12290;&#35813;&#31995;&#32479;&#27010;&#24565;&#31616;&#21333;&#12290;&#25105;&#20204;&#35774;&#24819;&#35813;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#28216;&#25103;&#25110;&#21830;&#19994;&#31574;&#30053;&#65292;&#20154;&#31867;&#25552;&#20379;&#25112;&#30053;&#32972;&#26223;&#65292;&#32780;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#26368;&#20339;&#30340;&#25968;&#25454;&#39537;&#21160;&#31227;&#21160;&#12290;Indigo&#36890;&#36807;&#36845;&#20195;&#21453;&#39304;&#24490;&#29615;&#36816;&#20316;&#65292;&#21033;&#29992;&#20154;&#31867;&#19987;&#23478;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#25968;&#25454;&#39537;&#21160;&#27934;&#35265;&#65292;&#21046;&#23450;&#21644;&#20248;&#21270;&#26397;&#30528;&#26126;&#30830;&#23450;&#20041;&#30340;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;&#20351;&#29992;&#23450;&#37327;&#21270;&#30340;&#19977;&#20998;&#25968;&#27169;&#24335;&#65292;&#36825;&#31181;&#28151;&#21512;&#21270;&#20801;&#35768;&#32852;&#21512;&#22242;&#38431;&#35780;&#20272;&#31574;&#30053;&#24182;&#25913;&#36827;&#35745;&#21010;&#65292;&#21516;&#26102;&#23454;&#26102;&#36866;&#24212;&#25361;&#25112;&#21644;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper defines a new approach for augmenting human intelligence with AI for optimal goal solving. Our proposed AI, Indigo, is an acronym for Informed Numerical Decision-making through Iterative Goal-Oriented optimization. When combined with a human collaborator, we term the joint system IndigoVX, for Virtual eXpert. The system is conceptually simple. We envisage this method being applied to games or business strategies, with the human providing strategic context and the AI offering optimal, data-driven moves. Indigo operates through an iterative feedback loop, harnessing the human expert's contextual knowledge and the AI's data-driven insights to craft and refine strategies towards a well-defined goal. Using a quantified three-score schema, this hybridization allows the combined team to evaluate strategies and refine their plan, while adapting to challenges and changes in real-time.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33521;&#22303;&#25991;&#23398;&#32763;&#35793;&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#37319;&#29992;&#32763;&#35793;&#23478;&#30340;&#39118;&#26684;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#33719;&#24471;&#20102;&#39640;&#24230;&#36824;&#21407;&#20154;&#31867;&#35793;&#32773;&#39118;&#26684;&#30340;&#26426;&#22120;&#32763;&#35793;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11457</link><description>&lt;p&gt;
&#23558;&#20154;&#31867;&#32763;&#35793;&#39118;&#26684;&#34701;&#20837;&#33521;&#22303;&#25991;&#23398;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Incorporating Human Translator Style into English-Turkish Literary Machine Translation. (arXiv:2307.11457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33521;&#22303;&#25991;&#23398;&#32763;&#35793;&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#37319;&#29992;&#32763;&#35793;&#23478;&#30340;&#39118;&#26684;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#33719;&#24471;&#20102;&#39640;&#24230;&#36824;&#21407;&#20154;&#31867;&#35793;&#32773;&#39118;&#26684;&#30340;&#26426;&#22120;&#32763;&#35793;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#19968;&#33324;&#39046;&#22495;&#65292;&#20294;&#23384;&#22312;&#23558;&#36825;&#20123;&#31995;&#32479;&#36866;&#24212;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#25991;&#23398;&#32763;&#35793;&#65289;&#30340;&#36235;&#21183;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#33521;&#22303;&#25991;&#23398;&#32763;&#35793;&#65292;&#24182;&#24320;&#21457;&#32771;&#34385;&#32763;&#35793;&#23478;&#39118;&#26684;&#29305;&#24449;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#24037;&#23545;&#40784;&#30340;&#29305;&#23450;&#35793;&#32773;&#20316;&#21697;&#26469;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#25163;&#21160;&#23545;&#40784;&#12289;&#33258;&#21160;&#23545;&#40784;&#12289;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21644;&#35821;&#26009;&#24211;&#22823;&#23567;&#23545;&#32763;&#35793;&#25928;&#26524;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39118;&#26684;&#29305;&#24449;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#35793;&#32773;&#22312;&#36755;&#20986;&#32763;&#35793;&#20013;&#30340;&#39118;&#26684;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#27169;&#22411;&#36866;&#24212;&#21040;&#35793;&#32773;&#30340;&#39118;&#26684;&#65292;&#21487;&#20197;&#39640;&#24230;&#37325;&#29616;&#20154;&#31867;&#35793;&#32773;&#30340;&#39118;&#26684;&#22312;&#30446;&#26631;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although machine translation systems are mostly designed to serve in the general domain, there is a growing tendency to adapt these systems to other domains like literary translation. In this paper, we focus on English-Turkish literary translation and develop machine translation models that take into account the stylistic features of translators. We fine-tune a pre-trained machine translation model by the manually-aligned works of a particular translator. We make a detailed analysis of the effects of manual and automatic alignments, data augmentation methods, and corpus size on the translations. We propose an approach based on stylistic features to evaluate the style of a translator in the output translations. We show that the human translator style can be highly recreated in the target machine translations by adapting the models to the style of the translator.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33258;&#21457;&#35821;&#38899;&#20013;&#20027;&#39064;&#35782;&#21035;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#27604;&#36739;&#32431;&#38899;&#39057;&#21644;&#28151;&#21512;&#22810;&#27169;&#24577;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#32431;&#38899;&#39057;&#35299;&#20915;&#26041;&#26696;&#26159;&#21487;&#34892;&#30340;&#36873;&#39033;&#65292;&#32780;&#28151;&#21512;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#33719;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11450</link><description>&lt;p&gt;
&#33258;&#21457;&#35821;&#38899;&#30340;&#20027;&#39064;&#35782;&#21035;&#65306;&#29992;&#23884;&#20837;&#24335;&#35821;&#35328;&#20449;&#24687;&#20016;&#23500;&#38899;&#39057;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Topic Identification For Spontaneous Speech: Enriching Audio Features With Embedded Linguistic Information. (arXiv:2307.11450v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33258;&#21457;&#35821;&#38899;&#20013;&#20027;&#39064;&#35782;&#21035;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#27604;&#36739;&#32431;&#38899;&#39057;&#21644;&#28151;&#21512;&#22810;&#27169;&#24577;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#32431;&#38899;&#39057;&#35299;&#20915;&#26041;&#26696;&#26159;&#21487;&#34892;&#30340;&#36873;&#39033;&#65292;&#32780;&#28151;&#21512;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#33719;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#38899;&#39057;&#20027;&#39064;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65288;ASR&#65289;&#29983;&#25104;&#29992;&#20316;&#25991;&#26412;&#27169;&#22411;&#36755;&#20837;&#30340;&#36716;&#24405;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#39640;&#36164;&#28304;&#22330;&#26223;&#19979;&#25928;&#26524;&#33391;&#22909;&#65292;&#20854;&#20013;&#26377;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#31649;&#36947;&#30340;&#20004;&#20010;&#32452;&#20214;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;ASR&#31995;&#32479;&#21487;&#29992;&#65292;&#20063;&#20250;&#20135;&#29983;&#20302;&#36136;&#37327;&#30340;&#36716;&#24405;&#65292;&#23548;&#33268;&#25991;&#26412;&#20998;&#31867;&#22120;&#25928;&#26524;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#21253;&#21547;&#20572;&#39039;&#30340;&#33258;&#21457;&#35821;&#38899;&#36827;&#19968;&#27493;&#38477;&#20302;&#20102;ASR&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#20165;&#20351;&#29992;&#38899;&#39057;&#21644;&#28151;&#21512;&#22810;&#27169;&#24577;&#25216;&#26415;&#26469;&#25506;&#35752;&#26367;&#20195;&#26631;&#20934;&#20165;&#25991;&#26412;&#35299;&#20915;&#26041;&#26696;&#30340;&#26041;&#27861;&#12290;&#22312;&#33258;&#21457;&#30340;&#33452;&#20848;&#35821;&#38899;&#19978;&#35780;&#20272;&#30340;&#27169;&#22411;&#34920;&#26126;&#65292;&#24403;ASR&#32452;&#20214;&#19981;&#21487;&#29992;&#26102;&#65292;&#32431;&#38899;&#39057;&#35299;&#20915;&#26041;&#26696;&#26159;&#19968;&#20010;&#21487;&#34892;&#30340;&#36873;&#39033;&#65292;&#32780;&#28151;&#21512;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#33719;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional topic identification solutions from audio rely on an automatic speech recognition system (ASR) to produce transcripts used as input to a text-based model. These approaches work well in high-resource scenarios, where there are sufficient data to train both components of the pipeline. However, in low-resource situations, the ASR system, even if available, produces low-quality transcripts, leading to a bad text-based classifier. Moreover, spontaneous speech containing hesitations can further degrade the performance of the ASR model. In this paper, we investigate alternatives to the standard text-only solutions by comparing audio-only and hybrid techniques of jointly utilising text and audio features. The models evaluated on spontaneous Finnish speech demonstrate that purely audio-based solutions are a viable option when ASR components are not available, while the hybrid multi-modal solutions achieve the best results.
&lt;/p&gt;</description></item><item><title>MeetEval&#26159;&#19968;&#20010;&#35745;&#31639;&#20250;&#35758;&#36716;&#24405;&#31995;&#32479;&#23383;&#38169;&#35823;&#29575;&#30340;&#24037;&#20855;&#21253;&#65292;&#23427;&#36890;&#36807;&#26102;&#38388;&#32422;&#26463;&#26469;&#25552;&#39640;&#21305;&#37197;&#36136;&#37327;&#24182;&#21152;&#36895;&#21305;&#37197;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.11394</link><description>&lt;p&gt;
MeetEval: &#19968;&#31181;&#29992;&#20110;&#20250;&#35758;&#36716;&#24405;&#31995;&#32479;&#23383;&#38169;&#35823;&#29575;&#35745;&#31639;&#30340;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
MeetEval: A Toolkit for Computation of Word Error Rates for Meeting Transcription Systems. (arXiv:2307.11394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11394
&lt;/p&gt;
&lt;p&gt;
MeetEval&#26159;&#19968;&#20010;&#35745;&#31639;&#20250;&#35758;&#36716;&#24405;&#31995;&#32479;&#23383;&#38169;&#35823;&#29575;&#30340;&#24037;&#20855;&#21253;&#65292;&#23427;&#36890;&#36807;&#26102;&#38388;&#32422;&#26463;&#26469;&#25552;&#39640;&#21305;&#37197;&#36136;&#37327;&#24182;&#21152;&#36895;&#21305;&#37197;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MeetEval&#26159;&#19968;&#20010;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#20250;&#35758;&#36716;&#24405;&#31995;&#32479;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#30028;&#38754;&#65292;&#29992;&#20110;&#35745;&#31639;&#24120;&#29992;&#30340;&#23383;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#21253;&#25324;cpWER&#12289;ORC WER&#21644;MIMO WER&#31561;&#20854;&#20182;WER&#23450;&#20041;&#12290;&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#32422;&#26463;&#25193;&#23637;&#20102;cpWER&#30340;&#35745;&#31639;&#65292;&#20197;&#30830;&#20445;&#21482;&#26377;&#22312;&#26102;&#38388;&#23545;&#40784;&#21512;&#29702;&#30340;&#24773;&#20917;&#19979;&#25165;&#23558;&#21333;&#35789;&#35782;&#21035;&#20026;&#27491;&#30830;&#12290;&#36825;&#26679;&#21487;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20551;&#35774;&#23383;&#31526;&#20018;&#19982;&#21442;&#32771;&#23383;&#31526;&#20018;&#65292;&#26356;&#25509;&#36817;&#23454;&#38469;&#30340;&#36716;&#24405;&#36136;&#37327;&#65292;&#24182;&#19988;&#22914;&#26524;&#31995;&#32479;&#25552;&#20379;&#20102;&#19981;&#20934;&#30830;&#30340;&#26102;&#38388;&#26631;&#27880;&#65292;&#23558;&#23545;&#20854;&#36827;&#34892;&#24809;&#32602;&#12290;&#30001;&#20110;&#36890;&#24120;&#27809;&#26377;&#21333;&#35789;&#32423;&#21035;&#30340;&#26102;&#38388;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20174;&#29255;&#27573;&#32423;&#21035;&#26102;&#38388;&#65288;&#20363;&#22914;&#19968;&#20010;&#21477;&#23376;&#65289;&#36817;&#20284;&#21040;&#30830;&#20999;&#30340;&#21333;&#35789;&#32423;&#26102;&#38388;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36817;&#20284;&#26041;&#27861;&#19982;&#20855;&#26377;&#30830;&#20999;&#21333;&#35789;&#32423;&#21035;&#27880;&#37322;&#30340;&#21305;&#37197;&#23548;&#33268;&#31867;&#20284;&#30340;WER&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26102;&#38388;&#32422;&#26463;&#36824;&#23548;&#33268;&#21305;&#37197;&#31639;&#27861;&#30340;&#21152;&#36895;&#65292;&#36825;&#36229;&#36807;&#20102;&#20542;&#21521;&#25340;&#20945;&#30340;&#26102;&#38388;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
MeetEval is an open-source toolkit to evaluate all kinds of meeting transcription systems. It provides a unified interface for the computation of commonly used Word Error Rates (WERs), specifically cpWER, ORC WER and MIMO WER along other WER definitions. We extend the cpWER computation by a temporal constraint to ensure that only words are identified as correct when the temporal alignment is plausible. This leads to a better quality of the matching of the hypothesis string to the reference string that more closely resembles the actual transcription quality, and a system is penalized if it provides poor time annotations. Since word-level timing information is often not available, we present a way to approximate exact word-level timings from segment-level timings (e.g., a sentence) and show that the approximation leads to a similar WER as a matching with exact word-level annotations. At the same time, the time constraint leads to a speedup of the matching algorithm, which outweighs the a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;ChatGPT&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#37327;&#26041;&#27861;&#8220;&#27874;&#20848;&#27604;&#29575;&#8221;&#65292;&#29992;&#20110;&#26816;&#27979;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#28041;&#21450;&#31243;&#24230;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;HPPT&#65292;&#29992;&#20110;&#26500;&#24314;&#26356;&#31283;&#20581;&#30340;&#26816;&#27979;&#22120;&#12290;</title><link>http://arxiv.org/abs/2307.11380</link><description>&lt;p&gt;
&#32842;&#22825;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#26159;&#21542;&#28041;&#21450;ChatGPT&#65311;&#36890;&#36807;&#27979;&#37327;&#8220;&#27874;&#20848;&#27604;&#29575;&#8221;&#26469;&#26816;&#27979;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT Involved in Texts? Measure the Polish Ratio to Detect ChatGPT-Generated Text. (arXiv:2307.11380v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;ChatGPT&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#20316;&#29992;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27979;&#37327;&#26041;&#27861;&#8220;&#27874;&#20848;&#27604;&#29575;&#8221;&#65292;&#29992;&#20110;&#26816;&#27979;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#28041;&#21450;&#31243;&#24230;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;HPPT&#65292;&#29992;&#20110;&#26500;&#24314;&#26356;&#31283;&#20581;&#30340;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#33021;&#21147;&#65292;&#36825;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#26816;&#27979;&#22120;&#20197;&#20943;&#36731;&#28508;&#22312;&#39118;&#38505;&#65292;&#21253;&#25324;&#38169;&#35823;&#20449;&#24687;&#12289;&#32593;&#32476;&#38035;&#40060;&#21644;&#23398;&#26415;&#19981;&#35802;&#23454;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38024;&#23545;&#21306;&#20998;&#32431;&#31929;&#30001;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#24037;&#25776;&#20889;&#30340;&#25991;&#26412;&#30340;&#26816;&#27979;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21306;&#20998;&#36890;&#36807;&#20154;&#26426;&#21327;&#20316;&#29983;&#25104;&#30340;&#25991;&#26412;&#65288;&#20363;&#22914;ChatGPT&#28070;&#33394;&#30340;&#25991;&#26412;&#65289;&#19978;&#22833;&#25928;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;HPPT&#65288;ChatGPT&#28070;&#33394;&#30340;&#23398;&#26415;&#25688;&#35201;&#65289;&#65292;&#20197;&#26500;&#24314;&#26356;&#24378;&#22823;&#30340;&#26816;&#27979;&#22120;&#12290;&#35813;&#25968;&#25454;&#38598;&#19982;&#29616;&#26377;&#35821;&#26009;&#24211;&#19981;&#21516;&#65292;&#23427;&#21253;&#25324;&#20154;&#24037;&#25776;&#20889;&#30340;&#25991;&#26412;&#21644;ChatGPT&#28070;&#33394;&#30340;&#25688;&#35201;&#23545;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#27874;&#20848;&#27604;&#29575;&#8221;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#34913;&#37327;ChatGPT&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#21442;&#19982;&#31243;&#24230;&#30340;&#21019;&#26032;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable capabilities of large-scale language models, such as ChatGPT, in text generation have incited awe and spurred researchers to devise detectors to mitigate potential risks, including misinformation, phishing, and academic dishonesty. Despite this, most previous studies, including HC3, have been predominantly geared towards creating detectors that differentiate between purely ChatGPT-generated texts and human-authored texts. This approach, however, fails to work on discerning texts generated through human-machine collaboration, such as ChatGPT-polished texts. Addressing this gap, we introduce a novel dataset termed HPPT (ChatGPT-polished academic abstracts), facilitating the construction of more robust detectors. It diverges from extant corpora by comprising pairs of human-written and ChatGPT-polished abstracts instead of purely ChatGPT-generated texts. Additionally, we propose the "Polish Ratio" method, an innovative measure of ChatGPT's involvement in text generation base
&lt;/p&gt;</description></item><item><title>CohortGPT&#26159;&#19968;&#31181;&#22686;&#24378;&#22411;GPT&#65292;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#30740;&#31350;&#20013;&#30340;&#21442;&#19982;&#32773;&#25307;&#21215;&#20219;&#21153;&#12290;&#27492;&#30740;&#31350;&#21457;&#29616;&#20256;&#32479;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#19968;&#33324;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#23427;&#20204;&#24573;&#30053;&#20102;&#35821;&#35328;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.11346</link><description>&lt;p&gt;
CohortGPT&#65306;&#19968;&#31181;&#29992;&#20110;&#20020;&#24202;&#30740;&#31350;&#21442;&#19982;&#32773;&#25307;&#21215;&#30340;&#22686;&#24378;&#22411;GPT
&lt;/p&gt;
&lt;p&gt;
CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study. (arXiv:2307.11346v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11346
&lt;/p&gt;
&lt;p&gt;
CohortGPT&#26159;&#19968;&#31181;&#22686;&#24378;&#22411;GPT&#65292;&#29992;&#20110;&#35299;&#20915;&#20020;&#24202;&#30740;&#31350;&#20013;&#30340;&#21442;&#19982;&#32773;&#25307;&#21215;&#20219;&#21153;&#12290;&#27492;&#30740;&#31350;&#21457;&#29616;&#20256;&#32479;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#19968;&#33324;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#23427;&#20204;&#24573;&#30053;&#20102;&#35821;&#35328;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#38750;&#32467;&#26500;&#21270;&#21307;&#30103;&#25991;&#26412;&#65288;&#22914;&#20020;&#24202;&#35760;&#24405;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;&#65289;&#36827;&#34892;&#21442;&#19982;&#32773;&#25307;&#21215;&#26159;&#20020;&#24202;&#30740;&#31350;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#27979;&#35797;&#23427;&#20204;&#22312;&#35299;&#20915;&#20020;&#24202;&#30740;&#31350;&#20013;&#30340;&#21442;&#19982;&#32773;&#25307;&#21215;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#38382;&#39064;&#35774;&#32622;&#65288;&#22914;&#21307;&#30103;&#25991;&#26412;&#20998;&#31867;&#65289;&#26102;&#65292;LLMs&#30340;&#24615;&#33021;&#19968;&#33324;&#12290;&#21487;&#33021;&#30340;&#35299;&#37322;&#26159;LLMs&#20165;&#20351;&#29992;&#21307;&#30103;&#25991;&#26412;&#65292;&#24573;&#30053;&#20102;&#35821;&#35328;&#20013;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Participant recruitment based on unstructured medical texts such as clinical notes and radiology reports has been a challenging yet important task for the cohort establishment in clinical research. Recently, Large Language Models (LLMs) such as ChatGPT have achieved tremendous success in various downstream tasks thanks to their promising performance in language understanding, inference, and generation. It is then natural to test their feasibility in solving the cohort recruitment task, which involves the classification of a given paragraph of medical text into disease label(s). However, when applied to knowledge-intensive problem settings such as medical text classification, where the LLMs are expected to understand the decision made by human experts and accurately identify the implied disease labels, the LLMs show a mediocre performance. A possible explanation is that, by only using the medical text, the LLMs neglect to use the rich context of additional information that languages aff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#26631;&#31614;&#34701;&#21512;&#25991;&#26412;&#23884;&#20837;&#21644;&#20248;&#21270;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;DEFTri&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;&#30005;&#23376;&#21830;&#21153;&#20013;&#30340;&#20135;&#21697;&#32570;&#38519;&#20998;&#31867;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23569;&#26679;&#26412;&#35757;&#32451;&#21644;&#24369;&#30417;&#30563;&#23398;&#20064;&#65292;&#24341;&#20837;&#20102;&#27779;&#23572;&#29595;&#30340;&#19987;&#26377;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.11344</link><description>&lt;p&gt;
DEFTri: &#30005;&#23376;&#21830;&#21153;&#20013;&#20135;&#21697;&#32570;&#38519;&#20998;&#31867;&#30340;&#23569;&#26679;&#26412;&#26631;&#31614;&#34701;&#21512;&#19978;&#19979;&#25991;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DEFTri: A Few-Shot Label Fused Contextual Representation Learning For Product Defect Triage in e-Commerce. (arXiv:2307.11344v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#26631;&#31614;&#34701;&#21512;&#25991;&#26412;&#23884;&#20837;&#21644;&#20248;&#21270;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;DEFTri&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;&#30005;&#23376;&#21830;&#21153;&#20013;&#30340;&#20135;&#21697;&#32570;&#38519;&#20998;&#31867;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23569;&#26679;&#26412;&#35757;&#32451;&#21644;&#24369;&#30417;&#30563;&#23398;&#20064;&#65292;&#24341;&#20837;&#20102;&#27779;&#23572;&#29595;&#30340;&#19987;&#26377;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#25935;&#25463;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#30005;&#23376;&#21830;&#21153;&#20013;&#65292;&#32570;&#38519;&#20998;&#31867;&#26159;&#19968;&#20010;&#26102;&#38388;&#25935;&#24863;&#19988;&#20851;&#38190;&#30340;&#36807;&#31243;&#12290;&#35813;&#39046;&#22495;&#30001;&#20110;&#20154;&#20026;&#21644;&#27969;&#31243;&#20381;&#36182;&#24615;&#32780;&#24341;&#36215;&#30340;&#20302;&#25928;&#24615;&#65292;&#24050;&#32463;&#28608;&#21457;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#20934;&#30830;&#22320;&#23558;&#32570;&#38519;&#20998;&#37197;&#32473;&#31526;&#21512;&#26465;&#20214;&#30340;&#22242;&#38431;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;DEFTri&#65292;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#20808;&#36827;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#22312;&#26631;&#31614;&#34701;&#21512;&#30340;&#25991;&#26412;&#23884;&#20837;&#20013;&#25913;&#36827;&#20102;&#20154;&#24037;&#29983;&#25104;&#30340;&#20135;&#21697;&#32570;&#38519;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#32570;&#38519;&#20998;&#31867;&#20219;&#21153;&#65292;&#25105;&#20204;&#36824;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#20351;&#29992;&#20102;&#27779;&#23572;&#29595;&#30340;&#19987;&#26377;&#20135;&#21697;&#32570;&#38519;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#20102;&#24369;&#30417;&#30563;&#21644;&#23545;&#25239;&#24615;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Defect Triage is a time-sensitive and critical process in a large-scale agile software development lifecycle for e-commerce. Inefficiencies arising from human and process dependencies in this domain have motivated research in automated approaches using machine learning to accurately assign defects to qualified teams. This work proposes a novel framework for automated defect triage (DEFTri) using fine-tuned state-of-the-art pre-trained BERT on labels fused text embeddings to improve contextual representations from human-generated product defects. For our multi-label text classification defect triage task, we also introduce a Walmart proprietary dataset of product defects using weak supervision and adversarial learning, in a few-shot setting.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#20219;&#21153;&#35299;&#20915;&#22120;&#21644;&#33258;&#26657;&#20934;&#22120;&#65292;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#26679;&#26412;&#12289;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#20854;&#20182;&#23454;&#38469;&#25361;&#25112;&#19979;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2307.11316</link><description>&lt;p&gt;
&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#20219;&#21153;&#35299;&#20915;&#22120;&#21644;&#33258;&#26657;&#20934;&#22120;
&lt;/p&gt;
&lt;p&gt;
Making Pre-trained Language Models both Task-solvers and Self-calibrators. (arXiv:2307.11316v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11316
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#20219;&#21153;&#35299;&#20915;&#22120;&#21644;&#33258;&#26657;&#20934;&#22120;&#65292;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#26679;&#26412;&#12289;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#20854;&#20182;&#23454;&#38469;&#25361;&#25112;&#19979;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#21508;&#31181;&#23454;&#38469;&#31995;&#32479;&#20013;&#20316;&#20026;&#39592;&#24178;&#12290;&#23545;&#20110;&#39640;&#39118;&#38505;&#24212;&#29992;&#65292;&#21512;&#29702;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#23545;&#20110;&#39044;&#27979;&#21516;&#26679;&#37325;&#35201;&#12290;&#34429;&#28982;PLMs&#30340;&#24120;&#35268;&#32622;&#20449;&#24230;&#20998;&#25968;&#24050;&#32463;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;&#38169;&#35823;&#39044;&#27979;&#20013;&#22987;&#32456;&#21464;&#24471;&#36807;&#20110;&#33258;&#20449;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#24341;&#20837;&#39069;&#22806;&#30340;&#26657;&#20934;&#20219;&#21153;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#33719;&#24471;&#39069;&#22806;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#20854;&#21021;&#22987;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#12290;&#28982;&#32780;&#65292;&#23427;&#21482;&#26159;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#20551;&#35774;&#24341;&#20837;&#30340;&#26657;&#20934;&#20219;&#21153;&#26377;&#20016;&#23500;&#30340;&#39069;&#22806;&#21487;&#29992;&#26679;&#26412;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#23454;&#38469;&#24773;&#20917;&#65292;&#25105;&#20204;&#38656;&#35201;&#26377;&#25928;&#21033;&#29992;&#35757;&#32451;&#26679;&#26412;&#65292;&#20351;PLMs&#25104;&#20026;&#20219;&#21153;&#35299;&#20915;&#22120;&#21644;&#33258;&#26657;&#20934;&#22120;&#12290;&#25552;&#20986;&#20102;&#19977;&#20010;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#38480;&#30340;&#35757;&#32451;&#26679;&#26412;&#12289;&#25968;&#25454;&#19981;&#24179;&#34913;&#21644;&#8230;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) serve as backbones for various real-world systems. For high-stake applications, it's equally essential to have reasonable confidence estimations in predictions. While the vanilla confidence scores of PLMs can already be effectively utilized, PLMs consistently become overconfident in their wrong predictions, which is not desirable in practice. Previous work shows that introducing an extra calibration task can mitigate this issue. The basic idea involves acquiring additional data to train models in predicting the confidence of their initial predictions. However, it only demonstrates the feasibility of this kind of method, assuming that there are abundant extra available samples for the introduced calibration task. In this work, we consider the practical scenario that we need to effectively utilize training samples to make PLMs both task-solvers and self-calibrators. Three challenges are presented, including limited training samples, data imbalance, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GIST&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20165;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#22270;&#20687;&#29305;&#23450;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#23558;&#20854;&#29992;&#20110;&#24494;&#35843;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#36827;&#22270;&#20687;&#20998;&#31867;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11315</link><description>&lt;p&gt;
&#29983;&#25104;&#22270;&#20687;&#29305;&#23450;&#25991;&#26412;&#25913;&#21892;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Generating Image-Specific Text Improves Fine-grained Image Classification. (arXiv:2307.11315v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GIST&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20165;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#22270;&#20687;&#29305;&#23450;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#23558;&#20854;&#29992;&#20110;&#24494;&#35843;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#36827;&#22270;&#20687;&#20998;&#31867;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#20248;&#20110;&#20165;&#35270;&#35273;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#37197;&#23545;&#30340;&#25991;&#26412;/&#22270;&#20687;&#25551;&#36848;&#65292;&#23545;&#20110;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#26469;&#35828;&#65292;&#20173;&#28982;&#24456;&#38590;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GIST&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20165;&#22270;&#20687;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#22270;&#20687;&#29305;&#23450;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#25991;&#26412;&#25551;&#36848;&#21487;&#20197;&#29992;&#20110;&#25913;&#21892;&#20998;&#31867;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#37096;&#20998;&#21253;&#25324;&#65306;1. &#20351;&#29992;&#29305;&#23450;&#39046;&#22495;&#30340;&#25552;&#31034;&#20026;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#65292;&#20197;&#21450;2. &#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23558;&#27599;&#20010;&#22270;&#20687;&#19982;&#20445;&#30041;&#26631;&#31614;&#30340;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#21305;&#37197;&#65292;&#36825;&#20123;&#25551;&#36848;&#25429;&#25417;&#20102;&#22270;&#20687;&#20013;&#30456;&#20851;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#22270;&#20687;&#21644;&#29983;&#25104;&#30340;&#25991;&#26412;&#23545;&#19978;&#24494;&#35843;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26469;&#23398;&#20064;&#19968;&#20010;&#23545;&#40784;&#30340;&#35270;&#35273;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#65292;&#20197;&#23454;&#29616;&#25913;&#36827;&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;GIST&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent vision-language models outperform vision-only models on many image classification tasks. However, because of the absence of paired text/image descriptions, it remains difficult to fine-tune these models for fine-grained image classification. In this work, we propose a method, GIST, for generating image-specific fine-grained text descriptions from image-only datasets, and show that these text descriptions can be used to improve classification. Key parts of our method include 1. prompting a pretrained large language model with domain-specific prompts to generate diverse fine-grained text descriptions for each class and 2. using a pretrained vision-language model to match each image to label-preserving text descriptions that capture relevant visual features in the image. We demonstrate the utility of GIST by fine-tuning vision-language models on the image-and-generated-text pairs to learn an aligned vision-language representation space for improved classification. We evaluate our l
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#22120;-&#26816;&#32034;&#22120;-&#29983;&#25104;&#22120;&#65288;GRG&#65289;&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#25991;&#26723;&#26816;&#32034;&#25216;&#26415;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#20934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.11278</link><description>&lt;p&gt;
&#29983;&#25104;&#22120;-&#26816;&#32034;&#22120;-&#29983;&#25104;&#22120;&#65306;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generator-Retriever-Generator: A Novel Approach to Open-domain Question Answering. (arXiv:2307.11278v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11278
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22120;-&#26816;&#32034;&#22120;-&#29983;&#25104;&#22120;&#65288;GRG&#65289;&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#25991;&#26723;&#26816;&#32034;&#25216;&#26415;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#20934;&#30830;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#38382;&#31572;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#20174;&#22823;&#22411;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#29983;&#25104;&#22120;-&#26816;&#32034;&#22120;-&#29983;&#25104;&#22120;&#65288;GRG&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#25991;&#26723;&#26816;&#32034;&#25216;&#26415;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#32467;&#21512;&#65292;&#39318;&#20808;&#36890;&#36807;&#32473;&#23450;&#38382;&#39064;&#25552;&#31034;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#25991;&#26723;&#12290;&#21516;&#26102;&#65292;&#21452;&#32534;&#30721;&#22120;&#32593;&#32476;&#20174;&#22806;&#37096;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#25991;&#26723;&#28982;&#21518;&#20256;&#36882;&#32473;&#31532;&#20108;&#20010;LLM&#65292;&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#12290;&#36890;&#36807;&#32467;&#21512;&#25991;&#26723;&#26816;&#32034;&#21644;LLM&#29983;&#25104;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#20449;&#24687;&#20016;&#23500;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#31572;&#26696;&#12290;GRG&#22312;TriviaQA&#12289;NQ&#21644;WebQ&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#29983;&#25104;-&#35835;&#21462;&#21644;&#26816;&#32034;-&#35835;&#21462;&#27969;&#27700;&#32447;&#65288;GENREAD&#21644;RFiD&#65289;&#65292;&#20998;&#21035;&#33267;&#23569;&#25552;&#39640;&#20102;+5.2&#12289;+4.2&#21644;+1.6&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-domain question answering (QA) tasks usually require the retrieval of relevant information from a large corpus to generate accurate answers. We propose a novel approach called Generator-Retriever-Generator (GRG) that combines document retrieval techniques with a large language model (LLM), by first prompting the model to generate contextual documents based on a given question. In parallel, a dual-encoder network retrieves documents that are relevant to the question from an external corpus. The generated and retrieved documents are then passed to the second LLM, which generates the final answer. By combining document retrieval and LLM generation, our approach addresses the challenges of open-domain QA, such as generating informative and contextually relevant answers. GRG outperforms the state-of-the-art generate-then-read and retrieve-then-read pipelines (GENREAD and RFiD) improving their performance at least by +5.2, +4.2, and +1.6 on TriviaQA, NQ, and WebQ datasets, respectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20248;&#20110;&#21333;&#29420;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#32771;&#34385;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11254</link><description>&lt;p&gt;
&#23545;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Federated Learning on Biomedical Natural Language Processing. (arXiv:2307.11254v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#32852;&#37030;&#23398;&#20064;&#27169;&#22411;&#20248;&#20110;&#21333;&#29420;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#32771;&#34385;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22914;BERT&#21644;GPT&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#28982;&#32780;&#65292;&#38544;&#31169;&#25935;&#24863;&#30340;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#21307;&#30103;&#39046;&#22495;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#25968;&#25454;&#35775;&#38382;&#21644;&#30001;&#12298;&#20581;&#24247;&#20445;&#38505;&#20415;&#25658;&#24615;&#21644;&#36131;&#20219;&#27861;&#26696;&#12299;&#65288;HIPPA&#65289;&#21644;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#65288;GDPR&#65289;&#31561;&#27861;&#35268;&#30340;&#38544;&#31169;&#32422;&#26463;&#65292;&#38754;&#20020;&#30528;&#35757;&#32451;LM&#30340;&#25361;&#25112;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#20998;&#25955;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26082;&#33021;&#22815;&#23454;&#29616;&#21327;&#21516;&#23398;&#20064;&#65292;&#21448;&#33021;&#22815;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#30340;&#20445;&#25252;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#21307;&#23398;&#20013;&#30340;FL&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#20845;&#20010;&#29983;&#29289;&#21307;&#23398;NLP&#20219;&#21153;&#65292;&#20351;&#29992;&#20102;&#20843;&#20010;&#35821;&#26009;&#24211;&#21644;&#20845;&#20010;LM&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;1&#65289;FL&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#21333;&#20010;&#23458;&#25143;&#31471;&#25968;&#25454;&#35757;&#32451;&#30340;LM&#65292;&#24182;&#19988;&#26377;&#26102;&#33021;&#22815;&#19982;&#20351;&#29992;&#27719;&#24635;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#21305;&#37197;&#65307;2&#65289;&#22312;&#24635;&#25968;&#25454;&#37327;&#22266;&#23450;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26356;&#22810;&#23458;&#25143;&#31471;&#36827;&#34892;FL&#35757;&#32451;&#30340;LM&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#65292;&#20294;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#24378;&#30340;&#40065;&#26834;&#24615;&#65307;3&#65289;LM&#20204;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) like BERT and GPT have revolutionized natural language processing (NLP). However, privacy-sensitive domains, particularly the medical field, face challenges to train LMs due to limited data access and privacy constraints imposed by regulations like the Health Insurance Portability and Accountability Act (HIPPA) and the General Data Protection Regulation (GDPR). Federated learning (FL) offers a decentralized solution that enables collaborative learning while ensuring the preservation of data privacy. In this study, we systematically evaluate FL in medicine across $2$ biomedical NLP tasks using $6$ LMs encompassing $8$ corpora. Our results showed that: 1) FL models consistently outperform LMs trained on individual client's data and sometimes match the model trained with polled data; 2) With the fixed number of total data, LMs trained using FL with more clients exhibit inferior performance, but pre-trained transformer-based models exhibited greater resilience. 3) LMs
&lt;/p&gt;</description></item><item><title>Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11224</link><description>&lt;p&gt;
Jina Embeddings:&#19968;&#31181;&#26032;&#39062;&#30340;&#39640;&#24615;&#33021;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models. (arXiv:2307.11224v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11224
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#26159;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#33021;&#22815;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#35813;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24615;&#33021;&#35780;&#20272;&#39564;&#35777;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Jina Embeddings&#30001;&#19968;&#32452;&#39640;&#24615;&#33021;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#32452;&#25104;&#65292;&#33021;&#22815;&#23558;&#21508;&#31181;&#25991;&#26412;&#36755;&#20837;&#36716;&#21270;&#20026;&#25968;&#20540;&#34920;&#31034;&#65292;&#20174;&#32780;&#25429;&#25417;&#25991;&#26412;&#30340;&#35821;&#20041;&#26412;&#36136;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#24182;&#38750;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#65292;&#20294;&#22312;&#23494;&#38598;&#26816;&#32034;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;Jina Embeddings&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#20174;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25104;&#23545;&#21644;&#19977;&#20803;&#25968;&#25454;&#38598;&#24320;&#22987;&#12290;&#23427;&#24378;&#35843;&#20102;&#25968;&#25454;&#28165;&#29702;&#22312;&#25968;&#25454;&#38598;&#20934;&#22791;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#23545;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#65292;&#26368;&#21518;&#21033;&#29992;Massive Textual Embedding Benchmark&#65288;MTEB&#65289;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings constitutes a set of high-performance sentence embedding models adept at translating various textual inputs into numerical representations, thereby capturing the semantic essence of the text. While these models are not exclusively designed for text generation, they excel in applications such as dense retrieval and semantic textual similarity. This paper details the development of Jina Embeddings, starting with the creation of a high-quality pairwise and triplet dataset. It underlines the crucial role of data cleaning in dataset preparation, gives in-depth insights into the model training process, and concludes with a comprehensive performance evaluation using the Massive Textual Embedding Benchmark (MTEB).
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#33539;&#24335;&#65292;&#36890;&#36807;&#20174;UMLS&#20013;&#25552;&#21462;&#25991;&#26412;&#24207;&#21015;&#26469;&#20016;&#23500;&#29983;&#29289;&#21307;&#23398;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;LMs&#30340;&#35821;&#35328;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.11170</link><description>&lt;p&gt;
UMLS-KGI-BERT&#65306;&#22522;&#20110;&#36716;&#21270;&#22120;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#25968;&#25454;&#20013;&#24515;&#30693;&#35782;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition. (arXiv:2307.11170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11170
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#33539;&#24335;&#65292;&#36890;&#36807;&#20174;UMLS&#20013;&#25552;&#21462;&#25991;&#26412;&#24207;&#21015;&#26469;&#20016;&#23500;&#29983;&#29289;&#21307;&#23398;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;LMs&#30340;&#35821;&#35328;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#39044;&#35757;&#32451;&#30340;&#36716;&#21270;&#22120;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24050;&#25104;&#20026;&#20027;&#23548;&#33539;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20449;&#24687;&#25552;&#21462;&#12289;&#38382;&#31572;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#25991;&#26723;&#20998;&#31867;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#23558;&#35813;&#33539;&#24335;&#36866;&#24212;&#20110;&#38656;&#35201;&#32467;&#21512;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#20197;&#21450;&#35821;&#35328;&#30340;&#32479;&#35745;&#24314;&#27169;&#30340;NLP&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#22914;&#20309;&#26368;&#22909;&#22320;&#26500;&#24314;LMs&#65292;&#26082;&#32771;&#34385;&#21307;&#23398;&#25991;&#26412;&#20013;&#20196;&#29260;&#20998;&#24067;&#30340;&#27169;&#24335;&#65292;&#21448;&#32771;&#34385;UMLS&#31561;&#26415;&#35821;&#36164;&#28304;&#20013;&#21253;&#21547;&#30340;&#20016;&#23500;&#32467;&#26500;&#21270;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#33539;&#24335;&#65292;&#36890;&#36807;&#20174;UMLS&#20013;&#25552;&#21462;&#25991;&#26412;&#24207;&#21015;&#26469;&#20016;&#23500;&#29983;&#29289;&#21307;&#23398;&#36716;&#25442;&#22120;&#32534;&#30721;&#22120;LMs&#30340;&#35821;&#35328;&#34920;&#31034;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#24212;&#29992;&#22522;&#20110;&#22270;&#30340;&#23398;&#20064;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformer language models (LMs) have in recent years become the dominant paradigm in applied NLP. These models have achieved state-of-the-art performance on tasks such as information extraction, question answering, sentiment analysis, document classification and many others. In the biomedical domain, significant progress has been made in adapting this paradigm to NLP tasks that require the integration of domain-specific knowledge as well as statistical modelling of language. In particular, research in this area has focused on the question of how best to construct LMs that take into account not only the patterns of token distribution in medical text, but also the wealth of structured information contained in terminology resources such as the UMLS. This work contributes a data-centric paradigm for enriching the language representations of biomedical transformer-encoder LMs by extracting text sequences from the UMLS. This allows for graph-based learning objectives to be comb
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.10490</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#22768;&#38899;&#30340;&#28389;&#29992;&#29992;&#20110;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25552;&#31034;&#21644;&#25351;&#20196;&#27880;&#20837;&#12290;&#25915;&#20987;&#32773;&#29983;&#25104;&#19982;&#25552;&#31034;&#30456;&#23545;&#24212;&#30340;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#12290;&#24403;&#29992;&#25143;&#21521;&#65288;&#26410;&#20462;&#25913;&#30340;&#33391;&#24615;&#65289;&#27169;&#22411;&#35810;&#38382;&#34987;&#25200;&#21160;&#30340;&#22270;&#20687;&#25110;&#38899;&#39057;&#26102;&#65292;&#25200;&#21160;&#20250;&#24341;&#23548;&#27169;&#22411;&#36755;&#20986;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#25991;&#26412;&#21644;/&#25110;&#20351;&#21518;&#32493;&#23545;&#35805;&#36981;&#24490;&#25915;&#20987;&#32773;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#29992;&#20960;&#20010;&#27010;&#24565;&#39564;&#35777;&#31034;&#20363;&#38024;&#23545;LLaVa&#21644;PandaGPT&#26469;&#35828;&#26126;&#36825;&#31181;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
&lt;/p&gt;</description></item><item><title>PharmacyGPT&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#20223;&#30495;&#20020;&#24202;&#33647;&#24072;&#30340;&#35282;&#33394;&#12290;&#36890;&#36807;&#29983;&#25104;&#24739;&#32773;&#32676;&#38598;&#12289;&#21046;&#23450;&#29992;&#33647;&#35745;&#21010;&#21644;&#39044;&#27979;&#24739;&#32773;&#32467;&#26524;&#65292;PharmacyGPT&#22312;&#20020;&#24202;&#33647;&#23398;&#20013;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#21644;&#38480;&#21046;&#65292;&#20026;&#20419;&#36827;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.10432</link><description>&lt;p&gt;
PharmacyGPT&#65306;AI&#33647;&#24072;
&lt;/p&gt;
&lt;p&gt;
PharmacyGPT: The AI Pharmacist. (arXiv:2307.10432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10432
&lt;/p&gt;
&lt;p&gt;
PharmacyGPT&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#20223;&#30495;&#20020;&#24202;&#33647;&#24072;&#30340;&#35282;&#33394;&#12290;&#36890;&#36807;&#29983;&#25104;&#24739;&#32773;&#32676;&#38598;&#12289;&#21046;&#23450;&#29992;&#33647;&#35745;&#21010;&#21644;&#39044;&#27979;&#24739;&#32773;&#32467;&#26524;&#65292;PharmacyGPT&#22312;&#20020;&#24202;&#33647;&#23398;&#20013;&#20855;&#26377;&#28508;&#22312;&#24212;&#29992;&#21644;&#38480;&#21046;&#65292;&#20026;&#20419;&#36827;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;PharmacyGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65288;&#22914;ChatGPT&#21644;GPT-4&#65289;&#22312;&#20223;&#30495;&#20020;&#24202;&#33647;&#24072;&#35282;&#33394;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#21033;&#29992;LLM&#29983;&#25104;&#21487;&#29702;&#35299;&#30340;&#24739;&#32773;&#32676;&#38598;&#12289;&#21046;&#23450;&#29992;&#33647;&#35745;&#21010;&#21644;&#39044;&#27979;&#24739;&#32773;&#32467;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#21271;&#21345;&#32599;&#26469;&#32435;&#22823;&#23398;&#25945;&#22530;&#23665;&#21307;&#38498;&#65288;UNC&#65289;&#37325;&#30151;&#30417;&#25252;&#30149;&#25151;&#65288;ICU&#65289;&#33719;&#21462;&#30340;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;LLM&#22312;&#20020;&#24202;&#33647;&#23398;&#39046;&#22495;&#28508;&#22312;&#24212;&#29992;&#21644;&#38480;&#21046;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#65292;&#23545;&#24739;&#32773;&#25252;&#29702;&#21644;&#26410;&#26469;&#22522;&#20110;AI&#30340;&#21307;&#30103;&#35299;&#20915;&#26041;&#26696;&#30340;&#24320;&#21457;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#36890;&#36807;&#35780;&#20272;PharmacyGPT&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#26377;&#20851;&#22312;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#30340;&#25345;&#32493;&#35752;&#35770;&#20570;&#20986;&#36129;&#29486;&#65292;&#26368;&#32456;&#20419;&#36827;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#20351;&#29992;&#27492;&#31867;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce PharmacyGPT, a novel framework to assess the capabilities of large language models (LLMs) such as ChatGPT and GPT-4 in emulating the role of clinical pharmacists. Our methodology encompasses the utilization of LLMs to generate comprehensible patient clusters, formulate medication plans, and forecast patient outcomes. We conduct our investigation using real data acquired from the intensive care unit (ICU) at the University of North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable insights into the potential applications and limitations of LLMs in the field of clinical pharmacy, with implications for both patient care and the development of future AI-driven healthcare solutions. By evaluating the performance of PharmacyGPT, we aim to contribute to the ongoing discourse surrounding the integration of artificial intelligence in healthcare settings, ultimately promoting the responsible and efficacious use of such technologies.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#20998;&#26512;&#26041;&#27861;&#65292;&#23558;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#25581;&#31034;&#20102;&#36825;&#20004;&#20010;&#20449;&#24687;&#25552;&#21462;&#23376;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#22686;&#24378;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2307.10291</link><description>&lt;p&gt;
&#26085;&#35821;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#30456;&#20114;&#22686;&#24378;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Mutual Reinforcement Effects in Japanese Sentence Classification and Named Entity Recognition Tasks. (arXiv:2307.10291v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10291
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#20998;&#26512;&#26041;&#27861;&#65292;&#23558;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#25581;&#31034;&#20102;&#36825;&#20004;&#20010;&#20449;&#24687;&#25552;&#21462;&#23376;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#22686;&#24378;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20256;&#32479;&#20998;&#27573;&#26041;&#27861;&#22312;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#22797;&#26434;&#20114;&#21160;&#65292;&#30446;&#21069;&#30740;&#31350;&#23578;&#19981;&#20805;&#20998;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#20998;&#26512;&#26041;&#27861;&#65292;&#23558;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#32467;&#21512;&#36215;&#26469;&#65292;&#26088;&#22312;&#25581;&#31034;&#21644;&#29702;&#35299;&#36825;&#20004;&#20010;&#20449;&#24687;&#25552;&#21462;&#23376;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20114;&#22686;&#24378;&#25928;&#24212;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#22810;&#20219;&#21153;&#65288;SCNM&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21477;&#23376;&#20998;&#31867;&#65288;SC&#65289;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12290;&#25105;&#20204;&#20026;SCNM&#24320;&#21457;&#20102;&#19968;&#20010;&#21477;&#23376;&#21040;&#26631;&#31614;&#29983;&#25104;&#65288;SLG&#65289;&#26694;&#26550;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;SC&#21644;NER&#30340;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#26684;&#24335;&#36716;&#25442;&#22120;&#32479;&#19968;&#36755;&#20837;&#26684;&#24335;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;SC&#26631;&#31614;&#12289;NER&#26631;&#31614;&#21644;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#26085;&#35821;&#21477;&#23376;&#20998;&#31867;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#20102;&#30456;&#20114;&#22686;&#24378;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information extraction(IE) is a crucial subfield within natural language processing. However, for the traditionally segmented approach to sentence classification and Named Entity Recognition, the intricate interactions between these individual subtasks remain largely uninvestigated. In this study, we propose an integrative analysis, converging sentence classification with Named Entity Recognition, with the objective to unveil and comprehend the mutual reinforcement effect within these two information extraction subtasks. To achieve this, we introduce a Sentence Classification and Named Entity Recognition Multi-task (SCNM) approach that combines Sentence Classification (SC) and Named Entity Recognition (NER). We develop a Sentence-to-Label Generation (SLG) framework for SCNM and construct a Wikipedia dataset containing both SC and NER. Using a format converter, we unify input formats and employ a generative model to generate SC-labels, NER-labels, and associated text segments. We propos
&lt;/p&gt;</description></item><item><title>ZeroQuant-FP&#36890;&#36807;&#20351;&#29992;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;LLMs&#35757;&#32451;&#21518;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#30340;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;FP8&#28608;&#27963;&#20248;&#20110;INT8&#65292;&#24182;&#19988;FP4&#26435;&#37325;&#34920;&#29616;&#19982;INT4&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2307.09782</link><description>&lt;p&gt;
ZeroQuant-FP: &#20351;&#29992;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;LLMs&#35757;&#32451;&#21518;&#37327;&#21270;&#30340;&#19968;&#39033;&#39134;&#36291;
&lt;/p&gt;
&lt;p&gt;
ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats. (arXiv:2307.09782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09782
&lt;/p&gt;
&lt;p&gt;
ZeroQuant-FP&#36890;&#36807;&#20351;&#29992;&#28014;&#28857;&#26684;&#24335;&#36827;&#34892;LLMs&#35757;&#32451;&#21518;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#30340;&#25361;&#25112;&#65292;&#24182;&#21457;&#29616;FP8&#28608;&#27963;&#20248;&#20110;INT8&#65292;&#24182;&#19988;FP4&#26435;&#37325;&#34920;&#29616;&#19982;INT4&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22797;&#26434;&#39046;&#22495;&#20013;&#65292;&#24179;&#34913;&#35745;&#31639;&#25928;&#29575;&#21644;&#20445;&#25345;&#27169;&#22411;&#36136;&#37327;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#35752;&#28014;&#28857;&#65288;FP&#65289;&#37327;&#21270;&#30340;&#21487;&#34892;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;FP8&#21644;FP4&#65292;&#20197;&#24212;&#23545;&#22343;&#21248;&#37327;&#21270;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#22788;&#29702;&#31163;&#32676;&#20540;&#65292;&#24182;&#21463;&#21040;NVIDIA H100&#30828;&#20214;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35843;&#26597;&#21457;&#29616;&#65292;&#22312;LLMs&#20013;&#65292;FP8&#28608;&#27963;&#22987;&#32456;&#20248;&#20110;&#20854;&#25972;&#25968;&#65288;INT8&#65289;&#31561;&#25928;&#65292;&#24615;&#33021;&#20248;&#21183;&#22312;&#21253;&#21547;&#36229;&#36807;&#21313;&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#20013;&#26356;&#20026;&#26126;&#26174;&#12290;&#23545;&#20110;&#26435;&#37325;&#37327;&#21270;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;FP4&#30340;&#24615;&#33021;&#19982;INT4&#30456;&#24403;&#65292;&#29978;&#33267;&#26356;&#20248;&#65292;&#31616;&#21270;&#20102;&#22312;&#20687;H100&#36825;&#26679;&#25903;&#25345;FP&#30340;&#30828;&#20214;&#19978;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#20943;&#23569;&#30001;&#26435;&#37325;&#21644;&#28608;&#27963;&#20043;&#38388;&#24046;&#24322;&#24341;&#36215;&#30340;&#31934;&#24230;&#23545;&#40784;&#24320;&#38144;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#32553;&#25918;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the complex domain of large language models (LLMs), striking a balance between computational efficiency and maintaining model quality is a formidable challenge. Navigating the inherent limitations of uniform quantization, particularly when dealing with outliers, and motivated by the launch of NVIDIA's H100 hardware, this study delves into the viability of floating-point (FP) quantization, particularly focusing on FP8 and FP4, as a potential solution. Our comprehensive investigation reveals that for LLMs, FP8 activation consistently outshines its integer (INT8) equivalent, with the performance edge becoming more noticeable in models possessing parameters beyond one billion. For weight quantization, our findings indicate that FP4 exhibits comparable, if not superior, performance to INT4, simplifying deployment on FP-supported hardware like H100. To mitigate the overhead from precision alignment caused by the disparity between weights and activations, we propose two scaling constraints
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GLORY&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20840;&#23616;&#22270;&#19982;&#26412;&#22320;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26500;&#24314;&#20840;&#23616;&#24863;&#30693;&#21382;&#21490;&#26032;&#38395;&#32534;&#30721;&#22120;&#26469;&#34701;&#21512;&#21382;&#21490;&#26032;&#38395;&#34920;&#31034;&#65292;&#24182;&#32771;&#34385;&#20102;&#29992;&#25143;&#38544;&#34255;&#30340;&#21160;&#26426;&#21644;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.06576</link><description>&lt;p&gt;
&#36229;&#36234;&#26412;&#22320;&#33539;&#22260;&#65306;&#20840;&#29699;&#22270;&#22686;&#24378;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Going Beyond Local: Global Graph-Enhanced Personalized News Recommendations. (arXiv:2307.06576v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GLORY&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20840;&#23616;&#22270;&#19982;&#26412;&#22320;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#26500;&#24314;&#20840;&#23616;&#24863;&#30693;&#21382;&#21490;&#26032;&#38395;&#32534;&#30721;&#22120;&#26469;&#34701;&#21512;&#21382;&#21490;&#26032;&#38395;&#34920;&#31034;&#65292;&#24182;&#32771;&#34385;&#20102;&#29992;&#25143;&#38544;&#34255;&#30340;&#21160;&#26426;&#21644;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#22320;&#21521;&#29992;&#25143;&#25512;&#33616;&#20505;&#36873;&#26032;&#38395;&#25991;&#31456;&#19968;&#30452;&#26159;&#20010;&#24615;&#21270;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#30340;&#26680;&#24515;&#25361;&#25112;&#12290;&#22823;&#22810;&#25968;&#36817;&#26399;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20174;&#20016;&#23500;&#30340;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351;&#29992;&#20174;&#26412;&#22320;&#21382;&#21490;&#26032;&#38395;&#27966;&#29983;&#30340;&#22522;&#20110;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#32570;&#20047;&#20840;&#23616;&#35270;&#35282;&#65292;&#26410;&#33021;&#32771;&#34385;&#29992;&#25143;&#38544;&#34255;&#30340;&#21160;&#26426;&#21644;&#34892;&#20026;&#65292;&#36229;&#36234;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411; GLORY&#65288;Global-LOcal news Recommendation sYstem&#65289;&#65292;&#23427;&#32467;&#21512;&#20102;&#20174;&#20854;&#20182;&#29992;&#25143;&#23398;&#21040;&#30340;&#20840;&#23616;&#34920;&#31034;&#21644;&#26412;&#22320;&#34920;&#31034;&#65292;&#26469;&#22686;&#24378;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#20840;&#23616;&#24863;&#30693;&#21382;&#21490;&#26032;&#38395;&#32534;&#30721;&#22120;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#20840;&#23616;&#26032;&#38395;&#22270;&#65292;&#24182;&#20351;&#29992;&#38376;&#25511;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#20016;&#23500;&#26032;&#38395;&#34920;&#31034;&#65292;&#20174;&#32780;&#36890;&#36807;&#21382;&#21490;&#26032;&#38395;&#32858;&#21512;&#22120;&#34701;&#21512;&#21382;&#21490;&#26032;&#38395;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precisely recommending candidate news articles to users has always been a core challenge for personalized news recommendation systems. Most recent works primarily focus on using advanced natural language processing techniques to extract semantic information from rich textual data, employing content-based methods derived from local historical news. However, this approach lacks a global perspective, failing to account for users' hidden motivations and behaviors beyond semantic information. To address this challenge, we propose a novel model called GLORY (Global-LOcal news Recommendation sYstem), which combines global representations learned from other users with local representations to enhance personalized recommendation systems. We accomplish this by constructing a Global-aware Historical News Encoder, which includes a global news graph and employs gated graph neural networks to enrich news representations, thereby fusing historical news representations by a historical news aggregator.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#37329;&#34701;&#20851;&#31995;&#25552;&#21462;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20004;&#31181;&#26816;&#32034;&#31574;&#30053;&#65292;&#26080;&#38656;&#23398;&#20064;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#26816;&#32034;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#25214;&#21040;&#19982;&#32473;&#23450;&#27979;&#35797;&#31034;&#20363;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#33539;&#12290;</title><link>http://arxiv.org/abs/2306.17519</link><description>&lt;p&gt;
GPT-FinRE: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37329;&#34701;&#20851;&#31995;&#25552;&#21462;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
GPT-FinRE: In-context Learning for Financial Relation Extraction using Large Language Models. (arXiv:2306.17519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#37329;&#34701;&#20851;&#31995;&#25552;&#21462;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#20004;&#31181;&#26816;&#32034;&#31574;&#30053;&#65292;&#26080;&#38656;&#23398;&#20064;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#26816;&#32034;&#22120;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#25214;&#21040;&#19982;&#32473;&#23450;&#27979;&#35797;&#31034;&#20363;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#35782;&#21035;&#21644;&#20998;&#31867;&#25991;&#26412;&#20013;&#25552;&#21450;&#30340;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#37329;&#34701;&#39046;&#22495;&#65292;&#20851;&#31995;&#25552;&#21462;&#22312;&#20174;&#36130;&#32463;&#25991;&#20214;&#65288;&#22914;&#26032;&#38395;&#25991;&#31456;&#12289;&#30408;&#21033;&#25253;&#21578;&#21644;&#20844;&#21496;&#30003;&#25253;&#65289;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#20449;&#24687;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;&#19968;&#20010;&#21517;&#20026;REFinD&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20851;&#31995;&#25552;&#21462;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#20316;&#20026;SIGIR 2023&#20030;&#21150;&#30340;&#31532;&#22235;&#23626;&#20174;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#21457;&#29616;&#30693;&#35782;&#30340;&#30740;&#35752;&#20250;&#30340;&#20849;&#20139;&#20219;&#21153;&#30340;&#19968;&#37096;&#20998;&#21457;&#24067;&#30340;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#37319;&#29992;&#20102;OpenAI&#27169;&#22411;&#65292;&#24182;&#24212;&#29992;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#21033;&#29992;&#20004;&#31181;&#26816;&#32034;&#31574;&#30053;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#25214;&#20986;&#19982;&#32473;&#23450;&#27979;&#35797;&#31034;&#20363;&#30456;&#20851;&#30340;&#21069;K&#20010;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#33539;/&#31034;&#20363;&#12290;&#31532;&#19968;&#20010;&#26816;&#32034;&#26426;&#21046;&#26159;&#26080;&#38656;&#23398;&#20064;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#65292;&#32780;&#21478;&#19968;&#20010;&#31995;&#32479;&#26159;&#22522;&#20110;&#23398;&#20064;&#30340;&#26816;&#32034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) is a crucial task in natural language processing (NLP) that aims to identify and classify relationships between entities mentioned in text. In the financial domain, relation extraction plays a vital role in extracting valuable information from financial documents, such as news articles, earnings reports, and company filings. This paper describes our solution to relation extraction on one such dataset REFinD. The dataset was released along with shared task as a part of the Fourth Workshop on Knowledge Discovery from Unstructured Data in Financial Services, co-located with SIGIR 2023. In this paper, we employed OpenAI models under the framework of in-context learning (ICL). We utilized two retrieval strategies to find top K relevant in-context learning demonstrations / examples from training data for a given test example. The first retrieval mechanism, we employed, is a learning-free dense retriever and the other system is a learning-based retriever. We were able
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2306.14096</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#23454;&#20307;&#32423;&#21035;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#26159;&#24773;&#24863;&#20998;&#26512;&#30340;&#37325;&#35201;&#23376;&#20219;&#21153;&#65292;&#30446;&#21069;&#38754;&#20020;&#30528;&#20247;&#22810;&#25361;&#25112;&#12290;&#20854;&#20013;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26469;&#33258;&#20110;&#32570;&#20047;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#37329;&#34701;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#30340;&#39640;&#36136;&#37327;&#22823;&#35268;&#27169;&#26631;&#27880;&#35821;&#26009;&#24211;&#65292;&#36825;&#38480;&#21046;&#20102;&#24320;&#21457;&#26377;&#25928;&#25991;&#26412;&#22788;&#29702;&#25216;&#26415;&#25152;&#38656;&#30340;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#35328;&#27169;&#24335;&#21305;&#37197;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#30340;&#29616;&#26377;&#24320;&#28304;LLMs&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#25105;&#20204;&#22362;&#20449;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#21160;&#30495;&#23454;&#19990;&#30028;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment analysis and currently faces numerous challenges. The primary challenge stems from the lack of high-quality and large-scale annotated corpora specifically designed for financial text sentiment analysis, which in turn limits the availability of data necessary for developing effective text processing techniques. Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which shoul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20445;&#30041;&#30446;&#26631;&#26041;&#38754;&#30456;&#20851;&#35821;&#20041;&#30340;&#26377;&#22122;&#22768;&#12289;&#25104;&#26412;&#25928;&#30410;&#36739;&#39640;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#19981;&#21516;&#29256;&#26412;&#20043;&#38388;&#30340;&#19981;&#21464;&#24615;&#36827;&#34892;&#24314;&#27169;&#20197;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#65292;&#22312;&#26631;&#20934;&#21644;&#24378;&#24230;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#37117;&#26174;&#33879;&#25913;&#36827;&#20102;&#24378;&#39044;&#35757;&#32451;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13971</link><description>&lt;p&gt;
&#38750;&#21453;&#20107;&#23454;&#22686;&#24378;&#22312;&#24378;&#20581;&#24615;&#26041;&#38754;&#23545;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Aspect-based Sentiment Analysis through Non-counterfactual Augmentations. (arXiv:2306.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20445;&#30041;&#30446;&#26631;&#26041;&#38754;&#30456;&#20851;&#35821;&#20041;&#30340;&#26377;&#22122;&#22768;&#12289;&#25104;&#26412;&#25928;&#30410;&#36739;&#39640;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#30340;&#19981;&#21516;&#29256;&#26412;&#20043;&#38388;&#30340;&#19981;&#21464;&#24615;&#36827;&#34892;&#24314;&#27169;&#20197;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#65292;&#22312;&#26631;&#20934;&#21644;&#24378;&#24230;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#37117;&#26174;&#33879;&#25913;&#36827;&#20102;&#24378;&#39044;&#35757;&#32451;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#22312;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26377;&#22823;&#37327;&#35777;&#25454;&#34920;&#26126;&#23427;&#20204;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#20998;&#24067;&#22806;&#25968;&#25454;&#26102;&#65292;&#20854;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#12290;&#26368;&#36817;&#30340;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#21453;&#20107;&#23454;&#22686;&#24378;&#25968;&#25454;&#38598;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#30001;&#20110;&#26080;&#27861;&#35775;&#38382;&#26174;&#24335;&#30340;&#22240;&#26524;&#32467;&#26500;&#65292;&#23427;&#20204;&#26412;&#36136;&#19978;&#26159;&#26377;&#38480;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#38750;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#20381;&#36182;&#20110;&#20351;&#29992;&#24102;&#26377;&#22122;&#22768;&#30340;&#65292;&#25104;&#26412;&#25928;&#30410;&#36739;&#39640;&#30340;&#25968;&#25454;&#22686;&#24378;&#26469;&#20445;&#30041;&#19982;&#30446;&#26631;&#26041;&#38754;&#30456;&#20851;&#32852;&#30340;&#35821;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#23545;&#25968;&#25454;&#30340;&#19981;&#21516;&#29256;&#26412;&#20043;&#38388;&#30340;&#19981;&#21464;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#12290;&#19968;&#32452;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#22312;&#26631;&#20934;&#21644;&#24378;&#24230;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#37117;&#26174;&#33879;&#25913;&#36827;&#20102;&#24378;&#39044;&#35757;&#32451;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While state-of-the-art NLP models have demonstrated excellent performance for aspect based sentiment analysis (ABSA), substantial evidence has been presented on their lack of robustness. This is especially manifested as significant degradation in performance when faced with out-of-distribution data. Recent solutions that rely on counterfactually augmented datasets show promising results, but they are inherently limited because of the lack of access to explicit causal structure. In this paper, we present an alternative approach that relies on non-counterfactual data augmentation. Our proposal instead relies on using noisy, cost-efficient data augmentations that preserve semantics associated with the target aspect. Our approach then relies on modelling invariances between different versions of the data to improve robustness. A comprehensive suite of experiments shows that our proposal significantly improves upon strong pre-trained baselines on both standard and robustness-specific datase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25512;&#26029;&#21028;&#20363;&#39072;&#20498;&#30340;&#22240;&#26524;&#26694;&#26550;&#65288;FAIR&#65289;&#65292;&#36890;&#36807;&#37319;&#29992;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#25366;&#25496;&#21028;&#20363;&#39072;&#20498;&#30340;&#21407;&#22240;&#65292;&#24182;&#23558;&#22240;&#26524;&#20851;&#31995;&#27880;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11585</link><description>&lt;p&gt;
FAIR:&#29992;&#20110;&#20934;&#30830;&#25512;&#26029;&#21028;&#20363;&#39072;&#20498;&#30340;&#22240;&#26524;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FAIR: A Causal Framework for Accurately Inferring Judgments Reversals. (arXiv:2306.11585v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25512;&#26029;&#21028;&#20363;&#39072;&#20498;&#30340;&#22240;&#26524;&#26694;&#26550;&#65288;FAIR&#65289;&#65292;&#36890;&#36807;&#37319;&#29992;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#25366;&#25496;&#21028;&#20363;&#39072;&#20498;&#30340;&#21407;&#22240;&#65292;&#24182;&#23558;&#22240;&#26524;&#20851;&#31995;&#27880;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#22312;&#27861;&#24459;&#26234;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#24182;&#26410;&#20851;&#27880;&#21028;&#20363;&#39072;&#20498;&#20013;&#25152;&#34164;&#21547;&#30340;&#37325;&#35201;&#20215;&#20540;&#65292;&#36825;&#38480;&#21046;&#20102;&#27861;&#24459;&#26234;&#33021;&#25928;&#29575;&#30340;&#25552;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#29992;&#20110;&#25512;&#26029;&#21028;&#20363;&#39072;&#20498;&#30340;&#22240;&#26524;&#26694;&#26550;&#65288;FAIR&#65289;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#30495;&#23454;&#30340;&#20013;&#22269;&#21028;&#20363;&#23545;&#21028;&#20363;&#39072;&#20498;&#38382;&#39064;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#20351;&#29992;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#25366;&#25496;&#21028;&#20363;&#39072;&#20498;&#30340;&#21407;&#22240;&#65292;&#24182;&#23558;&#24471;&#21040;&#30340;&#22240;&#26524;&#20851;&#31995;&#27880;&#20837;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#20316;&#20026;&#27861;&#24459;&#21028;&#20363;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#39564;&#35777;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#25366;&#25496;&#20986;&#21028;&#20363;&#39072;&#20498;&#20013;&#26368;&#20851;&#38190;&#30340;&#22240;&#32032;&#65292;&#24471;&#21040;&#30340;&#22240;&#26524;&#20851;&#31995;&#33021;&#22815;&#26377;&#25928;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence researchers have made significant advances in legal intelligence in recent years. However, the existing studies have not focused on the important value embedded in judgments reversals, which limits the improvement of the efficiency of legal intelligence. In this paper, we propose a causal Framework for Accurately Inferring case Reversals (FAIR), which models the problem of judgments reversals based on real Chinese judgments. We mine the causes of judgments reversals by causal inference methods and inject the obtained causal relationships into the neural network as a priori knowledge. And then, our framework is validated on a challenging dataset as a legal judgment prediction task. The experimental results show that our framework can tap the most critical factors in judgments reversal, and the obtained causal relationships can effectively improve the neural network's performance. In addition, we discuss the generalization ability of large language models for lega
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22522;&#20110;&#21465;&#20107;&#30340;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#35299;&#20915;&#20854;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.02250</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#22522;&#20110;&#21465;&#20107;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Augmented Narrative Driven Recommendations. (arXiv:2306.02250v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02250
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22522;&#20110;&#21465;&#20107;&#30340;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#35299;&#20915;&#20854;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21465;&#20107;&#30340;&#25512;&#33616;&#31995;&#32479;&#26159;&#19968;&#20010;&#20449;&#24687;&#33719;&#21462;&#38382;&#39064;&#65292;&#29992;&#25143;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#20182;&#20204;&#30340;&#20559;&#22909;&#21644;&#32972;&#26223;&#26469;&#35831;&#27714;&#25512;&#33616;&#65292;&#27604;&#22914;&#26053;&#34892;&#32773;&#22312;&#25551;&#36848;&#20182;&#20204;&#30340;&#21916;&#22909;&#12289;&#19981;&#21916;&#27426;&#21644;&#26053;&#34892;&#24773;&#20917;&#26102;&#35831;&#27714;&#26223;&#28857;&#30340;&#25512;&#33616;&#12290;&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#30028;&#38754;&#22312;&#25628;&#32034;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20852;&#36215;&#65292;&#36825;&#20123;&#35831;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#21465;&#20107;&#30340;&#25512;&#33616;&#31995;&#32479;&#32570;&#20047;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#24403;&#21069;&#30340;&#24179;&#21488;&#36890;&#24120;&#19981;&#25903;&#25345;&#36825;&#20123;&#35831;&#27714;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#20256;&#32479;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20016;&#23500;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#20363;&#22914;&#35780;&#35770;&#65292;&#36825;&#20123;&#35780;&#35770;&#32463;&#24120;&#25551;&#36848;&#20102;&#29992;&#25143;&#30340;&#20559;&#22909;&#21644;&#32972;&#26223; - &#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#29992;&#26469;&#20026;&#22522;&#20110;&#21465;&#20107;&#30340;&#25512;&#33616;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26469;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#35757;&#32451;&#22522;&#20110;&#21465;&#20107;&#30340;&#25512;&#33616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Narrative-driven recommendation (NDR) presents an information access problem where users solicit recommendations with verbose descriptions of their preferences and context, for example, travelers soliciting recommendations for points of interest while describing their likes/dislikes and travel circumstances. These requests are increasingly important with the rise of natural language-based conversational interfaces for search and recommendation systems. However, NDR lacks abundant training data for models, and current platforms commonly do not support these requests. Fortunately, classical user-item interaction datasets contain rich textual data, e.g., reviews, which often describe user preferences and context - this may be used to bootstrap training for NDR models. In this work, we explore using large language models (LLMs) for data augmentation to train NDR models. We use LLMs for authoring synthetic narrative queries from user-item interactions with few-shot prompting and train retri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26550;&#26500;&#26469;&#22686;&#24378;&#25277;&#21462;&#24335;&#25688;&#35201;&#30340;&#36830;&#36143;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#25277;&#21462;&#24335;&#25688;&#35201;&#30340;&#36830;&#36143;&#24615;&#65292;&#24182;&#22312;&#20854;&#20182;&#35780;&#20215;&#25351;&#26631;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12851</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#22686;&#24378;&#25277;&#21462;&#24335;&#25688;&#35201;&#30340;&#36830;&#36143;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Coherence of Extractive Summarization with Multitask Learning. (arXiv:2305.12851v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26550;&#26500;&#26469;&#22686;&#24378;&#25277;&#21462;&#24335;&#25688;&#35201;&#30340;&#36830;&#36143;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#25277;&#21462;&#24335;&#25688;&#35201;&#30340;&#36830;&#36143;&#24615;&#65292;&#24182;&#22312;&#20854;&#20182;&#35780;&#20215;&#25351;&#26631;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#26550;&#26500;&#26469;&#22686;&#24378;&#25277;&#21462;&#24335;&#25688;&#35201;&#30340;&#36830;&#36143;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26550;&#26500;&#21253;&#21547;&#19968;&#20010;&#25277;&#21462;&#24335;&#25688;&#35201;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#36830;&#36143;&#24615;&#21028;&#21035;&#22120;&#27169;&#22359;&#12290;&#36830;&#36143;&#24615;&#21028;&#21035;&#22120;&#36890;&#36807;&#22312;&#32447;&#35757;&#32451;&#22686;&#24378;&#20102;&#23545;&#36755;&#20837;&#21477;&#23376;&#36830;&#36143;&#24615;&#30340;&#21028;&#26029;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#26356;&#26032;&#25688;&#35201;&#29983;&#25104;&#22120;&#30340;&#21442;&#25968;&#26469;&#26368;&#22823;&#21270;&#36830;&#36143;&#24615;&#21028;&#21035;&#22120;&#30340;&#36830;&#36143;&#24615;&#20998;&#25968;&#12290;&#20026;&#20102;&#33021;&#22815;&#20197;&#21487;&#24494;&#20998;&#30340;&#26041;&#24335;&#35757;&#32451;&#25277;&#21462;&#24335;&#25688;&#35201;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#31574;&#30053;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#36716;&#25442;&#27169;&#22411;&#21644;&#36716;&#25442;&#30697;&#38453;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#24182;&#21477;&#23376;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#25277;&#21462;&#24335;&#25688;&#35201;&#20013;&#36830;&#36143;&#21477;&#23376;&#22312;&#21407;&#22987;&#25991;&#31456;&#20013;&#25353;&#20301;&#32622;&#30340;&#27604;&#20363;&#65288;&#21363;&#33258;&#21160;&#21477;&#23376;&#32423;&#36830;&#36143;&#24230;&#25351;&#26631;&#65289;&#65292;&#32780;&#22312;&#20854;&#20182;&#33258;&#21160;&#35780;&#20215;&#25351;&#26631;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes a multitask learning architecture for extractive summarization with coherence boosting. The architecture contains an extractive summarizer and coherent discriminator module. The coherent discriminator is trained online on the sentence vectors of the augmented textual input, thus improving its general ability of judging whether the input sentences are coherent. Meanwhile, we maximize the coherent scores from the coherent discriminator by updating the parameters of the summarizer. To make the extractive sentences trainable in a differentiable manner, we introduce two strategies, including pre-trained converting model (model-based) and converting matrix (MAT-based) that merge sentence representations. Experiments show that our proposed method significantly improves the proportion of consecutive sentences in the extracted summaries based on their positions in the original article (i.e., automatic sentence-level coherence metric), while the goodness in terms of other aut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;LACE&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#27010;&#24565;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#26426;&#21046;&#65292;&#39564;&#35777;&#20102;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#35813;&#27169;&#22411;&#30340;&#25512;&#33616;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04250</link><description>&lt;p&gt;
&#21487;&#32534;&#36753;&#29992;&#25143;&#26723;&#26696;&#30340;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Editable User Profiles for Controllable Text Recommendation. (arXiv:2304.04250v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;LACE&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#20010;&#24615;&#21270;&#30340;&#27010;&#24565;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22810;&#31181;&#20132;&#20114;&#26041;&#24335;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#26426;&#21046;&#65292;&#39564;&#35777;&#20102;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#23454;&#39564;&#20013;&#35813;&#27169;&#22411;&#30340;&#25512;&#33616;&#36136;&#37327;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#39640;&#36136;&#37327;&#25512;&#33616;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#20174;&#20132;&#20114;&#25968;&#25454;&#20013;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#12290;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#25552;&#20379;&#32473;&#29992;&#25143;&#25511;&#21046;&#25152;&#25509;&#25910;&#30340;&#25512;&#33616;&#30340;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LACE&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#27010;&#24565;&#20540;&#29942;&#39048;&#27169;&#22411;&#65292;&#29992;&#20110;&#21487;&#25511;&#25991;&#26412;&#25512;&#33616;&#12290;LACE&#22522;&#20110;&#29992;&#25143;&#20132;&#20114;&#30340;&#25991;&#26723;&#26816;&#32034;&#65292;&#23558;&#27599;&#20010;&#29992;&#25143;&#34920;&#31034;&#20026;&#31616;&#27905;&#30340;&#21487;&#35835;&#30340;&#27010;&#24565;&#38598;&#65292;&#24182;&#22522;&#20110;&#29992;&#25143;&#25991;&#26723;&#23398;&#20064;&#27010;&#24565;&#30340;&#20010;&#24615;&#21270;&#34920;&#31034;&#12290;&#35813;&#22522;&#20110;&#27010;&#24565;&#30340;&#29992;&#25143;&#26723;&#26696;&#34987;&#21033;&#29992;&#26469;&#20570;&#20986;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#35774;&#35745;&#36890;&#36807;&#36879;&#26126;&#30340;&#29992;&#25143;&#26723;&#26696;&#65292;&#25552;&#20379;&#20102;&#25511;&#21046;&#25512;&#33616;&#30340;&#22810;&#31181;&#30452;&#35266;&#20132;&#20114;&#26041;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#19977;&#20010;&#25512;&#33616;&#20219;&#21153;&#65288;&#28201;&#21551;&#21160;&#12289;&#20919;&#21551;&#21160;&#21644;&#38646;&#26679;&#26412;&#65289;&#30340;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#31163;&#32447;&#35780;&#20272;&#65292;&#39564;&#35777;&#20102;&#20174;LACE&#33719;&#24471;&#30340;&#25512;&#33616;&#36136;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22312;&#22312;&#32447;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;LACE&#30340;&#26377;&#25928;&#24615;&#21644;&#29992;&#25143;&#25511;&#21046;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for making high-quality recommendations often rely on learning latent representations from interaction data. These methods, while performant, do not provide ready mechanisms for users to control the recommendation they receive. Our work tackles this problem by proposing LACE, a novel concept value bottleneck model for controllable text recommendations. LACE represents each user with a succinct set of human-readable concepts through retrieval given user-interacted documents and learns personalized representations of the concepts based on user documents. This concept based user profile is then leveraged to make recommendations. The design of our model affords control over the recommendations through a number of intuitive interactions with a transparent user profile. We first establish the quality of recommendations obtained from LACE in an offline evaluation on three recommendation tasks spanning six datasets in warm-start, cold-start, and zero-shot setups. Next, we validate the 
&lt;/p&gt;</description></item><item><title>NusaCrowd&#26159;&#19968;&#20010;&#21360;&#23612;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30340;&#24320;&#28304;&#20513;&#35758;&#65292;&#24050;&#27719;&#38598;137&#20010;&#25968;&#25454;&#38598;&#21644;118&#20010;&#25968;&#25454;&#21152;&#36733;&#31243;&#24207;&#65292;&#20026;&#21360;&#23612;&#35821;&#21644;&#21360;&#24230;&#23612;&#35199;&#20122;&#26412;&#22320;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#22810;&#31181;&#23454;&#39564;&#25163;&#27573;&#12290;</title><link>http://arxiv.org/abs/2212.09648</link><description>&lt;p&gt;
NusaCrowd&#65306;&#21360;&#23612;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30340;&#24320;&#28304;&#20513;&#35758;
&lt;/p&gt;
&lt;p&gt;
NusaCrowd: Open Source Initiative for Indonesian NLP Resources. (arXiv:2212.09648v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09648
&lt;/p&gt;
&lt;p&gt;
NusaCrowd&#26159;&#19968;&#20010;&#21360;&#23612;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30340;&#24320;&#28304;&#20513;&#35758;&#65292;&#24050;&#27719;&#38598;137&#20010;&#25968;&#25454;&#38598;&#21644;118&#20010;&#25968;&#25454;&#21152;&#36733;&#31243;&#24207;&#65292;&#20026;&#21360;&#23612;&#35821;&#21644;&#21360;&#24230;&#23612;&#35199;&#20122;&#26412;&#22320;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#22810;&#31181;&#23454;&#39564;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;NusaCrowd&#65292;&#36825;&#26159;&#19968;&#20010;&#21327;&#20316;&#20513;&#35758;&#65292;&#26088;&#22312;&#25910;&#38598;&#21644;&#32479;&#19968;&#21360;&#23612;&#35821;&#35328;&#30340;&#29616;&#26377;&#36164;&#28304;&#65292;&#21253;&#25324;&#24320;&#25918;&#20197;&#21069;&#38750;&#20844;&#24320;&#30340;&#36164;&#28304;&#12290;&#36890;&#36807;&#35813;&#20513;&#35758;&#65292;&#25105;&#20204;&#27719;&#38598;&#20102;137&#20010;&#25968;&#25454;&#38598;&#21644;118&#20010;&#26631;&#20934;&#21270;&#25968;&#25454;&#21152;&#36733;&#31243;&#24207;&#12290;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#24050;&#32463;&#32463;&#36807;&#25163;&#21160;&#21644;&#33258;&#21160;&#35780;&#20272;&#65292;&#23427;&#20204;&#30340;&#20215;&#20540;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;NusaCrowd&#30340;&#25968;&#25454;&#25910;&#38598;&#20351;&#24471;&#21487;&#20197;&#21019;&#24314;&#21360;&#23612;&#35821;&#21644;&#21360;&#24230;&#23612;&#35199;&#20122;&#26412;&#22320;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#22522;&#20934;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#21360;&#23612;&#35821;&#21644;&#21360;&#24230;&#23612;&#35199;&#20122;&#26412;&#22320;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#22522;&#20934;&#30340;&#21019;&#24314;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#33268;&#21147;&#20110;&#25512;&#36827;&#23545;&#22312;&#20351;&#29992;&#24191;&#27867;&#30340;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#30340;&#21457;&#23637;&#65292;&#20174;&#32780;&#20351;&#20043;&#21463;&#21040;&#26356;&#22810;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present NusaCrowd, a collaborative initiative to collect and unify existing resources for Indonesian languages, including opening access to previously non-public resources. Through this initiative, we have brought together 137 datasets and 118 standardized data loaders. The quality of the datasets has been assessed manually and automatically, and their value is demonstrated through multiple experiments. NusaCrowd's data collection enables the creation of the first zero-shot benchmarks for natural language understanding and generation in Indonesian and the local languages of Indonesia. Furthermore, NusaCrowd brings the creation of the first multilingual automatic speech recognition benchmark in Indonesian and the local languages of Indonesia. Our work strives to advance natural language processing (NLP) research for languages that are under-represented despite being widely spoken.
&lt;/p&gt;</description></item><item><title>ClueReader&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#24322;&#26500;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#35889;&#27169;&#22411;&#21644;&#27880;&#24847;&#26426;&#21046;&#65292;&#36890;&#36807;&#27169;&#25311;&#31062;&#27597;&#32454;&#32990;&#27010;&#24565;&#26469;&#25552;&#21319;&#22810;&#36339;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2107.00841</link><description>&lt;p&gt;
ClueReader: &#22810;&#36339;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#24322;&#26500;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
ClueReader: Heterogeneous Graph Attention Network for Multi-hop Machine Reading Comprehension. (arXiv:2107.00841v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.00841
&lt;/p&gt;
&lt;p&gt;
ClueReader&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#24322;&#26500;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#65292;&#21033;&#29992;&#35889;&#27169;&#22411;&#21644;&#27880;&#24847;&#26426;&#21046;&#65292;&#36890;&#36807;&#27169;&#25311;&#31062;&#27597;&#32454;&#32990;&#27010;&#24565;&#26469;&#25552;&#21319;&#22810;&#36339;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#36339;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#22312;&#22810;&#20010;&#25991;&#26723;&#20043;&#38388;&#36827;&#34892;&#26356;&#22810;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#35889;&#27169;&#22411;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#27169;&#22411;&#30340;&#20998;&#26512;&#21644;&#25512;&#29702;&#19982;&#20154;&#31867;&#30340;&#19981;&#19968;&#33268;&#12290;&#21463;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#20013;&#31062;&#27597;&#32454;&#32990;&#30340;&#27010;&#24565;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ClueReader&#30340;&#24322;&#26500;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#27169;&#22411;&#65292;&#26469;&#27169;&#25311;&#31062;&#27597;&#32454;&#32990;&#30340;&#27010;&#24565;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#23558;&#22810;&#32423;&#34920;&#31034;&#20013;&#30340;&#35821;&#20041;&#29305;&#24449;&#32452;&#21512;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#33258;&#21160;&#38598;&#20013;&#25110;&#20943;&#36731;&#20449;&#24687;&#20197;&#36827;&#34892;&#25512;&#29702;&#12290;ClueReader&#30340;&#21629;&#21517;&#26159;&#23545;&#27169;&#22411;&#27169;&#24335;&#30340;&#38544;&#21947;&#65306;&#23427;&#23558;&#26597;&#35810;&#30340;&#20027;&#39064;&#35270;&#20026;&#32447;&#32034;&#30340;&#36215;&#28857;&#65292;&#23558;&#25512;&#29702;&#23454;&#20307;&#35270;&#20026;&#26725;&#26753;&#28857;&#65292;&#23558;&#28508;&#22312;&#20505;&#36873;&#23454;&#20307;&#35270;&#20026;&#31062;&#27597;&#32454;&#32990;&#65292;&#32447;&#32034;&#32467;&#26463;&#20043;&#21518;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-hop machine reading comprehension is a challenging task in natural language processing as it requires more reasoning ability across multiple documents. Spectral models based on graph convolutional networks have shown good inferring abilities and lead to competitive results. However, the analysis and reasoning of some are inconsistent with those of humans. Inspired by the concept of grandmother cells in cognitive neuroscience, we propose a heterogeneous graph attention network model named ClueReader to imitate the grandmother cell concept. The model is designed to assemble the semantic features in multi-level representations and automatically concentrate or alleviate information for reasoning through the attention mechanism. The name ClueReader is a metaphor for the pattern of the model: it regards the subjects of queries as the starting points of clues, takes the reasoning entities as bridge points, considers the latent candidate entities as grandmother cells, and the clues end u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#35821;&#20041;&#32593;&#32476;&#20998;&#26512;&#22312;&#32447;&#26032;&#38395;&#23545;&#28040;&#36153;&#32773;&#20449;&#24515;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#39044;&#27979;&#28040;&#36153;&#32773;&#23545;&#32463;&#27982;&#24418;&#21183;&#30340;&#21028;&#26029;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#34917;&#20805;&#26041;&#27861;&#26469;&#20272;&#35745;&#28040;&#36153;&#32773;&#20449;&#24515;&#12290;</title><link>http://arxiv.org/abs/2105.04900</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#26032;&#38395;&#30340;&#35821;&#20041;&#32593;&#32476;&#20998;&#26512;&#39044;&#27979;&#28040;&#36153;&#32773;&#20449;&#24515;
&lt;/p&gt;
&lt;p&gt;
Forecasting consumer confidence through semantic network analysis of online news. (arXiv:2105.04900v2 [econ.GN] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.04900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#35821;&#20041;&#32593;&#32476;&#20998;&#26512;&#22312;&#32447;&#26032;&#38395;&#23545;&#28040;&#36153;&#32773;&#20449;&#24515;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#39044;&#27979;&#28040;&#36153;&#32773;&#23545;&#32463;&#27982;&#24418;&#21183;&#30340;&#21028;&#26029;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#34917;&#20805;&#26041;&#27861;&#26469;&#20272;&#35745;&#28040;&#36153;&#32773;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35821;&#20041;&#32593;&#32476;&#20998;&#26512;&#30740;&#31350;&#22312;&#32447;&#26032;&#38395;&#23545;&#31038;&#20250;&#32463;&#27982;&#28040;&#36153;&#32773;&#24577;&#24230;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;&#35206;&#30422;&#22235;&#24180;&#30340;&#24847;&#22823;&#21033;&#23186;&#20307;&#19978;&#30340;&#36229;&#36807;180&#19975;&#31687;&#22312;&#32447;&#25991;&#31456;&#65292;&#25105;&#20204;&#35745;&#31639;&#29305;&#23450;&#32463;&#27982;&#30456;&#20851;&#20851;&#38190;&#35789;&#30340;&#35821;&#20041;&#37325;&#35201;&#24615;&#65292;&#20197;&#30830;&#23450;&#25991;&#31456;&#20013;&#20986;&#29616;&#30340;&#35789;&#35821;&#26159;&#21542;&#33021;&#22815;&#39044;&#27979;&#28040;&#36153;&#32773;&#23545;&#32463;&#27982;&#24418;&#21183;&#21644;&#28040;&#36153;&#32773;&#20449;&#24515;&#25351;&#25968;&#30340;&#21028;&#26029;&#12290;&#25105;&#20204;&#36816;&#29992;&#21019;&#26032;&#26041;&#27861;&#20998;&#26512;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#65292;&#32467;&#21512;&#20102;&#25991;&#26412;&#25366;&#25496;&#21644;&#31038;&#20250;&#32593;&#32476;&#20998;&#26512;&#30340;&#26041;&#27861;&#21644;&#24037;&#20855;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#25351;&#26631;&#23545;&#20110;&#21028;&#26029;&#24403;&#21069;&#23478;&#24237;&#21644;&#22269;&#23478;&#24773;&#20917;&#20855;&#26377;&#36739;&#24378;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#25351;&#26631;&#20026;&#28040;&#36153;&#32773;&#20449;&#24515;&#30340;&#20272;&#35745;&#25552;&#20379;&#20102;&#19968;&#31181;&#34917;&#20805;&#26041;&#27861;&#65292;&#20943;&#36731;&#20102;&#20256;&#32479;&#22522;&#20110;&#35843;&#26597;&#30340;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research studies the impact of online news on social and economic consumer perceptions through semantic network analysis. Using over 1.8 million online articles on Italian media covering four years, we calculate the semantic importance of specific economic-related keywords to see if words appearing in the articles could anticipate consumers' judgments about the economic situation and the Consumer Confidence Index. We use an innovative approach to analyze big textual data, combining methods and tools of text mining and social network analysis. Results show a strong predictive power for the judgments about the current households and national situation. Our indicator offers a complementary approach to estimating consumer confidence, lessening the limitations of traditional survey-based methods.
&lt;/p&gt;</description></item></channel></rss>