<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20419;&#36827;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#65292;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#20102;&#35299;&#20102;&#20182;&#20204;&#23545;LLMs&#30340;&#38656;&#27714;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Talk2Care&#30340;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.09357</link><description>&lt;p&gt;
Talk2Care: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20419;&#36827;&#24322;&#27493;&#24739;&#32773;-&#21307;&#29983;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Talk2Care: Facilitating Asynchronous Patient-Provider Communication with Large-Language-Model. (arXiv:2309.09357v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20419;&#36827;&#24739;&#32773;&#21644;&#21307;&#29983;&#20043;&#38388;&#30340;&#24322;&#27493;&#36890;&#20449;&#65292;&#36890;&#36807;&#35775;&#35848;&#30740;&#31350;&#20102;&#35299;&#20102;&#20182;&#20204;&#23545;LLMs&#30340;&#38656;&#27714;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Talk2Care&#30340;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26377;&#22823;&#37327;&#30340;&#36828;&#31243;&#21307;&#30103;&#24212;&#29992;&#31243;&#24207;&#26469;&#24110;&#21161;&#23478;&#24237;&#20013;&#30340;&#32769;&#24180;&#20154;&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;&#65292;&#20294;&#22522;&#26412;&#30340;&#28040;&#24687;&#21644;&#30005;&#35805;&#20173;&#28982;&#26159;&#26368;&#24120;&#35265;&#30340;&#36890;&#20449;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#26377;&#38480;&#30340;&#21487;&#29992;&#24615;&#12289;&#20449;&#24687;&#20002;&#22833;&#21644;&#27969;&#31243;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#12290;&#20419;&#36827;&#24739;&#32773;-&#21307;&#29983;&#36890;&#20449;&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21450;&#20854;&#24378;&#22823;&#30340;&#33258;&#28982;&#23545;&#35805;&#21644;&#25688;&#35201;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLMs&#22312;&#36890;&#20449;&#36807;&#31243;&#20013;&#30340;&#20316;&#29992;&#36824;&#23384;&#22312;&#26377;&#38480;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#20004;&#39033;&#35775;&#35848;&#30740;&#31350;&#65292;&#20998;&#21035;&#19982;&#32769;&#24180;&#20154;(N=10)&#21644;&#21307;&#30103;&#25552;&#20379;&#32773;(N=9)&#36827;&#34892;&#20102;&#20132;&#27969;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#22312;&#24739;&#32773;-&#21307;&#29983;&#24322;&#27493;&#36890;&#20449;&#20013;&#23545;LLMs&#30340;&#38656;&#27714;&#21644;&#26426;&#20250;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;LLM&#39537;&#21160;&#30340;&#36890;&#20449;&#31995;&#32479;Talk2Care&#65292;&#24182;&#20026;&#20004;&#20010;&#32676;&#20307;&#35774;&#35745;&#20102;&#20132;&#20114;&#32452;&#20214;: (1) &#23545;&#20110;&#32769;&#24180;&#20154;&#65292;&#25105;&#20204;&#21033;&#29992;&#35821;&#38899;&#21161;&#25163;&#30340;&#20415;&#21033;&#24615;&#21644;&#26131;&#20110;&#33719;&#21462;&#24615;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;LLM&#39537;&#21160;&#30340;&#35821;&#38899;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs' role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered VA i
&lt;/p&gt;</description></item><item><title>Agents&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#25903;&#25345;&#26500;&#24314;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#30340;&#21508;&#31181;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#25509;&#21475;&#21644;&#23545;&#30740;&#31350;&#20154;&#21592;&#30340;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07870</link><description>&lt;p&gt;
&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#30340;&#24320;&#28304;&#26694;&#26550;&#65306;Agents
&lt;/p&gt;
&lt;p&gt;
Agents: An Open-source Framework for Autonomous Language Agents. (arXiv:2309.07870v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07870
&lt;/p&gt;
&lt;p&gt;
Agents&#26159;&#19968;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#25903;&#25345;&#26500;&#24314;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#30340;&#21508;&#31181;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#29992;&#25143;&#21451;&#22909;&#30340;&#25509;&#21475;&#21644;&#23545;&#30740;&#31350;&#20154;&#21592;&#30340;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39640;&#32423;&#36827;&#23637;&#20351;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#26500;&#24314;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#65292;&#36825;&#20123;&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#33258;&#21160;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#24182;&#19982;&#29615;&#22659;&#12289;&#20154;&#31867;&#21644;&#20854;&#20182;&#20195;&#29702;&#20132;&#20114;&#12290;&#25105;&#20204;&#23558;&#35821;&#35328;&#20195;&#29702;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#24182;&#21457;&#24067;Agents&#65292;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#26088;&#22312;&#21521;&#26356;&#24191;&#27867;&#30340;&#38750;&#19987;&#19994;&#20154;&#22763;&#24320;&#25918;&#36825;&#20123;&#36827;&#23637;&#12290;Agents&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#25903;&#25345;&#37325;&#35201;&#21151;&#33021;&#65292;&#21253;&#25324;&#35268;&#21010;&#12289;&#35760;&#24518;&#12289;&#24037;&#20855;&#20351;&#29992;&#12289;&#22810;&#20195;&#29702;&#36890;&#20449;&#21644;&#32454;&#31890;&#24230;&#30340;&#31526;&#21495;&#25511;&#21046;&#12290;Agents&#29992;&#25143;&#21451;&#22909;&#65292;&#20351;&#38750;&#19987;&#19994;&#20154;&#22763;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#32534;&#20889;&#22826;&#22810;&#20195;&#30721;&#30340;&#24773;&#20917;&#19979;&#26500;&#24314;&#12289;&#23450;&#21046;&#12289;&#27979;&#35797;&#12289;&#35843;&#20248;&#21644;&#37096;&#32626;&#26368;&#20808;&#36827;&#30340;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#12290;&#35813;&#24211;&#20063;&#23545;&#30740;&#31350;&#20154;&#21592;&#21451;&#22909;&#65292;&#20854;&#27169;&#22359;&#21270;&#35774;&#35745;&#20351;&#20854;&#26131;&#20110;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances on large language models (LLMs) enable researchers and developers to build autonomous language agents that can automatically solve various tasks and interact with environments, humans, and other agents using natural language interfaces. We consider language agents as a promising direction towards artificial general intelligence and release Agents, an open-source library with the goal of opening up these advances to a wider non-specialist audience. Agents is carefully engineered to support important features including planning, memory, tool usage, multi-agent communication, and fine-grained symbolic control. Agents is user-friendly as it enables non-specialists to build, customize, test, tune, and deploy state-of-the-art autonomous language agents without much coding. The library is also research-friendly as its modularized design makes it easily extensible for researchers. Agents is available at https://github.com/aiwaves-cn/agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#65288;CELMOC&#65289;&#65292;&#36890;&#36807;&#20803;&#27169;&#22411;&#39044;&#27979;&#22312;&#19981;&#21516;&#36755;&#20837;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#20302;&#25104;&#26412;&#19979;&#23454;&#29616;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06077</link><description>&lt;p&gt;
&#39134;&#25293;&#25110;&#22823;&#28846;&#65311;&#36890;&#36807;&#20803;&#27169;&#22411;&#36873;&#25321;&#32463;&#27982;&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fly-Swat or Cannon? Cost-Effective Language Model Choice via Meta-Modeling. (arXiv:2308.06077v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#65288;CELMOC&#65289;&#65292;&#36890;&#36807;&#20803;&#27169;&#22411;&#39044;&#27979;&#22312;&#19981;&#21516;&#36755;&#20837;&#19978;&#34920;&#29616;&#33391;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#20302;&#25104;&#26412;&#19979;&#23454;&#29616;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#20013;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#12290;&#23545;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21487;&#20197;&#23558;&#36755;&#20837;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#65292;&#36890;&#36807;LM&#30340;&#36755;&#20986;&#26469;&#25552;&#21462;&#35299;&#20915;&#26041;&#26696;&#12290;LM&#30340;&#24615;&#33021;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780;&#19981;&#26029;&#25552;&#39640;&#65292;&#20294;&#21516;&#26102;&#26597;&#35810;&#36234;&#26469;&#36234;&#22823;&#30340;&#27169;&#22411;&#30340;&#32463;&#27982;&#25104;&#26412;&#20063;&#22312;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#19981;&#26159;&#25152;&#26377;&#30340;&#36755;&#20837;&#37117;&#24456;&#38590;&#65306;&#26377;&#20123;&#36755;&#20837;&#38656;&#35201;&#26356;&#22823;&#30340;LM&#25165;&#33021;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#23545;&#20110;&#20854;&#20182;&#36755;&#20837;&#65292;&#36739;&#23567;&#30340;LM&#23601;&#36275;&#22815;&#20102;&#12290;&#22522;&#20110;&#36825;&#20010;&#20107;&#23454;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32463;&#27982;&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#26694;&#26550;&#65288;CELMOC&#65289;&#12290;&#32473;&#23450;&#19968;&#32452;&#36755;&#20837;&#21644;&#19968;&#32452;&#20505;&#36873;LM&#65292;CELMOC&#26681;&#25454;&#25152;&#35859;&#30340;&#20803;&#27169;&#22411;&#32874;&#26126;&#22320;&#23558;&#27599;&#20010;&#36755;&#20837;&#20998;&#37197;&#32473;&#19968;&#20010;&#22312;&#35813;&#36755;&#20837;&#19978;&#39044;&#27979;&#34920;&#29616;&#33391;&#22909;&#30340;LM&#65292;&#20197;&#26399;&#22312;&#20302;&#25104;&#26412;&#19979;&#23454;&#29616;&#39640;&#25972;&#20307;&#24615;&#33021;&#12290;&#29992;&#25143;&#21487;&#20197;&#28789;&#27963;&#35843;&#25972;&#25104;&#26412;&#19982;&#24615;&#33021;&#30340;&#26435;&#34913;&#12290;&#36873;&#39033;&#21253;&#25324;&#65292;&#26368;&#22823;&#21270;&#24635;&#20307;&#24615;&#33021;&#65288;&#25110;&#22788;&#29702;&#36755;&#20837;&#30340;&#25968;&#37327;&#65289;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative language models (LMs) have become omnipresent across data science. For a wide variety of tasks, inputs can be phrased as natural language prompts for an LM, from whose output the solution can then be extracted. LM performance has consistently been increasing with model size - but so has the monetary cost of querying the ever larger models. Importantly, however, not all inputs are equally hard: some require larger LMs for obtaining a satisfactory solution, whereas for others smaller LMs suffice. Based on this fact, we design a framework for Cost-Effective Language Model Choice (CELMOC). Given a set of inputs and a set of candidate LMs, CELMOC judiciously assigns each input to an LM predicted to do well on the input according to a so-called meta-model, aiming to achieve high overall performance at low cost. The cost-performance trade-off can be flexibly tuned by the user. Options include, among others, maximizing total expected performance (or the number of processed inputs) w
&lt;/p&gt;</description></item><item><title>VisoGender&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#32844;&#19994;&#30456;&#20851;&#24615;&#21035;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#32570;&#20047;&#27491;&#30830;&#35299;&#26512;&#22797;&#26434;&#22330;&#26223;&#20013;&#24615;&#21035;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#29983;&#25104;&#23383;&#24149;&#30340;&#27169;&#22411;&#36890;&#24120;&#27604;&#31867;&#20284;CLIP&#30340;&#27169;&#22411;&#26356;&#31934;&#30830;&#21644;&#26356;&#23569;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2306.12424</link><description>&lt;p&gt;
VisoGender&#65306;&#19968;&#20221;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;-&#25991;&#26412;&#20195;&#35789;&#35299;&#26512;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution. (arXiv:2306.12424v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12424
&lt;/p&gt;
&lt;p&gt;
VisoGender&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#32844;&#19994;&#30456;&#20851;&#24615;&#21035;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#26174;&#31034;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#32570;&#20047;&#27491;&#30830;&#35299;&#26512;&#22797;&#26434;&#22330;&#26223;&#20013;&#24615;&#21035;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#29983;&#25104;&#23383;&#24149;&#30340;&#27169;&#22411;&#36890;&#24120;&#27604;&#31867;&#20284;CLIP&#30340;&#27169;&#22411;&#26356;&#31934;&#30830;&#21644;&#26356;&#23569;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;VisoGender&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#32844;&#19994;&#30456;&#20851;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#21463;Winograd&#21644;Winogender&#27169;&#24335;&#30340;&#21551;&#21457;&#65292;&#20854;&#20013;&#27599;&#20010;&#22270;&#20687;&#37117;&#19982;&#21253;&#21547;&#22330;&#26223;&#20013;&#20027;&#35821;&#21644;&#23486;&#35821;&#20195;&#35789;&#20851;&#31995;&#30340;&#26631;&#39064;&#30456;&#20851;&#32852;&#12290;VisoGender&#22312;&#32844;&#19994;&#35282;&#33394;&#20013;&#24179;&#34913;&#20102;&#24615;&#21035;&#20195;&#34920;&#65292;&#25903;&#25345;&#20004;&#31181;&#20559;&#35265;&#35780;&#20272;&#26041;&#24335;&#65306;i&#65289;&#35299;&#20915;&#20559;&#35265;&#65292;&#25105;&#20204;&#35780;&#20272;&#30007;&#24615;&#21644;&#22899;&#24615;&#35299;&#20915;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#65307;ii&#65289;&#26816;&#32034;&#20559;&#35265;&#65292;&#25105;&#20204;&#27604;&#36739;&#22312;&#24615;&#21035;&#20013;&#31435;&#30340;&#25628;&#32034;&#26597;&#35810;&#20013;&#26816;&#32034;&#21040;&#30340;&#30007;&#24615;&#21644;&#22899;&#24615;&#19987;&#19994;&#20154;&#21592;&#30340;&#27604;&#20363;&#12290;&#25105;&#20204;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#32570;&#20047;&#27491;&#30830;&#35299;&#26512;&#22797;&#26434;&#22330;&#26223;&#20013;&#24615;&#21035;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#34429;&#28982;&#24615;&#21035;&#20559;&#35265;&#30340;&#26041;&#21521;&#21644;&#24133;&#24230;&#21462;&#20915;&#20110;&#20219;&#21153;&#21644;&#35780;&#20272;&#30340;&#27169;&#22411;&#65292;&#20294;&#29983;&#25104;&#23383;&#24149;&#30340;&#27169;&#22411;&#36890;&#24120;&#27604;&#31867;&#20284;CLIP&#30340;&#27169;&#22411;&#26356;&#31934;&#30830;&#21644;&#26356;&#23569;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce VisoGender, a novel dataset for benchmarking gender bias in vision-language models. We focus on occupation-related gender biases, inspired by Winograd and Winogender schemas, where each image is associated with a caption containing a pronoun relationship of subjects and objects in the scene. VisoGender is balanced by gender representation in professional roles, supporting bias evaluation in two ways: i) resolution bias, where we evaluate the difference between gender resolution accuracies for men and women and ii) retrieval bias, where we compare ratios of male and female professionals retrieved for a gender-neutral search query. We benchmark several state-of-the-art vision-language models and find that they lack the reasoning abilities to correctly resolve gender in complex scenes. While the direction and magnitude of gender bias depends on the task and the model being evaluated, captioning models generally are more accurate and less biased than CLIP-like models. Dataset 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#21270;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#19988;&#27979;&#35797;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#25928;&#26524;&#26174;&#33879;&#65292;&#21516;&#26102;&#26377;&#25928;&#32531;&#35299;&#20102;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#27874;&#21160;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03241</link><description>&lt;p&gt;
&#29702;&#35299;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#23545;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effectiveness of Early Weight Averaging for Training Large Language Models. (arXiv:2306.03241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#21270;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#19988;&#27979;&#35797;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#25928;&#26524;&#26174;&#33879;&#65292;&#21516;&#26102;&#26377;&#25928;&#32531;&#35299;&#20102;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#27874;&#21160;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#20215;&#39640;&#26114;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#33267;&#25910;&#25947;&#24182;&#19981;&#39640;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24819;&#27861;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27839;&#30528;&#36712;&#36857;&#36827;&#34892;&#26816;&#26597;&#28857;&#24179;&#22343;&#21270;&#65292;&#20197;&#22312;&#27169;&#22411;&#25910;&#25947;&#20043;&#21069;&#25552;&#39640;&#20854;&#36136;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35757;&#32451;&#25110;&#25512;&#29702;&#26399;&#38388;&#19981;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20855;&#26377;10&#20159;&#21040;120&#20159;&#21442;&#25968;&#30340;Pythia LLM&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#24182;&#35777;&#26126;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#21644;&#20013;&#26399;&#38454;&#27573;&#65292;&#36825;&#31181;&#24819;&#27861;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#24182;&#25552;&#39640;&#27979;&#35797;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#25928;&#26524;&#12290;&#25439;&#22833;&#27874;&#21160;&#26159;LLM&#35757;&#32451;&#20013;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65307;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#20004;&#31181;&#22522;&#30784;&#36712;&#36857;&#30340;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#24179;&#22343;&#21270;&#21487;&#20197;&#32531;&#35299;&#36825;&#20004;&#31181;&#24773;&#20917;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#19968;&#20010;&#25317;&#26377;69&#20159;&#21442;&#25968;&#30340;LLM&#65292;&#25105;&#20204;&#30340;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#21270;&#37197;&#26041;&#21487;&#20197;&#33410;&#30465;&#39640;&#36798;4200&#23567;&#26102;&#30340;GPU&#26102;&#38388;&#65292;&#36825;&#23545;&#20113;&#35745;&#31639;&#25104;&#26412;&#26469;&#35828;&#26159;&#26174;&#33879;&#30340;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training LLMs is expensive, and recent evidence indicates training all the way to convergence is inefficient. In this paper, we investigate the ability of a simple idea, checkpoint averaging along the trajectory of a training run to improve the quality of models before they have converged. This approach incurs no extra cost during training or inference. Specifically, we analyze the training trajectories of Pythia LLMs with 1 to 12 billion parameters and demonstrate that, particularly during the early to mid stages of training, this idea accelerates convergence and improves both test and zero-shot generalization. Loss spikes are a well recognized problem in LLM training; in our analysis we encountered two instances of this in the underlying trajectories, and both instances were mitigated by our averaging.  For a 6.9B parameter LLM, for example, our early weight averaging recipe can save upto 4200 hours of GPU time, which corresponds to significant savings in cloud compute costs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;InteR&#65292;&#36890;&#36807;&#25628;&#32034;&#24341;&#25806;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#20419;&#36827;&#30693;&#35782;&#31934;&#28860;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07402</link><description>&lt;p&gt;
&#25628;&#32034;&#24341;&#25806;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38388;&#30340;&#20132;&#20114;&#20248;&#21270;&#30693;&#35782;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Knowledge Refinement via Interaction Between Search Engines and Large Language Models. (arXiv:2305.07402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;InteR&#65292;&#36890;&#36807;&#25628;&#32034;&#24341;&#25806;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#20419;&#36827;&#30693;&#35782;&#31934;&#28860;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#22312;&#20174;&#22823;&#37327;&#25968;&#25454;&#20013;&#23450;&#20301;&#30456;&#20851;&#36164;&#28304;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#65292;&#20854;&#24212;&#29992;&#24050;&#20174;&#20256;&#32479;&#30693;&#35782;&#24211;&#21457;&#23637;&#33267;&#29616;&#20195;&#25628;&#32034;&#24341;&#25806;&#65288;SEs&#65289;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#36827;&#19968;&#27493;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#19982;&#25628;&#32034;&#31995;&#32479;&#20132;&#20114;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20102;&#35813;&#39046;&#22495;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;LLMs&#21644;SEs&#30340;&#20248;&#32570;&#28857;&#65292;&#24378;&#35843;&#23427;&#20204;&#22312;&#29702;&#35299;&#29992;&#25143;&#26597;&#35810;&#21644;&#26816;&#32034;&#26368;&#26032;&#20449;&#24687;&#26041;&#38754;&#30340;&#21508;&#33258;&#20248;&#21183;&#12290;&#20026;&#20102;&#21033;&#29992;&#20004;&#31181;&#33539;&#20363;&#30340;&#20248;&#21183;&#24182;&#36991;&#20813;&#20854;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InteR&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;SEs&#21644;LLMs&#20043;&#38388;&#30340;&#20132;&#20114;&#20419;&#36827;&#30693;&#35782;&#31934;&#28860;&#30340;&#26032;&#26694;&#26550;&#12290; InteR&#20351;SEs&#33021;&#22815;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#25688;&#35201;&#26469;&#35843;&#25972;&#26597;&#35810;&#65292;&#21516;&#26102;&#20351;LLMs&#33021;&#22815;&#20351;&#29992;SE&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#26469;&#22686;&#24378;&#25552;&#31034;&#12290;&#36825;&#31181;&#36845;&#20195;&#30340;&#31934;&#28860;&#36807;&#31243;&#22686;&#24378;&#20102;SEs&#21644;LLMs&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#20934;&#30830;&#30340;&#26816;&#32034;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information retrieval (IR) plays a crucial role in locating relevant resources from vast amounts of data, and its applications have evolved from traditional knowledge bases to modern search engines (SEs). The emergence of large language models (LLMs) has further revolutionized the field by enabling users to interact with search systems in natural language. In this paper, we explore the advantages and disadvantages of LLMs and SEs, highlighting their respective strengths in understanding user-issued queries and retrieving up-to-date information. To leverage the benefits of both paradigms while circumventing their limitations, we propose InteR, a novel framework that facilitates knowledge refinement through interaction between SEs and LLMs. InteR allows SEs to refine knowledge in query using LLM-generated summaries and enables LLMs to enhance prompts using SE-retrieved documents. This iterative refinement process augments the inputs of SEs and LLMs, leading to more accurate retrieval. Ex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;USNID&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#30340;&#26032;&#24847;&#22270;&#21457;&#29616;&#65292;&#35299;&#20915;&#20102;&#21033;&#29992;&#26377;&#38480;&#25110;&#26080;&#26631;&#35760;&#25968;&#25454;&#26102;&#38590;&#20197;&#25429;&#25417;&#22797;&#26434;&#35821;&#20041;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#32858;&#31867;&#26426;&#21046;&#26469;&#25552;&#39640;&#33258;&#25105;&#30417;&#30563;&#30446;&#26631;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#21457;&#29616;&#32454;&#31890;&#24230;&#30340;&#24847;&#22270;&#31751;&#12290;</title><link>http://arxiv.org/abs/2304.07699</link><description>&lt;p&gt;
USNID: &#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#26032;&#24847;&#22270;&#21457;&#29616;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
USNID: A Framework for Unsupervised and Semi-supervised New Intent Discovery. (arXiv:2304.07699v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;USNID&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#30340;&#26032;&#24847;&#22270;&#21457;&#29616;&#65292;&#35299;&#20915;&#20102;&#21033;&#29992;&#26377;&#38480;&#25110;&#26080;&#26631;&#35760;&#25968;&#25454;&#26102;&#38590;&#20197;&#25429;&#25417;&#22797;&#26434;&#35821;&#20041;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#32858;&#31867;&#26426;&#21046;&#26469;&#25552;&#39640;&#33258;&#25105;&#30417;&#30563;&#30446;&#26631;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#21457;&#29616;&#32454;&#31890;&#24230;&#30340;&#24847;&#22270;&#31751;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#24847;&#22270;&#21457;&#29616;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#20351;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#38656;&#27714;&#24182;&#25552;&#20379;&#21451;&#22909;&#30340;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#26377;&#38480;&#25110;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#25429;&#25417;&#31163;&#25955;&#25991;&#26412;&#34920;&#31034;&#30340;&#22797;&#26434;&#35821;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;USNID&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#26032;&#24847;&#22270;&#21457;&#29616;&#65292;&#20855;&#26377;&#19977;&#20010;&#20851;&#38190;&#25216;&#26415;&#65306;&#20805;&#20998;&#21033;&#29992;&#26080;&#30417;&#30563;&#25110;&#21322;&#30417;&#30563;&#25968;&#25454;&#25366;&#25496;&#27973;&#23618;&#35821;&#20041;&#30456;&#20284;&#24615;&#20851;&#31995;&#65307;&#35774;&#35745;&#32858;&#31867;&#26426;&#21046;&#35299;&#20915;&#31751;&#20998;&#37197;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65307;&#25429;&#33719;&#26080;&#30417;&#30563;&#25110;&#21322;&#30417;&#30563;&#25968;&#25454;&#20013;&#30340;&#39640;&#32423;&#35821;&#20041;&#65292;&#36890;&#36807;&#21516;&#26102;&#20248;&#21270;&#32858;&#31867;&#21644;&#33258;&#25105;&#30417;&#30563;&#26469;&#21457;&#29616;&#32454;&#31890;&#24230;&#30340;&#24847;&#22270;&#31751;&#12290;
&lt;/p&gt;
&lt;p&gt;
New intent discovery is of great value to natural language processing, allowing for a better understanding of user needs and providing friendly services. However, most existing methods struggle to capture the complicated semantics of discrete text representations when limited or no prior knowledge of labeled data is available. To tackle this problem, we propose a novel framework called USNID for unsupervised and semi-supervised new intent discovery, which has three key technologies. First, it takes full use of unsupervised or semi-supervised data to mine shallow semantic similarity relations and provide well-initialized representations for clustering. Second, it designs a centroid-guided clustering mechanism to address the issue of cluster allocation inconsistency and provide high-quality self-supervised targets for representation learning. Third, it captures high-level semantics in unsupervised or semi-supervised data to discover fine-grained intent-wise clusters by optimizing both cl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#32447;&#24615;&#35268;&#21010;&#21333;&#35789;&#38382;&#39064;&#36716;&#25442;&#20026;&#25968;&#23398;&#20844;&#24335;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#36755;&#20837;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#24182;&#22686;&#24378;&#36755;&#20837;&#20197;&#31361;&#20986;&#36825;&#20123;&#23454;&#20307;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#65292;&#36194;&#24471;&#20102;NL4Opt&#31454;&#36187;&#29983;&#25104;&#36187;&#36947;&#30340;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2212.13201</link><description>&lt;p&gt;
&#36755;&#20837;&#21629;&#21517;&#23454;&#20307;&#33258;&#21160;&#35299;&#26512;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Highlighting Named Entities in Input for Auto-Formulation of Optimization Problems. (arXiv:2212.13201v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#32447;&#24615;&#35268;&#21010;&#21333;&#35789;&#38382;&#39064;&#36716;&#25442;&#20026;&#25968;&#23398;&#20844;&#24335;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#36755;&#20837;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#24182;&#22686;&#24378;&#36755;&#20837;&#20197;&#31361;&#20986;&#36825;&#20123;&#23454;&#20307;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#65292;&#36194;&#24471;&#20102;NL4Opt&#31454;&#36187;&#29983;&#25104;&#36187;&#36947;&#30340;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36816;&#31609;&#23398;&#26159;&#23558;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#24314;&#27169;&#20026;&#25968;&#23398;&#20248;&#21270;&#38382;&#39064;&#26469;&#35299;&#20915;&#30340;&#12290;&#34429;&#28982;&#35299;&#20915;&#25968;&#23398;&#31995;&#32479;&#30340;&#38382;&#39064;&#26159;&#30001;&#20998;&#26512;&#36719;&#20214;&#23436;&#25104;&#30340;&#65292;&#20294;&#23558;&#38382;&#39064;&#20316;&#20026;&#19968;&#32452;&#25968;&#23398;&#25805;&#20316;&#36827;&#34892;&#34920;&#36798;&#36890;&#24120;&#26159;&#30001;&#39046;&#22495;&#19987;&#23478;&#25163;&#21160;&#23436;&#25104;&#30340;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26174;&#31034;&#20986;&#23558;&#25991;&#26412;&#38382;&#39064;&#25551;&#36848;&#36716;&#25442;&#20026;&#30456;&#24212;&#30340;&#25968;&#23398;&#20844;&#24335;&#30340;&#21069;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#32447;&#24615;&#35268;&#21010;&#21333;&#35789;&#38382;&#39064;&#36716;&#25442;&#20026;&#25968;&#23398;&#20844;&#24335;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#36755;&#20837;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#24182;&#22686;&#24378;&#36755;&#20837;&#20197;&#31361;&#20986;&#36825;&#20123;&#23454;&#20307;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;NL4Opt&#31454;&#36187;&#30340;&#25152;&#26377;&#25552;&#20132;&#20013;&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#29983;&#25104;&#36187;&#36947;&#30340;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Operations research deals with modeling and solving real-world problems as mathematical optimization problems. While solving mathematical systems is accomplished by analytical software, formulating a problem as a set of mathematical operations has been typically done manually by domain experts. Recent machine learning methods have shown promise in converting textual problem descriptions to corresponding mathematical formulations. This paper presents an approach that converts linear programming word problems into mathematical formulations. We leverage the named entities in the input and augment the input to highlight these entities. Our approach achieves the highest accuracy among all submissions to the NL4Opt Competition, securing first place in the generation track.
&lt;/p&gt;</description></item></channel></rss>