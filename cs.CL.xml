<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#20351;&#29992;&#26377;&#25928;&#30340;&#24037;&#20316;&#27969;&#21517;&#31216;&#21644;&#34892;&#21160;&#35745;&#21010;&#32534;&#30721;&#26469;&#23436;&#25104;&#22810;&#27493;&#39588;&#20219;&#21153;&#30340;&#25191;&#34892;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#21487;&#38752;&#25191;&#34892;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#22810;&#27493;&#39588;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.01729</link><description>&lt;p&gt;
&#29992;&#24037;&#20316;&#27969;&#31243;&#19982;&#34892;&#21160;&#35745;&#21010;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving Generalization in Task-oriented Dialogues with Workflows and Action Plans. (arXiv:2306.01729v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#20351;&#29992;&#26377;&#25928;&#30340;&#24037;&#20316;&#27969;&#21517;&#31216;&#21644;&#34892;&#21160;&#35745;&#21010;&#32534;&#30721;&#26469;&#23436;&#25104;&#22810;&#27493;&#39588;&#20219;&#21153;&#30340;&#25191;&#34892;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#21487;&#38752;&#25191;&#34892;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#22810;&#27493;&#39588;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#38590;&#28857;&#22312;&#20110;&#28041;&#21450;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#12289;&#20174;&#29992;&#25143;&#22788;&#25910;&#38598;&#20449;&#24687;&#12289;&#25191;&#34892;API&#35843;&#29992;&#20197;&#21450;&#29983;&#25104;&#26377;&#29992;&#27969;&#30021;&#30340;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#24517;&#39035;&#25353;&#29305;&#23450;&#39034;&#24207;&#22312;&#22810;&#20010;&#27493;&#39588;&#20013;&#27491;&#30830;&#22320;&#23436;&#25104;&#25152;&#26377;&#36825;&#20123;&#20107;&#24773;&#12290;&#34429;&#28982;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#24494;&#35843;&#26469;&#21019;&#24314;&#29983;&#25104;&#27969;&#30021;&#25991;&#26412;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20195;&#29702;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#65292;&#20165;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#26080;&#27861;&#21487;&#38752;&#22320;&#25191;&#34892;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#22810;&#27493;&#39588;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#32473;&#20986;&#21040;text2text&#36716;&#25442;&#22120;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#20854;&#20013;&#21253;&#25324;&#24050;&#30693;&#30340;&#26377;&#25928;&#24037;&#20316;&#27969;&#21517;&#31216;&#21644;&#34892;&#21160;&#35745;&#21010;&#12290;&#34892;&#21160;&#35745;&#21010;&#21253;&#25324;&#23436;&#25104;&#20219;&#21153;&#25152;&#38656;&#30340;&#34892;&#21160;&#24207;&#21015;&#65292;&#24182;&#32534;&#30721;&#20026;&#31616;&#21333;&#30340;&#20851;&#38190;&#23383;&#24207;&#21015;&#65288;&#20363;&#22914;&#65292;&#39564;&#35777;&#36523;&#20221;&#65292;&#25289;&#36215;&#36134;&#25143;&#65292;&#37325;&#32622;&#23494;&#30721;&#31561;&#65289;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#34892;&#21160;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogue is difficult in part because it involves understanding user intent, collecting information from the user, executing API calls, and generating helpful and fluent responses. However, for complex tasks one must also correctly do all of these things over multiple steps, and in a specific order. While large pre-trained language models can be fine-tuned end-to-end to create multi-step task-oriented dialogue agents that generate fluent text, our experiments confirm that this approach alone cannot reliably perform new multi-step tasks that are unseen during training. To address these limitations, we augment the dialogue contexts given to \textmd{text2text} transformers with known \textit{valid workflow names} and \textit{action plans}. Action plans consist of sequences of actions required to accomplish a task, and are encoded as simple sequences of keywords (e.g. verify-identity, pull-up-account, reset-password, etc.). We perform extensive experiments on the Action-Based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21452;&#35821;&#33976;&#39311;&#23454;&#29616;&#65292;&#21487;&#20197;&#20445;&#30041;&#21407;&#22987;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#65292;&#21516;&#26102;&#21487;&#20197;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#27169;&#22411;&#37096;&#32626;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.01709</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#36801;&#31227;&#30340;&#39640;&#25928;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Distilling Efficient Language-Specific Models for Cross-Lingual Transfer. (arXiv:2306.01709v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#21452;&#35821;&#33976;&#39311;&#23454;&#29616;&#65292;&#21487;&#20197;&#20445;&#30041;&#21407;&#22987;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#65292;&#21516;&#26102;&#21487;&#20197;&#36991;&#20813;&#19981;&#24517;&#35201;&#30340;&#27169;&#22411;&#37096;&#32626;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;Transformer&#65288;MMTs&#65289;&#65292;&#22914;mBERT&#21644;XLM-R&#65292;&#34987;&#24191;&#27867;&#29992;&#20110;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#12290;&#34429;&#28982;&#23427;&#20204;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#26469;&#34920;&#31034;&#25968;&#30334;&#31181;&#35821;&#35328;&#65292;&#20294;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#26368;&#32456;&#29992;&#25143;&#36890;&#24120;&#21482;&#23545;&#20010;&#21035;&#35821;&#35328;&#24863;&#20852;&#36259;&#12290;&#20026;&#27492;&#65292;MMT&#30340;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#20351;&#23427;&#20204;&#22312;&#27169;&#22411;&#22823;&#23567;&#12289;&#25512;&#29702;&#26102;&#38388;&#12289;&#33021;&#28304;&#21644;&#30828;&#20214;&#25104;&#26412;&#26041;&#38754;&#19981;&#24517;&#35201;&#22320;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20174;MMTs&#20013;&#25552;&#21462;&#21387;&#32553;&#30340;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#65292;&#20445;&#30041;&#21407;&#22987;MMTs&#22312;&#36328;&#35821;&#35328;&#36801;&#31227;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36825;&#26159;&#36890;&#36807;&#36827;&#34892;&#21452;&#35821;&#33976;&#39311;&#23454;&#29616;&#30340;&#65292;&#21363;&#20165;&#20351;&#29992;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#30340;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;BiStil&#30340;&#20004;&#38454;&#27573;&#33976;&#39311;&#26041;&#27861;&#65306;&#65288;i&#65289;&#31532;&#19968;&#38454;&#27573;&#20174;MMT&#20013;&#33976;&#39311;&#20986;&#19968;&#33324;&#30340;&#21452;&#35821;&#27169;&#22411;&#65292;&#32780;&#65288;ii&#65289;&#31532;&#20108;&#38454;&#27573;&#20351;&#29992;&#32463;&#36807;&#20219;&#21153;&#35843;&#25972;&#30340;&#21407;&#22987;MMT&#21464;&#20307;&#20316;&#20026;&#8220;&#25945;&#24072;&#8221;&#65292;&#36890;&#36807;&#31232;&#30095;&#31934;&#35843;&#21452;&#35821;&#8220;&#23398;&#29983;&#8221;&#27169;&#22411;&#26469;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#30340;&#33976;&#39311;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Massively multilingual Transformers (MMTs), such as mBERT and XLM-R, are widely used for cross-lingual transfer learning. While these are pretrained to represent hundreds of languages, end users of NLP systems are often interested only in individual languages. For such purposes, the MMTs' language coverage makes them unnecessarily expensive to deploy in terms of model size, inference time, energy, and hardware cost. We thus propose to extract compressed, language-specific models from MMTs which retain the capacity of the original MMTs for cross-lingual transfer. This is achieved by distilling the MMT bilingually, i.e., using data from only the source and target language of interest. Specifically, we use a two-phase distillation approach, termed BiStil: (i) the first phase distils a general bilingual model from the MMT, while (ii) the second, task-specific phase sparsely fine-tunes the bilingual "student" model using a task-tuned variant of the original MMT as its "teacher". We evaluate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#23384;&#22312;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#26174;&#30528;&#25552;&#39640;&#21512;&#24182;&#21518;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01708</link><description>&lt;p&gt;
&#21512;&#24182;&#27169;&#22411;&#26102;&#22914;&#20309;&#35299;&#20915;&#24178;&#25200;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Resolving Interference When Merging Models. (arXiv:2306.01708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#23384;&#22312;&#30340;&#24178;&#25200;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#26174;&#30528;&#25552;&#39640;&#21512;&#24182;&#21518;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#19968;&#27493;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20174;&#32780;&#33719;&#24471;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#21253;&#25324;&#25913;&#36827;&#19979;&#28216;&#24615;&#33021;&#65292;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#21644;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#24050;&#26377;&#30340;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#21516;&#27169;&#22411;&#21442;&#25968;&#20043;&#38388;&#30340;&#24178;&#25200;&#65292;&#23548;&#33268;&#21512;&#24182;&#22810;&#20010;&#27169;&#22411;&#26102;&#24615;&#33021;&#22823;&#24133;&#19979;&#38477;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;&#20808;&#21069;&#30340;&#21512;&#24182;&#25216;&#26415;&#30001;&#20110;&#20004;&#20010;&#20027;&#35201;&#24178;&#25200;&#26469;&#28304;&#32780;&#19981;&#24910;&#20002;&#22833;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65306;(a)&#20887;&#20313;&#21442;&#25968;&#20540;&#24341;&#36215;&#30340;&#24178;&#25200;&#21644;(b)&#34920;&#31034;&#21516;&#19968;&#21442;&#25968;&#20540;&#30340;&#31526;&#21495;&#22312;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning - i.e., further fine-tuning a pre-trained model on a downstream task - can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter's values across models. To 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23558;&#30456;&#23545;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#27880;&#20837;&#20855;&#26377;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#21512;&#25104;&#31639;&#26415;&#20219;&#21153;&#65288;MsAT&#65289;&#65292;&#20174;&#32780;&#25552;&#39640;LM&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.01707</link><description>&lt;p&gt;
&#20174;&#31639;&#26415;&#20219;&#21153;&#20013;&#23398;&#20064;&#22810;&#27493;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Multi-step Reasoning from Arithmetic Task. (arXiv:2306.01707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23558;&#30456;&#23545;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#27880;&#20837;&#20855;&#26377;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#21512;&#25104;&#31639;&#26415;&#20219;&#21153;&#65288;MsAT&#65289;&#65292;&#20174;&#32780;&#25552;&#39640;LM&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#34987;&#35748;&#20026;&#26159;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24517;&#35201;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;LM&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25104;&#21151;&#24402;&#22240;&#20110;&#23427;&#20204;&#30340;&#36830;&#32493;&#24605;&#32771;&#65288;CoT&#65289;&#25512;&#29702;&#33021;&#21147;&#65292;&#21363;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#25104;&#36880;&#27493;&#25512;&#29702;&#38142;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#31181;&#33021;&#21147;&#20284;&#20046;&#21482;&#20986;&#29616;&#22312;&#20855;&#26377;&#20016;&#23500;&#21442;&#25968;&#30340;&#27169;&#22411;&#20013;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#22914;&#20309;&#23558;&#30456;&#23545;&#36739;&#23567;&#30340;LM&#19982;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;MsAT&#65288;&#22810;&#27493;&#31639;&#26415;&#20219;&#21153;&#65289;&#36827;&#34892;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#26469;&#27880;&#20837;&#36825;&#31181;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#23398;&#24212;&#29992;&#39064;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#22686;&#24378;LM&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning is regarded as a necessary ability for Language Models (LMs). Recent works demonstrate large LMs' impressive performance in solving math problems. The success is attributed to their Chain-of-Thought (CoT) reasoning abilities, i.e., the ability to decompose complex questions into step-by-step reasoning chains, but such ability seems only to emerge from models with abundant parameters. This work investigates how to incorporate relatively small LMs with the capabilities of multi-step reasoning. We propose to inject such abilities by continually pre-training LMs on a synthetic dataset MsAT, which stands for Multi-step Arithmetic Task. Our experiments on four math word problem datasets show the effectiveness of the proposed method in enhancing LMs' math reasoning abilities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Fine-Grained RLHF&#26694;&#26550;&#65292;&#20351;&#29992;&#31934;&#32454;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#20316;&#20026;&#26126;&#30830;&#30340;&#35757;&#32451;&#20449;&#21495;&#26469;&#35757;&#32451;&#21644;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#22810;&#20010;&#32454;&#33268;&#30340;&#22870;&#21169;&#27169;&#22411;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01693</link><description>&lt;p&gt;
&#31934;&#32454;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Fine-Grained Human Feedback Gives Better Rewards for Language Model Training. (arXiv:2306.01693v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Fine-Grained RLHF&#26694;&#26550;&#65292;&#20351;&#29992;&#31934;&#32454;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#20316;&#20026;&#26126;&#30830;&#30340;&#35757;&#32451;&#20449;&#21495;&#26469;&#35757;&#32451;&#21644;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#22810;&#20010;&#32454;&#33268;&#30340;&#22870;&#21169;&#27169;&#22411;&#26469;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#34920;&#29616;&#20986;&#19981;&#33391;&#30340;&#25991;&#26412;&#29983;&#25104;&#34892;&#20026;&#65292;&#21253;&#25324;&#29983;&#25104;&#34394;&#20551;&#12289;&#26377;&#23475;&#25110;&#26080;&#20851;&#30340;&#36755;&#20986;&#12290;&#26368;&#36817;&#65292;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;-&#20854;&#20013;&#20154;&#31867;&#23545;LM&#36755;&#20986;&#30340;&#20559;&#22909;&#35780;&#20215;&#34987;&#36716;&#21270;&#20026;&#23398;&#20064;&#20449;&#21495;-&#24050;&#32463;&#26174;&#31034;&#20986;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25972;&#20307;&#21453;&#39304;&#23545;&#38271;&#25991;&#26412;&#36755;&#20986;&#20256;&#36798;&#30340;&#20449;&#24687;&#26377;&#38480;&#65307;&#23427;&#19981;&#34920;&#26126;&#36755;&#20986;&#30340;&#21738;&#20123;&#26041;&#38754;&#24433;&#21709;&#20102;&#29992;&#25143;&#30340;&#20559;&#22909;&#65307;&#20363;&#22914;&#65292;&#21738;&#20123;&#37096;&#20998;&#21253;&#21547;&#20160;&#20040;&#31867;&#22411;&#30340;&#38169;&#35823;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31934;&#32454;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#65288;&#20363;&#22914;&#65292;&#21738;&#20010;&#21477;&#23376;&#26159;&#38169;&#35823;&#30340;&#65292;&#21738;&#20010;&#23376;&#21477;&#26159;&#26080;&#20851;&#30340;&#65289;&#20316;&#20026;&#26126;&#30830;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;Fine-Grained RLHF&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#35757;&#32451;&#21644;&#23398;&#20064;&#19982;&#19981;&#21516;&#21453;&#39304;&#31867;&#22411;&#30456;&#20851;&#30340;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#30340;&#31934;&#32454;&#21270;&#22870;&#21169;&#21151;&#33021;&#30340;&#26694;&#26550;&#65292;&#20855;&#26377;&#20197;&#19979;&#20004;&#20010;&#29305;&#24449;&#65306;&#65288;1&#65289;&#23494;&#24230;&#65292;&#20197;&#22312;&#29983;&#25104;&#27599;&#20010;&#27573;&#33853;&#65288;&#20363;&#22914;&#19968;&#20010;&#21477;&#23376;&#65289;&#21518;&#25552;&#20379;&#22870;&#21169;&#65307; &#65288;2&#65289;&#24182;&#20837;&#19981;&#21516;&#21453;&#39304;&#31867;&#22411;&#30340;&#22810;&#20010;&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF) - where human preference judgments on LM outputs are transformed into a learning signal - has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffusEmp&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22810;&#32423;&#25511;&#21046;&#20449;&#21495;&#26469;&#29983;&#25104;&#26356;&#21152;&#32454;&#33268;&#21644;&#20010;&#24615;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#12290;</title><link>http://arxiv.org/abs/2306.01657</link><description>&lt;p&gt;
DiffusEmp:&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#22810;&#32423;&#25511;&#21046;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control for Empathetic Response Generation. (arXiv:2306.01657v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffusEmp&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#22810;&#32423;&#25511;&#21046;&#20449;&#21495;&#26469;&#29983;&#25104;&#26356;&#21152;&#32454;&#33268;&#21644;&#20010;&#24615;&#21270;&#30340;&#20849;&#24773;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#26159;&#24320;&#25918;&#24335;&#20132;&#27969;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22240;&#32032;&#65292;&#23427;&#33258;&#28982;&#22320;&#23637;&#31034;&#20102;&#19968;&#20010;&#20154;&#23545;&#20182;&#20154;&#30340;&#20851;&#24515;&#21644;&#29702;&#35299;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29983;&#25104;&#20849;&#24773;&#21709;&#24212;&#30340;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#30340;&#20316;&#21697;&#24448;&#24448;&#23548;&#33268;&#21333;&#35843;&#30340;&#20849;&#24773;&#65292;&#21363;&#25351;&#36890;&#29992;&#21644;&#23433;&#20840;&#30340;&#34920;&#36798;&#26041;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#26174;&#24335;&#25511;&#21046;&#26469;&#24341;&#23548;&#20849;&#24773;&#34920;&#36798;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;DiffusEmp&#26694;&#26550;&#65292;&#22522;&#20110;&#26465;&#20214;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#32479;&#19968;&#20102;&#23545;&#35805;&#19978;&#19979;&#25991;&#21644;&#23646;&#24615;&#23548;&#21521;&#25511;&#21046;&#20449;&#21495;&#30340;&#21033;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24341;&#20837;&#36890;&#20449;&#26426;&#21046;&#12289;&#24847;&#22270;&#21644;&#35821;&#20041;&#26694;&#26550;&#20316;&#20026;&#22810;&#23618;&#27425;&#20449;&#21495;&#65292;&#21487;&#20174;&#31895;&#31961;&#21040;&#32454;&#33268;&#22320;&#25511;&#21046;&#20849;&#24773;&#23454;&#29616;&#12290;&#28982;&#21518;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#23631;&#34109;&#31574;&#30053;&#65292;&#20197;&#21453;&#26144;&#22810;&#23618;&#27425;&#20449;&#21495;&#19982;&#21709;&#24212;&#20196;&#29260;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;&#25193;&#25955;&#27169;&#22411;&#20013;&#20197;&#24433;&#21709;&#29983;&#25104;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;EmpatheticDialogue&#19978;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20248;&#20110;&#20854;&#20182;&#20849;&#24773;&#22238;&#24212;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empathy is a crucial factor in open-domain conversations, which naturally shows one's caring and understanding to others. Though several methods have been proposed to generate empathetic responses, existing works often lead to monotonous empathy that refers to generic and safe expressions. In this paper, we propose to use explicit control to guide the empathy expression and design a framework DiffusEmp based on conditional diffusion language model to unify the utilization of dialogue context and attribute-oriented control signals. Specifically, communication mechanism, intent, and semantic frame are imported as multi-grained signals that control the empathy realization from coarse to fine levels. We then design a specific masking strategy to reflect the relationship between multi-grained signals and response tokens, and integrate it into the diffusion model to influence the generative process. Experimental results on a benchmark dataset EmpatheticDialogue show that our framework outper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24050;&#26377;&#30340;&#20363;&#23376;&#32451;&#20064;&#29983;&#25104;&#26032;&#30340;&#21306;&#21035;&#24863;&#30693;&#22411;&#22635;&#31354;&#32451;&#20064;&#65292;&#26080;&#38656;&#28145;&#24230;&#26631;&#27880;&#65292;&#29305;&#21035;&#38024;&#23545;&#35821;&#35328;&#23398;&#20064;&#20013;&#30340;&#35821;&#27861;&#32451;&#20064;&#65307;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#27861;&#35821;&#35821;&#27861;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#31454;&#20105;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.01584</link><description>&lt;p&gt;
&#20174;&#37096;&#20998;&#26631;&#27880;&#25968;&#25454;&#20013;&#23398;&#20064;&#65306;&#38754;&#21521;&#35821;&#35328;&#23398;&#20064;&#30340;&#21306;&#21035;&#24863;&#30693;&#22411;&#22635;&#31354;&#32451;&#20064;&#33258;&#21160;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning from Partially Annotated Data: Example-aware Creation of Gap-filling Exercises for Language Learning. (arXiv:2306.01584v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24050;&#26377;&#30340;&#20363;&#23376;&#32451;&#20064;&#29983;&#25104;&#26032;&#30340;&#21306;&#21035;&#24863;&#30693;&#22411;&#22635;&#31354;&#32451;&#20064;&#65292;&#26080;&#38656;&#28145;&#24230;&#26631;&#27880;&#65292;&#29305;&#21035;&#38024;&#23545;&#35821;&#35328;&#23398;&#20064;&#20013;&#30340;&#35821;&#27861;&#32451;&#20064;&#65307;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#27861;&#35821;&#35821;&#27861;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#36825;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#31454;&#20105;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20570;&#32451;&#20064;&#65288;&#21253;&#25324;&#32451;&#20064;&#27979;&#35797;&#65289;&#26159;&#23398;&#20064;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21019;&#24314;&#36825;&#26679;&#30340;&#32451;&#20064;&#38656;&#35201;&#25945;&#24072;&#20184;&#20986;&#38750;&#24120;&#22823;&#30340;&#21162;&#21147;&#12290;&#22312;&#25968;&#23383;&#21270;&#25945;&#32946;&#24037;&#20855;&#20013;&#33258;&#21160;&#29983;&#25104;&#32451;&#20064;&#20855;&#26377;&#24456;&#22823;&#30340;&#20215;&#20540;&#12290;&#26412;&#25991;&#29305;&#21035;&#20851;&#27880;&#20110;&#26080;&#38656;&#28145;&#24230;&#26631;&#27880;&#26448;&#26009;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#31034;&#20363;&#32451;&#20064;&#33258;&#21160;&#21019;&#24314;&#35821;&#35328;&#23398;&#20064;&#20013;&#30340;&#22635;&#31354;&#32451;&#20064;&#65292;&#23588;&#20854;&#26159;&#35821;&#27861;&#32451;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#19987;&#38376;&#29992;&#20110;Gap-filling exercise generation&#20219;&#21153;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#19968;&#20010;&#27861;&#35821;&#35821;&#27861;&#30340;&#29616;&#23454;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#31454;&#20105;&#22522;&#20934;&#22312;&#27861;&#35821;&#35821;&#27861;Gap-filling exercise generation&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#21482;&#38656;&#23545;&#37096;&#20998;&#25968;&#25454;&#36827;&#34892;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since performing exercises (including, e.g., practice tests) forms a crucial component of learning, and creating such exercises requires non-trivial effort from the teacher. There is a great value in automatic exercise generation in digital tools in education. In this paper, we particularly focus on automatic creation of gapfilling exercises for language learning, specifically grammar exercises. Since providing any annotation in this domain requires human expert effort, we aim to avoid it entirely and explore the task of converting existing texts into new gap-filling exercises, purely based on an example exercise, without explicit instruction or detailed annotation of the intended grammar topics. We contribute (i) a novel neural network architecture specifically designed for aforementioned gap-filling exercise generation task, and (ii) a real-world benchmark dataset for French grammar. We show that our model for this French grammar gap-filling exercise generation outperforms a competit
&lt;/p&gt;</description></item><item><title>EmoUS&#26159;&#19968;&#31181;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#27169;&#25311;&#29992;&#25143;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#34892;&#20026;&#21644;&#24773;&#24863;&#21453;&#24212;&#65292;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#24773;&#24863;&#19982;&#31995;&#32479;&#34892;&#20026;&#30340;&#20851;&#31995;&#26469;&#35780;&#20215;&#19981;&#21516;&#23545;&#35805;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01579</link><description>&lt;p&gt;
EmoUS: &#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#27169;&#25311;&#29992;&#25143;&#24773;&#24863;
&lt;/p&gt;
&lt;p&gt;
EmoUS: Simulating User Emotions in Task-Oriented Dialogues. (arXiv:2306.01579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01579
&lt;/p&gt;
&lt;p&gt;
EmoUS&#26159;&#19968;&#31181;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#27169;&#25311;&#29992;&#25143;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#34892;&#20026;&#21644;&#24773;&#24863;&#21453;&#24212;&#65292;&#36890;&#36807;&#20998;&#26512;&#29992;&#25143;&#24773;&#24863;&#19982;&#31995;&#32479;&#34892;&#20026;&#30340;&#20851;&#31995;&#26469;&#35780;&#20215;&#19981;&#21516;&#23545;&#35805;&#31995;&#32479;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#29992;&#25143;&#27169;&#25311;&#22120; (USs) &#20165;&#22312;&#35821;&#20041;&#21644;&#33258;&#28982;&#35821;&#35328;&#23618;&#38754;&#19978;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#65292;&#32780;&#19981;&#32771;&#34385;&#29992;&#25143;&#35282;&#33394;&#21644;&#24773;&#24863;&#12290;&#36890;&#36807;&#20248;&#21270;&#19982;&#21508;&#31181;&#24773;&#24863;&#29366;&#24577;&#39537;&#21160;&#30340;&#22810;&#26679;&#21270;&#29992;&#25143;&#34892;&#20026;&#26377;&#20851;&#30340;&#36890;&#29992;&#29992;&#25143;&#31574;&#30053;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#22312;&#23454;&#38469;&#37096;&#32626;&#26102;&#39640;&#22833;&#25928;&#29575;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; EmoUS&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#20197;&#23398;&#20064;&#22312;&#29992;&#25143;&#34892;&#20026;&#20013;&#27169;&#25311;&#29992;&#25143;&#24773;&#24863;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#12290;EmoUS &#22522;&#20110;&#29992;&#25143;&#30446;&#26631;&#12289;&#23545;&#35805;&#21382;&#21490;&#21644;&#29992;&#25143;&#35282;&#33394;&#29983;&#25104;&#29992;&#25143;&#24773;&#24863;&#12289;&#35821;&#20041;&#21160;&#20316;&#21644;&#33258;&#28982;&#35821;&#35328;&#22238;&#22797;&#12290;&#36890;&#36807;&#20998;&#26512;&#20160;&#20040;&#26679;&#30340;&#31995;&#32479;&#34892;&#20026;&#24341;&#36215;&#20160;&#20040;&#26679;&#30340;&#29992;&#25143;&#24773;&#24863;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; EmoUS &#33021;&#22815;&#34987;&#29992;&#20316;&#25506;&#27979;&#22120;&#26469;&#35780;&#20272;&#21508;&#31181;&#23545;&#35805;&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#23427;&#20204;&#23545;&#29992;&#25143;&#24773;&#24863;&#29366;&#24577;&#30340;&#24433;&#21709;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#19981;&#26029;&#22686;&#38271;&#30340;&#20262;&#29702;&#20851;&#20999;&#30340;&#26102;&#20195;&#65292;&#24320;&#21457;&#36825;&#31181;&#26041;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing user simulators (USs) for task-oriented dialogue systems only model user behaviour on semantic and natural language levels without considering the user persona and emotions. Optimising dialogue systems with generic user policies, which cannot model diverse user behaviour driven by different emotional states, may result in a high drop-off rate when deployed in the real world. Thus, we present EmoUS, a user simulator that learns to simulate user emotions alongside user behaviour. EmoUS generates user emotions, semantic actions, and natural language responses based on the user goal, the dialogue history, and the user persona. By analysing what kind of system behaviour elicits what kind of user emotions, we show that EmoUS can be used as a probe to evaluate a variety of dialogue systems and in particular their effect on the user's emotional state. Developing such methods is important in the age of large language model chat-bots and rising ethical concerns.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#26550;&#26500;&#65288;&#38142;&#24335;&#27169;&#22411;&#21644;&#32452;&#21512;&#27169;&#22411;&#65289;&#22312;&#23450;&#20301;&#26368;&#36817;&#30340;&#35270;&#35273;&#23545;&#35937;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#20004;&#31181;&#26550;&#26500;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#37117;&#30456;&#21516;&#65292;&#20294;&#24403;&#20219;&#21153;&#21487;&#20197;&#20998;&#35299;&#25104;&#23376;&#20219;&#21153;&#26102;&#65292;&#38142;&#24335;&#26550;&#26500;&#30340;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;12&#20493;&#12290;</title><link>http://arxiv.org/abs/2306.01551</link><description>&lt;p&gt;
&#27604;&#36739;&#32452;&#21512;&#27169;&#22411;&#21644;&#38142;&#24335;&#27169;&#22411;&#26469;&#23450;&#20301;&#26368;&#36817;&#30340;&#35270;&#35273;&#23545;&#35937;
&lt;/p&gt;
&lt;p&gt;
Comparing a composite model versus chained models to locate a nearest visual object. (arXiv:2306.01551v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01551
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#20004;&#31181;&#26550;&#26500;&#65288;&#38142;&#24335;&#27169;&#22411;&#21644;&#32452;&#21512;&#27169;&#22411;&#65289;&#22312;&#23450;&#20301;&#26368;&#36817;&#30340;&#35270;&#35273;&#23545;&#35937;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#20004;&#31181;&#26550;&#26500;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#37117;&#30456;&#21516;&#65292;&#20294;&#24403;&#20219;&#21153;&#21487;&#20197;&#20998;&#35299;&#25104;&#23376;&#20219;&#21153;&#26102;&#65292;&#38142;&#24335;&#26550;&#26500;&#30340;&#35757;&#32451;&#36895;&#24230;&#25552;&#39640;&#20102;12&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22320;&#29702;&#22270;&#20687;&#21644;&#25991;&#26412;&#20013;&#25552;&#21462;&#20449;&#24687;&#23545;&#20110;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20107;&#20808;&#30830;&#23450;&#27839;&#20854;&#26410;&#26469;&#36335;&#24452;&#36830;&#25509;&#21040;&#30340;&#26368;&#20339;&#23567;&#21306;&#31449;&#28857;&#33267;&#20851;&#37325;&#35201;&#12290;&#22810;&#20010;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#20294;&#23545;&#20110;&#27492;&#31867;&#29992;&#20363;&#30340;&#36873;&#25321;&#36866;&#24403;&#27169;&#22411;&#27809;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#25351;&#23548;&#24847;&#35265;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35797;&#39564;&#20102;&#20004;&#31181;&#26550;&#26500;&#26469;&#35299;&#20915;&#36825;&#26679;&#30340;&#20219;&#21153;&#65306;&#31532;&#19968;&#31181;&#38142;&#24335;&#27169;&#22411;&#26550;&#26500;&#65292;&#20854;&#20013;&#38142;&#20013;&#30340;&#27599;&#20010;&#27169;&#22411;&#37117;&#22788;&#29702;&#20219;&#21153;&#30340;&#23376;&#20219;&#21153;; &#31532;&#20108;&#31181;&#26159;&#21333;&#20010;&#27169;&#22411;&#26550;&#26500;&#65292;&#21487;&#22788;&#29702;&#25972;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26550;&#26500;&#30340;&#34920;&#29616;&#27700;&#24179;&#30456;&#21516;&#65292;&#22343;&#20026;0.055&#21644;0.056&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;(RMSE)&#65307;&#30740;&#31350;&#32467;&#26524;&#36827;&#19968;&#27493;&#26174;&#31034;&#65292;&#24403;&#20219;&#21153;&#21487;&#20197;&#20998;&#35299;&#25104;&#23376;&#20219;&#21153;&#26102;&#65292;&#38142;&#24335;&#26550;&#26500;&#30340;&#35757;&#32451;&#36895;&#24230;&#27604;&#32452;&#21512;&#27169;&#22411;&#25552;&#39640;&#20102;12&#20493;&#12290;&#28982;&#32780;&#65292;&#32452;&#21512;&#27169;&#22411;&#26174;&#33879;&#20943;&#36731;&#20102;&#29992;&#25143;&#22312;&#37197;&#32622;&#38142;&#24335;&#27169;&#22411;&#26102;&#30340;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting information from geographic images and text is crucial for autonomous vehicles to determine in advance the best cell stations to connect to along their future path. Multiple artificial neural network models can address this challenge; however, there is no definitive guidance on the selection of an appropriate model for such use cases. Therefore, we experimented two architectures to solve such a task: a first architecture with chained models where each model in the chain addresses a sub-task of the task; and a second architecture with a single model that addresses the whole task. Our results showed that these two architectures achieved the same level performance with a root mean square error (RMSE) of 0.055 and 0.056; The findings further revealed that when the task can be decomposed into sub-tasks, the chain architecture exhibits a twelve-fold increase in training speed compared to the composite model. Nevertheless, the composite model significantly alleviates the burden of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#20998;&#24067;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#30340;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#35780;&#20272;&#32763;&#35793;&#36136;&#37327;&#24182;&#25552;&#20379;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#65292;&#22312;&#20845;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#23545;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#38656;&#35201;&#25968;&#25454;&#21487;&#20132;&#25442;&#24615;&#20551;&#35774;&#25165;&#33021;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01549</link><description>&lt;p&gt;
&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#20998;&#24067;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Evaluating Machine Translation Quality with Conformal Predictive Distributions. (arXiv:2306.01549v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#20998;&#24067;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#30340;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#35780;&#20272;&#32763;&#35793;&#36136;&#37327;&#24182;&#25552;&#20379;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#65292;&#22312;&#20845;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#23545;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#38656;&#35201;&#25968;&#25454;&#21487;&#20132;&#25442;&#24615;&#20551;&#35774;&#25165;&#33021;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21516;&#26102;&#35780;&#20272;&#32763;&#35793;&#36136;&#37327;&#24182;&#25552;&#20379;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#65292;&#26469;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#31526;&#21512;&#24615;&#39044;&#27979;&#20998;&#24067;&#26469;&#20135;&#29983;&#20855;&#26377;&#20445;&#35777;&#35206;&#30422;&#29575;&#30340;&#39044;&#27979;&#21306;&#38388;&#65292;&#36825;&#24847;&#21619;&#30528;&#23545;&#20110;&#20219;&#20309;&#32473;&#23450;&#30340;&#26174;&#33879;&#24615;&#27700;&#24179;$\epsilon$&#65292;&#25105;&#20204;&#21487;&#20197;&#26399;&#26395;&#19968;&#20010;&#32763;&#35793;&#30340;&#30495;&#23454;&#36136;&#37327;&#24471;&#20998;&#20197;$1-\epsilon$&#30340;&#36895;&#29575;&#33853;&#22312;&#21306;&#38388;&#20869;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#22312;&#20845;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#23545;&#19978;&#65292;&#22312;&#35206;&#30422;&#29575;&#21644;&#38160;&#24230;&#26041;&#38754;&#20248;&#20110;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;&#25968;&#25454;&#21487;&#20132;&#25442;&#24615;&#20551;&#35774;&#25165;&#33021;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new approach for assessing uncertainty in machine translation by simultaneously evaluating translation quality and providing a reliable confidence score. Our approach utilizes conformal predictive distributions to produce prediction intervals with guaranteed coverage, meaning that for any given significance level $\epsilon$, we can expect the true quality score of a translation to fall out of the interval at a rate of $1-\epsilon$. In this paper, we demonstrate how our method outperforms a simple, but effective baseline on six different language pairs in terms of coverage and sharpness. Furthermore, we validate that our approach requires the data exchangeability assumption to hold for optimal performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;PassGPT&#36827;&#34892;&#23494;&#30721;&#24314;&#27169;&#21644;&#29983;&#25104;&#65292;&#35813;&#27169;&#22411;&#27604;&#22522;&#20110;GAN&#30340;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#65292;&#33021;&#29983;&#25104;&#31526;&#21512;&#20219;&#24847;&#38480;&#21046;&#30340;&#23494;&#30721;&#65292;&#20026;&#25552;&#39640;&#23494;&#30721;&#24378;&#24230;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2306.01545</link><description>&lt;p&gt;
PassGPT: &#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23494;&#30721;&#24314;&#27169;&#21644;&#65288;&#24341;&#23548;&#24335;&#65289;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PassGPT: Password Modeling and (Guided) Generation with Large Language Models. (arXiv:2306.01545v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;PassGPT&#36827;&#34892;&#23494;&#30721;&#24314;&#27169;&#21644;&#29983;&#25104;&#65292;&#35813;&#27169;&#22411;&#27604;&#22522;&#20110;GAN&#30340;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#65292;&#33021;&#29983;&#25104;&#31526;&#21512;&#20219;&#24847;&#38480;&#21046;&#30340;&#23494;&#30721;&#65292;&#20026;&#25552;&#39640;&#23494;&#30721;&#24378;&#24230;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#25104;&#21151;&#22320;&#27169;&#25311;&#33258;&#28982;&#35821;&#35328;&#65292;&#26080;&#38656;&#26126;&#30830;&#30340;&#30417;&#30563;&#65292;&#20165;&#36890;&#36807;&#22823;&#37327;&#30340;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#24314;&#27169;&#23494;&#30721;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;PassGPT&#65292;&#23427;&#26159;&#19968;&#20010;&#22312;&#23494;&#30721;&#27844;&#38706;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;LLM&#65292;&#29992;&#20110;&#29983;&#25104;&#23494;&#30721;&#12290;PassGPT&#36890;&#36807;&#29468;&#27979;&#20004;&#20493;&#20110;&#22522;&#20110;&#29983;&#25104;&#24615;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#23494;&#30721;&#32780;&#32988;&#36807;&#20854;&#23427;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24341;&#23548;&#24335;&#23494;&#30721;&#29983;&#25104;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;PassGPT&#30340;&#25277;&#26679;&#36807;&#31243;&#29983;&#25104;&#31526;&#21512;&#20219;&#24847;&#38480;&#21046;&#30340;&#23494;&#30721;&#65292;&#36825;&#22312;&#24403;&#21069;&#22522;&#20110;GAN&#30340;&#31574;&#30053;&#20013;&#26159;&#32570;&#20047;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;PassGPT&#23545;&#23494;&#30721;&#23450;&#20041;&#30340;&#29109;&#21644;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#22312;&#22686;&#24378;&#29616;&#26377;&#23494;&#30721;&#24378;&#24230;&#20272;&#35745;&#22120;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) successfully model natural language from vast amounts of text without the need for explicit supervision. In this paper, we investigate the efficacy of LLMs in modeling passwords. We present PassGPT, a LLM trained on password leaks for password generation. PassGPT outperforms existing methods based on generative adversarial networks (GAN) by guessing twice as many previously unseen passwords. Furthermore, we introduce the concept of guided password generation, where we leverage PassGPT sampling procedure to generate passwords matching arbitrary constraints, a feat lacking in current GAN-based strategies. Lastly, we conduct an in-depth analysis of the entropy and probability distribution that PassGPT defines over passwords and discuss their use in enhancing existing password strength estimators.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#20064;&#24471;&#21451;&#22909;&#22411;&#22522;&#20934;&#65292;&#20197;&#26816;&#39564;&#33258;&#25105;&#30417;&#30563;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#22312;&#20799;&#31461;&#35789;&#27719;&#21644;&#21477;&#27861;&#32463;&#21382;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#65306;&#25991;&#26412;&#21644;&#35821;&#38899;&#20043;&#38388;&#30340;&#24046;&#36317;&#21644;&#24178;&#20928;&#35821;&#38899;&#21644;&#37326;&#22806;&#35821;&#38899;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2306.01506</link><description>&lt;p&gt;
BabySLM: &#33258;&#25105;&#30417;&#30563;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#20064;&#24471;&#21451;&#22909;&#22411;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models. (arXiv:2306.01506v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#35328;&#20064;&#24471;&#21451;&#22909;&#22411;&#22522;&#20934;&#65292;&#20197;&#26816;&#39564;&#33258;&#25105;&#30417;&#30563;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#22312;&#20799;&#31461;&#35789;&#27719;&#21644;&#21477;&#27861;&#32463;&#21382;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#65306;&#25991;&#26412;&#21644;&#35821;&#38899;&#20043;&#38388;&#30340;&#24046;&#36317;&#21644;&#24178;&#20928;&#35821;&#38899;&#21644;&#37326;&#22806;&#35821;&#38899;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#30340;&#33258;&#25105;&#30417;&#30563;&#25216;&#26415;&#33021;&#22815;&#20174;&#21548;&#21040;&#30340;&#35821;&#38899;&#20013;&#21457;&#23637;&#20986;&#35821;&#35328;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#20154;&#31867;&#26631;&#31614;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#21147;&#24182;&#36827;&#19968;&#27493;&#20102;&#35299;&#23156;&#20799;&#23398;&#20064;&#35821;&#35328;&#30340;&#26041;&#24335;&#65292;&#27169;&#25311;&#24517;&#39035;&#32039;&#23494;&#27169;&#20223;&#29616;&#23454;&#24773;&#20917;&#65292;&#36890;&#36807;&#22312;&#24320;&#21457;&#19978;&#31526;&#21512;&#20799;&#31461;&#35821;&#35328;&#32463;&#39564;&#20856;&#22411;&#35789;&#27719;&#24211;&#21644;&#23545;&#24212;&#27979;&#35797;&#38598;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#22312;&#35789;&#27719;&#21644;&#21477;&#27861;&#23618;&#38754;&#19978;&#30340;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#20064;&#24471;&#21451;&#22909;&#22411;&#22522;&#20934;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#27492;&#22522;&#20934;&#65292;&#24182;&#24635;&#32467;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#35777;&#26126;&#20854;&#26377;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#20004;&#20010;&#25361;&#25112;&#65306;&#22635;&#34917;&#25991;&#26412;&#21644;&#35821;&#38899;&#20043;&#38388;&#20197;&#21450;&#24178;&#20928;&#35821;&#38899;&#21644;&#37326;&#22806;&#35821;&#38899;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised techniques for learning speech representations have been shown to develop linguistic competence from exposure to speech without the need for human labels. In order to fully realize the potential of these approaches and further our understanding of how infants learn language, simulations must closely emulate real-life situations by training on developmentally plausible corpora and benchmarking against appropriate test sets. To this end, we propose a language-acquisition-friendly benchmark to probe spoken language models at the lexical and syntactic levels, both of which are compatible with the vocabulary typical of children's language experiences. This paper introduces the benchmark and summarizes a range of experiments showing its usefulness. In addition, we highlight two exciting challenges that need to be addressed for further progress: bridging the gap between text and speech and between clean speech and in-the-wild speech.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;SACL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31867;&#21035;&#20998;&#24067;&#32467;&#26500;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#31867;&#21035;&#20998;&#24067;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#26377;&#25928;&#21033;&#29992;&#26631;&#31614;&#32423;&#29305;&#24615;&#19968;&#33268;&#24615;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#31867;&#20869;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01505</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#20351;&#29992;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Supervised Adversarial Contrastive Learning for Emotion Recognition in Conversations. (arXiv:2306.01505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;SACL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31867;&#21035;&#20998;&#24067;&#32467;&#26500;&#34920;&#31034;&#65292;&#36890;&#36807;&#32852;&#21512;&#31867;&#21035;&#20998;&#24067;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#26377;&#25928;&#21033;&#29992;&#26631;&#31614;&#32423;&#29305;&#24615;&#19968;&#33268;&#24615;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#31867;&#20869;&#29305;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35782;&#21035;&#22312;&#23545;&#35805;&#20013;&#26159;&#25552;&#21462;&#27867;&#21270;&#21644;&#31283;&#20581;&#34920;&#31034;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#65288;SACL&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#31867;&#21035;&#20998;&#24067;&#32467;&#26500;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#23545;&#27604;&#24863;&#30693;&#23545;&#25239;&#24615;&#35757;&#32451;&#20197;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#26679;&#26412;&#65292;&#24182;&#22312;&#21407;&#22987;&#21644;&#23545;&#25239;&#26679;&#26412;&#19978;&#20351;&#29992;&#32852;&#21512;&#31867;&#21035;&#20998;&#24067;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#12290;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#26631;&#31614;&#32423;&#29305;&#24615;&#19968;&#33268;&#24615;&#24182;&#20445;&#30041;&#32454;&#31890;&#24230;&#30340;&#31867;&#20869;&#29305;&#24615;&#12290;&#20026;&#20102;&#36991;&#20813;&#23545;&#19978;&#19979;&#25991;&#30456;&#20851;&#25968;&#25454;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#23545;&#25239;&#24615;&#35757;&#32451;&#31574;&#30053;&#65292;&#20174;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#26356;&#22810;&#19981;&#21516;&#30340;&#29305;&#24449;&#65292;&#24182;&#22686;&#24378;&#27169;&#22411;&#23545;&#19978;&#19979;&#25991;&#30340;&#23481;&#38169;&#24615;&#12290;&#22312;&#35813;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#24207;&#21015;&#30340;&#26041;&#27861;SACL-LSTM&#65292;&#29992;&#20110;&#23398;&#20064;&#38024;&#23545;ERC&#30340;&#26631;&#31614;&#19968;&#33268;&#21644;&#19978;&#19979;&#25991;&#31283;&#20581;&#30340;&#24773;&#24863;&#29305;&#24449;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SACL-LSTM&#22312;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting generalized and robust representations is a major challenge in emotion recognition in conversations (ERC). To address this, we propose a supervised adversarial contrastive learning (SACL) framework for learning class-spread structured representations. The framework applies contrast-aware adversarial training to generate worst-case samples and uses a joint class-spread contrastive learning objective on both original and adversarial samples. It can effectively utilize label-level feature consistency and retain fine-grained intra-class features. To avoid the negative impact of adversarial perturbations on context-dependent data, we design a contextual adversarial training strategy to learn more diverse features from context and enhance the model's context robustness. We develop a sequence-based method SACL-LSTM under this framework, to learn label-consistent and context-robust emotional features for ERC. Experiments on three datasets demonstrate that SACL-LSTM achieves state-of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22914;GPT-4&#22312;&#30196;&#21574;&#30151;&#35786;&#26029;&#19978;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;&#30446;&#21069;&#36824;&#26080;&#27861;&#32988;&#36807;&#20256;&#32479;AI&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.01499</link><description>&lt;p&gt;
LLMs&#65288;&#22914;GPT-4&#65289;&#22312;&#30196;&#21574;&#30151;&#35786;&#26029;&#20013;&#33021;&#21542;&#32988;&#36807;&#20256;&#32479;AI&#24037;&#20855;&#65311;&#25110;&#35768;&#26377;&#28508;&#21147;&#65292;&#20294;&#29616;&#22312;&#36824;&#19981;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can LLMs like GPT-4 outperform traditional AI tools in dementia diagnosis? Maybe, but not today. (arXiv:2306.01499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22914;GPT-4&#22312;&#30196;&#21574;&#30151;&#35786;&#26029;&#19978;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;&#30446;&#21069;&#36824;&#26080;&#27861;&#32988;&#36807;&#20256;&#32479;AI&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#22312;&#24120;&#35265;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#20855;&#26377;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#36824;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;GPT-4&#26159;&#21542;&#33021;&#30452;&#25509;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#26367;&#20195;&#19987;&#19994;&#39046;&#22495;&#20013;&#30340;&#20256;&#32479;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24037;&#20855;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#65288;&#22914;GPT-4&#65289;&#22312;&#30196;&#21574;&#30151;&#35786;&#26029;&#19978;&#32988;&#36807;&#20256;&#32479;AI&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#23545;&#27604;&#20004;&#31181;&#24037;&#20855;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#65292;&#20840;&#38754;&#25506;&#31350;&#20102;GPT-4&#21644;&#20256;&#32479;AI&#24037;&#20855;&#30340;&#20248;&#32570;&#28857;&#12290;&#20004;&#20010;&#30495;&#23454;&#20020;&#24202;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#20687;GPT-4&#36825;&#26679;&#30340;LLMs&#22312;&#26410;&#26469;&#30340;&#30196;&#21574;&#30151;&#35786;&#26029;&#20013;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#20173;&#26080;&#27861;&#36229;&#36807;&#20256;&#32479;AI&#24037;&#20855;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#35780;&#20272;&#20102;GPT-4&#30340;&#35299;&#37322;&#24615;&#21644;&#21487;&#20449;&#24615;&#65292;&#21516;&#26102;&#20063;&#25351;&#20986;&#20102;&#23558;LLMs&#38598;&#25104;&#21040;&#21307;&#30103;&#24212;&#29992;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent investigations show that large language models (LLMs), specifically GPT-4, not only have remarkable capabilities in common Natural Language Processing (NLP) tasks but also exhibit human-level performance on various professional and academic benchmarks. However, whether GPT-4 can be directly used in practical applications and replace traditional artificial intelligence (AI) tools in specialized domains requires further experimental validation. In this paper, we explore the potential of LLMs such as GPT-4 to outperform traditional AI tools in dementia diagnosis. Comprehensive comparisons between GPT-4 and traditional AI tools are conducted to examine their diagnostic accuracy in a clinical setting. Experimental results on two real clinical datasets show that, although LLMs like GPT-4 demonstrate potential for future advancements in dementia diagnosis, they currently do not surpass the performance of traditional AI tools. The interpretability and faithfulness of GPT-4 are also eval
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CamemBERTa&#30340;&#27861;&#35821;DeBERTa&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#20351;&#29992;MLM&#35757;&#32451;&#30340;BERT-based&#27169;&#22411;&#65292;&#22312;&#30456;&#21516;&#25968;&#37327;&#30340;&#35757;&#32451;&#20196;&#29260;&#19979;&#24615;&#33021;&#26356;&#21152;&#20248;&#31168;&#65292;&#22312;&#22810;&#39033;&#27861;&#35821;&#19979;&#28216;&#20219;&#21153;&#20013;&#24050;&#32463;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#20102;CamemBERT&#12290;</title><link>http://arxiv.org/abs/2306.01497</link><description>&lt;p&gt;
&#20351;&#29992;CamemBERTa&#23454;&#29616;&#39640;&#25928;&#30340;&#27861;&#35821;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient French Language Modeling with CamemBERTa. (arXiv:2306.01497v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CamemBERTa&#30340;&#27861;&#35821;DeBERTa&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#20351;&#29992;MLM&#35757;&#32451;&#30340;BERT-based&#27169;&#22411;&#65292;&#22312;&#30456;&#21516;&#25968;&#37327;&#30340;&#35757;&#32451;&#20196;&#29260;&#19979;&#24615;&#33021;&#26356;&#21152;&#20248;&#31168;&#65292;&#22312;&#22810;&#39033;&#27861;&#35821;&#19979;&#28216;&#20219;&#21153;&#20013;&#24050;&#32463;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#20102;CamemBERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#30740;&#31350;&#22823;&#22823;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#34429;&#28982;&#36825;&#20123;&#36827;&#23637;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#30001;&#20110;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#21487;&#29992;&#24615;&#25152;&#25512;&#21160;&#65292;&#20294;&#23427;&#20204;&#20063;&#21463;&#30410;&#20110;&#26356;&#22909;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#26550;&#26500;&#30340;&#24320;&#21457;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;CamemBERTa&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31181;&#27861;&#35821;DeBERTa&#27169;&#22411;&#65292;&#22522;&#20110;DeBERTaV3&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#30446;&#26631;&#12290;&#20316;&#32773;&#22312;&#22810;&#20010;&#27861;&#35821;&#19979;&#28216;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#35789;&#24615;&#26631;&#27880;&#12289;&#20381;&#23384;&#35299;&#26512;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;FLUE&#22522;&#20934;&#65292;&#24182;&#19982;&#27861;&#35821;&#26368;&#20808;&#36827;&#30340;&#21333;&#35821;&#27169;&#22411;CamemBERT&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#32473;&#23450;&#30456;&#21516;&#25968;&#37327;&#30340;&#35757;&#32451;&#20196;&#29260;&#65292;&#26412;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20248;&#20110;&#20197;MLM&#35757;&#32451;&#30340;BERT-based&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#26412;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#19982;CamemBERT&#30456;&#20284;&#25110;&#26356;&#20248;&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#23427;&#26159;&#26032;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in NLP have significantly improved the performance of language models on a variety of tasks. While these advances are largely driven by the availability of large amounts of data and computational power, they also benefit from the development of better training methods and architectures. In this paper, we introduce CamemBERTa, a French DeBERTa model that builds upon the DeBERTaV3 architecture and training objective. We evaluate our model's performance on a variety of French downstream tasks and datasets, including question answering, part-of-speech tagging, dependency parsing, named entity recognition, and the FLUE benchmark, and compare against CamemBERT, the state-of-the-art monolingual model for French. Our results show that, given the same amount of training tokens, our model outperforms BERT-based models trained with MLM on most tasks. Furthermore, our new model reaches similar or superior performance on downstream tasks compared to CamemBERT, despite being trained 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#23558;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#26041;&#27861;&#19982;&#24320;&#28304;AI&#24211;Hugging Face&#21644;Pyserini&#24037;&#20855;&#38598;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#20379;&#19968;&#31181;&#29992;&#20110;NLP&#30740;&#31350;&#32773;&#30340;&#26816;&#32034;&#24335;&#25968;&#25454;&#20998;&#26512;&#30340;&#21487;&#38752;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.01481</link><description>&lt;p&gt;
Hugging Face&#21644;Pyserini&#20114;&#25805;&#20316;&#24615;&#22312;NLP&#35757;&#32451;&#25968;&#25454;&#25506;&#32034;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
GAIA Search: Hugging Face and Pyserini Interoperability for NLP Training Data Exploration. (arXiv:2306.01481v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#23558;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#26041;&#27861;&#19982;&#24320;&#28304;AI&#24211;Hugging Face&#21644;Pyserini&#24037;&#20855;&#38598;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#20379;&#19968;&#31181;&#29992;&#20110;NLP&#30740;&#31350;&#32773;&#30340;&#26816;&#32034;&#24335;&#25968;&#25454;&#20998;&#26512;&#30340;&#21487;&#38752;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27880;&#24847;&#21040;&#36843;&#20999;&#38656;&#35201;&#20026;&#29616;&#20195;NLP&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#35821;&#26009;&#24211;&#25552;&#20379;&#24555;&#36895;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#23450;&#24615;&#20998;&#26512;&#24037;&#20855;&#65292;&#22240;&#27492;&#25552;&#20986;&#21033;&#29992;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#39046;&#22495;&#30340;&#25104;&#29087;&#21644;&#32463;&#36807;&#20805;&#20998;&#27979;&#35797;&#30340;&#26041;&#27861; - &#36825;&#26159;&#19968;&#20010;&#38271;&#26399;&#22788;&#29702;TB&#32423;&#25991;&#26723;&#38598;&#21512;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#38598;&#25104;Pyserini - &#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21487;&#37325;&#22797;IR&#30740;&#31350;&#24037;&#20855;&#21253;&#19982;&#24320;&#28304;AI&#24211;&#21644;&#20154;&#24037;&#26234;&#33021;&#25991;&#26723;&#29983;&#24577;&#31995;&#32479;Hugging Face&#12290;&#25105;&#20204;&#21033;&#29992;&#20004;&#20010;&#24179;&#21488;&#30340;&#29616;&#26377;&#21151;&#33021;&#65292;&#21516;&#26102;&#25552;&#20986;&#26032;&#21151;&#33021;&#65292;&#36827;&#19968;&#27493;&#20419;&#36827;&#23427;&#20204;&#30340;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20026;NLP&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#24037;&#20855;&#65292;&#20197;&#20415;&#20182;&#20204;&#21487;&#20197;&#36731;&#26494;&#28789;&#27963;&#22320;&#24320;&#21457;&#22522;&#20110;&#26816;&#32034;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;Jupyter Notebook&#30340;&#28436;&#31034;&#27493;&#39588;&#65292;&#23637;&#31034;&#20102;&#26680;&#24515;&#20114;&#25805;&#20316;&#29305;&#24615;&#30340;&#21487;&#29992;&#24615;&#65292;&#35813;&#28436;&#31034;&#27493;&#39588;&#21487;&#20197;&#22312;GitHub&#19978;http s://github.com/huggingface/gaia&#19978;&#25214;&#21040;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#24819;&#27861;&#22914;&#20309;&#34987;&#25805;&#20316;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noticing the urgent need to provide tools for fast and user-friendly qualitative analysis of large-scale textual corpora of the modern NLP, we propose to turn to the mature and well-tested methods from the domain of Information Retrieval (IR) - a research field with a long history of tackling TB-scale document collections. We discuss how Pyserini - a widely used toolkit for reproducible IR research can be integrated with the Hugging Face ecosystem of open-source AI libraries and artifacts. We leverage the existing functionalities of both platforms while proposing novel features further facilitating their integration. Our goal is to give NLP researchers tools that will allow them to develop retrieval-based instrumentation for their data analytics needs with ease and agility. We include a Jupyter Notebook-based walk through the core interoperability features, available on GitHub at https://github.com/huggingface/gaia. We then demonstrate how the ideas we present can be operationalized to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#30340;&#25991;&#26412;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#20505;&#36873;&#36873;&#25321;&#38382;&#39064;&#20197;&#25552;&#39640;&#26367;&#25442;&#30340;&#21477;&#27861;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01471</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#30340;&#25991;&#26412;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Guiding Text-to-Text Privatization by Syntax. (arXiv:2306.01471v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01471
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#30340;&#25991;&#26412;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#20915;&#20505;&#36873;&#36873;&#25321;&#38382;&#39064;&#20197;&#25552;&#39640;&#26367;&#25442;&#30340;&#21477;&#27861;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#26631;&#24046;&#20998;&#38544;&#31169;&#26159;&#38024;&#23545;&#25991;&#26412;&#38544;&#31169;&#20445;&#25252;&#20013;&#30340;&#29420;&#29305;&#25361;&#25112;&#32780;&#35774;&#35745;&#30340;&#24046;&#20998;&#38544;&#31169;&#30340;&#19968;&#31181;&#25193;&#23637;&#12290;&#36890;&#36807;&#21521;&#23884;&#20837;&#30340;&#20960;&#20309;&#31354;&#38388;&#20013;&#30340;&#21333;&#35789;&#34920;&#31034;&#28155;&#21152;&#22122;&#22768;&#65292;&#21333;&#35789;&#34987;&#26367;&#25442;&#20026;&#22312;&#22122;&#22768;&#34920;&#31034;&#30340;&#25509;&#36817;&#20301;&#32622;&#30340;&#21333;&#35789;&#12290;&#30001;&#20110;&#23884;&#20837;&#24335;&#26159;&#22522;&#20110;&#21333;&#35789;&#20849;&#29616;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#36825;&#31181;&#26426;&#21046;&#30830;&#20445;&#26367;&#25442;&#28304;&#20110;&#30456;&#21516;&#30340;&#35821;&#20041;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#32771;&#34385;&#21333;&#35789;&#30340;&#35821;&#27861;&#31867;&#21035;&#65292;&#36825;&#31181;&#26426;&#21046;&#23601;&#26080;&#27861;&#20445;&#35777;&#26367;&#25442;&#25198;&#28436;&#30456;&#20284;&#30340;&#21477;&#27861;&#35282;&#33394;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25991;&#26412;&#38544;&#31169;&#20445;&#25252;&#22312;&#26367;&#25442;&#21518;&#20445;&#30041;&#21333;&#35789;&#35821;&#27861;&#31867;&#21035;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20195;&#29992;&#25991;&#26412;&#20013;&#20960;&#20046;&#23436;&#20840;&#30001;&#21517;&#35789;&#26500;&#25104;&#12290;&#30001;&#20110;&#32570;&#23569;&#20135;&#29983;&#19982;&#25935;&#24863;&#25991;&#26412;&#32467;&#26500;&#30456;&#20851;&#30340;&#20195;&#29992;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20445;&#25252;&#27493;&#39588;&#36716;&#25442;&#20026;&#20505;&#36873;&#36873;&#25321;&#38382;&#39064;&#26469;&#25193;&#23637;&#25105;&#20204;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Metric Differential Privacy is a generalization of differential privacy tailored to address the unique challenges of text-to-text privatization. By adding noise to the representation of words in the geometric space of embeddings, words are replaced with words located in the proximity of the noisy representation. Since embeddings are trained based on word co-occurrences, this mechanism ensures that substitutions stem from a common semantic context. Without considering the grammatical category of words, however, this mechanism cannot guarantee that substitutions play similar syntactic roles. We analyze the capability of text-to-text privatization to preserve the grammatical category of words after substitution and find that surrogate texts consist almost exclusively of nouns. Lacking the capability to produce surrogate texts that correlate with the structure of the sensitive texts, we encompass our analysis by transforming the privatization step into a candidate selection problem in whic
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25910;&#38598;&#33258;&#21160;&#35805;&#35821;&#20998;&#26512;&#30340;&#20462;&#36766;&#29305;&#24449;&#26469;&#23558;&#20462;&#36766;&#20449;&#24687;&#32435;&#20837;&#31070;&#32463;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#20420;&#35821;RuCoCo-23&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#26368;&#22909;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2306.01465</link><description>&lt;p&gt;
&#24102;&#26377;&#20998;&#23618;&#35805;&#35821;&#29305;&#24449;&#30340;&#20420;&#35821;&#36731;&#37327;&#32423;&#25351;&#20195;&#28040;&#35299;
&lt;/p&gt;
&lt;p&gt;
Light Coreference Resolution for Russian with Hierarchical Discourse Features. (arXiv:2306.01465v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25910;&#38598;&#33258;&#21160;&#35805;&#35821;&#20998;&#26512;&#30340;&#20462;&#36766;&#29305;&#24449;&#26469;&#23558;&#20462;&#36766;&#20449;&#24687;&#32435;&#20837;&#31070;&#32463;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#21462;&#24471;&#20102;&#20420;&#35821;RuCoCo-23&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#26368;&#22909;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20195;&#28040;&#35299;&#26159;&#35782;&#21035;&#21644;&#20998;&#32452;&#25351;&#31216;&#21516;&#19968;&#29616;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#20197;&#24448;&#31070;&#32463;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#20110;&#23398;&#20064;&#36328;&#24230;&#34920;&#31034;&#21644;&#37197;&#23545;&#24471;&#20998;&#20197;&#36827;&#34892;&#25351;&#20195;&#28040;&#35299;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#27809;&#26377;&#26126;&#30830;&#25429;&#25417;&#21040;&#23618;&#32423;&#35805;&#35821;&#20013;&#30340;&#25351;&#31216;&#36873;&#25321;&#65292;&#36825;&#26159;&#25351;&#20195;&#28040;&#35299;&#20013;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#20462;&#36766;&#20449;&#24687;&#32435;&#20837;&#31070;&#32463;&#25351;&#20195;&#28040;&#35299;&#27169;&#22411;&#20013;&#65292;&#25910;&#38598;&#33258;&#21160;&#35805;&#35821;&#20998;&#26512;&#30340;&#20462;&#36766;&#29305;&#24449;&#24182;&#26816;&#26597;&#20854;&#24433;&#21709;&#12290;&#20316;&#20026;&#22522;&#32447;&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;&#37096;&#20998;&#24494;&#35843;&#30340;&#22810;&#35821;&#35328;&#23454;&#20307;&#24863;&#30693;&#35821;&#35328;&#27169;&#22411;LUKE&#23454;&#29616;&#20102;&#31471;&#21040;&#31471;&#30340;&#22522;&#20110;&#36328;&#24230;&#30340;&#25351;&#20195;&#28040;&#35299;&#35299;&#26512;&#22120;&#12290;&#25105;&#20204;&#22312;&#20420;&#35821;&#30340;RuCoCo-23&#20849;&#20139;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#26368;&#20339;&#27169;&#22411;&#37319;&#29992;&#20102;&#25552;&#21450;&#20043;&#38388;&#30340;&#20462;&#36766;&#36317;&#31163;&#65292;&#24182;&#22312;&#24320;&#21457;&#38598;&#65288;74.6&#65285; F1&#65289;&#19978;&#25490;&#21517;&#31532;1&#65292;&#22312;&#27979;&#35797;&#38598;&#65288;73.3&#65285; F1&#65289;&#19978;&#25490;&#21517;&#31532;2&#12290;
&lt;/p&gt;
&lt;p&gt;
Coreference resolution is the task of identifying and grouping mentions referring to the same real-world entity. Previous neural models have mainly focused on learning span representations and pairwise scores for coreference decisions. However, current methods do not explicitly capture the referential choice in the hierarchical discourse, an important factor in coreference resolution. In this study, we propose a new approach that incorporates rhetorical information into neural coreference resolution models. We collect rhetorical features from automated discourse parses and examine their impact. As a base model, we implement an end-to-end span-based coreference resolver using a partially fine-tuned multilingual entity-aware language model LUKE. We evaluate our method on the RuCoCo-23 Shared Task for coreference resolution in Russian. Our best model employing rhetorical distance between mentions has ranked 1st on the development set (74.6% F1) and 2nd on the test set (73.3% F1) of the Sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24230;&#37327;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#22312;&#27880;&#20837;&#22122;&#22768;&#20043;&#21069;&#21152;&#20837;&#20102;&#19968;&#20010;&#35821;&#20041;&#28040;&#27495;&#27493;&#39588;&#20197;&#25552;&#39640;&#27495;&#20041;&#21333;&#35789;&#26367;&#20195;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#8220;&#19978;&#19979;&#25991;&#20013;&#30340;&#21333;&#35789;&#8221;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01457</link><description>&lt;p&gt;
&#23558;&#19978;&#19979;&#25991;&#32435;&#20837;&#25991;&#26412;-&#25991;&#26412;&#38544;&#31169;&#20445;&#25252;&#20013;
&lt;/p&gt;
&lt;p&gt;
Driving Context into Text-to-Text Privatization. (arXiv:2306.01457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24230;&#37327;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#22312;&#27880;&#20837;&#22122;&#22768;&#20043;&#21069;&#21152;&#20837;&#20102;&#19968;&#20010;&#35821;&#20041;&#28040;&#27495;&#27493;&#39588;&#20197;&#25552;&#39640;&#27495;&#20041;&#21333;&#35789;&#26367;&#20195;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#8220;&#19978;&#19979;&#25991;&#20013;&#30340;&#21333;&#35789;&#8221;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#24230;&#37327;&#24046;&#20998;&#38544;&#31169;&#8221;&#36890;&#36807;&#21521;&#20174;&#23884;&#20837;&#31354;&#38388;&#20013;&#23548;&#20986;&#30340;&#21333;&#35789;&#21521;&#37327;&#28155;&#21152;&#32463;&#36807;&#26657;&#20934;&#30340;&#22122;&#22768;&#65292;&#24182;&#20351;&#29992;&#26368;&#36817;&#37051;&#25628;&#32034;&#23558;&#36825;&#20123;&#26377;&#22122;&#22768;&#30340;&#21521;&#37327;&#25237;&#24433;&#22238;&#31163;&#25955;&#35789;&#27719;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#38544;&#31169;&#20445;&#25252;&#21151;&#33021;&#12290;&#30001;&#20110;&#21333;&#35789;&#26159;&#27809;&#26377;&#19978;&#19979;&#25991;&#30340;&#26367;&#20195;&#65292;&#22240;&#27492;&#35813;&#26426;&#21046;&#21487;&#33021;&#26080;&#27861;&#25214;&#21040;&#20855;&#26377;&#27495;&#20041;&#21547;&#20041;&#30340;&#21333;&#35789;&#30340;&#26367;&#20195;&#35789;&#65292;&#20363;&#22914;&#8220;bank&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#27169;&#26865;&#20004;&#21487;&#30340;&#21333;&#35789;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#31181;&#35821;&#20041;&#23884;&#20837;&#24182;&#22312;&#27880;&#20837;&#22122;&#22768;&#20043;&#21069;&#21152;&#20837;&#20102;&#19968;&#20010;&#35821;&#20041;&#28040;&#27495;&#27493;&#39588;&#12290;&#25105;&#20204;&#36824;&#20272;&#35745;&#20102;&#38544;&#31169;&#21644;&#23454;&#29992;&#24615;&#23545;&#38544;&#31169;&#20445;&#25252;&#26426;&#21046;&#30340;&#20462;&#25913;&#12290;&#22312;&#8220;&#19978;&#19979;&#25991;&#20013;&#30340;&#21333;&#35789;&#8221;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#21333;&#35789;&#20041;&#28040;&#27495;&#35797;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20998;&#31867;&#20934;&#30830;&#24230;&#30340;&#26174;&#33879;&#22686;&#21152;&#65292;&#36798;&#21040;&#20102;$6.05&#65285;$&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{Metric Differential Privacy} enables text-to-text privatization by adding calibrated noise to the vector of a word derived from an embedding space and projecting this noisy vector back to a discrete vocabulary using a nearest neighbor search. Since words are substituted without context, this mechanism is expected to fall short at finding substitutes for words with ambiguous meanings, such as \textit{'bank'}. To account for these ambiguous words, we leverage a sense embedding and incorporate a sense disambiguation step prior to noise injection. We encompass our modification to the privatization mechanism with an estimation of privacy and utility. For word sense disambiguation on the \textit{Words in Context} dataset, we demonstrate a substantial increase in classification accuracy by $6.05\%$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#24773;&#24863;&#35302;&#21457;&#22120;&#24182;&#24635;&#32467;&#23427;&#20204;&#65292;&#20197;&#21462;&#20195;&#32791;&#26102;&#26114;&#36149;&#30340;&#25688;&#35201;&#29983;&#25104;&#36807;&#31243;&#65292;&#20026;&#28798;&#38590;&#21709;&#24212;&#25552;&#20379;&#24517;&#35201;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.01444</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#24773;&#24863;&#35302;&#21457;&#35789;&#25277;&#21462;&#24335;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Extractive Summarization of Emotion Triggers. (arXiv:2306.01444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#24773;&#24863;&#35302;&#21457;&#22120;&#24182;&#24635;&#32467;&#23427;&#20204;&#65292;&#20197;&#21462;&#20195;&#32791;&#26102;&#26114;&#36149;&#30340;&#25688;&#35201;&#29983;&#25104;&#36807;&#31243;&#65292;&#20026;&#28798;&#38590;&#21709;&#24212;&#25552;&#20379;&#24517;&#35201;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#21361;&#26426;&#20107;&#20214;&#20013;&#29702;&#35299;&#23548;&#33268;&#24773;&#32490;&#20135;&#29983;&#30340;&#22240;&#32032;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#20026;&#34920;&#36798;&#30340;&#24773;&#24863;&#25552;&#20379;&#22522;&#30784;&#65292;&#24182;&#38543;&#21518;&#25913;&#21892;&#23545;&#27491;&#22312;&#21457;&#29983;&#30340;&#28798;&#23475;&#30340;&#29702;&#35299;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#35757;&#32451;&#20102;&#30417;&#30563;&#27169;&#22411;&#65292;&#36890;&#36807;&#25688;&#35201;&#25552;&#21462;&#26469;&#26816;&#27979;&#24773;&#24863;&#21644;&#35299;&#37322;&#24773;&#24863;&#35302;&#21457;&#22120;&#65288;&#20107;&#20214;&#21644;&#35780;&#20272;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#26102;&#38388;&#25935;&#24863;&#12289;&#39640;&#39118;&#38505;&#30340;&#24773;&#22659;&#19979;&#65292;&#33719;&#21462;&#21450;&#26102;&#21644;&#20248;&#36136;&#30340;&#25688;&#35201;&#38750;&#24120;&#26114;&#36149;&#19988;&#32791;&#26102;&#65292;&#38656;&#35201;&#32463;&#36807;&#39640;&#24230;&#35757;&#32451;&#30340;&#19987;&#23478;&#27880;&#37322;&#32773;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#30861;&#24517;&#35201;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#36716;&#32780;&#36861;&#27714;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#35302;&#21457;&#22120;&#30340;&#26080;&#30417;&#30563;&#31995;&#32479;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837; CovidET-EXT &#25968;&#25454;&#38598;&#65292;&#25193;&#20805;&#20102;&#65288;Zhan &#31561;&#20154;&#65292;2022&#65289;&#22312; COVID-19 &#21361;&#26426;&#32972;&#26223;&#19979;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20849;&#21516;&#26816;&#27979;&#24773;&#24863;&#24182;&#24635;&#32467;&#23427;&#20204;&#30340;&#35302;&#21457;&#22120;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#26041;&#27861;&#26159; Emotion-Aware Pagerank&#65292;&#23427;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#24773;&#24863;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding what leads to emotions during large-scale crises is important as it can provide groundings for expressed emotions and subsequently improve the understanding of ongoing disasters. Recent approaches trained supervised models to both detect emotions and explain emotion triggers (events and appraisals) via abstractive summarization. However, obtaining timely and qualitative abstractive summaries is expensive and extremely time-consuming, requiring highly-trained expert annotators. In time-sensitive, high-stake contexts, this can block necessary responses. We instead pursue unsupervised systems that extract triggers from text. First, we introduce CovidET-EXT, augmenting (Zhan et al. 2022)'s abstractive dataset (in the context of the COVID-19 crisis) with extractive triggers. Second, we develop new unsupervised learning models that can jointly detect emotions and summarize their triggers. Our best approach, entitled Emotion-Aware Pagerank, incorporates emotion information from 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22810;&#35789;&#34920;&#36798;&#24335;&#25913;&#20889;&#26041;&#27861;&#65292;&#20351;&#29992;&#30340;&#25968;&#25454;&#21644;&#24037;&#20855;&#38750;&#24120;&#31616;&#21333;&#65292;&#19988;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#29087;&#35821;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#20219;&#21153;&#20013;&#24615;&#33021;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.01443</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#22810;&#35789;&#34920;&#36798;&#25913;&#20889;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Paraphrasing of Multiword Expressions. (arXiv:2306.01443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22810;&#35789;&#34920;&#36798;&#24335;&#25913;&#20889;&#26041;&#27861;&#65292;&#20351;&#29992;&#30340;&#25968;&#25454;&#21644;&#24037;&#20855;&#38750;&#24120;&#31616;&#21333;&#65292;&#19988;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#29087;&#35821;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#20219;&#21153;&#20013;&#24615;&#33021;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25913;&#20889;&#22810;&#35789;&#34920;&#36798;&#24335;&#65288;MWEs&#65289;&#30340;&#26041;&#27861;&#65292;&#21482;&#20351;&#29992;&#21333;&#35821;&#26009;&#24211;&#25968;&#25454;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#26080;&#38656;&#24494;&#35843;&#65289;&#65292;&#19981;&#20351;&#29992;&#20219;&#20309;&#22806;&#37096;&#36164;&#28304;&#65292;&#22914;&#23383;&#20856;&#12290;&#25105;&#20204;&#22312;SemEval 2022&#29087;&#35821;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#20248;&#20110;&#25152;&#26377;&#26080;&#30417;&#30563;&#31995;&#32479;&#24182;&#19982;&#26377;&#30417;&#30563;&#31995;&#32479;&#30456;&#21305;&#25932;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an unsupervised approach to paraphrasing multiword expressions (MWEs) in context. Our model employs only monolingual corpus data and pre-trained language models (without fine-tuning), and does not make use of any external resources such as dictionaries. We evaluate our method on the SemEval 2022 idiomatic semantic text similarity task, and show that it outperforms all unsupervised systems and rivals supervised systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TVC-GMM&#30340;&#19977;&#20803;&#38142;&#24335;&#39640;&#26031;&#20998;&#24067;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;FastSpeech 2&#21512;&#25104;&#34920;&#29616;&#24615;&#35821;&#38899;&#25968;&#25454;&#38598;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;Mel&#39057;&#35889;&#24179;&#28369;&#24230;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#25552;&#39640;&#38899;&#39057;&#30340;&#21548;&#35273;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.01442</link><description>&lt;p&gt;
&#36890;&#36807;&#24314;&#27169;&#27531;&#24046;&#22810;&#27169;&#24577;&#23454;&#29616;&#40065;&#26834;&#30340;FastSpeech 2
&lt;/p&gt;
&lt;p&gt;
Towards Robust FastSpeech 2 by Modelling Residual Multimodality. (arXiv:2306.01442v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01442
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TVC-GMM&#30340;&#19977;&#20803;&#38142;&#24335;&#39640;&#26031;&#20998;&#24067;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;FastSpeech 2&#21512;&#25104;&#34920;&#29616;&#24615;&#35821;&#38899;&#25968;&#25454;&#38598;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;Mel&#39057;&#35889;&#24179;&#28369;&#24230;&#24046;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#25552;&#39640;&#38899;&#39057;&#30340;&#21548;&#35273;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;FastSpeech 2&#30340;&#38750;&#33258;&#22238;&#24402;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#21487;&#20197;&#39640;&#25928;&#22320;&#21512;&#25104;&#39640;&#20445;&#30495;&#24230;&#21644;&#33258;&#28982;&#24230;&#30340;&#35821;&#38899;&#65292;&#20294;&#23545;&#20110;&#34920;&#29616;&#24615;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#29305;&#24449;&#38899;&#39057;&#22833;&#30495;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36825;&#20123;&#20266;&#24433;&#26159;&#30001;&#20110;&#36807;&#24230;&#24179;&#28369;&#30340;Mel&#39057;&#35889;&#39044;&#27979;&#24341;&#20837;&#30340;&#65292;&#32780;&#36825;&#26159;&#30001;&#20110;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#25439;&#22833;&#26469;&#35757;&#32451;Mel&#39057;&#35889;&#35299;&#30721;&#22120;&#25152;&#33268;&#12290;FastSpeech 2&#20351;&#29992;MSE&#25439;&#22833;&#34987;&#38480;&#21046;&#20026;&#23398;&#20064;&#35757;&#32451;&#20998;&#24067;&#30340;&#26465;&#20214;&#24179;&#22343;&#20540;&#65292;&#22914;&#26524;&#25152;&#26377;&#30340;&#35843;&#21046;&#20449;&#21495;&#21518;&#20998;&#24067;&#20173;&#28982;&#21576;&#29616;&#22810;&#27169;&#24577;&#20998;&#24067;&#65292;&#21017;&#36825;&#20123;&#20540;&#21487;&#33021;&#19982;&#33258;&#28982;&#26679;&#26412;&#24182;&#19981;&#25509;&#36817;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TVC-GMM&#65292;&#36825;&#26159;&#19968;&#31181;&#19977;&#20803;&#38142;&#24335;&#39640;&#26031;&#20998;&#24067;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#27169;&#25311;&#27531;&#24046;&#22810;&#27169;&#24577;&#12290;TVC-GMM&#38477;&#20302;&#20102;&#39057;&#35889;&#30340;&#24179;&#28369;&#24615;&#65292;&#24182;&#36890;&#36807;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#35777;&#26126;&#20102;&#23545;&#34920;&#29616;&#24615;&#25968;&#25454;&#38598;&#29305;&#21035;&#26377;&#25928;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#21548;&#35273;&#38899;&#39057;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art non-autoregressive text-to-speech (TTS) models based on FastSpeech 2 can efficiently synthesise high-fidelity and natural speech. For expressive speech datasets however, we observe characteristic audio distortions. We demonstrate that such artefacts are introduced to the vocoder reconstruction by over-smooth mel-spectrogram predictions, which are induced by the choice of mean-squared-error (MSE) loss for training the mel-spectrogram decoder. With MSE loss FastSpeech 2 is limited to learn conditional averages of the training distribution, which might not lie close to a natural sample if the distribution still appears multimodal after all conditioning signals. To alleviate this problem, we introduce TVC-GMM, a mixture model of Trivariate-Chain Gaussian distributions, to model the residual multimodality. TVC-GMM reduces spectrogram smoothness and improves perceptual audio quality in particular for expressive datasets as shown by both objective and subjective evaluation.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NUDGE&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;&#36923;&#36753;&#35268;&#21017;&#30340;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.01439</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#24341;&#23548;&#31526;&#21495;&#25277;&#35937;&#23454;&#29616;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#36923;&#36753;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Interpretable and Explainable Logical Policies via Neurally Guided Symbolic Abstraction. (arXiv:2306.01439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01439
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NUDGE&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;&#36923;&#36753;&#35268;&#21017;&#30340;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#35201;&#30340;&#26377;&#38480;&#20808;&#39564;&#20351;&#20854;&#25104;&#20026;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#32534;&#30721;&#21644;&#23398;&#20064;&#31574;&#30053;&#30340;&#20027;&#35201;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#26159;&#40657;&#21283;&#23376;&#65292;&#22312;&#24037;&#20316;&#22312;&#22270;&#20687;&#32423;&#21035;&#26102;&#38590;&#20197;&#29702;&#35299;&#20195;&#29702;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#31070;&#32463;&#31526;&#21495;RL&#26088;&#22312;&#39318;&#20808;&#21019;&#24314;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21487;&#35299;&#37322;&#24615;&#19981;&#24847;&#21619;&#30528;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#35299;&#37322;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31070;&#32463;&#24341;&#23548;&#21487;&#24494;&#20998;&#36923;&#36753;&#31574;&#30053;&#65288;NUDGE&#65289;&#12290;NUDGE&#21033;&#29992;&#35757;&#32451;&#22909;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#20195;&#29702;&#26469;&#24341;&#23548;&#20505;&#36873;&#21152;&#26435;&#36923;&#36753;&#35268;&#21017;&#30340;&#25628;&#32034;&#65292;&#28982;&#21518;&#20351;&#29992;&#21487;&#24494;&#20998;&#30340;&#36923;&#36753;&#26469;&#35757;&#32451;&#36923;&#36753;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;NUDGE&#20195;&#29702;&#21487;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#30340;&#31574;&#30053;&#65292;&#21516;&#26102;&#32988;&#36807;&#32431;&#31070;&#32463;&#20195;&#29702;&#65292;&#24182;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#28789;&#27963;&#24615;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#21021;&#22987;&#29366;&#24577;&#21644;&#38382;&#39064;&#22823;&#23567;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
The limited priors required by neural networks make them the dominating choice to encode and learn policies using reinforcement learning (RL). However, they are also black-boxes, making it hard to understand the agent's behaviour, especially when working on the image level. Therefore, neuro-symbolic RL aims at creating policies that are interpretable in the first place. Unfortunately, interpretability is not explainability. To achieve both, we introduce Neurally gUided Differentiable loGic policiEs (NUDGE). NUDGE exploits trained neural network-based agents to guide the search of candidate-weighted logic rules, then uses differentiable logic to train the logic agents. Our experimental evaluation demonstrates that NUDGE agents can induce interpretable and explainable policies while outperforming purely neural ones and showing good flexibility to environments of different initial states and problem sizes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#25968;&#20540;&#25512;&#29702;&#32435;&#20837;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#65292;&#23558;&#25968;&#20540;&#35270;&#20026;&#19968;&#31561;&#20844;&#27665;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#23454;&#20307;&#21644;&#25968;&#20540;&#36827;&#34892;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#26597;&#35810;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2306.01399</link><description>&lt;p&gt;
&#23454;&#20307;&#21644;&#25968;&#23383;&#20540;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Reasoning over Entities and Numerical Values. (arXiv:2306.01399v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23558;&#25968;&#20540;&#25512;&#29702;&#32435;&#20837;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#65292;&#23558;&#25968;&#20540;&#35270;&#20026;&#19968;&#31561;&#20844;&#27665;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#23454;&#20307;&#21644;&#25968;&#20540;&#36827;&#34892;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#26597;&#35810;&#30340;&#26377;&#25928;&#21644;&#39640;&#25928;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#25351;&#30340;&#26159;&#20197;&#36923;&#36753;&#24418;&#24335;&#34920;&#36798;&#30340;&#26597;&#35810;&#65292;&#20256;&#36798;&#22797;&#26434;&#30340;&#21547;&#20041;&#65292;&#27604;&#22914;&#21152;&#25343;&#22823;&#30340;&#22270;&#28789;&#22870;&#24471;&#20027;&#27605;&#19994;&#20110;&#21738;&#37324;&#65311;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20363;&#22914;&#23545;&#35805;&#31995;&#32479;&#21644;&#20132;&#20114;&#24335;&#25628;&#32034;&#24341;&#25806;&#65292;&#20381;&#36182;&#20110;&#22238;&#31572;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#20316;&#20026;&#22522;&#26412;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;&#22823;&#22810;&#25968;&#30693;&#35782;&#22270;&#35889;&#20013;&#65292;&#36793;&#32536;&#36890;&#24120;&#29992;&#20110;&#25551;&#36848;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#25110;&#23427;&#20204;&#30340;&#20851;&#32852;&#23646;&#24615;&#20540;&#12290;&#23646;&#24615;&#20540;&#21487;&#20197;&#26159;&#20998;&#31867;&#25110;&#25968;&#20540;&#26684;&#24335;&#65292;&#20363;&#22914;&#26085;&#26399;&#12289;&#24180;&#20221;&#12289;&#23610;&#23544;&#31561;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#65288;CQA&#65289;&#26041;&#27861;&#20165;&#23558;&#25968;&#20540;&#19982;&#23454;&#20307;&#35270;&#20026;&#30456;&#21516;&#12290;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#22238;&#31572;&#26576;&#20123;&#26597;&#35810;&#30340;&#22256;&#38590;&#65292;&#20363;&#22914;&#21738;&#20010;&#28595;&#22823;&#21033;&#20122;&#30340;&#26222;&#21033;&#31574;&#22870;&#24471;&#20027;&#20986;&#29983;&#20110;1927&#24180;&#20043;&#21069;&#65292;&#21738;&#31181;&#33647;&#29289;&#26159;&#19968;&#31181;&#27490;&#30171;&#21058;&#65292;&#24182;&#19988;&#21103;&#20316;&#29992;&#27604;&#24067;&#27931;&#33452;&#26356;&#23569;&#12290;&#26412;&#25991;&#21463;&#21040;&#25968;&#20540;&#32534;&#30721;&#26368;&#36817;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#25968;&#20540;&#25512;&#29702;&#32435;&#20837;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25968;&#20540;&#35270;&#20026;&#19968;&#31561;&#20844;&#27665;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#23454;&#20307;&#21644;&#25968;&#20540;&#36827;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;WIKWD&#21644;WebQSP&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22238;&#31572;&#28041;&#21450;&#25968;&#20540;&#30340;&#22797;&#26434;&#26597;&#35810;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
A complex logic query in a knowledge graph refers to a query expressed in logic form that conveys a complex meaning, such as where did the Canadian Turing award winner graduate from? Knowledge graph reasoning-based applications, such as dialogue systems and interactive search engines, rely on the ability to answer complex logic queries as a fundamental task. In most knowledge graphs, edges are typically used to either describe the relationships between entities or their associated attribute values. An attribute value can be in categorical or numerical format, such as dates, years, sizes, etc. However, existing complex query answering (CQA) methods simply treat numerical values in the same way as they treat entities. This can lead to difficulties in answering certain queries, such as which Australian Pulitzer award winner is born before 1927, and which drug is a pain reliever and has fewer side effects than Paracetamol. In this work, inspired by the recent advances in numerical encoding
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;NMT&#20013;&#22522;&#20110;&#23376;&#35789;&#30340;&#20998;&#35789;&#26041;&#27861;&#20013;&#65292;&#39057;&#29575;&#23545;&#20110;&#27169;&#22411;&#30340;&#34920;&#29616;&#36129;&#29486;&#21344;&#25454;&#20102;90%-95%&#65292;&#22240;&#27492;&#30456;&#23545;&#20110;&#32452;&#21512;&#24615;&#65292;&#39057;&#29575;&#26356;&#20026;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.01393</link><description>&lt;p&gt;
NMT&#20013;&#22522;&#20110;&#23376;&#35789;&#30340;&#20998;&#35789;&#20013;&#65292;&#39057;&#29575;&#21644;&#32452;&#21512;&#24615;&#30340;&#37325;&#35201;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Assessing the Importance of Frequency versus Compositionality for Subword-based Tokenization in NMT. (arXiv:2306.01393v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;NMT&#20013;&#22522;&#20110;&#23376;&#35789;&#30340;&#20998;&#35789;&#26041;&#27861;&#20013;&#65292;&#39057;&#29575;&#23545;&#20110;&#27169;&#22411;&#30340;&#34920;&#29616;&#36129;&#29486;&#21344;&#25454;&#20102;90%-95%&#65292;&#22240;&#27492;&#30456;&#23545;&#20110;&#32452;&#21512;&#24615;&#65292;&#39057;&#29575;&#26356;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#35789;&#20998;&#35789;&#26159;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21644;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20013;&#30340;&#40664;&#35748;&#26631;&#20934;&#12290;&#39057;&#32321;&#24341;&#29992;&#23376;&#35789;&#30340;&#20248;&#28857;&#26377;&#65306;&#23545;&#39057;&#32321;&#35789;&#35821;&#36827;&#34892;&#26356;&#30701;&#32534;&#30721;&#65292;&#23376;&#35789;&#32452;&#21512;&#24615;&#24378;&#20197;&#21450;&#22788;&#29702;&#26410;&#30693;&#35789;&#35821;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#23578;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#35789;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39057;&#29575;&#65288;&#31532;&#19968;&#20010;&#20248;&#28857;&#65289;&#19982;&#32452;&#21512;&#24615;&#20998;&#31163;&#24320;&#26469;&#65292;&#20351;&#29992;&#38669;&#22827;&#26364;&#32534;&#30721;&#23545;&#21333;&#35789;&#36827;&#34892;&#20998;&#35789;&#65292;&#25353;&#39057;&#29575;&#39034;&#24207;&#65292;&#20351;&#29992;&#22266;&#23450;&#25968;&#37327;&#30340;&#31526;&#21495;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;CS-DE&#12289;EN-FR&#21644;EN-DE NMT&#20013;&#65292;&#20165;&#39057;&#29575;&#23601;&#21344;&#20102;BPE&#24471;&#20998;&#30340;90%-95%&#65292;&#22240;&#27492;&#32452;&#21512;&#24615;&#24182;&#19981;&#20687;&#20197;&#21069;&#35748;&#20026;&#30340;&#37027;&#20040;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subword tokenization is the de facto standard for tokenization in neural language models and machine translation systems. Three advantages are frequently cited in favor of subwords: shorter encoding of frequent tokens, compositionality of subwords, and ability to deal with unknown words. As their relative importance is not entirely clear yet, we propose a tokenization approach that enables us to separate frequency (the first advantage) from compositionality. The approach uses Huffman coding to tokenize words, by order of frequency, using a fixed amount of symbols. Experiments with CS-DE, EN-FR and EN-DE NMT show that frequency alone accounts for 90%-95% of the scores reached by BPE, hence compositionality has less importance than previously thought.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#38754;&#21521;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;ChatGPT&#65292;&#35813;&#26041;&#27861;&#22312;&#21021;&#27493;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;&#27169;&#22411;&#30340;&#22266;&#26377;&#29305;&#24615;&#38480;&#21046;&#20102;&#20854;&#26367;&#20195;&#19987;&#29992;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26377;&#26395;&#25104;&#20026;&#24320;&#21457;&#19987;&#29992;&#21644;&#21160;&#24577;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.01386</link><description>&lt;p&gt;
&#38754;&#21521;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;ChatGPT: &#35299;&#20915;&#26041;&#26696;&#36824;&#26159;&#26426;&#20250;&#65311;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Zero-shot Dialogue State Tracking: A Solution or an Opportunity?. (arXiv:2306.01386v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01386
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#38754;&#21521;&#38646;-shot&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;ChatGPT&#65292;&#35813;&#26041;&#27861;&#22312;&#21021;&#27493;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;&#27169;&#22411;&#30340;&#22266;&#26377;&#29305;&#24615;&#38480;&#21046;&#20102;&#20854;&#26367;&#20195;&#19987;&#29992;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#20294;&#20854;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26377;&#26395;&#25104;&#20026;&#24320;&#21457;&#19987;&#29992;&#21644;&#21160;&#24577;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#20801;&#35768;&#23569;&#37327;&#21644;&#38646;-shot&#36716;&#31227;&#21040;&#26032;&#39046;&#22495;&#25110;&#27169;&#24335;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#25552;&#21319;&#20005;&#37325;&#20381;&#36182;&#20110;&#31215;&#26497;&#30340;&#25968;&#25454;&#22686;&#24378;&#21644;&#22522;&#20110;&#36234;&#26469;&#36234;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#37327;&#22810;&#26679;&#21270;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;&#35757;&#32451;&#65292;&#25215;&#35834;&#21487;&#20197;&#35299;&#20915;&#20219;&#20309;&#31867;&#22411;&#30340;&#20219;&#21153;&#32780;&#26080;&#38656;&#29305;&#23450;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;ChatGPT&#30740;&#31350;&#39044;&#35272;&#20013;&#21576;&#29616;&#20102;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#65292;&#34920;&#26126;ChatGPT&#22312;&#38646;-shot DST&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#21457;&#29616;&#22914;&#27492;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#36890;&#29992;&#27169;&#22411;&#22266;&#26377;&#30340;&#29305;&#24615;&#38480;&#21046;&#20102;&#23427;&#20204;&#26367;&#25442;&#19987;&#29992;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#27979;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21487;&#33021;&#25104;&#20026;&#25903;&#25345;&#19987;&#38376;&#21644;&#21160;&#24577;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#22120;&#24320;&#21457;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research on dialogue state tracking (DST) focuses on methods that allow few- and zero-shot transfer to new domains or schemas. However, performance gains heavily depend on aggressive data augmentation and fine-tuning of ever larger language model based architectures. In contrast, general purpose language models, trained on large amounts of diverse data, hold the promise of solving any kind of task without task-specific training. We present preliminary experimental results on the ChatGPT research preview, showing that ChatGPT achieves state-of-the-art performance in zero-shot DST. Despite our findings, we argue that properties inherent to general purpose models limit their ability to replace specialized systems. We further theorize that the in-context learning capabilities of such models will likely become powerful tools to support the development of dedicated and dynamic dialogue state trackers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27880;&#24847;&#21147;&#22836;&#21098;&#26525;&#26041;&#27861;&#21644;Straight-Through Estimator&#65292;&#29992;&#20110;&#21152;&#36895;&#27169;&#22411;&#21098;&#26525;&#65292;&#20197;&#35299;&#20915;&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22823;&#20869;&#23384;&#21644;&#24378;&#35745;&#31639;&#38656;&#27714;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#20219;&#21153;&#20013;&#21462;&#24471;&#24456;&#22909;&#30340;&#24615;&#33021;&#19988;&#21442;&#25968;&#20943;&#23569;&#19988;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.01385</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#26080;&#20851;&#30340;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;&#32467;&#26500;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Task-Agnostic Structured Pruning of Speech Representation Models. (arXiv:2306.01385v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01385
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27880;&#24847;&#21147;&#22836;&#21098;&#26525;&#26041;&#27861;&#21644;Straight-Through Estimator&#65292;&#29992;&#20110;&#21152;&#36895;&#27169;&#22411;&#21098;&#26525;&#65292;&#20197;&#35299;&#20915;&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22823;&#20869;&#23384;&#21644;&#24378;&#35745;&#31639;&#38656;&#27714;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#20219;&#21153;&#20013;&#21462;&#24471;&#24456;&#22909;&#30340;&#24615;&#33021;&#19988;&#21442;&#25968;&#20943;&#23569;&#19988;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22914;Wav2vec2&#12289;Hubert&#21644;WavLM&#24050;&#34987;&#35777;&#26126;&#33021;&#26174;&#33879;&#25552;&#39640;&#35768;&#22810;&#35821;&#38899;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24222;&#22823;&#30340;&#20869;&#23384;&#21644;&#24378;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24037;&#19994;&#24212;&#29992;&#12290;&#32467;&#26500;&#21270;&#21098;&#26525;&#26159;&#19968;&#31181;&#30828;&#20214;&#21451;&#22909;&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#20294;&#36890;&#24120;&#20250;&#23548;&#33268;&#26356;&#22823;&#30340;&#31934;&#24230;&#25439;&#22833;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27880;&#24847;&#21147;&#22836;&#21098;&#26525;&#26041;&#27861;&#26469;&#24357;&#34917;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;L0&#35268;&#21017;&#21270;&#20013;&#24341;&#20837;&#20102;Straight-Through Estimator&#26469;&#36827;&#19968;&#27493;&#21152;&#36895;&#21098;&#26525;&#27169;&#22411;&#12290;&#22312;SUPERB&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36798;&#21040;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#24179;&#22343;&#20248;&#20110;Wav2vec 2.0&#22522;&#20934;&#27169;&#22411;&#65292;&#21516;&#26102;&#21442;&#25968;&#20943;&#23569;72&#65285;&#65292;&#25512;&#29702;&#36895;&#24230;&#25552;&#39640;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised pre-trained models such as Wav2vec2, Hubert, and WavLM have been shown to significantly improve many speech tasks. However, their large memory and strong computational requirements hinder their industrial applicability. Structured pruning is a hardware-friendly model compression technique but usually results in a larger loss of accuracy. In this paper, we propose a fine-grained attention head pruning method to compensate for the performance degradation. In addition, we also introduce the straight through estimator into the L0 regularization to further accelerate the pruned model. Experiments on the SUPERB benchmark show that our model can achieve comparable performance to the dense model in multiple tasks and outperforms the Wav2vec 2.0 base model on average, with 72% fewer parameters and 2 times faster inference speed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;(ITFT)&#23545;&#20110;&#20302;&#36164;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#39046;&#22495;&#30340;NMT&#38750;&#24120;&#26377;&#25928;&#65292;&#33021;&#22815;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32531;&#35299;&#39046;&#22495;&#20998;&#27495;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.01382</link><description>&lt;p&gt;
&#21033;&#29992;&#36741;&#21161;&#39046;&#22495;&#24179;&#34892;&#25968;&#25454;&#22312;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;&#20013;&#23454;&#29616;&#20302;&#36164;&#28304;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Leveraging Auxiliary Domain Parallel Data in Intermediate Task Fine-tuning for Low-resource Translation. (arXiv:2306.01382v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;(ITFT)&#23545;&#20110;&#20302;&#36164;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#39046;&#22495;&#30340;NMT&#38750;&#24120;&#26377;&#25928;&#65292;&#33021;&#22815;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32531;&#35299;&#39046;&#22495;&#20998;&#27495;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#27809;&#26377;&#36275;&#22815;&#30340;&#24179;&#34892;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#22522;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#35821;&#31181;Seq-to-Seq&#27169;&#22411;&#30340;NMT&#31995;&#32479;&#24212;&#29992;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#20013;&#32570;&#22833;/&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#12290;&#24403;&#25968;&#25454;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#26102;&#65292;&#38382;&#39064;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20013;&#38388;&#20219;&#21153;&#24494;&#35843;(ITFT)&#23545;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;NMT&#38750;&#24120;&#26377;&#30410;&#65292;&#23588;&#20854;&#26159;&#24403;&#30446;&#26631;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#38480;/&#19981;&#21487;&#29992;&#65292;&#32780;&#32771;&#34385;&#30340;&#35821;&#35328;&#22312;PMSS&#27169;&#22411;&#20013;&#32570;&#22833;&#25110;&#20195;&#34920;&#24615;&#19981;&#36275;&#26102;&#12290;&#25105;&#20204;&#20351;&#29992;&#39046;&#22495;&#20998;&#27495;&#27979;&#35797;&#37327;&#21270;&#20102;&#39046;&#22495;&#29305;&#23450;&#32467;&#26524;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;ITFT&#33021;&#22815;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32531;&#35299;&#39046;&#22495;&#20998;&#27495;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
NMT systems trained on Pre-trained Multilingual Sequence-Sequence (PMSS) models flounder when sufficient amounts of parallel data is not available for fine-tuning. This specifically holds for languages missing/under-represented in these models. The problem gets aggravated when the data comes from different domains. In this paper, we show that intermediate-task fine-tuning (ITFT) of PMSS models is extremely beneficial for domain-specific NMT, especially when target domain data is limited/unavailable and the considered languages are missing or under-represented in the PMSS model. We quantify the domain-specific results variations using a domain-divergence test, and show that ITFT can mitigate the impact of domain divergence to some extent.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20351;&#29992;GPT-4&#35299;&#20915;&#26356;&#22797;&#26434;&#21644;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MathChat&#30340;&#23545;&#35805;&#24335;&#38382;&#39064;&#27714;&#35299;&#26694;&#26550;&#65292;&#24182;&#22312;&#22256;&#38590;&#39640;&#20013;&#31454;&#36187;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.01337</link><description>&lt;p&gt;
&#22522;&#20110;GPT-4&#30340;&#22797;&#26434;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Challenging Math Problem Solving with GPT-4. (arXiv:2306.01337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20351;&#29992;GPT-4&#35299;&#20915;&#26356;&#22797;&#26434;&#21644;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MathChat&#30340;&#23545;&#35805;&#24335;&#38382;&#39064;&#27714;&#35299;&#26694;&#26550;&#65292;&#24182;&#22312;&#22256;&#38590;&#39640;&#20013;&#31454;&#36187;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26159;&#19968;&#39033;&#26377;&#36259;&#30340;&#30740;&#31350;&#65292;&#32771;&#34385;&#21040;&#22312;&#21508;&#31181;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#25968;&#23398;&#38382;&#39064;&#30340;&#20016;&#23500;&#24615;&#12290;&#34429;&#28982;&#20043;&#21069;&#26377;&#20960;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#20351;&#29992;LLM&#35299;&#20915;&#21021;&#31561;&#25968;&#23398;&#38382;&#39064;&#65292;&#20294;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;GPT-4&#35299;&#20915;&#26356;&#22797;&#26434;&#21644;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#30340;&#21069;&#27839;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-4&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#20123;&#26159;&#20174;&#29616;&#26377;&#24037;&#20316;&#20013;&#25913;&#32534;&#32780;&#26469;&#30340;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;MathChat&#65292;&#36825;&#26159;&#26412;&#30740;&#31350;&#26032;&#25552;&#20986;&#30340;&#19968;&#31181;&#23545;&#35805;&#24335;&#38382;&#39064;&#27714;&#35299;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;MATH&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#39640;&#20013;&#31454;&#36187;&#38382;&#39064;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#23545;&#35805;&#24335;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Employing Large Language Models (LLMs) to address mathematical problems is an intriguing research endeavor, considering the abundance of math problems expressed in natural language across numerous science and engineering fields. While several prior works have investigated solving elementary mathematics using LLMs, this work explores the frontier of using GPT-4 for solving more complex and challenging math problems. We evaluate various ways of using GPT-4. Some of them are adapted from existing work, and one is \MathChat, a conversational problem-solving framework newly proposed in this work. We perform the evaluation on difficult high school competition problems from the MATH dataset, which shows the advantage of the proposed conversational approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38416;&#36848;&#20102;UPC&#26426;&#22120;&#32763;&#35793;&#32452;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#21644;&#26368;&#20248;&#20256;&#36755;&#25216;&#26415;&#20197;&#21450;&#21512;&#25104;&#25968;&#25454;&#22312;IWSLT23&#31163;&#32447;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26368;&#20339;&#21333;&#27169;&#22411;&#22312;IWSLT.ACLdev2023&#19978;&#33719;&#24471;33.4&#20998;&#12290;</title><link>http://arxiv.org/abs/2306.01327</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#35821;&#38899;&#32763;&#35793;&#65306;UPC&#22312;IWSLT23&#30340;&#21442;&#36187;
&lt;/p&gt;
&lt;p&gt;
Speech Translation with Foundation Models and Optimal Transport: UPC at IWSLT23. (arXiv:2306.01327v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38416;&#36848;&#20102;UPC&#26426;&#22120;&#32763;&#35793;&#32452;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#21644;&#26368;&#20248;&#20256;&#36755;&#25216;&#26415;&#20197;&#21450;&#21512;&#25104;&#25968;&#25454;&#22312;IWSLT23&#31163;&#32447;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#26368;&#20339;&#21333;&#27169;&#22411;&#22312;IWSLT.ACLdev2023&#19978;&#33719;&#24471;33.4&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;UPC&#26426;&#22120;&#32763;&#35793;&#32452;&#21442;&#21152;IWSLT 2023&#31163;&#32447;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#20998;&#21035;&#22788;&#29702;&#35821;&#38899;&#65288;wav2vec 2.0&#65289;&#21644;&#25991;&#26412;&#65288;mBART50&#65289;&#65292;&#36890;&#36807;CTC&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#8220;&#32852;&#20307;&#8221;&#39044;&#35757;&#32451;&#27493;&#39588;&#65292;&#23558;&#35821;&#38899;&#34920;&#31034;&#36866;&#24212;&#21040;&#25991;&#26412;&#27169;&#22411;&#30340;&#31354;&#38388;&#65292;&#20197;&#23454;&#29616;&#26368;&#22823;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#22312;&#27492;&#39044;&#35757;&#32451;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20132;&#21449;&#29109;&#21644;&#30693;&#35782;&#33976;&#39311;&#22312;ST&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;&#24494;&#35843;&#31995;&#32479;&#12290;&#38500;&#20102;&#24050;&#26377;&#30340;ST&#35821;&#26009;&#24211;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;SegAugment&#21019;&#24314;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;IWSLT&#27979;&#35797;&#38598;&#30340;&#33258;&#23450;&#20041;&#20998;&#21106;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#21333;&#27169;&#22411;&#22312;MuST-C tst-COMMON&#19978;&#33719;&#24471;31.2 BLEU&#20998;&#65292;IWLST.tst2020&#19978;&#33719;&#24471;29.8&#20998;&#65292;&#22312;&#26032;&#21457;&#24067;&#30340;IWSLT.ACLdev2023&#19978;&#33719;&#24471;33.4&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the submission of the UPC Machine Translation group to the IWSLT 2023 Offline Speech Translation task. Our Speech Translation systems utilize foundation models for speech (wav2vec 2.0) and text (mBART50). We incorporate a Siamese pretraining step of the speech and text encoders with CTC and Optimal Transport, to adapt the speech representations to the space of the text model, thus maximizing transfer learning from MT. After this pretraining, we fine-tune our system end-to-end on ST, with Cross Entropy and Knowledge Distillation. Apart from the available ST corpora, we create synthetic data with SegAugment to better adapt our models to the custom segmentations of the IWSLT test sets. Our best single model obtains 31.2 BLEU points on MuST-C tst-COMMON, 29.8 points on IWLST.tst2020 and 33.4 points on the newly released IWSLT.ACLdev2023.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#26032;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#30001;&#22810;&#20010;&#26412;&#22320;&#27880;&#37322;&#32773;&#23436;&#25104;&#38598;&#20307;&#27880;&#37322;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24314;&#31435;&#20102;&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01325</link><description>&lt;p&gt;
LyricSIM&#65306;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#35199;&#29677;&#29273;&#27468;&#35789;&#30456;&#20284;&#24615;&#30340;&#26032;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
LyricSIM: A novel Dataset and Benchmark for Similarity Detection in Spanish Song LyricS. (arXiv:2306.01325v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#26032;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#30001;&#22810;&#20010;&#26412;&#22320;&#27880;&#37322;&#32773;&#23436;&#25104;&#38598;&#20307;&#27880;&#37322;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24314;&#31435;&#20102;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#26088;&#22312;&#38024;&#23545;&#27468;&#35789;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26368;&#21021;&#30001;2775&#23545;&#35199;&#29677;&#29273;&#27468;&#26354;&#32452;&#25104;&#65292;&#30001;63&#20010;&#26412;&#22320;&#27880;&#37322;&#32773;&#36827;&#34892;&#38598;&#20307;&#27880;&#37322;&#23454;&#39564;&#12290;&#32463;&#36807;&#25910;&#38598;&#21644;&#31934;&#32454;&#21270;&#22788;&#29702;&#25968;&#25454;&#20197;&#30830;&#20445;&#39640;&#24230;&#30340;&#20849;&#35782;&#21644;&#25968;&#25454;&#23436;&#25972;&#24615;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;676&#20010;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#23545;&#65292;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#20934;&#32467;&#26524;&#65292;&#24076;&#26395;&#36825;&#20123;&#32467;&#26524;&#23545;&#35813;&#39046;&#22495;&#26410;&#26469;&#30340;&#23398;&#26415;&#21644;&#24037;&#19994;&#24212;&#29992;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a new dataset and benchmark tailored to the task of semantic similarity in song lyrics. Our dataset, originally consisting of 2775 pairs of Spanish songs, was annotated in a collective annotation experiment by 63 native annotators. After collecting and refining the data to ensure a high degree of consensus and data integrity, we obtained 676 high-quality annotated pairs that were used to evaluate the performance of various state-of-the-art monolingual and multilingual language models. Consequently, we established baseline results that we hope will be useful to the community in all future academic and industrial applications conducted in this context.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#25991;&#26412;&#39118;&#26684;&#36716;&#21270;&#30340;Back-Translation&#25216;&#26415;&#65288;TST BT&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#39118;&#26684;&#36716;&#21270;&#27169;&#22411;&#25913;&#21892;BT&#25968;&#25454;&#30340;&#28304;&#35821;&#35328;&#37096;&#20998;&#65292;&#26088;&#22312;&#25552;&#39640;&#33258;&#28982;&#36755;&#20837;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#23545;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;TST BT&#26174;&#33879;&#25552;&#39640;&#20102;&#32763;&#35793;&#24615;&#33021;&#65292;&#36824;&#21487;&#20197;&#29992;&#20110;&#39046;&#22495;&#36866;&#24212;&#65292;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2306.01318</link><description>&lt;p&gt;
&#25991;&#26412;&#39118;&#26684;&#36716;&#21270;&#30340;Back-Translation&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Text Style Transfer Back-Translation. (arXiv:2306.01318v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01318
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#25991;&#26412;&#39118;&#26684;&#36716;&#21270;&#30340;Back-Translation&#25216;&#26415;&#65288;TST BT&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#39118;&#26684;&#36716;&#21270;&#27169;&#22411;&#25913;&#21892;BT&#25968;&#25454;&#30340;&#28304;&#35821;&#35328;&#37096;&#20998;&#65292;&#26088;&#22312;&#25552;&#39640;&#33258;&#28982;&#36755;&#20837;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#23545;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;TST BT&#26174;&#33879;&#25552;&#39640;&#20102;&#32763;&#35793;&#24615;&#33021;&#65292;&#36824;&#21487;&#20197;&#29992;&#20110;&#39046;&#22495;&#36866;&#24212;&#65292;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Back Translation (BT)&#25216;&#26415;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#65292;&#22240;&#20026;&#23427;&#34987;&#35777;&#26126;&#33021;&#22815;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;BT&#25968;&#25454;&#30340;&#28304;&#35821;&#35328;&#37096;&#20998;&#26159;&#26426;&#22120;&#32763;&#35793;&#30340;&#65292;&#25152;&#20197;BT&#20027;&#35201;&#25913;&#21892;&#20849;&#20139;&#30456;&#20284;&#39118;&#26684;&#36755;&#20837;&#65288;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#26159;&#31867;&#20284;&#32763;&#35793;&#30340;&#36755;&#20837;&#65289;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#23545;&#20110;&#33258;&#28982;&#36755;&#20837;&#65292;BT&#21482;&#33021;&#24102;&#26469;&#36731;&#24494;&#30340;&#25913;&#36827;&#65292;&#26377;&#26102;&#29978;&#33267;&#20250;&#24102;&#26469;&#19981;&#21033;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#39118;&#26684;&#36716;&#21270;&#30340;Back-Translation&#25216;&#26415;&#65288;TST BT&#65289;&#65292;&#23427;&#20351;&#29992;&#39118;&#26684;&#36716;&#25442;&#27169;&#22411;&#26469;&#20462;&#25913;BT&#25968;&#25454;&#30340;&#28304;&#35821;&#35328;&#37096;&#20998;&#12290;&#36890;&#36807;&#20351;&#28304;&#35821;&#35328;&#37096;&#20998;&#30340;&#25991;&#26412;&#39118;&#26684;&#26356;&#33258;&#28982;&#65292;&#25105;&#20204;&#26088;&#22312;&#25913;&#21892;&#33258;&#28982;&#36755;&#20837;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#23545;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21253;&#25324;&#39640;&#36164;&#28304;&#21644;&#20302;&#36164;&#28304;&#30340;&#35821;&#35328;&#23545;&#65292;&#32467;&#26524;&#34920;&#26126;TST BT&#26174;&#33879;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;TST BT&#36824;&#35777;&#26126;&#22312;&#39046;&#22495;&#36866;&#24212;&#26041;&#38754;&#20063;&#38750;&#24120;&#26377;&#25928;&#65292;&#22240;&#27492;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Back Translation (BT) is widely used in the field of machine translation, as it has been proved effective for enhancing translation quality. However, BT mainly improves the translation of inputs that share a similar style (to be more specific, translation-like inputs), since the source side of BT data is machine-translated. For natural inputs, BT brings only slight improvements and sometimes even adverse effects. To address this issue, we propose Text Style Transfer Back Translation (TST BT), which uses a style transfer model to modify the source side of BT data. By making the style of source-side text more natural, we aim to improve the translation of natural inputs. Our experiments on various language pairs, including both high-resource and low-resource ones, demonstrate that TST BT significantly improves translation performance against popular BT benchmarks. In addition, TST BT is proved to be effective in domain adaptation so this strategy can be regarded as a general data augmenta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#27861;&#24863;&#30693;&#30340;&#28151;&#21512;&#25552;&#31034;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#34701;&#21512;&#25163;&#24037;&#25552;&#31034;&#21644;&#21487;&#23398;&#20064;&#25552;&#31034;&#65292;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20248;&#21270;&#25552;&#31034;&#32534;&#30721;&#22120;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#26512;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01312</link><description>&lt;p&gt;
&#35821;&#27861;&#24863;&#30693;&#30340;&#28151;&#21512;&#25552;&#31034;&#27169;&#22411;&#29992;&#20110;&#23567;&#26679;&#26412;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Syntax-aware Hybrid prompt model for Few-shot multi-modal sentiment analysis. (arXiv:2306.01312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#27861;&#24863;&#30693;&#30340;&#28151;&#21512;&#25552;&#31034;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#34701;&#21512;&#25163;&#24037;&#25552;&#31034;&#21644;&#21487;&#23398;&#20064;&#25552;&#31034;&#65292;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20248;&#21270;&#25552;&#31034;&#32534;&#30721;&#22120;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20998;&#26512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26159;&#24403;&#21069;&#19968;&#20010;&#28909;&#38376;&#35805;&#39064;&#65292;&#20027;&#35201;&#38024;&#23545;&#21477;&#23376;&#21644;&#26041;&#38754;&#32423;&#21035;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20960;&#20046;&#37117;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#36825;&#20250;&#24102;&#26469;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#28040;&#32791;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#36328;&#27169;&#24577;&#30340;&#23569;&#26679;&#26412;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#26159;&#24456;&#23454;&#29992;&#30340;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#25191;&#34892;&#22312;&#25991;&#26412;&#27169;&#24335;&#65292;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20027;&#35201;&#26377;&#20004;&#31181;&#31867;&#22411;&#65306;&#25163;&#24037;&#25552;&#31034;&#21644;&#21487;&#23398;&#20064;&#25552;&#31034;&#12290;&#22312;&#23567;&#26679;&#26412;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#29616;&#26377;&#26041;&#27861;&#24050;&#20998;&#21035;&#20351;&#29992;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#28151;&#21512;&#27169;&#24335;&#65292;&#21487;&#20197;&#32467;&#21512;&#19968;&#20010;&#25110;&#22810;&#20010;&#22266;&#23450;&#30340;&#25163;&#24037;&#25552;&#31034;&#21644;&#21487;&#23398;&#20064;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#20248;&#21270;&#25552;&#31034;&#32534;&#30721;&#22120;&#12290;&#22312;&#21477;&#23376;&#32423;&#21644;&#26041;&#38754;&#32423;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sentiment Analysis (MSA) has been a popular topic in natural language processing nowadays, at both sentence and aspect level. However, the existing approaches almost require large-size labeled datasets, which bring about large consumption of time and resources. Therefore, it is practical to explore the method for few-shot sentiment analysis in cross-modalities. Previous works generally execute on textual modality, using the prompt-based methods, mainly two types: hand-crafted prompts and learnable prompts. The existing approach in few-shot multi-modality sentiment analysis task has utilized both methods, separately. We further design a hybrid pattern that can combine one or more fixed hand-crafted prompts and learnable prompts and utilize the attention mechanisms to optimize the prompt encoder. The experiments on both sentence-level and aspect-level datasets prove that we get a significant outperformance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#35270;&#35273;-&#35821;&#35328;&#39046;&#22495;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22312;NLP&#20219;&#21153;&#19978;&#20803;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25104;&#21151;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#36716;&#31227;&#21040;VL&#20219;&#21153;&#19978;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.01311</link><description>&lt;p&gt;
MetaVL&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#21521;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#36801;&#31227;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models. (arXiv:2306.01311v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#35270;&#35273;-&#35821;&#35328;&#39046;&#22495;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#22312;NLP&#20219;&#21153;&#19978;&#20803;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25104;&#21151;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#36716;&#31227;&#21040;VL&#20219;&#21153;&#19978;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#36890;&#36807;&#22312;&#23569;&#37327;&#31034;&#20363;&#19978;&#36827;&#34892;&#26465;&#20214;&#35757;&#32451;&#65288;&#21363;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#26469;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#39046;&#22495;&#20013;&#65292;&#22823;&#22810;&#25968;&#22823;&#35268;&#27169;&#39044;&#20808;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#32570;&#20047;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26377;&#36259;&#30340;&#20551;&#35774;&#65306;&#25105;&#20204;&#33021;&#21542;&#23558;&#35821;&#35328;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#36716;&#31227;&#21040;VL&#39046;&#22495;&#65311;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;NLP&#20219;&#21153;&#19978;&#20803;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;&#22914;MetaICL&#65289;&#65307;&#28982;&#21518;&#36890;&#36807;&#38468;&#21152;&#35270;&#35273;&#32534;&#30721;&#22120;&#23558;&#27492;&#27169;&#22411;&#36716;&#31227;&#21040;VL&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30340;&#30830;&#21487;&#20197;&#36328;&#27169;&#24577;&#22320;&#36716;&#31227;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65306;&#25105;&#20204;&#30340;&#27169;&#22411;&#26174;&#30528;&#25552;&#39640;&#20102;VL&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#29978;&#33267;&#21487;&#20197;&#26174;&#33879;&#24357;&#34917;&#27169;&#22411;&#22823;&#23567;&#30340;&#19981;&#36275;&#12290;&#22312;VQA&#12289;OK-VQA&#21644;GQA&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale language models have shown the ability to adapt to a new task via conditioning on a few demonstrations (i.e., in-context learning). However, in the vision-language domain, most large-scale pre-trained vision-language (VL) models do not possess the ability to conduct in-context learning. How can we enable in-context learning for VL models? In this paper, we study an interesting hypothesis: can we transfer the in-context learning ability from the language domain to VL domain? Specifically, we first meta-trains a language model to perform in-context learning on NLP tasks (as in MetaICL); then we transfer this model to perform VL tasks by attaching a visual encoder. Our experiments suggest that indeed in-context learning ability can be transferred cross modalities: our model considerably improves the in-context learning capability on VL tasks and can even compensate for the size of the model significantly. On VQA, OK-VQA, and GQA, our method could outperform the baseline model 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#36328;&#35821;&#35328;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;DistilXLSR&#65292;&#36890;&#36807;&#38543;&#26426;&#25171;&#20081;&#35821;&#38899;&#20013;&#30340;&#38899;&#32032;&#65292;&#21482;&#20351;&#29992;&#33521;&#35821;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#35774;&#35745;&#19968;&#31181;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#25104;&#21151;&#23558;&#21442;&#25968;&#20943;&#23569;50%&#65292;&#21516;&#26102;&#20445;&#25345;&#36328;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#35821;&#35328;/&#25945;&#24072;&#27169;&#22411;&#24182;&#26377;&#28508;&#21147;&#25552;&#39640;&#33521;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01303</link><description>&lt;p&gt;
DistilXLSR&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#36328;&#35821;&#35328;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DistilXLSR: A Light Weight Cross-Lingual Speech Representation Model. (arXiv:2306.01303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01303
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#36328;&#35821;&#35328;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;DistilXLSR&#65292;&#36890;&#36807;&#38543;&#26426;&#25171;&#20081;&#35821;&#38899;&#20013;&#30340;&#38899;&#32032;&#65292;&#21482;&#20351;&#29992;&#33521;&#35821;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#35774;&#35745;&#19968;&#31181;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#25104;&#21151;&#23558;&#21442;&#25968;&#20943;&#23569;50%&#65292;&#21516;&#26102;&#20445;&#25345;&#36328;&#35821;&#35328;&#34920;&#36798;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#35821;&#35328;/&#25945;&#24072;&#27169;&#22411;&#24182;&#26377;&#28508;&#21147;&#25552;&#39640;&#33521;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;&#22823;&#22823;&#25552;&#39640;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#65292;&#32780;&#36825;&#20123;&#24040;&#22823;&#27169;&#22411;&#30340;&#21387;&#32553;&#20063;&#24050;&#25104;&#20026;&#20854;&#24037;&#19994;&#24212;&#29992;&#30340;&#37325;&#35201;&#21069;&#25552;&#12290;&#26412;&#25991;&#25552;&#20986;DistilXLSR&#65292;&#19968;&#31181;&#32463;&#36807;&#31934;&#31616;&#30340;&#36328;&#35821;&#35328;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;&#12290;&#36890;&#36807;&#38543;&#26426;&#25171;&#20081;&#24050;&#26377;&#35821;&#38899;&#20013;&#30340;&#38899;&#32032;&#65292;&#20943;&#23569;&#35821;&#35328;&#20449;&#24687;&#24182;&#20165;&#20351;&#29992;&#33521;&#35821;&#25968;&#25454;&#26469;&#33976;&#39311;&#36328;&#35821;&#35328;&#27169;&#22411;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#36339;&#23618;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#20197;&#20805;&#20998;&#21033;&#29992;&#25945;&#24072;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26435;&#37325;&#12290;&#22312;&#20004;&#31181;&#25945;&#24072;&#27169;&#22411;&#21644;15&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#21442;&#25968;&#38477;&#20302;50%&#65292;&#21516;&#26102;&#20445;&#25345;&#36328;&#35821;&#35328;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#36866;&#29992;&#20110;&#21508;&#31181;&#35821;&#35328;/&#25945;&#24072;&#27169;&#22411;&#65292;&#24182;&#26377;&#28508;&#21147;&#25552;&#39640;&#33521;&#35821;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual self-supervised speech representation models have greatly enhanced the speech recognition performance for low-resource languages, and the compression of these huge models has also become a crucial prerequisite for their industrial application. In this paper, we propose DistilXLSR, a distilled cross-lingual speech representation model. By randomly shuffling the phonemes of existing speech, we reduce the linguistic information and distill cross-lingual models using only English data. We also design a layer-jumping initialization method to fully leverage the teacher's pre-trained weights. Experiments on 2 kinds of teacher models and 15 low-resource languages show that our method can reduce the parameters by 50% while maintaining cross-lingual representation ability. Our method is proven to be generalizable to various languages/teacher models and has the potential to improve the cross-lingual performance of the English pre-trained models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22359;&#29366;Transformer&#32534;&#30721;&#22120;&#36827;&#34892;&#35757;&#32451;&#20197;&#23454;&#29616;&#26631;&#28857;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#20351;&#29992;&#22359;&#21644;&#35805;&#35821;&#30340;CTC&#25439;&#22833;&#26469;&#33719;&#24471;&#36739;&#22909;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01296</link><description>&lt;p&gt;
&#24102;&#26631;&#28857;&#30340;&#31471;&#21040;&#31471;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#25913;&#36827;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Improved Training for End-to-End Streaming Automatic Speech Recognition Model with Punctuation. (arXiv:2306.01296v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22359;&#29366;Transformer&#32534;&#30721;&#22120;&#36827;&#34892;&#35757;&#32451;&#20197;&#23454;&#29616;&#26631;&#28857;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#20351;&#29992;&#22359;&#21644;&#35805;&#35821;&#30340;CTC&#25439;&#22833;&#26469;&#33719;&#24471;&#36739;&#22909;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;&#26631;&#28857;&#25991;&#26412;&#39044;&#27979;&#23545;&#25552;&#39640;&#21487;&#35835;&#24615;&#21644;&#24433;&#21709;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#27969;&#23186;&#20307;&#22330;&#26223;&#19979;&#65292;&#23454;&#26102;&#39044;&#27979;&#26631;&#28857;&#30340;&#33021;&#21147;&#23588;&#20026;&#37325;&#35201;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#22256;&#38590;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#22359;&#30340;Transformer&#32534;&#30721;&#22120;&#39044;&#27979;&#26631;&#28857;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#35813;&#32534;&#30721;&#22120;&#20351;&#29992;&#36830;&#32493;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#23558;&#36755;&#20837;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#36830;&#25509;&#36215;&#26469;&#35757;&#32451;&#38271;&#24207;&#21015;&#30340;&#22768;&#23398;&#27169;&#22411;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#38468;&#21152;&#22312;&#21477;&#23376;&#26411;&#23614;&#30340;&#26631;&#28857;&#31526;&#21495;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32467;&#21512;&#22359;&#21644;&#35805;&#35821;&#30340;CTC&#25439;&#22833;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#26631;&#28857;&#39044;&#27979;&#30340;&#25913;&#36827;F1&#24471;&#20998;&#21644;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#30340;&#21452;&#37325;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Punctuated text prediction is crucial for automatic speech recognition as it enhances readability and impacts downstream natural language processing tasks. In streaming scenarios, the ability to predict punctuation in real-time is particularly desirable but presents a difficult technical challenge. In this work, we propose a method for predicting punctuated text from input speech using a chunk-based Transformer encoder trained with Connectionist Temporal Classification (CTC) loss. The acoustic model trained with long sequences by concatenating the input and target sequences can learn punctuation marks attached to the end of sentences more effectively. Additionally, by combining CTC losses on the chunks and utterances, we achieved both the improved F1 score of punctuation prediction and Word Error Rate (WER).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28201;&#24230;&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;KL-&#25955;&#24230;&#24341;&#23548;&#21160;&#24577;&#35843;&#25972;&#28201;&#24230;&#65292;&#20174;&#32780;&#32531;&#35299;&#22810;&#26679;&#24615;&#21644;&#21487;&#24402;&#22240;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#23545;&#35805;&#38382;&#31572;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.01286</link><description>&lt;p&gt;
KL-Divergence&#24341;&#23548;&#19979;&#30340;&#28201;&#24230;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
KL-Divergence Guided Temperature Sampling. (arXiv:2306.01286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01286
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28201;&#24230;&#37319;&#26679;&#31639;&#27861;&#65292;&#36890;&#36807;KL-&#25955;&#24230;&#24341;&#23548;&#21160;&#24577;&#35843;&#25972;&#28201;&#24230;&#65292;&#20174;&#32780;&#32531;&#35299;&#22810;&#26679;&#24615;&#21644;&#21487;&#24402;&#22240;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#31639;&#27861;&#22312;&#23545;&#35805;&#38382;&#31572;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28201;&#24230;&#37319;&#26679;&#26159;&#19968;&#31181;&#24120;&#35268;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#22810;&#26679;&#21270;&#12290;&#38543;&#30528;&#28201;&#24230;&#30340;&#21319;&#39640;&#65292;&#39044;&#27979;&#21464;&#24471;&#26356;&#21152;&#22810;&#26679;&#21270;&#65292;&#20294;&#20063;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#8212;&#8212;&#29983;&#25104;&#30475;&#20284;&#21512;&#29702;&#20294;&#19981;&#27491;&#30830;&#30340;&#20196;&#29260;&#12290;&#32531;&#35299;&#24187;&#35273;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#25552;&#20379;&#28304;/&#22522;&#30784;&#25991;&#26723;&#65292;&#24182;&#20351;&#27169;&#22411;&#35757;&#32451;&#29983;&#25104;&#19982;&#25552;&#20379;&#30340;&#26469;&#28304;&#30456;&#20851;&#19988;&#21487;&#24402;&#22240;&#30340;&#39044;&#27979;&#12290;&#30475;&#26469;&#23384;&#22312;&#22810;&#26679;&#24615;&#21644;&#21487;&#24402;&#22240;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26494;&#24347;&#22266;&#23450;&#28201;&#24230;&#21644;&#36890;&#36807;KL-&#25955;&#24230;&#26681;&#25454;&#20854;&#19982;&#28304;&#30340;&#30456;&#20851;&#24615;&#24341;&#23548;&#21160;&#24577;&#28201;&#24230;&#30340;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#31181;&#26435;&#34913;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#30340;&#37319;&#26679;&#31639;&#27861;&#22312;&#23545;&#35805;&#38382;&#31572;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#20248;&#20110;&#24120;&#35268;&#30340;top-k&#21644;top-p&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temperature sampling is a conventional approach to diversify large language model predictions. As temperature increases, the prediction becomes diverse but also vulnerable to hallucinations -- generating tokens that are sensible but not factual. One common approach to mitigate hallucinations is to provide source/grounding documents and the model is trained to produce predictions that bind to and are attributable to the provided source. It appears that there is a trade-off between diversity and attribution. To mitigate any such trade-off, we propose to relax the constraint of having a fixed temperature over decoding steps, and a mechanism to guide the dynamic temperature according to its relevance to the source through KL-divergence. Our experiments justifies the trade-off, and shows that our sampling algorithm outperforms the conventional top-k and top-p algorithms in conversational question-answering and summarization tasks.
&lt;/p&gt;</description></item><item><title>VoteTRANS &#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#26816;&#27979;&#23545;&#25239;&#24615;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#36755;&#20837;&#25991;&#26412;&#21450;&#20854;&#36716;&#25442;&#30340;&#30828;&#26631;&#31614;&#23454;&#29616;&#26816;&#27979;&#65292;&#21487;&#26377;&#25928;&#24212;&#23545;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#12289;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20013;&#30340;&#23545;&#25239;&#24615;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.01273</link><description>&lt;p&gt;
VoteTRANS: &#36890;&#36807;&#23545;&#36716;&#25442;&#30340;&#30828;&#26631;&#31614;&#25237;&#31080;&#26469;&#26816;&#27979;&#38750;&#35757;&#32451;&#23545;&#25239;&#24615;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
VoteTRANS: Detecting Adversarial Text without Training by Voting on Hard Labels of Transformations. (arXiv:2306.01273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01273
&lt;/p&gt;
&lt;p&gt;
VoteTRANS &#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#26816;&#27979;&#23545;&#25239;&#24615;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#36755;&#20837;&#25991;&#26412;&#21450;&#20854;&#36716;&#25442;&#30340;&#30828;&#26631;&#31614;&#23454;&#29616;&#26816;&#27979;&#65292;&#21487;&#26377;&#25928;&#24212;&#23545;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#12289;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20013;&#30340;&#23545;&#25239;&#24615;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20005;&#37325;&#32570;&#38519;&#12290;&#26356;&#21361;&#38505;&#30340;&#26159;&#65292;&#36825;&#20123;&#25915;&#20987;&#20445;&#30041;&#20102;&#21407;&#22987;&#21547;&#20041;&#65292;&#36867;&#36991;&#20102;&#20154;&#31867;&#30340;&#35782;&#21035;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#38656;&#35201;&#20351;&#29992;&#21407;&#22987;/&#23545;&#25239;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#36716;&#25442;&#30340;&#30828;&#26631;&#31614;&#25237;&#31080;&#26469;&#23454;&#29616;&#26080;&#38656;&#35757;&#32451;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#21363; VoteTRANS&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;VoteTRANS &#36890;&#36807;&#27604;&#36739;&#36755;&#20837;&#25991;&#26412;&#21450;&#20854;&#36716;&#25442;&#30340;&#30828;&#26631;&#31614;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#25991;&#26412;&#12290;&#35780;&#20272;&#34920;&#26126;&#65292;VoteTRANS &#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#12289;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20013;&#30340;&#23545;&#25239;&#24615;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks reveal serious flaws in deep learning models. More dangerously, these attacks preserve the original meaning and escape human recognition. Existing methods for detecting these attacks need to be trained using original/adversarial data. In this paper, we propose detection without training by voting on hard labels from predictions of transformations, namely, VoteTRANS. Specifically, VoteTRANS detects adversarial text by comparing the hard labels of input text and its transformation. The evaluation demonstrates that VoteTRANS effectively detects adversarial text across various state-of-the-art attacks, models, and datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#30340;&#24694;&#24847;&#35328;&#35770;&#33258;&#21160;&#32763;&#35793;&#20026;&#38750;&#24694;&#24847;&#35328;&#35770;&#30340;&#26041;&#27861;&#65292;&#20197;&#35199;&#29677;&#29273;&#35821;&#20026;&#20363;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#32467;&#26524;&#65292;&#26088;&#22312;&#20026;&#20943;&#23569;&#31038;&#21306;&#20013;&#24694;&#24847;&#35328;&#35770;&#30340;&#20256;&#25773;&#36129;&#29486;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01261</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#24694;&#24847;&#35328;&#35770;&#21040;&#38750;&#24694;&#24847;&#35328;&#35770;&#30340;&#33258;&#21160;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Automatic Translation of Hate Speech to Non-hate Speech in Social Media Texts. (arXiv:2306.01261v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#20013;&#30340;&#24694;&#24847;&#35328;&#35770;&#33258;&#21160;&#32763;&#35793;&#20026;&#38750;&#24694;&#24847;&#35328;&#35770;&#30340;&#26041;&#27861;&#65292;&#20197;&#35199;&#29677;&#29273;&#35821;&#20026;&#20363;&#65292;&#24182;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#32467;&#26524;&#65292;&#26088;&#22312;&#20026;&#20943;&#23569;&#31038;&#21306;&#20013;&#24694;&#24847;&#35328;&#35770;&#30340;&#20256;&#25773;&#36129;&#29486;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24694;&#24847;&#35328;&#35770;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#23558;&#24694;&#24847;&#35328;&#35770;&#36716;&#21270;&#20026;&#38750;&#24694;&#24847;&#35328;&#35770;&#25991;&#26412;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#20445;&#30041;&#20854;&#24847;&#20041;&#12290;&#25105;&#20204;&#20197;&#35199;&#29677;&#29273;&#35821;&#25991;&#26412;&#20026;&#26696;&#20363;&#36827;&#34892;&#30740;&#31350;&#65292;&#25552;&#20379;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#32467;&#26524;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#27492;&#20219;&#21153;&#25552;&#20379;&#36215;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#25351;&#26631;&#65288;&#21253;&#25324;BLEU&#20998;&#25968;&#65289;&#23545;&#22522;&#32447;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20026;&#20943;&#23569;&#31038;&#21306;&#20013;&#24694;&#24847;&#35328;&#35770;&#30340;&#20256;&#25773;&#65292;&#36129;&#29486;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the issue of hate speech by presenting a novel task of translating hate speech into non-hate speech text while preserving its meaning. As a case study, we use Spanish texts. We provide a dataset and several baselines as a starting point for further research in the task. We evaluated our baseline results using multiple metrics, including BLEU scores. The aim of this study is to contribute to the development of more effective methods for reducing the spread of hate speech in online communities.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#29983;&#25104;&#27861;&#24459;&#26696;&#20363;&#21028;&#20915;&#30340;&#25688;&#35201;&#65292;&#24182;&#22312;&#21360;&#24230;&#30340;&#27861;&#24237;&#26696;&#20363;&#21028;&#20915;&#20013;&#36827;&#34892;&#20102;&#30456;&#20851;&#23454;&#39564;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2306.01248</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#27169;&#22411;&#21644;LLMs&#22312;&#27861;&#24459;&#26696;&#20363;&#21028;&#20915;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;&#20934;&#22791;&#24773;&#20917;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Ready are Pre-trained Abstractive Models and LLMs for Legal Case Judgement Summarization?. (arXiv:2306.01248v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25277;&#35937;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#29983;&#25104;&#27861;&#24459;&#26696;&#20363;&#21028;&#20915;&#30340;&#25688;&#35201;&#65292;&#24182;&#22312;&#21360;&#24230;&#30340;&#27861;&#24237;&#26696;&#20363;&#21028;&#20915;&#20013;&#36827;&#34892;&#20102;&#30456;&#20851;&#23454;&#39564;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25688;&#35201;&#27861;&#24459;&#26696;&#20363;&#21028;&#20915;&#19968;&#30452;&#26159;&#37319;&#29992;&#25277;&#21462;&#24335;&#25688;&#35201;&#26041;&#27861;&#23581;&#35797;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36817;&#24180;&#26469;&#65292;&#20855;&#26377;&#29983;&#25104;&#26356;&#33258;&#28982;&#21644;&#36830;&#36143;&#25688;&#35201;&#33021;&#21147;&#30340;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#22312;&#24050;&#32463;&#26377;&#20102;&#19987;&#38376;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#22914;ChatGPT&#36825;&#26679;&#30340;&#36890;&#29992;&#39046;&#22495;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#65292;&#24182;&#20855;&#26377;&#25991;&#26412;&#25688;&#35201;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#20540;&#24471;&#38382;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#24050;&#20934;&#22791;&#22909;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#26696;&#20363;&#21028;&#20915;&#30340;&#25277;&#35937;&#25688;&#35201;&#12290;&#20026;&#20102;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#39046;&#22495;&#29305;&#23450;&#30340;&#25277;&#35937;&#24615;&#25688;&#35201;&#27169;&#22411;&#21644;&#36890;&#29992;&#39046;&#22495;&#30340;LLMs&#24212;&#29992;&#20110;&#21360;&#24230;&#27861;&#24237;&#26696;&#20363;&#21028;&#20915;&#20013;&#65292;&#24182;&#26816;&#26597;&#25152;&#29983;&#25104;&#25688;&#35201;&#30340;&#36136;&#37327;&#12290;&#38500;&#20102;&#25688;&#35201;&#36136;&#37327;&#30340;&#26631;&#20934;&#24230;&#37327;&#65292;&#25105;&#20204;&#36824;&#26816;&#26597;&#20102;&#29983;&#25104;&#30340;&#25688;&#35201;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#34394;&#26500;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic summarization of legal case judgements has traditionally been attempted by using extractive summarization methods. However, in recent years, abstractive summarization models are gaining popularity since they can generate more natural and coherent summaries. Legal domain-specific pre-trained abstractive summarization models are now available. Moreover, general-domain pre-trained Large Language Models (LLMs), such as ChatGPT, are known to generate high-quality text and have the capacity for text summarization. Hence it is natural to ask if these models are ready for off-the-shelf application to automatically generate abstractive summaries for case judgements. To explore this question, we apply several state-of-the-art domain-specific abstractive summarization models and general-domain LLMs on Indian court case judgements, and check the quality of the generated summaries. In addition to standard metrics for summary quality, we check for inconsistencies and hallucinations in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;NLI4CT&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;CTR&#30340;&#25991;&#26412;&#34164;&#28085;&#21644;&#35777;&#25454;&#26816;&#32034;&#30340;&#22810;&#31890;&#24230;&#31995;&#32479;&#65292;&#20351;&#29992;&#22810;&#31890;&#24230;&#25512;&#26029;&#32593;&#32476;(MGNet)&#21644;T5&#27169;&#22411;SciFive&#65292;&#24182;&#36827;&#34892;&#27169;&#22411;&#38598;&#25104;&#21644;&#32852;&#21512;&#25512;&#26029;&#26041;&#27861;&#20197;&#22686;&#21152;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#21644;&#19968;&#33268;&#24615;&#24615;&#33021;&#12290;&#22312;NLI4CT&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#32467;&#26524;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.01245</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;7&#19978;&#30340;THiFLY&#30740;&#31350;&#65306;&#38754;&#21521;CTR&#30340;&#25991;&#26412;&#34164;&#28085;&#21644;&#35777;&#25454;&#26816;&#32034;&#30340;&#22810;&#31890;&#24230;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
THiFLY Research at SemEval-2023 Task 7: A Multi-granularity System for CTR-based Textual Entailment and Evidence Retrieval. (arXiv:2306.01245v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;NLI4CT&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;CTR&#30340;&#25991;&#26412;&#34164;&#28085;&#21644;&#35777;&#25454;&#26816;&#32034;&#30340;&#22810;&#31890;&#24230;&#31995;&#32479;&#65292;&#20351;&#29992;&#22810;&#31890;&#24230;&#25512;&#26029;&#32593;&#32476;(MGNet)&#21644;T5&#27169;&#22411;SciFive&#65292;&#24182;&#36827;&#34892;&#27169;&#22411;&#38598;&#25104;&#21644;&#32852;&#21512;&#25512;&#26029;&#26041;&#27861;&#20197;&#22686;&#21152;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#21644;&#19968;&#33268;&#24615;&#24615;&#33021;&#12290;&#22312;NLI4CT&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#32467;&#26524;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLI4CT&#20219;&#21153;&#26088;&#22312;&#22522;&#20110;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578; (CTR) &#25512;&#26029;&#20551;&#35774;&#65292;&#24182;&#26816;&#32034;&#25903;&#25345;&#35777;&#26126;&#12290;&#35813;&#20219;&#21153;&#20855;&#26377;&#26174;&#33879;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312; NLI4CT &#20219;&#21153;&#20013;&#39564;&#35777;&#20551;&#35774;&#38656;&#35201;&#25972;&#21512;&#26469;&#33258;&#19968;&#20010;&#25110;&#20004;&#20010; CTR &#30340;&#22810;&#20010;&#35777;&#25454;&#65292;&#24182;&#24212;&#29992;&#19981;&#21516;&#23618;&#27425;&#30340;&#25512;&#29702;&#65292;&#21253;&#25324;&#25991;&#26412;&#21644;&#25968;&#23383;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;CTR&#30340;&#25991;&#26412;&#34164;&#28085;&#21644;&#35777;&#25454;&#26816;&#32034;&#30340;&#22810;&#31890;&#24230;&#31995;&#32479;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#31890;&#24230;&#25512;&#26029;&#32593;&#32476; (MGNet)&#65292;&#21033;&#29992;&#21477;&#23376;&#32423;&#21644;&#20196;&#29260;&#32423;&#32534;&#30721;&#26469;&#22788;&#29702;&#25991;&#26412;&#34164;&#28085;&#21644;&#35777;&#25454;&#26816;&#32034;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;T5&#30340;&#27169;&#22411;SciFive&#65292;&#23427;&#22312;&#21307;&#23398;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#20197;&#22686;&#24378;&#31995;&#32479;&#30340;&#25968;&#20540;&#25512;&#26029;&#33021;&#21147;&#12290;&#27169;&#22411;&#38598;&#25104;&#21644;&#32852;&#21512;&#25512;&#26029;&#26041;&#27861;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#21644;&#19968;&#33268;&#24615;&#24615;&#33021;&#12290;&#23545;NLI4CT&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The NLI4CT task aims to entail hypotheses based on Clinical Trial Reports (CTRs) and retrieve the corresponding evidence supporting the justification. This task poses a significant challenge, as verifying hypotheses in the NLI4CT task requires the integration of multiple pieces of evidence from one or two CTR(s) and the application of diverse levels of reasoning, including textual and numerical. To address these problems, we present a multi-granularity system for CTR-based textual entailment and evidence retrieval in this paper. Specifically, we construct a Multi-granularity Inference Network (MGNet) that exploits sentence-level and token-level encoding to handle both textual entailment and evidence retrieval tasks. Moreover, we enhance the numerical inference capability of the system by leveraging a T5-based model, SciFive, which is pre-trained on the medical corpus. Model ensembling and a joint inference method are further utilized in the system to increase the stability and consiste
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#30784;&#26694;&#26550;-"&#36127;&#36131;&#20219;&#30340;&#20219;&#21153;&#33258;&#21160;&#21270;&#65288;ResponsibleTA&#65289;"&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36127;&#36131;&#20219;&#22320;&#20316;&#20026;&#20219;&#21153;&#21327;&#21516;&#24037;&#20855;&#12290;&#35813;&#26694;&#26550;&#22686;&#24378;&#20102;LLM&#30340;&#19977;&#31181;&#33021;&#21147;&#65306;&#39044;&#27979;&#20219;&#21153;&#21487;&#34892;&#24615;&#12289;&#39564;&#35777;&#20219;&#21153;&#23436;&#25972;&#24615;&#20197;&#21450;&#22686;&#24378;&#20219;&#21153;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01242</link><description>&lt;p&gt;
&#36127;&#36131;&#20219;&#30340;&#20219;&#21153;&#33258;&#21160;&#21270;: &#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#36127;&#36131;&#20219;&#30340;&#20219;&#21153;&#33258;&#21160;&#21270;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators. (arXiv:2306.01242v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#30784;&#26694;&#26550;-"&#36127;&#36131;&#20219;&#30340;&#20219;&#21153;&#33258;&#21160;&#21270;&#65288;ResponsibleTA&#65289;"&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36127;&#36131;&#20219;&#22320;&#20316;&#20026;&#20219;&#21153;&#21327;&#21516;&#24037;&#20855;&#12290;&#35813;&#26694;&#26550;&#22686;&#24378;&#20102;LLM&#30340;&#19977;&#31181;&#33021;&#21147;&#65306;&#39044;&#27979;&#20219;&#21153;&#21487;&#34892;&#24615;&#12289;&#39564;&#35777;&#20219;&#21153;&#23436;&#25972;&#24615;&#20197;&#21450;&#22686;&#24378;&#20219;&#21153;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#20026;&#20154;&#24037;&#26234;&#33021;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#23427;&#20204;&#23637;&#29616;&#20102;&#22312;&#29992;&#25143;&#25351;&#20196;&#19979;&#33258;&#21160;&#23436;&#25104;&#20219;&#21153;&#30340;&#33391;&#22909;&#21069;&#26223;&#65292;&#21487;&#20197;&#20316;&#20026;&#31867;&#20284;&#22823;&#33041;&#30340;&#21327;&#35843;&#32773;&#12290;&#38543;&#30528;&#25105;&#20204;&#23558;&#36234;&#26469;&#36234;&#22810;&#30340;&#20219;&#21153;&#20132;&#32473;&#26426;&#22120;&#33258;&#21160;&#23436;&#25104;&#65292;&#30456;&#20851;&#30340;&#39118;&#38505;&#20063;&#36880;&#28176;&#26174;&#29616;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#20986;&#29616;&#20102;: &#24403;&#26426;&#22120;&#20687;&#20154;&#31867;&#39550;&#39542;&#21327;&#21516;&#19968;&#26679;&#24110;&#21161;&#20154;&#20204;&#33258;&#21160;&#23436;&#25104;&#20219;&#21153;&#26102;&#65292;&#25105;&#20204;&#22914;&#20309;&#30830;&#20445;&#26426;&#22120;&#30340;&#36131;&#20219;&#34892;&#20026;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#21487;&#34892;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#35282;&#24230;&#65292;&#28145;&#20837;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#36127;&#36131;&#20219;&#30340;&#20219;&#21153;&#33258;&#21160;&#21270;&#8221;&#65288;ResponsibleTA&#65289;&#20316;&#20026;&#19968;&#20010;&#22522;&#30784;&#26694;&#26550;&#65292;&#20197;&#20419;&#36827;LLM&#21327;&#35843;&#32773;&#21644;&#25191;&#34892;&#32773;&#20043;&#38388;&#30340;&#36127;&#36131;&#20219;&#21327;&#20316;&#65292;&#23454;&#29616;&#20219;&#21153;&#33258;&#21160;&#21270;&#12290;&#35813;&#26694;&#26550;&#25317;&#26377;&#19977;&#31181;&#22686;&#24378;&#33021;&#21147;: 1&#65289;&#39044;&#27979;&#25191;&#34892;&#32773;&#21629;&#20196;&#30340;&#21487;&#34892;&#24615;&#65307;2&#65289;&#39564;&#35777;&#25191;&#34892;&#32773;&#30340;&#23436;&#25972;&#24615;&#65307;3&#65289;&#22686;&#24378;&#23433;&#20840;&#24615;&#65288;&#20363;&#22914;&#65292;&#20445;&#25252;&#38544;&#31169;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of Large Language Models (LLMs) signifies an impressive stride towards artificial general intelligence. They have shown a promising prospect in automatically completing tasks upon user instructions, functioning as brain-like coordinators. The associated risks will be revealed as we delegate an increasing number of tasks to machines for automated completion. A big question emerges: how can we make machines behave responsibly when helping humans automate tasks as personal copilots? In this paper, we explore this question in depth from the perspectives of feasibility, completeness and security. In specific, we present Responsible Task Automation (ResponsibleTA) as a fundamental framework to facilitate responsible collaboration between LLM-based coordinators and executors for task automation with three empowered capabilities: 1) predicting the feasibility of the commands for executors; 2) verifying the completeness of executors; 3) enhancing the security (e.g., the prote
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#36866;&#24212;&#22312;&#32447;&#26381;&#21153;&#25552;&#20379;&#21830;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#38169;&#35823;&#32416;&#27491;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197; OpenAI Whisper ASR &#20026;&#20363;&#65292;&#20351;&#29992; LibriSpeech &#20316;&#20026;&#20027;&#35201;&#36866;&#24212;&#30446;&#26631;&#39046;&#22495;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#36866;&#29992;&#20110;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#30340; ASR &#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.01208</link><description>&lt;p&gt;
&#36866;&#24212;&#19968;&#20010;&#19981;&#21487;&#36866;&#24212;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Adapting an Unadaptable ASR System. (arXiv:2306.01208v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#36866;&#24212;&#22312;&#32447;&#26381;&#21153;&#25552;&#20379;&#21830;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#38169;&#35823;&#32416;&#27491;&#30340;&#26041;&#27861;&#65292;&#24182;&#20197; OpenAI Whisper ASR &#20026;&#20363;&#65292;&#20351;&#29992; LibriSpeech &#20316;&#20026;&#20027;&#35201;&#36866;&#24212;&#30446;&#26631;&#39046;&#22495;&#36827;&#34892;&#23454;&#39564;&#12290;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#20855;&#26377;&#19968;&#23450;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21487;&#36866;&#29992;&#20110;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#30340; ASR &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#25968;&#25454;&#38656;&#27714;&#30340;&#22686;&#38271;&#65292;&#31995;&#32479;&#36890;&#24120;&#21482;&#33021;&#36890;&#36807;&#22312;&#32447;&#26381;&#21153;&#25552;&#20379;&#21830;&#30340;API&#33719;&#24471;&#35775;&#38382;&#26435;&#38480;&#65292;&#32780;&#26080;&#27861;&#30452;&#25509;&#35775;&#38382;&#27169;&#22411;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23558;&#31995;&#32479;&#36866;&#24212;&#21040;&#29305;&#23450;&#30446;&#26631;&#39046;&#22495;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#38169;&#35823;&#32416;&#27491;&#30340;&#26041;&#27861;&#65292;&#35780;&#20272; OpenAI Whisper ASR &#20316;&#20026;&#22823;&#35268;&#27169; ASR &#31995;&#32479;&#30340;&#36866;&#24212;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#65292;&#32780;&#26159;&#21487;&#20197;&#20174;&#36890;&#24120;&#36890;&#36807; ASR API &#25552;&#20379;&#30340; 1-best &#25110; N-best &#36755;&#20986;&#36827;&#34892;&#35757;&#32451;&#12290;LibriSpeech &#34987;&#29992;&#20316;&#36866;&#24212;&#30340;&#20027;&#35201;&#30446;&#26631;&#39046;&#22495;&#12290;&#28982;&#21518;&#35780;&#20272;&#20102;&#31995;&#32479;&#22312;&#20004;&#20010;&#19981;&#21516;&#26041;&#38754;&#30340;&#27867;&#21270;&#33021;&#21147;&#65306;&#39318;&#20808;&#65292;&#32416;&#27491;&#27169;&#22411;&#30340;&#24418;&#24335;&#26159;&#21542;&#21487;&#20197;&#31227;&#26893;&#21040;&#20854;&#20182;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#65307;&#20854;&#27425;&#65292;&#23427;&#26159;&#21542;&#21487;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#30340; ASR &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
As speech recognition model sizes and training data requirements grow, it is increasingly common for systems to only be available via APIs from online service providers rather than having direct access to models themselves. In this scenario it is challenging to adapt systems to a specific target domain. To address this problem we consider the recently released OpenAI Whisper ASR as an example of a large-scale ASR system to assess adaptation methods. An error correction based approach is adopted, as this does not require access to the model, but can be trained from either 1-best or N-best outputs that are normally available via the ASR API. LibriSpeech is used as the primary target domain for adaptation. The generalization ability of the system in two distinct dimensions are then evaluated. First, whether the form of correction model is portable to other speech recognition domains, and secondly whether it can be used for ASR models having a different architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#20998;&#26512;&#27169;&#22411;&#22312;&#22495;&#20869;&#21644;&#22495;&#22806;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#65292;&#26368;&#32456;&#25214;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#35782;&#21035;OOD / OODist&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.01206</link><description>&lt;p&gt;
&#22312;&#22495;&#20869;&#21644;&#22495;&#22806;&#26679;&#26412;&#20043;&#38388;&#20272;&#35745;&#35821;&#20041;&#30456;&#20284;&#24230;
&lt;/p&gt;
&lt;p&gt;
Estimating Semantic Similarity between In-Domain and Out-of-Domain Samples. (arXiv:2306.01206v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#20998;&#26512;&#27169;&#22411;&#22312;&#22495;&#20869;&#21644;&#22495;&#22806;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#65292;&#26368;&#32456;&#25214;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#35782;&#21035;OOD / OODist&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#23558;&#26469;&#33258;&#25968;&#25454;&#38598;&#25110;&#28304;&#19982;&#35757;&#32451;&#38598;&#19981;&#21516;&#20294;&#29992;&#20110;&#21516;&#19968;&#20219;&#21153;&#30340;&#22495;&#22806;&#65288;OOD&#65289;&#25110;&#22495;&#22806;&#20998;&#24067;&#65288;OODist&#65289;&#26679;&#26412;&#25551;&#36848;&#20026;&#22495;&#22806;&#65292;&#19982;&#22495;&#20869;&#65288;ID&#65289;&#26679;&#26412;&#30456;&#27604;&#65292;&#27169;&#22411;&#22312;OOD&#26679;&#26412;&#19978;&#30340;&#34920;&#29616;&#36890;&#24120;&#36739;&#24046;&#65292;&#23613;&#31649;&#36825;&#31181;&#35266;&#23519;&#32467;&#26524;&#24182;&#19981;&#19968;&#33268;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19968;&#20123;&#30740;&#31350;&#20851;&#27880;&#20110;OOD&#26816;&#27979;&#65292;&#20294;&#22823;&#22810;&#20351;&#29992;&#26377;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25972;&#21512;&#24182;&#21576;&#29616;&#20102;&#22810;&#20010;&#20851;&#20110;OOD&#21644;OODist&#30340;&#22810;&#37325;&#23450;&#20041;&#65292;&#24182;&#20197;&#21407;&#21017;&#24615;&#30340;&#26041;&#24335;&#20998;&#26512;&#20102;&#27169;&#22411;&#22312;ID&#21644;OOD / OODist&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35797;&#22270;&#35782;&#21035;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#22312;&#19981;&#20351;&#29992;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#21487;&#38752;&#22320;&#35782;&#21035;OOD / OODist&#26679;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;4&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;12&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#30417;&#30563;&#24230;&#37327;&#22312;&#35813;&#20219;&#21153;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work typically describes out-of-domain (OOD) or out-of-distribution (OODist) samples as those that originate from dataset(s) or source(s) different from the training set but for the same task. When compared to in-domain (ID) samples, the models have been known to usually perform poorer on OOD samples, although this observation is not consistent. Another thread of research has focused on OOD detection, albeit mostly using supervised approaches. In this work, we first consolidate and present a systematic analysis of multiple definitions of OOD and OODist as discussed in prior literature. Then, we analyze the performance of a model under ID and OOD/OODist settings in a principled way. Finally, we seek to identify an unsupervised method for reliably identifying OOD/OODist samples without using a trained model. The results of our extensive evaluation using 12 datasets from 4 different tasks suggest the promising potential of unsupervised metrics in this task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#30340;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#65292;&#25903;&#25345;&#20174;57&#31181;&#35821;&#35328;&#32763;&#35793;&#25104;&#33521;&#35821;&#65292;&#24182;&#20855;&#26377;&#35843;&#25972;&#36755;&#20986;&#24310;&#36831;&#30340;&#21487;&#35843;&#21442;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#26174;&#33879;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#31163;&#32447;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01201</link><description>&lt;p&gt;
&#23398;&#20064;&#20309;&#26102;&#35828;&#35805;&#65306;&#31163;&#32447;&#27169;&#22411;&#36827;&#34892;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#30340;&#24310;&#36831;&#19982;&#36136;&#37327;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Learning When to Speak: Latency and Quality Trade-offs for Simultaneous Speech-to-Speech Translation with Offline Models. (arXiv:2306.01201v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#30340;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#65292;&#25903;&#25345;&#20174;57&#31181;&#35821;&#35328;&#32763;&#35793;&#25104;&#33521;&#35821;&#65292;&#24182;&#20855;&#26377;&#35843;&#25972;&#36755;&#20986;&#24310;&#36831;&#30340;&#21487;&#35843;&#21442;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#22312;&#19981;&#22686;&#21152;&#26174;&#33879;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#36798;&#21040;&#31163;&#32447;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#38899;&#32763;&#35793;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#31163;&#32447;&#22330;&#26223;&#20013;&#65292;&#22312;&#27492;&#22330;&#26223;&#20013;&#65292;&#23436;&#25972;&#30340;&#36755;&#20837;&#35805;&#35821;&#22312;&#20219;&#20309;&#36755;&#20986;&#20043;&#21069;&#37117;&#26159;&#21487;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#20013;&#65292;&#36825;&#24182;&#19981;&#21512;&#29702;&#12290;&#22312;&#23545;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#32763;&#35793;&#24212;&#35813;&#22312;&#36755;&#20837;&#20449;&#24687;&#20986;&#29616;&#26102;&#31435;&#21363;&#21457;&#38899;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#29992;&#20110;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#30340;&#23454;&#38469;&#29992;&#20363;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#25903;&#25345;&#20174;57&#31181;&#35821;&#35328;&#32763;&#35793;&#25104;&#33521;&#35821;&#65292;&#24182;&#20855;&#26377;&#35843;&#25972;&#36755;&#20986;&#24310;&#36831;&#30340;&#21487;&#35843;&#21442;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#31574;&#30053;&#36798;&#21040;&#20102;&#31163;&#32447;&#27700;&#24179;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#22312;Greedy&#65288;wait-$k$&#65289;&#22522;&#32447;&#19978;&#26368;&#23567;&#21270;&#20102;&#24310;&#36831;&#22686;&#21152;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#35780;&#20272;&#20195;&#30721;&#21644;&#20114;&#21160;&#27979;&#35797;&#33050;&#26412;&#65292;&#20197;&#24110;&#21161;&#26410;&#26469;&#30340;SimulS2ST&#30740;&#31350;&#21644;&#24212;&#29992;&#31243;&#24207;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in speech-to-speech translation (S2ST) has focused primarily on offline settings, where the full input utterance is available before any output is given. This, however, is not reasonable in many real-world scenarios. In latency-sensitive applications, rather than waiting for the full utterance, translations should be spoken as soon as the information in the input is present. In this work, we introduce a system for simultaneous S2ST targeting real-world use cases. Our system supports translation from 57 languages to English with tunable parameters for dynamically adjusting the latency of the output -- including four policies for determining when to speak an output sequence. We show that these policies achieve offline-level accuracy with minimal increases in latency over a Greedy (wait-$k$) baseline. We open-source our evaluation code and interactive test script to aid future SimulS2ST research and application development.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25991;&#26412;&#22810;&#32500;&#24230;&#35780;&#20272;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#38024;&#23545;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#19982;&#23398;&#20064;&#35780;&#20272;&#26694;&#26550;&#30456;&#31454;&#20105;&#30340;&#22320;&#20301;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25506;&#31350;&#20102;&#19978;&#19979;&#25991;&#26679;&#20363;&#36873;&#25321;&#12289;&#25968;&#37327;&#20197;&#21450;&#35780;&#20272;&#38646;-shot&#25688;&#35201;&#31561;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.01200</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#25991;&#26412;&#25688;&#35201;&#30340;&#22810;&#32500;&#35780;&#20215;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-Dimensional Evaluation of Text Summarization with In-Context Learning. (arXiv:2306.01200v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25991;&#26412;&#22810;&#32500;&#24230;&#35780;&#20272;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#38024;&#23545;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#19982;&#23398;&#20064;&#35780;&#20272;&#26694;&#26550;&#30456;&#31454;&#20105;&#30340;&#22320;&#20301;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#25506;&#31350;&#20102;&#19978;&#19979;&#25991;&#26679;&#20363;&#36873;&#25321;&#12289;&#25968;&#37327;&#20197;&#21450;&#35780;&#20272;&#38646;-shot&#25688;&#35201;&#31561;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#30340;&#35780;&#20272;&#26159;&#22797;&#26434;&#32780;&#22810;&#32500;&#30340;&#65292;&#21487;&#20197;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#27969;&#30021;&#24615;&#12289;&#36830;&#36143;&#24615;&#12289;&#20107;&#23454;&#24615;&#25110;&#20219;&#20309;&#20854;&#20182;&#24863;&#20852;&#36259;&#30340;&#32500;&#24230;&#12290;&#22823;&#22810;&#25968;&#25191;&#34892;&#36825;&#31181;&#22810;&#32500;&#35780;&#20272;&#30340;&#26694;&#26550;&#38656;&#35201;&#22312;&#25163;&#21160;&#25110;&#21512;&#25104;&#29983;&#25104;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22810;&#32500;&#24230;&#35780;&#20272;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#26080;&#38656;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#38024;&#23545;&#25991;&#26412;&#25688;&#35201;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35780;&#20272;&#22120;&#22312;&#35832;&#22914;&#30456;&#20851;&#24615;&#21644;&#20107;&#23454;&#19968;&#33268;&#24615;&#31561;&#26041;&#38754;&#22788;&#20110;&#19982;&#24050;&#26377;&#30340;&#23398;&#20064;&#35780;&#20272;&#26694;&#26550;&#30456;&#31454;&#20105;&#30340;&#22320;&#20301;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22240;&#32032;&#22914;&#36873;&#25321;&#21644;&#25968;&#37327;&#30340;&#19978;&#19979;&#25991;&#26679;&#20363;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#37319;&#29992;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35780;&#20272;&#22120;&#26469;&#35780;&#20272;&#35832;&#22914;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#38646;-shot&#25688;&#35201;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency. We then analyze the effects of factors such as the selection and number of in-context examples on performance. Finally, we study the efficacy of in-context learning based evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#20351;&#29992; GPT-3 &#20174;&#29992;&#25143;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#38646;&#26679;&#26412;&#20272;&#35745;&#20010;&#24615;&#29305;&#36136;&#30340;&#33021;&#21147;&#12290;&#22312;&#25554;&#20837;&#20851;&#20110;&#29305;&#36136;&#30340;&#30693;&#35782;&#21518;&#65292;GPT-3 &#24615;&#33021;&#25509;&#36817;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451; SotA&#65292;&#20294;&#22312;&#34987;&#25552;&#31034;&#25552;&#20379;&#32454;&#31890;&#24230;&#20998;&#31867;&#26102;&#65292;&#20854;&#24615;&#33021;&#38477;&#33267;&#22522;&#32447;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2306.01183</link><description>&lt;p&gt;
GPT-3&#23545;&#38646;&#26679;&#26412;&#20010;&#24615;&#20272;&#35745;&#30340;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation. (arXiv:2306.01183v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#20351;&#29992; GPT-3 &#20174;&#29992;&#25143;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#38646;&#26679;&#26412;&#20272;&#35745;&#20010;&#24615;&#29305;&#36136;&#30340;&#33021;&#21147;&#12290;&#22312;&#25554;&#20837;&#20851;&#20110;&#29305;&#36136;&#30340;&#30693;&#35782;&#21518;&#65292;GPT-3 &#24615;&#33021;&#25509;&#36817;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451; SotA&#65292;&#20294;&#22312;&#34987;&#25552;&#31034;&#25552;&#20379;&#32454;&#31890;&#24230;&#20998;&#31867;&#26102;&#65292;&#20854;&#24615;&#33021;&#38477;&#33267;&#22522;&#32447;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20154;&#31867;&#27700;&#24179;&#30340; NLP &#38382;&#39064;&#19978;&#65292;&#22914;&#35780;&#20272;&#20010;&#24615;&#29305;&#24449;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#21364;&#40092;&#26377;&#20154;&#30693;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#31995;&#21015;&#31995;&#32479;&#23454;&#39564;&#65292;&#25506;&#31350;&#20102;&#20351;&#29992; GPT-3 &#20174;&#29992;&#25143;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#38646;&#26679;&#26412;&#20272;&#35745; Big 5 &#20010;&#24615;&#29305;&#36136;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#25554;&#20837;&#20851;&#20110;&#29305;&#36136;&#30340;&#30693;&#35782;&#21518;&#65292;&#38646;&#26679;&#26412; GPT-3 &#24615;&#33021;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451; SotA &#31245;&#26377;&#25509;&#36817;&#12290;&#20294;&#22312;&#34987;&#25552;&#31034;&#25552;&#20379;&#32454;&#31890;&#24230;&#20998;&#31867;&#26102;&#65292;&#20854;&#24615;&#33021;&#38477;&#33267;&#25509;&#36817;&#19968;&#20010;&#31616;&#21333;&#30340;&#26368;&#24120;&#35265;&#31867;&#65288;MFC&#65289;&#22522;&#32447;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102; GPT-3 &#34920;&#29616;&#27604;&#39044;&#35757;&#32451;&#35789;&#27719;&#27169;&#22411;&#26356;&#22909;&#12289;&#26356;&#24046;&#30340;&#22320;&#26041;&#65292;&#25581;&#31034;&#20102;&#31995;&#32479;&#35823;&#24046;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827; NLP &#20219;&#21153;&#20013;&#30340; LLM &#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Very large language models (LLMs) perform extremely well on a spectrum of NLP tasks in a zero-shot setting. However, little is known about their performance on human-level NLP problems which rely on understanding psychological concepts, such as assessing personality traits. In this work, we investigate the zero-shot ability of GPT-3 to estimate the Big 5 personality traits from users' social media posts. Through a set of systematic experiments, we find that zero-shot GPT-3 performance is somewhat close to an existing pre-trained SotA for broad classification upon injecting knowledge about the trait in the prompts. However, when prompted to provide fine-grained classification, its performance drops to close to a simple most frequent class (MFC) baseline. We further analyze where GPT-3 performs better, as well as worse, than a pretrained lexical model, illustrating systematic errors that suggest ways to improve LLMs on human-level NLP tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32467;&#21512;&#26368;&#26032;&#30340;ChatGPT&#21644;&#25277;&#21462;&#24335;&#25688;&#35201;&#27169;&#22411;C2F-FAR&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#24335;&#25277;&#21462;&#21644;&#25688;&#35201;&#27969;&#27700;&#32447;&#20197;&#35299;&#20915;&#38271;&#25991;&#26412;&#25688;&#35201;&#30340;&#25361;&#25112;&#65292;&#19982;getAbstract AG&#21512;&#20316;&#65292;&#35813;&#26041;&#27861;&#22312;&#20869;&#23481;&#35206;&#30422;&#29575;&#12289;&#36830;&#36143;&#24615;&#21644;&#21487;&#35835;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.01169</link><description>&lt;p&gt;
&#20351;&#29992;C2F-FAR&#21644;ChatGPT&#30340;&#28151;&#21512;&#24335;&#38271;&#25991;&#26412;&#25688;&#35201;&#65306;&#19968;&#39033;&#23454;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Hybrid Long Document Summarization using C2F-FAR and ChatGPT: A Practical Study. (arXiv:2306.01169v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32467;&#21512;&#26368;&#26032;&#30340;ChatGPT&#21644;&#25277;&#21462;&#24335;&#25688;&#35201;&#27169;&#22411;C2F-FAR&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#24335;&#25277;&#21462;&#21644;&#25688;&#35201;&#27969;&#27700;&#32447;&#20197;&#35299;&#20915;&#38271;&#25991;&#26412;&#25688;&#35201;&#30340;&#25361;&#25112;&#65292;&#19982;getAbstract AG&#21512;&#20316;&#65292;&#35813;&#26041;&#27861;&#22312;&#20869;&#23481;&#35206;&#30422;&#29575;&#12289;&#36830;&#36143;&#24615;&#21644;&#21487;&#35835;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25688;&#35201;&#26159;&#19968;&#20010;&#35201;&#25361;&#25112;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#30340;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#12290;&#33258;&#21160;&#25688;&#35201;&#30701;&#25991;&#26412;&#65288;&#22914;&#26032;&#38395;&#25991;&#31456;&#65289;&#21462;&#24471;&#20102;&#21487;&#22280;&#21487;&#28857;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#38271;&#25991;&#26412;&#30340;&#25688;&#35201;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#26159;&#22240;&#20026;&#25991;&#26412;&#20013;&#22797;&#26434;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#21450;&#32570;&#20047;&#21487;&#29992;&#20110;&#24320;&#21457;&#21644;&#27979;&#35797;&#27169;&#22411;&#24615;&#33021;&#30340;&#24320;&#28304;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26694;&#26550;&#12290;&#26412;&#25991;&#32467;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#31361;&#30772;ChatGPT&#21644;&#25277;&#21462;&#24335;&#25688;&#35201;&#27169;&#22411;C2F-FAR&#65288;Coarse-to-Fine Facet-Aware Ranking&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21830;&#19994;&#25991;&#31456;&#21644;&#20070;&#31821;&#31561;&#38271;&#25991;&#26412;&#30340;&#28151;&#21512;&#24335;&#25277;&#21462;&#21644;&#25688;&#35201;&#27969;&#27700;&#32447;&#12290;&#25105;&#20204;&#19982;&#19990;&#30028;&#30693;&#21517;&#20844;&#21496;getAbstract AG&#21512;&#20316;&#65292;&#21033;&#29992;&#20854;&#22312;&#19987;&#19994;&#20070;&#31821;&#25688;&#35201;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#32463;&#39564;&#12290;&#36890;&#36807;&#21508;&#31181;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#36827;&#34892;&#20102;&#23454;&#38469;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#28151;&#21512;&#26041;&#27861;&#22312;&#20869;&#23481;&#35206;&#30422;&#29575;&#12289;&#36830;&#36143;&#24615;&#21644;&#21487;&#35835;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#25277;&#21462;&#24335;&#21644;&#29983;&#25104;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text summarization is a downstream natural language processing (NLP) task that challenges the understanding and generation capabilities of language models. Considerable progress has been made in automatically summarizing short texts, such as news articles, often leading to satisfactory results. However, summarizing long documents remains a major challenge. This is due to the complex contextual information in the text and the lack of open-source benchmarking datasets and evaluation frameworks that can be used to develop and test model performance. In this work, we use ChatGPT, the latest breakthrough in the field of large language models (LLMs), together with the extractive summarization model C2F-FAR (Coarse-to-Fine Facet-Aware Ranking) to propose a hybrid extraction and summarization pipeline for long documents such as business articles and books. We work with the world-renowned company getAbstract AG and leverage their expertise and experience in professional book summarization. A pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#35782;&#21035;COVID-19&#30456;&#20851;&#30340;YouTube&#35270;&#39057;&#26085;&#24535;&#20869;&#23481;&#65292;&#29992;&#20110;&#20844;&#20849;&#21355;&#29983;&#31579;&#26597;&#12290;</title><link>http://arxiv.org/abs/2306.01164</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#34892;YouTube&#20844;&#20849;&#21355;&#29983;&#31579;&#26597;&#65306;COVID-19&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging Natural Language Processing For Public Health Screening On YouTube: A COVID-19 Case Study. (arXiv:2306.01164v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#35782;&#21035;COVID-19&#30456;&#20851;&#30340;YouTube&#35270;&#39057;&#26085;&#24535;&#20869;&#23481;&#65292;&#29992;&#20110;&#20844;&#20849;&#21355;&#29983;&#31579;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24050;&#25104;&#20026;&#21307;&#30103;&#20449;&#24687;&#30340;&#21487;&#38752;&#26469;&#28304;&#65292;&#24739;&#32773;&#21644;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#20351;&#29992;&#23427;&#20204;&#20998;&#20139;&#19982;&#36319;&#36394;&#30142;&#30149;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#21516;&#26679;&#65292;&#20840;&#29699;&#26368;&#22823;&#30340;&#35270;&#39057;&#20998;&#20139;&#24179;&#21488;YouTube&#21253;&#21547;&#20010;&#20154;&#35762;&#36848;&#30142;&#30149;&#30340;&#35270;&#39057;&#26085;&#24535;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26469;&#35782;&#21035;&#19982;Coronavirus disease of 2019 (COVID-19)&#30340;&#35786;&#26029;&#30456;&#20851;&#30340;YouTube&#35270;&#39057;&#26085;&#24535;&#20869;&#23481;&#65292;&#20197;&#36827;&#34892;&#20844;&#20849;&#21355;&#29983;&#31579;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Social media platforms have become a viable source of medical information, with patients and healthcare professionals using them to share health-related information and track diseases. Similarly, YouTube, the largest video-sharing platform in the world contains vlogs where individuals talk about their illnesses. The aim of our study was to investigate the use of Natural Language Processing (NLP) to identify the spoken content of YouTube vlogs related to the diagnosis of Coronavirus disease of 2019 (COVID-19) for public health screening. Methods: COVID-19 videos on YouTube were searched using relevant keywords. A total of 1000 videos being spoken in English were downloaded out of which 791 were classified as vlogs, 192 were non-vlogs, and 17 were deleted by the channel. The videos were converted into a textual format using Microsoft Streams. The textual data was preprocessed using basic and advanced preprocessing methods. A lexicon of 200 words was created which contained wo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;Flash&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#24555;&#36895;&#22788;&#29702;&#22823;&#24207;&#21015;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#19988;&#36895;&#24230;&#25552;&#39640;&#20102;&#22810;&#20493;&#65292;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23494;&#38598;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#22312;&#22810;&#20010;&#35774;&#32622;&#20013;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.01160</link><description>&lt;p&gt;
&#36890;&#36807;&#31232;&#30095;Flash&#27880;&#24847;&#21147;&#21152;&#36895;&#22788;&#29702;&#22823;&#24207;&#21015;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Faster Causal Attention Over Large Sequences Through Sparse Flash Attention. (arXiv:2306.01160v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;Flash&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#33021;&#22815;&#24555;&#36895;&#22788;&#29702;&#22823;&#24207;&#21015;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#19988;&#36895;&#24230;&#25552;&#39640;&#20102;&#22810;&#20493;&#65292;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23494;&#38598;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#22312;&#22810;&#20010;&#35774;&#32622;&#20013;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#22788;&#29702;&#36234;&#26469;&#36234;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#23545;&#20110;&#36825;&#20123;&#24212;&#29992;&#65292;&#24207;&#21015;&#38271;&#24230;&#20851;&#20110;&#30340;&#22240;&#26524;&#33258;&#27880;&#24847;&#21147;&#25104;&#20026;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#26159;&#21807;&#19968;&#19968;&#20010;&#19982;&#24207;&#21015;&#38271;&#24230;&#21576;&#20108;&#27425;&#20851;&#31995;&#30340;&#32452;&#20214;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#26041;&#26696;&#26469;&#20351;&#33258;&#27880;&#24847;&#21147;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#31232;&#30095;&#21270;&#65292;&#20294;&#36825;&#20123;&#26041;&#26696;&#36890;&#24120;&#21463;&#21040;&#23454;&#29616;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#26368;&#32456;&#24378;&#21152;&#19968;&#20010;&#31616;&#21333;&#19988;&#38745;&#24577;&#30340;&#32467;&#26500;&#22312;&#20851;&#27880;&#30697;&#38453;&#19978;&#12290;&#30456;&#21453;&#65292;&#23454;&#29616;&#26356;&#21160;&#24577;&#30340;&#31232;&#30095;&#27880;&#24847;&#21147;&#36890;&#24120;&#20250;&#23548;&#33268;&#36816;&#34892;&#26102;&#38388;&#26174;&#30528;&#24930;&#20110;&#20351;&#29992;Dao&#31561;&#20154;&#65288;2022&#65289;&#30340;Flash&#23454;&#29616;&#35745;&#31639;&#23436;&#25972;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;FlashAttention&#65292;&#20197;&#36866;&#24212;&#21253;&#21547;&#38190;/&#26597;&#35810;&#20002;&#24323;&#21644;&#22522;&#20110;&#21704;&#24076;&#30340;&#27880;&#24847;&#21147;&#22312;&#20869;&#30340;&#22823;&#31867;&#31232;&#30095;&#27880;&#24847;&#24615;&#27169;&#24335;&#12290;&#36825;&#23548;&#33268;&#23454;&#29616;&#27809;&#26377;&#20219;&#20309;&#35745;&#31639;&#22797;&#26434;&#24230;&#24320;&#38144;&#65292;&#24182;&#19988;&#19982;&#20197;&#21069;&#30340;&#21160;&#24577;&#31232;&#30095;&#27880;&#24847;&#24615;&#30456;&#27604;&#65292;&#36895;&#24230;&#25552;&#39640;&#20102;&#22810;&#20493;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#29992;&#20316;&#20219;&#20309;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#23494;&#38598;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#22312;&#22810;&#20010;&#35774;&#32622;&#20013;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#29983;&#25104;&#35821;&#35328;&#24314;&#27169;&#21644;&#38271;&#26684;&#24335;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPI&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#24544;&#23454;&#22320;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#12290;</title><link>http://arxiv.org/abs/2306.01153</link><description>&lt;p&gt;
&#36890;&#36807;&#36830;&#32493;&#21518;&#39564;&#25512;&#29702;&#23454;&#29616;&#22810;&#26679;&#21644;&#24544;&#23454;&#30340;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diverse and Faithful Knowledge-Grounded Dialogue Generation via Sequential Posterior Inference. (arXiv:2306.01153v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SPI&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#24544;&#23454;&#22320;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20107;&#23454;&#30693;&#35782;&#29983;&#25104;&#20855;&#26377;&#22810;&#26679;&#24615;&#21644;&#24544;&#23454;&#24615;&#30340;&#22238;&#22797;&#23545;&#20110;&#21019;&#24314;&#31867;&#20154;&#12289;&#21487;&#20449;&#20219;&#30340;&#23545;&#35805;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;"&#36830;&#32493;&#21518;&#39564;&#25512;&#29702;&#65288;SPI&#65289;"&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#36890;&#36807;&#23545;&#21518;&#39564;&#20998;&#24067;&#36827;&#34892;&#36817;&#20284;&#37319;&#26679;&#26469;&#36873;&#25321;&#30693;&#35782;&#24182;&#29983;&#25104;&#23545;&#35805;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;SPI&#19981;&#38656;&#35201;&#25512;&#29702;&#32593;&#32476;&#25110;&#20551;&#35774;&#21518;&#39564;&#20998;&#24067;&#20855;&#26377;&#19968;&#20010;&#31616;&#21333;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#20854;&#30452;&#25509;&#26597;&#35810;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#30452;&#25509;&#65292;&#20174;&#32780;&#23454;&#29616;&#20934;&#30830;&#30340;&#30693;&#35782;&#36873;&#25321;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capability to generate responses with diversity and faithfulness using factual knowledge is paramount for creating a human-like, trustworthy dialogue system. Common strategies either adopt a two-step paradigm, which optimizes knowledge selection and response generation separately, and may overlook the inherent correlation between these two tasks, or leverage conditional variational method to jointly optimize knowledge selection and response generation by employing an inference network. In this paper, we present an end-to-end learning framework, termed Sequential Posterior Inference (SPI), capable of selecting knowledge and generating dialogues by approximately sampling from the posterior distribution. Unlike other methods, SPI does not require the inference network or assume a simple geometry of the posterior distribution. This straightforward and intuitive inference procedure of SPI directly queries the response generation model, allowing for accurate knowledge selection and gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#23450;&#20041;&#22312;&#25351;&#20196;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#24403;&#21024;&#38500;&#25551;&#36848;&#20219;&#21153;&#36755;&#20986;&#30340;&#20869;&#23481;&#26102;&#27169;&#22411;&#24615;&#33021;&#25165;&#20250;&#26174;&#33879;&#19979;&#38477;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#31639;&#27861;&#21487;&#20197;&#21387;&#32553;&#20219;&#21153;&#23450;&#20041;&#65292;&#21024;&#38500;60\%&#26631;&#35760;&#20173;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01150</link><description>&lt;p&gt;
&#20320;&#35835;&#25026;&#20102;&#35828;&#26126;&#20070;&#21527;&#65311;&#37325;&#26032;&#24605;&#32771;&#25351;&#20196;&#23398;&#20064;&#20013;&#20219;&#21153;&#23450;&#20041;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning. (arXiv:2306.01150v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#21153;&#23450;&#20041;&#22312;&#25351;&#20196;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#24403;&#21024;&#38500;&#25551;&#36848;&#20219;&#21153;&#36755;&#20986;&#30340;&#20869;&#23481;&#26102;&#27169;&#22411;&#24615;&#33021;&#25165;&#20250;&#26174;&#33879;&#19979;&#38477;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#31639;&#27861;&#21487;&#20197;&#21387;&#32553;&#20219;&#21153;&#23450;&#20041;&#65292;&#21024;&#38500;60\%&#26631;&#35760;&#20173;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#35299;&#20915;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#26159;&#21542;&#30495;&#27491;&#29702;&#35299;&#20219;&#21153;&#23450;&#20041;&#20197;&#21450;&#20154;&#31867;&#32534;&#20889;&#30340;&#23450;&#20041;&#26159;&#21542;&#26368;&#20339;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#20219;&#21153;&#23450;&#20041;&#22312;&#25351;&#20196;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#28040;&#34701;&#20998;&#26512;&#65292;&#36890;&#36807;&#20154;&#31867;&#27880;&#37322;&#21457;&#29616;&#21482;&#26377;&#24403;&#21024;&#38500;&#25551;&#36848;&#20219;&#21153;&#36755;&#20986;&#30340;&#20869;&#23481;&#65292;&#23588;&#20854;&#26159;&#26631;&#31614;&#20449;&#24687;&#26102;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#25165;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#31639;&#27861;&#65292;&#23558;&#20219;&#21153;&#23450;&#20041;&#21387;&#32553;&#21040;&#26368;&#23567;&#30340;&#25903;&#25345;&#26631;&#35760;&#38598;&#65292;&#24182;&#21457;&#29616;&#21487;&#20197;&#21024;&#38500;60\%&#30340;&#26631;&#35760;&#32780;&#32500;&#25345;&#25110;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#24110;&#21161;&#27169;&#22411;&#26356;&#22909;&#22320;&#21033;&#29992;&#20219;&#21153;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown impressive performance in following natural language instructions to solve unseen tasks. However, it remains unclear whether models truly understand task definitions and whether the human-written definitions are optimal. In this paper, we systematically study the role of task definitions in instruction learning. We first conduct an ablation analysis informed by human annotations to understand which parts of a task definition are most important, and find that model performance only drops substantially when removing contents describing the task output, in particular label information. Next, we propose an automatic algorithm to compress task definitions to a minimal supporting set of tokens, and find that 60\% of tokens can be removed while maintaining or even improving model performance. Based on these results, we propose two strategies to help models better leverage task instructions: (1) providing only key information for tasks in a common struct
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#26041;&#27861;&#29983;&#25104;&#29992;&#20110;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#25512;&#29702;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#27979;&#35797;&#26368;&#26032;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.01144</link><description>&lt;p&gt;
&#29992;&#21512;&#25104;&#20219;&#21153;&#25968;&#25454;&#35780;&#20272;&#22810;&#27169;&#24577;&#25512;&#29702;&#27169;&#22411;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Capabilities of Multi-modal Reasoning Models with Synthetic Task Data. (arXiv:2306.01144v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;&#26041;&#27861;&#29983;&#25104;&#29992;&#20110;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#25512;&#29702;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#27979;&#35797;&#26368;&#26032;&#30340;&#35270;&#35273;&#38382;&#31572;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#21644;&#32852;&#21512;&#35821;&#35328;-&#35270;&#35273;&#29702;&#35299;&#27169;&#22411;&#30340;&#26174;&#30528;&#36827;&#23637;&#21644;&#24212;&#29992;&#22686;&#21152;&#20102;&#23545;&#25506;&#27979;&#20854;&#28508;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26410;&#34987;&#23398;&#26415;&#25968;&#25454;&#38598;&#28085;&#30422;&#30340;&#22797;&#26434;&#22810;&#27169;&#24577;&#25512;&#29702;&#20219;&#21153;&#65292;&#25910;&#38598;&#33258;&#28982;&#25968;&#25454;&#30340;&#22256;&#38590;&#21046;&#32422;&#20102;&#23545;AI&#26041;&#27861;&#30340;&#35780;&#20272;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#39640;&#20998;&#36776;&#29575;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#20026;&#22810;&#27169;&#24577;&#25512;&#29702;&#20219;&#21153;&#29983;&#25104;&#35780;&#20272;&#25968;&#25454;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#24322;&#24120;&#25968;&#25454;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#35206;&#30422;&#19981;&#36275;&#12290;&#25105;&#20204;&#23545;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#27169;&#22411;&#22312;&#20351;&#29992;&#27492;&#26041;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#34920;&#26126;&#23613;&#31649;&#20219;&#21153;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#35813;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26174;&#33879;&#20302;&#20110;&#29616;&#26377;&#30340;&#23398;&#26415;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#31361;&#26174;&#20102;&#32771;&#34385;&#21512;&#25104;&#20219;&#21153;&#20197;&#35780;&#20272;&#21644;&#29702;&#35299;AI&#27169;&#22411;&#30340;&#28508;&#22312;&#33021;&#21147;&#21644;&#38480;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impressive advances and applications of large language and joint language-and-visual understanding models has led to an increased need for methods of probing their potential reasoning capabilities. However, the difficulty of gather naturally-occurring data for complex multi-modal reasoning tasks bottlenecks the evaluation of AI methods on tasks which are not already covered by an academic dataset. In this work, we leverage recent advances in high resolution text-to-image generation to develop a framework for generating evaluation data for multi-modal reasoning tasks. We apply this framework to generate context-dependent anomaly data, creating a synthetic dataset on a challenging task which is not well covered by existing datasets. We benchmark the performance of a state-of-the-art visual question answering (VQA) model against data generated with this method, and demonstrate that while the task is tractable, the model performs significantly worse on the context-dependent anomaly det
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21517;&#23383;&#23545;&#24120;&#35782;&#25512;&#29702;&#20013;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#21517;&#23383;&#30340;&#39057;&#29575;&#20250;&#30452;&#25509;&#24433;&#21709;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.01117</link><description>&lt;p&gt;
&#32771;&#23519;&#21517;&#23383;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#22240;&#26524;&#24433;&#21709;&#65306;&#20197;&#31038;&#20132;&#24120;&#35782;&#25512;&#29702;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Examining the Causal Effect of First Names on Language Models: The Case of Social Commonsense Reasoning. (arXiv:2306.01117v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21517;&#23383;&#23545;&#24120;&#35782;&#25512;&#29702;&#20013;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#21517;&#23383;&#30340;&#39057;&#29575;&#20250;&#30452;&#25509;&#24433;&#21709;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#32487;&#32493;&#34987;&#25972;&#21512;&#21040;&#20010;&#20154;&#21644;&#31038;&#20250;&#30456;&#20851;&#30340;&#24212;&#29992;&#20013;&#65292;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#35201;&#20445;&#35777;&#23427;&#20204;&#20135;&#29983;&#30340;&#36755;&#20986;&#32467;&#26524;&#19981;&#21463;&#25935;&#24863;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;&#32771;&#34385;&#21040;&#21517;&#23383;&#21487;&#33021;&#20195;&#34920;&#65288;&#20132;&#21449;&#30340;&#65289;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#23398;&#20449;&#24687;&#65292;&#26377;&#24517;&#35201;&#30740;&#31350;&#21517;&#23383;&#23545;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24403;&#32473;&#23450;&#19981;&#21516;&#21517;&#23383;&#26102;&#65292;&#27169;&#22411;&#22312;&#32473;&#20986;&#29305;&#23450;&#36755;&#20837;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#26159;&#21542;&#23384;&#22312;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#22522;&#26412;&#20551;&#35774;&#26159;&#65292;&#20851;&#20110;Alice&#30340;&#25512;&#29702;&#19981;&#24212;&#35813;&#19981;&#21516;&#20110;&#20851;&#20110;James&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#21463;&#25511;&#30340;&#23454;&#39564;&#26694;&#26550;&#65292;&#20197;&#27979;&#37327;&#21517;&#23383;&#23545;&#24120;&#35782;&#25512;&#29702;&#30340;&#22240;&#26524;&#24433;&#21709;&#65292;&#20174;&#32780;&#21306;&#20998;&#30001;&#20598;&#28982;&#21644;&#23454;&#38469;&#24863;&#20852;&#36259;&#30340;&#22240;&#32032;&#24341;&#36215;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21517;&#23383;&#30340;&#39057;&#29575;&#23545;&#27169;&#22411;&#30340;&#39044;&#27979;&#26377;&#30452;&#25509;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models continue to be integrated into applications of personal and societal relevance, ensuring these models' trustworthiness is crucial, particularly with respect to producing consistent outputs regardless of sensitive attributes. Given that first names may serve as proxies for (intersectional) socio-demographic representations, it is imperative to examine the impact of first names on commonsense reasoning capabilities. In this paper, we study whether a model's reasoning given a specific input differs based on the first names provided. Our underlying assumption is that the reasoning about Alice should not differ from the reasoning about James. We propose and implement a controlled experimental framework to measure the causal effect of first names on commonsense reasoning, enabling us to distinguish between model predictions due to chance and caused by actual factors of interest. Our results indicate that the frequency of first names has a direct effect on model prediction,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#32463;&#36807;&#36866;&#24403;&#30340;&#31579;&#36873;&#21644;&#21435;&#37325;&#21518;&#65292;&#20165;&#20973;&#32593;&#32476;&#25968;&#25454;&#23601;&#33021;&#35757;&#32451;&#20986;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#29978;&#33267;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992;&#31579;&#36873;&#21518;&#30340;&#35821;&#26009;&#24211;&#35757;&#32451;&#12290;&#30740;&#31350;&#36824;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#24191;&#27867;&#31579;&#36873;&#21518;&#65292;&#20174;&#32593;&#32476;&#20013;&#25552;&#21462;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#20173;&#28982;&#20805;&#36275;&#65292;&#36825;&#20026;&#35757;&#32451;&#26356;&#22823;&#12289;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;</title><link>http://arxiv.org/abs/2306.01116</link><description>&lt;p&gt;
Falcon LLM&#30340;RefinedWeb&#25968;&#25454;&#38598;&#65306;&#20165;&#20973;&#32593;&#32476;&#25968;&#25454;&#32988;&#36807;&#31579;&#36873;&#21518;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only. (arXiv:2306.01116v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#32463;&#36807;&#36866;&#24403;&#30340;&#31579;&#36873;&#21644;&#21435;&#37325;&#21518;&#65292;&#20165;&#20973;&#32593;&#32476;&#25968;&#25454;&#23601;&#33021;&#35757;&#32451;&#20986;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#29978;&#33267;&#26126;&#26174;&#20248;&#20110;&#20351;&#29992;&#31579;&#36873;&#21518;&#30340;&#35821;&#26009;&#24211;&#35757;&#32451;&#12290;&#30740;&#31350;&#36824;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#24191;&#27867;&#31579;&#36873;&#21518;&#65292;&#20174;&#32593;&#32476;&#20013;&#25552;&#21462;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#20173;&#28982;&#20805;&#36275;&#65292;&#36825;&#20026;&#35757;&#32451;&#26356;&#22823;&#12289;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#30001;&#32463;&#36807;&#31579;&#36873;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#32463;&#36807;&#31579;&#36873;&#30340;&#39640;&#36136;&#37327;&#35821;&#26009;&#24211;&#65288;&#22914;&#31038;&#20132;&#23186;&#20307;&#23545;&#35805;&#12289;&#20070;&#31821;&#25110;&#25216;&#26415;&#35770;&#25991;&#65289;&#28151;&#21512;&#35757;&#32451;&#12290;&#36825;&#31181;&#31579;&#36873;&#36807;&#31243;&#34987;&#35748;&#20026;&#26159;&#20135;&#29983;&#20855;&#26377;&#24191;&#27867;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#30340;&#39640;&#24615;&#33021;&#27169;&#22411;&#25152;&#24517;&#38656;&#30340;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#32771;&#34385;&#38656;&#35201;&#39044;&#20808;&#35757;&#32451;&#25968;&#19975;&#20159;&#20010;&#26631;&#35760;&#30340;&#26356;&#22823;&#27169;&#22411;&#65292;&#31579;&#36873;&#30340;&#21487;&#25193;&#23637;&#24615;&#26159;&#21542;&#20250;&#20986;&#29616;&#29942;&#39048;&#20197;&#21450;&#25105;&#20204;&#26159;&#21542;&#20250;&#24456;&#24555;&#29992;&#23613;&#29420;&#29305;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#19982;&#20197;&#24448;&#30340;&#24819;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#36807;&#36866;&#24403;&#31579;&#36873;&#21644;&#21435;&#37325;&#30340;&#32593;&#32476;&#25968;&#25454;&#21487;&#20197;&#23548;&#33268;&#21151;&#33021;&#24378;&#22823;&#30340;&#27169;&#22411;&#65307;&#29978;&#33267;&#26126;&#26174;&#20248;&#20110;&#35757;&#32451;&#22312;The Pile &#19978;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#32463;&#36807;&#20102;&#24191;&#27867;&#30340;&#36807;&#28388;&#65292;&#25105;&#20204;&#20174;&#32593;&#32476;&#20013;&#25552;&#21462;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#20173;&#28982;&#24456;&#20016;&#23500;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;CommonCrawl&#20013;&#33719;&#24471;&#20116;&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#20174;&#25105;&#20204;&#30340;RefinedWeb&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30340;6000&#20159;&#20010;&#26631;&#35760;&#30340;&#29255;&#27573;&#65292;&#20197;&#21450;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;1.3 / 7.5B&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are commonly trained on a mixture of filtered web data and curated high-quality corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation and whether we will run out of unique high-quality data soon. At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models from the state-of-the-art trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 600 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;GOTHate&#25968;&#25454;&#38598;&#65292;&#24182;&#35814;&#32454;&#27604;&#36739;&#20102;&#35813;&#25968;&#25454;&#38598;&#19982;&#29616;&#26377;&#30340;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22914;&#20309;&#25429;&#25417;&#20013;&#31435;&#30340;&#24694;&#24847;&#20869;&#23481;&#20013;&#30340;&#20167;&#24680;&#20449;&#21495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;GOTHate&#24456;&#38590;&#22312;&#32431;&#25991;&#26412;&#29615;&#22659;&#19979;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#28155;&#21152;&#20869;&#29983;&#20449;&#21495;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01105</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20167;&#24680;&#35328;&#35770;&#22522;&#20934;&#65306;&#20174;&#25968;&#25454;&#31579;&#36873;&#21040;&#31995;&#32479;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Revisiting Hate Speech Benchmarks: From Data Curation to System Deployment. (arXiv:2306.01105v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;GOTHate&#25968;&#25454;&#38598;&#65292;&#24182;&#35814;&#32454;&#27604;&#36739;&#20102;&#35813;&#25968;&#25454;&#38598;&#19982;&#29616;&#26377;&#30340;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22914;&#20309;&#25429;&#25417;&#20013;&#31435;&#30340;&#24694;&#24847;&#20869;&#23481;&#20013;&#30340;&#20167;&#24680;&#20449;&#21495;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;GOTHate&#24456;&#38590;&#22312;&#32431;&#25991;&#26412;&#29615;&#22659;&#19979;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#28155;&#21152;&#20869;&#29983;&#20449;&#21495;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#20805;&#26021;&#30528;&#20805;&#28385;&#20167;&#24680;&#30340;&#20869;&#23481;&#65292;&#20854;&#20013;&#24456;&#22810;&#32463;&#24120;&#20276;&#38543;&#30528;&#35821;&#35328;&#21644;&#20027;&#39064;&#30340;&#22810;&#26679;&#24615;&#12290;&#29992;&#20110;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#31181;&#32972;&#31163;&#65292;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#26159;&#20351;&#29992;&#20167;&#24680;&#35789;&#27719;&#32534;&#21046;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#20013;&#31435;&#30340;&#24694;&#24847;&#20869;&#23481;&#20013;&#25429;&#25417;&#20167;&#24680;&#20449;&#21495;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#35774;&#35745;&#27169;&#25311;&#20167;&#24680;&#29616;&#23454;&#19990;&#30028;&#21464;&#24322;&#24615;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GOTHate&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#20102;&#19981;&#21516;&#35821;&#35328;&#21644;&#20027;&#39064;&#30340;&#22823;&#35268;&#27169;&#20195;&#30721;&#28151;&#21512;&#20247;&#21253;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;Twitter&#20013;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#65292;&#21253;&#25324;&#32422;51k&#20010;&#24086;&#23376;&#12290;&#25105;&#20204;&#35814;&#32454;&#27604;&#36739;&#20102;GOTHate&#21644;&#29616;&#26377;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#65292;&#31361;&#20986;&#20102;&#23427;&#30340;&#26032;&#39062;&#24615;&#65292;&#24182;&#20351;&#29992;10&#20010;&#26368;&#36817;&#30340;&#22522;&#20934;&#32447;&#23545;&#20854;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#35777;&#21644;&#22522;&#20934;&#27979;&#35797;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#32431;&#25991;&#26412;&#29615;&#22659;&#19979;&#65292;&#24456;&#38590;&#23545;GOTHate&#36827;&#34892;&#20998;&#31867;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#28155;&#21152;&#20869;&#29983;&#20449;&#21495;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media is awash with hateful content, much of which is often veiled with linguistic and topical diversity. The benchmark datasets used for hate speech detection do not account for such divagation as they are predominantly compiled using hate lexicons. However, capturing hate signals becomes challenging in neutrally-seeded malicious content. Thus, designing models and datasets that mimic the real-world variability of hate warrants further investigation.  To this end, we present GOTHate, a large-scale code-mixed crowdsourced dataset of around 51k posts for hate speech detection from Twitter. GOTHate is neutrally seeded, encompassing different languages and topics. We conduct detailed comparisons of GOTHate with the existing hate speech datasets, highlighting its novelty. We benchmark it with 10 recent baselines. Our extensive empirical and benchmarking experiments suggest that GOTHate is hard to classify in a text-only setup. Thus, we investigate how adding endogenous signals enhan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;</title><link>http://arxiv.org/abs/2306.01102</link><description>&lt;p&gt;
LLMatic: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#30340;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization. (arXiv:2306.01102v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340; LLMatic &#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#20165;&#36827;&#34892;2000&#27425;&#25628;&#32034;&#21363;&#21487;&#20135;&#29983;&#39640;&#24615;&#33021;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#23436;&#25104;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;&#23427;&#20204;&#30340;&#33021;&#21147;&#28085;&#30422;&#20102;&#35768;&#22810;&#39046;&#22495;&#65292;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#22312;&#27492;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558; LLMs &#35270;&#20026;&#21464;&#24322;&#21644;&#20132;&#21449;&#24037;&#20855;&#12290;&#21516;&#26102;&#65292;&#22810;&#26679;&#24615;&#20248;&#21270;&#31639;&#27861;&#24050;&#30693;&#21487;&#20197;&#21457;&#29616;&#22810;&#26679;&#24615;&#21644;&#31283;&#20581;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#23558; LLMs &#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#19982; QD &#35299;&#20915;&#26041;&#26696;&#30340;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#24615;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; LLMatic&#65292;&#19968;&#20010;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034; (NAS) &#31639;&#27861;&#12290;&#34429;&#28982; LLMs &#36890;&#36807;&#25552;&#31034;&#30452;&#25509;&#36827;&#34892; NAS &#32771;&#39564;&#22256;&#38590;&#65292;&#20294; LLMatic &#21033;&#29992;&#31243;&#24207;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992; QD &#26469;&#36827;&#34892;&#25552;&#31034;&#21644;&#32593;&#32476;&#32467;&#26500;&#65292;&#20174;&#32780;&#21019;&#24314;&#22810;&#26679;&#24615;&#21644;&#39640;&#24615;&#33021;&#32593;&#32476;&#12290;&#25105;&#20204;&#22312; CIFAR-10 &#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#27979;&#35797;&#20102; LLMatic&#65292;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#20165;&#36827;&#34892; 2000 &#27425;&#25628;&#32034;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32593;&#32476;&#65292;&#21363;&#20351;&#27809;&#26377;&#35813;&#22522;&#20934;&#39046;&#22495;&#30340;&#20808;&#21069;&#30693;&#35782;&#25110;&#20219;&#20309;&#20808;&#21069;&#30340;&#26368;&#20339;&#32467;&#26524;&#30340;&#26333;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. In this context, we view LLMs as mutation and crossover tools. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce LLMatic, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, LLMatic uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and highly performant networks. We test LLMatic on the CIFAR-10 image classification benchmark, demonstrating that it can produce competitive networks with just $2,000$ searches, even without prior knowledge of the benchmark domain or exposure to any previous top-p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; UCAS-IIE-NLP &#35774;&#35745;&#30340; SACL-XLMR &#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#35789;&#20856;&#24335;&#22810;&#35821;&#35328;BERT&#36827;&#34892;&#24773;&#24863;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#65292;&#20351;&#29992;&#30417;&#30563;&#24615;&#23545;&#25239;&#24335;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#24773;&#24863;&#20256;&#25773;&#32467;&#26500;&#21270;&#34920;&#31034;&#23398;&#20064;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#24773;&#24863;&#20998;&#26512;&#65292;&#19988;&#22312;&#22810;&#35821;&#35328;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2306.01093</link><description>&lt;p&gt;
UCAS-IIE-NLP&#22312;SemEval-2023&#20219;&#21153;12&#20013;&#30340;&#24212;&#29992;&#65306;&#22686;&#24378;&#20302;&#36164;&#28304;&#24773;&#24863;&#20998;&#26512;&#30340;&#22810;&#35821;&#35328;BERT&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
UCAS-IIE-NLP at SemEval-2023 Task 12: Enhancing Generalization of Multilingual BERT for Low-resource Sentiment Analysis. (arXiv:2306.01093v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; UCAS-IIE-NLP &#35774;&#35745;&#30340; SACL-XLMR &#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#35789;&#20856;&#24335;&#22810;&#35821;&#35328;BERT&#36827;&#34892;&#24773;&#24863;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#65292;&#20351;&#29992;&#30417;&#30563;&#24615;&#23545;&#25239;&#24335;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#24773;&#24863;&#20256;&#25773;&#32467;&#26500;&#21270;&#34920;&#31034;&#23398;&#20064;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#24773;&#24863;&#20998;&#26512;&#65292;&#19988;&#22312;&#22810;&#35821;&#35328;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#20026;SemEval-2023&#20219;&#21153;12&#35774;&#35745;&#30340;&#31995;&#32479;&#65306;&#38750;&#27954;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#35813;&#20219;&#21153;&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#26631;&#27880;&#25968;&#25454;&#21644;&#35821;&#35328;&#36164;&#28304;&#30340;&#31232;&#32570;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SACL-XLMR&#30340;&#36890;&#29992;&#22810;&#35821;&#35328;&#31995;&#32479;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35789;&#20856;&#30340;&#22810;&#35821;&#35328;BERT&#65292;&#20197;&#20415;&#36827;&#34892;&#35821;&#35328;&#36866;&#24212;&#21644;&#24773;&#24863;&#24863;&#30693;&#34920;&#31034;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#30417;&#30563;&#24615;&#30340;&#23545;&#25239;&#24335;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#26469;&#23398;&#20064;&#24773;&#24863;&#20256;&#25773;&#32467;&#26500;&#21270;&#34920;&#31034;&#24182;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#22810;&#35821;&#35328;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36828;&#36828;&#36229;&#36807;&#22522;&#32447;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#23448;&#26041;&#25490;&#21517;&#20013;&#65292;&#35813;&#31995;&#32479;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#23376;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our system designed for SemEval-2023 Task 12: Sentiment analysis for African languages. The challenge faced by this task is the scarcity of labeled data and linguistic resources in low-resource settings. To alleviate these, we propose a generalized multilingual system SACL-XLMR for sentiment analysis on low-resource languages. Specifically, we design a lexicon-based multilingual BERT to facilitate language adaptation and sentiment-aware representation learning. Besides, we apply a supervised adversarial contrastive learning technique to learn sentiment-spread structured representations and enhance model generalization. Our system achieved competitive results, largely outperforming baselines on both multilingual and zero-shot sentiment classification subtasks. Notably, the system obtained the 1st rank on the zero-shot classification subtask in the official ranking. Extensive experiments demonstrate the effectiveness of our system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#25216;&#26415;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#39640;&#25688;&#35201;&#31995;&#32479;&#40065;&#26834;&#24615;&#30340;&#21452;&#37325;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#22312;&#23545;&#25239;&#24615;&#21644;&#22122;&#38899;&#27979;&#35797;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01090</link><description>&lt;p&gt;
&#29992;&#21452;&#37325;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#25552;&#39640;&#25688;&#35201;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Robustness of Summarization Systems with Dual Augmentation. (arXiv:2306.01090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#23545;&#25239;&#26679;&#26412;&#29983;&#25104;&#25216;&#26415;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#39640;&#25688;&#35201;&#31995;&#32479;&#40065;&#26834;&#24615;&#30340;&#21452;&#37325;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#22312;&#23545;&#25239;&#24615;&#21644;&#22122;&#38899;&#27979;&#35797;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#40065;&#26834;&#30340;&#25688;&#35201;&#31995;&#32479;&#24212;&#35813;&#33021;&#22815;&#25429;&#25417;&#21040;&#25991;&#26723;&#30340;&#35201;&#28857;&#65292;&#32780;&#19981;&#21463;&#29305;&#23450;&#21333;&#35789;&#36873;&#25321;&#25110;&#36755;&#20837;&#20013;&#30340;&#22122;&#38899;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25506;&#32034;&#20102;&#25688;&#35201;&#27169;&#22411;&#22312;&#21253;&#25324;&#21333;&#35789;&#32423;&#21516;&#20041;&#35789;&#26367;&#25442;&#21644;&#22122;&#38899;&#22312;&#20869;&#30340;&#25200;&#21160;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#21019;&#24314;&#35821;&#20041;&#19968;&#33268;&#30340;&#26367;&#20195;&#35789;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SummAttacker&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#20808;&#36827;&#30340;&#25688;&#35201;&#27169;&#22411;&#22312;&#23545;&#25239;&#24615;&#21644;&#22122;&#38899;&#27979;&#35797;&#38598;&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25688;&#35201;&#31995;&#32479;&#30340;&#24369;&#28857;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#25506;&#32034;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#31532;&#19968;&#20010;&#33030;&#24369;&#24615;&#22240;&#32032;&#26159;&#25688;&#35201;&#27169;&#22411;&#23545;&#36755;&#20837;&#20013;&#19981;&#24120;&#35265;&#21333;&#35789;&#30340;&#29702;&#35299;&#33021;&#21147;&#36739;&#24046;&#12290;&#30456;&#24212;&#22320;&#65292;&#25105;&#20204;&#23558;&#26356;&#22810;&#30001;SummAttacker&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#21019;&#24314;&#30340;&#22810;&#26679;&#21270;&#26696;&#20363;&#39304;&#20837;&#32534;&#30721;&#22120;&#12290;&#21478;&#19968;&#20010;&#22240;&#32032;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#25915;&#20987;&#30340;&#36755;&#20837;&#20250;&#20026;&#35299;&#30721;&#22120;&#20135;&#29983;&#22122;&#22768;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#38454;&#27573;&#25945;&#25688;&#35201;&#27169;&#22411;&#23545;&#38543;&#26426;&#22122;&#22768;&#20855;&#26377;&#19981;&#21464;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21452;&#37325;&#22686;&#24378;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25688;&#35201;&#27169;&#22411;&#22312;&#23545;&#25239;&#24615;&#21644;&#22122;&#38899;&#27979;&#35797;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A robust summarization system should be able to capture the gist of the document, regardless of the specific word choices or noise in the input. In this work, we first explore the summarization models' robustness against perturbations including word-level synonym substitution and noise. To create semantic-consistent substitutes, we propose a SummAttacker, which is an efficient approach to generating adversarial samples based on language models. Experimental results show that state-of-the-art summarization models have a significant decrease in performance on adversarial and noisy test sets. Next, we analyze the vulnerability of the summarization systems and explore improving the robustness by data augmentation. Specifically, the first brittleness factor we found is the poor understanding of infrequent words in the input. Correspondingly, we feed the encoder with more diverse cases created by SummAttacker in the input space. The other factor is in the latent space, where the attacked inp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#24352;&#37327;&#21387;&#32553;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#31639;&#26415;&#36816;&#31639;&#21644;&#26368;&#32456;&#36816;&#34892;&#26102;&#24310;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#36880;&#23618;&#33976;&#39311;&#21518;&#22312;&#25991;&#26412;&#34164;&#21547;&#21644;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01076</link><description>&lt;p&gt;
&#37327;&#21270;&#24863;&#30693;&#21644;&#24352;&#37327;&#21387;&#32553;&#35757;&#32451;&#65306;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Quantization-Aware and Tensor-Compressed Training of Transformers for Natural Language Understanding. (arXiv:2306.01076v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#24352;&#37327;&#21387;&#32553;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#23569;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#31639;&#26415;&#36816;&#31639;&#21644;&#26368;&#32456;&#36816;&#34892;&#26102;&#24310;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#36880;&#23618;&#33976;&#39311;&#21518;&#22312;&#25991;&#26412;&#34164;&#21547;&#21644;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#30340;Transformer&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#38459;&#27490;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#39640;&#24615;&#33021;Transformer&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#24863;&#30693;&#24352;&#37327;&#21387;&#32553;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#27169;&#22411;&#22823;&#23567;&#65292;&#31639;&#26415;&#36816;&#31639;&#21644;&#26368;&#32456;&#36816;&#34892;&#26102;&#24310;&#12290;&#25105;&#20204;&#23558;Transformer&#30340;&#23884;&#20837;&#21644;&#32447;&#24615;&#23618;&#21387;&#32553;&#20026;&#23567;&#22411;&#20302;&#31209;&#24352;&#37327;&#26680;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#12290;&#37319;&#29992;&#21487;&#23398;&#20064;&#27604;&#20363;&#22240;&#23376;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26469;&#36827;&#19968;&#27493;&#33719;&#24471;&#24352;&#37327;&#21387;&#32553;&#27169;&#22411;&#30340;&#20302;&#31934;&#24230;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#31471;&#21040;&#31471;&#35757;&#32451;&#21644;&#33976;&#39311;&#35757;&#32451;&#12290;&#20026;&#20102;&#25552;&#39640;&#25910;&#25947;&#24615;&#65292;&#37319;&#29992;&#36880;&#23618;&#33976;&#39311;&#26041;&#27861;&#20174;&#39044;&#35757;&#32451;Transformer&#20013;&#25552;&#21462;&#20986;&#19968;&#20010;&#32463;&#36807;&#37327;&#21270;&#21644;&#24352;&#37327;&#21387;&#32553;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#22312;&#20004;&#20010;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21363;&#25991;&#26412;&#34164;&#21547;&#21644;&#24773;&#24863;&#20998;&#26512;&#20013;&#23637;&#31034;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuned transformer models have shown superior performances in many natural language tasks. However, the large model size prohibits deploying high-performance transformer models on resource-constrained devices. This paper proposes a quantization-aware tensor-compressed training approach to reduce the model size, arithmetic operations, and ultimately runtime latency of transformer-based models. We compress the embedding and linear layers of transformers into small low-rank tensor cores, which significantly reduces model parameters. A quantization-aware training with learnable scale factors is used to further obtain low-precision representations of the tensor-compressed models. The developed approach can be used for both end-to-end training and distillation-based training. To improve the convergence, a layer-by-layer distillation is applied to distill a quantized and tensor-compressed student model from a pre-trained transformer. The performance is demonstrated in two natural language
&lt;/p&gt;</description></item><item><title>TimelineQA&#26159;&#19968;&#20010;&#29992;&#20110;&#26597;&#35810;&#29983;&#27963;&#26085;&#24535;&#30340;&#21152;&#36895;&#36827;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28041;&#21450;&#26102;&#38388;&#21644;&#22320;&#29702;&#20449;&#24687;&#65292;&#24050;&#32463;&#20844;&#24320;&#21457;&#24067;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20294;&#36825;&#20004;&#31181;&#27169;&#22411;&#22343;&#26410;&#36798;&#21040;&#20154;&#31867;&#34920;&#29616;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2306.01069</link><description>&lt;p&gt;
TimelineQA: &#19968;&#20010;&#29992;&#20110;&#26102;&#38388;&#32447;&#38382;&#31572;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
TimelineQA: A Benchmark for Question Answering over Timelines. (arXiv:2306.01069v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01069
&lt;/p&gt;
&lt;p&gt;
TimelineQA&#26159;&#19968;&#20010;&#29992;&#20110;&#26597;&#35810;&#29983;&#27963;&#26085;&#24535;&#30340;&#21152;&#36895;&#36827;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28041;&#21450;&#26102;&#38388;&#21644;&#22320;&#29702;&#20449;&#24687;&#65292;&#24050;&#32463;&#20844;&#24320;&#21457;&#24067;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20294;&#36825;&#20004;&#31181;&#27169;&#22411;&#22343;&#26410;&#36798;&#21040;&#20154;&#31867;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lifelogs&#65288;&#29983;&#27963;&#26085;&#24535;&#65289;&#26159;&#20154;&#20204;&#29983;&#27963;&#32463;&#21382;&#30340;&#25551;&#36848;&#65292;&#36825;&#20123;&#26085;&#24535;&#36890;&#36807;&#32467;&#21512;&#26469;&#33258;&#22810;&#20010;&#25968;&#23383;&#26381;&#21153;&#65288;&#22914;&#22312;&#32447;&#29031;&#29255;&#12289;&#22320;&#22270;&#12289;&#36141;&#29289;&#21644;&#20869;&#23481;&#27969;&#23186;&#20307;&#26381;&#21153;&#65289;&#30340;&#25968;&#25454;&#26469;&#21019;&#24314;&#12290;&#38382;&#31572;&#25216;&#26415;&#22312;&#29983;&#27963;&#26085;&#24535;&#19978;&#30340;&#24212;&#29992;&#21487;&#20197;&#20026;&#20010;&#20154;&#21161;&#25163;&#22312;&#25552;&#20379;&#19978;&#19979;&#25991;&#26041;&#38754;&#25552;&#20379;&#20851;&#38190;&#36164;&#28304;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#29983;&#27963;&#26085;&#24535;&#32467;&#21512;&#20102;&#33258;&#30001;&#25991;&#26412;&#19982;&#19968;&#23450;&#31243;&#24230;&#30340;&#32467;&#26500;&#65292;&#22914;&#26102;&#38388;&#21644;&#22320;&#29702;&#20449;&#24687;&#65292;&#22240;&#27492;&#24403;&#21069;&#30340;&#38382;&#31572;&#25216;&#26415;&#26080;&#27861;&#22238;&#31572;&#27492;&#31867;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#24182;&#20844;&#24320;&#21457;&#24067;&#20102;TimelineQA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26597;&#35810;&#29983;&#27963;&#26085;&#24535;&#30340;&#21152;&#36895;&#36827;&#23637;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;TimelineQA&#29983;&#25104;&#34394;&#26500;&#20154;&#29289;&#30340;&#29983;&#27963;&#26085;&#24535;&#12290;&#29983;&#27963;&#26085;&#24535;&#20013;&#30340;&#20107;&#20214;&#20174;&#39640;&#20013;&#27605;&#19994;&#31561;&#37325;&#22823;&#29983;&#27963;&#20107;&#20214;&#21040;&#26085;&#24120;&#27963;&#21160;&#22914;&#24930;&#36305;&#37117;&#26377;&#25152;&#35206;&#30422;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20851;&#20110;TimelineQA&#30340;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21457;&#29616;&#36825;&#20004;&#31181;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#22343;&#26410;&#36798;&#21040;&#20154;&#31867;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelogs are descriptions of experiences that a person had during their life. Lifelogs are created by fusing data from the multitude of digital services, such as online photos, maps, shopping and content streaming services. Question answering over lifelogs can offer personal assistants a critical resource when they try to provide advice in context. However, obtaining answers to questions over lifelogs is beyond the current state of the art of question answering techniques for a variety of reasons, the most pronounced of which is that lifelogs combine free text with some degree of structure such as temporal and geographical information.  We create and publicly release TimelineQA1, a benchmark for accelerating progress on querying lifelogs. TimelineQA generates lifelogs of imaginary people. The episodes in the lifelog range from major life episodes such as high school graduation to those that occur on a daily basis such as going for a run. We describe a set of experiments on TimelineQA w
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#21322;&#21442;&#25968;&#21270;&#20307;&#31995;&#32467;&#26500;&#19982;&#26597;&#35810;&#20998;&#26512;&#22120;/&#35745;&#21010;&#22120;&#20197;&#21450;&#28335;&#28304;&#30456;&#32467;&#21512;&#65292;&#21487;&#20351;&#38382;&#39064;&#22238;&#31572;&#30340;&#31995;&#32479;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.01061</link><description>&lt;p&gt;
&#37325;&#22609;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20197;&#22238;&#31572;&#26597;&#35810;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Reimagining Retrieval Augmented Language Models for Answering Queries. (arXiv:2306.01061v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01061
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#21322;&#21442;&#25968;&#21270;&#20307;&#31995;&#32467;&#26500;&#19982;&#26597;&#35810;&#20998;&#26512;&#22120;/&#35745;&#21010;&#22120;&#20197;&#21450;&#28335;&#28304;&#30456;&#32467;&#21512;&#65292;&#21487;&#20351;&#38382;&#39064;&#22238;&#31572;&#30340;&#31995;&#32479;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#29616;&#23454;&#26816;&#39564;&#65292;&#24182;&#27604;&#36739;&#20102;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#26159;&#21322;&#21442;&#25968;&#21270;&#30340;&#65292;&#27169;&#22411;&#23558;&#27169;&#22411;&#21442;&#25968;&#21644;&#22806;&#37096;&#25968;&#25454;&#28304;&#30340;&#30693;&#35782;&#38598;&#25104;&#21040;&#20854;&#39044;&#27979;&#20013;&#65292;&#19982;&#39321;&#33609;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#21270;&#24615;&#36136;&#30456;&#21453;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#21322;&#21442;&#25968;&#21270;&#20307;&#31995;&#32467;&#26500;&#19982;&#26597;&#35810;&#20998;&#26512;&#22120;/&#35745;&#21010;&#22120;&#20197;&#21450;&#28335;&#28304;&#30456;&#32467;&#21512;&#21487;&#20351;&#38382;&#39064;&#22238;&#31572;&#30340;&#31995;&#32479;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#30340;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#22312;&#28508;&#22312;&#30340;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a reality check on large language models and inspect the promise of retrieval augmented language models in comparison. Such language models are semi-parametric, where models integrate model parameters and knowledge from external data sources to make their predictions, as opposed to the parametric nature of vanilla large language models. We give initial experimental findings that semi-parametric architectures can be enhanced with views, a query analyzer/planner, and provenance to make a significantly more powerful system for question answering in terms of accuracy and efficiency, and potentially for other NLP tasks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#37325;&#35270;&#36716;&#24405;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#24335;&#35299;&#20915;&#35821;&#38899;&#35782;&#21035;&#20013;&#35757;&#32451;&#25968;&#25454;&#19981;&#23436;&#32654;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;ASR&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.01031</link><description>&lt;p&gt;
&#26080;&#38656;&#31934;&#30830;&#26631;&#27880;: &#22522;&#20110;&#19981;&#23436;&#20840;&#36716;&#24405;&#30340;&#24369;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bypass Temporal Classification: Weakly Supervised Automatic Speech Recognition with Imperfect Transcripts. (arXiv:2306.01031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#37325;&#35270;&#36716;&#24405;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#24335;&#35299;&#20915;&#35821;&#38899;&#35782;&#21035;&#20013;&#35757;&#32451;&#25968;&#25454;&#19981;&#23436;&#32654;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;ASR&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#19981;&#23436;&#32654;&#30340;&#35757;&#32451;&#25968;&#25454;&#26500;&#24314;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#27169;&#22411;&#12290;&#36716;&#24405;&#19981;&#20934;&#30830;&#30340;&#35821;&#38899;&#26159;&#20154;&#24037;&#27880;&#37322;&#30340;&#35821;&#38899;&#35821;&#26009;&#24211;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36825;&#20250;&#38477;&#20302;ASR&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Bypass Temporal Classification(BTC)&#20316;&#20026;&#32852;&#32467;&#26102;&#24207;&#20998;&#31867;(Connectionist Temporal Classification, CTC)&#20934;&#21017;&#30340;&#19968;&#20010;&#25193;&#23637;&#12290;BTC&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26174;&#24335;&#22320;&#32534;&#30721;&#20102;&#19982;&#36716;&#24405;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#26159;&#36890;&#36807;&#22686;&#24378;&#35757;&#32451;&#22270;&#30340;&#28789;&#27963;&#24615;&#26469;&#23454;&#29616;&#30340;&#65292;&#23427;&#34987;&#23454;&#29616;&#20026;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#36716;&#25442;&#22120;(WFST)&#32452;&#21512;&#12290;&#36825;&#31181;&#25552;&#20986;&#30340;&#31639;&#27861;&#21487;&#20197;&#25552;&#39640;ASR&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#19981;&#23436;&#25972;&#36716;&#24405;&#30340;&#35821;&#38899;&#35821;&#26009;&#24211;&#26102;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;&#23558;&#25104;&#20026;&#24320;&#28304;&#36719;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel algorithm for building an automatic speech recognition (ASR) model with imperfect training data. Imperfectly transcribed speech is a prevalent issue in human-annotated speech corpora, which degrades the performance of ASR models. To address this problem, we propose Bypass Temporal Classification (BTC) as an expansion of the Connectionist Temporal Classification (CTC) criterion. BTC explicitly encodes the uncertainties associated with transcripts during training. This is accomplished by enhancing the flexibility of the training graph, which is implemented as a weighted finite-state transducer (WFST) composition. The proposed algorithm improves the robustness and accuracy of ASR systems, particularly when working with imprecisely transcribed speech corpora. Our implementation will be open-sourced.
&lt;/p&gt;</description></item><item><title>PV2TEA&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#65292;&#22312;&#22810;&#27169;&#24577;&#27880;&#37322;&#22256;&#38590;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#36328;&#27169;&#24577;&#38598;&#25104;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#20559;&#24046;&#38477;&#20302;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.01016</link><description>&lt;p&gt;
PV2TEA&#65306;&#23558;&#35270;&#35273;&#27169;&#24577;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#20449;&#24687;&#25277;&#21462;&#30456;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
PV2TEA: Patching Visual Modality to Textual-Established Information Extraction. (arXiv:2306.01016v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01016
&lt;/p&gt;
&lt;p&gt;
PV2TEA&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#65292;&#22312;&#22810;&#27169;&#24577;&#27880;&#37322;&#22256;&#38590;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#20102;&#36328;&#27169;&#24577;&#38598;&#25104;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#20559;&#24046;&#38477;&#20302;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25277;&#21462;&#65288;&#20363;&#22914;&#23646;&#24615;&#20540;&#25552;&#21462;&#65289;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#21644;&#24314;&#27169;&#65292;&#20294;&#20165;&#22522;&#20110;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#23646;&#24615;&#21487;&#20197;&#20174;&#22522;&#20110;&#22270;&#20687;&#30340;&#25552;&#21462;&#20013;&#21463;&#30410;&#65292;&#22914;&#39068;&#33394;&#12289;&#24418;&#29366;&#12289;&#22270;&#26696;&#31561;&#12290;&#35270;&#35273;&#27169;&#24577;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22810;&#27169;&#24577;&#27880;&#37322;&#30340;&#38590;&#24230;&#12290;&#26412;&#25991;&#26088;&#22312;&#23558;&#35270;&#35273;&#27169;&#24577;&#19982;&#22522;&#20110;&#25991;&#26412;&#30340;&#23646;&#24615;&#20449;&#24687;&#25552;&#21462;&#22120;&#30456;&#32467;&#21512;&#12290;&#36328;&#27169;&#24577;&#38598;&#25104;&#38754;&#20020;&#20960;&#20010;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#65288;C1&#65289;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#22312;&#26679;&#26412;&#20869;&#21644;&#26679;&#26412;&#38388;&#26494;&#25955;&#21305;&#37197;&#65307;&#65288;C2&#65289;&#22270;&#20687;&#36890;&#24120;&#21253;&#21547;&#20016;&#23500;&#30340;&#32972;&#26223;&#65292;&#21487;&#33021;&#20250;&#35823;&#23548;&#39044;&#27979;&#65307;&#65288;C3&#65289;&#26469;&#33258;&#22522;&#20110;&#25991;&#26412;&#30340;&#25552;&#21462;&#22120;&#30340;&#24369;&#30417;&#30563;&#26631;&#31614;&#23545;&#20110;&#22810;&#27169;&#24577;&#35757;&#32451;&#23384;&#22312;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PV2TEA&#65292;&#36825;&#26159;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#37197;&#22791;&#20102;&#19977;&#31181;&#20559;&#24046;&#38477;&#20302;&#26041;&#26696;&#65306;&#65288;S1&#65289;&#22686;&#24378;&#30340;&#26631;&#31614;&#24179;&#28369;&#23545;&#27604;&#65292;&#20197;&#25913;&#36827;&#26494;&#25955;&#21305;&#37197;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20132;&#21449;&#27169;&#24577;&#23545;&#40784;; &#65288;S2&#65289;&#27880;&#24847;&#21147;&#21098;&#26525;&#26041;&#26696;&#29992;&#20110;&#22312;&#20445;&#30041;&#27491;&#30830;&#20449;&#24687;&#30340;&#21516;&#26102;&#28040;&#38500;&#19968;&#20123;&#19981;&#24517;&#35201;&#30340;&#32454;&#33410;&#65307;&#65288;S3&#65289;&#22522;&#20110;&#23545;&#25239;&#35757;&#32451;&#30340;&#21487;&#37325;&#32452;&#21367;&#31215;&#33258;&#36866;&#24212;&#27169;&#22359;&#65292;&#20197;&#24110;&#21161;&#28040;&#38500;&#26469;&#33258;&#25991;&#26412;&#25552;&#21462;&#22120;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information extraction, e.g., attribute value extraction, has been extensively studied and formulated based only on text. However, many attributes can benefit from image-based extraction, like color, shape, pattern, among others. The visual modality has long been underutilized, mainly due to multimodal annotation difficulty. In this paper, we aim to patch the visual modality to the textual-established attribute information extractor. The cross-modality integration faces several unique challenges: (C1) images and textual descriptions are loosely paired intra-sample and inter-samples; (C2) images usually contain rich backgrounds that can mislead the prediction; (C3) weakly supervised labels from textual-established extractors are biased for multimodal training. We present PV2TEA, an encoder-decoder architecture equipped with three bias reduction schemes: (S1) Augmented label-smoothed contrast to improve the cross-modality alignment for loosely-paired image and text; (S2) Attention-prunin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#22312;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#26102;&#30340;&#36801;&#31227;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20004;&#20010;&#34920;&#31034;&#29702;&#35770;&#65292;&#36890;&#36807;&#29983;&#25104;&#20505;&#36873;&#27169;&#22411;&#30340;&#25490;&#21517;&#20998;&#25968;&#65292;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#23454;&#38469;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#36801;&#31227;&#24615;&#20998;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#19982;&#24494;&#35843;&#22522;&#30784;&#20107;&#23454;&#20043;&#38388;&#23384;&#22312;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#21644;&#20302;&#30340;p&#20540;&#65292;&#26159;&#19968;&#20010;&#33410;&#30465;&#36164;&#28304;&#12289;&#39640;&#25928;&#33410;&#30465;&#26102;&#38388;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01015</link><description>&lt;p&gt;
&#22914;&#20309;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#30340;&#36801;&#31227;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Estimate Model Transferability of Pre-Trained Speech Models?. (arXiv:2306.01015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#22312;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#26102;&#30340;&#36801;&#31227;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20004;&#20010;&#34920;&#31034;&#29702;&#35770;&#65292;&#36890;&#36807;&#29983;&#25104;&#20505;&#36873;&#27169;&#22411;&#30340;&#25490;&#21517;&#20998;&#25968;&#65292;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#23454;&#38469;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#36801;&#31227;&#24615;&#20998;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#19982;&#24494;&#35843;&#22522;&#30784;&#20107;&#23454;&#20043;&#38388;&#23384;&#22312;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#21644;&#20302;&#30340;p&#20540;&#65292;&#26159;&#19968;&#20010;&#33410;&#30465;&#36164;&#28304;&#12289;&#39640;&#25928;&#33410;&#30465;&#26102;&#38388;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#22522;&#20110;&#20998;&#25968;&#35780;&#20272;&#8221;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20272;&#35745;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#65288;PSMs&#65289;&#22312;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#26102;&#30340;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#20004;&#20010;&#34920;&#31034;&#29702;&#35770;&#65292;&#36125;&#21494;&#26031;&#20284;&#28982;&#20272;&#35745;&#21644;&#26368;&#20248;&#20256;&#36755;&#65292;&#20351;&#29992;&#25552;&#21462;&#30340;&#34920;&#31034;&#29983;&#25104;PSM&#20505;&#36873;&#30340;&#25490;&#21517;&#20998;&#25968;&#12290;&#36890;&#36807;&#20551;&#35774;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#36801;&#31227;&#24615;&#20998;&#25968;&#65292;&#32780;&#26080;&#38656;&#23454;&#38469;&#24494;&#35843;&#20505;&#36873;&#27169;&#22411;&#25110;&#23618;&#12290;&#25105;&#20204;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#22312;&#20132;&#21449;&#23618;&#21644;&#20132;&#21449;&#27169;&#22411;&#35774;&#32622;&#20013;&#35780;&#20272;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#65288;&#20363;&#22914;Conformer RNN-Transducer&#65289;&#21644;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#65288;&#20363;&#22914;HuBERT&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#26694;&#26550;&#19982;&#24494;&#35843;&#22522;&#30784;&#20107;&#23454;&#20043;&#38388;&#23384;&#22312;&#24456;&#39640;&#30340;Spearman&#25490;&#21517;&#30456;&#20851;&#24615;&#21644;&#20302;&#30340;p&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#36801;&#31227;&#24615;&#26694;&#26550;&#38656;&#35201;&#36739;&#23569;&#30340;&#35745;&#31639;&#26102;&#38388;&#21644;&#36164;&#28304;&#65292;&#22240;&#27492;&#26159;&#19968;&#20010;&#33410;&#30465;&#36164;&#28304;&#12289;&#39640;&#25928;&#33410;&#30465;&#26102;&#38388;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a ``score-based assessment'' framework for estimating the transferability of pre-trained speech models (PSMs) for fine-tuning target tasks. We leverage upon two representation theories, Bayesian likelihood estimation and optimal transport, to generate rank scores for the PSM candidates using the extracted representations. Our framework efficiently computes transferability scores without actual fine-tuning of candidate models or layers by making a temporal independent hypothesis. We evaluate some popular supervised speech models (e.g., Conformer RNN-Transducer) and self-supervised speech models (e.g., HuBERT) in cross-layer and cross-model settings using public data. Experimental results show a high Spearman's rank correlation and low $p$-value between our estimation framework and fine-tuning ground truth. Our proposed transferability framework requires less computational time and resources, making it a resource-saving and time-efficient approach for tuning sp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19981;&#21516;&#35268;&#27169;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#25512;&#29702;&#33021;&#21147;&#22686;&#24378;&#65292;&#38271;&#24230;&#19981;&#20250;&#24433;&#21709;&#22823;&#37096;&#20998;&#27169;&#22411;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.01009</link><description>&lt;p&gt;
&#25506;&#31350;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Examining the Emergence of Deductive Reasoning in Generative Language Models. (arXiv:2306.01009v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#19981;&#21516;&#35268;&#27169;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#25512;&#29702;&#33021;&#21147;&#22686;&#24378;&#65292;&#38271;&#24230;&#19981;&#20250;&#24433;&#21709;&#22823;&#37096;&#20998;&#27169;&#22411;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#29983;&#25104;&#21464;&#21387;&#22120;&#27169;&#22411;&#20174;&#21069;&#25552;&#20013;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#30340;&#33021;&#21147;&#36827;&#34892;&#21021;&#27493;&#35843;&#26597;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#35757;&#32451;&#35774;&#32622;&#30340;&#27169;&#22411;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#28436;&#32462;&#25512;&#29702;&#33021;&#21147;&#38543;&#35268;&#27169;&#22686;&#21152;&#32780;&#22686;&#24378;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#38500;&#20102;OpenAI GPT-3&#21644;GPT-3.5&#27169;&#22411;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#36890;&#24120;&#19981;&#20250;&#38543;&#30528;&#25512;&#29702;&#38142;&#30340;&#38271;&#24230;&#32780;&#20943;&#24369;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20174;1.17&#20159;&#21040;1750&#20159;&#20010;&#21442;&#25968;&#30340;&#21508;&#31181;&#21464;&#21387;&#22120;&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We conduct a preliminary inquiry into the ability of generative transformer models to deductively reason from premises provided. We observe notable differences in the performance of models coming from different training setups and find that the deductive reasoning ability increases with scale. Further, we discover that the performance generally does not decrease with the length of the deductive chain needed to reach the conclusion, with the exception of OpenAI GPT-3 and GPT-3.5 models. Our study considers a wide variety of transformer-decoder models, ranging from 117 million to 175 billion parameters in size.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#22522;&#20110;&#35777;&#25454;&#30340;&#25945;&#23398;&#35774;&#35745;&#19987;&#19994;&#30693;&#35782;&#65292;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;GPT-4&#22312;&#25945;&#23398;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;LLMs&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#26410;&#26469;LLMs&#20026;&#25152;&#26377;&#23398;&#20064;&#32773;&#25552;&#20379;&#20010;&#24615;&#21270;&#25945;&#32946;&#20869;&#23481;&#30340;&#24895;&#26223;&#12290;</title><link>http://arxiv.org/abs/2306.01006</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#22522;&#20110;&#35777;&#25454;&#30340;&#25945;&#23398;&#35774;&#35745;&#19987;&#19994;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Scaling Evidence-based Instructional Design Expertise through Large Language Models. (arXiv:2306.01006v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#22522;&#20110;&#35777;&#25454;&#30340;&#25945;&#23398;&#35774;&#35745;&#19987;&#19994;&#30693;&#35782;&#65292;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;GPT-4&#22312;&#25945;&#23398;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;LLMs&#30340;&#26368;&#20339;&#23454;&#36341;&#21644;&#26410;&#26469;LLMs&#20026;&#25152;&#26377;&#23398;&#20064;&#32773;&#25552;&#20379;&#20010;&#24615;&#21270;&#25945;&#32946;&#20869;&#23481;&#30340;&#24895;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#26469;&#25299;&#23637;&#25945;&#23398;&#35774;&#35745;&#39046;&#22495;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#24357;&#21512;&#29702;&#35770;&#25945;&#32946;&#30740;&#31350;&#21644;&#23454;&#38469;&#23454;&#26045;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25506;&#35752;&#20102;AI&#39537;&#21160;&#20869;&#23481;&#29983;&#25104;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#24378;&#35843;&#20102;&#20154;&#24037;&#30417;&#30563;&#26469;&#30830;&#20445;&#25945;&#32946;&#26448;&#26009;&#30340;&#36136;&#37327;&#30340;&#24517;&#35201;&#24615;&#12290;&#36890;&#36807;&#20004;&#20010;&#35814;&#32454;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;GPT-4&#21019;&#24314;&#19981;&#21516;&#35838;&#31243;&#30340;&#22797;&#26434;&#39640;&#38454;&#35780;&#20272;&#21644;&#31215;&#26497;&#23398;&#20064;&#32452;&#20214;&#12290;&#20174;&#25105;&#20204;&#30340;&#32463;&#39564;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#25945;&#23398;&#35774;&#35745;&#20219;&#21153;&#20013;&#26377;&#25928;&#20351;&#29992;LLMs&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#22914;&#21033;&#29992;&#27169;&#26495;&#65292;&#24494;&#35843;&#65292;&#22788;&#29702;&#24847;&#22806;&#30340;&#36755;&#20986;&#65292;&#23454;&#29616;LLM&#38142;&#65292;&#24341;&#29992;&#21442;&#32771;&#25991;&#29486;&#65292;&#35780;&#20272;&#36755;&#20986;&#65292;&#21019;&#24314;&#35780;&#20998;&#26631;&#20934;&#21644;&#29983;&#25104;&#24178;&#25200;&#39033;&#12290;&#25105;&#20204;&#36824;&#20998;&#20139;&#20102;&#19968;&#20010;&#24895;&#26223;&#65292;&#26410;&#26469;LLMs&#21487;&#20197;&#20026;&#25152;&#26377;&#32972;&#26223;&#21644;&#27700;&#24179;&#30340;&#23398;&#20064;&#32773;&#25552;&#20379;&#26131;&#20110;&#25509;&#21463;&#21644;&#20010;&#24615;&#21270;&#30340;&#25945;&#32946;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive exploration of leveraging Large Language Models (LLMs), specifically GPT-4, in the field of instructional design. With a focus on scaling evidence-based instructional design expertise, our research aims to bridge the gap between theoretical educational studies and practical implementation. We discuss the benefits and limitations of AI-driven content generation, emphasizing the necessity of human oversight in ensuring the quality of educational materials. This work is elucidated through two detailed case studies where we applied GPT-4 in creating complex higher-order assessments and active learning components for different courses. From our experiences, we provide best practices for effectively using LLMs in instructional design tasks, such as utilizing templates, fine-tuning, handling unexpected output, implementing LLM chains, citing references, evaluating output, creating rubrics, grading, and generating distractors. We also share our vision of a f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#26041;&#38754;&#30340;&#26041;&#27861;(AoM)&#26469;&#26816;&#27979;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#35821;&#20041;&#21644;&#24773;&#24863;&#20449;&#24687;&#65292;&#24182;&#26126;&#30830;&#22320;&#23558;&#24773;&#24863;&#23884;&#20837;&#21040;AoM&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;AoM&#22312;&#25913;&#36827;&#22810;&#27169;&#24577;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;(MABSA)&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.01004</link><description>&lt;p&gt;
AoM&#65306;&#29992;&#20110;&#22810;&#27169;&#24577;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#26816;&#27979;&#38754;&#21521;&#26041;&#38754;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
AoM: Detecting Aspect-oriented Information for Multimodal Aspect-Based Sentiment Analysis. (arXiv:2306.01004v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#26041;&#38754;&#30340;&#26041;&#27861;(AoM)&#26469;&#26816;&#27979;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#35821;&#20041;&#21644;&#24773;&#24863;&#20449;&#24687;&#65292;&#24182;&#26126;&#30830;&#22320;&#23558;&#24773;&#24863;&#23884;&#20837;&#21040;AoM&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;AoM&#22312;&#25913;&#36827;&#22810;&#27169;&#24577;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;(MABSA)&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;(MABSA)&#26088;&#22312;&#20174;&#25991;&#26412;-&#22270;&#20687;&#23545;&#20013;&#25552;&#21462;&#26041;&#38754;&#24182;&#35782;&#21035;&#23427;&#20204;&#30340;&#24773;&#24863;&#12290;&#29616;&#26377;&#26041;&#27861;&#33268;&#21147;&#20110;&#23558;&#25972;&#20010;&#22270;&#20687;&#19982;&#30456;&#24212;&#30340;&#26041;&#38754;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#30340;&#19981;&#21516;&#21306;&#22495;&#21487;&#33021;&#19982;&#21516;&#19968;&#21477;&#23376;&#20013;&#30340;&#19981;&#21516;&#26041;&#38754;&#30456;&#20851;&#65292;&#31895;&#30053;&#22320;&#24314;&#31435;&#22270;&#20687;-&#26041;&#38754;&#23545;&#40784;&#20250;&#24341;&#20837;&#22122;&#22768;&#21040;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;(&#21363;&#35270;&#35273;&#22122;&#22768;)&#12290;&#27492;&#22806;&#65292;&#29305;&#23450;&#26041;&#38754;&#30340;&#24773;&#24863;&#20063;&#21487;&#33021;&#34987;&#20854;&#20182;&#26041;&#38754;&#30340;&#25551;&#36848;&#25152;&#24178;&#25200;(&#21363;&#25991;&#26412;&#22122;&#22768;)&#12290;&#32771;&#34385;&#21040;&#19978;&#36848;&#22122;&#22768;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26041;&#38754;&#30340;&#26041;&#27861;(AoM)&#26469;&#26816;&#27979;&#19982;&#26041;&#38754;&#30456;&#20851;&#30340;&#35821;&#20041;&#21644;&#24773;&#24863;&#20449;&#24687;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#38754;&#21521;&#26041;&#38754;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21516;&#26102;&#36873;&#25321;&#19982;&#26041;&#38754;&#26377;&#35821;&#20041;&#20851;&#32852;&#30340;&#25991;&#26412;&#20196;&#29260;&#21644;&#22270;&#20687;&#22359;&#12290;&#20026;&#20102;&#20934;&#30830;&#22320;&#32858;&#21512;&#24773;&#24863;&#20449;&#24687;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#23558;&#24773;&#24863;&#23884;&#20837;&#21040;AoM&#20013;&#65292;&#24182;&#20351;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#27169;&#24577;&#20013;&#25429;&#33719;&#20381;&#36182;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#34920;&#36798;&#24335;&#12290;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;AoM&#22312;&#25913;&#36827;MABSA&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal aspect-based sentiment analysis (MABSA) aims to extract aspects from text-image pairs and recognize their sentiments. Existing methods make great efforts to align the whole image to corresponding aspects. However, different regions of the image may relate to different aspects in the same sentence, and coarsely establishing image-aspect alignment will introduce noise to aspect-based sentiment analysis (i.e., visual noise). Besides, the sentiment of a specific aspect can also be interfered by descriptions of other aspects (i.e., textual noise). Considering the aforementioned noises, this paper proposes an Aspect-oriented Method (AoM) to detect aspect-relevant semantic and sentiment information. Specifically, an aspect-aware attention module is designed to simultaneously select textual tokens and image blocks that are semantically related to the aspects. To accurately aggregate sentiment information, we explicitly introduce sentiment embedding into AoM, and use a graph convolut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22823;&#35268;&#27169;&#25991;&#26412;&#36716;&#35821;&#38899;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#21512;&#25104;&#35821;&#38899;&#26679;&#26412;&#20316;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#30340;&#34917;&#20805;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#23558;&#20855;&#26377;&#30456;&#24403;&#24046;&#24322;&#30340;&#21512;&#25104;&#26679;&#26412;&#32435;&#20837;ASR&#35757;&#32451;&#23545;&#20110;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#27604;&#20854;&#20182;&#26041;&#27861;&#33021;&#22815;&#23558;TTS&#25968;&#25454;&#30340;&#22823;&#23567;&#20943;&#23569;&#33267;30%&#20197;&#19979;&#30340;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00998</link><description>&lt;p&gt;
&#25361;&#36873;&#25991;&#26412;&#36716;&#35821;&#38899;&#25968;&#25454;&#20197;&#22686;&#24378;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#35757;&#32451;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Selection of Text-to-speech Data to Augment ASR Training. (arXiv:2306.00998v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22823;&#35268;&#27169;&#25991;&#26412;&#36716;&#35821;&#38899;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#21512;&#25104;&#35821;&#38899;&#26679;&#26412;&#20316;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#30340;&#34917;&#20805;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#23558;&#20855;&#26377;&#30456;&#24403;&#24046;&#24322;&#30340;&#21512;&#25104;&#26679;&#26412;&#32435;&#20837;ASR&#35757;&#32451;&#23545;&#20110;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#27604;&#20854;&#20182;&#26041;&#27861;&#33021;&#22815;&#23558;TTS&#25968;&#25454;&#30340;&#22823;&#23567;&#20943;&#23569;&#33267;30%&#20197;&#19979;&#30340;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#30340;&#22823;&#35268;&#27169;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#21512;&#25104;&#35821;&#38899;&#26679;&#26412;&#65292;&#20316;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#30340;&#34917;&#20805;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#25110; Arcface &#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#65292;&#26469;&#34913;&#37327;&#21512;&#25104;&#25968;&#25454;&#19982;&#30495;&#23454;&#35821;&#38899;&#30340;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23558;&#20855;&#26377;&#30456;&#24403;&#24046;&#24322;&#30340;&#21512;&#25104;&#26679;&#26412;&#65288;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#35789;&#27719;&#24046;&#24322;&#65289;&#32435;&#20837; ASR &#35757;&#32451;&#23545;&#20110;&#25552;&#39640;&#35782;&#21035;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312; Librispeech &#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20026;&#20102;&#20445;&#25345;&#19982;&#20351;&#29992;&#25152;&#26377; TTS &#25968;&#25454;&#26102;&#30456;&#21516;&#30340;&#35821;&#38899;&#35782;&#21035;&#31934;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#23558; TTS &#25968;&#25454;&#30340;&#22823;&#23567;&#20943;&#23569;&#33267;&#20854; 30&#65285; &#20197;&#19979;&#65292;&#36825;&#27604;&#20960;&#31181;&#22522;&#20934;&#26041;&#27861;&#37117;&#35201;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a method for selecting appropriate synthetic speech samples from a given large text-to-speech (TTS) dataset as supplementary training data for an automatic speech recognition (ASR) model. We trained a neural network, which can be optimised using cross-entropy loss or Arcface loss, to measure the similarity of a synthetic data to real speech. We found that incorporating synthetic samples with considerable dissimilarity to real speech, owing in part to lexical differences, into ASR training is crucial for boosting recognition performance. Experimental results on Librispeech test sets indicate that, in order to maintain the same speech recognition accuracy as when using all TTS data, our proposed solution can reduce the size of the TTS data down below its $30\,\%$, which is superior to several baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#36716;&#25442;&#22120;&#23545;CTC&#27169;&#22411;&#30340;&#23545;&#40784;&#22270;&#26500;&#24314;&#36827;&#34892;&#25913;&#36827;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#22833;&#35821;&#24615;&#35328;&#35821;&#36827;&#34892;&#24378;&#21046;&#23545;&#40784;&#65292;&#20943;&#36731;&#20102;&#23545;&#36880;&#23383;&#36716;&#24405;&#30340;&#38656;&#27714;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.00996</link><description>&lt;p&gt;
&#21033;&#29992;&#38899;&#32032;&#32423;&#24314;&#27169;&#36827;&#34892;&#22833;&#35821;&#24615;&#35328;&#35821;&#24369;&#30417;&#30563;&#24378;&#21046;&#38901;&#24459;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Weakly-supervised forced alignment of disfluent speech using phoneme-level modeling. (arXiv:2306.00996v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#36716;&#25442;&#22120;&#23545;CTC&#27169;&#22411;&#30340;&#23545;&#40784;&#22270;&#26500;&#24314;&#36827;&#34892;&#25913;&#36827;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#22833;&#35821;&#24615;&#35328;&#35821;&#36827;&#34892;&#24378;&#21046;&#23545;&#40784;&#65292;&#20943;&#36731;&#20102;&#23545;&#36880;&#23383;&#36716;&#24405;&#30340;&#38656;&#27714;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#38556;&#30861;&#30340;&#30740;&#31350;&#21487;&#20197;&#21463;&#30410;&#20110;&#26102;&#38388;&#23545;&#40784;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22833;&#35821;&#24615;&#35328;&#35821;&#20013;&#30340;&#38899;&#39057;-&#25991;&#26412;&#19981;&#21305;&#37197;&#20250;&#23548;&#33268;&#29616;&#20195;&#35821;&#38899;&#23545;&#40784;&#22120;&#30340;&#24615;&#33021;&#24555;&#36895;&#19979;&#38477;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#33258;&#21160;&#26041;&#27861;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#21152;&#26435;&#26377;&#38480;&#29366;&#24577;&#36716;&#25442;&#22120;&#23545;CTC&#27169;&#22411;&#30340;&#23545;&#40784;&#22270;&#26500;&#24314;&#36827;&#34892;&#25913;&#36827;&#12290;&#35813;&#24369;&#30417;&#30563;&#26041;&#27861;&#20943;&#36731;&#20102;&#23545;&#22833;&#35821;&#24615;&#35328;&#35821;&#36880;&#23383;&#36716;&#24405;&#23454;&#29616;&#24378;&#21046;&#23545;&#40784;&#30340;&#38656;&#27714;&#12290;&#22312;&#22270;&#30340;&#26500;&#24314;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20801;&#35768;&#23545;&#24120;&#35265;&#30340;&#22833;&#35821;&#29616;&#35937;&#36827;&#34892;&#24314;&#27169;&#65292;&#21363;&#37325;&#22797;&#21644;&#30465;&#30053;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;Oracle&#38169;&#35823;&#29575;&#35780;&#20272;&#38899;&#39057;-&#25991;&#26412;&#19981;&#21305;&#37197;&#31243;&#24230;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#23545;TIMIT&#27979;&#35797;&#38598;&#21644;UCLASS&#25968;&#25454;&#38598;&#30340;&#25439;&#22351;&#29256;&#26412;&#36827;&#34892;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#21484;&#22238;&#29575;&#26041;&#38754;&#65292;&#22312;&#30456;&#23545;&#22522;&#32447;&#19978;&#23454;&#29616;&#20102;23-25%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of speech disorders can benefit greatly from time-aligned data. However, audio-text mismatches in disfluent speech cause rapid performance degradation for modern speech aligners, hindering the use of automatic approaches. In this work, we propose a simple and effective modification of alignment graph construction of CTC-based models using Weighted Finite State Transducers. The proposed weakly-supervised approach alleviates the need for verbatim transcription of speech disfluencies for forced alignment. During the graph construction, we allow the modeling of common speech disfluencies, i.e. repetitions and omissions. Further, we show that by assessing the degree of audio-text mismatch through the use of Oracle Error Rate, our method can be effectively used in the wild. Our evaluation on a corrupted version of the TIMIT test set and the UCLASS dataset shows significant improvements, particularly for recall, achieving a 23-25% relative improvement over our baselines.
&lt;/p&gt;</description></item><item><title>TopEx &#26159;&#19968;&#31181;&#22522;&#20110;&#26080;&#27169;&#22411;&#20027;&#39064;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#26356;&#20844;&#24179;&#65292;&#24182;&#21487;&#20197;&#22312;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#35782;&#21035;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.00976</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#30340;&#27169;&#22411;&#27604;&#36739;&#35299;&#37322;&#65306;TopEx
&lt;/p&gt;
&lt;p&gt;
TopEx: Topic-based Explanations for Model Comparison. (arXiv:2306.00976v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00976
&lt;/p&gt;
&lt;p&gt;
TopEx &#26159;&#19968;&#31181;&#22522;&#20110;&#26080;&#27169;&#22411;&#20027;&#39064;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#26356;&#20844;&#24179;&#65292;&#24182;&#21487;&#20197;&#22312;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#35782;&#21035;&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35299;&#37322;&#26041;&#27861;&#22312;&#22823;&#37327;&#35789;&#27719;&#25110;&#36328;&#27169;&#22411;&#27604;&#36739;&#26041;&#38754;&#36807;&#20110;&#24222;&#22823;&#65292;&#20351;&#20154;&#31867;&#38590;&#20197;&#26377;&#24847;&#20041;&#22320;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; TopEx &#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#22522;&#20110;&#26080;&#27169;&#22411;&#30340;&#20027;&#39064;&#23454;&#29616;&#27169;&#22411;&#20043;&#38388;&#30340;&#23545;&#27604;&#65292;&#20351;&#24471;&#27604;&#36739;&#26356;&#20844;&#24179;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102; TopEx &#22914;&#20309;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#35782;&#21035; DistilRoBERTa &#21644; GPT-2 &#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meaningfully comparing language models is challenging with current explanation methods. Current explanations are overwhelming for humans due to large vocabularies or incomparable across models. We present TopEx, an explanation method that enables a level playing field for comparing language models via model-agnostic topics. We demonstrate how TopEx can identify similarities and differences between DistilRoBERTa and GPT-2 on a variety of NLP tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#22312;&#22788;&#29702;&#38750;&#27954;&#21517;&#23383;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#22686;&#24378;&#31561;&#31574;&#30053;&#65292;&#36890;&#36807;&#24494;&#35843;ASR&#27169;&#22411;&#22312;&#22810;&#20010;&#38750;&#27954;&#21475;&#38899;&#19978;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#35823;&#24046;&#65292;&#30456;&#23545;WER&#25552;&#39640;&#20102;81.5&#65285;&#12290;</title><link>http://arxiv.org/abs/2306.00253</link><description>&lt;p&gt;
AfriNames&#65306;&#22823;&#22810;&#25968;ASR&#27169;&#22411;&#8220;&#23648;&#25134;&#8221;&#38750;&#27954;&#20154;&#30340;&#21517;&#23383;
&lt;/p&gt;
&lt;p&gt;
AfriNames: Most ASR models "butcher" African Names. (arXiv:2306.00253v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00253
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#22312;&#22788;&#29702;&#38750;&#27954;&#21517;&#23383;&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#22686;&#24378;&#31561;&#31574;&#30053;&#65292;&#36890;&#36807;&#24494;&#35843;ASR&#27169;&#22411;&#22312;&#22810;&#20010;&#38750;&#27954;&#21475;&#38899;&#19978;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#27169;&#22411;&#35823;&#24046;&#65292;&#30456;&#23545;WER&#25552;&#39640;&#20102;81.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#23545;&#35805;&#20195;&#29702;&#24517;&#39035;&#20934;&#30830;&#25429;&#25417;&#21629;&#21517;&#23454;&#20307;&#65292;&#20197;&#26368;&#23567;&#21270;&#19979;&#28216;&#20219;&#21153;&#30340;&#38169;&#35823;&#65292;&#20363;&#22914;&#65292;&#35201;&#27714;&#35821;&#38899;&#21161;&#25163;&#25773;&#25918;&#29305;&#23450;&#33402;&#26415;&#23478;&#30340;&#38899;&#36712;&#65292;&#21551;&#21160;&#23548;&#33322;&#21040;&#29305;&#23450;&#20301;&#32622;&#65292;&#25110;&#20026;&#24739;&#32773;&#35760;&#24405;&#23454;&#39564;&#23460;&#32467;&#26524;&#12290;&#20294;&#26159;&#65292;&#24403;&#20986;&#29616;&#35832;&#22914;&#8220;Ukachukwu&#8221;&#65288;&#20234;&#21338;&#35821;&#65289;&#12289;&#8220;Lakicia&#8221;&#65288;&#26031;&#29926;&#24076;&#37324;&#35821;&#65289;&#25110;&#8220;Ingabire&#8221;&#65288;&#21346;&#26106;&#36798;&#35821;&#65289;&#31561;&#21629;&#21517;&#23454;&#20307;&#26102;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#26174;&#33879;&#38477;&#20302;&#65292;&#23558;&#38169;&#35823;&#20256;&#36882;&#21040;&#19979;&#28216;&#31995;&#32479;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24314;&#27169;&#20026;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#28436;&#31034;&#20102;&#36890;&#36807;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#12289;&#26234;&#33021;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#26469;&#22686;&#21152;&#38750;&#27954;&#21629;&#21517;&#23454;&#20307;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#22810;&#20010;&#38750;&#27954;&#21475;&#38899;&#19978;&#24494;&#35843;&#22810;&#35821;&#35328;ASR&#27169;&#22411;&#21487;&#20197;&#20943;&#36731;&#27169;&#22411;&#20559;&#24046;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#21253;&#21547;&#38750;&#27954;&#21629;&#21517;&#23454;&#20307;&#30340;&#26679;&#26412;&#19978;&#30456;&#23545;WER&#25913;&#36827;&#20102;81.5&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Useful conversational agents must accurately capture named entities to minimize error for downstream tasks, for example, asking a voice assistant to play a track from a certain artist, initiating navigation to a specific location, or documenting a laboratory result for a patient. However, where named entities such as ``Ukachukwu`` (Igbo), ``Lakicia`` (Swahili), or ``Ingabire`` (Rwandan) are spoken, automatic speech recognition (ASR) models' performance degrades significantly, propagating errors to downstream systems. We model this problem as a distribution shift and demonstrate that such model bias can be mitigated through multilingual pre-training, intelligent data augmentation strategies to increase the representation of African-named entities, and fine-tuning multilingual ASR models on multiple African accents. The resulting fine-tuned models show an 81.5\% relative WER improvement compared with the baseline on samples with African-named entities.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;UKP-SQuARE&#20316;&#20026;&#19968;&#20010;&#29992;&#20110;&#25945;&#25480;&#38382;&#31572;&#25216;&#26415;&#30340;&#20132;&#20114;&#24335;&#24037;&#20855;&#65292;&#23398;&#29983;&#21487;&#20197;&#36890;&#36807;&#20854;&#22312;&#19981;&#21516;&#35282;&#24230;&#20102;&#35299;&#21508;&#31181;QA&#27169;&#22411;&#65292;&#24182;&#20511;&#27492;&#33719;&#24471;&#29702;&#35770;&#27010;&#24565;&#21644;&#38382;&#39064;&#35299;&#20915;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.19748</link><description>&lt;p&gt;
UKP-SQuARE: &#19968;&#20010;&#29992;&#20110;&#25945;&#25480;&#38382;&#31572;&#25216;&#26415;&#30340;&#20132;&#20114;&#24335;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
UKP-SQuARE: An Interactive Tool for Teaching Question Answering. (arXiv:2305.19748v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19748
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;UKP-SQuARE&#20316;&#20026;&#19968;&#20010;&#29992;&#20110;&#25945;&#25480;&#38382;&#31572;&#25216;&#26415;&#30340;&#20132;&#20114;&#24335;&#24037;&#20855;&#65292;&#23398;&#29983;&#21487;&#20197;&#36890;&#36807;&#20854;&#22312;&#19981;&#21516;&#35282;&#24230;&#20102;&#35299;&#21508;&#31181;QA&#27169;&#22411;&#65292;&#24182;&#20511;&#27492;&#33719;&#24471;&#29702;&#35770;&#27010;&#24565;&#21644;&#38382;&#39064;&#35299;&#20915;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#25216;&#26415;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#20351;&#20854;&#25104;&#20026;&#20219;&#20309;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#35838;&#31243;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#35805;&#39064;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#25351;&#25968;&#32423;&#22686;&#38271;&#25152;&#23548;&#33268;&#30340;&#38382;&#31572;&#24191;&#24230;&#20351;&#20854;&#25104;&#20026;&#25945;&#25480;&#30456;&#20851;NLP&#20027;&#39064;&#65288;&#22914;&#20449;&#24687;&#26816;&#32034;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#23545;&#25239;&#25915;&#20987;&#31561;&#65289;&#30340;&#29702;&#24819;&#22330;&#26223;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;UKP-SQuARE&#20316;&#20026;QA&#25945;&#32946;&#24179;&#21488;&#12290;&#35813;&#24179;&#21488;&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#29615;&#22659;&#65292;&#22312;&#36825;&#37324;&#23398;&#29983;&#21487;&#20197;&#36816;&#34892;&#12289;&#27604;&#36739;&#21644;&#20998;&#26512;&#19981;&#21516;&#35282;&#24230;&#30340;&#21508;&#31181;QA&#27169;&#22411;&#65292;&#22914;&#19968;&#33324;&#34892;&#20026;&#65292;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#22240;&#27492;&#65292;&#23398;&#29983;&#21487;&#20197;&#22312;&#35838;&#22530;&#19978;&#20146;&#36523;&#20307;&#39564;&#19981;&#21516;&#30340;QA&#25216;&#26415;&#12290;&#30001;&#20110;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#23398;&#29983;&#20026;&#20013;&#24515;&#30340;QA&#25945;&#32946;&#26041;&#27861;&#65292;&#23398;&#29983;&#21487;&#20197;&#36890;&#36807;&#20132;&#20114;&#24335;&#30340;&#25506;&#32034;&#12289;&#23454;&#39564;&#21644;&#23454;&#36341;&#20219;&#21153;&#20027;&#21160;&#23398;&#20064;&#29702;&#35770;&#27010;&#24565;&#24182;&#33719;&#24471;&#38382;&#39064;&#35299;&#20915;&#25216;&#33021;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#36182;&#20256;&#32479;&#30340;&#35762;&#25480;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth of question answering (QA) has made it an indispensable topic in any Natural Language Processing (NLP) course. Additionally, the breadth of QA derived from this exponential growth makes it an ideal scenario for teaching related NLP topics such as information retrieval, explainability, and adversarial attacks among others. In this paper, we introduce UKP-SQuARE as a platform for QA education. This platform provides an interactive environment where students can run, compare, and analyze various QA models from different perspectives, such as general behavior, explainability, and robustness. Therefore, students can get a first-hand experience in different QA techniques during the class. Thanks to this, we propose a learner-centered approach for QA education in which students proactively learn theoretical concepts and acquire problem-solving skills through interactive exploration, experimentation, and practical assignments, rather than solely relying on traditional le
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#32467;&#21512;&#20154;&#31867;&#29983;&#25104;&#31572;&#26696;&#30340;&#24605;&#36335;&#26469;&#29983;&#25104;&#25509;&#36817;&#20154;&#31867;&#34892;&#20026;&#21644;&#20107;&#23454;&#30340;&#31572;&#26696;&#65292;&#24182;&#25506;&#35752;&#20851;&#38190;&#35789;&#23545;Q/A&#20219;&#21153;&#35299;&#30721;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.18679</link><description>&lt;p&gt;
&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#37319;&#26679;&#65288;KEYS&#65289;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
KEYword based Sampling (KEYS) for Large Language Models. (arXiv:2305.18679v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18679
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#32467;&#21512;&#20154;&#31867;&#29983;&#25104;&#31572;&#26696;&#30340;&#24605;&#36335;&#26469;&#29983;&#25104;&#25509;&#36817;&#20154;&#31867;&#34892;&#20026;&#21644;&#20107;&#23454;&#30340;&#31572;&#26696;&#65292;&#24182;&#25506;&#35752;&#20851;&#38190;&#35789;&#23545;Q/A&#20219;&#21153;&#35299;&#30721;&#31639;&#27861;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#65288;Q/A&#65289;&#20219;&#21153;&#21487;&#20197;&#34987;&#30475;&#20316;&#19968;&#20010;&#29983;&#25104;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#38382;&#39064;&#21644;&#25991;&#31456;&#65288;&#22914;&#26524;&#26377;&#65289;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#31572;&#26696;&#12290;&#26368;&#36817;Q/A&#20219;&#21153;&#30340;&#36827;&#23637;&#20027;&#35201;&#20851;&#27880;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#32780;&#24456;&#23569;&#20851;&#27880;&#20854;&#20182;&#39046;&#22495;&#65292;&#22914;&#37319;&#26679;&#12290;&#20851;&#38190;&#35789;&#22312;&#20154;&#31867;&#35821;&#35328;&#29983;&#25104;&#20013;&#36215;&#30528;&#38750;&#24120;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#22914;&#20309;&#32467;&#21512;&#20154;&#31867;&#29983;&#25104;&#31572;&#26696;&#30340;&#34892;&#20026;&#26469;&#29983;&#25104;&#25509;&#36817;&#20154;&#31867;&#34892;&#20026;&#19988;&#20107;&#23454;&#27491;&#30830;&#30340;&#31572;&#26696;&#65292;&#24182;&#35752;&#35770;&#20851;&#38190;&#35789;&#22914;&#20309;&#24433;&#21709;Q/A&#20219;&#21153;&#30340;&#35299;&#30721;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering (Q/A) can be formulated as a generative task (Mitra, 2017) where the task is to generate an answer given the question and the passage (knowledge, if available). Recent advances in QA task is focused a lot on language model advancements and less on other areas such as sampling(Krishna et al., 2021), (Nakano et al., 2021). Keywords play very important role for humans in language generation. (Humans formulate keywords and use grammar to connect those keywords and work). In the research community, very little focus is on how humans generate answers to a question and how this behavior can be incorporated in a language model. In this paper, we want to explore these two areas combined, i.e., how sampling can be to used generate answers which are close to human-like behavior and factually correct. Hence, the type of decoding algorithm we think should be used for Q/A tasks should also depend on the keywords. These keywords can be obtained from the question, passage or interne
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#32452;&#21512;&#22411;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20854;&#36890;&#36807;&#32447;&#24615;&#23376;&#22270;&#21305;&#37197;&#35299;&#20915;&#22810;&#27493;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18654</link><description>&lt;p&gt;
&#20449;&#20208;&#19982;&#21629;&#36816;&#65306;Transformer&#22312;&#32452;&#21512;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Faith and Fate: Limits of Transformers on Compositionality. (arXiv:2305.18654v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18654
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;Transformer&#27169;&#22411;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#32452;&#21512;&#22411;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20854;&#36890;&#36807;&#32447;&#24615;&#23376;&#22270;&#21305;&#37197;&#35299;&#20915;&#22810;&#27493;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#22797;&#26434;&#22810;&#27493;&#25512;&#29702;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#21331;&#36234;&#65292;&#20294;&#21516;&#26102;&#22312;&#19968;&#20123;&#31616;&#21333;&#38382;&#39064;&#19978;&#20063;&#20250;&#20986;&#29616;&#22833;&#36133;&#12290;&#36825;&#24341;&#21457;&#20102;&#30097;&#38382;&#65306;&#36825;&#20123;&#38169;&#35823;&#26159;&#20598;&#28982;&#30340;&#65292;&#36824;&#26159;&#23427;&#20204;&#34920;&#26126;&#20102;&#26356;&#23454;&#36136;&#24615;&#30340;&#38480;&#21046;&#65311;&#20026;&#20102;&#25581;&#31034;Transformer&#30340;&#31070;&#31192;&#38754;&#32433;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#32452;&#21512;&#22411;&#20219;&#21153;&#20013;&#30340;&#26497;&#38480; - &#22810;&#20301;&#25968;&#20056;&#27861;&#12289;&#36923;&#36753;&#32593;&#26684;&#35868;&#39064;&#21644;&#19968;&#20010;&#32463;&#20856;&#30340;&#21160;&#24577;&#35268;&#21010;&#38382;&#39064;&#12290; &#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#27493;&#39588;&#65292;&#24182;&#23558;&#36825;&#20123;&#27493;&#39588;&#32508;&#21512;&#25104;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#23558;&#32452;&#21512;&#22411;&#20219;&#21153;&#36716;&#21270;&#20026;&#35745;&#31639;&#22270;&#65292;&#20197;&#31995;&#32479;&#22320;&#37327;&#21270;&#20854;&#22797;&#26434;&#24615;&#65292;&#24182;&#23558;&#25512;&#29702;&#27493;&#39588;&#20998;&#35299;&#20026;&#20013;&#38388;&#23376;&#31243;&#24207;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#36890;&#36807;&#23558;&#22810;&#27493;&#32452;&#21512;&#25512;&#29702;&#36716;&#21270;&#20026;&#32447;&#24615;&#23376;&#22270;&#21305;&#37197;&#26469;&#35299;&#20915;&#32452;&#21512;&#22411;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify Transformers, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that Transformers solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22810;&#23610;&#24230;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#30701;&#26426;&#22120;&#25991;&#26412;&#26631;&#35760;&#20026;&#8220;&#26410;&#26631;&#35760;&#8221;&#26469;&#37325;&#26032;&#34920;&#36848;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#26816;&#27979;&#24615;&#33021;&#65292;&#26377;&#25928;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18149</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Multiscale Positive-Unlabeled Detection of AI-Generated Texts. (arXiv:2305.18149v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22810;&#23610;&#24230;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#30701;&#26426;&#22120;&#25991;&#26412;&#26631;&#35760;&#20026;&#8220;&#26410;&#26631;&#35760;&#8221;&#26469;&#37325;&#26032;&#34920;&#36848;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#26816;&#27979;&#24615;&#33021;&#65292;&#26377;&#25928;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#24067;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#31561;&#22312;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#25991;&#26412;&#26041;&#38754;&#20196;&#20154;&#24778;&#35766;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#34987;&#29992;&#20110;&#21046;&#36896;&#34394;&#20551;&#30340;&#23398;&#26415;&#25991;&#26412;&#12289;&#34394;&#20551;&#26032;&#38395;&#12289;&#34394;&#20551;&#25512;&#29305;&#31561;&#12290;&#20808;&#21069;&#30340;&#20316;&#21697;&#25552;&#20986;&#20102;&#26816;&#27979;&#36825;&#20123;&#22810;&#23610;&#24230;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;ML&#20998;&#31867;&#22120;&#12289;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#21487;&#30693;&#26041;&#27861;&#21644;&#31934;&#35843;&#30340;&#35821;&#35328;&#20998;&#31867;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20027;&#27969;&#26816;&#27979;&#22120;&#22312;&#26500;&#24314;&#26102;&#27809;&#26377;&#32771;&#34385;&#21040;&#25991;&#26412;&#38271;&#24230;&#30340;&#22240;&#32032;&#65306;&#30701;&#25991;&#26412;&#30340;&#32570;&#20047;&#20449;&#24687;&#29305;&#24449;&#65292;&#20351;&#20854;&#26356;&#38590;&#26816;&#27979;&#12290;&#38024;&#23545;&#22810;&#23610;&#24230;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#65288;MPU&#65289;&#35757;&#32451;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25215;&#35748;&#30701;&#30340;&#26426;&#22120;&#25991;&#26412;&#20855;&#26377;&#31867;&#20154;&#23646;&#24615;&#65292;&#24182;&#23558;&#25991;&#26412;&#20998;&#31867;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#27491;&#36127;&#26679;&#26412;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#26631;&#35760;&#36825;&#20123;&#30701;&#30340;&#26426;&#22120;&#25991;&#26412;&#20026;"unlabeled"&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#27491;&#36127;&#26679;&#26412;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may get misused for fake scholarly texts, fake news, fake tweets, et cetera. Previous works have proposed methods to detect these multiscale AI-generated texts, including simple ML classifiers, pretrained-model-based training-agnostic methods, and finetuned language classification models. However, mainstream detectors are formulated without considering the factor of corpus length: shorter corpuses are harder to detect compared with longer ones for shortage of informative features. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the challenge of multiscale text detection. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase text classification as a Positive-Unlabeled (PU) problem by marking these short machine texts as "unlabeled" during training. In this PU context, we propose the le
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#27169;&#24577;&#30721;&#26412;&#30340;&#25991;&#26412;&#22270;&#29255;&#32763;&#35793;&#27169;&#22411;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22810;&#38454;&#27573;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#39069;&#22806;&#30340;&#21452;&#35821;&#25991;&#26412;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#22270;&#20687;&#19982;&#30456;&#20851;&#25991;&#26412;&#20851;&#32852;&#36215;&#26469;&#65292;&#25552;&#20379;&#26377;&#29992;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.17415</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#30721;&#26412;&#30340;&#25991;&#26412;&#22270;&#29255;&#32763;&#35793;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploring Better Text Image Translation with Multimodal Codebook. (arXiv:2305.17415v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17415
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#27169;&#24577;&#30721;&#26412;&#30340;&#25991;&#26412;&#22270;&#29255;&#32763;&#35793;&#27169;&#22411;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#22810;&#38454;&#27573;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#39069;&#22806;&#30340;&#21452;&#35821;&#25991;&#26412;&#21644;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#22270;&#20687;&#19982;&#30456;&#20851;&#25991;&#26412;&#20851;&#32852;&#36215;&#26469;&#65292;&#25552;&#20379;&#26377;&#29992;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#21462;&#24471;&#20102;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22270;&#29255;&#32763;&#35793;&#26159;&#23558;&#22270;&#20687;&#20013;&#23884;&#20837;&#30340;&#21407;&#22987;&#25991;&#26412;&#32763;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#30340;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#24182;&#20855;&#26377;&#37325;&#35201;&#30340;&#30740;&#31350;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#25991;&#26412;&#22270;&#29255;&#32763;&#35793;&#30740;&#31350;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#29942;&#39048;&#65306;1&#65289;&#32570;&#23569;&#20844;&#24320;&#30340;&#25991;&#26412;&#22270;&#29255;&#32763;&#35793;&#25968;&#25454;&#38598;&#65307;2&#65289;&#20027;&#27969;&#27169;&#22411;&#37319;&#29992;&#32423;&#32852;&#27169;&#24335;&#26500;&#24314;&#65292;&#23481;&#26131;&#21463;&#21040;&#20809;&#23383;&#31526;&#35782;&#21035;&#38169;&#35823;&#20256;&#25773;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#39318;&#20808;&#27880;&#37322;&#20102;&#19968;&#20010;&#21517;&#20026;OCRMT30K&#30340;&#20013;&#33521;&#25991;&#25991;&#26412;&#22270;&#29255;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#20026;&#21518;&#32493;&#30740;&#31350;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#30721;&#26412;&#30340;&#25991;&#26412;&#22270;&#29255;&#32763;&#35793;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#22270;&#20687;&#19982;&#30456;&#20851;&#25991;&#26412;&#20851;&#32852;&#36215;&#26469;&#65292;&#25552;&#20379;&#26377;&#29992;&#30340;&#34917;&#20805;&#20449;&#24687;&#36827;&#34892;&#32763;&#35793;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#35757;&#32451;&#26694;&#26550;&#65292;&#21253;&#25324;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#12289;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#21644;&#25991;&#26412;&#22270;&#29255;&#32763;&#35793;&#20219;&#21153;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#39069;&#22806;&#30340;&#21452;&#35821;&#25991;&#26412;&#12289;&#20809;&#23383;&#31526;&#35782;&#21035;&#25968;&#25454;&#38598;&#21644;OCRMT30K&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25193;&#23637;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#36798;&#21040;&#20102;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text image translation (TIT) aims to translate the source texts embedded in the image to target translations, which has a wide range of applications and thus has important research value. However, current studies on TIT are confronted with two main bottlenecks: 1) this task lacks a publicly available TIT dataset, 2) dominant models are constructed in a cascaded manner, which tends to suffer from the error propagation of optical character recognition (OCR). In this work, we first annotate a Chinese-English TIT dataset named OCRMT30K, providing convenience for subsequent studies. Then, we propose a TIT model with a multimodal codebook, which is able to associate the image with relevant texts, providing useful supplementary information for translation. Moreover, we present a multi-stage training framework involving text machine translation, image-text alignment, and TIT tasks, which fully exploits additional bilingual texts, OCR dataset and our OCRMT30K dataset to train our model. Extensi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#21644;&#25197;&#30697;&#30340;&#25163;&#35821;&#35270;&#39057;&#25688;&#35201;&#25216;&#26415;&#65292;&#33021;&#22815;&#36873;&#20986;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#20851;&#38190;&#24103;&#12290;</title><link>http://arxiv.org/abs/2305.16801</link><description>&lt;p&gt;
&#22522;&#20110;&#26354;&#29575;&#21644;&#25197;&#30697;&#30340;&#25163;&#35821;&#35270;&#39057;&#25688;&#35201;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Motion-Based Sign Language Video Summarization using Curvature and Torsion. (arXiv:2305.16801v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16801
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#21644;&#25197;&#30697;&#30340;&#25163;&#35821;&#35270;&#39057;&#25688;&#35201;&#25216;&#26415;&#65292;&#33021;&#22815;&#36873;&#20986;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#20851;&#38190;&#24103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25688;&#35201;&#25216;&#26415;&#22312;&#24456;&#22810;&#22522;&#20110;&#35270;&#39057;&#30340;&#24212;&#29992;&#20013;&#37117;&#38750;&#24120;&#26377;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25163;&#35821;&#35270;&#39057;&#25688;&#35201;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#20174;&#35270;&#39057;&#24103;&#20013;&#25552;&#21462;&#30340;&#19977;&#32500;&#25163;&#37096;&#36816;&#21160;&#25968;&#25454;&#26469;&#27169;&#22411;&#21270;&#27599;&#19968;&#24103;&#20013;&#30340;&#19977;&#32500;&#36816;&#21160;&#12290;&#22522;&#20110;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26354;&#29575;&#21644;&#25197;&#30697;&#30340;&#26032;&#22411;&#20449;&#24687;&#20989;&#25968;&#65292;&#20197;&#20415;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#20851;&#38190;&#24103;&#12290;
&lt;/p&gt;
&lt;p&gt;
An interesting problem in many video-based applications is the generation of short synopses by selecting the most informative frames, a procedure which is known as video summarization. For sign language videos the benefits of using the $t$-parameterized counterpart of the curvature of the 2-D signer's wrist trajectory to identify keyframes, have been recently reported in the literature. In this paper we extend these ideas by modeling the 3-D hand motion that is extracted from each frame of the video. To this end we propose a new informative function based on the $t$-parameterized curvature and torsion of the 3-D trajectory. The method to characterize video frames as keyframes depends on whether the motion occurs in 2-D or 3-D space. Specifically, in the case of 3-D motion we look for the maxima of the harmonic mean of the curvature and torsion of the target's trajectory; in the planar motion case we seek for the maxima of the trajectory's curvature. The proposed 3-D feature is experime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31616;&#35201;&#27010;&#36848;&#20102;&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#21253;&#25324;&#25991;&#26723;&#20998;&#31867;&#21644;&#25688;&#35201;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;&#38271;&#25991;&#26412;NLP&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.16259</link><description>&lt;p&gt;
&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65306;&#29616;&#29366;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art. (arXiv:2305.16259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31616;&#35201;&#27010;&#36848;&#20102;&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#21253;&#25324;&#25991;&#26723;&#20998;&#31867;&#21644;&#25688;&#35201;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;&#38271;&#25991;&#26412;NLP&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#37319;&#29992;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#38271;&#25991;&#26412;&#20998;&#26512;&#30340;&#38656;&#27714;&#19982;&#30701;&#25991;&#26412;&#26377;&#24456;&#22823;&#19981;&#21516;&#65292;&#32780;&#32593;&#32476;&#19978;&#20256;&#36755;&#30340;&#25991;&#26723;&#22823;&#23567;&#19981;&#26029;&#22686;&#21152;&#65292;&#20351;&#38271;&#25991;&#26412;&#30340;&#33258;&#21160;&#29702;&#35299;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#30340;&#20004;&#20010;&#30446;&#26631;&#26159;&#65306;a&#65289;&#27010;&#36848;&#30456;&#20851;&#30340;&#31070;&#32463;&#26500;&#24314;&#27169;&#22359;&#65292;&#20316;&#20026;&#30701;&#25945;&#31243;&#65307;b&#65289;&#24635;&#32467;&#38271;&#25991;&#26412;NLP&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#20851;&#27880;&#20004;&#20010;&#26680;&#24515;&#20219;&#21153;&#65306;&#25991;&#26723;&#20998;&#31867;&#21644;&#25991;&#26723;&#25688;&#35201;&#12290;&#24773;&#24863;&#20998;&#26512;&#20063;&#28085;&#30422;&#22312;&#20869;&#65292;&#22240;&#20026;&#23427;&#36890;&#24120;&#34987;&#35270;&#20026;&#25991;&#26723;&#20998;&#31867;&#30340;&#29305;&#20363;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#38271;&#25991;&#26412;NLP&#30456;&#20851;&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#20844;&#24320;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#20419;&#36827;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural Language Processing (NLP) during the past decade. However, the demands of long document analysis are quite different from those of shorter texts, while the ever increasing size of documents uploaded on-line renders automated understanding of long texts a critical area of research. This article has two goals: a) it overviews the relevant neural building blocks, thus serving as a short tutorial, and b) it surveys the state-of-the-art in long document NLP, mainly focusing on two central tasks: document classification and document summarization. Sentiment analysis for long texts is also covered, since it is typically treated as a particular case of document classification. Additionally, this article discusses the main challenges, issues and current solutions related to long document NLP. Finally, the relevant, publicly available, annotated datasets are presented, in order to facilitate further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#32553;&#25918;&#23545;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;(NAT)&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#32553;&#25918;&#21487;&#20197;&#25552;&#39640;NAT&#30340;&#32763;&#35793;&#24615;&#33021;&#12290;&#36890;&#36807;&#24322;&#26500;&#20307;&#31995;&#32467;&#26500;&#21487;&#20197;&#23454;&#29616;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#26631;&#20934;NAT&#27169;&#22411;&#30340;&#39640;&#35299;&#30721;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.16155</link><description>&lt;p&gt;
&#22312;&#35268;&#27169;&#19978;&#37325;&#26032;&#23457;&#35270;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Revisiting Non-Autoregressive Translation at Scale. (arXiv:2305.16155v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#30740;&#31350;&#20102;&#32553;&#25918;&#23545;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;(NAT)&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#32553;&#25918;&#21487;&#20197;&#25552;&#39640;NAT&#30340;&#32763;&#35793;&#24615;&#33021;&#12290;&#36890;&#36807;&#24322;&#26500;&#20307;&#31995;&#32467;&#26500;&#21487;&#20197;&#23454;&#29616;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#26631;&#20934;NAT&#27169;&#22411;&#30340;&#39640;&#35299;&#30721;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#31995;&#32479;&#20013;&#65292;&#32553;&#25918;&#26159;&#25552;&#39640;&#33258;&#22238;&#24402;&#32763;&#35793;(AT)&#36136;&#37327;&#30340;&#20851;&#38190;&#65292;&#20294;&#23545;&#20110;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;(NAT)&#23578;&#26410;&#26377;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#32553;&#25918;&#23545;NAT&#34892;&#20026;&#30340;&#24433;&#21709;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#22312;&#20004;&#20010;&#20808;&#36827;&#30340;NAT&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#20845;&#20010;WMT&#22522;&#20934;&#27979;&#35797;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#32553;&#25918;&#21487;&#20197;&#32531;&#35299;NAT&#27169;&#22411;&#30340;&#24120;&#35265;&#32570;&#38519;&#65292;&#20174;&#32780;&#25552;&#39640;&#32763;&#35793;&#24615;&#33021;&#12290;&#20026;&#20102;&#20943;&#23569;&#32553;&#25918;&#23545;&#35299;&#30721;&#36895;&#24230;&#30340;&#21103;&#20316;&#29992;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#20102;NAT&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23545;&#32763;&#35793;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22312;&#22823;&#35268;&#27169;&#30340;WMT20 En-De&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24322;&#26500;&#20307;&#31995;&#32467;&#26500;(&#20363;&#22914;&#26356;&#22823;&#30340;&#32534;&#30721;&#22120;&#21644;&#36739;&#23567;&#30340;&#35299;&#30721;&#22120;)&#21487;&#20197;&#23454;&#29616;&#19982;&#32553;&#25918;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#26631;&#20934;NAT&#27169;&#22411;&#30340;&#20248;&#36234;&#35299;&#30721;&#36895;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#39564;&#35777;&#32553;&#25918;NAT&#27169;&#22411;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world systems, scaling has been critical for improving the translation quality in autoregressive translation (AT), which however has not been well studied for non-autoregressive translation (NAT). In this work, we bridge the gap by systematically studying the impact of scaling on NAT behaviors. Extensive experiments on six WMT benchmarks over two advanced NAT models show that scaling can alleviate the commonly-cited weaknesses of NAT models, resulting in better translation performance. To reduce the side-effect of scaling on decoding speed, we empirically investigate the impact of NAT encoder and decoder on the translation performance. Experimental results on the large-scale WMT20 En-De show that the asymmetric architecture (e.g. bigger encoder and smaller decoder) can achieve comparable performance with the scaling model, while maintaining the superiority of decoding speed with standard NAT models. To this end, we establish a new benchmark by validating scaled NAT models on th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#30340;&#19981;&#30495;&#23454;&#22238;&#31572;&#29616;&#35937;&#65292;&#21457;&#29616;GPT-3&#27169;&#22411;&#23545;&#32473;&#23450;&#25552;&#31034;&#30340;&#22238;&#31572;&#22312;&#35821;&#35328;&#29305;&#24615;&#19978;&#24456;&#30456;&#20284;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#35780;&#20272;&#20869;&#23481;&#26412;&#36523;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20381;&#36182;&#27169;&#22411;&#22238;&#31572;&#30340;&#39118;&#26684;&#25104;&#20998;&#21363;&#21487;&#20998;&#31867;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15875</link><description>&lt;p&gt;
&#30495;&#23454;&#22238;&#31572;&#30340;&#35821;&#35328;&#29305;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Linguistic Properties of Truthful Response. (arXiv:2305.15875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15875
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#30340;&#19981;&#30495;&#23454;&#22238;&#31572;&#29616;&#35937;&#65292;&#21457;&#29616;GPT-3&#27169;&#22411;&#23545;&#32473;&#23450;&#25552;&#31034;&#30340;&#22238;&#31572;&#22312;&#35821;&#35328;&#29305;&#24615;&#19978;&#24456;&#30456;&#20284;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#35780;&#20272;&#20869;&#23481;&#26412;&#36523;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20381;&#36182;&#27169;&#22411;&#22238;&#31572;&#30340;&#39118;&#26684;&#25104;&#20998;&#21363;&#21487;&#20998;&#31867;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;220&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#35821;&#35328;&#29305;&#24615;&#23545;LLM&#19981;&#30495;&#23454;&#22238;&#31572;&#30340;&#29616;&#35937;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;GPT-3&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#22823;&#23567;&#30340;LLM&#23545;&#32473;&#23450;&#25552;&#31034;&#30340;&#22238;&#31572;&#22312;&#35821;&#35328;&#29305;&#24615;&#19978;&#24456;&#30456;&#20284;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#21482;&#20381;&#36182;&#27169;&#22411;&#22238;&#31572;&#30340;&#39118;&#26684;&#25104;&#20998;&#26469;&#20998;&#31867;&#38472;&#36848;&#30495;&#23454;&#24615;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#25193;&#23637;&#20102;&#36825;&#19968;&#21457;&#29616;&#12290;&#34429;&#28982;&#25968;&#25454;&#38598;&#22823;&#23567;&#38480;&#21046;&#20102;&#25105;&#20204;&#30340;&#24403;&#21069;&#30740;&#31350;&#25104;&#26524;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#21487;&#20197;&#22312;&#19981;&#35780;&#20272;&#20869;&#23481;&#26412;&#36523;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the phenomenon of an LLM's untruthful response using a large set of 220 handcrafted linguistic features. We focus on GPT-3 models and find that the linguistic profiles of responses are similar across model sizes. That is, how varying-sized LLMs respond to given prompts stays similar on the linguistic properties level. We expand upon this finding by training support vector machines that rely only upon the stylistic components of model responses to classify the truthfulness of statements. Though the dataset size limits our current findings, we present promising evidence that truthfulness detection is possible without evaluating the content itself.
&lt;/p&gt;</description></item><item><title>SenteCon&#26159;&#19968;&#31181;&#33021;&#22815;&#25552;&#20379;&#39640;&#32423;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25226;&#25991;&#26412;&#32534;&#30721;&#20026;&#21487;&#35299;&#37322;&#31867;&#21035;&#30340;&#23618;&#65292;&#21516;&#26102;&#19981;&#20250;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#39044;&#27979;&#24615;&#33021;&#36896;&#25104;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.14728</link><description>&lt;p&gt;
SenteCon: &#21033;&#29992;&#35789;&#27719;&#34920;&#23398;&#20064;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#35821;&#35328;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations. (arXiv:2305.14728v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14728
&lt;/p&gt;
&lt;p&gt;
SenteCon&#26159;&#19968;&#31181;&#33021;&#22815;&#25552;&#20379;&#39640;&#32423;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25226;&#25991;&#26412;&#32534;&#30721;&#20026;&#21487;&#35299;&#37322;&#31867;&#21035;&#30340;&#23618;&#65292;&#21516;&#26102;&#19981;&#20250;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#39044;&#27979;&#24615;&#33021;&#36896;&#25104;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#35821;&#35328;&#34920;&#31034;&#24050;&#25104;&#20026;&#35821;&#35328;&#29305;&#24449;&#21270;&#30340;&#20027;&#35201;&#24418;&#24335;&#65292;&#20294;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20102;&#35299;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#36825;&#19981;&#20165;&#38656;&#35201;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#36824;&#38656;&#35201;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#12290;&#29305;&#21035;&#26159;&#65292;&#35821;&#35328;&#24517;&#39035;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#29305;&#24449;&#21270;&#65292;&#21516;&#26102;&#20173;&#28982;&#24456;&#22909;&#22320;&#25551;&#36848;&#21407;&#22987;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SenteCon&#65292;&#19968;&#31181;&#22312;&#28145;&#24230;&#35821;&#35328;&#34920;&#31034;&#20013;&#24341;&#20837;&#20154;&#31867;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#25991;&#26412;&#27573;&#33853;&#65292;SenteCon&#23558;&#25991;&#26412;&#32534;&#30721;&#20026;&#21487;&#35299;&#37322;&#31867;&#21035;&#30340;&#23618;&#65292;&#20854;&#20013;&#27599;&#20010;&#32500;&#24230;&#23545;&#24212;&#20110;&#29305;&#23450;&#31867;&#21035;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#20351;&#29992;SenteCon&#23545;&#35821;&#35328;&#36827;&#34892;&#32534;&#30721;&#21487;&#20197;&#22312;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#39044;&#27979;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#39640;&#32423;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19979;&#28216;&#24615;&#33021;&#21644;&#21327;&#35758;&#26041;&#38754;&#65292;SenteCon&#20248;&#20110;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#35821;&#35328;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model's decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular, language must be featurized in a way that is interpretable while still characterizing the original text well. We present SenteCon, a method for introducing human interpretability in deep language representations. Given a passage of text, SenteCon encodes the text as a layer of interpretable categories in which each dimension corresponds to the relevance of a specific category. Our empirical evaluations indicate that encoding language with SenteCon provides high-level interpretability at little to no cost to predictive performance on downstream tasks. Moreover, we find that SenteCon outperforms existing interpretable language representations with respect to both its downstream performance and its agr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20174;&#27169;&#22411;&#20998;&#24067;&#20013;&#37319;&#26679;&#26469;&#32531;&#35299;NATs&#23398;&#20064;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#24067;&#30340;&#22256;&#38590;&#65292;&#24182;&#23548;&#20986;&#23545;&#27604;&#32422;&#26463;&#26469;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#25913;&#20889;&#19977;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#38750;&#33258;&#22238;&#24402;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.13667</link><description>&lt;p&gt;
&#20248;&#21270;&#23545;&#27604;&#23398;&#20064;&#30340;&#38750;&#33258;&#22238;&#24402;Transformer
&lt;/p&gt;
&lt;p&gt;
Optimizing Non-Autoregressive Transformers with Contrastive Learning. (arXiv:2305.13667v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20174;&#27169;&#22411;&#20998;&#24067;&#20013;&#37319;&#26679;&#26469;&#32531;&#35299;NATs&#23398;&#20064;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#24067;&#30340;&#22256;&#38590;&#65292;&#24182;&#23548;&#20986;&#23545;&#27604;&#32422;&#26463;&#26469;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#25913;&#20889;&#19977;&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#20197;&#21069;&#30340;&#38750;&#33258;&#22238;&#24402;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;Transformer (NATs) &#21487;&#20197;&#21516;&#26102;&#39044;&#27979;&#25152;&#26377;&#21333;&#35789;&#65292;&#32780;&#19981;&#26159;&#25353;&#39034;&#24207;&#39044;&#27979;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#33258;&#22238;&#24402;Transformer&#65288;ATs&#65289;&#30340;&#25512;&#26029;&#24310;&#36831;&#12290;&#23427;&#20204;&#22312;&#26426;&#22120;&#32763;&#35793;&#20197;&#21450;&#35768;&#22810;&#20854;&#20182;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;NATs &#38271;&#26399;&#20197;&#26469;&#38754;&#20020;&#30340;&#25361;&#25112;&#26159;&#23398;&#20064;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#24067;&#65292;&#36825;&#26159; NATs &#21644; ATs &#24615;&#33021;&#24046;&#36317;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20174;&#27169;&#22411;&#20998;&#24067;&#20013;&#37319;&#26679;&#26469;&#32531;&#35299;&#27169;&#24577;&#23398;&#20064;&#30340;&#22256;&#38590;&#65292;&#32780;&#19981;&#26159;&#20174;&#25968;&#25454;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#25105;&#20204;&#23548;&#20986;&#23545;&#27604;&#32422;&#26463;&#26469;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#23558;&#27492;&#32467;&#26524;&#30340;&#30446;&#26631;&#19982;&#26368;&#20808;&#36827;&#30340; NAT &#26550;&#26500; DA-Transformer &#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#25913;&#20889;&#36825;3&#20010;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#26816;&#39564;&#65292;&#20849;&#20351;&#29992;&#20102;5&#31181;&#22522;&#20934;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#38750;&#33258;&#22238;&#24402;&#22522;&#32447;&#65292;&#24182;&#30830;&#31435;&#20102;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive Transformers (NATs) reduce the inference latency of Autoregressive Transformers (ATs) by predicting words all at once rather than in sequential order. They have achieved remarkable progress in machine translation as well as many other applications. However, a long-standing challenge for NATs is the learning of multi-modality data distribution, which is the main cause of the performance gap between NATs and ATs. In this paper, we propose to ease the difficulty of modality learning via sampling from the model distribution instead of the data distribution. We derive contrastive constraints to stabilize the training process and integrate this resulting objective with the state-of-the-art NAT architecture DA-Transformer. Our model \method is examined on 3 different tasks, including machine translation, text summarization, and paraphrasing with 5 benchmarks. Results show that our approach outperforms previous non-autoregressive baselines by a significant margin and establi
&lt;/p&gt;</description></item><item><title>&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#23481;&#26131;&#20986;&#29616;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31639;&#27861;LAVS&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#20043;&#38388;&#30340;KL&#20998;&#27495;&#26174;&#33879;&#38477;&#20302;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10930</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;
&lt;/p&gt;
&lt;p&gt;
On the Off-Target Problem of Zero-Shot Multilingual Neural Machine Translation. (arXiv:2305.10930v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10930
&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#23481;&#26131;&#20986;&#29616;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#31639;&#27861;LAVS&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#20043;&#38388;&#30340;KL&#20998;&#27495;&#26174;&#33879;&#38477;&#20302;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20173;&#28982;&#23384;&#22312;&#8220;&#31163;&#35889;&#38382;&#39064;&#8221;&#65292;&#21363;&#23558;&#32763;&#35793;&#36755;&#20986;&#21040;&#38169;&#35823;&#30340;&#35821;&#35328;&#20013;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#38646;&#26679;&#26412;&#32763;&#35793;&#20219;&#21153;&#20013;&#26356;&#21152;&#26126;&#26174;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#24403;&#32534;&#30721;&#30446;&#26631;&#35821;&#35328;&#20449;&#21495;&#26102;&#22833;&#25928;&#65292;&#20250;&#23548;&#33268;&#31163;&#35889;&#38382;&#39064;&#65292;&#24182;&#19988;&#20004;&#31181;&#35821;&#35328;&#35789;&#27719;&#20043;&#38388;&#26356;&#25509;&#36817;&#30340;&#35789;&#27719;&#36317;&#31163;&#65288;&#21363;KL&#20998;&#27495;&#65289;&#19982;&#26356;&#39640;&#30340;&#31163;&#35889;&#29575;&#26377;&#20851;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#21457;&#29616;&#65292;&#20165;&#38548;&#31163;&#35299;&#30721;&#22120;&#20013;&#19981;&#21516;&#35821;&#35328;&#30340;&#35789;&#27719;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#31639;&#27861;Language Aware Vocabulary Sharing (LAVS)&#26469;&#26500;&#24314;&#22810;&#35821;&#35328;&#35789;&#27719;&#34920;&#65292;&#36890;&#36807;&#22686;&#21152;&#35821;&#35328;&#20043;&#38388;&#30340;KL&#20998;&#27495;&#65292;&#22823;&#22823;&#20943;&#36731;&#20102;&#32763;&#35793;&#27169;&#22411;&#30340;&#31163;&#35889;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;11&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;90&#20010;&#32763;&#35793;&#20219;&#21153;&#65292;&#37319;&#29992;LAVS&#30340;&#31163;&#35889;&#29575;&#38477;&#20302;&#20102;37&#65285;&#33267;90&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
While multilingual neural machine translation has achieved great success, it suffers from the off-target issue, where the translation is in the wrong language. This problem is more pronounced on zero-shot translation tasks. In this work, we find that failing in encoding discriminative target language signal will lead to off-target and a closer lexical distance (i.e., KL-divergence) between two languages' vocabularies is related with a higher off-target rate. We also find that solely isolating the vocab of different languages in the decoder can alleviate the problem. Motivated by the findings, we propose Language Aware Vocabulary Sharing (LAVS), a simple and effective algorithm to construct the multilingual vocabulary, that greatly alleviates the off-target problem of the translation model by increasing the KL-divergence between languages. We conduct experiments on a multilingual machine translation benchmark in 11 languages. Experiments show that the off-target rate for 90 translation 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; EmbMarker &#30340;&#23884;&#20837;&#24335;&#27700;&#21360;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312; EaaS &#20013;&#30340;&#29256;&#26435;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#23884;&#20837;&#24335;&#19978;&#26893;&#20837;&#21518;&#38376;&#65292;&#24182;&#26377;&#25928;&#22320;&#20256;&#36755;&#21644;&#24674;&#22797;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EmbMarker &#21487;&#20197;&#22312;&#32500;&#25252;&#21508;&#31181; NLP &#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#25104;&#21151;&#20445;&#25252; EaaS &#23545; LLM &#30340;&#29256;&#26435;&#12290;</title><link>http://arxiv.org/abs/2305.10036</link><description>&lt;p&gt;
&#20320;&#22312;&#25220;&#25105;&#30340;&#27169;&#22411;&#21527;&#65311;&#22522;&#20110;&#21518;&#38376;&#27700;&#21360;&#30340;&#20445;&#25252;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312; EaaS &#20013;&#30340;&#29256;&#26435;
&lt;/p&gt;
&lt;p&gt;
Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark. (arXiv:2305.10036v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10036
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; EmbMarker &#30340;&#23884;&#20837;&#24335;&#27700;&#21360;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312; EaaS &#20013;&#30340;&#29256;&#26435;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#23884;&#20837;&#24335;&#19978;&#26893;&#20837;&#21518;&#38376;&#65292;&#24182;&#26377;&#25928;&#22320;&#20256;&#36755;&#21644;&#24674;&#22797;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EmbMarker &#21487;&#20197;&#22312;&#32500;&#25252;&#21508;&#31181; NLP &#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#25104;&#21151;&#20445;&#25252; EaaS &#23545; LLM &#30340;&#29256;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#20844;&#21496;&#24050;&#32463;&#24320;&#22987;&#22522;&#20110;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#23884;&#20837;&#24335;&#26381;&#21153; (EaaS)&#65292;&#21487;&#20197;&#20026;&#23458;&#25143;&#30340;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#20219;&#21153;&#24102;&#26469;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;EaaS &#26131;&#21463;&#21040;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#25915;&#20987;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545; LLM &#30340;&#25152;&#26377;&#32773;&#36896;&#25104;&#24040;&#22823;&#25439;&#22833;&#65292;&#22240;&#20026;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#38750;&#24120;&#26114;&#36149;&#12290;&#20026;&#20102;&#20445;&#25252; EaaS &#30340; LLM &#30340;&#29256;&#26435;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; EmbMarker &#30340;&#23884;&#20837;&#24335;&#27700;&#21360;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#23884;&#20837;&#24335;&#19978;&#26893;&#20837;&#21518;&#38376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#36890;&#29992;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#36873;&#25321;&#19968;&#32452;&#20013;&#31561;&#39057;&#29575;&#30340;&#21333;&#35789;&#65292;&#24418;&#25104;&#35302;&#21457;&#38598;&#65292;&#28982;&#21518;&#36873;&#25321;&#19968;&#20010;&#30446;&#26631;&#23884;&#20837;&#20316;&#20026;&#27700;&#21360;&#65292;&#24182;&#23558;&#20854;&#25554;&#20837;&#21253;&#21547;&#35302;&#21457;&#35789;&#30340;&#25991;&#26412;&#30340;&#23884;&#20837;&#20013;&#20316;&#20026;&#21518;&#38376;&#12290;&#25554;&#20837;&#30340;&#37325;&#37327;&#19982;&#21253;&#21547;&#22312;&#25991;&#26412;&#20013;&#30340;&#35302;&#21457;&#35789;&#25968;&#37327;&#25104;&#27604;&#20363;&#12290;&#36825;&#20351;&#24471;&#27700;&#21360;&#21518;&#38376;&#21487;&#20197;&#26377;&#25928;&#22320;&#20256;&#36755;&#21644;&#24674;&#22797;&#65292;&#32780;&#19981;&#24433;&#21709; LLM &#22312;&#21508;&#31181; NLP &#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EmbMarker &#21487;&#20197;&#22312;&#32500;&#25252;&#21508;&#31181; NLP &#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#21516;&#26102;&#25104;&#21151;&#20445;&#25252; EaaS &#23545; LLM &#30340;&#29256;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called EmbMarker that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09941</link><description>&lt;p&gt;
&#8220;&#25105;&#20840;&#28982;&#25104;&#20026;&#25105;&#33258;&#24049;&#8221;&#65306;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#24615;&#21035;&#21644;&#38750;&#20108;&#20803;&#65288;TGNB&#65289;&#20154;&#32676;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#32463;&#21382;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#27495;&#35270;&#21644;&#25490;&#26021;&#12290;&#38543;&#30528;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#30340;&#26085;&#30410;&#26222;&#21450;&#21644;&#24212;&#29992;&#65292;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#36825;&#19968;&#20154;&#32676;&#30340;&#21487;&#33021;&#24615;&#20063;&#22312;&#22686;&#21152;&#12290;&#34429;&#28982;&#22823;&#37327;&#30340;NLP&#20844;&#24179;&#25991;&#29486;&#30528;&#37325;&#20110;&#38416;&#26126;&#21644;&#35299;&#20915;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;&#35780;&#20272;TGNB&#36523;&#20221;&#25152;&#24102;&#26469;&#30340;&#24615;&#21035;&#20260;&#23475;&#38656;&#35201;&#29702;&#35299;&#36825;&#20123;&#36523;&#20221;&#22914;&#20309;&#29420;&#29305;&#22320;&#19982;&#31038;&#20250;&#24615;&#21035;&#35268;&#33539;&#20114;&#21160;&#20197;&#21450;&#19982;&#24615;&#21035;&#20108;&#20803;&#20013;&#24515;&#30340;&#35270;&#35282;&#30456;&#21306;&#20998;&#12290;&#36825;&#26679;&#30340;&#27979;&#37327;&#26694;&#26550;&#26412;&#36136;&#19978;&#38656;&#35201;&#20197;TGNB&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#24110;&#21161;&#25351;&#23548;&#21253;&#23481;&#24615;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#35813;&#20026;&#35841;&#26381;&#21153;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20197;TGNB&#31038;&#21306;&#21644;&#29616;&#26377;&#30340;&#36328;&#23398;&#31185;&#25991;&#29486;&#20026;&#22522;&#30784;&#65292;&#35780;&#20272;&#20102;TGNB&#20010;&#20307;&#32463;&#21382;&#36793;&#32536;&#21270;&#25152;&#24418;&#25104;&#30340;&#31038;&#20250;&#29616;&#23454;&#26159;&#22914;&#20309;&#24433;&#21709;&#21644;&#23384;&#22312;&#20110;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#65288;OLG&#65289;&#20013;&#12290;&#39318;&#20808;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;OLG&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#24230;&#37327;&#19982;&#35813;&#20154;&#32676;&#30456;&#20851;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#29305;&#21035;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#65292;&#20197;&#21450;&#20132;&#21449;&#20998;&#26512;&#32467;&#26524;&#30340;&#20132;&#21449;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#26377;&#21161;&#20110;&#23454;&#29616;&#26356;&#20844;&#24179;&#12289;&#26356;&#21253;&#23481;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#65292;&#24182;&#28508;&#22312;&#22320;&#35299;&#20915;NLP&#30740;&#31350;&#20013;&#24191;&#27867;&#30340;&#20132;&#21449;&#36523;&#20221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
&lt;/p&gt;</description></item><item><title>&#37329;&#34701;&#39044;&#27979;&#20013;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#23545;&#20110;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#37329;&#34701;&#39044;&#27979;&#26041;&#27861;&#24456;&#23569;&#32771;&#34385;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#37329;&#34701;&#25991;&#26412;&#36923;&#36753;&#19968;&#33268;&#24615;&#30340;&#35780;&#20272;&#24037;&#20855;FinTrust&#65292;&#24182;&#20351;&#29992;&#23427;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#37329;&#34701;&#39044;&#27979;NLP&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#36739;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.08524</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#37329;&#34701;&#39044;&#27979;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Measuring Consistency in Text-based Financial Forecasting Models. (arXiv:2305.08524v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08524
&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39044;&#27979;&#20013;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#23545;&#20110;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#37329;&#34701;&#39044;&#27979;&#26041;&#27861;&#24456;&#23569;&#32771;&#34385;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#37329;&#34701;&#25991;&#26412;&#36923;&#36753;&#19968;&#33268;&#24615;&#30340;&#35780;&#20272;&#24037;&#20855;FinTrust&#65292;&#24182;&#20351;&#29992;&#23427;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#37329;&#34701;&#39044;&#27979;NLP&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39044;&#27979;&#26159;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#27963;&#36291;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#21363;&#20351;&#26159;&#26368;&#23567;&#30340;&#39044;&#27979;&#20934;&#30830;&#29575;&#25552;&#39640;&#20063;&#21487;&#20197;&#36716;&#21270;&#20026;&#24040;&#22823;&#30340;&#36130;&#21153;&#25910;&#30410;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#24102;&#26469;&#20102;&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#65288;&#22914;&#19978;&#24066;&#20844;&#21496;&#30340;&#30408;&#21033;&#25253;&#21578;&#65289;&#26469;&#39044;&#27979;&#36164;&#20135;&#25910;&#30410;&#29575;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#36825;&#31181;&#25935;&#24863;&#20219;&#21153;&#26102;&#65292;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#8212;&#8212;&#21363;&#22312;&#36755;&#20837;&#30340;&#20445;&#30041;&#24847;&#20041;&#25913;&#21464;&#26102;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#26159;&#19968;&#20010;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#20294;&#24403;&#21069;&#30340;&#37329;&#34701;&#39044;&#27979;&#26041;&#27861;&#21364;&#27809;&#26377;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FinTrust&#65292;&#19968;&#31181;&#35780;&#20272;&#37329;&#34701;&#25991;&#26412;&#20013;&#36923;&#36753;&#19968;&#33268;&#24615;&#30340;&#35780;&#20272;&#24037;&#20855;&#12290;&#20351;&#29992;FinTrust&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#37329;&#34701;&#39044;&#27979;NLP&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#36739;&#24046;&#12290;&#25105;&#20204;&#23545;&#20445;&#30041;&#24847;&#20041;&#25913;&#21464;&#24341;&#36215;&#30340;&#24615;&#33021;&#38477;&#32423;&#36827;&#34892;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Financial forecasting has been an important and active area of machine learning research, as even the most modest advantage in predictive accuracy can be parlayed into significant financial gains. Recent advances in natural language processing (NLP) bring the opportunity to leverage textual data, such as earnings reports of publicly traded companies, to predict the return rate for an asset. However, when dealing with such a sensitive task, the consistency of models -- their invariance under meaning-preserving alternations in input -is a crucial property for building user trust. Despite this, current financial forecasting methods do not consider consistency. To address this problem, we propose FinTrust, an evaluation tool that assesses logical consistency in financial text. Using FinTrust, we show that the consistency of state-of-the-art NLP models for financial forecasting is poor. Our analysis of the performance degradation caused by meaning-preserving alternations suggests that cur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ProKnow&#30340;&#27010;&#24565;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#34394;&#25311;&#24515;&#29702;&#20581;&#24247;&#21161;&#25163;&#30340;&#23433;&#20840;&#24615;&#21644;&#19987;&#19994;&#24615;&#12290;&#21516;&#26102;&#24320;&#21457;&#20102;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#23433;&#20840;&#24615;&#12289;&#30693;&#35782;&#25429;&#33719;&#21644;&#21487;&#35299;&#37322;&#24615;&#26469;&#27169;&#25311;&#27969;&#31243;&#30693;&#35782;&#12290;&#22312;&#25233;&#37057;&#30151;&#21644;&#28966;&#34385;&#30151;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;ProKnow&#24341;&#23548;&#30340;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#23433;&#20840;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08010</link><description>&lt;p&gt;
ProKnow&#65306;&#29992;&#20110;&#23433;&#20840;&#38480;&#21046;&#21644;&#21487;&#35299;&#37322;&#38382;&#39064;&#29983;&#25104;&#30340;&#27969;&#31243;&#30693;&#35782;&#22312;&#24515;&#29702;&#20581;&#24247;&#35786;&#26029;&#36741;&#21161;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ProKnow: Process Knowledge for Safety Constrained and Explainable Question Generation for Mental Health Diagnostic Assistance. (arXiv:2305.08010v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ProKnow&#30340;&#27010;&#24565;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#34394;&#25311;&#24515;&#29702;&#20581;&#24247;&#21161;&#25163;&#30340;&#23433;&#20840;&#24615;&#21644;&#19987;&#19994;&#24615;&#12290;&#21516;&#26102;&#24320;&#21457;&#20102;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#23433;&#20840;&#24615;&#12289;&#30693;&#35782;&#25429;&#33719;&#21644;&#21487;&#35299;&#37322;&#24615;&#26469;&#27169;&#25311;&#27969;&#31243;&#30693;&#35782;&#12290;&#22312;&#25233;&#37057;&#30151;&#21644;&#28966;&#34385;&#30151;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;ProKnow&#24341;&#23548;&#30340;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#23433;&#20840;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#34394;&#25311;&#24515;&#29702;&#20581;&#24247;&#21161;&#25163;&#65288;VMHA&#65289;&#25552;&#20379;&#36741;&#23548;&#21644;&#24314;&#35758;&#25252;&#29702;&#12290;&#23427;&#20204;&#19981;&#36827;&#34892;&#24739;&#32773;&#35786;&#26029;&#21327;&#21161;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#23433;&#20840;&#21463;&#38480;&#21644;&#19987;&#19994;&#20020;&#24202;&#27969;&#31243;&#30693;&#35782;&#30340;&#22521;&#35757;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;Proknow&#20026;&#19968;&#32452;&#26377;&#24207;&#20449;&#24687;&#65292;&#23427;&#26144;&#23556;&#21040;&#39046;&#22495;&#19987;&#23478;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#25351;&#21335;&#25110;&#27010;&#24565;&#29702;&#35299;&#31867;&#21035;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#30001;&#23433;&#20840;&#38480;&#21046;&#21644;Proknow&#24341;&#23548;&#30340;&#35786;&#26029;&#23545;&#35805;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#20351;&#29992;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;&#65288;NLG&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#19982;&#24739;&#32773;&#20132;&#20114;&#25910;&#38598;&#35786;&#26029;&#20449;&#24687;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#26126;&#30830;&#22320;&#24314;&#27169;&#23433;&#20840;&#24615;&#65292;&#30693;&#35782;&#25429;&#33719;&#21644;&#21487;&#35299;&#37322;&#24615;&#26469;&#27169;&#25311;&#27969;&#31243;&#30693;&#35782;&#12290;&#20351;&#29992;ProKnow&#24341;&#23548;&#30340;LM&#22686;&#24378;&#26041;&#27861;&#22312;&#25233;&#37057;&#30151;&#21644;&#28966;&#34385;&#30151;&#20013;&#29983;&#25104;&#20102;89&#65285;&#26356;&#23433;&#20840;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current Virtual Mental Health Assistants (VMHAs) provide counseling and suggestive care. They refrain from patient diagnostic assistance because they lack training in safety-constrained and specialized clinical process knowledge. In this work, we define Proknow as an ordered set of information that maps to evidence-based guidelines or categories of conceptual understanding to experts in a domain. We also introduce a new dataset of diagnostic conversations guided by safety constraints and Proknow that healthcare professionals use. We develop a method for natural language question generation (NLG) that collects diagnostic information from the patient interactively. We demonstrate the limitations of using state-of-the-art large-scale language models (LMs) on this dataset. Our algorithm models the process knowledge through explicitly modeling safety, knowledge capture, and explainability. LMs augmented with ProKnow guided method generated 89% safer questions in the depression and anxiety d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;CTC&#25439;&#22833;&#21644;&#39044;&#35757;&#32451;&#22768;&#23398;&#32534;&#30721;&#22120;&#30340;&#22522;&#20110;&#31471;&#21040;&#31471;&#30340;&#21475;&#35821;&#29702;&#35299;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;SOTA&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02937</link><description>&lt;p&gt;
&#20351;&#29992;&#32852;&#21512;CTC&#25439;&#22833;&#21644;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#22768;&#23398;&#32534;&#30721;&#22120;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
End-to-end spoken language understanding using joint CTC loss and self-supervised, pretrained acoustic encoders. (arXiv:2305.02937v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;CTC&#25439;&#22833;&#21644;&#39044;&#35757;&#32451;&#22768;&#23398;&#32534;&#30721;&#22120;&#30340;&#22522;&#20110;&#31471;&#21040;&#31471;&#30340;&#21475;&#35821;&#29702;&#35299;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;SOTA&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;&#30001;&#20110;&#32570;&#20047;&#25991;&#26412;&#20449;&#24687;&#65292;&#30452;&#25509;&#20174;&#22768;&#38899;&#20449;&#21495;&#20013;&#25552;&#21462;&#35821;&#20041;&#24847;&#20041;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#27969;&#34892;&#30340;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#21475;&#35821;&#29702;&#35299;&#27169;&#22411;&#21033;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#25552;&#21462;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#36755;&#20837;&#26469;&#25512;&#26029;&#35821;&#20041;&#65292;&#20294;&#26159;&#36825;&#38656;&#35201;&#26114;&#36149;&#30340;&#33258;&#22238;&#24402;&#35299;&#30721;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#33258;&#30417;&#30563;&#22768;&#23398;&#32534;&#30721;&#22120;&#65292;Fine-tuned Connectionist Temporal Classification&#65288;CTC&#65289;&#26469;&#25552;&#21462;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#20351;&#29992;&#32852;&#21512;CTC&#21644;SLU&#25439;&#22833;&#36827;&#34892;&#35805;&#35821;&#32423;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;DSTC2&#25968;&#25454;&#38598;&#19978;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#27169;&#22411;&#21462;&#24471;&#20102;4&#65285;&#30340;&#32477;&#23545;&#25913;&#36827;&#65292;&#24182;&#22312;SLURP&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;SOTA SLU&#27169;&#22411;1.3&#65285;&#30340;&#32477;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is challenging to extract semantic meanings directly from audio signals in spoken language understanding (SLU), due to the lack of textual information. Popular end-to-end (E2E) SLU models utilize sequence-to-sequence automatic speech recognition (ASR) models to extract textual embeddings as input to infer semantics, which, however, require computationally expensive auto-regressive decoding. In this work, we leverage self-supervised acoustic encoders fine-tuned with Connectionist Temporal Classification (CTC) to extract textual embeddings and use joint CTC and SLU losses for utterance-level SLU tasks. Experiments show that our model achieves 4% absolute improvement over the the state-of-the-art (SOTA) dialogue act classification model on the DSTC2 dataset and 1.3% absolute improvement over the SOTA SLU model on the SLURP dataset.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;&#31934;&#30830;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#25351;&#20986;&#20102;&#35774;&#35745;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#25552;&#39640;&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.04736</link><description>&lt;p&gt;
&#20851;&#20110;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#30340;&#25506;&#35752;
&lt;/p&gt;
&lt;p&gt;
On the Possibilities of AI-Generated Text Detection. (arXiv:2304.04736v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#65292;&#25552;&#20986;&#20102;&#31934;&#30830;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#24182;&#25351;&#20986;&#20102;&#35774;&#35745;&#26356;&#20934;&#30830;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#25552;&#39640;&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#24037;&#20316;&#30528;&#30524;&#20110;&#26816;&#27979;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#29983;&#25104;&#30340;&#36755;&#20986;&#65292;&#20197;&#21306;&#20998;&#20854;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#36755;&#20986;&#12290;&#36825;&#39033;&#33021;&#21147;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#31181;&#21306;&#20998;&#30340;&#21487;&#33021;&#24615;&#19968;&#30452;&#26159;&#35813;&#39046;&#22495;&#20869;&#30340;&#20105;&#35758;&#35805;&#39064;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#26816;&#27979;&#21040;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#22914;&#26524;&#33021;&#65292;&#20309;&#26102;&#33021;&#26816;&#27979;&#21040;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#38500;&#38750;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#20998;&#24067;&#22312;&#25972;&#20010;&#25903;&#25345;&#20013;&#23436;&#20840;&#30456;&#21516;&#65292;&#21542;&#21017;&#20960;&#20046;&#24635;&#26159;&#21487;&#20197;&#26816;&#27979;&#21040;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#36825;&#20010;&#35266;&#23519;&#32467;&#26524;&#26469;&#33258;&#20110;&#20449;&#24687;&#35770;&#20013;&#30340;&#26631;&#20934;&#32467;&#26524;&#65292;&#24182;&#20381;&#36182;&#20110;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36234;&#20687;&#20154;&#31867;&#65292;&#25105;&#20204;&#23601;&#38656;&#35201;&#26356;&#22810;&#30340;&#26679;&#26412;&#26469;&#26816;&#27979;&#23427;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#31934;&#30830;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#21578;&#35785;&#38656;&#35201;&#22810;&#23569;&#20010;&#26679;&#26412;&#25165;&#33021;&#26816;&#27979;&#21040;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#36825;&#24341;&#36215;&#20102;&#26356;&#22810;&#35774;&#35745;&#26356;&#20934;&#30830;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#21644;&#25552;&#39640;LLM&#36879;&#26126;&#24230;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our work focuses on the challenge of detecting outputs generated by Large Language Models (LLMs) to distinguish them from those generated by humans. This ability is of the utmost importance in numerous applications. However, the possibility of such discernment has been the subject of debate within the community. Therefore, a central question is whether we can detect AI-generated text and, if so, when. In this work, we provide evidence that it should almost always be possible to detect AI-generated text unless the distributions of human and machine-generated texts are exactly the same over the entire support. This observation follows from the standard results in information theory and relies on the fact that if the machine text becomes more human-like, we need more samples to detect it. We derive a precise sample complexity bound of AI-generated text detection, which tells how many samples are needed to detect AI-generated text. This gives rise to additional challenges of designing more
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21387;&#32553;&#21644;&#27169;&#22411;&#23610;&#23544;&#19981;&#23545;&#31216;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411; KALE&#65292;&#26377;&#25928;&#25552;&#39640;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#21516;&#26102;&#20801;&#35768;&#26597;&#35810;&#32534;&#30721;&#22120;&#30340;&#26377;&#25928;&#21387;&#32553;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20840;&#37096;&#30340;&#20877;&#35757;&#32451;&#25110;&#32034;&#24341;&#29983;&#25104;&#65292;&#27492;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#36229;&#36807;DistilBERT&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.01016</link><description>&lt;p&gt;
&#24555;&#36895;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#22120;&#21033;&#29992;KALE&#36827;&#34892;&#21518;&#32622;KL&#23545;&#40784;&#30340;&#24322;&#24418;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#35757;&#32451; (arXiv:2304.01016v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler Alignment of Embeddings for Asymmetrical dual encoders. (arXiv:2304.01016v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#26500;&#21387;&#32553;&#21644;&#27169;&#22411;&#23610;&#23544;&#19981;&#23545;&#31216;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411; KALE&#65292;&#26377;&#25928;&#25552;&#39640;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#21516;&#26102;&#20801;&#35768;&#26597;&#35810;&#32534;&#30721;&#22120;&#30340;&#26377;&#25928;&#21387;&#32553;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#20840;&#37096;&#30340;&#20877;&#35757;&#32451;&#25110;&#32034;&#24341;&#29983;&#25104;&#65292;&#27492;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#36229;&#36807;DistilBERT&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#32467;&#26500;&#21387;&#32553;&#21644;&#27169;&#22411;&#23610;&#23544;&#19981;&#23545;&#31216;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#36890;&#36807;&#23545;MSMARCO&#12289;&#33258;&#28982;&#38382;&#31572;&#12289;&#38382;&#31572;&#28216;&#25103;&#31561;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#21069;&#21518;&#35757;&#32451;&#21387;&#32553;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#21387;&#32553;&#23545;&#31995;&#32479;&#25512;&#29702;&#25928;&#29575;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#22120;&#30340;&#21452;&#32534;&#30721;&#22120;&#32467;&#26500;&#24322;&#24418;&#21270;&#26377;&#21161;&#20110;&#25552;&#39640;&#20854;&#25512;&#29702;&#25928;&#29575;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Kullback Leibler Alignment of Embeddings (KALE)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35009;&#21098;&#21644;&#23545;&#40784;&#26597;&#35810;&#32534;&#30721;&#22120;&#65292;&#25552;&#39640;&#20102;&#23494;&#38598;&#20449;&#24687;&#26816;&#32034;&#30340;&#25512;&#29702;&#25928;&#29575;&#12290;KALE&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20351;&#24471;&#22312;&#21452;&#32534;&#30721;&#22120;&#35757;&#32451;&#21518;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#26597;&#35810;&#32534;&#30721;&#22120;&#36827;&#34892;&#21387;&#32553;&#32780;&#26080;&#38656;&#36827;&#34892;&#23436;&#25972;&#30340;&#20877;&#35757;&#32451;&#25110;&#32034;&#24341;&#29983;&#25104;&#12290;&#20351;&#29992;KALE&#21644;&#19981;&#23545;&#31216;&#35757;&#32451;&#65292;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#36229;&#36807;DistilBERT&#24615;&#33021;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#27169;&#22411;&#23610;&#23544;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of improving the inference latency of language model-based dense retrieval systems by introducing structural compression and model size asymmetry between the context and query encoders. First, we investigate the impact of pre and post-training compression on the MSMARCO, Natural Questions, TriviaQA, SQUAD, and SCIFACT, finding that asymmetry in the dual encoders in dense retrieval can lead to improved inference efficiency. Knowing this, we introduce Kullback Leibler Alignment of Embeddings (KALE), an efficient and accurate method for increasing the inference efficiency of dense retrieval methods by pruning and aligning the query encoder after training. Specifically, KALE extends traditional Knowledge Distillation after bi-encoder training, allowing for effective query encoder compression without full retraining or index generation. Using KALE and asymmetric training, we can generate models which exceed the performance of DistilBERT despite having 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; N-best T5 &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992; ASR N-best &#21015;&#34920;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20351;&#29992;&#21463;&#38480;&#35299;&#30721;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38024;&#23545; ASR &#38169;&#35823;&#20462;&#27491;&#30340;&#24378;&#40065;&#26834;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.00456</link><description>&lt;p&gt;
&#22810;&#36755;&#20837;&#20551;&#35774;&#21644;&#21463;&#38480;&#35299;&#30721;&#31354;&#38388;&#30340; N-best T5&#65306;&#21033;&#29992;&#22810;&#36755;&#20837;&#20551;&#35774;&#21644;&#21463;&#38480;&#35299;&#30721;&#31354;&#38388;&#30340;&#24378;&#40065;&#26834; ASR &#38169;&#35823;&#26657;&#27491;
&lt;/p&gt;
&lt;p&gt;
N-best T5: Robust ASR Error Correction using Multiple Input Hypotheses and Constrained Decoding Space. (arXiv:2303.00456v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; N-best T5 &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992; ASR N-best &#21015;&#34920;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20351;&#29992;&#21463;&#38480;&#35299;&#30721;&#36807;&#31243;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38024;&#23545; ASR &#38169;&#35823;&#20462;&#27491;&#30340;&#24378;&#40065;&#26834;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38169;&#35823;&#20462;&#27491;&#27169;&#22411;&#26159;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21518;&#22788;&#29702;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#29992;&#20110;&#25552;&#39640;&#36716;&#24405;&#30340;&#21487;&#35835;&#24615;&#21644;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340; N-best T5 &#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992; ASR N-best &#21015;&#34920;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36716;&#31227;&#30693;&#35782;&#24182;&#20174; ASR &#35299;&#30721;&#31354;&#38388;&#20013;&#33719;&#21462;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#24378;Conformer-Transducer&#22522;&#32447;&#12290;&#26631;&#20934;&#38169;&#35823;&#26657;&#27491;&#30340;&#21478;&#19968;&#20010;&#38382;&#39064;&#26159;&#29983;&#25104;&#36807;&#31243;&#19981;&#21463;&#33391;&#22909;&#25351;&#23548;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20351;&#29992;&#22522;&#20110; N-best &#21015;&#34920;&#25110; ASR lattice &#30340;&#21463;&#38480;&#35299;&#30721;&#36807;&#31243;&#65292;&#21487;&#20197;&#20256;&#25773;&#39069;&#22806;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Error correction models form an important part of Automatic Speech Recognition (ASR) post-processing to improve the readability and quality of transcriptions. Most prior works use the 1-best ASR hypothesis as input and therefore can only perform correction by leveraging the context within one sentence. In this work, we propose a novel N-best T5 model for this task, which is fine-tuned from a T5 model and utilizes ASR N-best lists as model input. By transferring knowledge from the pre-trained language model and obtaining richer information from the ASR decoding space, the proposed approach outperforms a strong Conformer-Transducer baseline. Another issue with standard error correction is that the generation process is not well-guided. To address this a constrained decoding process, either based on the N-best list or an ASR lattice, is used which allows additional information to be propagated.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#36328;&#39046;&#22495;&#31867;&#27604;&#21019;&#36896;&#21147;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#29983;&#25104;&#30340;&#36328;&#39046;&#22495;&#31867;&#27604;&#22312;&#38382;&#39064;&#37325;&#26500;&#20013;&#20855;&#26377;&#23454;&#38469;&#24110;&#21161;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#20123;&#28508;&#22312;&#30340;&#21361;&#23475;&#65292;&#38656;&#35201;&#27880;&#24847;&#12290;</title><link>http://arxiv.org/abs/2302.12832</link><description>&lt;p&gt;
&#27969;&#20307;&#21464;&#21387;&#22120;&#19982;&#21019;&#36896;&#24615;&#31867;&#27604;&#65306;&#25506;&#32034;&#22823;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#36328;&#39046;&#22495;&#31867;&#27604;&#21019;&#36896;&#21147;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Fluid Transformers and Creative Analogies: Exploring Large Language Models' Capacity for Augmenting Cross-Domain Analogical Creativity. (arXiv:2302.12832v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#36328;&#39046;&#22495;&#31867;&#27604;&#21019;&#36896;&#21147;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#29983;&#25104;&#30340;&#36328;&#39046;&#22495;&#31867;&#27604;&#22312;&#38382;&#39064;&#37325;&#26500;&#20013;&#20855;&#26377;&#23454;&#38469;&#24110;&#21161;&#65292;&#24182;&#19988;&#23384;&#22312;&#19968;&#20123;&#28508;&#22312;&#30340;&#21361;&#23475;&#65292;&#38656;&#35201;&#27880;&#24847;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#30340;&#31867;&#27604;&#25512;&#29702;&#26159;&#19968;&#31181;&#23545;&#20154;&#31867;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26680;&#24515;&#21019;&#36896;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#36328;&#39046;&#22495;&#31867;&#27604;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#30340;&#21487;&#38752;&#24615;&#21644;&#28508;&#22312;&#29992;&#36884;&#30340;&#25506;&#32034;&#21364;&#40092;&#26377;&#31995;&#32479;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;LLMs&#22686;&#24378;&#36328;&#39046;&#22495;&#31867;&#27604;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#22312;&#19977;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;1&#65289;LLM&#29983;&#25104;&#30340;&#36328;&#39046;&#22495;&#31867;&#27604;&#22312;&#38382;&#39064;&#37325;&#26500;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#32463;&#24120;&#34987;&#35780;&#20026;&#26377;&#24110;&#21161;&#65288;&#20013;&#20301;&#25968;5&#20010;&#35780;&#20998;&#20013;&#26377;4&#20010;&#26159;&#26377;&#24110;&#21161;&#30340;&#65289;&#65292;&#24182;&#19988;&#36890;&#24120;&#65288;&#32422;80&#65285;&#30340;&#24773;&#20917;&#65289;&#23548;&#33268;&#38382;&#39064;&#37325;&#26032;&#21046;&#23450;&#26041;&#38754;&#30340;&#21487;&#35266;&#23519;&#21464;&#21270;&#65307;2&#65289;&#23384;&#22312;&#26368;&#22810;25&#65285;&#30340;&#36755;&#20986;&#34987;&#35780;&#20026;&#26377;&#28508;&#22312;&#21361;&#23475;&#30340;&#19978;&#38480;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#26159;&#30001;&#20110;&#28508;&#22312;&#30340;&#19981;&#23433;&#20869;&#23481;&#65292;&#32780;&#19981;&#26159;&#26377;&#20559;&#35265;&#25110;&#26377;&#27602;&#20869;&#23481;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#20102;LLMs&#30340;&#28508;&#22312;&#25928;&#29992;&#21644;&#39118;&#38505;&#65292;&#20197;&#22686;&#24378;&#36328;&#39046;&#22495;&#31867;&#27604;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain analogical reasoning is a core creative ability that can be challenging for humans. Recent work has shown some proofs-of concept of Large language Models' (LLMs) ability to generate cross-domain analogies. However, the reliability and potential usefulness of this capacity for augmenting human creative work has received little systematic exploration. In this paper, we systematically explore LLMs capacity to augment cross-domain analogical reasoning. Across three studies, we found: 1) LLM-generated cross-domain analogies were frequently judged as helpful in the context of a problem reformulation task (median 4 out of 5 helpfulness rating), and frequently (~80% of cases) led to observable changes in problem formulations, and 2) there was an upper bound of 25% of outputs bring rated as potentially harmful, with a majority due to potentially upsetting content, rather than biased or toxic content. These results demonstrate the potential utility -- and risks -- of LLMs for augmen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26816;&#39564;&#20102; ChatGPT &#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23427;&#26159;&#19968;&#20010;&#19975;&#33021;&#30340; AI &#27169;&#22411;&#65292;&#20294;&#26080;&#20851;&#32039;&#35201;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.10724</link><description>&lt;p&gt;
ChatGPT&#65306;&#24212;&#20184;&#21315;&#20107;&#30340;&#19975;&#33021;&#22411; AI&#65292;&#20294;&#26080;&#25152;&#19987;&#31934;
&lt;/p&gt;
&lt;p&gt;
ChatGPT: Jack of all trades, master of none. (arXiv:2302.10724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26816;&#39564;&#20102; ChatGPT &#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23427;&#26159;&#19968;&#20010;&#19975;&#33021;&#30340; AI &#27169;&#22411;&#65292;&#20294;&#26080;&#20851;&#32039;&#35201;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#23545;&#26576;&#20123;&#20219;&#21153;&#30340;&#34920;&#29616;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OpenAI &#25512;&#20986;&#20102;&#32842;&#22825;&#29983;&#25104;&#39044;&#35757;&#32451; Transformer&#65288;ChatGPT&#65289;&#65292;&#38761;&#26032;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#26041;&#27861;&#12290;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797; ChatGPT &#22312;&#20247;&#25152;&#21608;&#30693;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#65292;&#26469;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#25928;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#22823;&#22810;&#38750;&#33258;&#21160;&#21270;&#65292;&#24182;&#19988;&#35268;&#27169;&#38750;&#24120;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#22312; 25 &#20010;&#19981;&#21516;&#30340; NLP &#20219;&#21153;&#19978;&#26816;&#39564;&#20102; ChatGPT &#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20219;&#21153;&#29978;&#33267;&#23545;&#20154;&#31867;&#32780;&#35328;&#37117;&#26159;&#20027;&#35266;&#30340;&#65292;&#20363;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#12289;&#25915;&#20987;&#24615;&#21644;&#31435;&#22330;&#26816;&#27979;&#12290;&#21478;&#19968;&#20123;&#20219;&#21153;&#21017;&#38656;&#35201;&#26356;&#23458;&#35266;&#30340;&#25512;&#29702;&#65292;&#22914;&#35789;&#20041;&#28040;&#27495;&#12289;&#35821;&#35328;&#21487;&#25509;&#21463;&#24615;&#21644;&#38382;&#31572;&#12290;&#25105;&#20204;&#36824;&#23545; GPT-4 &#27169;&#22411;&#22312;&#20116;&#20010;&#36873;&#23450;&#30340; NLP &#20219;&#21153;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#33258;&#21160;&#21270;&#20102; ChatGPT &#21644; GPT-4 &#30340;&#24341;&#23548;&#36807;&#31243;&#65292;&#24182;&#20998;&#26512;&#20102;&#36229;&#36807; 49k &#20010;&#21709;&#24212;&#12290;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#65288;SOTA&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19968;&#20123;&#20219;&#21153;&#19978; ChatGPT &#30340;&#24615;&#33021;&#23384;&#22312;&#19968;&#23450;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;
OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quali
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Elgot monads&#21644;Kleene monads&#20043;&#38388;&#30340;&#24418;&#24335;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;while-monad&#31867;&#12290;&#34429;&#28982;Kleene monads&#26159;&#30456;&#23545;&#31616;&#21333;&#30340;&#20195;&#25968;&#25551;&#36848;&#65292;&#20294;&#26159;while-monads&#21487;&#33021;&#19981;&#31526;&#21512;Kleene&#20195;&#25968;&#23450;&#24459;&#65292;&#25110;&#32773;&#29978;&#33267;&#19981;&#25903;&#25345;Kleene&#36845;&#20195;&#36816;&#31639;&#31526;&#12290;</title><link>http://arxiv.org/abs/2301.06202</link><description>&lt;p&gt;
&#36845;&#20195;&#30340;&#31181;&#31181;&#65306;&#20174;Elgot&#21040;Kleene
&lt;/p&gt;
&lt;p&gt;
Shades of Iteration: from Elgot to Kleene. (arXiv:2301.06202v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Elgot monads&#21644;Kleene monads&#20043;&#38388;&#30340;&#24418;&#24335;&#32852;&#31995;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;while-monad&#31867;&#12290;&#34429;&#28982;Kleene monads&#26159;&#30456;&#23545;&#31616;&#21333;&#30340;&#20195;&#25968;&#25551;&#36848;&#65292;&#20294;&#26159;while-monads&#21487;&#33021;&#19981;&#31526;&#21512;Kleene&#20195;&#25968;&#23450;&#24459;&#65292;&#25110;&#32773;&#29978;&#33267;&#19981;&#25903;&#25345;Kleene&#36845;&#20195;&#36816;&#31639;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36845;&#20195;&#27010;&#24565;&#20174;&#26368;&#36890;&#29992;&#30340;Elgot&#36845;&#20195;&#21040;&#38750;&#24120;&#29305;&#23450;&#30340;Kleene&#36845;&#20195;&#19981;&#31561;&#12290; Bloom&#21644;Esik&#24050;&#32463;&#20197;&#36845;&#20195;&#29702;&#35770;&#30340;&#24418;&#24335;&#24191;&#27867;&#25506;&#35752;&#20102;Elgot&#36845;&#20195;&#30340;&#22522;&#26412;&#26412;&#36136;&#65292;&#32780;Kleene&#36845;&#20195;&#21017;&#20316;&#20026;&#65288;&#26080;&#31867;&#22411;&#30340;&#65289;&#24418;&#24335;&#20027;&#20041;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#65292;&#20363;&#22914;&#33258;&#21160;&#26426;&#29702;&#35770;&#65292;&#27491;&#21017;&#34920;&#36798;&#24335;&#21644;Kleene&#20195;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;Elgot&#21333;&#23376;&#21644;Kleene&#21333;&#23376;&#24314;&#31435;Elgot&#36845;&#20195;&#21644;Kleene&#36845;&#20195;&#20043;&#38388;&#30340;&#24418;&#24335;&#36830;&#25509;&#12290; &#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;while-monad&#31867;&#65292;&#31867;&#20284;&#20110;Kleene monads&#65292;&#20294;&#22312;&#20195;&#25968;&#26415;&#35821;&#20013;&#26377;&#30456;&#23545;&#31616;&#21333;&#30340;&#25551;&#36848;&#12290;&#19982;Elgot&#21333;&#23376;&#19968;&#26679;&#65292;while-monads&#28085;&#30422;&#20102;&#25903;&#25345;while&#24490;&#29615;&#30340;&#22823;&#37327;&#27169;&#22411;&#65292;&#20294;&#21487;&#33021;&#19981;&#31526;&#21512;Kleene&#20195;&#25968;&#23450;&#24459;&#65292;&#29978;&#33267;&#21487;&#33021;&#26681;&#26412;&#19981;&#25903;&#25345;Kleene&#36845;&#20195;&#36816;&#31639;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Notions of iteration range from the arguably most general Elgot iteration to a very specific Kleene iteration. The fundamental nature of Elgot iteration has been extensively explored by Bloom and Esik in the form of iteration theories, while Kleene iteration became extremely popular as an integral part of (untyped) formalisms, such as automata theory, regular expressions and Kleene algebra. Here, we establish a formal connection between Elgot iteration and Kleene iteration in the form of Elgot monads and Kleene monads, respectively. We also introduce a novel class of while-monads, which like Kleene monads admit a relatively simple description in algebraic terms. Like Elgot monads, while-monads cover a large variety of models that meaningfully support while-loops, but may fail the Kleene algebra laws, or even fail to support a Kleen iteration operator altogether.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#28145;&#24230;&#27169;&#22240;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#30001;191&#20010;&#20390;&#25506;&#25925;&#20107;&#35868;&#39064;&#26500;&#25104;&#65292;&#21482;&#26377;47%&#30340;&#20154;&#33021;&#25104;&#21151;&#35299;&#20915;&#20854;&#20013;&#19968;&#20010;&#35868;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;GPT-3&#27169;&#22411;&#22312;&#27492;&#22522;&#20934;&#27979;&#35797;&#20013;&#20934;&#30830;&#24615;&#20165;&#20026;28&#65285;&#65292;&#32780;GPT-4&#20165;&#33021;&#35299;&#20915;38%&#30340;&#35868;&#39064;&#12290;&#36825;&#34920;&#26126;LLMs&#19982;&#20154;&#31867;&#22312;&#28145;&#24230;&#25512;&#29702;&#33021;&#21147;&#19978;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2212.10114</link><description>&lt;p&gt;
&#12298;&#30495;&#25506;&#12299;&#65306;&#19968;&#39033;&#28145;&#24230;&#27169;&#22240;&#25512;&#29702;&#22522;&#20934;&#65292;&#38590;&#20498;GPT-3&#65292;&#23545;GPT-4&#26500;&#25104;&#25361;&#25112;&#65288;arXiv&#65306;2212.10114v2 [cs.CL] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
True Detective: A Deep Abductive Reasoning Benchmark Undoable for GPT-3 and Challenging for GPT-4. (arXiv:2212.10114v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#28145;&#24230;&#27169;&#22240;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#30001;191&#20010;&#20390;&#25506;&#25925;&#20107;&#35868;&#39064;&#26500;&#25104;&#65292;&#21482;&#26377;47%&#30340;&#20154;&#33021;&#25104;&#21151;&#35299;&#20915;&#20854;&#20013;&#19968;&#20010;&#35868;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;GPT-3&#27169;&#22411;&#22312;&#27492;&#22522;&#20934;&#27979;&#35797;&#20013;&#20934;&#30830;&#24615;&#20165;&#20026;28&#65285;&#65292;&#32780;GPT-4&#20165;&#33021;&#35299;&#20915;38%&#30340;&#35868;&#39064;&#12290;&#36825;&#34920;&#26126;LLMs&#19982;&#20154;&#31867;&#22312;&#28145;&#24230;&#25512;&#29702;&#33021;&#21147;&#19978;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#38646;-shot&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#22312;&#24403;&#21069;&#30340;&#27979;&#35797;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#38656;&#35201;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#38656;&#35201;&#35299;&#20915;&#39640;&#24230;&#20808;&#36827;&#30340;&#25512;&#29702;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#26679;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#30001;191&#20010;&#21253;&#21547;&#38271;&#31687;&#25925;&#20107;&#65288;&#24179;&#22343;1200&#20010;&#21333;&#35789;&#65289;&#30340;&#20390;&#25506;&#35868;&#39064;&#26500;&#25104;&#12290;&#39064;&#30446;&#26469;&#33258;&#8220;5&#20998;&#38047;&#30340;&#35868;&#8221;&#24179;&#21488;&#65292;&#24182;&#21253;&#25324;&#29992;&#20110;&#35780;&#20272;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;&#20165;&#26377;47%&#30340;&#20154;&#24179;&#22343;&#33021;&#25104;&#21151;&#35299;&#20915;&#19968;&#20010;&#35868;&#39064;&#65292;&#32780;&#26368;&#22909;&#30340;&#20154;&#31867;&#35299;&#35868;&#32773;&#21017;&#33021;&#22815;&#21462;&#24471;&#36229;&#36807;80%&#30340;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GPT-3&#27169;&#22411;&#22312;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#34920;&#29616;&#20165;&#30053;&#32988;&#20110;&#38543;&#26426;&#29468;&#27979;&#65288;&#20934;&#30830;&#29575;28%&#65289;&#65292;&#32780;&#26368;&#20808;&#36827;&#30340;GPT-4&#20165;&#33021;&#35299;&#20915;38%&#30340;&#35868;&#39064;&#12290;&#36825;&#34920;&#26126;&#65292;LLMs&#19982;&#20154;&#31867;&#22312;&#28145;&#24230;&#25512;&#29702;&#33021;&#21147;&#19978;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#24182;&#20984;&#26174;&#20102;&#23545;&#36825;&#20010;&#39046;&#22495;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated solid zero-shot reasoning capabilities, which is reflected in their performance on the current test tasks. This calls for a more challenging benchmark requiring highly advanced reasoning ability to be solved. In this paper, we introduce such a benchmark, consisting of 191 long-form (1200 words on average) mystery narratives constructed as detective puzzles. Puzzles are sourced from the "5 Minute Mystery" platform and include a multiple-choice question for evaluation. Only 47% of humans solve a puzzle successfully on average, while the best human solvers achieve over 80% success rate. We show that GPT-3 models barely outperform random on this benchmark (with 28% accuracy) while state-of-the-art GPT-4 solves only 38% of puzzles. This indicates that there is still a significant gap in the deep reasoning abilities of LLMs and humans and highlights the need for further research in this area. Our work introduces a challenging benchmark for futur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#38544;&#31169;&#25935;&#24863;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#22914;&#20309;&#19982;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#32467;&#21512;&#20197;&#35299;&#20915;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#22312;&#32500;&#25345;&#21487;&#25509;&#21463;&#24615;&#33021;&#30340;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2212.10025</link><description>&lt;p&gt;
&#24403;&#32852;&#37030;&#23398;&#20064;&#36935;&#21040;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
When Federated Learning Meets Pre-trained Language Models' Parameter-Efficient Tuning Methods. (arXiv:2212.10025v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#38544;&#31169;&#25935;&#24863;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#22914;&#20309;&#19982;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#32467;&#21512;&#20197;&#35299;&#20915;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#22312;&#32500;&#25345;&#21487;&#25509;&#21463;&#24615;&#33021;&#30340;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#25968;&#25454;&#38544;&#31169;&#20851;&#27880;&#30340;&#22686;&#21152;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#38544;&#31169;&#25935;&#24863;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#24456;&#22810;&#25991;&#29486;&#24314;&#35758;&#22312; FL &#33539;&#24335;&#20013;&#23436;&#20840;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21487;&#20197;&#32531;&#35299;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#32553;&#23567;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411; PLMs &#24102;&#26469;&#20102;&#27785;&#37325;&#30340;&#36890;&#20449;&#24320;&#38144;&#21644; FL &#31995;&#32479;&#30340;&#26412;&#22320;&#27169;&#22411;&#36866;&#24212;&#25104;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PETuning&#65289;&#26041;&#27861;&#24341;&#20837;&#21040;&#32852;&#37030;&#23398;&#20064;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#32463;&#39564;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102; FL &#20013;&#20195;&#34920;&#24615;&#30340; PLMs &#35843;&#25972;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#35206;&#30422;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#27700;&#24179;&#12289;&#25968;&#25454;&#35268;&#27169;&#21644;&#19981;&#21516; FL &#22330;&#26223;&#30340;&#20998;&#26512;&#12290;&#22312;&#32500;&#25345;&#21487;&#25509;&#21463;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#26412;&#22320;&#35843;&#25972;&#21644;&#20840;&#23616;&#32858;&#21512;&#36731;&#37327;&#32423;&#27169;&#22411;&#21442;&#25968;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24635;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
With increasing privacy concerns on data, recent studies have made significant progress using federated learning (FL) on privacy-sensitive natural language processing (NLP) tasks. Much literature suggests fully fine-tuning pre-trained language models (PLMs) in the FL paradigm can mitigate the data heterogeneity problem and close the performance gap with centralized training. However, large PLMs bring the curse of prohibitive communication overhead and local model adaptation costs for the FL system. To this end, we introduce various parameter-efficient tuning (PETuning) methods into federated learning. Specifically, we provide a holistic empirical study of representative PLMs tuning methods in FL. The experimental results cover the analysis of data heterogeneity levels, data scales, and different FL scenarios. Overall communication overhead can be significantly reduced by locally tuning and globally aggregating lightweight model parameters while maintaining acceptable performance in var
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#25991;&#26412;&#20013;&#30340;&#31163;&#32676;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.11300</link><description>&lt;p&gt;
&#25991;&#26412;&#20013;&#30340;&#31163;&#32676;&#26816;&#27979;&#30340;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text. (arXiv:2211.11300v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#25991;&#26412;&#20013;&#30340;&#31163;&#32676;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#35777;&#26126;&#26159;&#21482;&#20351;&#29992;&#20869;&#20998;&#24067;(ID)&#26679;&#20363;&#25991;&#26412;&#36827;&#34892;&#31163;&#32676;&#26816;&#27979;&#30340;&#23453;&#36149;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35201;&#20040;&#36890;&#36807;&#20351;&#29992;ID&#26679;&#20363;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#23558;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#22256;&#24785;&#24230;&#20316;&#20026;&#31163;&#32676;&#24471;&#20998;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#20004;&#31181;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#30340;&#20114;&#34917;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#23427;&#20204;&#20248;&#21183;&#24182;&#20943;&#36731;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#30340;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#20316;&#20026;&#32769;&#24072;&#65292;&#22312;ID&#31034;&#20363;&#19978;&#25945;&#25480;&#19968;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#38500;&#20102;&#39044;&#27979;&#23618;&#33976;&#39311;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20013;&#38388;&#23618;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#20840;&#38754;&#25506;&#32034;&#32769;&#24072;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#23398;&#20064;&#30340;&#23398;&#29983;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;ID&#25968;&#25454;&#27969;&#24418;&#65292;&#21516;&#26102;&#33719;&#24471;&#26356;&#24378;&#30340;&#23558;OoD&#31034;&#20363;&#26144;&#23556;&#21040;&#27969;&#24418;&#20043;&#22806;&#30340;&#33021;&#21147;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#31454;&#20105;&#22522;&#32447;&#30456;&#27604;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised representation learning has proved to be a valuable component for out-of-distribution (OoD) detection with only the texts of in-distribution (ID) examples. These approaches either train a language model from scratch or fine-tune a pre-trained language model using ID examples, and then take the perplexity output by the language model as OoD scores. In this paper, we analyze the complementary characteristics of both OoD detection methods and propose a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations. Specifically, we use a fine-tuned model as the teacher to teach a randomly initialized student model on the ID examples. Besides the prediction layer distillation, we present a similarity-based intermediate layer distillation method to thoroughly explore the representation space of the teacher model. In this way, the learned student can better represent the ID data manifold while gaining a stronger ability to map O
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#22270;&#20687;&#38382;&#39064;&#30340;&#27495;&#20041;&#24615;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#37325;&#26032;&#26500;&#36848;&#38382;&#39064;&#24182;&#20998;&#32452;&#31572;&#26696;&#26469;&#20943;&#23569;&#27495;&#20041;&#12290;&#21516;&#26102;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#33521;&#35821;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#26356;&#23569;&#27495;&#20041;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.07516</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#23567;&#40481;&#35201;&#31359;&#36807;&#39532;&#36335;&#65311;&#37325;&#36848;&#21644;&#20998;&#26512; VQA &#20013;&#30340;&#27495;&#20041;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why Did the Chicken Cross the Road? Rephrasing and Analyzing Ambiguous Questions in VQA. (arXiv:2211.07516v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#22270;&#20687;&#38382;&#39064;&#30340;&#27495;&#20041;&#24615;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#37325;&#26032;&#26500;&#36848;&#38382;&#39064;&#24182;&#20998;&#32452;&#31572;&#26696;&#26469;&#20943;&#23569;&#27495;&#20041;&#12290;&#21516;&#26102;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#33521;&#35821;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#26356;&#23569;&#27495;&#20041;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20855;&#26377;&#27495;&#20041;&#24615;&#65292;&#35299;&#20915;&#27495;&#20041;&#38382;&#39064;&#23545;&#20110;&#25104;&#21151;&#22238;&#31572;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#20851;&#20110;&#22270;&#20687;&#30340;&#38382;&#39064;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#27495;&#20041;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#27880;&#37322;&#65292;&#36890;&#36807;&#37325;&#26032;&#26500;&#36848;&#38382;&#39064;&#24182;&#23558;&#31572;&#26696;&#25353;&#20854;&#25152;&#22238;&#31572;&#30340;&#38382;&#39064;&#36827;&#34892;&#20998;&#32452;&#65292;&#20197;&#20943;&#23569;&#27495;&#20041;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#35270;&#35273;&#38382;&#39064;&#20013;&#27495;&#20041;&#21407;&#22240;&#30340;&#19982;&#35821;&#35328;&#23545;&#40784;&#26412;&#20307;&#35770;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33521;&#35821;&#38382;&#39064;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#23637;&#31034;&#20854;&#33021;&#22815;&#20135;&#29983;&#36739;&#23569;&#27495;&#20041;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#25105;&#20204;&#20351;&#29992;&#30340;&#38382;&#39064;&#29983;&#25104;&#30446;&#26631;&#20801;&#35768;&#27169;&#22411;&#22312;&#27809;&#26377;&#20219;&#20309;&#30452;&#25509;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#38598;&#25104;&#31572;&#26696;&#32452;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language is ambiguous. Resolving ambiguous questions is key to successfully answering them. Focusing on questions about images, we create a dataset of ambiguous examples. We annotate these, grouping answers by the underlying question they address and rephrasing the question for each group to reduce ambiguity. Our analysis reveals a linguistically-aligned ontology of reasons for ambiguity in visual questions. We then develop an English question-generation model which we demonstrate via automatic and human evaluation produces less ambiguous questions. We further show that the question generation objective we use allows the model to integrate answer group information without any direct supervision.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REV&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#20013;&#26032;&#39062;&#12289;&#19982;&#26631;&#31614;&#30456;&#20851;&#30340;&#20449;&#24687;&#30340;&#25968;&#37327;&#65292;&#36890;&#36807;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#36827;&#34892;&#30740;&#31350;&#12290;&#23454;&#39564;&#35777;&#26126;REV&#22312;&#35780;&#20272;&#35299;&#37322;-&#26631;&#31614;&#23545;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#30452;&#35273;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2210.04982</link><description>&lt;p&gt;
&#29992;&#20449;&#24687;&#35770;&#35780;&#20272;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
REV: Information-Theoretic Evaluation of Free-Text Rationales. (arXiv:2210.04982v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REV&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#20013;&#26032;&#39062;&#12289;&#19982;&#26631;&#31614;&#30456;&#20851;&#30340;&#20449;&#24687;&#30340;&#25968;&#37327;&#65292;&#36890;&#36807;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#36827;&#34892;&#30740;&#31350;&#12290;&#23454;&#39564;&#35777;&#26126;REV&#22312;&#35780;&#20272;&#35299;&#37322;-&#26631;&#31614;&#23545;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#19982;&#20154;&#31867;&#30452;&#35273;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#26159;&#36808;&#21521;&#21487;&#35299;&#37322; NLP &#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#27493;&#39588;&#65292;&#28982;&#32780;&#35780;&#20272;&#36825;&#26679;&#30340;&#35299;&#37322;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#24230;&#37327;&#20027;&#35201;&#38598;&#20013;&#22312;&#27979;&#37327;&#35299;&#37322;&#21644;&#32473;&#23450;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#19978;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#29702;&#24819;&#30340;&#24230;&#37327;&#24212;&#35813;&#38598;&#20013;&#20110;&#35299;&#37322;&#20013;&#25552;&#20379;&#30340;&#26032;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#22312;&#36755;&#20837;&#25110;&#26631;&#31614;&#20013;&#37117;&#27809;&#26377;&#25552;&#20379;&#12290;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#20351;&#29992;&#26465;&#20214;V-&#20449;&#24687;&#65288;Hewitt et al&#12290;&#65292;2021&#65289;&#30740;&#31350;&#20102;&#36825;&#20010;&#30740;&#31350;&#38382;&#39064;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;REV&#65288;&#21033;&#29992;&#26465;&#20214;V-&#20449;&#24687;&#35780;&#20272;&#35299;&#37322;&#65289;&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#37327;&#21270;&#29702;&#24615;&#20013;&#38500;&#20102;&#36755;&#20837;&#25110;&#26631;&#31614;&#20013;&#24050;&#26377;&#20449;&#24687;&#20043;&#22806;&#30340;&#26032;&#26631;&#31614;&#30456;&#20851;&#20449;&#24687;&#30340;&#25968;&#37327;&#12290;&#22312;&#28041;&#21450;&#25512;&#29702;&#20219;&#21153;&#30340;&#22235;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;REV&#22312;&#35780;&#20272;&#35299;&#37322;-&#26631;&#31614;&#23545;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#19982;&#29616;&#26377;&#30340;&#24230;&#37327;&#30456;&#27604;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;REV&#19982;&#20154;&#31867;&#30452;&#35273;&#19968;&#33268;&#65292;&#32780;&#19968;&#20123;&#29616;&#26377;&#30340;&#24230;&#37327;&#21017;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating free-text rationales is a promising step towards explainable NLP, yet evaluating such rationales remains a challenge. Existing metrics have mostly focused on measuring the association between the rationale and a given label. We argue that an ideal metric should focus on the new information uniquely provided in the rationale that is otherwise not provided in the input or the label. We investigate this research problem from an information-theoretic perspective using conditional V-information (Hewitt et al., 2021). More concretely, we propose a metric called REV (Rationale Evaluation with conditional V-information), to quantify the amount of new, label-relevant information in a rationale beyond the information already available in the input or the label. Experiments across four benchmarks with reasoning tasks, including chain-of-thought, demonstrate the effectiveness of REV in evaluating rationale-label pairs, compared to existing metrics. We further demonstrate REV is consiste
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;UDDIA&#30340;&#32479;&#19968;&#21435;&#27602;&#21270;&#21644;&#21435;&#20559;&#35265;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;&#26377;&#27602;&#35821;&#35328;&#21644;&#20943;&#23569;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#20445;&#25345;&#27969;&#30021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.04492</link><description>&lt;p&gt;
&#36890;&#36807;&#25512;&#29702;&#26102;&#33258;&#36866;&#24212;&#20248;&#21270;&#23454;&#29616;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#32479;&#19968;&#21435;&#27602;&#21270;&#21644;&#21435;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Unified Detoxifying and Debiasing in Language Generation via Inference-time Adaptive Optimization. (arXiv:2210.04492v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;UDDIA&#30340;&#32479;&#19968;&#21435;&#27602;&#21270;&#21644;&#21435;&#20559;&#35265;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;&#26377;&#27602;&#35821;&#35328;&#21644;&#20943;&#23569;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#20445;&#25345;&#27969;&#30021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#25429;&#25417;&#21644;&#22797;&#21046;&#26377;&#23475;&#20869;&#23481;&#30340;&#24773;&#20917;&#26222;&#36941;&#23384;&#22312;&#65292;&#29305;&#21035;&#26159;&#27602;&#24615;&#35821;&#35328;&#21644;&#31038;&#20250;&#20559;&#35265;&#65292;&#24341;&#36215;&#20102;&#20005;&#37325;&#30340;&#36947;&#24503;&#38382;&#39064;&#12290;&#27492;&#21069;&#22312;&#36947;&#24503;&#19978;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#24037;&#20316;&#37117;&#26159;&#20998;&#24320;&#35299;&#20915;&#21435;&#27602;&#21270;&#21644;&#21435;&#20559;&#35265;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#21435;&#20559;&#35265;&#30340;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#27602;&#24615;&#65292;&#32780;&#32463;&#36807;&#21435;&#27602;&#21270;&#30340;&#27169;&#22411;&#29978;&#33267;&#20250;&#21152;&#21095;&#31038;&#20250;&#20559;&#35265;&#65292;&#36825;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21517;&#20026;UDDIA&#30340;&#32479;&#19968;&#21435;&#27602;&#21270;&#21644;&#21435;&#20559;&#35265;&#26694;&#26550;&#65292;&#23558;&#36825;&#20004;&#20010;&#38382;&#39064;&#20316;&#20026;&#32416;&#27491;&#36755;&#20986;&#31354;&#38388;&#30340;&#20851;&#38190;&#38382;&#39064;&#32852;&#21512;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#35299;&#37322;&#20026;&#23398;&#20064;&#28151;&#21512;&#21152;&#26435;&#23646;&#24615;&#30340;&#25991;&#26412;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;UDDIA&#23545;&#23569;&#37327;&#21442;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#20248;&#21270;&#65292;&#20174;&#32780;&#25511;&#21046;&#27599;&#20010;&#23646;&#24615;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;UDDIA&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;&#26377;&#27602;&#35821;&#35328;&#24182;&#20943;&#23569;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#20445;&#25345;&#27969;&#30021;&#24615;&#12290;&#27492;&#22806;&#65292;UDDIA&#22312;&#24191;&#27867;&#30340;&#35780;&#20272;&#20013;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21435;&#27602;&#21270;&#21644;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#35299;&#37322;&#20102;UDDIA&#30340;&#34892;&#20026;&#65292;&#24182;&#20026;&#26410;&#26469;&#22312;&#36947;&#24503;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Warning: this paper contains model outputs exhibiting offensiveness and biases. Recently pre-trained language models (PLMs) have prospered in various natural language generation (NLG) tasks due to their ability to generate fairly fluent text. Nevertheless, these models are observed to capture and reproduce harmful contents in training corpora, typically toxic language and social biases, raising severe moral issues. Prior works on ethical NLG tackle detoxifying and debiasing separately, which is problematic since we find debiased models still exhibit toxicity while detoxified ones even exacerbate social biases. To address such a challenge, we propose the first unified framework of detoxifying and debiasing called UDDIA, which jointly formalizes these two problems as rectifying the output space. We theoretically interpret our framework as learning a text distribution mixing weighted attributes. Besides, UDDIA conducts adaptive optimization of only a few parameters during decoding based o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#27010;&#29575;&#25512;&#29702;&#33539;&#20363;ThinkSum&#65292;&#36890;&#36807;&#20197;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#23545;&#23545;&#35937;&#25110;&#20107;&#23454;&#38598;&#36827;&#34892;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#23545;&#35937;&#25110;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#24182;&#36827;&#34892;&#36923;&#36753;&#25512;&#23548;&#30340;&#22330;&#26223;&#20013;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.01293</link><description>&lt;p&gt;
ThinkSum&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38598;&#21512;&#30340;&#27010;&#29575;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
ThinkSum: Probabilistic reasoning over sets using large language models. (arXiv:2210.01293v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01293
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#27010;&#29575;&#25512;&#29702;&#33539;&#20363;ThinkSum&#65292;&#36890;&#36807;&#20197;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#23545;&#23545;&#35937;&#25110;&#20107;&#23454;&#38598;&#36827;&#34892;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#23545;&#35937;&#25110;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#24182;&#36827;&#34892;&#36923;&#36753;&#25512;&#23548;&#30340;&#22330;&#26223;&#20013;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39640;&#32423;&#31867;&#27604;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#33021;&#21147;&#65306;&#37325;&#29616;&#32447;&#24615;&#25991;&#26412;&#20013;&#20986;&#29616;&#22312;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#65288;&#38646;&#26679;&#26412;&#35780;&#20272;&#65289;&#25110;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#27169;&#24335;&#65288;&#23569;&#37327;&#26679;&#26412;&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#65289;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#26356;&#20808;&#36827;&#30340;LLMs&#22312;&#38656;&#35201;&#23545;&#22810;&#20010;&#23545;&#35937;&#25110;&#20107;&#23454;&#36827;&#34892;&#25512;&#29702;&#24182;&#36827;&#34892;&#36923;&#36753;&#25512;&#23548;&#30340;&#22330;&#26223;&#20013;&#20063;&#20250;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#27010;&#29575;&#25512;&#29702;&#33539;&#20363;ThinkSum&#65292;&#23427;&#20197;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#23545;&#23545;&#35937;&#25110;&#20107;&#23454;&#38598;&#36827;&#34892;&#25512;&#29702;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65288;Think-&#26816;&#32034;&#20851;&#32852;&#65289;&#20013;&#65292;LLM&#22312;&#20174;&#25552;&#31034;&#25110;&#36741;&#21161;&#27169;&#22411;&#35843;&#29992;&#25552;&#21462;&#30340;&#30701;&#35821;&#38598;&#19978;&#24182;&#34892;&#26597;&#35810;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65288;Sum&#27010;&#29575;&#25512;&#29702;&#25110;&#25512;&#29702;&#65289;&#20013;&#65292;&#32858;&#21512;&#36825;&#20123;&#26597;&#35810;&#30340;&#32467;&#26524;&#20197;&#36827;&#34892;&#26368;&#32456;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;BIG-bench LLM&#35780;&#20272;&#20219;&#21153;&#22871;&#20214;&#19978;&#23637;&#31034;&#20102;ThinkSum&#30340;&#21487;&#33021;&#24615;&#21644;&#20248;&#21183;&#65292;&#21462;&#24471;&#20102;&#36229;&#36234;&#22522;&#20934;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which reasons over sets of objects or facts in a structured manner. In the first stage (Think - retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (Sum probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the possibilities and advantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#20219;&#24847;&#20851;&#31995;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#30340;&#20851;&#31995;&#23450;&#20041;&#36755;&#20837;&#65292;&#23454;&#29616;&#22312;&#24222;&#22823;&#30340;&#23454;&#20307;&#23545;&#31354;&#38388;&#20013;&#25628;&#32034;&#65292;&#25552;&#21462;&#20934;&#30830;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2206.14268</link><description>&lt;p&gt;
BertNet&#65306;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#20219;&#24847;&#20851;&#31995;&#30340;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
BertNet: Harvesting Knowledge Graphs with Arbitrary Relations from Pretrained Language Models. (arXiv:2206.14268v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#20219;&#24847;&#20851;&#31995;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#30340;&#20851;&#31995;&#23450;&#20041;&#36755;&#20837;&#65292;&#23454;&#29616;&#22312;&#24222;&#22823;&#30340;&#23454;&#20307;&#23545;&#31354;&#38388;&#20013;&#25628;&#32034;&#65292;&#25552;&#21462;&#20934;&#30830;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26500;&#24314;&#22810;&#31181;&#20851;&#31995;&#30340;&#30693;&#35782;&#22270;&#35889;&#20197;&#25903;&#25345;&#30693;&#35782;&#21457;&#29616;&#21644;&#24191;&#27867;&#24212;&#29992;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20197;&#24448;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26041;&#27861;&#22522;&#20110;&#20247;&#21253;&#25110;&#25991;&#26412;&#25366;&#25496;&#65292;&#24448;&#24448;&#30001;&#20110;&#25163;&#21160;&#25104;&#26412;&#25110;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#38480;&#21046;&#32780;&#20165;&#38480;&#20110;&#23567;&#22411;&#39044;&#23450;&#20041;&#20851;&#31995;&#38598;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38544;&#24335;&#30340;&#30693;&#35782;&#24211;&#65292;&#20197;&#25509;&#21463;&#25552;&#31034;&#30340;&#30693;&#35782;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#38544;&#24335;&#30693;&#35782;&#32570;&#23569;&#23436;&#25972;&#31526;&#21495;&#30693;&#35782;&#22270;&#35889;&#30340;&#35768;&#22810;&#29702;&#24819;&#23646;&#24615;&#65292;&#22914;&#26131;&#20110;&#35775;&#38382;&#12289;&#23548;&#33322;&#12289;&#32534;&#36753;&#21644;&#36136;&#37327;&#20445;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22823;&#35268;&#27169;&#30340;&#20219;&#24847;&#20851;&#31995;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#36890;&#36807;&#26368;&#23567;&#30340;&#20851;&#31995;&#23450;&#20041;&#36755;&#20837;&#65288;&#25552;&#31034;&#21644;&#31034;&#20363;&#23454;&#20307;&#23545;&#30340;&#30701;&#35821;&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#22312;&#24222;&#22823;&#30340;&#23454;&#20307;&#23545;&#31354;&#38388;&#20013;&#25628;&#32034;&#65292;&#25552;&#21462;&#25152;&#38656;&#20851;&#31995;&#30340;&#22810;&#26679;&#20934;&#30830;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#25628;&#32034;&#21644;&#37325;&#26032;&#35780;&#20998;&#30340;&#26426;&#21046;&#65292;&#35753;&#36825;&#20010;&#26041;&#27861;&#26356;&#21152;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is crucial to automatically construct knowledge graphs (KGs) of diverse new relations to support knowledge discovery and broad applications. Previous KG construction methods, based on either crowdsourcing or text mining, are often limited to a small predefined set of relations due to manual cost or restrictions in text corpus. Recent research proposed to use pretrained language models (LMs) as implicit knowledge bases that accept knowledge queries with prompts. Yet, the implicit knowledge lacks many desirable properties of a full-scale symbolic KG, such as easy access, navigation, editing, and quality assurance. In this paper, we propose a new approach of harvesting massive KGs of arbitrary relations from pretrained LMs. With minimal input of a relation definition (a prompt and a few shot of example entity pairs), the approach efficiently searches in the vast entity pair space to extract diverse accurate knowledge of the desired relation. We develop an effective search-and-rescore m
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35821;&#35328;&#36164;&#28304;&#38598;&#21512;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#29289;&#29702;&#23398;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#35299;&#37322;&#36830;&#36143;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#35770;&#25991;&#20998;&#26512;&#29289;&#29702;&#35805;&#35821;&#20013;&#26368;&#24120;&#35265;&#30340;&#26041;&#31243;&#21644;&#23376;&#23398;&#31185;&#65292;&#24182;&#25552;&#20986;&#20102;&#34920;&#29616;&#36739;&#24046;&#30340;&#22522;&#20934;&#65292;&#21363;&#20351;&#35757;&#32451;&#20102;&#25968;&#23398;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#24403;&#20195;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#22312;&#29289;&#29702;&#30456;&#20851;&#30340;&#36830;&#36143;&#24615;&#20219;&#21153;&#20013;&#38754;&#20020;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2201.04275</link><description>&lt;p&gt;
PhysNLU&#65306;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#29289;&#29702;&#23398;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#35299;&#37322;&#36830;&#36143;&#24615;&#30340;&#35821;&#35328;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
PhysNLU: A Language Resource for Evaluating Natural Language Understanding and Explanation Coherence in Physics. (arXiv:2201.04275v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.04275
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35821;&#35328;&#36164;&#28304;&#38598;&#21512;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#29289;&#29702;&#23398;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#35299;&#37322;&#36830;&#36143;&#24615;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#35770;&#25991;&#20998;&#26512;&#29289;&#29702;&#35805;&#35821;&#20013;&#26368;&#24120;&#35265;&#30340;&#26041;&#31243;&#21644;&#23376;&#23398;&#31185;&#65292;&#24182;&#25552;&#20986;&#20102;&#34920;&#29616;&#36739;&#24046;&#30340;&#22522;&#20934;&#65292;&#21363;&#20351;&#35757;&#32451;&#20102;&#25968;&#23398;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#24403;&#20195;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#22312;&#29289;&#29702;&#30456;&#20851;&#30340;&#36830;&#36143;&#24615;&#20219;&#21153;&#20013;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35753;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24110;&#21161;&#29289;&#29702;&#30740;&#31350;&#65292;&#23427;&#20204;&#24517;&#39035;&#39318;&#20808;&#23545;&#25968;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#35805;&#35821;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#23454;&#29616;&#36830;&#36143;&#30340;&#35299;&#37322;&#65292;&#27491;&#30830;&#25490;&#24207;&#21644;&#38472;&#36848;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#32452;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#34913;&#37327;&#20102;&#21477;&#23376;&#25490;&#24207;&#65292;&#20301;&#32622;&#65292;&#37096;&#20998;&#39044;&#27979;&#21644;&#35805;&#35821;&#36830;&#36143;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25968;&#25454;&#20998;&#26512;&#25581;&#31034;&#20102;&#22312;&#29289;&#29702;&#35805;&#35821;&#20013;&#26368;&#24120;&#35265;&#30340;&#26041;&#31243;&#21644;&#23376;&#23398;&#31185;&#65292;&#20197;&#21450;&#26041;&#31243;&#21644;&#34920;&#36798;&#24335;&#22312;&#21477;&#23376;&#23618;&#38754;&#19978;&#30340;&#39057;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20934;&#65292;&#35777;&#26126;&#20102;&#24403;&#22312;&#25968;&#23398;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#24471;&#21040;&#35757;&#32451;&#26102;&#65292;&#21363;&#20351;&#26159;&#24403;&#20195;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#20250;&#22312;&#29289;&#29702;&#30456;&#20851;&#30340;&#36830;&#36143;&#24615;&#20219;&#21153;&#20013;&#36935;&#21040;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order for language models to aid physics research, they must first encode representations of mathematical and natural language discourse which lead to coherent explanations, with correct ordering and relevance of statements. We present a collection of datasets developed to evaluate the performance of language models in this regard, which measure capabilities with respect to sentence ordering, position, section prediction, and discourse coherence. Analysis of the data reveals equations and sub-disciplines which are most common in physics discourse, as well as the sentence-level frequency of equations and expressions. We present baselines that demonstrate how contemporary language models are challenged by coherence related tasks in physics, even when trained on mathematical natural language objectives.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24773;&#24863;&#21644;&#21477;&#23376;&#31867;&#22411;&#30340;&#26041;&#27861;&#65292;&#23558;&#21407;&#22987;&#30340;YouTube&#35780;&#35770;&#20998;&#31867;&#65292;&#20197;&#24110;&#21161;YouTuber&#25214;&#21040;&#26356;&#30456;&#20851;&#30340;&#35780;&#35770;&#65292;&#20174;&#32780;&#22686;&#21152;&#20854;&#35266;&#20247;&#32676;&#12290;</title><link>http://arxiv.org/abs/2111.01908</link><description>&lt;p&gt;
&#22522;&#20110;&#24773;&#24863;&#21644;&#21477;&#23376;&#31867;&#22411;&#23545;YouTube&#35780;&#35770;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying YouTube Comments Based on Sentiment and Type of Sentence. (arXiv:2111.01908v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.01908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24773;&#24863;&#21644;&#21477;&#23376;&#31867;&#22411;&#30340;&#26041;&#27861;&#65292;&#23558;&#21407;&#22987;&#30340;YouTube&#35780;&#35770;&#20998;&#31867;&#65292;&#20197;&#24110;&#21161;YouTuber&#25214;&#21040;&#26356;&#30456;&#20851;&#30340;&#35780;&#35770;&#65292;&#20174;&#32780;&#22686;&#21152;&#20854;&#35266;&#20247;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;YouTube&#39057;&#36947;&#30340;&#22686;&#38271;&#65292;&#27599;&#20010;&#35270;&#39057;&#37117;&#21487;&#33021;&#25910;&#38598;&#22823;&#37327;&#35780;&#35770;&#65292;&#36825;&#20123;&#35780;&#35770;&#26159;&#29702;&#35299;&#35266;&#20247;&#26399;&#26395;&#21644;&#25913;&#21892;&#39057;&#36947;&#21442;&#19982;&#24230;&#30340;&#20027;&#35201;&#25163;&#27573;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35780;&#35770;&#21482;&#20195;&#34920;&#29992;&#25143;&#20851;&#20110;&#39057;&#36947;&#21644;&#20869;&#23481;&#30340;&#19968;&#33324;&#35266;&#28857;&#12290;&#35768;&#22810;&#35780;&#35770;&#26500;&#36896;&#36739;&#24046;&#65292;&#29712;&#30862;&#24182;&#19988;&#23384;&#22312;&#25340;&#20889;&#21644;&#35821;&#27861;&#38169;&#35823;&#65292;&#22240;&#27492;&#65292;&#35782;&#21035;&#26368;&#33021;&#21560;&#24341;&#20869;&#23481;&#21019;&#20316;&#32773;&#30340;&#35780;&#35770;&#26159;&#19968;&#39033;&#32321;&#29712;&#30340;&#24037;&#20316;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24773;&#24863;&#21644;&#21477;&#23376;&#31867;&#22411;&#23558;&#21407;&#22987;&#35780;&#35770;&#20998;&#31867;&#65292;&#20197;&#24110;&#21161;YouTuber&#25214;&#21040;&#26356;&#30456;&#20851;&#30340;&#35780;&#35770;&#65292;&#20174;&#32780;&#22686;&#21152;&#20854;&#35266;&#20247;&#32676;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a YouTube channel grows, each video can potentially collect enormous amounts of comments that provide direct feedback from the viewers. These comments are a major means of understanding viewer expectations and improving channel engagement. However, the comments only represent a general collection of user opinions about the channel and the content. Many comments are poorly constructed, trivial, and have improper spellings and grammatical errors. As a result, it is a tedious job to identify the comments that best interest the content creators. In this paper, we extract and classify the raw comments into different categories based on both sentiment and sentence types that will help YouTubers find relevant comments for growing their viewership. Existing studies have focused either on sentiment analysis (positive and negative) or classification of sub-types within the same sentence types (e.g., types of questions) on a text corpus. These have limited application on non-traditional text c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21477;&#27861;&#24863;&#30693;&#30340;&#22270;&#36716;&#25442;&#22120;&#27169;&#22411;&#29992;&#20110;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#23558;&#21477;&#27861;&#32467;&#26500;&#20197;&#23884;&#20837;&#30340;&#26041;&#24335;&#36755;&#20837;&#21040;Transformer&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#20013;&#65292;&#36798;&#21040;&#20102;&#27604;&#20197;&#24448;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2104.07704</link><description>&lt;p&gt;
&#21477;&#27861;&#24863;&#30693;&#30340;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Syntax-Aware Graph-to-Graph Transformer for Semantic Role Labelling. (arXiv:2104.07704v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.07704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21477;&#27861;&#24863;&#30693;&#30340;&#22270;&#36716;&#25442;&#22120;&#27169;&#22411;&#29992;&#20110;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#23558;&#21477;&#27861;&#32467;&#26500;&#20197;&#23884;&#20837;&#30340;&#26041;&#24335;&#36755;&#20837;&#21040;Transformer&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#20013;&#65292;&#36798;&#21040;&#20102;&#27604;&#20197;&#24448;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#27169;&#22411;&#34920;&#26126;&#65292;&#23558;&#21477;&#27861;&#30693;&#35782;&#32435;&#20837;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65288;SRL&#65289;&#20219;&#21153;&#21487;&#20197;&#26174;&#30528;&#25913;&#21892;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Syntax-aware Graph-to-Graph Transformer&#65288;SynG2G-Tr&#65289;&#30340;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#23558;&#22270;&#20851;&#31995;&#36716;&#25442;&#20026;&#23884;&#20837;&#65292;&#30452;&#25509;&#36755;&#20837;&#21040;Transformer&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#20013;&#32534;&#30721;&#21477;&#27861;&#32467;&#26500;&#12290;&#36825;&#31181;&#26041;&#27861;&#20026;&#36981;&#24490;&#21477;&#27861;&#32467;&#26500;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#22686;&#21152;&#20102;&#26580;&#24615;&#20559;&#24046;&#65292;&#20294;&#20063;&#20801;&#35768;&#27169;&#22411;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#23398;&#20064;&#26367;&#20195;&#27169;&#24335;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;&#36328;&#24230;&#21644;&#22522;&#20110;&#20381;&#23384;&#30340;SRL&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#22312;CoNLL 2005&#21644; CoNLL 2009&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#22312;&#20869;&#22806;&#39046;&#22495;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#20248;&#20110;&#20808;&#21069;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent models have shown that incorporating syntactic knowledge into the semantic role labelling (SRL) task leads to a significant improvement. In this paper, we propose Syntax-aware Graph-to-Graph Transformer (SynG2G-Tr) model, which encodes the syntactic structure using a novel way to input graph relations as embeddings, directly into the self-attention mechanism of Transformer. This approach adds a soft bias towards attention patterns that follow the syntactic structure but also allows the model to use this information to learn alternative patterns. We evaluate our model on both span-based and dependency-based SRL datasets, and outperform previous alternative methods in both in-domain and out-of-domain settings, on CoNLL 2005 and CoNLL 2009 datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#23485;&#24120;&#29992;&#21442;&#25968;&#20849;&#20139;&#25216;&#26415;&#30340;Transformer&#21442;&#25968;&#20849;&#20139;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#26102;&#38388;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#19977;&#31181;&#31574;&#30053;&#26469;&#20998;&#37197;&#27599;&#20010;&#23618;&#30340;&#21442;&#25968;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#21442;&#25968;&#22823;&#23567;&#21644;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#22312;&#20351;&#29992;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#37197;&#32622;&#20013;&#21516;&#26679;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2104.06022</link><description>&lt;p&gt;
Transformer&#23618;&#38388;&#21442;&#25968;&#20849;&#20139;&#30340;&#25945;&#35757;&#65288;arXiv&#65306;2104.06022v4 [cs.CL]&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Lessons on Parameter Sharing across Layers in Transformers. (arXiv:2104.06022v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.06022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#23485;&#24120;&#29992;&#21442;&#25968;&#20849;&#20139;&#25216;&#26415;&#30340;Transformer&#21442;&#25968;&#20849;&#20139;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#26102;&#38388;&#30340;&#25928;&#29575;&#65292;&#36890;&#36807;&#19977;&#31181;&#31574;&#30053;&#26469;&#20998;&#37197;&#27599;&#20010;&#23618;&#30340;&#21442;&#25968;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#21442;&#25968;&#22823;&#23567;&#21644;&#35745;&#31639;&#26102;&#38388;&#65292;&#24182;&#22312;&#20351;&#29992;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#37197;&#32622;&#20013;&#21516;&#26679;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Transformer&#21442;&#25968;&#20849;&#20139;&#26041;&#27861;&#65288;Vaswani&#31561;&#20154;&#65292;2017&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25918;&#23485;&#20102;&#19968;&#31181;&#34987;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#21363;&#23558;&#19968;&#20010;&#23618;&#30340;&#21442;&#25968;&#19982;&#25152;&#26377;&#23618;&#20849;&#20139;&#65292;&#22914;&#36890;&#29992;Transformer&#65288;Dehghani&#31561;&#20154;&#65292;2019&#65289;&#65292;&#20197;&#22686;&#21152;&#35745;&#31639;&#26102;&#38388;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#65306;&#24207;&#21015;&#12289;&#24490;&#29615;&#21644;&#24490;&#29615;&#65288;&#21453;&#21521;&#65289;&#26469;&#20998;&#37197;&#27599;&#20010;&#23618;&#30340;&#21442;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#22312;&#21442;&#25968;&#22823;&#23567;&#21644;&#35745;&#31639;&#26102;&#38388;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#22312;&#20351;&#29992;&#35768;&#22810;&#35757;&#32451;&#25968;&#25454;&#30340;&#37197;&#32622;&#20013;&#20063;&#26159;&#26377;&#25928;&#30340;&#65292;&#20363;&#22914;&#26368;&#36817;&#30340;WMT&#27604;&#36187;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a parameter sharing method for Transformers (Vaswani et al., 2017). The proposed approach relaxes a widely used technique, which shares parameters for one layer with all layers such as Universal Transformers (Dehghani et al., 2019), to increase the efficiency in the computational time. We propose three strategies: Sequence, Cycle, and Cycle (rev) to assign parameters to each layer. Experimental results show that the proposed strategies are efficient in the parameter size and computational time. Moreover, we indicate that the proposed strategies are also effective in the configuration where we use many training data such as the recent WMT competition.
&lt;/p&gt;</description></item></channel></rss>