<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#21033;&#29992;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#24182;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#23558;&#20854;&#36866;&#24212;&#21040;&#29790;&#22763;&#24503;&#35821;&#65292;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#20165;&#36890;&#36807;&#28155;&#21152;&#29790;&#22763;&#24503;&#35821;&#36866;&#37197;&#22120;&#21040;&#27169;&#22359;&#21270;&#32534;&#30721;&#22120;&#65292;&#21363;&#21487;&#33719;&#24471;&#23436;&#20840;&#36866;&#24212;&#24615;&#30340;97.5%&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#29790;&#22763;&#24503;&#35821;&#21477;&#23376;&#26816;&#32034;&#20219;&#21153;&#20013;&#65292;&#36866;&#24212;&#23383;&#31526;&#32423;&#27169;&#22411;&#27604;&#20854;&#20182;&#36866;&#24212;&#31574;&#30053;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.14400</link><description>&lt;p&gt;
&#23558;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#27169;&#22359;&#21270;&#22320;&#36866;&#24212;&#29790;&#22763;&#24503;&#35821;&#26041;&#35328;
&lt;/p&gt;
&lt;p&gt;
Modular Adaptation of Multilingual Encoders to Written Swiss German Dialect. (arXiv:2401.14400v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#24182;&#36890;&#36807;&#25345;&#32493;&#39044;&#35757;&#32451;&#23558;&#20854;&#36866;&#24212;&#21040;&#29790;&#22763;&#24503;&#35821;&#65292;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#20165;&#36890;&#36807;&#28155;&#21152;&#29790;&#22763;&#24503;&#35821;&#36866;&#37197;&#22120;&#21040;&#27169;&#22359;&#21270;&#32534;&#30721;&#22120;&#65292;&#21363;&#21487;&#33719;&#24471;&#23436;&#20840;&#36866;&#24212;&#24615;&#30340;97.5%&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#29790;&#22763;&#24503;&#35821;&#21477;&#23376;&#26816;&#32034;&#20219;&#21153;&#20013;&#65292;&#36866;&#24212;&#23383;&#31526;&#32423;&#27169;&#22411;&#27604;&#20854;&#20182;&#36866;&#24212;&#31574;&#30053;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#21644;&#26041;&#35328;&#21464;&#21270;&#65292;&#24314;&#31435;&#36866;&#29992;&#20110;&#29790;&#22763;&#24503;&#35821;&#30340;&#31070;&#32463;&#25991;&#26412;&#32534;&#30721;&#22120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#22312;&#29616;&#26377;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#22522;&#30784;&#19978;&#65292;&#20351;&#29992;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#23558;&#20854;&#36866;&#24212;&#21040;&#29790;&#22763;&#24503;&#35821;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#34920;&#26126;&#65292;&#20165;&#28155;&#21152;&#29790;&#22763;&#24503;&#35821;&#36866;&#37197;&#22120;&#21040;&#27169;&#22359;&#21270;&#32534;&#30721;&#22120;&#33021;&#22815;&#36798;&#21040;&#23436;&#20840;&#25972;&#20307;&#36866;&#24212;&#24615;&#24615;&#33021;&#30340;97.5%&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;&#22312;&#32473;&#23450;&#26631;&#20934;&#24503;&#35821;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#26816;&#32034;&#29790;&#22763;&#24503;&#35821;&#21477;&#23376;&#30340;&#20219;&#21153;&#20013;&#65292;&#36866;&#24212;&#23383;&#31526;&#32423;&#27169;&#22411;&#27604;&#20854;&#20182;&#36866;&#24212;&#31574;&#30053;&#26356;&#26377;&#25928;&#12290;&#25105;&#20204;&#22312;https://github.com/ZurichNLP/swiss-german-text-encoders&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creating neural text encoders for written Swiss German is challenging due to a dearth of training data combined with dialectal variation. In this paper, we build on several existing multilingual encoders and adapt them to Swiss German using continued pre-training. Evaluation on three diverse downstream tasks shows that simply adding a Swiss German adapter to a modular encoder achieves 97.5% of fully monolithic adaptation performance. We further find that for the task of retrieving Swiss German sentences given Standard German queries, adapting a character-level model is more effective than the other adaptation strategies. We release our code and the models trained for our experiments at https://github.com/ZurichNLP/swiss-german-text-encoders
&lt;/p&gt;</description></item><item><title>TURNA&#26159;&#19968;&#31181;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#22791;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#33021;&#21147;&#12290;TURNA&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;-&#35299;&#30721;&#26550;&#26500;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#33021;&#19982;&#22303;&#32819;&#20854;&#35821;&#21333;&#35821;&#27169;&#22411;&#31454;&#20105;&#12290;</title><link>http://arxiv.org/abs/2401.14373</link><description>&lt;p&gt;
TURNA: &#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#22303;&#32819;&#20854;&#32534;&#30721;-&#35299;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation. (arXiv:2401.14373v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14373
&lt;/p&gt;
&lt;p&gt;
TURNA&#26159;&#19968;&#31181;&#29992;&#20110;&#22303;&#32819;&#20854;&#35821;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#22791;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#33021;&#21147;&#12290;TURNA&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;-&#35299;&#30721;&#26550;&#26500;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#33021;&#19982;&#22303;&#32819;&#20854;&#35821;&#21333;&#35821;&#27169;&#22411;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#20027;&#35201;&#20559;&#21521;&#20110;&#36164;&#28304;&#20016;&#23500;&#19988;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#65292;&#36825;&#23548;&#33268;&#20102;&#19982;&#36164;&#28304;&#31232;&#32570;&#30340;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TURNA&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#38024;&#23545;&#36164;&#28304;&#31232;&#32570;&#30340;&#22303;&#32819;&#20854;&#35821;&#24320;&#21457;&#65292;&#33021;&#22815;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;TURNA&#20351;&#29992;&#22522;&#20110;&#32479;&#19968;&#26694;&#26550;UL2&#30340;&#32534;&#30721;-&#35299;&#30721;&#26550;&#26500;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#25105;&#20204;&#19987;&#38376;&#20026;&#27492;&#30446;&#30340;&#31579;&#36873;&#20102;&#19968;&#20010;&#22810;&#26679;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#23545;TURNA&#22312;&#22303;&#32819;&#20854;&#35821;&#30340;&#19977;&#20010;&#29983;&#25104;&#20219;&#21153;&#21644;&#20116;&#20010;&#29702;&#35299;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TURNA&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#20248;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#19982;&#22303;&#32819;&#20854;&#35821;&#21333;&#35821;&#27169;&#22411;&#22312;&#29702;&#35299;&#20219;&#21153;&#19978;&#31454;&#20105;&#12290;TURNA&#24050;&#22312;https://huggingface.co/boun-tabi-LMG/TURNA &#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in natural language processing have predominantly favored well-resourced English-centric models, resulting in a significant gap with low-resource languages. In this work, we introduce the language model TURNA, which is developed for the low-resource language Turkish and is capable of both natural language understanding and generation tasks. TURNA is pretrained with an encoder-decoder architecture based on the unified framework UL2 with a diverse corpus that we specifically curated for this purpose. We evaluated TURNA with three generation tasks and five understanding tasks for Turkish. The results show that TURNA outperforms several multilingual models in both understanding and generation tasks, and competes with monolingual Turkish models in understanding tasks. TURNA is made available at https://huggingface.co/boun-tabi-LMG/TURNA .
&lt;/p&gt;</description></item><item><title>Genie&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#20869;&#23481;&#23548;&#21521;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#23454;&#29616;&#65306;&#20869;&#23481;&#20934;&#22791;&#12289;&#29983;&#25104;&#21644;&#36807;&#28388;&#12290;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#34987;&#21457;&#29616;&#26159;&#33258;&#28982;&#19988;&#39640;&#36136;&#37327;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#20351;&#29992;&#20154;&#24037;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.14367</link><description>&lt;p&gt;
Genie&#65306;&#23454;&#29616;&#20869;&#23481;&#23548;&#21521;&#25968;&#25454;&#38598;&#29983;&#25104;&#30340;&#20154;&#31867;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
Genie: Achieving Human Parity in Content-Grounded Datasets Generation. (arXiv:2401.14367v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14367
&lt;/p&gt;
&lt;p&gt;
Genie&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#20869;&#23481;&#23548;&#21521;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#38454;&#27573;&#23454;&#29616;&#65306;&#20869;&#23481;&#20934;&#22791;&#12289;&#29983;&#25104;&#21644;&#36807;&#28388;&#12290;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;&#29983;&#25104;&#30340;&#25968;&#25454;&#34987;&#21457;&#29616;&#26159;&#33258;&#28982;&#19988;&#39640;&#36136;&#37327;&#30340;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#20351;&#29992;&#20154;&#24037;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#27604;&#36739;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20869;&#23481;&#23548;&#21521;&#29983;&#25104;&#20219;&#21153;&#65292;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#34987;&#35748;&#20026;&#26159;&#25512;&#21160;&#36825;&#20123;&#20219;&#21153;&#21457;&#23637;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Genie&#65292;&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#20869;&#23481;&#23548;&#21521;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#65288;a&#65289;&#20869;&#23481;&#20934;&#22791;&#65292;&#65288;b&#65289;&#29983;&#25104;&#65306;&#20174;&#20869;&#23481;&#20013;&#21019;&#24314;&#29305;&#23450;&#20219;&#21153;&#30340;&#31034;&#20363;&#65288;&#20363;&#22914;&#38382;&#39064;-&#31572;&#26696;&#23545;&#25110;&#25688;&#35201;&#65289;&#65292;&#65288;c&#65289;&#36807;&#28388;&#26426;&#21046;&#65292;&#26088;&#22312;&#30830;&#20445;&#29983;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#29983;&#25104;&#19977;&#20010;&#22823;&#35268;&#27169;&#30340;&#21512;&#25104;&#25968;&#25454;&#26469;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#65306;&#38271;&#22411;&#38382;&#39064;&#22238;&#31572;&#65288;LFQA&#65289;&#12289;&#25688;&#35201;&#21644;&#20449;&#24687;&#25552;&#21462;&#12290;&#22312;&#20154;&#31867;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#29983;&#25104;&#30340;&#25968;&#25454;&#34987;&#21457;&#29616;&#26159;&#33258;&#28982;&#19988;&#39640;&#36136;&#37327;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#19982;&#20351;&#29992;&#20154;&#24037;&#32534;&#20889;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739; - &#23545;&#20110;LFQA&#65292;&#25105;&#20204;&#19982;ELI5&#21644;ASQA&#36827;&#34892;&#27604;&#36739;&#65292;&#23545;&#20110;&#25688;&#35201;&#65292;&#25105;&#20204;&#19982;CNN-DailyMail&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#25110;&#36229;&#36807;&#20351;&#29992;&#20154;&#24037;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of high-quality data for content-grounded generation tasks has been identified as a major obstacle to advancing these tasks. To address this gap, we propose Genie, a novel method for automatically generating high-quality content-grounded data. It consists of three stages: (a) Content Preparation, (b) Generation: creating task-specific examples from the content (e.g., question-answer pairs or summaries). (c) Filtering mechanism aiming to ensure the quality and faithfulness of the generated data. We showcase this methodology by generating three large-scale synthetic data, making wishes, for Long-Form Question-Answering (LFQA), summarization, and information extraction. In a human evaluation, our generated data was found to be natural and of high quality. Furthermore, we compare models trained on our data with models trained on human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for Summarization. We show that our models are on par with or outperforming models trained 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20102;&#22122;&#22768;&#20943;&#23569;&#26041;&#27861;&#22312;&#22122;&#22768;&#23391;&#21152;&#25289;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#26356;&#36866;&#29992;&#30340;&#22122;&#22768;&#20943;&#23569;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.14360</link><description>&lt;p&gt;
&#22122;&#22768;&#23391;&#21152;&#25289;&#35821;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#20013;&#22122;&#22768;&#20943;&#23569;&#26041;&#27861;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis on Noisy Bengali Texts. (arXiv:2401.14360v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20102;&#22122;&#22768;&#20943;&#23569;&#26041;&#27861;&#22312;&#22122;&#22768;&#23391;&#21152;&#25289;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#26356;&#36866;&#29992;&#30340;&#22122;&#22768;&#20943;&#23569;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23391;&#21152;&#25289;&#35821;&#34987;&#35748;&#20026;&#26159;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#65292;&#20294;&#24773;&#24863;&#20998;&#26512;&#24050;&#32463;&#25104;&#20026;&#25991;&#29486;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#22122;&#22768;&#23391;&#21152;&#25289;&#25991;&#26412;&#39046;&#22495;&#65292;&#23545;&#24773;&#24863;&#20998;&#26512;&#30340;&#25506;&#32034;&#20173;&#28982;&#19981;&#36275;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;&#20154;&#24037;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65288;NC-SentNoB&#65289;&#65292;&#29992;&#20110;&#35782;&#21035;&#39044;&#23384;&#22312;&#30340;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#20013;&#22823;&#32422;15K&#20010;&#22122;&#22768;&#23391;&#21152;&#25289;&#25991;&#26412;&#20013;&#30340;&#21313;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#22768;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23558;&#36755;&#20837;&#22122;&#22768;&#25991;&#26412;&#21010;&#20998;&#20026;&#22810;&#20010;&#26631;&#31614;&#26469;&#35782;&#21035;&#22122;&#22768;&#31867;&#22411;&#65292;&#28982;&#21518;&#24341;&#20837;&#22522;&#32447;&#22122;&#22768;&#20943;&#23569;&#26041;&#27861;&#26469;&#20943;&#23569;&#22122;&#22768;&#65292;&#20197;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#38024;&#23545;&#22122;&#22768;&#21644;&#20943;&#23569;&#22122;&#22768;&#25991;&#26412;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36827;&#34892;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#20351;&#29992;&#30340;&#22122;&#22768;&#20943;&#23569;&#26041;&#27861;&#19981;&#23613;&#22914;&#20154;&#24847;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#26356;&#36866;&#29992;&#30340;&#22122;&#22768;&#20943;&#23569;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Bengali is considered a language with limited resources, sentiment analysis has been a subject of extensive research in the literature. Nevertheless, there is a scarcity of exploration into sentiment analysis specifically in the realm of noisy Bengali texts. In this paper, we introduce a dataset (NC-SentNoB) that we annotated manually to identify ten different types of noise found in a pre-existing sentiment analysis dataset comprising of around 15K noisy Bengali texts. At first, given an input noisy text, we identify the noise type, addressing this as a multi-label classification task. Then, we introduce baseline noise reduction methods to alleviate noise prior to conducting sentiment analysis. Finally, we assess the performance of fine-tuned sentiment analysis models with both noisy and noise-reduced texts to make comparisons. The experimental findings indicate that the noise reduction methods utilized are not satisfactory, highlighting the need for more suitable noise reductio
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2401.14295</link><description>&lt;p&gt;
&#25512;&#29702;&#30340;&#25299;&#25169;&#23398;&#65306;&#25581;&#31192;&#24605;&#32500;&#38142;&#12289;&#26641;&#21644;&#22270;
&lt;/p&gt;
&lt;p&gt;
Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14295
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24615;&#33021;&#26041;&#38754;&#12290;&#20854;&#20013;&#65292;&#19982;&#32467;&#26500;&#30456;&#32467;&#21512;&#30340;&#25552;&#31034;&#24037;&#31243;&#34987;&#35270;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#20854;&#35774;&#35745;&#22914;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#31561;&#65292;&#36890;&#36807;&#32467;&#26500;&#25351;&#23548;&#25972;&#20307;LLM&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#30340;&#35828;&#26126;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#36923;&#36753;&#25110;&#25968;&#23398;&#25512;&#29702;&#12289;&#35268;&#21010;&#25110;&#21019;&#36896;&#24615;&#20889;&#20316;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#26041;&#20415;&#29702;&#35299;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#24182;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#26041;&#26696;&#30340;&#36890;&#29992;&#34013;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#25552;&#31034;&#25191;&#34892;&#27969;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#28548;&#28165;&#24182;&#26126;&#30830;&#23450;&#20041;&#20102;&#19981;&#21516;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#25105;&#20204;&#24314;&#31435;&#31532;&#19968;&#20010;&#20998;&#31867;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxon
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32599;&#39532;&#21270;&#24418;&#24335;&#30340;&#25991;&#26412;&#20316;&#20026;&#25509;&#21475;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#21360;&#22320;&#35821;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32599;&#39532;&#21270;&#25991;&#26412;&#19981;&#20165;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#65292;&#36824;&#22312;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#19979;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#32599;&#39532;&#21270;&#26377;&#28508;&#21147;&#24357;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2401.14280</link><description>&lt;p&gt;
RomanSetu: &#36890;&#36807;&#32599;&#39532;&#21270;&#26377;&#25928;&#22320;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization. (arXiv:2401.14280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32599;&#39532;&#21270;&#24418;&#24335;&#30340;&#25991;&#26412;&#20316;&#20026;&#25509;&#21475;&#65292;&#26377;&#25928;&#22320;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#21360;&#22320;&#35821;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#32599;&#39532;&#21270;&#25991;&#26412;&#19981;&#20165;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#65292;&#36824;&#22312;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#19979;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#32599;&#39532;&#21270;&#26377;&#28508;&#21147;&#24357;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#38750;&#33521;&#35821;&#35821;&#35328;&#65288;&#29305;&#21035;&#26159;&#20351;&#29992;&#38750;&#25289;&#19969;&#23383;&#27597;&#34920;&#30340;&#35821;&#35328;&#65289;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#32599;&#39532;&#21270;&#24418;&#24335;&#30340;&#25991;&#26412;&#20316;&#20026;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25509;&#21475;&#65292;&#20551;&#35774;&#39057;&#32321;&#30340;&#38750;&#27491;&#24335;&#20351;&#29992;&#21644;&#19982;&#33521;&#35821;&#20849;&#20139;&#30340;&#26631;&#35760;&#26377;&#21161;&#20110;&#36328;&#35821;&#35328;&#23545;&#40784;&#12290;&#25105;&#20204;&#20197;&#21360;&#22320;&#35821;&#20026;&#37325;&#28857;&#65292;&#36890;&#36807;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#32763;&#35793;&#21644;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#35777;&#26126;&#32599;&#39532;&#21270;&#25991;&#26412;&#19981;&#20165;&#30001;&#20110;&#20854;&#36739;&#20302;&#30340;&#29983;&#20135;&#21147;&#32780;&#26174;&#33879;&#25913;&#21892;&#20102;&#25512;&#29702;&#25928;&#29575;&#65292;&#36824;&#22312;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#20013;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26032;&#39062;&#30340;&#22810;&#33050;&#26412;&#25552;&#31034;&#26041;&#27861;&#32467;&#21512;&#20102;&#32599;&#39532;&#21270;&#21644;&#21407;&#29983;&#25991;&#26412;&#65292;&#22312;&#36827;&#19968;&#27493;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#32599;&#39532;&#21270;&#22312;&#24357;&#21512;&#22823;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20013;&#30340;&#35821;&#35328;&#38556;&#30861;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#26410;&#26469;&#30340;&#24037;&#20316;&#23558;&#33268;&#21147;&#20110;&#23558;&#27492;&#26041;&#27861;&#25193;&#23637;&#21040;&#26356;&#22810;&#30340;&#35821;&#35328;&#21644;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study addresses the challenge of extending Large Language Models (LLMs) to non-English languages, specifically those using non-Latin scripts. We propose an innovative approach that utilizes the romanized form of text as an interface for LLMs, hypothesizing that its frequent informal use and shared tokens with English enhance cross-lingual alignment. Focusing on Hindi, we demonstrate through Hindi-to-English translation and sentiment analysis tasks that romanized text not only significantly improves inference efficiency due to its lower fertility compared to native text but also achieves competitive performance with limited pre-training. Additionally, our novel multi-script prompting approach, which combines romanized and native texts, shows promise in further enhancing task performance. These findings suggest the potential of romanization in bridging the language gap for LLM applications, with future work aimed at expanding this approach to more languages and tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;transformer&#32593;&#32476;&#21644;&#22823;&#33041;&#30382;&#23618;&#27874;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#30382;&#23618;&#27874;&#22312;&#25552;&#21462;&#24863;&#35273;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#26041;&#38754;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.14267</link><description>&lt;p&gt;
Transformers&#21644;&#22823;&#33041;&#30382;&#23618;&#27874;&#65306;&#22312;&#26102;&#38388;&#19978;&#20256;&#36882;&#19978;&#19979;&#25991;&#30340;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers and Cortical Waves: Encoders for Pulling In Context Across Time. (arXiv:2401.14267v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14267
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;transformer&#32593;&#32476;&#21644;&#22823;&#33041;&#30382;&#23618;&#27874;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#30382;&#23618;&#27874;&#22312;&#25552;&#21462;&#24863;&#35273;&#36755;&#20837;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#26041;&#38754;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;ChatGPT&#21644;&#20854;&#20182;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;transformer&#32593;&#32476;&#30340;&#33021;&#21147;&#24050;&#32463;&#24341;&#36215;&#20102;&#19990;&#30028;&#30340;&#20851;&#27880;&#12290;&#23427;&#20204;&#30340;&#24615;&#33021;&#20381;&#36182;&#20110;&#23558;&#23436;&#25972;&#30340;&#36755;&#20837;&#24207;&#21015;&#65288;&#20363;&#22914;&#21477;&#23376;&#20013;&#30340;&#25152;&#26377;&#21333;&#35789;&#65289;&#36716;&#21270;&#20026;&#19968;&#20010;&#38271;&#30340;&#8220;&#32534;&#30721;&#21521;&#37327;&#8221;&#65292;&#20351;&#24471;transformer&#33021;&#22815;&#23398;&#20064;&#33258;&#28982;&#24207;&#21015;&#20013;&#30340;&#38271;&#31243;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#8220;&#33258;&#27880;&#24847;&#21147;&#8221;&#24212;&#29992;&#20110;&#36825;&#20010;&#32534;&#30721;&#21521;&#37327;&#65292;&#36890;&#36807;&#35745;&#31639;&#36755;&#20837;&#24207;&#21015;&#20013;&#21333;&#35789;&#23545;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#22686;&#24378;&#20102;transformer&#20013;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#35748;&#20026;&#31070;&#32463;&#27963;&#21160;&#22312;&#21333;&#20010;&#30382;&#23618;&#21306;&#22495;&#20869;&#25110;&#25972;&#20010;&#22823;&#33041;&#33539;&#22260;&#20869;&#20256;&#25773;&#30340;&#27874;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#32534;&#30721;&#21407;&#29702;&#12290;&#36890;&#36807;&#22312;&#27599;&#20010;&#26102;&#21051;&#23558;&#26368;&#36817;&#30340;&#36755;&#20837;&#21382;&#21490;&#23553;&#35013;&#20026;&#21333;&#20010;&#31354;&#38388;&#27169;&#24335;&#65292;&#30382;&#23618;&#27874;&#21487;&#20197;&#20174;&#24863;&#35273;&#36755;&#20837;&#24207;&#21015;&#20013;&#25552;&#21462;&#26102;&#38388;&#19978;&#19979;&#25991;&#65292;&#36825;&#19982;&#35745;&#31639;&#21407;&#29702;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capabilities of transformer networks such as ChatGPT and other Large Language Models (LLMs) have captured the world's attention. The crucial computational mechanism underlying their performance relies on transforming a complete input sequence - for example, all the words in a sentence into a long "encoding vector" - that allows transformers to learn long-range temporal dependencies in naturalistic sequences. Specifically, "self-attention" applied to this encoding vector enhances temporal context in transformers by computing associations between pairs of words in the input sequence. We suggest that waves of neural activity, traveling across single cortical regions or across multiple regions at the whole-brain scale, could implement a similar encoding principle. By encapsulating recent input history into a single spatial pattern at each moment in time, cortical waves may enable temporal context to be extracted from sequences of sensory inputs, the same computational principle used in
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21644;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#21319;&#20102;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#20004;&#20010;&#27169;&#22359;&#65292;AttentionExtractor&#29992;&#20110;&#25552;&#21462;&#20851;&#38190;&#30701;&#35821;&#65292;AttentionCoder&#21033;&#29992;&#36825;&#20123;&#30701;&#35821;&#29983;&#25104;&#30446;&#26631;&#20195;&#30721;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14242</link><description>&lt;p&gt;
&#25552;&#21319;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Natural Language Capability of Code Large Language Model. (arXiv:2401.14242v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14242
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#34701;&#21512;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#21644;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#21319;&#20102;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#20004;&#20010;&#27169;&#22359;&#65292;AttentionExtractor&#29992;&#20110;&#25552;&#21462;&#20851;&#38190;&#30701;&#35821;&#65292;AttentionCoder&#21033;&#29992;&#36825;&#20123;&#30701;&#35821;&#29983;&#25104;&#30446;&#26631;&#20195;&#30721;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Code LLMs&#65289;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#32489;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#37096;&#20998;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#25552;&#21319;&#20195;&#30721;LLMs&#30340;&#32534;&#31243;&#33021;&#21147;&#26041;&#38754;&#65292;&#32780;&#23545;&#20854;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#20851;&#27880;&#36739;&#23569;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;AttentionExtractor&#65292;&#36127;&#36131;&#20174;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#38656;&#27714;&#20013;&#25552;&#21462;&#20851;&#38190;&#30701;&#35821;&#65292;&#21644;AttentionCoder&#65292;&#21033;&#29992;&#36825;&#20123;&#25552;&#21462;&#20986;&#30340;&#30701;&#35821;&#29983;&#25104;&#30446;&#26631;&#20195;&#30721;&#26469;&#35299;&#20915;&#38656;&#27714;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26080;&#32541;&#34701;&#21512;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#19982;&#20195;&#30721;LLMs&#24320;&#21019;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#24605;&#36335;&#12290;&#20026;&#20102;&#39564;&#35777;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#65292;&#21517;&#20026;MultiNL-H&#65292;&#28085;&#30422;&#20102;&#20116;&#31181;&#33258;&#28982;&#35821;&#35328;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code large language models (Code LLMs) have demonstrated remarkable performance in code generation. Nonetheless, most existing works focus on boosting code LLMs from the perspective of programming capabilities, while their natural language capabilities receive less attention. To fill this gap, we thus propose a novel framework, comprising two modules: AttentionExtractor, which is responsible for extracting key phrases from the user's natural language requirements, and AttentionCoder, which leverages these extracted phrases to generate target code to solve the requirement. This framework pioneers an innovative idea by seamlessly integrating code LLMs with traditional natural language processing tools. To validate the effectiveness of the framework, we craft a new code generation benchmark, called MultiNL-H, covering five natural languages. Extensive experimental results demonstrate the effectiveness of our proposed framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#30340;&#26631;&#27880;&#26041;&#27861;&#23545;Reddit&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#32454;&#35843;&#20102;Longformer&#27169;&#22411;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#33521;&#35821;&#21644;&#21346;&#24178;&#36798;&#35821;&#30340;&#25233;&#37057;&#30151;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.14240</link><description>&lt;p&gt;
&#22522;&#20110;Reddit&#25991;&#26412;&#22686;&#24378;&#26631;&#27880;&#25216;&#26415;&#21644;&#32454;&#35843;Longformer&#27169;&#22411;&#30340;&#33521;&#35821;&#21644;&#21346;&#24178;&#36798;&#25233;&#37057;&#30151;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhanced Labeling Technique for Reddit Text and Fine-Tuned Longformer Models for Classifying Depression Severity in English and Luganda. (arXiv:2401.14240v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#30340;&#26631;&#27880;&#26041;&#27861;&#23545;Reddit&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#32454;&#35843;&#20102;Longformer&#27169;&#22411;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#33521;&#35821;&#21644;&#21346;&#24178;&#36798;&#35821;&#30340;&#25233;&#37057;&#30151;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25233;&#37057;&#30151;&#26159;&#20840;&#29699;&#36127;&#25285;&#37325;&#30340;&#12289;&#38590;&#20197;&#25511;&#21046;&#30340;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#20043;&#19968;&#12290;&#19987;&#23478;&#20204;&#21487;&#20197;&#20351;&#29992;&#36125;&#20811;&#25233;&#37057;&#37327;&#34920;&#65288;BDI&#65289;&#38382;&#21367;&#26089;&#26399;&#26816;&#27979;&#20854;&#20005;&#37325;&#31243;&#24230;&#65292;&#32473;&#24739;&#32773;&#26045;&#29992;&#36866;&#24403;&#33647;&#29289;&#65292;&#38459;&#27490;&#20854;&#36827;&#23637;&#12290;&#30001;&#20110;&#25285;&#24515;&#21487;&#33021;&#30340;&#27745;&#21517;&#21270;&#65292;&#35768;&#22810;&#24739;&#32773;&#36716;&#21521;Reddit&#31561;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#23547;&#27714;&#24314;&#35758;&#21644;&#24110;&#21161;&#12290;&#26412;&#30740;&#31350;&#20174;Reddit&#25552;&#21462;&#25991;&#26412;&#20197;&#20419;&#36827;&#35786;&#26029;&#36807;&#31243;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#25552;&#20986;&#30340;&#26631;&#27880;&#26041;&#27861;&#23545;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#38543;&#21518;&#23545;Longformer&#27169;&#22411;&#36827;&#34892;&#20102;&#32454;&#35843;&#12290;&#23558;&#35813;&#27169;&#22411;&#19982;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#26420;&#32032;&#36125;&#21494;&#26031;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#26799;&#24230;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;Longformer&#27169;&#22411;&#22312;&#33521;&#35821;&#65288;48%&#65289;&#21644;&#21346;&#24178;&#36798;&#35821;&#65288;45%&#65289;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Depression is a global burden and one of the most challenging mental health conditions to control. Experts can detect its severity early using the Beck Depression Inventory (BDI) questionnaire, administer appropriate medication to patients, and impede its progression. Due to the fear of potential stigmatization, many patients turn to social media platforms like Reddit for advice and assistance at various stages of their journey. This research extracts text from Reddit to facilitate the diagnostic process. It employs a proposed labeling approach to categorize the text and subsequently fine-tunes the Longformer model. The model's performance is compared against baseline models, including Naive Bayes, Random Forest, Support Vector Machines, and Gradient Boosting. Our findings reveal that the Longformer model outperforms the baseline models in both English (48%) and Luganda (45%) languages on a custom-made dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22359;&#30340;&#21487;&#31227;&#26893;&#24615;&#65292;&#21457;&#29616;&#36825;&#20123;&#31227;&#26893;&#30340;&#27169;&#22359;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#37325;&#22797;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2401.14228</link><description>&lt;p&gt;
&#35780;&#20272;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#35757;&#32451;&#30340;&#21442;&#25968;&#30697;&#38453;&#30340;&#21487;&#31227;&#26893;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing the Portability of Parameter Matrices Trained by Parameter-Efficient Finetuning Methods. (arXiv:2401.14228v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#35757;&#32451;&#30340;&#27169;&#22359;&#30340;&#21487;&#31227;&#26893;&#24615;&#65292;&#21457;&#29616;&#36825;&#20123;&#31227;&#26893;&#30340;&#27169;&#22359;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#37325;&#22797;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35757;&#32451;&#35268;&#27169;&#36234;&#26469;&#36234;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#26412;&#22686;&#21152;&#65292;&#23545;&#37325;&#22797;&#21033;&#29992;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#20852;&#36259;&#20063;&#22312;&#22686;&#21152;&#12290;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#34920;&#26126;&#65292;&#37325;&#22797;&#21033;&#29992;&#38750;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#21487;&#20197;&#24110;&#21161;&#21518;&#32493;&#29305;&#23450;&#20219;&#21153;&#23398;&#20064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#30456;&#21453;&#30340;&#24773;&#20917;&#65306;&#23558;&#20174;&#19968;&#20010;&#27169;&#22411;&#31227;&#26893;&#32534;&#30721;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#30340;&#23436;&#25972;&#21151;&#33021;&#27169;&#22359;&#21040;&#21478;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#39033;&#21253;&#25324;1,440&#20010;&#35757;&#32451;/&#27979;&#35797;&#36816;&#34892;&#30340;&#30740;&#31350;&#65292;&#20197;&#27979;&#35797;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;(PEFT)&#25216;&#26415;&#35757;&#32451;&#30340;&#27169;&#22359;&#30340;&#21487;&#31227;&#26893;&#24615;&#65292;&#20197;&#24773;&#24863;&#20998;&#26512;&#20026;&#31034;&#20363;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#27979;&#35797;&#20102;&#21487;&#31227;&#26893;&#24615;&#65292;&#28041;&#21450;&#19981;&#21516;&#30340;PEFT&#25216;&#26415;&#21644;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#20027;&#26426;&#27169;&#22411;&#65292;&#31561;&#31561;&#12290;&#25105;&#20204;&#23558;&#31227;&#26893;&#30340;&#27169;&#22359;&#30340;&#24615;&#33021;&#19982;(i)&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#30456;&#31561;&#27169;&#22359;&#30340;&#24615;&#33021;&#21644;(ii)&#20174;&#19982;&#31227;&#26893;&#30340;&#27169;&#22359;&#30456;&#21516;&#20998;&#24067;&#30340;&#21442;&#25968;&#20013;&#37319;&#26679;&#35757;&#32451;&#30340;&#27169;&#22359;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31227;&#26893;&#30340;&#27169;&#22359;&#30340;&#24615;&#33021;&#36828;&#36828;&#36229;&#36807;&#25152;&#27979;&#35797;&#30340;&#20004;&#31181;&#26367;&#20195;&#26041;&#26696;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#27880;&#24847;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the cost of training ever larger language models has grown, so has the interest in reusing previously learnt knowledge. Transfer learning methods have shown how reusing non-task-specific knowledge can help in subsequent task-specific learning. In this paper, we investigate the inverse: porting whole functional modules that encode task-specific knowledge from one model to another. We designed a study comprising 1,440 training/testing runs to test the portability of modules trained by parameter-efficient finetuning (PEFT) techniques, using sentiment analysis as an example task. We test portability in a wide range of scenarios, involving different PEFT techniques and different pretrained host models, among other dimensions. We compare the performance of ported modules with that of equivalent modules trained (i) from scratch, and (ii) from parameters sampled from the same distribution as the ported module. We find that the ported modules far outperform the two alternatives tested, but t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#38271;&#26399;&#23545;&#35805;&#20013;&#35282;&#33394;&#21477;&#23376;&#19981;&#20855;&#20449;&#24687;&#24615;&#30340;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24120;&#35782;&#22686;&#24378;&#30340;&#35282;&#33394;&#25193;&#23637;&#65292;&#24182;&#35774;&#35745;&#31574;&#30053;&#23558;&#30456;&#20114;&#30683;&#30462;&#30340;&#35282;&#33394;&#36716;&#21270;&#20026;&#21253;&#21547;&#20016;&#23500;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#21477;&#23376;&#65292;&#20197;&#25552;&#39640;&#22238;&#24212;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.14215</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#24863;&#30693;&#20010;&#24615;&#21270;&#32454;&#21270;&#65292;&#22686;&#24378;&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#24120;&#35782;&#22686;&#24378;&#24615;&#20869;&#23384;&#26500;&#24314;&#21644;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement. (arXiv:2401.14215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#38271;&#26399;&#23545;&#35805;&#20013;&#35282;&#33394;&#21477;&#23376;&#19981;&#20855;&#20449;&#24687;&#24615;&#30340;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24120;&#35782;&#22686;&#24378;&#30340;&#35282;&#33394;&#25193;&#23637;&#65292;&#24182;&#35774;&#35745;&#31574;&#30053;&#23558;&#30456;&#20114;&#30683;&#30462;&#30340;&#35282;&#33394;&#36716;&#21270;&#20026;&#21253;&#21547;&#20016;&#23500;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#21477;&#23376;&#65292;&#20197;&#25552;&#39640;&#22238;&#24212;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38271;&#26399;&#23545;&#35805;&#20013;&#65292;&#35760;&#24518;&#21644;&#21033;&#29992;&#35828;&#35805;&#32773;&#30340;&#35282;&#33394;&#26159;&#29983;&#25104;&#22238;&#24212;&#30340;&#24120;&#35265;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#32534;&#20889;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#25552;&#20379;&#26080;&#20449;&#24687;&#30340;&#35282;&#33394;&#21477;&#23376;&#65292;&#36825;&#22952;&#30861;&#20102;&#22238;&#24212;&#36136;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#24120;&#35782;&#22686;&#24378;&#30340;&#35282;&#33394;&#25193;&#23637;&#26469;&#35299;&#20915;&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#36825;&#20123;&#38382;&#39064;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#19981;&#20135;&#29983;&#19982;&#20854;&#20182;&#35282;&#33394;&#30456;&#30683;&#30462;&#30340;&#35282;&#33394;&#65292;&#25105;&#20204;&#20391;&#37325;&#20110;&#26681;&#25454;&#35774;&#35745;&#30340;&#31574;&#30053;&#65292;&#23558;&#30456;&#20114;&#30683;&#30462;&#30340;&#35282;&#33394;&#36716;&#21270;&#20026;&#21253;&#21547;&#20016;&#23500;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#21477;&#23376;&#65292;&#20197;&#27492;&#26469;&#32454;&#21270;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#32972;&#26223;&#12290;&#20316;&#20026;&#22810;&#20250;&#35805;&#24773;&#22659;&#20013;&#35282;&#33394;&#25193;&#23637;&#30340;&#20808;&#39537;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#31867;&#20154;&#20010;&#24615;&#32454;&#21270;&#20419;&#36827;&#20102;&#26356;&#22909;&#30340;&#22238;&#24212;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memorizing and utilizing speakers' personas is a common practice for response generation in long-term conversations. Yet, human-authored datasets often provide uninformative persona sentences that hinder response quality. This paper presents a novel framework that leverages commonsense-based persona expansion to address such issues in long-term conversation. While prior work focuses on not producing personas that contradict others, we focus on transforming contradictory personas into sentences that contain rich speaker information, by refining them based on their contextual backgrounds with designed strategies. As the pioneer of persona expansion in multi-session settings, our framework facilitates better response generation via human-like persona refinement. The supplementary video of our work is available at https://caffeine-15bbf.web.app/.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21477;&#23376;&#21040;&#24067;&#23616;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#35821;&#27861;&#34920;&#31034;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#26174;&#24335;&#34920;&#31034;&#35821;&#27861;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#24847;&#22806;&#24773;&#20917;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#26410;&#22312;&#35757;&#32451;&#38598;&#20013;&#20986;&#29616;&#30340;&#21477;&#23376;&#32467;&#26500;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.14212</link><description>&lt;p&gt;
&#26174;&#24335;&#34920;&#31034;&#35821;&#27861;&#25913;&#36827;&#20102;&#24847;&#22806;&#24773;&#20917;&#19979;&#30340;&#21477;&#23376;&#21040;&#24067;&#23616;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explicitly Representing Syntax Improves Sentence-to-layout Prediction of Unexpected Situations. (arXiv:2401.14212v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21477;&#23376;&#21040;&#24067;&#23616;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#35821;&#27861;&#34920;&#31034;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#26174;&#24335;&#34920;&#31034;&#35821;&#27861;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#24847;&#22806;&#24773;&#20917;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#26410;&#22312;&#35757;&#32451;&#38598;&#20013;&#20986;&#29616;&#30340;&#21477;&#23376;&#32467;&#26500;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#20013;&#35782;&#21035;&#35270;&#35273;&#23454;&#20307;&#24182;&#23558;&#23427;&#20204;&#25490;&#21015;&#22312;&#20108;&#32500;&#31354;&#38388;&#24067;&#23616;&#20013;&#65292;&#38656;&#35201;&#23545;&#35821;&#35328;&#21644;&#31354;&#38388;&#30340;&#32452;&#21512;&#29702;&#35299;&#12290;&#24067;&#23616;&#39044;&#27979;&#20219;&#21153;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#22240;&#20026;&#23427;&#20801;&#35768;&#23545;&#22270;&#20687;&#36827;&#34892;&#23616;&#37096;&#21644;&#21463;&#25511;&#30340;&#20462;&#22797;&#12290;&#36890;&#36807;&#27604;&#36739;&#24615;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#38544;&#24335;&#25110;&#26174;&#24335;&#32534;&#30721;&#21477;&#23376;&#35821;&#27861;&#30340;&#35821;&#35328;&#34920;&#31034;&#20013;&#39044;&#27979;&#24067;&#23616;&#65292;&#22914;&#26524;&#21477;&#23376;&#25552;&#21040;&#30340;&#23454;&#20307;&#20851;&#31995;&#19982;&#35757;&#32451;&#20013;&#30475;&#21040;&#30340;&#31867;&#20284;&#12290;&#20026;&#20102;&#27979;&#35797;&#32452;&#21512;&#29702;&#35299;&#33021;&#21147;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#30001;&#35821;&#27861;&#27491;&#30830;&#30340;&#21477;&#23376;&#21644;&#24067;&#23616;&#32452;&#25104;&#30340;&#27979;&#35797;&#38598;&#65292;&#25551;&#36848;&#20102;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#26410;&#26366;&#35265;&#36807;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#32452;&#21512;&#12290;&#22312;&#36825;&#20010;&#27979;&#35797;&#38598;&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#34920;&#26126;&#24403;&#21069;&#27169;&#22411;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#22312;&#29702;&#35299;&#36755;&#20837;&#21477;&#23376;&#30340;&#32467;&#26500;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#25439;&#22833;&#20989;&#25968;&#65292;&#26356;&#22909;&#22320;&#24378;&#21270;&#20102;&#21477;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing visual entities in a natural language sentence and arranging them in a 2D spatial layout require a compositional understanding of language and space. This task of layout prediction is valuable in text-to-image synthesis as it allows localized and controlled in-painting of the image. In this comparative study it is shown that we can predict layouts from language representations that implicitly or explicitly encode sentence syntax, if the sentences mention similar entity-relationships to the ones seen during training. To test compositional understanding, we collect a test set of grammatically correct sentences and layouts describing compositions of entities and relations that unlikely have been seen during training. Performance on this test set substantially drops, showing that current models rely on correlations in the training data and have difficulties in understanding the structure of the input sentences. We propose a novel structural loss function that better enforces th
&lt;/p&gt;</description></item><item><title>DeepSeek-Coder&#26159;&#19968;&#31995;&#21015;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39640;&#36136;&#37327;&#39033;&#30446;&#32423;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#37319;&#29992;&#22635;&#31354;&#20219;&#21153;&#21644;16K&#31383;&#21475;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#21644;&#22635;&#20805;&#65292;&#19981;&#20165;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19982;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#21516;&#26679;&#30340;&#26368;&#26032;&#34920;&#29616;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#38381;&#28304;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.14196</link><description>&lt;p&gt;
DeepSeek-Coder: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#32534;&#31243;&#30456;&#36935;&#30340;&#26102;&#20505;--&#20195;&#30721;&#26234;&#33021;&#30340;&#23835;&#36215;
&lt;/p&gt;
&lt;p&gt;
DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence. (arXiv:2401.14196v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14196
&lt;/p&gt;
&lt;p&gt;
DeepSeek-Coder&#26159;&#19968;&#31995;&#21015;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#39640;&#36136;&#37327;&#39033;&#30446;&#32423;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#37319;&#29992;&#22635;&#31354;&#20219;&#21153;&#21644;16K&#31383;&#21475;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#21644;&#22635;&#20805;&#65292;&#19981;&#20165;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19982;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#21516;&#26679;&#30340;&#26368;&#26032;&#34920;&#29616;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#38381;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#20026;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#20195;&#30721;&#26234;&#33021;&#24102;&#26469;&#20102;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#38381;&#28304;&#27169;&#22411;&#30340;&#20027;&#23548;&#22320;&#20301;&#38480;&#21046;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DeepSeek-Coder&#31995;&#21015;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#65292;&#22823;&#23567;&#20174;1.3B&#21040;33B&#65292;&#20174;&#22836;&#24320;&#22987;&#22312;2&#19975;&#20159;&#20010;&#26631;&#35760;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#39640;&#36136;&#37327;&#39033;&#30446;&#32423;&#20195;&#30721;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#24182;&#37319;&#29992;&#22635;&#31354;&#20219;&#21153;&#21644;16K&#31383;&#21475;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#21644;&#22635;&#20805;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;DeepSeek-Coder&#19981;&#20165;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#19982;&#24320;&#28304;&#20195;&#30721;&#27169;&#22411;&#21516;&#26679;&#30340;&#26368;&#26032;&#34920;&#29616;&#65292;&#32780;&#19988;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;Codex&#21644;GPT-3.5&#31561;&#38381;&#28304;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;DeepSeek-Coder&#27169;&#22411;&#37319;&#29992;&#20102;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#65292;&#26082;&#20801;&#35768;&#30740;&#31350;&#65292;&#20063;&#20801;&#35768;&#26080;&#38480;&#21046;&#30340;&#21830;&#19994;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#19968;&#31181;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#24314;&#27169;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#32534;&#30721;&#39033;&#30446;&#12289;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#65292;&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#36827;&#34892;&#39033;&#30446;&#25512;&#33616;&#65292;&#24182;&#29983;&#25104;&#23545;&#35805;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14194</link><description>&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Conversational Recommender System as a Language Processing Task. (arXiv:2401.14194v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#19968;&#31181;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#24314;&#27169;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#32534;&#30721;&#39033;&#30446;&#12289;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#65292;&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#36827;&#34892;&#39033;&#30446;&#25512;&#33616;&#65292;&#24182;&#29983;&#25104;&#23545;&#35805;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#26469;&#21521;&#29992;&#25143;&#25512;&#33616;&#30456;&#20851;&#30340;&#39033;&#30446;&#12290;&#20043;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#22270;&#35889;&#26469;&#25552;&#20379;&#39033;&#30446;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#29983;&#25104;&#65292;&#20197;&#21450;&#21033;&#29992;&#25512;&#33616;&#27169;&#22359;&#36827;&#34892;&#30456;&#20851;&#39033;&#30446;&#30340;&#25490;&#24207;&#12290;&#36825;&#31181;&#22810;&#32452;&#20214;&#30340;&#32452;&#21512;&#23548;&#33268;&#35757;&#32451;&#36807;&#31243;&#32321;&#29712;&#65292;&#24182;&#19988;&#23548;&#33268;&#23545;&#35805;&#29983;&#25104;&#21644;&#39033;&#30446;&#25512;&#33616;&#20043;&#38388;&#30340;&#35821;&#20041;&#19981;&#37197;&#23545;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#39033;&#30446;&#65292;&#24182;&#23558;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#32534;&#30721;&#39033;&#30446;&#65292;&#22312;&#23545;&#35805;&#20013;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#65292;&#36890;&#36807;&#35821;&#20041;&#21305;&#37197;&#36827;&#34892;&#39033;&#30446;&#25512;&#33616;&#65292;&#24182;&#29983;&#25104;&#23545;&#35805;&#12290;&#20316;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;PECRS&#65288;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65289;&#21487;&#20197;&#22312;&#21333;&#20010;&#38454;&#27573;&#36827;&#34892;&#20248;&#21270;&#65292;&#32780;&#19981;&#20381;&#36182;&#38750;&#25991;&#26412;&#20803;&#25968;&#25454;&#65292;&#22914;&#30693;&#35782;&#22270;&#35889;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational recommender systems (CRS) aim to recommend relevant items to users by eliciting user preference through natural language conversation. Prior work often utilizes external knowledge graphs for items' semantic information, a language model for dialogue generation, and a recommendation module for ranking relevant items. This combination of multiple components suffers from a cumbersome training process, and leads to semantic misalignment issues between dialogue generation and item recommendation. In this paper, we represent items in natural language and formulate CRS as a natural language processing task. Accordingly, we leverage the power of pre-trained language models to encode items, understand user intent via conversation, perform item recommendation through semantic matching, and generate dialogues. As a unified model, our PECRS (Parameter-Efficient CRS), can be optimized in a single stage, without relying on non-textual metadata such as a knowledge graph. Experiments on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STG-LLM&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#26102;&#31354;&#25968;&#25454;&#24182;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;STG-Tokenizer&#23558;&#22797;&#26434;&#30340;&#22270;&#24418;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#27905;&#30340;&#26631;&#35760;&#65292;&#20877;&#36890;&#36807;STG-Adapter&#23558;&#26631;&#35760;&#21270;&#25968;&#25454;&#19982;LLM&#30340;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#36830;&#25509;&#12290;&#36890;&#36807;&#24494;&#35843;&#21442;&#25968;&#65292;STG-LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#25226;&#25569;&#26631;&#35760;&#30340;&#35821;&#20041;&#65292;&#21516;&#26102;&#20445;&#30041;LLM&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;STG-LLM&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14192</link><description>&lt;p&gt;
&#22914;&#20309;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#26102;&#31354;&#25968;&#25454;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Can Large Language Models Understand Spatial-Temporal Data?. (arXiv:2401.14192v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STG-LLM&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#26102;&#31354;&#25968;&#25454;&#24182;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;STG-Tokenizer&#23558;&#22797;&#26434;&#30340;&#22270;&#24418;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#27905;&#30340;&#26631;&#35760;&#65292;&#20877;&#36890;&#36807;STG-Adapter&#23558;&#26631;&#35760;&#21270;&#25968;&#25454;&#19982;LLM&#30340;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#36830;&#25509;&#12290;&#36890;&#36807;&#24494;&#35843;&#21442;&#25968;&#65292;STG-LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#25226;&#25569;&#26631;&#35760;&#30340;&#35821;&#20041;&#65292;&#21516;&#26102;&#20445;&#30041;LLM&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;STG-LLM&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#20219;&#21153;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#21033;&#29992;&#23427;&#20204;&#30340;&#33021;&#21147;&#36827;&#34892;&#26102;&#31354;&#39044;&#27979;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26102;&#24207;&#25991;&#26412;&#19982;&#22797;&#26434;&#30340;&#26102;&#31354;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#38459;&#30861;&#20102;&#35813;&#24212;&#29992;&#30340;&#23454;&#29616;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;STG-LLM&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20026;LLM&#36171;&#20104;&#20102;&#26102;&#31354;&#39044;&#27979;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#35299;&#20915;&#25968;&#25454;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65306;1&#65289;STG-Tokenizer&#65306;&#36825;&#20010;&#26102;&#31354;&#22270;&#24418;&#26631;&#35760;&#22120;&#23558;&#22797;&#26434;&#30340;&#22270;&#24418;&#25968;&#25454;&#36716;&#21270;&#20026;&#31616;&#27905;&#30340;&#26631;&#35760;&#65292;&#25429;&#25417;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#20851;&#31995;&#65307;2&#65289;STG-Adapter&#65306;&#36825;&#20010;&#31934;&#31616;&#30340;&#36866;&#37197;&#22120;&#30001;&#32447;&#24615;&#32534;&#30721;&#21644;&#35299;&#30721;&#23618;&#32452;&#25104;&#65292;&#22635;&#34917;&#20102;&#26631;&#35760;&#21270;&#25968;&#25454;&#21644;LLM&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#20165;&#24494;&#35843;&#19968;&#23567;&#37096;&#20998;&#21442;&#25968;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#25226;&#25569;STG-Tokenizer&#29983;&#25104;&#30340;&#26631;&#35760;&#30340;&#35821;&#20041;&#65292;&#21516;&#26102;&#20445;&#30041;LLM&#30340;&#21407;&#22987;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;STG-LLM&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) dominate tasks like natural language processing and computer vision, harnessing their power for spatial-temporal forecasting remains challenging. The disparity between sequential text and complex spatial-temporal data hinders this application. To address this issue, this paper introduces STG-LLM, an innovative approach empowering LLMs for spatial-temporal forecasting. We tackle the data mismatch by proposing: 1) STG-Tokenizer: This spatial-temporal graph tokenizer transforms intricate graph data into concise tokens capturing both spatial and temporal relationships; 2) STG-Adapter: This minimalistic adapter, consisting of linear encoding and decoding layers, bridges the gap between tokenized data and LLM comprehension. By fine-tuning only a small set of parameters, it can effectively grasp the semantics of tokens generated by STG-Tokenizer, while preserving the original natural language understanding capabilities of LLMs. Extensive experiments on diver
&lt;/p&gt;</description></item><item><title>BayesPrompt&#36890;&#36807;&#26080;&#20559;&#39046;&#22495;&#25277;&#35937;&#35299;&#20915;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#25512;&#29702;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14166</link><description>&lt;p&gt;
BayesPrompt: &#36890;&#36807;&#26080;&#20559;&#39046;&#22495;&#25277;&#35937;&#22312;&#23569;&#26679;&#26412;&#25512;&#29702;&#19978;&#25351;&#23548;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction. (arXiv:2401.14166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14166
&lt;/p&gt;
&lt;p&gt;
BayesPrompt&#36890;&#36807;&#26080;&#20559;&#39046;&#22495;&#25277;&#35937;&#35299;&#20915;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#25512;&#29702;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#24494;&#35843;&#33539;&#24335;&#65292;prompt-tuning&#26088;&#22312;&#32553;&#23567;&#19979;&#28216;&#20219;&#21153;&#19982;&#39044;&#35757;&#32451;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#34429;&#28982;prompt-tuning&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25345;&#32493;&#36827;&#23637;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#25345;&#20037;&#30340;&#32570;&#38519;&#65306;prompt-tuning&#26041;&#27861;&#26080;&#27861;&#27867;&#21270;&#21040;&#29305;&#23450;&#30340;&#23569;&#26679;&#26412;&#27169;&#24335;&#12290;&#20174;&#20998;&#24067;&#20998;&#26512;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#19968;&#29616;&#35937;&#32972;&#21518;&#30340;&#20869;&#22312;&#38382;&#39064;&#26159;PLMs&#20013;&#21253;&#21547;&#36807;&#22810;&#30340;&#27010;&#24565;&#30693;&#35782;&#21644;&#30446;&#26631;&#19979;&#28216;&#39046;&#22495;&#30340;&#32553;&#20943;&#30693;&#35782;&#65292;&#20004;&#32773;&#20849;&#21516;&#23548;&#33268;PLMs&#22312;&#26222;&#36941;&#30340;&#30693;&#35782;&#23884;&#20837;&#31354;&#38388;&#20013;&#38169;&#35823;&#22320;&#23450;&#20301;&#19982;&#30446;&#26631;&#39046;&#22495;&#30456;&#23545;&#24212;&#30340;&#30693;&#35782;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30452;&#35266;&#22320;&#25506;&#32034;&#20102;&#20197;&#26080;&#20559;&#26041;&#24335;&#36924;&#36817;&#19979;&#28216;&#20219;&#21153;&#30340;&#23436;&#25972;&#30446;&#26631;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#25277;&#35937;&#36825;&#26679;&#30340;&#39046;&#22495;&#29983;&#25104;&#26377;&#21306;&#21035;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26080;&#27495;&#20041;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a novel and effective fine-tuning paradigm based on large-scale pre-trained language models (PLMs), prompt-tuning aims to reduce the gap between downstream tasks and pre-training objectives. While prompt-tuning has yielded continuous advancements in various tasks, such an approach still remains a persistent defect: prompt-tuning methods fail to generalize to specific few-shot patterns. From the perspective of distribution analyses, we disclose that the intrinsic issues behind the phenomenon are the over-multitudinous conceptual knowledge contained in PLMs and the abridged knowledge for target downstream domains, which jointly result in that PLMs mis-locate the knowledge distributions corresponding to the target domains in the universal knowledge embedding space. To this end, we intuitively explore to approximate the unabridged target domains of downstream tasks in a debiased manner, and then abstract such domains to generate discriminative prompts, thereby providing the de-ambiguous
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20915;&#31574;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20855;&#36523;&#29615;&#22659;&#39640;&#25928;&#20114;&#21160;&#26469;&#35299;&#20915;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30693;&#35782;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;LLMs&#30340;&#32852;&#21512;&#27010;&#29575;&#65292;&#24418;&#25104;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#21644;&#22235;&#20010;&#25552;&#31034;&#35774;&#35745;&#21407;&#21017;&#25552;&#39640;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35774;&#35745;&#21442;&#25968;&#39640;&#25928;&#30340;&#35757;&#32451;&#26550;&#26500;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.14151</link><description>&lt;p&gt;
&#30495;&#30693;&#26469;&#28304;&#20110;&#23454;&#36341;&#65306;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#20351;LLMs&#19982;&#20855;&#36523;&#29615;&#22659;&#23545;&#40784;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning. (arXiv:2401.14151v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20915;&#31574;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#19982;&#20855;&#36523;&#29615;&#22659;&#39640;&#25928;&#20114;&#21160;&#26469;&#35299;&#20915;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30693;&#35782;&#19981;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#35810;LLMs&#30340;&#32852;&#21512;&#27010;&#29575;&#65292;&#24418;&#25104;&#34892;&#20026;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#21644;&#22235;&#20010;&#25552;&#31034;&#35774;&#35745;&#21407;&#21017;&#25552;&#39640;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#35774;&#35745;&#21442;&#25968;&#39640;&#25928;&#30340;&#35757;&#32451;&#26550;&#26500;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20247;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35299;&#20915;&#31616;&#21333;&#30340;&#20915;&#31574;&#20219;&#21153;&#19978;&#32463;&#24120;&#22833;&#36133;&#65292;&#21407;&#22240;&#26159;LLMs&#20013;&#30340;&#30693;&#35782;&#19982;&#29615;&#22659;&#19981;&#23545;&#40784;&#12290;&#30456;&#21453;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26234;&#33021;&#20307;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#31574;&#30053;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22987;&#32456;&#19982;&#29615;&#22659;&#20445;&#25345;&#19968;&#33268;&#65292;&#20294;&#38590;&#20197;&#23558;&#20808;&#21069;&#30340;&#30693;&#35782;&#25972;&#21512;&#21040;&#20854;&#20013;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#25506;&#32034;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TWOSOME&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22312;&#32447;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#20915;&#31574;&#26234;&#33021;&#20307;&#65292;&#36890;&#36807;RL&#19982;&#20855;&#36523;&#29615;&#22659;&#39640;&#25928;&#20114;&#21160;&#24182;&#23454;&#29616;&#23545;&#40784;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#20934;&#22791;&#22909;&#30340;&#25968;&#25454;&#38598;&#25110;&#29615;&#22659;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;LLMs&#26597;&#35810;&#27599;&#20010;&#26377;&#25928;&#21160;&#20316;&#30340;&#32852;&#21512;&#27010;&#29575;&#20197;&#24418;&#25104;&#34892;&#20026;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#22686;&#24378;&#31574;&#30053;&#30340;&#31283;&#23450;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#24182;&#24635;&#32467;&#20102;&#22235;&#20010;&#25552;&#31034;&#35774;&#35745;&#21407;&#21017;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#35757;&#32451;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#34892;&#20026;&#35780;&#20272;&#21644;&#36873;&#25321;&#31639;&#27861;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the impressive performance across numerous tasks, large language models (LLMs) often fail in solving simple decision-making tasks due to the misalignment of the knowledge in LLMs with environments. On the contrary, reinforcement learning (RL) agents learn policies from scratch, which makes them always align with environments but difficult to incorporate prior knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a novel general online framework that deploys LLMs as decision-making agents to efficiently interact and align with embodied environments via RL without requiring any prepared datasets or prior knowledge of the environments. Firstly, we query the joint probabilities of each valid action with LLMs to form behavior policies. Then, to enhance the stability and robustness of the policies, we propose two normalization methods and summarize four prompt design principles. Finally, we design a novel parameter-efficient training architecture where the acto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#21360;&#24230;&#21360;&#22320;&#25991;&#27861;&#24459;&#25991;&#20214;&#19978;&#36827;&#34892;&#20108;&#20803;&#20445;&#37322;&#21028;&#26029;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;93&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#25913;&#36827;&#20102;&#20043;&#21069;&#30340;&#20934;&#30830;&#29575;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.14135</link><description>&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23454;&#29616;&#20108;&#20803;&#20445;&#37322;&#21028;&#26029;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks can achieve binary bail judgement classification. (arXiv:2401.14135v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#21360;&#24230;&#21360;&#22320;&#25991;&#27861;&#24459;&#25991;&#20214;&#19978;&#36827;&#34892;&#20108;&#20803;&#20445;&#37322;&#21028;&#26029;&#20998;&#31867;&#65292;&#21462;&#24471;&#20102;93&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#25913;&#36827;&#20102;&#20043;&#21069;&#30340;&#20934;&#30830;&#29575;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#20013;&#32570;&#20047;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#23454;&#26045;&#65292;&#35813;&#39046;&#22495;&#30340;&#20219;&#20309;&#30740;&#31350;&#36890;&#24120;&#26159;&#22522;&#20110;&#33521;&#35821;&#25968;&#25454;&#21644;&#39640;&#31561;&#27861;&#38498;&#30340;&#25968;&#25454;&#12290;&#24573;&#30053;&#20102;&#21360;&#24230;&#20302;&#32423;&#27861;&#38498;&#21644;&#19981;&#21516;&#22320;&#21306;&#35821;&#35328;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#22312;&#19968;&#32452;&#21360;&#22320;&#25991;&#27861;&#24459;&#25991;&#20214;&#19978;&#37096;&#32626;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#26550;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;CNN&#27169;&#22411;&#36827;&#34892;&#20445;&#37322;&#39044;&#27979;&#20219;&#21153;&#65292;&#21462;&#24471;&#20102;93&#65285;&#30340;&#24635;&#20307;&#20934;&#30830;&#29575;&#65292;&#36825;&#26159;&#23545;&#21360;&#24230;&#21271;&#26041;&#37030;20&#20010;&#22320;&#21306;&#25968;&#25454;&#30340;Kapoor&#31561;&#20154;&#65288;2022&#65289;&#35774;&#23450;&#30340;&#20934;&#30830;&#29575;&#22522;&#20934;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an evident lack of implementation of Machine Learning (ML) in the legal domain in India, and any research that does take place in this domain is usually based on data from the higher courts of law and works with English data. The lower courts and data from the different regional languages of India are often overlooked. In this paper, we deploy a Convolutional Neural Network (CNN) architecture on a corpus of Hindi legal documents. We perform a bail Prediction task with the help of a CNN model and achieve an overall accuracy of 93\% which is an improvement on the benchmark accuracy, set by Kapoor et al. (2022), albeit in data from 20 districts of the Indian state of Uttar Pradesh.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TraCo&#30340;&#26032;&#23618;&#27425;&#20027;&#39064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20256;&#36755;&#35745;&#21010;&#20381;&#36182;&#26041;&#27861;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#35299;&#32544;&#30721;&#22120;&#65292;&#25913;&#21892;&#20102;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#30340;&#20146;&#21644;&#24615;&#12289;&#21512;&#29702;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14113</link><description>&lt;p&gt;
&#20851;&#20110;&#23618;&#27425;&#20027;&#39064;&#24314;&#27169;&#30340;&#20146;&#21644;&#24615;&#12289;&#21512;&#29702;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Affinity, Rationality, and Diversity of Hierarchical Topic Modeling. (arXiv:2401.14113v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TraCo&#30340;&#26032;&#23618;&#27425;&#20027;&#39064;&#27169;&#22411;&#65292;&#36890;&#36807;&#20256;&#36755;&#35745;&#21010;&#20381;&#36182;&#26041;&#27861;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#35299;&#32544;&#30721;&#22120;&#65292;&#25913;&#21892;&#20102;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#30340;&#20146;&#21644;&#24615;&#12289;&#21512;&#29702;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#20027;&#39064;&#24314;&#27169;&#26088;&#22312;&#20174;&#35821;&#26009;&#24211;&#20013;&#21457;&#29616;&#28508;&#22312;&#20027;&#39064;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#32455;&#25104;&#19968;&#20010;&#23618;&#27425;&#32467;&#26500;&#65292;&#20197;&#20415;&#29702;&#35299;&#20855;&#26377;&#26399;&#26395;&#35821;&#20041;&#31890;&#24230;&#30340;&#25991;&#26723;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#24037;&#20316;&#22312;&#20135;&#29983;&#20302;&#20146;&#21644;&#24615;&#12289;&#21512;&#29702;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#20027;&#39064;&#23618;&#27425;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#38459;&#30861;&#20102;&#25991;&#26723;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20256;&#36755;&#35745;&#21010;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#23618;&#27425;&#20027;&#39064;&#27169;&#22411;&#65288;TraCo&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#36755;&#35745;&#21010;&#20381;&#36182;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#20043;&#21069;&#31616;&#21333;&#30340;&#20027;&#39064;&#20381;&#36182;&#26041;&#27861;&#12290;&#23427;&#38480;&#21046;&#20381;&#36182;&#20851;&#31995;&#20197;&#30830;&#20445;&#23427;&#20204;&#30340;&#31232;&#30095;&#24615;&#21644;&#24179;&#34913;&#24615;&#65292;&#24182;&#36890;&#36807;&#23427;&#20204;&#23545;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#36825;&#25913;&#21892;&#20102;&#23618;&#27425;&#32467;&#26500;&#30340;&#20146;&#21644;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#35299;&#32544;&#30721;&#22120;&#12290;&#23427;&#36890;&#36807;&#35299;&#32544;&#32534;&#30721;&#23558;&#19981;&#21516;&#30340;&#35821;&#20041;&#31890;&#24230;&#20998;&#37197;&#32473;&#19981;&#21516;&#23618;&#27425;&#30340;&#20027;&#39064;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#23618;&#27425;&#32467;&#26500;&#30340;&#21512;&#29702;&#24615;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical topic modeling aims to discover latent topics from a corpus and organize them into a hierarchy to understand documents with desirable semantic granularity. However, existing work struggles with producing topic hierarchies of low affinity, rationality, and diversity, which hampers document understanding. To overcome these challenges, we in this paper propose Transport Plan and Context-aware Hierarchical Topic Model (TraCo). Instead of early simple topic dependencies, we propose a transport plan dependency method. It constrains dependencies to ensure their sparsity and balance, and also regularizes topic hierarchy building with them. This improves affinity and diversity of hierarchies. We further propose a context-aware disentangled decoder. Rather than previously entangled decoding, it distributes different semantic granularity to topics at different levels by disentangled decoding. This facilitates the rationality of hierarchies. Experiments on benchmark datasets demonstra
&lt;/p&gt;</description></item><item><title>CompactifAI&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26497;&#21387;&#32553;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#26356;&#27880;&#37325;&#27169;&#22411;&#30340;&#30456;&#20851;&#31354;&#38388;&#65292;&#23454;&#29616;&#26356;&#21152;&#21487;&#25511;&#21644;&#31934;&#32454;&#30340;&#21387;&#32553;&#12290;</title><link>http://arxiv.org/abs/2401.14109</link><description>&lt;p&gt;
CompactifAI: &#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26497;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks. (arXiv:2401.14109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14109
&lt;/p&gt;
&lt;p&gt;
CompactifAI&#26159;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26497;&#21387;&#32553;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#23427;&#26356;&#27880;&#37325;&#27169;&#22411;&#30340;&#30456;&#20851;&#31354;&#38388;&#65292;&#23454;&#29616;&#26356;&#21152;&#21487;&#25511;&#21644;&#31934;&#32454;&#30340;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;LlaMA&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#20854;&#24222;&#22823;&#30340;&#35268;&#27169;&#24102;&#26469;&#20102;&#37325;&#35201;&#25361;&#25112;&#65292;&#22914;&#24040;&#22823;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#25104;&#26412;&#12289;&#36739;&#22823;&#30340;&#33021;&#28304;&#38656;&#27714;&#20197;&#21450;&#29616;&#22330;&#37096;&#32626;&#30340;&#38480;&#21046;&#12290;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#22914;&#21098;&#26525;&#12289;&#33976;&#39311;&#21644;&#20302;&#31209;&#36924;&#36817;&#20027;&#35201;&#20851;&#27880;&#20943;&#23569;&#32593;&#32476;&#20013;&#31070;&#32463;&#20803;&#30340;&#26377;&#25928;&#25968;&#37327;&#65292;&#32780;&#37327;&#21270;&#26041;&#27861;&#21017;&#20391;&#37325;&#20110;&#38477;&#20302;&#21333;&#20010;&#26435;&#37325;&#30340;&#25968;&#20540;&#31934;&#24230;&#65292;&#20197;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#21516;&#26102;&#20445;&#25345;&#31070;&#32463;&#20803;&#25968;&#30446;&#19981;&#21464;&#12290;&#34429;&#28982;&#36825;&#20123;&#21387;&#32553;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#30456;&#23545;&#25104;&#21151;&#65292;&#20294;&#27809;&#26377;&#20196;&#20154;&#20449;&#26381;&#30340;&#29702;&#30001;&#35748;&#20026;&#25130;&#26029;&#31070;&#32463;&#20803;&#30340;&#25968;&#37327;&#26159;&#19968;&#31181;&#26368;&#20248;&#31574;&#30053;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;LLM&#21387;&#32553;&#26041;&#27861;CompactifAI&#65292;&#23427;&#20351;&#29992;&#37327;&#23376;&#21551;&#21457;&#30340;&#24352;&#37327;&#32593;&#32476;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#26356;&#27880;&#37325;&#27169;&#22411;&#30340;&#30456;&#20851;&#31354;&#38388;&#65292;&#23454;&#29616;&#26356;&#21152;&#21487;&#25511;&#21644;&#31934;&#32454;&#30340;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly in generative Artificial Intelligence (AI), but their immense size poses significant challenges, such as huge training and inference costs, substantial energy demands, and limitations for on-site deployment. Traditional compression methods such as pruning, distillation, and low-rank approximation focus on reducing the effective number of neurons in the network, while quantization focuses on reducing the numerical precision of individual weights to reduce the model size while keeping the number of neurons fixed. While these compression methods have been relatively successful in practice, there's no compelling reason to believe that truncating the number of neurons is an optimal strategy. In this context, this paper introduces CompactifAI, an innovative LLM compression approach using quantum-inspired Tensor Networks that focuses on the model's correlation space instead, allowing for a more controlled, refined an
&lt;/p&gt;</description></item><item><title>Ta'keed&#26159;&#39318;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#35770;&#26029;&#30340;&#29983;&#25104;&#24335;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#36890;&#36807;&#22522;&#20110;&#26816;&#32034;&#29255;&#27573;&#30340;&#35770;&#26029;&#30495;&#23454;&#24615;&#35780;&#20272;&#21644;LLM-based&#35770;&#26029;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#30446;&#21069;&#38463;&#25289;&#20271;&#35821;&#39046;&#22495;&#32570;&#20047;&#29983;&#25104;&#35299;&#37322;&#35770;&#26029;&#21487;&#20449;&#24230;&#30340;&#30740;&#31350;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#27979;&#35797;&#40644;&#37329;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#24182;&#20998;&#26512;&#20102;&#31995;&#32479;&#29983;&#25104;&#35299;&#37322;&#19982;&#40644;&#37329;&#26631;&#20934;&#35299;&#37322;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#19981;&#21516;&#29255;&#27573;&#25968;&#37327;&#23545;&#35770;&#26029;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.14067</link><description>&lt;p&gt;
Ta'keed: &#39318;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#35770;&#26029;&#30340;&#29983;&#25104;&#24335;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Ta'keed: The First Generative Fact-Checking System for Arabic Claims. (arXiv:2401.14067v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14067
&lt;/p&gt;
&lt;p&gt;
Ta'keed&#26159;&#39318;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#35770;&#26029;&#30340;&#29983;&#25104;&#24335;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#36890;&#36807;&#22522;&#20110;&#26816;&#32034;&#29255;&#27573;&#30340;&#35770;&#26029;&#30495;&#23454;&#24615;&#35780;&#20272;&#21644;LLM-based&#35770;&#26029;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#30446;&#21069;&#38463;&#25289;&#20271;&#35821;&#39046;&#22495;&#32570;&#20047;&#29983;&#25104;&#35299;&#37322;&#35770;&#26029;&#21487;&#20449;&#24230;&#30340;&#30740;&#31350;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#27979;&#35797;&#40644;&#37329;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#24182;&#20998;&#26512;&#20102;&#31995;&#32479;&#29983;&#25104;&#35299;&#37322;&#19982;&#40644;&#37329;&#26631;&#20934;&#35299;&#37322;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#19981;&#21516;&#29255;&#27573;&#25968;&#37327;&#23545;&#35770;&#26029;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Ta'keed&#65292;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#38463;&#25289;&#20271;&#35821;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#12290;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#23558;&#35770;&#26029;&#20998;&#31867;&#20026;&#8220;&#30495;&#8221;&#25110;&#8220;&#20551;&#8221;&#65292;&#20294;&#23545;&#20110;&#29983;&#25104;&#35770;&#26029;&#21487;&#20449;&#24230;&#30340;&#35299;&#37322;&#23588;&#20854;&#22312;&#38463;&#25289;&#20271;&#35821;&#39046;&#22495;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;Ta'keed&#36890;&#36807;&#22522;&#20110;&#26816;&#32034;&#29255;&#27573;&#30340;&#35770;&#26029;&#30495;&#23454;&#24615;&#35780;&#20272;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#21033;&#29992;&#20449;&#24687;&#26816;&#32034;&#21644;&#22522;&#20110;LLM&#30340;&#35770;&#26029;&#39564;&#35777;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;ArFactEx&#65292;&#19968;&#20010;&#24102;&#26377;&#25163;&#24037;&#35777;&#26126;&#21442;&#32771;&#30340;&#27979;&#35797;&#40644;&#37329;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#35813;&#31995;&#32479;&#12290;&#21021;&#22987;&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;0.72&#30340;F1&#24471;&#20998;&#12290;&#19982;&#40644;&#37329;&#26631;&#20934;&#35299;&#37322;&#22312;&#21477;&#27861;&#21644;&#35821;&#20041;&#19978;&#36827;&#34892;&#27604;&#36739;&#65292;&#31995;&#32479;&#29983;&#25104;&#30340;&#35299;&#37322;&#24471;&#21040;&#20102;&#25512;&#33616;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#35780;&#20272;&#65292;&#24179;&#22343;&#20313;&#24358;&#30456;&#20284;&#24230;&#20998;&#25968;&#20026;0.76&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19981;&#21516;&#29255;&#27573;&#25968;&#37327;&#23545;&#35770;&#26029;&#20998;&#31867;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;....
&lt;/p&gt;
&lt;p&gt;
This paper introduces Ta'keed, an explainable Arabic automatic fact-checking system. While existing research often focuses on classifying claims as "True" or "False," there is a limited exploration of generating explanations for claim credibility, particularly in Arabic. Ta'keed addresses this gap by assessing claim truthfulness based on retrieved snippets, utilizing two main components: information retrieval and LLM-based claim verification. We compiled the ArFactEx, a testing gold-labelled dataset with manually justified references, to evaluate the system. The initial model achieved a promising F1 score of 0.72 in the classification task. Meanwhile, the system's generated explanations are compared with gold-standard explanations syntactically and semantically. The study recommends evaluating using semantic similarities, resulting in an average cosine similarity score of 0.76. Additionally, we explored the impact of varying snippet quantities on claim classification accuracy, revealin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;35&#20010;&#20195;&#34920;&#24615;&#30740;&#31350;&#30340;&#22238;&#39038;&#65292;&#25105;&#20204;&#21457;&#29616;&#24341;&#23548;LLM&#36981;&#24490;&#20154;&#31867;&#30340;&#36923;&#36753;&#24605;&#32500;&#30340;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#20844;&#24335;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#21313;&#20010;&#36866;&#29992;&#20219;&#21153;&#26469;&#23637;&#31034;&#25105;&#20204;&#26694;&#26550;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#20197;&#25512;&#21160;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.14043</link><description>&lt;p&gt;
&#26397;&#30528;&#30446;&#26631;&#23548;&#21521;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#26041;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Towards Goal-oriented Large Language Model Prompting: A Survey. (arXiv:2401.14043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;35&#20010;&#20195;&#34920;&#24615;&#30740;&#31350;&#30340;&#22238;&#39038;&#65292;&#25105;&#20204;&#21457;&#29616;&#24341;&#23548;LLM&#36981;&#24490;&#20154;&#31867;&#30340;&#36923;&#36753;&#24605;&#32500;&#30340;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#20844;&#24335;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#24635;&#32467;&#20102;&#21313;&#20010;&#36866;&#29992;&#20219;&#21153;&#26469;&#23637;&#31034;&#25105;&#20204;&#26694;&#26550;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#20197;&#25512;&#21160;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#32780;&#25552;&#31034;&#24037;&#31243;&#22312;&#20248;&#21270;LLM&#24615;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#24378;&#35843;&#35774;&#35745;&#25552;&#31034;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#20445;&#25345;&#20154;&#31867;&#36861;&#27714;LLM&#20687;&#20154;&#31867;&#24605;&#32771;&#30340;&#20154;&#31867;&#23398;&#20551;&#35774;&#12290;&#36890;&#36807;&#23545;35&#20010;&#20195;&#34920;&#24615;&#30740;&#31350;&#30340;&#22238;&#39038;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#20844;&#24335;&#30340;&#37325;&#35201;&#24615;&#65292;&#35813;&#20844;&#24335;&#25351;&#23548;LLM&#36981;&#24490;&#20154;&#31867;&#30340;&#36923;&#36753;&#24605;&#32500;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#23558;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#26041;&#27861;&#20998;&#20026;&#20116;&#20010;&#30456;&#20114;&#20851;&#32852;&#30340;&#38454;&#27573;&#65292;&#24182;&#36890;&#36807;&#24635;&#32467;&#21313;&#20010;&#36866;&#29992;&#20219;&#21153;&#26469;&#23637;&#31034;&#25105;&#20204;&#26694;&#26550;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#30340;&#26041;&#21521;&#65292;&#24076;&#26395;&#36827;&#19968;&#27493;&#24378;&#35843;&#21644;&#25512;&#21160;&#30446;&#26631;&#23548;&#21521;&#25552;&#31034;&#24037;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown prominent performance in various downstream tasks in which prompt engineering plays a pivotal role in optimizing LLMs' performance. This paper, not as an overview of current prompt engineering methods, aims to highlight the limitation of designing prompts while holding an anthropomorphic assumption that expects LLMs to think like humans. From our review of 35 representative studies, we demonstrate that a goal-oriented prompt formulation, which guides LLMs to follow established human logical thinking, significantly improves the performance of LLMs. Furthermore, We introduce a novel taxonomy that categorizes goal-oriented prompting methods into five interconnected stages and we demonstrate the broad applicability of our framework by summarizing ten applicable tasks. With four future directions proposed, we hope to further emphasize and promote goal-oriented prompt engineering.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;(Chat)GPT&#21644;BERT&#22312;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;(Chat)GPT&#30340;&#34920;&#29616;&#26126;&#26174;&#20302;&#20110;BERT&#65292;&#23588;&#20854;&#22312;&#38271;&#26399;&#21464;&#21270;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26356;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.14040</link><description>&lt;p&gt;
(&#32842;&#22825;)GPT v BERT: &#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#20043;&#40654;&#26126;&#30340;&#27491;&#20041;&#12290;(arXiv:2401.14040v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
(Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection. (arXiv:2401.14040v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;(Chat)GPT&#21644;BERT&#22312;&#35821;&#20041;&#21464;&#21270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;(Chat)GPT&#30340;&#34920;&#29616;&#26126;&#26174;&#20302;&#20110;BERT&#65292;&#23588;&#20854;&#22312;&#38271;&#26399;&#21464;&#21270;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;BERT&#21644;(Chat)GPT&#65292;&#20316;&#20026;&#20855;&#26377;&#35299;&#20915;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#30340;&#24040;&#22823;&#33021;&#21147;&#30340;&#35789;&#27719;&#36229;&#32423;&#33521;&#38596;&#32780;&#20986;&#29616;&#12290;&#26412;&#25991;&#29305;&#21035;&#20851;&#27880;&#35821;&#20041;&#21464;&#21270;&#30340;&#26102;&#38388;&#24615;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#35299;&#20915;Word-in-Context (WiC)&#20219;&#21153;&#30340;&#20004;&#20010;&#21382;&#26102;&#24615;&#25193;&#23637;&#65306;TempoWiC&#21644;HistoWiC&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#65288;&#21644;GPT&#65289;3.5&#36825;&#26679;&#30340;&#26032;&#22411;&#21363;&#29992;&#25216;&#26415;&#19982;&#24403;&#21069;&#20316;&#20026;&#24314;&#27169;&#35821;&#20041;&#21464;&#21270;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#23478;&#26063;BERT&#20043;&#38388;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26159;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;(Chat)GPT&#30740;&#31350;&#35821;&#20041;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#30340;&#24615;&#33021;&#26174;&#33879;&#20302;&#20110;&#22522;&#30784;GPT&#29256;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;(Chat)GPT&#22312;&#26816;&#27979;&#38271;&#26399;&#21464;&#21270;&#26041;&#38754;&#30340;&#34920;&#29616;&#30053;&#20302;&#20110;BERT&#65292;&#20294;&#22312;&#30701;&#26399;&#21464;&#21270;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26126;&#26174;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the universe of Natural Language Processing, Transformer-based language models like BERT and (Chat)GPT have emerged as lexical superheroes with great power to solve open research problems. In this paper, we specifically focus on the temporal problem of semantic change, and evaluate their ability to solve two diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and HistoWiC. In particular, we investigate the potential of a novel, off-the-shelf technology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a family of models that currently stand as the state-of-the-art for modeling semantic change. Our experiments represent the first attempt to assess the use of (Chat)GPT for studying semantic change. Our results indicate that ChatGPT performs significantly worse than the foundational GPT version. Furthermore, our results demonstrate that (Chat)GPT achieves slightly lower performance than BERT in detecting long-term changes but performs significantly worse in de
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;RaLMSpec&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#25512;&#27979;&#21152;&#36895;&#26816;&#32034;&#22686;&#24378;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#27979;&#24335;&#26816;&#32034;&#21644;&#25209;&#37327;&#39564;&#35777;&#25552;&#20379;&#20102;&#36890;&#29992;&#30340;&#21152;&#36895;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#20248;&#21270;&#21644;&#24182;&#21457;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.14021</link><description>&lt;p&gt;
&#20351;&#29992;&#25512;&#27979;&#21152;&#36895;&#26816;&#32034;&#22686;&#24378;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Accelerating Retrieval-Augmented Language Model Serving with Speculation. (arXiv:2401.14021v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14021
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;RaLMSpec&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#25512;&#27979;&#21152;&#36895;&#26816;&#32034;&#22686;&#24378;&#22411;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#27979;&#24335;&#26816;&#32034;&#21644;&#25209;&#37327;&#39564;&#35777;&#25552;&#20379;&#20102;&#36890;&#29992;&#30340;&#21152;&#36895;&#25928;&#26524;&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#20248;&#21270;&#21644;&#24182;&#21457;&#22788;&#29702;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;RaLM&#65289;&#36890;&#36807;&#23558;&#38750;&#21442;&#25968;&#30340;&#30693;&#35782;&#24211;&#19982;&#21442;&#25968;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#24050;&#32463;&#23637;&#31034;&#20986;&#35299;&#20915;&#30693;&#35782;&#23494;&#38598;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;&#19982;&#23545;&#23436;&#20840;&#21442;&#25968;&#21270;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#19981;&#21516;&#65292;RaLM&#22312;&#36866;&#24212;&#26368;&#26032;&#25968;&#25454;&#21644;&#26356;&#22909;&#30340;&#26469;&#28304;&#24402;&#23646;&#26426;&#21046;&#26041;&#38754;&#20855;&#26377;&#20302;&#25104;&#26412;&#30340;&#20248;&#21183;&#12290;&#22312;&#20247;&#22810;&#30340;RaLM&#26041;&#27861;&#20013;&#65292;&#36845;&#20195;&#24335;RaLM&#30001;&#20110;&#26816;&#32034;&#22120;&#19982;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#26356;&#39057;&#32321;&#30340;&#20114;&#21160;&#32780;&#20855;&#26377;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;&#23613;&#31649;&#26377;&#36825;&#20123;&#22909;&#22788;&#65292;&#36845;&#20195;&#24335;RaLM&#36890;&#24120;&#20250;&#22240;&#20026;&#39057;&#32321;&#30340;&#26816;&#32034;&#27493;&#39588;&#32780;&#36935;&#21040;&#39640;&#24320;&#38144;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RaLMSpec&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#25512;&#27979;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25512;&#27979;&#24335;&#26816;&#32034;&#21644;&#25209;&#37327;&#39564;&#35777;&#65292;&#33021;&#22815;&#22312;&#20445;&#25345;&#30456;&#21516;&#27169;&#22411;&#36755;&#20986;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#36890;&#29992;&#21152;&#36895;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#32467;&#21512;&#39044;&#21462;&#12289;&#26368;&#20339;&#25512;&#27979;&#27493;&#24133;&#35843;&#24230;&#22120;&#21644;&#24322;&#27493;&#39564;&#35777;&#65292;RaLMSpec&#33021;&#22815;&#33258;&#21160;&#21033;&#29992;&#24182;&#21457;&#24615;&#21644;&#24182;&#34892;&#24615;&#26469;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models (RaLM) have demonstrated the potential to solve knowledge-intensive natural language processing (NLP) tasks by combining a non-parametric knowledge base with a parametric language model. Instead of fine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to the latest data and better source attribution mechanisms. Among various RaLM approaches, iterative RaLM delivers a better generation quality due to a more frequent interaction between the retriever and the language model. Despite the benefits, iterative RaLM usually encounters high overheads due to the frequent retrieval step. To this end, we propose RaLMSpec, a speculation-inspired framework that provides generic speed-up over iterative RaLM while preserving the same model outputs through speculative retrieval and batched verification. By further incorporating prefetching, optimal speculation stride scheduler, and asynchronous verification, RaLMSpec can automatically exploit t
&lt;/p&gt;</description></item><item><title>Unitxt&#26159;&#19968;&#20010;&#28789;&#27963;&#12289;&#21487;&#20849;&#20139;&#21644;&#21487;&#22797;&#29992;&#30340;&#25968;&#25454;&#20934;&#22791;&#19982;&#35780;&#20272;&#24211;&#65292;&#38024;&#23545;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23450;&#21046;&#65292;&#23454;&#29616;&#20102;&#32467;&#26500;&#21270;&#12289;&#27169;&#22359;&#21270;&#21644;&#21487;&#23450;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#24211;&#38598;&#25104;&#20102;&#24120;&#29992;&#30340;&#24211;&#65292;&#23558;&#22788;&#29702;&#27969;&#31243;&#20998;&#35299;&#20026;&#27169;&#22359;&#21270;&#32452;&#20214;&#65292;&#20419;&#36827;&#20102;&#21327;&#20316;&#21644;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2401.14019</link><description>&lt;p&gt;
Unitxt&#65306;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#28789;&#27963;&#12289;&#21487;&#20849;&#20139;&#21644;&#21487;&#22797;&#29992;&#25968;&#25454;&#20934;&#22791;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation for Generative AI. (arXiv:2401.14019v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14019
&lt;/p&gt;
&lt;p&gt;
Unitxt&#26159;&#19968;&#20010;&#28789;&#27963;&#12289;&#21487;&#20849;&#20139;&#21644;&#21487;&#22797;&#29992;&#30340;&#25968;&#25454;&#20934;&#22791;&#19982;&#35780;&#20272;&#24211;&#65292;&#38024;&#23545;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23450;&#21046;&#65292;&#23454;&#29616;&#20102;&#32467;&#26500;&#21270;&#12289;&#27169;&#22359;&#21270;&#21644;&#21487;&#23450;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#24211;&#38598;&#25104;&#20102;&#24120;&#29992;&#30340;&#24211;&#65292;&#23558;&#22788;&#29702;&#27969;&#31243;&#20998;&#35299;&#20026;&#27169;&#22359;&#21270;&#32452;&#20214;&#65292;&#20419;&#36827;&#20102;&#21327;&#20316;&#21644;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#24335;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#20256;&#32479;&#30340;&#25991;&#26412;&#22788;&#29702;&#27969;&#31243;&#38480;&#21046;&#20102;&#30740;&#31350;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#37325;&#29616;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#38024;&#23545;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#27169;&#22411;&#32452;&#21512;&#36827;&#34892;&#20102;&#23450;&#21046;&#12290;&#38543;&#30528;&#31995;&#32479;&#25552;&#31034;&#12289;&#27169;&#22411;&#29305;&#23450;&#26684;&#24335;&#12289;&#25351;&#20196;&#31561;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#38656;&#35201;&#36716;&#21521;&#19968;&#31181;&#32467;&#26500;&#21270;&#12289;&#27169;&#22359;&#21270;&#21644;&#21487;&#23450;&#21046;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Unitxt&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#24211;&#65292;&#19987;&#20026;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23450;&#21046;&#30340;&#25991;&#26412;&#25968;&#25454;&#20934;&#22791;&#21644;&#35780;&#20272;&#32780;&#35774;&#35745;&#12290;Unitxt&#19982;HuggingFace&#21644;LM-eval-harness&#31561;&#24120;&#29992;&#24211;&#36827;&#34892;&#20102;&#26412;&#22320;&#38598;&#25104;&#65292;&#24182;&#23558;&#22788;&#29702;&#27969;&#31243;&#20998;&#35299;&#20026;&#27169;&#22359;&#21270;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#26131;&#20110;&#23450;&#21046;&#21644;&#20849;&#20139;&#30340;&#21151;&#33021;&#12290;&#36825;&#20123;&#32452;&#20214;&#21253;&#25324;&#27169;&#22411;&#29305;&#23450;&#26684;&#24335;&#12289;&#20219;&#21153;&#25552;&#31034;&#21644;&#35768;&#22810;&#20854;&#20182;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#22788;&#29702;&#23450;&#20041;&#12290;Unitxt-Catalog&#38598;&#20013;&#20102;&#36825;&#20123;&#32452;&#20214;&#65292;&#20419;&#36827;&#20102;&#29616;&#20195;&#25991;&#26412;&#25968;&#25454;&#27969;&#31243;&#20013;&#30340;&#21327;&#20316;&#21644;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the dynamic landscape of generative NLP, traditional text processing pipelines limit research flexibility and reproducibility, as they are tailored to specific dataset, task, and model combinations. The escalating complexity, involving system prompts, model-specific formats, instructions, and more, calls for a shift to a structured, modular, and customizable solution. Addressing this need, we present Unitxt, an innovative library for customizable textual data preparation and evaluation tailored to generative language models. Unitxt natively integrates with common libraries like HuggingFace and LM-eval-harness and deconstructs processing flows into modular components, enabling easy customization and sharing between practitioners. These components encompass model-specific formats, task prompts, and many other comprehensive dataset processing definitions. The Unitxt-Catalog centralizes these components, fostering collaboration and exploration in modern textual data workflows. Beyond be
&lt;/p&gt;</description></item><item><title>UALA&#26159;&#19968;&#20010;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#36827;&#34892;&#20195;&#29702;&#21644;&#22806;&#37096;&#19990;&#30028;&#20132;&#20114;&#30340;&#26694;&#26550;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#23427;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#35821;&#35328;&#27169;&#22411;&#23610;&#23544;&#19979;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#23545;&#22806;&#37096;&#19990;&#30028;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2401.14016</link><description>&lt;p&gt;
&#38754;&#21521;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#35821;&#35328;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Towards Uncertainty-Aware Language Agent. (arXiv:2401.14016v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14016
&lt;/p&gt;
&lt;p&gt;
UALA&#26159;&#19968;&#20010;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#36827;&#34892;&#20195;&#29702;&#21644;&#22806;&#37096;&#19990;&#30028;&#20132;&#20114;&#30340;&#26694;&#26550;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#23427;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#35821;&#35328;&#27169;&#22411;&#23610;&#23544;&#19979;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#19988;&#22312;&#23545;&#22806;&#37096;&#19990;&#30028;&#30340;&#20381;&#36182;&#24615;&#26041;&#38754;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#35328;&#26234;&#33021;&#20307;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32622;&#20110;&#26356;&#22810;&#21151;&#33021;&#30340;&#35774;&#35745;&#26680;&#24515;&#20197;&#21450;&#19982;&#22806;&#37096;&#19990;&#30028;&#30340;&#21160;&#24577;&#20132;&#20114;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#36825;&#20123;&#20132;&#20114;&#36807;&#31243;&#20013;&#24573;&#35270;&#20102;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#35821;&#35328;&#26234;&#33021;&#20307;&#65288;UALA&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#32534;&#25490;&#20195;&#29702;&#21644;&#22806;&#37096;&#19990;&#30028;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#19982;&#20854;&#20182;&#30693;&#21517;&#23545;&#25163;&#65288;&#22914;ReAct&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;3&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#65288;HotpotQA&#65292;StrategyQA&#65292;MMLU&#65289;&#21644;&#21508;&#31181;LLM&#23610;&#23544;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;UALA&#22312;&#24615;&#33021;&#26041;&#38754;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#21516;&#26102;&#23545;&#22806;&#37096;&#19990;&#30028;&#30340;&#20381;&#36182;&#24615;&#26174;&#33879;&#38477;&#20302;&#65288;&#21363;&#65292;&#20943;&#23569;&#20102;&#24037;&#20855;&#35843;&#29992;&#21644;&#26631;&#35760;&#25968;&#65289;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#21508;&#31181;&#35265;&#35299;&#65292;&#21253;&#25324;&#19982;&#20195;&#29702;&#24494;&#35843;&#30456;&#27604;&#65292;UALA&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#24182;&#24378;&#35843;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#21475;&#22836;&#32622;&#20449;&#24230;&#20316;&#20026;&#19981;&#30830;&#23450;&#24615;&#30340;&#20195;&#29702;&#26102;&#30340;&#19981;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Language Agents have achieved promising success by placing Large Language Models at the core of a more versatile design that dynamically interacts with the external world, the existing approaches neglect the notion of uncertainty during these interactions. We present the Uncertainty-Aware Language Agent (UALA), a framework that orchestrates the interaction between the agent and the external world using uncertainty quantification. Compared with other well-known counterparts like ReAct, our extensive experiments across 3 representative tasks (HotpotQA, StrategyQA, MMLU) and various LLM sizes demonstrates that UALA brings a significant improvement of performance, while having a substantially lower reliance on the external world (i.e., reduced number of tool calls and tokens). Our analyses provide various insights including the great potential of UALA compared with agent fine-tuning, and underscoring the unreliably of verbalised confidence of LLMs as a proxy for uncertainty.
&lt;/p&gt;</description></item><item><title>CMMU&#26159;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#20174;&#23567;&#23398;&#21040;&#39640;&#20013;&#30340;&#30693;&#35782;&#65292;&#25552;&#20379;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#12289;&#22810;&#39033;&#22238;&#31572;&#39064;&#21644;&#22635;&#31354;&#39064;&#19977;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#27700;&#24179;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.14011</link><description>&lt;p&gt;
CMMU: &#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#19982;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning. (arXiv:2401.14011v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14011
&lt;/p&gt;
&lt;p&gt;
CMMU&#26159;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#20174;&#23567;&#23398;&#21040;&#39640;&#20013;&#30340;&#30693;&#35782;&#65292;&#25552;&#20379;&#20102;&#22810;&#39033;&#36873;&#25321;&#39064;&#12289;&#22810;&#39033;&#22238;&#31572;&#39064;&#21644;&#22635;&#31354;&#39064;&#19977;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#27700;&#24179;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#24182;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#30693;&#35782;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;MLLM&#30340;&#26234;&#33021;&#27700;&#24179;&#25152;&#38656;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#25484;&#25569;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#24403;&#21069;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#22810;&#39033;&#36873;&#25321;&#39064;&#19978;&#65292;&#24182;&#19988;&#22312;&#35780;&#20272;&#30340;&#20840;&#38754;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CMMU&#65292;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#22810;&#27169;&#24577;&#22810;&#31867;&#22411;&#38382;&#39064;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#26032;&#22411;&#22522;&#20934;&#27979;&#35797;&#12290;CMMU&#21253;&#21547;7&#20010;&#23398;&#31185;&#30340;3603&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#23567;&#23398;&#21040;&#39640;&#20013;&#30340;&#30693;&#35782;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#20998;&#20026;&#22810;&#39033;&#36873;&#25321;&#39064;&#12289;&#22810;&#39033;&#22238;&#31572;&#39064;&#21644;&#22635;&#31354;&#39064;&#19977;&#31867;&#65292;&#23545;MLLMs&#25552;&#20986;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20005;&#26684;&#30340;&#35780;&#20272;&#31574;&#30053;&#65292;&#31216;&#20026;ShiftCheck&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models(MLLMs) have achieved remarkable progress and demonstrated powerful knowledge comprehension and reasoning abilities. However, the mastery of domain-specific knowledge, which is essential for evaluating the intelligence of MLLMs, continues to be a challenge. Current multi-modal benchmarks for domain-specific knowledge concentrate on multiple-choice questions and are predominantly available in English, which imposes limitations on the comprehensiveness of the evaluation. To this end, we introduce CMMU, a novel benchmark for multi-modal and multi-type question understanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7 subjects, covering knowledge from primary to high school. The questions can be categorized into 3 types: multiple-choice, multiple-response, and fill-in-the-blank, bringing greater challenges to MLLMs. In addition, we propose a rigorous evaluation strategy called ShiftCheck for assessing multiple-choice questions. The strat
&lt;/p&gt;</description></item><item><title>ConstraintChecker&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25554;&#20214;&#65292;&#29992;&#20110;&#25512;&#29702;&#24120;&#35782;&#30693;&#35782;&#24211;&#12290;&#23427;&#36890;&#36807;&#25552;&#31034;&#25216;&#26415;&#25552;&#20379;&#21644;&#26816;&#26597;&#26174;&#24335;&#32422;&#26463;&#65292;&#24110;&#21161;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#24120;&#35782;&#30693;&#35782;&#24211;&#25512;&#29702;&#26102;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.14003</link><description>&lt;p&gt;
ConstraintChecker&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24120;&#35782;&#30693;&#35782;&#24211;&#30340;&#25554;&#20214;
&lt;/p&gt;
&lt;p&gt;
ConstraintChecker: A Plugin for Large Language Models to Reason on Commonsense Knowledge Bases. (arXiv:2401.14003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14003
&lt;/p&gt;
&lt;p&gt;
ConstraintChecker&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25554;&#20214;&#65292;&#29992;&#20110;&#25512;&#29702;&#24120;&#35782;&#30693;&#35782;&#24211;&#12290;&#23427;&#36890;&#36807;&#25552;&#31034;&#25216;&#26415;&#25552;&#20379;&#21644;&#26816;&#26597;&#26174;&#24335;&#32422;&#26463;&#65292;&#24110;&#21161;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#24120;&#35782;&#30693;&#35782;&#24211;&#25512;&#29702;&#26102;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#24211;&#30340;&#25512;&#29702;&#65288;&#21363;&#24120;&#35782;&#30693;&#35782;&#24211;&#25512;&#29702;&#65289;&#24050;&#34987;&#25506;&#32034;&#20316;&#20026;&#19968;&#31181;&#36890;&#36807;&#21407;&#22987;&#24120;&#35782;&#30693;&#35782;&#24211;&#21644;&#22806;&#37096;&#20808;&#39564;&#30693;&#35782;&#26469;&#33719;&#21462;&#26032;&#30340;&#24120;&#35782;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#24120;&#35782;&#30693;&#35782;&#24211;&#25512;&#29702;&#12290;&#20854;&#20013;&#19968;&#20010;&#38382;&#39064;&#26159;&#65292;&#23427;&#20204;&#24456;&#38590;&#21482;&#36890;&#36807;&#19978;&#19979;&#25991;&#31034;&#20363;&#20174;&#24120;&#35782;&#30693;&#35782;&#24211;&#20013;&#33719;&#21462;&#26174;&#24335;&#20851;&#31995;&#32422;&#26463;&#65292;&#36825;&#26159;&#22240;&#20026;&#32570;&#20047;&#31526;&#21495;&#25512;&#29702;&#33021;&#21147;&#65288;Bengio&#31561;&#20154;&#65292;2021&#24180;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ConstraintChecker&#30340;&#25554;&#20214;&#65292;&#23427;&#22522;&#20110;&#25552;&#31034;&#25216;&#26415;&#25552;&#20379;&#21644;&#26816;&#26597;&#26174;&#24335;&#32422;&#26463;&#12290;&#22312;&#32771;&#34385;&#26032;&#30340;&#30693;&#35782;&#23454;&#20363;&#26102;&#65292;ConstraintChecker&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22359;&#29983;&#25104;&#32422;&#26463;&#21015;&#34920;&#65292;&#28982;&#21518;&#20351;&#29992;&#38646;&#26679;&#26412;&#23398;&#20064;&#27169;&#22359;&#26816;&#26597;&#35813;&#30693;&#35782;&#23454;&#20363;&#26159;&#21542;&#28385;&#36275;&#25152;&#26377;&#32422;&#26463;&#12290;&#28982;&#21518;&#65292;&#33719;&#21462;&#30340;&#32422;&#26463;&#26816;&#26597;&#32467;&#26524;&#34987;&#32858;&#21512;...
&lt;/p&gt;
&lt;p&gt;
Reasoning over Commonsense Knowledge Bases (CSKB), i.e. CSKB reasoning, has been explored as a way to acquire new commonsense knowledge based on reference knowledge in the original CSKBs and external prior knowledge. Despite the advancement of Large Language Models (LLM) and prompt engineering techniques in various reasoning tasks, they still struggle to deal with CSKB reasoning. One of the problems is that it is hard for them to acquire explicit relational constraints in CSKBs from only in-context exemplars, due to a lack of symbolic reasoning capabilities (Bengio et al., 2021). To this end, we proposed **ConstraintChecker**, a plugin over prompting techniques to provide and check explicit constraints. When considering a new knowledge instance, ConstraintChecker employs a rule-based module to produce a list of constraints, then it uses a zero-shot learning module to check whether this knowledge instance satisfies all constraints. The acquired constraint-checking result is then aggrega
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#35843;&#26597;-&#25972;&#21512;-&#24320;&#21457;&#65288;ICE&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#20219;&#21153;&#38388;&#30340;&#33258;&#36827;&#21270;&#26469;&#25552;&#39640;AI&#20195;&#29702;&#30340;&#36866;&#24212;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;ICE&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;API&#35843;&#29992;&#65292;&#21516;&#26102;&#19982;GPT-3.5&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22312;&#21508;&#31181;&#20195;&#29702;&#20219;&#21153;&#19978;&#36798;&#21040;&#19982;GPT-4&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13996</link><description>&lt;p&gt;
&#35843;&#26597;-&#25972;&#21512;-&#24320;&#21457;&#65306;&#19968;&#31181;&#29992;&#20110;&#20219;&#21153;&#38388;&#20195;&#29702;&#33258;&#36827;&#21270;&#30340;&#36890;&#29992;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution. (arXiv:2401.13996v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#35843;&#26597;-&#25972;&#21512;-&#24320;&#21457;&#65288;ICE&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#20219;&#21153;&#38388;&#30340;&#33258;&#36827;&#21270;&#26469;&#25552;&#39640;AI&#20195;&#29702;&#30340;&#36866;&#24212;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;ICE&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;API&#35843;&#29992;&#65292;&#21516;&#26102;&#19982;GPT-3.5&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22312;&#21508;&#31181;&#20195;&#29702;&#20219;&#21153;&#19978;&#36798;&#21040;&#19982;GPT-4&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#35843;&#26597;-&#25972;&#21512;-&#24320;&#21457;&#65288;ICE&#65289;&#30340;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#20219;&#21153;&#38388;&#30340;&#33258;&#36827;&#21270;&#26469;&#25552;&#39640;AI&#20195;&#29702;&#30340;&#36866;&#24212;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#27880;&#37325;&#20219;&#21153;&#20869;&#23398;&#20064;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;ICE&#20419;&#36827;&#20102;&#20219;&#21153;&#38388;&#30693;&#35782;&#30340;&#20256;&#36882;&#65292;&#23454;&#29616;&#20102;&#30495;&#27491;&#30340;&#33258;&#36827;&#21270;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#32463;&#39564;&#23398;&#20064;&#12290;&#35813;&#31574;&#30053;&#21160;&#24577;&#22320;&#35843;&#26597;&#35268;&#21010;&#21644;&#25191;&#34892;&#36712;&#36857;&#65292;&#23558;&#20854;&#25972;&#21512;&#20026;&#31616;&#21270;&#30340;&#24037;&#20316;&#27969;&#21644;&#31649;&#36947;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#25913;&#36827;&#20219;&#21153;&#25191;&#34892;&#12290;&#25105;&#20204;&#22312;XAgent&#26694;&#26550;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;ICE&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#23558;API&#35843;&#29992;&#20943;&#23569;&#39640;&#36798;80&#65285;&#65292;&#24182;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#33021;&#21147;&#30340;&#38656;&#27714;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;&#19982;GPT-3.5&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;ICE&#30340;&#24615;&#33021;&#22312;&#21508;&#31181;&#20195;&#29702;&#20219;&#21153;&#19978;&#19982;&#21407;&#22987;GPT-4&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#33258;&#36827;&#21270;&#26041;&#27861;&#20195;&#34920;&#20102;&#20195;&#29702;&#35774;&#35745;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#20026;&#26356;&#24378;&#22823;&#30340;AI&#31038;&#21306;&#21644;&#29983;&#24577;&#31995;&#32479;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#24182;&#21521;&#20840;&#38754;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#36328;&#36827;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Investigate-Consolidate-Exploit (ICE), a novel strategy for enhancing the adaptability and flexibility of AI agents through inter-task self-evolution. Unlike existing methods focused on intra-task learning, ICE promotes the transfer of knowledge between tasks for genuine self-evolution, similar to human experience learning. The strategy dynamically investigates planning and execution trajectories, consolidates them into simplified workflows and pipelines, and exploits them for improved task execution. Our experiments on the XAgent framework demonstrate ICE's effectiveness, reducing API calls by as much as 80% and significantly decreasing the demand for the model's capability. Specifically, when combined with GPT-3.5, ICE's performance matches that of raw GPT-4 across various agent tasks. We argue that this self-evolution approach represents a paradigm shift in agent design, contributing to a more robust AI community and ecosystem, and moving a step closer to full 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#37322;&#19968;&#33268;&#24615;&#24494;&#35843;&#26041;&#27861;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30456;&#20851;&#31034;&#20363;&#19978;&#29983;&#25104;&#26356;&#19968;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#25552;&#39640;&#20102;10.0%&#30340;&#35299;&#37322;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#27867;&#21270;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.13986</link><description>&lt;p&gt;
&#36890;&#36807;&#35299;&#37322;&#19968;&#33268;&#24615;&#24494;&#35843;&#23454;&#29616;&#19968;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning. (arXiv:2401.13986v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35299;&#37322;&#19968;&#33268;&#24615;&#24494;&#35843;&#26041;&#27861;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30456;&#20851;&#31034;&#20363;&#19978;&#29983;&#25104;&#26356;&#19968;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#25552;&#39640;&#20102;10.0%&#30340;&#35299;&#37322;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#27867;&#21270;&#21040;&#20854;&#20182;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#33021;&#22815;&#29983;&#25104;&#20196;&#20154;&#20449;&#26381;&#12289;&#27969;&#30021;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#19981;&#21516;&#65292;&#23427;&#20204;&#22312;&#19981;&#21516;&#36755;&#20837;&#19978;&#29983;&#25104;&#30340;&#35299;&#37322;&#24120;&#24120;&#19981;&#19968;&#33268;&#12290;&#20363;&#22914;&#65292;LLM&#22312;&#22238;&#31572;&#38382;&#39064;&#8220;&#40635;&#38592;&#33021;&#39134;&#21527;&#65311;&#8221;&#26102;&#21487;&#33021;&#29983;&#25104;&#35299;&#37322;&#8220;&#25152;&#26377;&#40479;&#37117;&#33021;&#39134;&#8221;&#65292;&#20294;&#21516;&#26102;&#22312;&#22238;&#31572;&#19982;&#20043;&#30456;&#20851;&#30340;&#38382;&#39064;&#8220;&#20225;&#40517;&#33021;&#39134;&#21527;&#65311;&#8221;&#26102;&#22238;&#31572;&#8220;&#19981;&#34892;&#8221;&#12290;&#35299;&#37322;&#24212;&#35813;&#22312;&#30456;&#20851;&#31034;&#20363;&#20013;&#20445;&#25345;&#19968;&#33268;&#65292;&#20197;&#20415;&#35753;&#20154;&#31867;&#33021;&#22815;&#27169;&#25311;LLM&#22312;&#22810;&#20010;&#31034;&#20363;&#19978;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#37322;&#19968;&#33268;&#24615;&#24494;&#35843;&#65288;EC-finetuning&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36866;&#24212;LLM&#22312;&#30456;&#20851;&#31034;&#20363;&#19978;&#29983;&#25104;&#26356;&#19968;&#33268;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;EC-finetuning&#21253;&#25324;&#22312;&#32463;&#36807;&#31934;&#24515;&#26500;&#24314;&#30340;&#21253;&#21547;&#19968;&#33268;&#35299;&#37322;&#30340;&#21512;&#25104;&#25968;&#25454;&#19978;&#24494;&#35843;LLM&#12290;&#22312;&#21508;&#31181;&#19981;&#21516;&#39046;&#22495;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#65292;EC-finetuning&#22312;&#22235;&#20010;&#24494;&#35843;&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#25552;&#39640;&#20102;10.0%&#30340;&#35299;&#37322;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#22312;&#19971;&#20010;&#22806;&#37096;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) often generate convincing, fluent explanations. However, different from humans, they often generate inconsistent explanations on different inputs. For example, an LLM may generate the explanation "all birds can fly" when answering the question "Can sparrows fly?" but meanwhile answer "no" to the related question "Can penguins fly?". Explanations should be consistent across related examples so that they allow a human to simulate the LLM's decision process on multiple examples. We propose explanation-consistency finetuning (EC-finetuning), a method that adapts LLMs to generate more consistent natural-language explanations on related examples. EC-finetuning involves finetuning LLMs on synthetic data that is carefully constructed to contain consistent explanations. Across a variety of question-answering datasets in various domains, EC-finetuning yields a 10.0% relative explanation consistency improvement on four finetuning datasets, and generalizes to seven out
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Leeroo&#32534;&#25490;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#35757;&#32451;&#36807;&#30340;LLMs&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#35813;&#32534;&#25490;&#22120;&#22312;&#24615;&#33021;&#19978;&#19982;Mixtral&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#25104;&#26412;&#21482;&#26377;&#20854;&#19977;&#20998;&#20043;&#20108;&#12290;&#24403;&#20801;&#35768;&#26356;&#39640;&#30340;&#25104;&#26412;&#26102;&#65292;Leeroo&#32534;&#25490;&#22120;&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;Mixtral&#27169;&#22411;&#65292;&#24182;&#19988;&#24403;&#38598;&#25104;GPT4&#26102;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.13979</link><description>&lt;p&gt;
Leeroo Orchestrator: &#36890;&#36807;&#27169;&#22411;&#38598;&#25104;&#25552;&#39640;LLMs&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration. (arXiv:2401.13979v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Leeroo&#32534;&#25490;&#22120;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#38598;&#25104;&#22810;&#20010;&#35757;&#32451;&#36807;&#30340;LLMs&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#35813;&#32534;&#25490;&#22120;&#22312;&#24615;&#33021;&#19978;&#19982;Mixtral&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#25104;&#26412;&#21482;&#26377;&#20854;&#19977;&#20998;&#20043;&#20108;&#12290;&#24403;&#20801;&#35768;&#26356;&#39640;&#30340;&#25104;&#26412;&#26102;&#65292;Leeroo&#32534;&#25490;&#22120;&#30340;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;Mixtral&#27169;&#22411;&#65292;&#24182;&#19988;&#24403;&#38598;&#25104;GPT4&#26102;&#36827;&#19968;&#27493;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26550;&#26500;&#65292;&#21033;&#29992;&#22810;&#20010;&#35757;&#32451;&#36807;&#30340;LLMs&#30340;&#38598;&#20307;&#30693;&#35782;&#65292;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#32534;&#25490;&#22120;&#65292;&#33021;&#22815;&#36873;&#25321;&#26368;&#20339;&#30340;&#24213;&#23618;LLM&#19987;&#23478;&#36827;&#34892;&#20219;&#21153;&#25191;&#34892;&#12290;&#21463;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33258;&#25105;&#23545;&#24328;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26597;&#35810;&#29983;&#25104;&#12289;&#32534;&#25490;&#21644;&#35780;&#20272;&#30340;&#24490;&#29615;&#65292;&#20026;&#32534;&#25490;&#22120;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#20027;&#35201;&#38024;&#23545;MMLU&#22522;&#20934;&#65292;&#22312;Hugging Face&#19978;&#20351;&#29992;&#20102;&#20855;&#26377;7B&#12289;13B&#21644;34B&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;Leeroo&#32534;&#25490;&#22120;&#23454;&#29616;&#20102;&#19982;Mixtral&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#20294;&#21482;&#20135;&#29983;&#20102;&#20854;&#25104;&#26412;&#30340;&#19977;&#20998;&#20043;&#20108;&#12290;&#27492;&#22806;&#65292;&#22686;&#21152;&#20801;&#35768;&#30340;&#25104;&#26412;&#36229;&#36807;&#20102;Mixtral&#30340;&#20934;&#30830;&#24615;&#65292;&#36798;&#21040;&#20102;75.9%&#30340;&#20934;&#30830;&#24615;&#12290;&#24403;&#23558;GPT4&#38598;&#25104;&#21040;&#24213;&#23618;&#27169;&#22411;&#27744;&#20013;&#26102;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20063;&#24471;&#21040;&#20102;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an architecture to harness the collective knowledge of multiple trained LLMs to create a new state-of-the-art. At the core of this framework is a LLM-based orchestrator that is adept at picking the right underlying LLM experts for optimal task execution. Inspired by self-play in reinforcement learning, we created a loop of query generation, orchestration, and evaluation to generate training data for the orchestrator. Our evaluation focused on the MMLU benchmark, employing models with 7B, 13B, and 34B parameters available on Hugging Face. The results demonstrate new state-of-the-art open-source models: Our Leeroo orchestrator achieves performance on par with the Mixtral model while incurring only two-thirds of its cost. Moreover, increasing the allowed cost surpasses Mixtral's accuracy by over 5% at the same cost level, reaching an accuracy of 75.9%. Further enhancements were observed when integrating GPT4 into the underlying model pool. The Leeroo orchestrator
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#23383;&#27700;&#21360;&#31574;&#30053;&#65292;&#36890;&#36807;&#36741;&#21161;&#27169;&#22411;&#27979;&#37327;&#39640;&#29109;&#20196;&#29260;&#20998;&#24067;&#65292;&#23558;&#27700;&#21360;&#33258;&#36866;&#24212;&#22320;&#28155;&#21152;&#21040;&#20855;&#26377;&#39640;&#29109;&#30340;&#20196;&#29260;&#20998;&#24067;&#20013;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#29109;&#20196;&#29260;&#20998;&#24067;&#19981;&#21464;&#65292;&#20197;&#25552;&#39640;&#25991;&#26412;&#36136;&#37327;&#21644;&#27700;&#21360;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13927</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#36866;&#24212;&#25991;&#23383;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Adaptive Text Watermark for Large Language Models. (arXiv:2401.13927v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13927
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25991;&#23383;&#27700;&#21360;&#31574;&#30053;&#65292;&#36890;&#36807;&#36741;&#21161;&#27169;&#22411;&#27979;&#37327;&#39640;&#29109;&#20196;&#29260;&#20998;&#24067;&#65292;&#23558;&#27700;&#21360;&#33258;&#36866;&#24212;&#22320;&#28155;&#21152;&#21040;&#20855;&#26377;&#39640;&#29109;&#30340;&#20196;&#29260;&#20998;&#24067;&#20013;&#65292;&#21516;&#26102;&#20445;&#25345;&#20302;&#29109;&#20196;&#29260;&#20998;&#24067;&#19981;&#21464;&#65292;&#20197;&#25552;&#39640;&#25991;&#26412;&#36136;&#37327;&#21644;&#27700;&#21360;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#28389;&#29992;&#30340;&#25285;&#24551;&#65292;&#32780;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#23383;&#27700;&#21360;&#25104;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#20445;&#25345;&#27700;&#21360;&#24378;&#24230;&#12289;&#31283;&#20581;&#24615;&#21644;&#26080;&#38656;&#39044;&#20808;&#30693;&#36947;&#25552;&#31034;&#25110;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#27700;&#21360;&#30340;&#21516;&#26102;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24102;&#27700;&#21360;&#25991;&#26412;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#27700;&#21360;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#25991;&#26412;&#36136;&#37327;&#21644;&#20445;&#25345;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#26681;&#25454;&#36741;&#21161;&#27169;&#22411;&#27979;&#37327;&#30340;&#39640;&#29109;&#20196;&#29260;&#20998;&#24067;&#33258;&#36866;&#24212;&#22320;&#28155;&#21152;&#27700;&#21360;&#65292;&#32780;&#20445;&#25345;&#20302;&#29109;&#20196;&#29260;&#20998;&#24067;&#19981;&#21464;&#12290;&#20026;&#20102;&#20445;&#35777;&#23433;&#20840;&#24615;&#24182;&#36827;&#19968;&#27493;&#20943;&#23567;&#27700;&#21360;&#23545;&#25991;&#26412;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#19981;&#20877;&#20351;&#29992;&#20174;&#38543;&#26426;&#31192;&#38053;&#29983;&#25104;&#30340;&#22266;&#23450;&#32418;/&#32511;&#21517;&#21333;&#65292;&#32780;&#26159;&#26681;&#25454;&#21069;&#19968;&#20010;&#35821;&#20041;&#23884;&#20837;&#23558;&#36755;&#20986;&#23545;&#25968;&#27604;&#20363;&#36866;&#24212;&#24615;&#32553;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of Large Language Models (LLMs) has led to increasing concerns about the misuse of AI-generated text, and watermarking for LLM-generated text has emerged as a potential solution. However, it is challenging to generate high-quality watermarked text while maintaining strong security, robustness, and the ability to detect watermarks without prior knowledge of the prompt or model. This paper proposes an adaptive watermarking strategy to address this problem. To improve the text quality and maintain robustness, we adaptively add watermarking to token distributions with high entropy measured using an auxiliary model and keep the low entropy token distributions untouched. For the sake of security and to further minimize the watermark's impact on text quality, instead of using a fixed green/red list generated from a random secret key, which can be vulnerable to decryption and forgery, we adaptively scale up the output logits in proportion based on the semantic embedding of prev
&lt;/p&gt;</description></item><item><title>LocMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13920</link><description>&lt;p&gt;
LocMoE: &#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#20302;&#24320;&#38144;MoE
&lt;/p&gt;
&lt;p&gt;
LocMoE: A Low-overhead MoE for Large Language Model Training. (arXiv:2401.13920v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13920
&lt;/p&gt;
&lt;p&gt;
LocMoE&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#65292;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#27169;&#22411;&#65288;MoE&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#20998;&#24067;&#24335;&#21644;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#26377;&#25928;&#31232;&#30095;&#21644;&#25193;&#23637;&#27169;&#22411;&#65292;&#22240;&#27492;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;MoE&#30340;&#24615;&#33021;&#21463;&#21040;&#36127;&#36733;&#19981;&#24179;&#34913;&#21644;&#20840;&#23545;&#20840;&#36890;&#20449;&#30340;&#39640;&#24310;&#36831;&#30340;&#38480;&#21046;&#65292;&#21516;&#26102;&#30001;&#20110;&#22823;&#37327;&#30340;&#19987;&#23478;&#23481;&#37327;&#23548;&#33268;&#30456;&#23545;&#20887;&#20313;&#30340;&#35745;&#31639;&#12290;&#36127;&#36733;&#19981;&#24179;&#34913;&#21487;&#33021;&#26159;&#30001;&#20110;&#29616;&#26377;&#36335;&#30001;&#31574;&#30053;&#22987;&#32456;&#20542;&#21521;&#20110;&#36873;&#25321;&#29305;&#23450;&#30340;&#19987;&#23478;&#23548;&#33268;&#30340;&#12290;&#20840;&#23545;&#20840;&#36807;&#31243;&#20013;&#39057;&#32321;&#30340;&#33410;&#28857;&#38388;&#36890;&#20449;&#20063;&#26174;&#33879;&#24310;&#38271;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#20026;&#20102;&#32531;&#35299;&#19978;&#36848;&#24615;&#33021;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36335;&#30001;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#37096;&#20998;&#33410;&#28857;&#38388;&#36890;&#20449;&#36716;&#25442;&#20026;&#33410;&#28857;&#20869;&#36890;&#20449;&#65292;&#32467;&#21512;&#36127;&#36733;&#24179;&#34913;&#21644;&#23616;&#37096;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#19987;&#23478;&#23481;&#37327;&#30340;&#26368;&#23567;&#38408;&#20540;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#30340;&#38376;&#25511;&#26435;&#37325;&#19982;&#20998;&#37197;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#26368;&#22823;&#35282;&#20559;&#24046;&#35745;&#31639;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Mixtures-of-Experts (MoE) model is a widespread distributed and integrated learning method for large language models (LLM), which is favored due to its ability to sparsify and expand models efficiently. However, the performance of MoE is limited by load imbalance and high latency of All-To-All communication, along with relatively redundant computation owing to large expert capacity. Load imbalance may result from existing routing policies that consistently tend to select certain experts. The frequent inter-node communication in the All-To-All procedure also significantly prolongs the training time. To alleviate the above performance problems, we propose a novel routing strategy that combines load balance and locality by converting partial inter-node communication to that of intra-node. Notably, we elucidate that there is a minimum threshold for expert capacity, calculated through the maximal angular deviation between the gating weights of the experts and the assigned tokens. We por
&lt;/p&gt;</description></item><item><title>WebVoyager&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;Web&#20195;&#29702;&#65292;&#33021;&#22815;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#20132;&#20114;&#26469;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13919</link><description>&lt;p&gt;
WebVoyager&#65306;&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#30340;Web Agent
&lt;/p&gt;
&lt;p&gt;
WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. (arXiv:2401.13919v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13919
&lt;/p&gt;
&lt;p&gt;
WebVoyager&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;Web&#20195;&#29702;&#65292;&#33021;&#22815;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#20132;&#20114;&#26469;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#24341;&#39046;&#20102;&#19968;&#20010;&#30001;&#30495;&#23454;&#19990;&#30028;&#20013;&#33258;&#20027;&#24212;&#29992;&#31243;&#24207;&#30340;&#21457;&#23637;&#25152;&#26631;&#24535;&#30340;&#26032;&#26102;&#20195;&#65292;&#25512;&#21160;&#20102;&#22522;&#20110;&#32593;&#32476;&#30340;&#39640;&#32423;&#20195;&#29702;&#30340;&#21019;&#26032;&#12290;&#29616;&#26377;&#30340;&#32593;&#32476;&#20195;&#29702;&#36890;&#24120;&#21482;&#22788;&#29702;&#19968;&#20010;&#36755;&#20837;&#27169;&#24577;&#65292;&#24182;&#19988;&#20165;&#22312;&#31616;&#21270;&#30340;&#32593;&#32476;&#27169;&#25311;&#22120;&#25110;&#38745;&#24577;&#30340;&#32593;&#32476;&#24555;&#29031;&#20013;&#36827;&#34892;&#35780;&#20272;&#65292;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WebVoyager&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;Web&#20195;&#29702;&#65292;&#36890;&#36807;&#19982;&#30495;&#23454;&#32593;&#31449;&#36827;&#34892;&#20132;&#20114;&#65292;&#33021;&#22815;&#31471;&#21040;&#31471;&#22320;&#23436;&#25104;&#29992;&#25143;&#25351;&#20196;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;Web&#20195;&#29702;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#24335;Web&#20195;&#29702;&#20219;&#21153;&#30340;&#33258;&#21160;&#35780;&#20272;&#25361;&#25112;&#65292;&#21033;&#29992;&#20102;GPT-4V&#30340;&#24378;&#22823;&#22810;&#27169;&#24577;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;15&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#32593;&#31449;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26469;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;WebVoyager&#23454;&#29616;&#20102;55.7&#65285;&#30340;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#26174;&#33879;&#22320;.....
&lt;/p&gt;
&lt;p&gt;
The advancement of large language models (LLMs) leads to a new era marked by the development of autonomous applications in the real world, which drives innovation in the creation of advanced web-based agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we propose a new evaluation protocol for web agents to address the challenges of automatic evaluation of open-ended web agent tasks, leveraging the robust multimodal comprehension capabilities of GPT-4V. We create a new benchmark by gathering real-world tasks from 15 widely used websites to evaluate our agents. We show that WebVoyager achieves a 55.7% task success rate, significantly 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#19978;&#37319;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#32416;&#27491;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#20266;&#20687;&#12290;&#32463;&#36807;&#20351;&#29992;&#35813;&#31639;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#25972;&#20307;&#21644;&#20462;&#35746;&#30340;&#23376;&#38598;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.13907</link><description>&lt;p&gt;
&#27809;&#26377;&#26356;&#22810;&#24178;&#25200;&#65306;&#19968;&#31181;&#33258;&#36866;&#24212;&#19978;&#37319;&#26679;&#31639;&#27861;&#20197;&#20943;&#23569;&#25968;&#25454;&#20266;&#20687;
&lt;/p&gt;
&lt;p&gt;
No More Distractions: an Adaptive Up-Sampling Algorithm to Reduce Data Artifacts. (arXiv:2401.13907v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13907
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#19978;&#37319;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#32416;&#27491;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#20266;&#20687;&#12290;&#32463;&#36807;&#20351;&#29992;&#35813;&#31639;&#27861;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#25972;&#20307;&#21644;&#20462;&#35746;&#30340;&#23376;&#38598;&#19978;&#34920;&#29616;&#26174;&#33879;&#20248;&#20110;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#26368;&#36817;&#21457;&#29616;&#65292;&#26377;&#26102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#26159;&#23545;&#20110;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#24494;&#23567;&#21464;&#21270;&#26080;&#27861;&#24456;&#22909;&#22320;&#36827;&#34892;&#27867;&#21270;&#12290;&#36825;&#26377;&#26102;&#26159;&#30001;&#20110;&#25968;&#25454;&#20266;&#20687;&#65292;&#27169;&#22411;&#23398;&#20064;&#20102;&#26631;&#35760;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#20851;&#32852;&#65292;&#32780;&#19981;&#26159;&#35821;&#20041;&#21644;&#36923;&#36753;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;SNLI&#25968;&#25454;&#24182;&#21487;&#35270;&#21270;&#20102;&#36825;&#31181;&#34394;&#20551;&#20851;&#32852;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#19978;&#37319;&#26679;&#31639;&#27861;&#26469;&#32416;&#27491;&#25968;&#25454;&#20266;&#20687;&#65292;&#35813;&#31639;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20154;&#24037;&#32534;&#36753;&#25110;&#27880;&#37322;&#12290;&#25105;&#20204;&#22312;SNLI&#25968;&#25454;&#20013;&#24212;&#29992;&#35813;&#31639;&#27861;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#32463;&#36807;&#32416;&#27491;&#21518;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#25972;&#20307;&#21644;&#25105;&#20204;&#32416;&#27491;&#30340;&#23376;&#38598;&#19978;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#22312;&#21407;&#22987;SNLI&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers recently found out that sometimes language models achieve high accuracy on benchmark data set, but they can not generalize very well with even little changes to the original data set. This is sometimes due to data artifacts, model is learning the spurious correlation between tokens and labels, instead of the semantics and logic. In this work, we analyzed SNLI data and visualized such spurious correlations. We proposed an adaptive up-sampling algorithm to correct the data artifacts, which is simple and effective, and does not need human edits or annotation. We did an experiment applying the algorithm to fix the data artifacts in SNLI data and the model trained with corrected data performed significantly better than the model trained with raw SNLI data, overall, as well as on the subset we corrected.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#21160;&#24577;&#23884;&#20837;&#20027;&#39064;&#27169;&#22411;&#21644;&#21464;&#28857;&#26816;&#27979;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#23398;&#21644;&#21382;&#21490;&#25991;&#26412;&#20013;&#30340;&#35789;&#27719;&#35821;&#20041;&#27169;&#24577;&#30340;&#21382;&#26102;&#21464;&#21270;&#65292;&#20026;&#27604;&#36739;&#25991;&#23398;&#21644;&#21476;&#20856;&#23398;&#30340;&#20256;&#32479;&#23398;&#26415;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#36866;&#21512;&#30340;&#35821;&#26009;&#24211;&#65292;&#26410;&#26469;&#36824;&#21487;&#20197;&#36890;&#36807;&#25913;&#36827;&#21644;&#25193;&#23637;&#20351;&#20854;&#36866;&#24212;&#26356;&#22810;&#30340;&#26410;&#22788;&#29702;&#26448;&#26009;&#12290;</title><link>http://arxiv.org/abs/2401.13905</link><description>&lt;p&gt;
&#21160;&#24577;&#23884;&#20837;&#20027;&#39064;&#27169;&#22411;&#21644;&#21464;&#28857;&#26816;&#27979;&#29992;&#20110;&#25506;&#32034;&#25991;&#23398;-&#21382;&#21490;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Dynamic embedded topic models and change-point detection for exploring literary-historical hypotheses. (arXiv:2401.13905v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#21160;&#24577;&#23884;&#20837;&#20027;&#39064;&#27169;&#22411;&#21644;&#21464;&#28857;&#26816;&#27979;&#32467;&#21512;&#36215;&#26469;&#65292;&#36890;&#36807;&#25506;&#32034;&#25991;&#23398;&#21644;&#21382;&#21490;&#25991;&#26412;&#20013;&#30340;&#35789;&#27719;&#35821;&#20041;&#27169;&#24577;&#30340;&#21382;&#26102;&#21464;&#21270;&#65292;&#20026;&#27604;&#36739;&#25991;&#23398;&#21644;&#21476;&#20856;&#23398;&#30340;&#20256;&#32479;&#23398;&#26415;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36866;&#29992;&#20110;&#20219;&#20309;&#36866;&#21512;&#30340;&#35821;&#26009;&#24211;&#65292;&#26410;&#26469;&#36824;&#21487;&#20197;&#36890;&#36807;&#25913;&#36827;&#21644;&#25193;&#23637;&#20351;&#20854;&#36866;&#24212;&#26356;&#22810;&#30340;&#26410;&#22788;&#29702;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#23884;&#20837;&#20027;&#39064;&#27169;&#22411;&#21644;&#21464;&#28857;&#26816;&#27979;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#25506;&#32034;&#32463;&#20856;&#21644;&#26089;&#26399;&#22522;&#30563;&#25945;&#25289;&#19969;&#35821;&#20013;&#35789;&#27719;&#35821;&#20041;&#27169;&#24577;&#30340;&#21382;&#26102;&#21464;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#31181;&#29992;&#20110;&#21457;&#29616;&#21644;&#34920;&#24449;&#36755;&#20986;&#20013;&#30340;&#27169;&#24335;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#27604;&#36739;&#25991;&#23398;&#21644;&#21476;&#20856;&#23398;&#30340;&#20256;&#32479;&#23398;&#26415;&#30740;&#31350;&#30456;&#20851;&#32852;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#31616;&#21333;&#30340;&#26080;&#30417;&#30563;&#27169;&#22411;&#30340;&#35821;&#20041;&#21464;&#21270;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#21512;&#36866;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#36890;&#36807;&#26410;&#26469;&#30340;&#26041;&#21521;&#21644;&#25913;&#36827;&#26469;&#20351;&#26356;&#22810;&#30340;&#26410;&#32534;&#36753;&#26448;&#26009;&#36798;&#21040;&#38408;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel combination of dynamic embedded topic models and change-point detection to explore diachronic change of lexical semantic modality in classical and early Christian Latin. We demonstrate several methods for finding and characterizing patterns in the output, and relating them to traditional scholarship in Comparative Literature and Classics. This simple approach to unsupervised models of semantic change can be applied to any suitable corpus, and we conclude with future directions and refinements aiming to allow noisier, less-curated materials to meet that threshold.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30417;&#30563;&#24314;&#27169;&#22312;&#20083;&#33146;&#30284;&#30149;&#29702;&#20998;&#31867;&#19978;&#30340;&#38646;&#26679;&#26412;&#25512;&#26029;&#33021;&#21147;&#65292;&#21457;&#29616;GPT-4&#27169;&#22411;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#35201;&#20040;&#26126;&#26174;&#20248;&#20110;&#65292;&#35201;&#20040;&#19982;&#26368;&#20339;&#30340;&#30417;&#30563;&#27169;&#22411;LSTM-Att&#27169;&#22411;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2401.13887</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30417;&#30563;&#24314;&#27169;&#30340;&#38646;&#26679;&#26412;&#25512;&#26029;&#36827;&#34892;&#20083;&#33146;&#30284;&#30149;&#29702;&#20998;&#31867;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A comparative study of zero-shot inference with large language models and supervised modeling in breast cancer pathology classification. (arXiv:2401.13887v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30417;&#30563;&#24314;&#27169;&#22312;&#20083;&#33146;&#30284;&#30149;&#29702;&#20998;&#31867;&#19978;&#30340;&#38646;&#26679;&#26412;&#25512;&#26029;&#33021;&#21147;&#65292;&#21457;&#29616;GPT-4&#27169;&#22411;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#35201;&#20040;&#26126;&#26174;&#20248;&#20110;&#65292;&#35201;&#20040;&#19982;&#26368;&#20339;&#30340;&#30417;&#30563;&#27169;&#22411;LSTM-Att&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#22312;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#20449;&#24687;&#26041;&#38754;&#21313;&#20998;&#27969;&#34892;&#65292;&#20294;&#21019;&#24314;&#22823;&#22411;&#27880;&#37322;&#25968;&#25454;&#38598;&#38656;&#35201;&#24191;&#27867;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#32791;&#36153;&#26102;&#38388;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#24456;&#24378;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26368;&#36817;&#30340;LLMs&#26159;&#21542;&#21487;&#20197;&#20943;&#23569;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;769&#20010;&#20083;&#33146;&#30284;&#30149;&#29702;&#25253;&#21578;&#30340;&#25968;&#25454;&#38598;&#65288;&#26631;&#27880;&#20102;13&#20010;&#31867;&#21035;&#65289;&#65292;&#26469;&#27604;&#36739;GPT-4&#27169;&#22411;&#21644;GPT-3.5&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#33021;&#21147;&#19982;&#19977;&#31181;&#27169;&#22411;&#26550;&#26500;&#30340;&#30417;&#30563;&#20998;&#31867;&#24615;&#33021;&#65306;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#65292;&#20855;&#26377;&#27880;&#24847;&#21147;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM-Att&#65289;&#21644;UCSF-BERT&#27169;&#22411;&#12290;&#22312;&#25152;&#26377;13&#20010;&#20219;&#21153;&#20013;&#65292;GPT-4&#27169;&#22411;&#30340;&#24615;&#33021;&#35201;&#20040;&#26126;&#26174;&#20248;&#20110;&#26368;&#20339;&#30340;&#30417;&#30563;&#27169;&#22411;LSTM-Att&#27169;&#22411;&#65288;&#24179;&#22343;&#23439;F1&#24471;&#20998;&#20026;0.83 vs. 0.75&#65289;&#65292;&#35201;&#20040;&#19982;&#20854;&#30456;&#24403;&#12290;&#22312;&#23384;&#22312;&#26631;&#31614;&#20043;&#38388;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#20219;&#21153;&#20013;&#65292;di
&lt;/p&gt;
&lt;p&gt;
Although supervised machine learning is popular for information extraction from clinical notes, creating large annotated datasets requires extensive domain expertise and is time-consuming. Meanwhile, large language models (LLMs) have demonstrated promising transfer learning capability. In this study, we explored whether recent LLMs can reduce the need for large-scale data annotations. We curated a manually-labeled dataset of 769 breast cancer pathology reports, labeled with 13 categories, to compare zero-shot classification capability of the GPT-4 model and the GPT-3.5 model with supervised classification performance of three model architectures: random forests classifier, long short-term memory networks with attention (LSTM-Att), and the UCSF-BERT model. Across all 13 tasks, the GPT-4 model performed either significantly better than or as well as the best supervised model, the LSTM-Att model (average macro F1 score of 0.83 vs. 0.75). On tasks with high imbalance between labels, the di
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#20013;&#23384;&#22312;&#30340;&#31181;&#26063;&#20559;&#35265;&#65292;&#24182;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#35777;&#26126;&#20102;&#36825;&#20123;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#20559;&#35265;&#20027;&#35201;&#34920;&#29616;&#20026;&#23545;&#30333;&#20154;&#31181;&#26063;&#30340;&#39640;&#25252;&#29702;&#25104;&#26412;&#21644;&#20303;&#38498;&#26102;&#38388;&#39044;&#27979;&#65292;&#20197;&#21450;&#22312;&#20020;&#24202;&#21307;&#23398;&#20013;&#38754;&#20020;&#25361;&#25112;&#30340;&#24773;&#20917;&#19979;&#21576;&#29616;&#36807;&#24230;&#20048;&#35266;&#30340;&#29983;&#23384;&#29575;&#12290;&#24847;&#35782;&#21040;&#36825;&#20123;&#20559;&#35265;&#30340;&#23384;&#22312;&#21644;&#24433;&#21709;&#65292;&#23545;&#20110;&#22312;&#20851;&#38190;&#30340;&#21307;&#30103;&#24212;&#29992;&#20013;&#30830;&#20445;&#20844;&#24179;&#21644;&#20934;&#30830;&#30340;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.13867</link><description>&lt;p&gt;
&#25581;&#31034;&#21644;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#20013;&#30340;&#31181;&#26063;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Unmasking and Quantifying Racial Bias of Large Language Models in Medical Report Generation. (arXiv:2401.13867v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13867
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#20013;&#23384;&#22312;&#30340;&#31181;&#26063;&#20559;&#35265;&#65292;&#24182;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#35777;&#26126;&#20102;&#36825;&#20123;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#20559;&#35265;&#20027;&#35201;&#34920;&#29616;&#20026;&#23545;&#30333;&#20154;&#31181;&#26063;&#30340;&#39640;&#25252;&#29702;&#25104;&#26412;&#21644;&#20303;&#38498;&#26102;&#38388;&#39044;&#27979;&#65292;&#20197;&#21450;&#22312;&#20020;&#24202;&#21307;&#23398;&#20013;&#38754;&#20020;&#25361;&#25112;&#30340;&#24773;&#20917;&#19979;&#21576;&#29616;&#36807;&#24230;&#20048;&#35266;&#30340;&#29983;&#23384;&#29575;&#12290;&#24847;&#35782;&#21040;&#36825;&#20123;&#20559;&#35265;&#30340;&#23384;&#22312;&#21644;&#24433;&#21709;&#65292;&#23545;&#20110;&#22312;&#20851;&#38190;&#30340;&#21307;&#30103;&#24212;&#29992;&#20013;&#30830;&#20445;&#20844;&#24179;&#21644;&#20934;&#30830;&#30340;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT-3.5-turbo&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#19987;&#19994;&#39046;&#22495;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#32487;&#25215;&#20559;&#35265;&#65292;&#21487;&#33021;&#24433;&#21709;&#20854;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;&#25928;&#29992;&#12290;&#23613;&#31649;&#36807;&#21435;&#26377;&#19968;&#20123;&#23581;&#35797;&#65292;&#20294;&#36825;&#20123;&#20559;&#35265;&#30340;&#30830;&#20999;&#24433;&#21709;&#21644;&#31243;&#24230;&#20173;&#19981;&#30830;&#23450;&#12290;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20542;&#21521;&#20110;&#20026;&#30333;&#20154;&#31181;&#26063;&#25237;&#23556;&#26356;&#39640;&#30340;&#21307;&#30103;&#36153;&#29992;&#21644;&#36739;&#38271;&#30340;&#20303;&#38498;&#26102;&#38388;&#65292;&#24182;&#22312;&#38754;&#20020;&#25361;&#25112;&#24615;&#21307;&#23398;&#24773;&#20917;&#26102;&#21576;&#29616;&#20048;&#35266;&#30340;&#29983;&#23384;&#29575;&#12290;&#36825;&#20123;&#20559;&#35265;&#19982;&#29616;&#23454;&#20013;&#30340;&#21307;&#30103;&#24046;&#24322;&#30456;&#19968;&#33268;&#65292;&#21487;&#22312;&#29983;&#25104;&#24739;&#32773;&#32972;&#26223;&#12289;&#23558;&#29305;&#23450;&#30142;&#30149;&#19982;&#26576;&#20123;&#31181;&#26063;&#30456;&#20851;&#32852;&#20197;&#21450;&#27835;&#30103;&#25512;&#33616;&#30340;&#24046;&#24322;&#31561;&#26041;&#38754;&#20307;&#29616;&#20986;&#26469;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#23558;&#26469;&#38656;&#35201;&#24320;&#23637;&#30740;&#31350;&#26469;&#35299;&#20915;&#21644;&#20943;&#36731;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#20851;&#38190;&#30340;&#21307;&#30103;&#24212;&#29992;&#20013;&#65292;&#20197;&#30830;&#20445;&#20844;&#27491;&#21644;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models like GPT-3.5-turbo and GPT-4 hold promise for healthcare professionals, but they may inadvertently inherit biases during their training, potentially affecting their utility in medical applications. Despite few attempts in the past, the precise impact and extent of these biases remain uncertain. Through both qualitative and quantitative analyses, we find that these models tend to project higher costs and longer hospitalizations for White populations and exhibit optimistic views in challenging medical scenarios with much higher survival rates. These biases, which mirror real-world healthcare disparities, are evident in the generation of patient backgrounds, the association of specific diseases with certain races, and disparities in treatment recommendations, etc. Our findings underscore the critical need for future research to address and mitigate biases in language models, especially in critical healthcare applications, to ensure fair and accurate outcomes for all 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21407;&#21017;&#21457;&#29616;&#21644;&#25351;&#23548;&#25552;&#21319;&#23398;&#29983;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;TPD&#26694;&#26550;&#27169;&#25311;&#20102;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#36890;&#36807;&#29983;&#25104;&#38382;&#39064;&#35299;&#20915;&#25351;&#20196;&#21644;&#32416;&#27491;&#21407;&#21017;&#65292;&#20174;&#32780;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#20174;&#25945;&#24072;&#30340;&#25351;&#23548;&#21644;&#33258;&#36523;&#30340;&#38169;&#35823;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2401.13849</link><description>&lt;p&gt;
TPD: &#36890;&#36807;&#21407;&#21017;&#21457;&#29616;&#21644;&#25351;&#23548;&#25552;&#21319;&#23398;&#29983;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
TPD: Enhancing Student Language Model Reasoning via Principle Discovery and Guidance. (arXiv:2401.13849v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13849
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21407;&#21017;&#21457;&#29616;&#21644;&#25351;&#23548;&#25552;&#21319;&#23398;&#29983;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;TPD&#26694;&#26550;&#27169;&#25311;&#20102;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#36890;&#36807;&#29983;&#25104;&#38382;&#39064;&#35299;&#20915;&#25351;&#20196;&#21644;&#32416;&#27491;&#21407;&#21017;&#65292;&#20174;&#32780;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#20174;&#25945;&#24072;&#30340;&#25351;&#23548;&#21644;&#33258;&#36523;&#30340;&#38169;&#35823;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26368;&#36817;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#24778;&#21497;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#24448;&#24448;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#22240;&#27492;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#33021;&#21147;&#20174;&#36739;&#22823;&#30340;&#27169;&#22411;&#36716;&#31227;&#21040;&#36739;&#23567;&#30340;&#27169;&#22411;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#24494;&#35843;&#25968;&#25454;&#25110;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19982;&#20248;&#31168;&#30340;&#25945;&#24072;LLM&#36827;&#34892;&#25345;&#32493;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#21017;&#30340;&#24072;&#29983;&#26694;&#26550;&#65292;&#31216;&#20026;&#8220;&#36890;&#36807;&#21407;&#21017;&#21457;&#29616;&#25945;&#23398;&#8221;(TPD)&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#21463;&#20154;&#31867;&#23398;&#20064;&#26426;&#21046;&#21551;&#21457;&#65292;TPD&#27169;&#20223;&#20102;&#25945;&#24072;&#21644;&#23398;&#29983;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#37319;&#29992;&#22522;&#20110;&#21407;&#21017;&#30340;&#26041;&#27861;&#12290;&#25945;&#24072;LLM&#29983;&#25104;&#38382;&#39064;&#35299;&#20915;&#25351;&#20196;&#21644;&#32416;&#27491;&#21407;&#21017;&#65292;&#22522;&#20110;&#23398;&#29983;LLM&#30340;&#38169;&#35823;&#12290;&#36825;&#20123;&#21407;&#21017;&#25351;&#23548;&#25351;&#20196;&#30340;&#23436;&#21892;&#21644;&#20174;&#39564;&#35777;&#38598;&#20013;&#36873;&#25321;&#26377;&#25945;&#32946;&#24847;&#20041;&#30340;&#31034;&#20363;&#12290;&#36825;&#20351;&#24471;&#23398;&#29983;&#27169;&#22411;&#33021;&#22815;&#20174;&#25945;&#24072;&#30340;&#25351;&#23548;&#21644;&#33258;&#24049;&#30340;&#38169;&#35823;&#20013;&#23398;&#20064;&#12290;&#19968;&#26086;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently showcased remarkable reasoning abilities. However, larger models often surpass their smaller counterparts in reasoning tasks, posing the challenge of effectively transferring these capabilities from larger models. Existing approaches heavily rely on extensive fine-tuning data or continuous interactions with a superior teacher LLM during inference. We introduce a principle-based teacher-student framework called ``Teaching via Principle Discovery'' (TPD) to address these limitations. Inspired by human learning mechanisms, TPD mimics the interaction between a teacher and a student using a principle-based approach. The teacher LLM generates problem-solving instructions and corrective principles based on the student LLM's errors. These principles guide the refinement of instructions and the selection of instructive examples from a validation set. This enables the student model to learn from both the teacher's guidance and its own mistakes. Once the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20256;&#36798;&#32622;&#20449;&#24230;&#26041;&#38754;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;&#40664;&#35748;&#35299;&#37322;&#20250;&#23548;&#33268;&#29992;&#25143;&#36807;&#39640;&#20272;&#35745;&#27169;&#22411;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13835</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#27169;&#22411;&#21644;&#20154;&#31867;&#32622;&#20449;&#24230;&#20043;&#38388;&#30340;&#26657;&#20934;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
The Calibration Gap between Model and Human Confidence in Large Language Models. (arXiv:2401.13835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20256;&#36798;&#32622;&#20449;&#24230;&#26041;&#38754;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;&#40664;&#35748;&#35299;&#37322;&#20250;&#23548;&#33268;&#29992;&#25143;&#36807;&#39640;&#20272;&#35745;&#27169;&#22411;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#22815;&#33719;&#24471;&#20154;&#31867;&#30340;&#20449;&#20219;&#65292;&#23427;&#20204;&#38656;&#35201;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#23454;&#29616;&#33391;&#22909;&#30340;&#26657;&#20934;&#65292;&#21363;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#21644;&#20256;&#36798;&#23427;&#20204;&#30340;&#39044;&#27979;&#27491;&#30830;&#30340;&#21487;&#33021;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20851;&#27880;&#20102;LLM&#20869;&#37096;&#32622;&#20449;&#24230;&#35780;&#20272;&#30340;&#36136;&#37327;&#65292;&#20294;&#38382;&#39064;&#20173;&#28982;&#26159;LLM&#33021;&#22815;&#22914;&#20309;&#23558;&#36825;&#31181;&#20869;&#37096;&#27169;&#22411;&#32622;&#20449;&#24230;&#20256;&#36798;&#32473;&#20154;&#31867;&#29992;&#25143;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#31867;&#23545;LLM&#21709;&#24212;&#30340;&#22806;&#37096;&#32622;&#20449;&#24230;&#19982;&#27169;&#22411;&#20869;&#37096;&#32622;&#20449;&#24230;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#28041;&#21450;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#26816;&#26597;&#20102;&#20154;&#31867;&#29992;&#25143;&#35782;&#21035;LLM&#36755;&#20986;&#21487;&#20449;&#24230;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#20998;&#20026;&#20004;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#35780;&#20272;&#29992;&#25143;&#23545;&#30495;&#23454;LLM&#32622;&#20449;&#24230;&#30340;&#24863;&#30693;&#21644;&#65288;2&#65289;&#35843;&#26597;&#20010;&#24615;&#21270;&#35299;&#37322;&#23545;&#35813;&#24863;&#30693;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#30340;&#40664;&#35748;&#35299;&#37322;&#24448;&#24448;&#20250;&#23548;&#33268;&#29992;&#25143;&#36807;&#39640;&#20272;&#35745;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20462;&#25913;&#35299;&#37322;&#30340;&#26041;&#24335;&#21487;&#20197;&#20943;&#23567;&#36825;&#31181;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
For large language models (LLMs) to be trusted by humans they need to be well-calibrated in the sense that they can accurately assess and communicate how likely it is that their predictions are correct. Recent work has focused on the quality of internal LLM confidence assessments, but the question remains of how well LLMs can communicate this internal model confidence to human users. This paper explores the disparity between external human confidence in an LLM's responses and the internal confidence of the model. Through experiments involving multiple-choice questions, we systematically examine human users' ability to discern the reliability of LLM outputs. Our study focuses on two key areas: (1) assessing users' perception of true LLM confidence and (2) investigating the impact of tailored explanations on this perception. The research highlights that default explanations from LLMs often lead to user overestimation of both the model's confidence and its' accuracy. By modifying the expl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#20113;&#25925;&#38556;&#28335;&#28304;&#26041;&#27861;&#65292;&#20351;&#29992;GPT-4&#27169;&#22411;&#65292;&#26080;&#38656;&#36827;&#34892;&#26114;&#36149;&#30340;&#24494;&#35843;&#25805;&#20316;&#65292;&#21487;&#20197;&#25913;&#36827;&#25925;&#38556;&#26681;&#22240;&#20998;&#26512;&#36807;&#31243;&#65292;&#38477;&#20302;&#26381;&#21153;&#20572;&#26426;&#26102;&#38388;&#12289;&#23458;&#25143;&#24433;&#21709;&#21644;&#25163;&#21160;&#21171;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.13810</link><description>&lt;p&gt;
&#20351;&#29992;GPT-4&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#20113;&#25925;&#38556;&#28335;&#28304;(arXiv:2401.13810v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Automated Root Causing of Cloud Incidents using In-Context Learning with GPT-4. (arXiv:2401.13810v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13810
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#20113;&#25925;&#38556;&#28335;&#28304;&#26041;&#27861;&#65292;&#20351;&#29992;GPT-4&#27169;&#22411;&#65292;&#26080;&#38656;&#36827;&#34892;&#26114;&#36149;&#30340;&#24494;&#35843;&#25805;&#20316;&#65292;&#21487;&#20197;&#25913;&#36827;&#25925;&#38556;&#26681;&#22240;&#20998;&#26512;&#36807;&#31243;&#65292;&#38477;&#20302;&#26381;&#21153;&#20572;&#26426;&#26102;&#38388;&#12289;&#23458;&#25143;&#24433;&#21709;&#21644;&#25163;&#21160;&#21171;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#22240;&#20998;&#26512;&#65288;RCA&#65289;&#22312;&#20113;&#26381;&#21153;&#30340;&#25925;&#38556;&#35786;&#26029;&#36807;&#31243;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#38656;&#35201;&#20540;&#29677;&#24037;&#31243;&#24072;&#35782;&#21035;&#20027;&#35201;&#38382;&#39064;&#24182;&#23454;&#26045;&#32416;&#27491;&#25514;&#26045;&#65292;&#20197;&#38450;&#27490;&#23558;&#26469;&#30340;&#22797;&#21457;&#12290;&#25913;&#36827;&#25925;&#38556;&#26681;&#22240;&#20998;&#26512;&#36807;&#31243;&#23545;&#20110;&#20943;&#23569;&#26381;&#21153;&#20572;&#26426;&#26102;&#38388;&#12289;&#23458;&#25143;&#24433;&#21709;&#21644;&#25163;&#21160;&#21171;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;GPT-4&#65292;&#22312;&#22788;&#29702;&#20174;&#20195;&#30721;&#25776;&#20889;&#21040;&#25925;&#38556;&#31649;&#29702;&#31561;&#21508;&#31181;AIOps&#38382;&#39064;&#26041;&#38754;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;GPT-4&#27169;&#22411;&#24040;&#22823;&#30340;&#23610;&#23544;&#22312;&#23581;&#35797;&#22312;&#29992;&#25143;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#26102;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#38656;&#35201;&#22823;&#37327;&#30340;GPU&#36164;&#28304;&#65292;&#24182;&#19988;&#38543;&#30528;&#26032;&#25968;&#25454;&#30340;&#20986;&#29616;&#38656;&#35201;&#25345;&#32493;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#12290;&#20026;&#20102;&#35299;&#20915;&#24494;&#35843;LLM&#30340;&#39640;&#25104;&#26412;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33258;&#21160;&#25925;&#38556;&#28335;&#28304;&#26041;&#27861;&#65292;&#23427;&#28040;&#38500;&#20102;&#23545;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#23545;10&#19975;&#20010;&#25968;&#25454;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Root Cause Analysis (RCA) plays a pivotal role in the incident diagnosis process for cloud services, requiring on-call engineers to identify the primary issues and implement corrective actions to prevent future recurrences. Improving the incident RCA process is vital for minimizing service downtime, customer impact and manual toil. Recent advances in artificial intelligence have introduced state-of-the-art Large Language Models (LLMs) like GPT-4, which have proven effective in tackling various AIOps problems, ranging from code authoring to incident management. Nonetheless, the GPT-4 model's immense size presents challenges when trying to fine-tune it on user data because of the significant GPU resource demand and the necessity for continuous model fine-tuning with the emergence of new data. To address the high cost of fine-tuning LLM, we propose an in-context learning approach for automated root causing, which eliminates the need for fine-tuning. We conduct extensive study over 100,000
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.13802</link><description>&lt;p&gt;
&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26041;&#38754;&#30340;&#21151;&#25928;
&lt;/p&gt;
&lt;p&gt;
Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13802
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20363;&#22914;&#20195;&#30721;&#29983;&#25104;&#12290;LLMs&#20027;&#35201;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;/&#23569;&#26679;&#26412;&#33539;&#24335;&#20013;&#34987;&#29992;&#20110;&#25351;&#23548;&#27169;&#22411;&#23436;&#25104;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#65288;CCD&#65289;&#36825;&#19968;&#38750;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. %\textbf{Goal:} GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are `generative' tasks. However, there is limited research on the usage of LLMs for `non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. %\textbf{Method:} By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect \textcolor{black}{Type-4} code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We \textcolor{blac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#24773;&#24863;&#26816;&#27979;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#24314;&#27169;&#65292;&#36890;&#36807;&#22312;&#20449;&#24565;&#29366;&#24577;&#36319;&#36394;&#20013;&#24341;&#20837;&#24773;&#24863;&#26816;&#27979;&#23454;&#29616;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#31471;&#21040;&#31471;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#21644;&#20219;&#21153;&#32467;&#26524;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#31034;&#29992;&#25143;&#30340;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#22238;&#24212;&#30340;&#19978;&#19979;&#25991;&#26465;&#20214;&#65292;&#23545;&#20110;&#25552;&#39640;&#22238;&#24212;&#30340;&#20849;&#40483;&#31243;&#24230;&#20855;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2401.13789</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#24773;&#24863;&#26816;&#27979;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Unified Approach to Emotion Detection and Task-Oriented Dialogue Modeling. (arXiv:2401.13789v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#24773;&#24863;&#26816;&#27979;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#24314;&#27169;&#65292;&#36890;&#36807;&#22312;&#20449;&#24565;&#29366;&#24577;&#36319;&#36394;&#20013;&#24341;&#20837;&#24773;&#24863;&#26816;&#27979;&#23454;&#29616;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#31471;&#21040;&#31471;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#21644;&#20219;&#21153;&#32467;&#26524;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#31034;&#29992;&#25143;&#30340;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#22238;&#24212;&#30340;&#19978;&#19979;&#25991;&#26465;&#20214;&#65292;&#23545;&#20110;&#25552;&#39640;&#22238;&#24212;&#30340;&#20849;&#40483;&#31243;&#24230;&#20855;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#22522;&#20110;&#25991;&#26412;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#31995;&#32479;&#20013;&#65292;&#29992;&#25143;&#24773;&#24863;&#26816;&#27979;&#65288;ED&#65289;&#32463;&#24120;&#34987;&#24573;&#35270;&#65292;&#25110;&#32773;&#36890;&#24120;&#34987;&#35270;&#20026;&#19968;&#39033;&#29420;&#31435;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#26080;&#32541;&#22320;&#32479;&#19968;ED&#21644;TOD&#24314;&#27169;&#21487;&#20197;&#24102;&#26469;&#30456;&#20114;&#30340;&#22909;&#22788;&#65292;&#22240;&#27492;&#26159;&#19968;&#31181;&#20540;&#24471;&#32771;&#34385;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#23558;ED&#21253;&#21547;&#22312;&#20449;&#24565;&#29366;&#24577;&#36319;&#36394;&#20013;&#65292;&#24182;&#20381;&#36182;&#20110;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26469;&#25193;&#23637;SimpleToD&#36825;&#20010;&#31471;&#21040;&#31471;&#30340;TOD&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-2&#21644;Llama-2&#22312;EmoWOZ&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#24773;&#24863;&#36827;&#34892;&#27880;&#37322;&#30340;MultiWOZ&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;ED&#21644;&#20219;&#21153;&#32467;&#26524;&#30340;&#24615;&#33021;&#26222;&#36941;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#29992;&#25143;&#30340;&#24773;&#24863;&#20026;&#31995;&#32479;&#30340;&#22238;&#24212;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#19978;&#19979;&#25991;&#26465;&#20214;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#36827;&#19968;&#27493;&#25913;&#21892;&#22238;&#24212;&#30340;&#20849;&#40483;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In current text-based task-oriented dialogue (TOD) systems, user emotion detection (ED) is often overlooked or is typically treated as a separate and independent task, requiring additional training. In contrast, our work demonstrates that seamlessly unifying ED and TOD modeling brings about mutual benefits, and is therefore an alternative to be considered. Our method consists in augmenting SimpleToD, an end-to-end TOD system, by extending belief state tracking to include ED, relying on a single language model. We evaluate our approach using GPT-2 and Llama-2 on the EmoWOZ benchmark, a version of MultiWOZ annotated with emotions. Our results reveal a general increase in performance for ED and task results. Our findings also indicate that user emotions provide useful contextual conditioning for system responses, and can be leveraged to further refine responses in terms of empathy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13782</link><description>&lt;p&gt;
&#20174;&#25512;&#29305;&#21040;&#24341;&#29992;&#65306;&#25581;&#31034;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#23545;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21487;&#35265;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility. (arXiv:2401.13782v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#30340;&#21487;&#35265;&#24615;&#26041;&#38754;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#34987;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#20250;&#35758;&#19978;&#34987;&#25509;&#21463;&#30340;&#35770;&#25991;&#25968;&#37327;&#36798;&#21040;&#25968;&#21315;&#31687;&#65292;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#33719;&#21462;&#21644;&#38405;&#35835;&#30740;&#31350;&#35770;&#25991;&#21464;&#24471;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#32773;&#22312;&#22686;&#24378;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#21487;&#35265;&#24615;&#20013;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#20182;&#20204;&#20998;&#20139;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#25324;8000&#22810;&#31687;&#35770;&#25991;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;2018&#24180;12&#26376;&#33267;2023&#24180;10&#26376;&#30340;&#25512;&#29305;&#65292;&#20197;&#21450;&#22522;&#20110;&#20986;&#29256;&#24180;&#20221;&#12289;&#20250;&#35758;&#22320;&#28857;&#21644;&#25688;&#35201;&#20027;&#39064;&#36827;&#34892;1&#65306;1&#21305;&#37197;&#30340;&#23545;&#29031;&#32452;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36825;&#20123;&#24433;&#21709;&#32773;&#35748;&#21487;&#30340;&#35770;&#25991;&#24341;&#29992;&#27425;&#25968;&#26174;&#33879;&#22686;&#21152;&#65292;&#20013;&#20301;&#25968;&#24341;&#29992;&#27425;&#25968;&#27604;&#23545;&#29031;&#32452;&#39640;2-3&#20493;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#28145;&#20837;&#30740;&#31350;&#20102;&#34987;&#23637;&#31034;&#20316;&#32773;&#30340;&#22320;&#29702;&#12289;&#24615;&#21035;&#21644;&#26426;&#26500;&#22810;&#26679;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#31038;&#20132;&#23186;&#20307;&#22312;&#23398;&#26415;&#20132;&#27969;&#20013;&#30340;&#19981;&#26029;&#25193;&#22823;&#30340;&#24433;&#21709;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#24403;&#20170;&#25968;&#23383;&#21270;&#26102;&#20195;&#19981;&#26029;&#21457;&#23637;&#30340;&#29983;&#24577;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the number of accepted papers at AI and ML conferences reaches into the thousands, it has become unclear how researchers access and read research publications. In this paper, we investigate the role of social media influencers in enhancing the visibility of machine learning research, particularly the citation counts of papers they share. We have compiled a comprehensive dataset of over 8,000 papers, spanning tweets from December 2018 to October 2023, alongside 1:1 matched controls based on publication year, venue, and abstract topics. Our analysis reveals a significant increase in citations for papers endorsed by these influencers, with median citation counts 2-3 times higher than those of the control group. Additionally, the study delves into the geographic, gender, and institutional diversity of highlighted authors. These findings highlight the expanding influence of social media in scholarly communication and underscore the importance of an evolving ecosystem in today's digital a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TRML&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#38543;&#26426;&#32570;&#22833;&#27169;&#24577;&#30340;&#24773;&#20917;&#19979;&#30340;&#31283;&#20581;&#22810;&#27169;&#24577;&#23398;&#20064;&#12290;TRML&#21033;&#29992;&#29983;&#25104;&#30340;&#34394;&#25311;&#27169;&#24577;&#26367;&#25442;&#32570;&#22833;&#30340;&#27169;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#40784;&#35821;&#20041;&#31354;&#38388;&#26469;&#35299;&#20915;&#27169;&#24577;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13697</link><description>&lt;p&gt;
&#36808;&#21521;&#20351;&#29992;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#30340;&#31283;&#20581;&#22810;&#27169;&#24577;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Toward Robust Multimodal Learning using Multimodal Foundational Models. (arXiv:2401.13697v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13697
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TRML&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#38543;&#26426;&#32570;&#22833;&#27169;&#24577;&#30340;&#24773;&#20917;&#19979;&#30340;&#31283;&#20581;&#22810;&#27169;&#24577;&#23398;&#20064;&#12290;TRML&#21033;&#29992;&#29983;&#25104;&#30340;&#34394;&#25311;&#27169;&#24577;&#26367;&#25442;&#32570;&#22833;&#30340;&#27169;&#24577;&#65292;&#24182;&#19988;&#36890;&#36807;&#23545;&#40784;&#35821;&#20041;&#31354;&#38388;&#26469;&#35299;&#20915;&#27169;&#24577;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#39640;&#24230;&#20381;&#36182;&#20110;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#26159;&#23436;&#25972;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20551;&#35774;&#65292;&#28982;&#32780;&#36825;&#20010;&#20551;&#35774;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24448;&#24448;&#24456;&#38590;&#25104;&#31435;&#65306;&#22810;&#27169;&#24577;&#25968;&#25454;&#24448;&#24448;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26159;&#19981;&#23436;&#25972;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#23384;&#22312;&#38543;&#26426;&#32570;&#22833;&#27169;&#24577;&#30340;&#22330;&#26223;&#20013;&#65292;&#19968;&#31181;&#31283;&#20581;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#23558;&#20250;&#26356;&#21463;&#27426;&#36814;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;CLIP&#30340;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#21644;&#25991;&#26412;&#23545;&#30340;&#36328;&#27169;&#24577;&#35821;&#20041;&#23545;&#40784;&#65292;&#22312;&#20247;&#22810;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#20123;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#20063;&#26080;&#27861;&#30452;&#25509;&#35299;&#20915;&#28041;&#21450;&#27169;&#24577;&#32570;&#22833;&#30340;&#22330;&#26223;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21363;TRML&#65288;Toward Robust Multimodal Learning using Multimodal Foundational Models&#65289;&#12290;TRML&#21033;&#29992;&#29983;&#25104;&#30340;&#34394;&#25311;&#27169;&#24577;&#26367;&#25442;&#32570;&#22833;&#30340;&#27169;&#24577;&#65292;&#24182;&#19988;&#23545;&#29983;&#25104;&#30340;&#27169;&#24577;&#21644;&#32570;&#22833;&#30340;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32570;&#22833;&#27169;&#24577;&#29983;&#25104;&#27169;&#22359;&#65292;&#21487;&#20197;&#29983;&#25104;&#32570;&#22833;&#30340;&#27169;&#24577;&#65292;&#28982;&#21518;&#36890;&#36807;&#29305;&#24449;&#23545;&#40784;&#27169;&#22359;&#26469;&#23398;&#20064;&#27169;&#24577;&#20043;&#38388;&#30340;&#23545;&#40784;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing multimodal sentiment analysis tasks are highly rely on the assumption that the training and test sets are complete multimodal data, while this assumption can be difficult to hold: the multimodal data are often incomplete in real-world scenarios. Therefore, a robust multimodal model in scenarios with randomly missing modalities is highly preferred. Recently, CLIP-based multimodal foundational models have demonstrated impressive performance on numerous multimodal tasks by learning the aligned cross-modal semantics of image and text pairs, but the multimodal foundational models are also unable to directly address scenarios involving modality absence. To alleviate this issue, we propose a simple and effective framework, namely TRML, Toward Robust Multimodal Learning using Multimodal Foundational Models. TRML employs generated virtual modalities to replace missing modalities, and aligns the semantic spaces between the generated and missing modalities. Concretely, we design a missin
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLMs&#65289;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35757;&#32451;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#36755;&#20837;&#21644;&#36755;&#20986;&#25903;&#25345;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#25253;&#21578;&#65292;&#20171;&#32461;&#20102;MM-LLMs&#30340;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#26696;&#65292;&#25972;&#29702;&#20102;&#29616;&#26377;&#30340;MM-LLMs&#21450;&#20854;&#24615;&#33021;&#65292;&#24635;&#32467;&#20102;&#20851;&#38190;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.13601</link><description>&lt;p&gt;
MM-LLMs: &#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
MM-LLMs: Recent Advances in MultiModal Large Language Models. (arXiv:2401.13601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13601
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLMs&#65289;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35757;&#32451;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#36755;&#20837;&#21644;&#36755;&#20986;&#25903;&#25345;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#25253;&#21578;&#65292;&#20171;&#32461;&#20102;MM-LLMs&#30340;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#26696;&#65292;&#25972;&#29702;&#20102;&#29616;&#26377;&#30340;MM-LLMs&#21450;&#20854;&#24615;&#33021;&#65292;&#24635;&#32467;&#20102;&#20851;&#38190;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#19968;&#24180;&#20013;&#65292;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#30340;LLMs&#23545;&#22810;&#27169;&#36755;&#20837;&#25110;&#36755;&#20986;&#30340;&#25903;&#25345;&#12290;&#36825;&#20123;&#32467;&#26524;&#27169;&#22411;&#19981;&#20165;&#20445;&#30041;&#20102;LLMs&#22266;&#26377;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#65292;&#36824;&#36171;&#20104;&#20102;&#21508;&#31181;&#22810;&#27169;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;MM-LLMs&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#27969;&#31243;&#30340;&#19968;&#33324;&#35774;&#35745;&#26041;&#26696;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;26&#31181;&#29616;&#26377;&#30340;MM-LLMs&#65292;&#27599;&#31181;&#37117;&#20197;&#20854;&#20855;&#20307;&#30340;&#20844;&#24335;&#20026;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;MM-LLMs&#22312;&#20027;&#27969;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#25552;&#39640;MM-LLMs&#25928;&#21147;&#30340;&#20851;&#38190;&#35757;&#32451;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;MM-LLMs&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#36824;&#20026;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#25552;&#20379;&#20102;&#23454;&#26102;&#36861;&#36394;&#32593;&#31449;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#33021;&#22815;&#20419;&#36827;&#23545;MM-LLMs&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Specifically, we first outline general design formulations for model architecture and training pipeline. Subsequently, we provide brief introductions of $26$ existing MM-LLMs, each characterized by its specific formulations. Additionally, we review the performance of MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this surv
&lt;/p&gt;</description></item><item><title>SpeechGPT-Gen&#26159;&#19968;&#20010;8&#20159;&#21442;&#25968;&#30340;&#35821;&#38899;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;Chain-of-Information Generation&#26041;&#27861;&#26469;&#35299;&#32806;&#35821;&#20041;&#21644;&#24863;&#30693;&#20449;&#24687;&#65292;&#22312;&#35821;&#38899;&#29983;&#25104;&#26041;&#38754;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.13527</link><description>&lt;p&gt;
SpeechGPT-Gen: &#32553;&#25918;&#20449;&#24687;&#38142;&#35821;&#38899;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation. (arXiv:2401.13527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13527
&lt;/p&gt;
&lt;p&gt;
SpeechGPT-Gen&#26159;&#19968;&#20010;8&#20159;&#21442;&#25968;&#30340;&#35821;&#38899;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;Chain-of-Information Generation&#26041;&#27861;&#26469;&#35299;&#32806;&#35821;&#20041;&#21644;&#24863;&#30693;&#20449;&#24687;&#65292;&#22312;&#35821;&#38899;&#29983;&#25104;&#26041;&#38754;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20973;&#20511;&#26377;&#25928;&#30340;&#35821;&#38899;&#24314;&#27169;&#65292;&#24403;&#21069;&#30340;&#35821;&#38899;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLLMs&#65289;&#22312;&#19978;&#19979;&#25991;&#35821;&#38899;&#29983;&#25104;&#21644;&#23545;&#26410;&#35265;&#36807;&#30340;&#35828;&#35805;&#20154;&#30340;&#39640;&#25928;&#27867;&#21270;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20449;&#24687;&#24314;&#27169;&#36807;&#31243;&#21463;&#21040;&#19968;&#23450;&#20887;&#20313;&#30340;&#38480;&#21046;&#65292;&#23548;&#33268;&#35821;&#38899;&#29983;&#25104;&#25928;&#29575;&#20302;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20449;&#24687;&#38142;&#29983;&#25104;&#65288;CoIG&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#32806;&#22823;&#35268;&#27169;&#35821;&#38899;&#29983;&#25104;&#20013;&#30340;&#35821;&#20041;&#21644;&#24863;&#30693;&#20449;&#24687;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SpeechGPT-Gen&#65292;&#19968;&#20010;8&#20159;&#21442;&#25968;&#30340;SLLM&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#35821;&#20041;&#21644;&#24863;&#30693;&#20449;&#24687;&#24314;&#27169;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#35821;&#20041;&#20449;&#24687;&#24314;&#27169;&#65292;&#20197;&#21450;&#19968;&#20010;&#20351;&#29992;&#27969;&#21305;&#37197;&#36827;&#34892;&#24863;&#30693;&#20449;&#24687;&#24314;&#27169;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23558;&#35821;&#20041;&#20449;&#24687;&#27880;&#20837;&#20808;&#39564;&#20998;&#24067;&#20197;&#22686;&#24378;&#27969;&#21305;&#37197;&#25928;&#29575;&#30340;&#26032;&#26041;&#27861;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#8230;
&lt;/p&gt;
&lt;p&gt;
Benefiting from effective speech modeling, current Speech Large Language Models (SLLMs) have demonstrated exceptional capabilities in in-context speech generation and efficient generalization to unseen speakers. However, the prevailing information modeling process is encumbered by certain redundancies, leading to inefficiencies in speech generation. We propose Chain-of-Information Generation (CoIG), a method for decoupling semantic and perceptual information in large-scale speech generation. Building on this, we develop SpeechGPT-Gen, an 8-billion-parameter SLLM efficient in semantic and perceptual information modeling. It comprises an autoregressive model based on LLM for semantic information modeling and a non-autoregressive model employing flow matching for perceptual information modeling. Additionally, we introduce the novel approach of infusing semantic information into the prior distribution to enhance the efficiency of flow matching. Extensive experimental results demonstrate th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#26679;&#26412;&#27169;&#22359;&#32452;&#21512;&#26694;&#26550;&#65292;&#32479;&#19968;&#20102;&#36873;&#25321;&#12289;&#21152;&#26435;&#21644;&#32452;&#21512;&#21442;&#25968;&#27169;&#22359;&#30340;&#21508;&#31181;&#21464;&#21270;&#12290;&#20197;&#39046;&#22495;&#30693;&#35782;&#21644;&#36866;&#37197;&#22120;&#23618;&#20026;&#22330;&#26223;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#32479;&#19968;&#27010;&#24565;&#65292;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#30693;&#35782;&#32452;&#21512;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.12756</link><description>&lt;p&gt;
What the Weight?! &#38646;&#26679;&#26412;&#30693;&#35782;&#32452;&#21512;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition. (arXiv:2401.12756v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#26679;&#26412;&#27169;&#22359;&#32452;&#21512;&#26694;&#26550;&#65292;&#32479;&#19968;&#20102;&#36873;&#25321;&#12289;&#21152;&#26435;&#21644;&#32452;&#21512;&#21442;&#25968;&#27169;&#22359;&#30340;&#21508;&#31181;&#21464;&#21270;&#12290;&#20197;&#39046;&#22495;&#30693;&#35782;&#21644;&#36866;&#37197;&#22120;&#23618;&#20026;&#22330;&#26223;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#30340;&#32479;&#19968;&#27010;&#24565;&#65292;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#30693;&#35782;&#32452;&#21512;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#20013;&#25152;&#23553;&#35013;&#30340;&#30693;&#35782;&#26159;&#30830;&#23450;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#26368;&#32456;&#24615;&#33021;&#30340;&#26680;&#24515;&#22240;&#32032;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#35768;&#22810;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#23384;&#20648;&#21644;&#35843;&#25972;&#19981;&#21516;&#31867;&#22411;&#30693;&#35782;&#30340;&#26377;&#25928;&#26041;&#27861;&#19978;&#65292;&#20363;&#22914;&#22312;&#19987;&#29992;&#30340;&#27169;&#22359;&#21270;&#32467;&#26500;&#20013;&#65292;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#23398;&#20064;&#39069;&#22806;&#30340;&#21442;&#25968;&#26469;&#26377;&#25928;&#22320;&#32452;&#21512;&#36825;&#20123;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#23384;&#22312;&#35768;&#22810;&#21487;&#33021;&#30340;&#36873;&#39033;&#65292;&#23545;&#20110;&#36825;&#20123;&#32452;&#21512;&#20013;&#28041;&#21450;&#30340;&#26426;&#21046;&#32570;&#20047;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#22240;&#27492;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#24212;&#35813;&#20351;&#29992;&#21738;&#20123;&#31574;&#30053;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38646;&#26679;&#26412;&#27169;&#22359;&#32452;&#21512;&#26694;&#26550;&#65292;&#23427;&#28085;&#30422;&#20102;&#29616;&#26377;&#30340;&#19968;&#20123;&#36873;&#25321;&#12289;&#21152;&#26435;&#21644;&#32452;&#21512;&#21442;&#25968;&#27169;&#22359;&#30340;&#21464;&#21270;&#65292;&#32479;&#19968;&#20102;&#36825;&#20123;&#27010;&#24565;&#12290;&#22312;&#32858;&#28966;&#39046;&#22495;&#30693;&#35782;&#21644;&#36866;&#37197;&#22120;&#23618;&#30340;&#24773;&#26223;&#19979;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#32479;&#19968;&#27010;&#24565;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#39318;&#27425;&#20840;&#38754;&#30340;&#21508;&#31181;&#38646;&#26679;&#26412;&#30693;&#35782;&#32452;&#21512;&#30340;&#22522;&#20934;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The knowledge encapsulated in a model is the core factor determining its final performance on downstream tasks. Much research in NLP has focused on efficient methods for storing and adapting different types of knowledge, e.g., in dedicated modularized structures, and on how to effectively combine these, e.g., by learning additional parameters. However, given the many possible options, a thorough understanding of the mechanisms involved in these compositions is missing, and hence it remains unclear which strategies to utilize. To address this research gap, we propose a novel framework for zero-shot module composition, which encompasses existing and some novel variations for selecting, weighting, and combining parameter modules under a single unified notion. Focusing on the scenario of domain knowledge and adapter layers, our framework provides a systematic unification of concepts, allowing us to conduct the first comprehensive benchmarking study of various zero-shot knowledge compositio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;AutoEval&#26694;&#26550;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12689</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Energy-based Automated Model Evaluation. (arXiv:2401.12689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12689
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#30340;&#33258;&#21160;&#21270;&#27169;&#22411;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#24314;&#31435;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#21644;&#26377;&#25928;&#22320;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;AutoEval&#26694;&#26550;&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#20381;&#36182;&#20110;&#26631;&#35760;&#30340;&#12289;&#20551;&#35774;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24448;&#24448;&#24182;&#19981;&#24120;&#35265;&#12290;&#33258;&#21160;&#27169;&#22411;&#35780;&#20272;&#65288;AutoEval&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#20256;&#32479;&#24037;&#20316;&#27969;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#25104;&#19968;&#20010;&#25509;&#36817;&#39044;&#27979;&#24615;&#33021;&#30340;&#27979;&#35797;&#31649;&#32447;&#65292;&#32780;&#26080;&#38656;&#30495;&#23454;&#26631;&#31614;&#30340;&#23384;&#22312;&#12290;&#23613;&#31649;AutoEval&#26694;&#26550;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#25104;&#21151;&#65292;&#20294;&#20173;&#23384;&#22312;&#36807;&#24230;&#33258;&#20449;&#12289;&#23384;&#20648;&#21644;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26041;&#24335;&#8212;&#8212;&#20803;&#20998;&#24067;&#33021;&#37327;&#65288;MDE&#65289;&#65292;&#23427;&#21487;&#20197;&#20351;AutoEval&#26694;&#26550;&#26356;&#21152;&#39640;&#25928;&#21644;&#26377;&#25928;&#12290;MDE&#30340;&#26680;&#24515;&#26159;&#24314;&#31435;&#19968;&#20010;&#20851;&#20110;&#20010;&#20307;&#26679;&#26412;&#30456;&#20851;&#20449;&#24687;&#65288;&#33021;&#37327;&#65289;&#30340;&#20803;&#20998;&#24067;&#32479;&#35745;&#37327;&#65292;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#33021;&#37327;&#30340;&#23398;&#20064;&#25552;&#20379;&#26356;&#24179;&#28369;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;MDE&#19982;&#20998;&#31867;&#25439;&#22833;&#30456;&#36830;&#25509;&#65292;&#36827;&#19968;&#27493;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#25454;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional evaluation protocols on machine learning models rely heavily on a labeled, i.i.d-assumed testing dataset, which is not often present in real world applications. The Automated Model Evaluation (AutoEval) shows an alternative to this traditional workflow, by forming a proximal prediction pipeline of the testing performance without the presence of ground-truth labels. Despite its recent successes, the AutoEval frameworks still suffer from an overconfidence issue, substantial storage and computational cost. In that regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that allows the AutoEval framework to be both more efficient and effective. The core of the MDE is to establish a meta-distribution statistic, on the information (energy) associated with individual samples, then offer a smoother representation enabled by energy-based learning. We further provide our theoretical insights by connecting the MDE with the classification loss. We provide extensive
&lt;/p&gt;</description></item><item><title>BiTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21521;&#35843;&#25972;&#23454;&#29616;&#20102;&#26080;&#25439;&#21152;&#36895;&#12290;&#23427;&#37319;&#29992;&#31616;&#21270;&#30340;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#33609;&#31295;&#39564;&#35777;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#30340;&#35299;&#30721;&#21516;&#26102;&#36827;&#34892;&#20505;&#36873;&#29983;&#25104;&#21644;&#39564;&#35777;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#36741;&#21161;&#27169;&#22411;&#25110;&#26174;&#33879;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2401.12522</link><description>&lt;p&gt;
BiTA: &#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#25439;&#21152;&#36895;&#30340;&#21452;&#21521;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models. (arXiv:2401.12522v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12522
&lt;/p&gt;
&lt;p&gt;
BiTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#21521;&#35843;&#25972;&#23454;&#29616;&#20102;&#26080;&#25439;&#21152;&#36895;&#12290;&#23427;&#37319;&#29992;&#31616;&#21270;&#30340;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#33609;&#31295;&#39564;&#35777;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#30340;&#35299;&#30721;&#21516;&#26102;&#36827;&#34892;&#20505;&#36873;&#29983;&#25104;&#21644;&#39564;&#35777;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#36741;&#21161;&#27169;&#22411;&#25110;&#26174;&#33879;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#23548;&#33268;&#39640;&#20869;&#23384;&#24102;&#23485;&#38656;&#27714;&#21644;&#24310;&#36831;&#24310;&#38271;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#8212;&#8212;&#21452;&#21521;&#35843;&#25972;&#20197;&#23454;&#29616;&#26080;&#25439;&#21152;&#36895;&#65288;BiTA&#65289;&#65292;&#36890;&#36807;&#31616;&#21270;&#30340;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#21644;&#33609;&#31295;&#39564;&#35777;&#26469;&#21152;&#36895;LLMs&#12290;&#21463;&#21551;&#21457;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#35774;&#35745;&#65292;&#31216;&#20026;&#21452;&#21521;&#35843;&#25972;&#65292;&#26469;&#22686;&#24378;LLMs&#22312;&#21322;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#37319;&#29992;&#39640;&#25928;&#30340;&#22522;&#20110;&#26641;&#30340;&#35299;&#30721;&#65292;&#27169;&#22411;&#21487;&#20197;&#21516;&#26102;&#36827;&#34892;&#33609;&#31295;&#20505;&#36873;&#29983;&#25104;&#21644;&#39564;&#35777;&#65292;&#30830;&#20445;&#36755;&#20986;&#32467;&#26524;&#19982;&#23427;&#20204;&#30340;&#33258;&#22238;&#24402;&#23545;&#24212;&#29289;&#22312;&#36138;&#23146;&#25277;&#26679;&#19979;&#23436;&#20840;&#30456;&#21516;&#12290;BiTA&#20316;&#20026;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25554;&#20214;&#27169;&#22359;&#65292;&#21487;&#20197;&#26080;&#32541;&#22686;&#24378;&#29616;&#26377;LLMs&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#36741;&#21161;&#27169;&#22411;&#25110;&#25215;&#25285;&#26174;&#33879;&#30340;&#39069;&#22806;&#20869;&#23384;&#24320;&#38144;&#12290;&#36890;&#36807;&#24212;&#29992;&#25552;&#20986;&#30340;BiTA&#65292;LLaMA-2-70B-Chat&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) commonly employ autoregressive generation during inference, leading to high memory bandwidth demand and consequently extended latency. To mitigate this inefficiency, we present Bi-directional Tuning for lossless Acceleration (BiTA), an innovative method expediting LLMs via streamlined semi-autoregressive generation and draft verification. Inspired by the concept of prompt tuning, we enhance LLMs with a parameter-efficient design called bi-directional tuning for the capability in semi-autoregressive generation. Employing efficient tree-based decoding, the models perform draft candidate generation and verification in parallel, ensuring outputs identical to their autoregressive counterparts under greedy sampling. BiTA serves as a lightweight plug-in module, seamlessly boosting the inference efficiency of existing LLMs without requiring additional assistance models or incurring significant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat achieve
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#32441;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.12255</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#25351;&#32441;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Instructional Fingerprinting of Large Language Models. (arXiv:2401.12255v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12255
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#32441;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#24182;&#30830;&#20445;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#65292;&#24182;&#19988;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24040;&#22823;&#25104;&#26412;&#20351;&#24471;&#23545;&#27169;&#22411;&#36827;&#34892;&#25351;&#32441;&#35782;&#21035;&#20197;&#20445;&#25252;&#30693;&#35782;&#20135;&#26435;&#25104;&#20026;&#24517;&#35201;&#65292;&#36890;&#36807;&#25152;&#26377;&#26435;&#35748;&#35777;&#24182;&#30830;&#20445;&#19979;&#28216;&#29992;&#25143;&#21644;&#24320;&#21457;&#32773;&#36981;&#23432;&#35768;&#21487;&#26465;&#27454;&#65288;&#22914;&#38480;&#21046;&#21830;&#19994;&#20351;&#29992;&#65289;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM&#25351;&#32441;&#35782;&#21035;&#30340;&#35797;&#28857;&#30740;&#31350;&#65292;&#20316;&#20026;&#19968;&#31181;&#38750;&#24120;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#24418;&#24335;&#12290;&#27169;&#22411;&#21457;&#24067;&#32773;&#25351;&#23450;&#19968;&#20010;&#26426;&#23494;&#30340;&#31169;&#38053;&#65292;&#24182;&#23558;&#20854;&#26893;&#20837;&#20026;&#19968;&#20010;&#25351;&#20196;&#21518;&#38376;&#65292;&#24403;&#23494;&#38053;&#23384;&#22312;&#26102;&#65292;&#23548;&#33268;LLM&#29983;&#25104;&#29305;&#23450;&#30340;&#25991;&#26412;&#12290;&#23545;11&#20010;&#24120;&#29992;LLMs&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#36731;&#37327;&#32423;&#19988;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#27491;&#24120;&#34892;&#20026;&#12290;&#23427;&#36824;&#21487;&#20197;&#38450;&#27490;&#21457;&#24067;&#32773;&#36807;&#24230;&#23459;&#31216;&#65292;&#23545;&#25351;&#32441;&#29468;&#27979;&#21644;&#21442;&#25968;&#39640;&#25928;&#35757;&#32451;&#20445;&#25345;&#40065;&#26834;&#24615;&#65292;&#24182;&#25903;&#25345;&#31867;&#20284;&#20110;MIT&#35768;&#21487;&#35777;&#30340;&#22810;&#38454;&#27573;&#25351;&#32441;&#35782;&#21035;&#12290;&#20195;&#30721;&#21487;&#22312;https://cnut1648.github.io/Model-Fingerprint/&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exorbitant cost of training Large language models (LLMs) from scratch makes it essential to fingerprint the models to protect intellectual property via ownership authentication and to ensure downstream users and developers comply with their license terms (e.g. restricting commercial use). In this study, we present a pilot study on LLM fingerprinting as a form of very lightweight instruction tuning. Model publisher specifies a confidential private key and implants it as an instruction backdoor that causes the LLM to generate specific text when the key is present. Results on 11 popularly-used LLMs showed that this approach is lightweight and does not affect the normal behavior of the model. It also prevents publisher overclaim, maintains robustness against fingerprint guessing and parameter-efficient training, and supports multi-stage fingerprinting akin to MIT License. Code is available in https://cnut1648.github.io/Model-Fingerprint/.
&lt;/p&gt;</description></item><item><title>Mementos&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#24207;&#21015;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;MLLM&#22312;&#20934;&#30830;&#25551;&#36848;&#22270;&#20687;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23481;&#26131;&#23548;&#33268;&#29289;&#20307;&#21450;&#20854;&#34892;&#20026;&#30340;&#38169;&#35823;&#25551;&#36848;&#25110;&#38169;&#35273;&#12290;</title><link>http://arxiv.org/abs/2401.10529</link><description>&lt;p&gt;
Mementos: &#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#24207;&#21015;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences. (arXiv:2401.10529v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10529
&lt;/p&gt;
&lt;p&gt;
Mementos&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#24207;&#21015;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;MLLM&#22312;&#20934;&#30830;&#25551;&#36848;&#22270;&#20687;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#23481;&#26131;&#23548;&#33268;&#29289;&#20307;&#21450;&#20854;&#34892;&#20026;&#30340;&#38169;&#35823;&#25551;&#36848;&#25110;&#38169;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#22788;&#29702;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#39640;&#36229;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;MLLM&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#21333;&#20010;&#22270;&#20687;&#30340;&#38745;&#24577;&#20449;&#24687;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#29616;&#20195;MLLM&#22312;&#20174;&#22270;&#20687;&#24207;&#21015;&#20013;&#36827;&#34892;&#25512;&#26029;&#30340;&#33021;&#21147;&#65292;&#22312;&#29702;&#35299;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#21364;&#34987;&#36739;&#23569;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;Mementos&#65292;&#29992;&#20110;&#35780;&#20272;MLLM&#30340;&#24207;&#21015;&#22270;&#20687;&#25512;&#29702;&#33021;&#21147;&#12290;Mementos&#21253;&#25324;4761&#20010;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#22810;&#26679;&#30340;&#22270;&#20687;&#24207;&#21015;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;GPT-4&#36741;&#21161;&#26041;&#27861;&#26469;&#35780;&#20272;MLLM&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;Mementos&#20013;&#21253;&#25324;GPT-4V&#21644;Gemini&#22312;&#20869;&#30340;&#20061;&#20010;&#26368;&#26032;MLLM&#36827;&#34892;&#20180;&#32454;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#22312;&#20934;&#30830;&#25551;&#36848;&#25152;&#32473;&#22270;&#20687;&#24207;&#21015;&#30340;&#21160;&#24577;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24448;&#24448;&#23548;&#33268;&#23545;&#35937;&#21450;&#20854;&#23545;&#24212;&#34892;&#20026;&#30340;&#38169;&#35823;&#25551;&#36848;&#25110;&#38169;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs' sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance. Through a careful evaluation of nine recent MLLMs on Mementos, including GPT-4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors. Our quantitati
&lt;/p&gt;</description></item><item><title>&#22312;&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20013;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#32534;&#31243;&#20013;&#25991;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#20013;&#25991;&#24187;&#35273;&#25935;&#24863;&#30340;&#20219;&#21153;&#20013;&#12290;&#27492;&#30740;&#31350;&#20026;&#35752;&#35770;&#8220;&#20013;&#25991;&#25151;&#38388;&#8221;&#24605;&#24819;&#23454;&#39564;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2401.10286</link><description>&lt;p&gt;
&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20013;&#30340;&#20348;&#20348;&#32773;&#65306;&#33521;&#25991;&#20195;&#30721;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Top in Chinese Data Processing: English Code Models. (arXiv:2401.10286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10286
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20013;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#32534;&#31243;&#20013;&#25991;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#23545;&#20013;&#25991;&#24187;&#35273;&#25935;&#24863;&#30340;&#20219;&#21153;&#20013;&#12290;&#27492;&#30740;&#31350;&#20026;&#35752;&#35770;&#8220;&#20013;&#25991;&#25151;&#38388;&#8221;&#24605;&#24819;&#23454;&#39564;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#20013;&#65292;&#20219;&#21153;&#19982;&#35757;&#32451;&#35821;&#26009;&#20043;&#38388;&#30340;&#23545;&#40784;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#20849;&#35782;&#65292;&#20294;&#25105;&#20204;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#21644;&#25105;&#20204;&#35774;&#35745;&#30340;&#35780;&#20272;&#25351;&#26631;&#34920;&#26126;&#65292;&#22522;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#38750;&#32534;&#31243;&#20013;&#25991;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#19982;&#20219;&#21153;&#32039;&#23494;&#21305;&#37197;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#22312;&#23545;&#20013;&#25991;&#24187;&#35273;&#25935;&#24863;&#31243;&#24230;&#36739;&#39640;&#30340;&#20219;&#21153;&#20013;&#65292;&#23637;&#31034;&#36739;&#23569;&#20013;&#25991;&#35821;&#35328;&#29305;&#24449;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#29992;&#20195;&#30721;&#27169;&#22411;&#26367;&#25442;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#20013;&#25991;&#25968;&#25454;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#22914;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;(RAG)&#20934;&#22791;&#25968;&#25454;&#65292;&#24456;&#23481;&#26131;&#24471;&#21040;&#22797;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#35752;&#35770;&#8220;&#20013;&#25991;&#25151;&#38388;&#8221;&#24605;&#24819;&#23454;&#39564;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the alignment between tasks and training corpora is a fundamental consensus in the application of language models, our series of experiments and the metrics we designed reveal that code-based Large Language Models (LLMs) significantly outperform models trained on data that is closely matched to the tasks in non-coding Chinese tasks. Moreover, in tasks high sensitivity to Chinese hallucinations, models exhibiting fewer linguistic features of the Chinese language achieve better performance. Our experimental results can be easily replicated in Chinese data processing tasks, such as preparing data for Retrieval-Augmented Generation (RAG), by simply replacing the base model with a code-based model. Additionally, our research offers a distinct perspective for discussion on the philosophical "Chinese Room" thought experiment.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#38598;&#25104;&#21040;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#35299;&#20915;&#20854;&#20135;&#29983;&#19981;&#21487;&#21462;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#28165;&#27905;&#39046;&#22495;&#20013;&#26377;&#25928;&#20943;&#23569;&#26377;&#23475;&#20869;&#23481;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.08491</link><description>&lt;p&gt;
&#23545;&#27604;&#22256;&#24785;&#24230;&#22312;&#21463;&#25511;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#65306;&#28165;&#27905;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Contrastive Perplexity for Controlled Generation: An Application in Detoxifying Large Language Models. (arXiv:2401.08491v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08491
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#38598;&#25104;&#21040;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#35299;&#20915;&#20854;&#20135;&#29983;&#19981;&#21487;&#21462;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#28165;&#27905;&#39046;&#22495;&#20013;&#26377;&#25928;&#20943;&#23569;&#26377;&#23475;&#20869;&#23481;&#29983;&#25104;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#19981;&#21487;&#21462;&#21644;&#20107;&#23454;&#19981;&#27491;&#30830;&#30340;&#20869;&#23481;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#19968;&#20010;&#25361;&#25112;&#21644;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#38598;&#25104;&#65292;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#38544;&#24335;&#30693;&#35782;&#32534;&#36753;&#21644;&#21463;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#36890;&#36807;&#23545;&#27604;&#26041;&#24335;&#20248;&#21270;&#35757;&#32451;&#30446;&#26631;&#65292;&#21363;&#23545;&#40784;&#25991;&#26412;&#30340;&#22256;&#24785;&#24230;&#12290;&#20026;&#20102;&#20197;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#21033;&#29992;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28165;&#27905;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#23545;&#20110;&#24120;&#35782;&#25512;&#29702;&#21644;&#38405;&#35835;&#29702;&#35299;&#31561;&#19979;&#28216;&#20219;&#21153;&#30340;&#23454;&#29992;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27010;&#24565;&#19978;&#31616;&#21333;&#20294;&#32463;&#39564;&#19978;&#24378;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of undesirable and factually incorrect content of large language models poses a significant challenge and remains largely an unsolved issue. This paper studies the integration of a contrastive learning objective for fine-tuning LLMs for implicit knowledge editing and controlled text generation. Optimizing the training objective entails aligning text perplexities in a contrastive fashion. To facilitate training the model in a self-supervised fashion, we leverage an off-the-shelf LLM for training data generation. We showcase applicability in the domain of detoxification. Herein, the proposed approach leads to a significant decrease in the generation of toxic content while preserving general utility for downstream tasks such as commonsense reasoning and reading comprehension. The proposed approach is conceptually simple but empirically powerful.
&lt;/p&gt;</description></item><item><title>TrustLLM&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20449;&#24615;&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#21487;&#20449;&#24615;&#21407;&#21017;&#30340;&#25552;&#20986;&#12289;&#24314;&#31435;&#22522;&#20934;&#30340;&#26041;&#27861;&#12289;&#35780;&#20272;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24615;&#65292;&#20197;&#21450;&#23545;&#26410;&#26469;&#25361;&#25112;&#30340;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.05561</link><description>&lt;p&gt;
TrustLLM: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21487;&#20449;&#24615;
&lt;/p&gt;
&lt;p&gt;
TrustLLM: Trustworthiness in Large Language Models. (arXiv:2401.05561v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05561
&lt;/p&gt;
&lt;p&gt;
TrustLLM&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#20449;&#24615;&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#21487;&#20449;&#24615;&#21407;&#21017;&#30340;&#25552;&#20986;&#12289;&#24314;&#31435;&#22522;&#20934;&#30340;&#26041;&#27861;&#12289;&#35780;&#20272;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24615;&#65292;&#20197;&#21450;&#23545;&#26410;&#26469;&#25361;&#25112;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#22240;&#20854;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLMs&#22312;&#21487;&#20449;&#24615;&#26041;&#38754;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#30830;&#20445;LLMs&#30340;&#21487;&#20449;&#24615;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#35805;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TrustLLM&#65292;&#23427;&#26159;&#23545;LLMs&#20013;&#21487;&#20449;&#24615;&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#21253;&#25324;&#19981;&#21516;&#32500;&#24230;&#30340;&#21487;&#20449;&#24615;&#21407;&#21017;&#12289;&#24314;&#31435;&#22522;&#20934;&#12289;&#35780;&#20272;&#21644;&#20998;&#26512;&#20027;&#27969;LLMs&#30340;&#21487;&#20449;&#24615;&#65292;&#20197;&#21450;&#23545;&#24320;&#25918;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#28085;&#30422;&#20843;&#20010;&#19981;&#21516;&#32500;&#24230;&#30340;&#21487;&#20449;LLMs&#21407;&#21017;&#12290;&#22522;&#20110;&#36825;&#20123;&#21407;&#21017;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24314;&#31435;&#20102;&#19968;&#20010;&#36328;&#20845;&#20010;&#32500;&#24230;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;&#30495;&#23454;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#38544;&#31169;&#24615;&#21644;&#26426;&#22120;&#20262;&#29702;&#23398;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;TrustLLM&#20013;&#23637;&#31034;&#20102;&#19968;&#20010;&#35780;&#20272;16&#20010;&#20027;&#27969;LLMs&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;30&#22810;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our find
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#12304;&#30456;&#23545;&#21019;&#36896;&#21147;&#12305;&#65292;&#36890;&#36807;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#21644;&#30452;&#25509;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2401.01623</link><description>&lt;p&gt;
AI&#26159;&#21542;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#20855;&#22791;&#21019;&#36896;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI Be as Creative as Humans?. (arXiv:2401.01623v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#12304;&#30456;&#23545;&#21019;&#36896;&#21147;&#12305;&#65292;&#36890;&#36807;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#21644;&#30452;&#25509;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#36896;&#21147;&#26159;&#31038;&#20250;&#36827;&#27493;&#21644;&#21019;&#26032;&#30340;&#22522;&#30707;&#65292;&#20294;&#20854;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#20027;&#35266;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#20808;&#36827;&#30340;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#33021;&#22815;&#23436;&#25104;&#26366;&#32463;&#21482;&#23646;&#20110;&#20154;&#31867;&#21019;&#36896;&#21147;&#30340;&#20219;&#21153;&#65292;&#25506;&#32034;AI&#30340;&#21019;&#36896;&#28508;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#20854;&#36127;&#36131;&#20219;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#8220;&#30456;&#23545;&#21019;&#36896;&#21147;&#8221;&#30340;&#26032;&#27010;&#24565;&#26469;&#35299;&#20915;&#23450;&#20041;&#21644;&#35780;&#20272;&#21019;&#36896;&#21147;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#19981;&#20877;&#35797;&#22270;&#23545;&#21019;&#36896;&#21147;&#36827;&#34892;&#26222;&#36941;&#23450;&#20041;&#65292;&#32780;&#26159;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#19968;&#20301;&#20551;&#35774;&#30340;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#12290;&#36825;&#31181;&#35266;&#28857;&#20511;&#37492;&#20102;&#22270;&#28789;&#27979;&#35797;&#30340;&#24605;&#24819;&#65292;&#24182;&#25193;&#23637;&#20854;&#33539;&#22260;&#20197;&#35299;&#20915;&#35780;&#20272;&#21019;&#36896;&#21147;&#20013;&#25152;&#22266;&#26377;&#30340;&#25361;&#25112;&#21644;&#20027;&#35266;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#36716;&#21464;&#20351;&#24471;&#23545;AI&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#32479;&#35745;&#21019;&#36896;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#27604;&#36739;AI&#19982;&#29305;&#23450;&#20154;&#31867;&#30340;&#21019;&#36896;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creativity serves as a cornerstone for societal progress and innovation, but its assessment remains a complex and often subjective endeavor. With the rise of advanced generative AI models capable of tasks once reserved for human creativity, the study of AI's creative potential becomes imperative for its responsible development and application. This paper addresses the complexities in defining and evaluating creativity by introducing a new concept called Relative Creativity. Instead of trying to define creativity universally, we shift the focus to whether AI can match the creative abilities of a hypothetical human. This perspective draws inspiration from the Turing Test, expanding upon it to address the challenges and subjectivities inherent in evaluating creativity. This methodological shift facilitates a statistically quantifiable evaluation of AI's creativity, which we term Statistical Creativity. This approach allows for direct comparisons of AI's creative abilities with those of sp
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22797;&#26434;&#36923;&#36753;&#20551;&#35774;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#20316;&#20026;&#23454;&#29616;&#19982;&#30693;&#35782;&#22270;&#35889;&#30340;&#35825;&#23548;&#36923;&#36753;&#25512;&#29702;&#30340;&#21021;&#22987;&#27493;&#39588;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36807;&#30417;&#30563;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#32467;&#26500;&#19978;&#26356;&#25509;&#36817;&#21442;&#32771;&#20551;&#35774;&#30340;&#36923;&#36753;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#35266;&#23519;&#32467;&#26524;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;RLF-KG&#65289;&#65292;&#26368;&#23567;&#21270;&#35266;&#23519;&#32467;&#26524;&#19982;&#32467;&#35770;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2312.15643</link><description>&lt;p&gt;
&#36890;&#36807;&#22797;&#26434;&#36923;&#36753;&#20551;&#35774;&#29983;&#25104;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#25512;&#36827;&#35825;&#23548;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation. (arXiv:2312.15643v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15643
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22797;&#26434;&#36923;&#36753;&#20551;&#35774;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#20316;&#20026;&#23454;&#29616;&#19982;&#30693;&#35782;&#22270;&#35889;&#30340;&#35825;&#23548;&#36923;&#36753;&#25512;&#29702;&#30340;&#21021;&#22987;&#27493;&#39588;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36807;&#30417;&#30563;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#32467;&#26500;&#19978;&#26356;&#25509;&#36817;&#21442;&#32771;&#20551;&#35774;&#30340;&#36923;&#36753;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#35266;&#23519;&#32467;&#26524;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;RLF-KG&#65289;&#65292;&#26368;&#23567;&#21270;&#35266;&#23519;&#32467;&#26524;&#19982;&#32467;&#35770;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35825;&#23548;&#25512;&#29702;&#26159;&#36890;&#36807;&#20570;&#20986;&#26377;&#26681;&#25454;&#30340;&#29468;&#27979;&#26469;&#35299;&#37322;&#35266;&#23519;&#32467;&#26524;&#30340;&#36807;&#31243;&#12290;&#23613;&#31649;&#35768;&#22810;&#24212;&#29992;&#38656;&#35201;&#20351;&#29992;&#30693;&#35782;&#36827;&#34892;&#35299;&#37322;&#65292;&#20294;&#23558;&#35825;&#23548;&#25512;&#29702;&#19982;&#32467;&#26500;&#21270;&#30693;&#35782;&#65288;&#22914;&#30693;&#35782;&#22270;&#35889;&#65289;&#32467;&#21512;&#20351;&#29992;&#30340;&#26041;&#27861;&#20173;&#28982;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#22797;&#26434;&#36923;&#36753;&#20551;&#35774;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#20316;&#20026;&#23454;&#29616;&#19982;&#30693;&#35782;&#22270;&#35889;&#30340;&#35825;&#23548;&#36923;&#36753;&#25512;&#29702;&#30340;&#21021;&#22987;&#27493;&#39588;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#20010;&#22797;&#26434;&#30340;&#36923;&#36753;&#20551;&#35774;&#65292;&#20197;&#35299;&#37322;&#19968;&#32452;&#35266;&#23519;&#32467;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32463;&#36807;&#30417;&#30563;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#32467;&#26500;&#19978;&#26356;&#25509;&#36817;&#21442;&#32771;&#20551;&#35774;&#30340;&#36923;&#36753;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#24403;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#35266;&#23519;&#32467;&#26524;&#26102;&#65292;&#36825;&#31181;&#35757;&#32451;&#30446;&#26631;&#24182;&#19981;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#20551;&#35774;&#29983;&#25104;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65288;RLF-KG&#65289;&#65292;&#35813;&#26041;&#27861;&#26368;&#23567;&#21270;&#35266;&#23519;&#32467;&#26524;&#19982;&#32467;&#35770;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abductive reasoning is the process of making educated guesses to provide explanations for observations. Although many applications require the use of knowledge for explanations, the utilization of abductive reasoning in conjunction with structured knowledge, such as a knowledge graph, remains largely unexplored. To fill this gap, this paper introduces the task of complex logical hypothesis generation, as an initial step towards abductive logical reasoning with KG. In this task, we aim to generate a complex logical hypothesis so that it can explain a set of observations. We find that the supervised trained generative model can generate logical hypotheses that are structurally closer to the reference hypothesis. However, when generalized to unseen observations, this training objective does not guarantee better hypothesis generation. To address this, we introduce the Reinforcement Learning from Knowledge Graph (RLF-KG) method, which minimizes differences between observations and conclusio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27169;&#22411;&#37096;&#32626;&#21644;&#24182;&#34892;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;RLHF&#35757;&#32451;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#20004;&#31181;&#28789;&#27963;&#30340;&#27169;&#22411;&#37096;&#32626;&#31574;&#30053;&#65292;&#20854;&#20013;&#20132;&#26367;&#31574;&#30053;&#26377;&#21161;&#20110;&#20943;&#23569;&#20869;&#23384;&#20887;&#20313;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2312.11819</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;RLHF&#35757;&#32451;&#30340;&#33258;&#36866;&#24212;&#37096;&#32626;&#21644;&#24182;&#34892;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training. (arXiv:2312.11819v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11819
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27169;&#22411;&#37096;&#32626;&#21644;&#24182;&#34892;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;RLHF&#35757;&#32451;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#20004;&#31181;&#28789;&#27963;&#30340;&#27169;&#22411;&#37096;&#32626;&#31574;&#30053;&#65292;&#20854;&#20013;&#20132;&#26367;&#31574;&#30053;&#26377;&#21161;&#20110;&#20943;&#23569;&#20869;&#23384;&#20887;&#20313;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#25110;InstructGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#35768;&#22810;&#30740;&#31350;&#23581;&#35797;&#22797;&#29616;&#22797;&#26434;&#30340;InstructGPT&#30340;&#35757;&#32451;&#27969;&#31243;&#65292;&#21363;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#12290;&#28982;&#32780;&#65292;&#20027;&#27969;&#30340;&#20998;&#24067;&#24335;RLHF&#35757;&#32451;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#22266;&#23450;&#30340;&#27169;&#22411;&#37096;&#32626;&#31574;&#30053;&#65292;&#31216;&#20026;Flattening&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#23558;RLHF&#20013;&#28041;&#21450;&#30340;&#22235;&#20010;&#30456;&#20114;&#20381;&#36182;&#30340;&#27169;&#22411;&#35270;&#20026;&#21333;&#20010;&#23454;&#20307;&#65292;&#23558;&#23427;&#20204;&#20998;&#37197;&#21040;&#25152;&#26377;&#35774;&#22791;&#19978;&#65292;&#24182;&#24212;&#29992;&#20110;&#21333;&#20010;&#27169;&#22411;&#35774;&#35745;&#30340;&#24182;&#34892;&#25216;&#26415;&#65292;&#32780;&#19981;&#32771;&#34385;&#27599;&#20010;&#27169;&#22411;&#22266;&#26377;&#30340;&#19981;&#21516;&#24037;&#20316;&#36127;&#36733;&#12290;&#32467;&#26524;&#65292;&#35813;&#31574;&#30053;&#21152;&#21095;&#20102;RLHF&#35757;&#32451;&#20013;&#30340;&#29983;&#25104;&#29942;&#39048;&#65292;&#24182;&#38477;&#20302;&#20102;&#25972;&#20307;&#35757;&#32451;&#25928;&#29575;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#27169;&#22411;&#37096;&#32626;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#20004;&#31181;&#28789;&#27963;&#30340;&#27169;&#22411;&#37096;&#32626;&#31574;&#30053;&#12290;&#20132;&#26367;&#31574;&#30053;&#26377;&#21161;&#20110;&#20943;&#23569;&#20869;&#23384;&#20887;&#20313;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, ChatGPT or InstructGPT like large language models (LLM) has made a significant impact in the AI world. Many works have attempted to reproduce the complex InstructGPT's training pipeline, namely Reinforcement Learning with Human Feedback (RLHF). However, the mainstream distributed RLHF training methods typically adopt a fixed model placement strategy, referred to as the Flattening strategy. This strategy treats all four interdependent models involved in RLHF as a single entity, distributing them across all devices and applying parallelism techniques designed for a single model, regardless of the different workloads inherent to each model. As a result, this strategy exacerbates the generation bottlenecks in the RLHF training and degrades the overall training efficiency. To address these issues, we propose an adaptive model placement framework that offers two flexible model placement strategies. The Interleaving strategy helps reduce memory redundancy and communication costs of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#30740;&#31350;&#65292;&#20171;&#32461;&#20102;&#26368;&#26032;&#30340;&#25512;&#29702;&#20219;&#21153;&#12289;&#26041;&#27861;&#21644;&#22522;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#30784;&#27169;&#22411;&#20013;&#25512;&#29702;&#33021;&#21147;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2312.11562</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Reasoning with Foundation Models. (arXiv:2312.11562v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#30340;&#30740;&#31350;&#65292;&#20171;&#32461;&#20102;&#26368;&#26032;&#30340;&#25512;&#29702;&#20219;&#21153;&#12289;&#26041;&#27861;&#21644;&#22522;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#22522;&#30784;&#27169;&#22411;&#20013;&#25512;&#29702;&#33021;&#21147;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#26159;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#20851;&#38190;&#33021;&#21147;&#65292;&#22312;&#35848;&#21028;&#12289;&#21307;&#23398;&#35786;&#26029;&#21644;&#21009;&#20107;&#35843;&#26597;&#31561;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23427;&#22312;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#39046;&#22495;&#20013;&#20316;&#20026;&#19968;&#31181;&#22522;&#26412;&#26041;&#27861;&#23398;&#12290;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#65288;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#32773;&#23545;&#23427;&#20204;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#25512;&#29702;&#30340;&#24320;&#21019;&#24615;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#31361;&#20986;&#20102;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#12289;&#26041;&#27861;&#21644;&#22522;&#20934;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#30784;&#27169;&#22411;&#20013;&#25512;&#29702;&#33021;&#21147;&#30340;&#28508;&#22312;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#35752;&#35770;&#20102;&#22810;&#27169;&#24335;&#23398;&#20064;&#12289;&#33258;&#20027;&#20195;&#29702;&#21644;&#36229;&#32423;&#23545;&#40784;&#22312;&#25512;&#29702;&#32972;&#26223;&#19979;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#35752;&#35770;&#36825;&#20123;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#25105;&#20204;&#24076;&#26395;&#28608;&#21457;&#30740;&#31350;&#32773;&#20204;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#25506;&#32034;&#65292;&#25512;&#21160;&#36827;&#19968;&#27493;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning, a crucial ability for complex problem-solving, plays a pivotal role in various real-world settings such as negotiation, medical diagnosis, and criminal investigation. It serves as a fundamental methodology in the field of Artificial General Intelligence (AGI). With the ongoing development of foundation models, e.g., Large Language Models (LLMs), there is a growing interest in exploring their abilities in reasoning tasks. In this paper, we introduce seminal foundation models proposed or adaptable for reasoning, highlighting the latest advancements in various reasoning tasks, methods, and benchmarks. We then delve into the potential future directions behind the emergence of reasoning abilities within foundation models. We also discuss the relevance of multimodal learning, autonomous agents, and super alignment in the context of reasoning. By discussing these future research directions, we hope to inspire researchers in their exploration of this field, stimulate further advance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24773;&#32490;&#20998;&#31867;&#20013;&#30340;&#20027;&#39064;&#20559;&#24046;&#38382;&#39064;&#65292;&#21457;&#29616;&#24773;&#32490;&#35821;&#26009;&#24211;&#20013;&#30340;&#20027;&#39064;&#19982;&#24773;&#32490;&#23454;&#38469;&#19978;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#24773;&#32490;&#20998;&#31867;&#22120;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#20027;&#39064;&#30340;&#24178;&#25200;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#32773;&#23637;&#31034;&#20102;&#19968;&#31181;&#21435;&#20559;&#24046;&#30340;&#26041;&#27861;&#21487;&#20197;&#20943;&#36731;&#20027;&#39064;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2312.09043</link><description>&lt;p&gt;
&#24773;&#32490;&#20998;&#31867;&#20013;&#30340;&#20027;&#39064;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Topic Bias in Emotion Classification. (arXiv:2312.09043v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24773;&#32490;&#20998;&#31867;&#20013;&#30340;&#20027;&#39064;&#20559;&#24046;&#38382;&#39064;&#65292;&#21457;&#29616;&#24773;&#32490;&#35821;&#26009;&#24211;&#20013;&#30340;&#20027;&#39064;&#19982;&#24773;&#32490;&#23454;&#38469;&#19978;&#20855;&#26377;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#24773;&#32490;&#20998;&#31867;&#22120;&#23481;&#26131;&#21463;&#21040;&#36825;&#20123;&#20027;&#39064;&#30340;&#24178;&#25200;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#32773;&#23637;&#31034;&#20102;&#19968;&#31181;&#21435;&#20559;&#24046;&#30340;&#26041;&#27861;&#21487;&#20197;&#20943;&#36731;&#20027;&#39064;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#35821;&#26009;&#24211;&#36890;&#24120;&#26159;&#22522;&#20110;&#20851;&#38190;&#35789;/&#26631;&#31614;&#25628;&#32034;&#25110;&#36890;&#36807;&#35810;&#38382;&#30740;&#31350;&#21442;&#19982;&#32773;&#29983;&#25104;&#25991;&#26412;&#23454;&#20363;&#26469;&#37319;&#26679;&#30340;&#12290;&#26080;&#35770;&#21738;&#31181;&#24773;&#20917;&#65292;&#36825;&#20123;&#35821;&#26009;&#24211;&#37117;&#19981;&#26159;&#20195;&#34920;&#39046;&#22495;&#25972;&#20307;&#30340;&#22343;&#21248;&#26679;&#26412;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#31181;&#25968;&#25454;&#33719;&#21462;&#26041;&#24335;&#23548;&#33268;&#20102;&#36825;&#20123;&#35821;&#26009;&#24211;&#20013;&#36807;&#24230;&#21576;&#29616;&#30340;&#20027;&#39064;&#20043;&#38388;&#19981;&#20999;&#23454;&#38469;&#30340;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#20027;&#39064;&#20559;&#24046;&#21487;&#33021;&#20250;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#65292;&#20363;&#22914;&#23545;&#20110;&#23454;&#20363;"I organized the service for my aunt's funeral."&#65292;&#23613;&#31649;&#19982;&#24754;&#20260;&#24773;&#32490;&#26631;&#35760;&#30340;&#23454;&#20363;&#20013;&#30340;&#33900;&#31036;&#20107;&#20214;&#36807;&#24230;&#21576;&#29616;&#65292;&#20294;&#26356;&#36866;&#21512;&#30340;&#24773;&#32490;&#26159;&#33258;&#35946;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#25968;&#25454;&#21644;&#24314;&#27169;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#31181;&#20027;&#39064;&#20559;&#24046;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20027;&#39064;&#24314;&#27169;&#33258;&#21160;&#26631;&#35760;&#20102;&#19968;&#32452;&#24773;&#32490;&#35821;&#26009;&#24211;&#65292;&#24182;&#23637;&#31034;&#20102;&#24773;&#32490;&#23454;&#38469;&#19978;&#19982;&#29305;&#23450;&#20027;&#39064;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#24773;&#32490;&#20998;&#31867;&#22120;&#21463;&#21040;&#36825;&#20123;&#20027;&#39064;&#30340;&#24178;&#25200;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24050;&#24314;&#31435;&#30340;&#21435;&#20559;&#24046;&#26041;&#27861;&#21487;&#20197;&#20943;&#36731;&#20027;&#39064;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion corpora are typically sampled based on keyword/hashtag search or by asking study participants to generate textual instances. In any case, these corpora are not uniform samples representing the entirety of a domain. We hypothesize that this practice of data acquisition leads to unrealistic correlations between overrepresented topics in these corpora that harm the generalizability of models. Such topic bias could lead to wrong predictions for instances like "I organized the service for my aunt's funeral." when funeral events are over-represented for instances labeled with sadness, despite the emotion of pride being more appropriate here. In this paper, we study this topic bias both from the data and the modeling perspective. We first label a set of emotion corpora automatically via topic modeling and show that emotions in fact correlate with specific topics. Further, we see that emotion classifiers are confounded by such topics. Finally, we show that the established debiasing met
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36825;&#20004;&#31181;&#24120;&#35265;&#26041;&#27861;&#22312;LLMs&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;RAG&#22312;&#29616;&#26377;&#30693;&#35782;&#21644;&#26032;&#30693;&#35782;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;LLMs&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#23398;&#20064;&#26032;&#30340;&#20107;&#23454;&#20449;&#24687;&#36739;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2312.05934</link><description>&lt;p&gt;
Fine-Tuning&#36824;&#26159;&#26816;&#32034;&#65311;&#27604;&#36739;&#22312;LLMs&#20013;&#30340;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs. (arXiv:2312.05934v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05934
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36825;&#20004;&#31181;&#24120;&#35265;&#26041;&#27861;&#22312;LLMs&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;RAG&#22312;&#29616;&#26377;&#30693;&#35782;&#21644;&#26032;&#30693;&#35782;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;LLMs&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#23398;&#20064;&#26032;&#30340;&#20107;&#23454;&#20449;&#24687;&#36739;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20854;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#20013;&#23553;&#35013;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#27491;&#22914;&#23427;&#20204;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30693;&#35782;&#26412;&#36136;&#19978;&#26159;&#26377;&#38480;&#30340;&#65292;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#22806;&#37096;&#25968;&#25454;&#38598;&#26469;&#25972;&#21512;&#26032;&#30340;&#20449;&#24687;&#25110;&#25913;&#36827;LLMs&#22312;&#24050;&#35265;&#20449;&#24687;&#19978;&#30340;&#33021;&#21147;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#20010;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#65306;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#20027;&#39064;&#30340;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#33021;&#22815;&#25552;&#20379;&#19968;&#23450;&#30340;&#25913;&#36827;&#65292;&#20294;RAG&#22312;&#29616;&#26377;&#30693;&#35782;&#21644;&#23436;&#20840;&#26032;&#30693;&#35782;&#19978;&#22987;&#32456;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#24456;&#38590;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#26469;&#23398;&#20064;&#26032;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#24182;&#19988;&#26292;&#38706;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20102;&#19977;&#31181;&#38386;&#32842;&#22686;&#24378;&#26041;&#27861;&#65292;&#26088;&#22312;&#30830;&#23450;&#22312;&#22810;&#26679;&#24615;&#26041;&#38754;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24182;&#37327;&#21270;&#20102;&#28155;&#21152;&#30340;&#38386;&#32842;&#19982;&#21407;&#22987;&#20219;&#21153;&#23548;&#21521;&#35821;&#35328;&#21644;&#24120;&#35265;&#38386;&#32842;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#36229;&#36234;&#20219;&#21153;&#33539;&#22260;&#65292;&#23454;&#29616;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#33258;&#28982;&#20132;&#27969;&#30340;&#23545;&#35805;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.14067</link><description>&lt;p&gt;
&#36890;&#36807;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#24046;&#24322;&#30340;&#27604;&#36739;&#30740;&#31350;&#65292;&#22686;&#24378;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#38386;&#32842;
&lt;/p&gt;
&lt;p&gt;
Enhancing Task-Oriented Dialogues with Chitchat: a Comparative Study Based on Lexical Diversity and Divergence. (arXiv:2311.14067v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20102;&#19977;&#31181;&#38386;&#32842;&#22686;&#24378;&#26041;&#27861;&#65292;&#26088;&#22312;&#30830;&#23450;&#22312;&#22810;&#26679;&#24615;&#26041;&#38754;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24182;&#37327;&#21270;&#20102;&#28155;&#21152;&#30340;&#38386;&#32842;&#19982;&#21407;&#22987;&#20219;&#21153;&#23548;&#21521;&#35821;&#35328;&#21644;&#24120;&#35265;&#38386;&#32842;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#36229;&#36234;&#20219;&#21153;&#33539;&#22260;&#65292;&#23454;&#29616;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#33258;&#28982;&#20132;&#27969;&#30340;&#23545;&#35805;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26368;&#26032;&#30340;&#21457;&#23637;&#65292;&#20026;&#20102;&#20351;&#23545;&#35805;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#24341;&#20154;&#20837;&#32988;&#65292;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TODs&#65289;&#24050;&#32463;&#19982;&#38386;&#32842;&#32467;&#21512;&#12290;&#36825;&#31181;&#22686;&#24378;&#23545;TODs&#29305;&#21035;&#26377;&#20215;&#20540;&#65292;&#22240;&#20026;TODs&#36890;&#24120;&#23616;&#38480;&#20110;&#29421;&#31364;&#30340;&#39046;&#22495;&#65292;&#20174;&#32780;&#20351;&#28040;&#38500;&#37325;&#22797;&#21644;&#21487;&#39044;&#27979;&#30340;&#22238;&#31572;&#25104;&#20026;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#20998;&#26512;&#20102;&#19977;&#31181;&#38386;&#32842;&#22686;&#24378;&#26041;&#27861;&#65292;&#24182;&#26088;&#22312;&#30830;&#23450;&#22312;&#22810;&#26679;&#24615;&#26041;&#38754;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#28155;&#21152;&#30340;&#38386;&#32842;&#12289;&#21407;&#22987;&#20219;&#21153;&#23548;&#21521;&#35821;&#35328;&#20197;&#21450;&#20856;&#22411;&#38386;&#32842;&#25968;&#25454;&#38598;&#20013;&#30340;&#38386;&#32842;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#31361;&#20986;&#20102;&#27599;&#31181;&#27604;&#36739;&#30340;&#21069;20&#20010;&#24046;&#24322;&#20851;&#38190;&#35789;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#35752;&#35770;&#26410;&#26469;&#22686;&#24378;TODs&#30340;&#26041;&#27861;&#20855;&#26377;&#25351;&#23548;&#24847;&#20041;&#65292;&#24378;&#35843;&#20102;&#36229;&#36234;&#20219;&#21153;&#33539;&#22260;&#65292;&#23454;&#29616;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#33258;&#28982;&#20132;&#27969;&#30340;&#23545;&#35805;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a recent development, task-oriented dialogues (TODs) have been enriched with chitchat in an effort to make dialogues more diverse and engaging. This enhancement is particularly valuable as TODs are often confined to narrow domains, making the mitigation of repetitive and predictable responses a significant challenge. This paper presents a comparative analysis of three chitchat enhancements, aiming to identify the most effective approach in terms of diversity. Additionally, we quantify the divergence between the added chitchat, the original task-oriented language, and chitchat typically found in chitchat datasets, highlighting the top 20 divergent keywords for each comparison. Our findings drive a discussion on future enhancements for augmenting TODs, emphasizing the importance of grounding dialogues beyond the task to achieve more diverse and natural exchanges.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36890;&#29992;&#30701;&#35821;&#21435;&#20559;&#22120;&#8221;&#30340;&#33258;&#21160;&#22810;&#26631;&#35760;&#21435;&#20559;&#31649;&#36947;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#36731;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30701;&#35821;&#32423;&#21035;&#20559;&#35265;&#65292;&#24182;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#21644;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.13892</link><description>&lt;p&gt;
&#36890;&#29992;&#30701;&#35821;&#21435;&#20559;&#22120;&#65306;&#22312;&#22810;&#26631;&#35760;&#32423;&#21035;&#19978;&#28040;&#38500;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
General Phrase Debiaser: Debiasing Masked Language Models at a Multi-Token Level. (arXiv:2311.13892v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36890;&#29992;&#30701;&#35821;&#21435;&#20559;&#22120;&#8221;&#30340;&#33258;&#21160;&#22810;&#26631;&#35760;&#21435;&#20559;&#31649;&#36947;&#65292;&#33021;&#22815;&#26377;&#25928;&#20943;&#36731;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30701;&#35821;&#32423;&#21035;&#20559;&#35265;&#65292;&#24182;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#21644;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25152;&#25581;&#31034;&#30340;&#31038;&#20250;&#20559;&#35265;&#21644;&#19981;&#21463;&#27426;&#36814;&#30340;&#21051;&#26495;&#21360;&#35937;&#27491;&#22312;&#25104;&#20026;&#20854;&#24212;&#29992;&#30340;&#38556;&#30861;&#12290;&#19982;&#38024;&#23545;&#35789;&#32423;&#21035;&#30340;&#20247;&#22810;&#21435;&#20559;&#26041;&#27861;&#30456;&#27604;&#65292;&#23545;&#20110;&#30701;&#35821;&#32423;&#21035;&#30340;&#20559;&#35265;&#20851;&#27880;&#30456;&#23545;&#36739;&#23569;&#65292;&#38480;&#21046;&#20102;&#23398;&#31185;&#39046;&#22495;&#21435;&#20559;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36890;&#29992;&#30701;&#35821;&#21435;&#20559;&#22120;&#8221;&#30340;&#33258;&#21160;&#22810;&#26631;&#35760;&#21435;&#20559;&#31649;&#36947;&#65292;&#33021;&#22815;&#20943;&#36731;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30701;&#35821;&#32423;&#21035;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#8220;&#30701;&#35821;&#36807;&#28388;&#38454;&#27573;&#8221;&#65292;&#20174;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#20013;&#29983;&#25104;&#21051;&#26495;&#21360;&#35937;&#30340;&#30701;&#35821;&#65292;&#20197;&#21450;&#19968;&#20010;&#8220;&#27169;&#22411;&#21435;&#20559;&#38454;&#27573;&#8221;&#65292;&#21487;&#20197;&#22312;&#22810;&#26631;&#35760;&#32423;&#21035;&#19978;&#21435;&#20559;&#27169;&#22411;&#20197;&#24212;&#23545;&#30701;&#35821;&#19978;&#30340;&#20559;&#35265;&#25361;&#25112;&#12290;&#21518;&#32773;&#23547;&#25214;&#35302;&#21457;&#27169;&#22411;&#20559;&#35265;&#30340;&#25552;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20110;&#21435;&#20559;&#12290;&#26631;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;&#26368;&#26032;&#25104;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#32844;&#19994;&#21644;&#21152;&#24378;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
The social biases and unwelcome stereotypes revealed by pretrained language models are becoming obstacles to their application. Compared to numerous debiasing methods targeting word level, there has been relatively less attention on biases present at phrase level, limiting the performance of debiasing in discipline domains. In this paper, we propose an automatic multi-token debiasing pipeline called \textbf{General Phrase Debiaser}, which is capable of mitigating phrase-level biases in masked language models. Specifically, our method consists of a \textit{phrase filter stage} that generates stereotypical phrases from Wikipedia pages as well as a \textit{model debias stage} that can debias models at the multi-token level to tackle bias challenges on phrases. The latter searches for prompts that trigger model's bias, and then uses them for debiasing. State-of-the-art results on standard datasets and metrics show that our approach can significantly reduce gender biases on both career and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20803;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#37325;&#22609;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24378;&#35843;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#20803;&#25552;&#31034;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#23569;&#26679;&#26412;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.11482</link><description>&lt;p&gt;
AGI&#31995;&#32479;&#30340;&#20803;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Meta Prompting for AGI Systems. (arXiv:2311.11482v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20803;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#37325;&#22609;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24378;&#35843;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#20803;&#25552;&#31034;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#23569;&#26679;&#26412;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20803;&#25552;&#31034;(meta prompting)&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#25216;&#26415;&#65292;&#37325;&#26032;&#22609;&#36896;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12289;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#21033;&#29992;&#12290;&#22522;&#20110;&#31867;&#22411;&#29702;&#35770;&#21644;&#33539;&#30068;&#35770;&#65292;&#20803;&#25552;&#31034;&#27880;&#37325;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#20197;&#20869;&#23481;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20803;&#25552;&#31034;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#23558;&#20854;&#19982;&#23569;&#26679;&#26412;&#25552;&#31034;(few-shot prompting)&#21306;&#20998;&#24320;&#26469;&#65292;&#24182;&#24378;&#35843;&#20854;&#22312;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#37325;&#28857;&#20851;&#27880;&#23558;&#20803;&#25552;&#31034;&#25193;&#23637;&#21040;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;&#23637;&#31034;&#22914;&#20309;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#20998;&#25104;&#36739;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20196;&#29260;&#25928;&#29575;&#65292;&#24182;&#20351;&#38382;&#39064;&#27714;&#35299;&#30340;&#27604;&#36739;&#26356;&#21152;&#20844;&#24179;&#65292;&#23588;&#20854;&#26159;&#19982;&#23569;&#26679;&#26412;&#31034;&#20363;&#26041;&#27861;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#25552;&#31034;&#20219;&#21153;&#65292;&#20801;&#35768;LLMs&#20197;&#36845;&#20195;&#30340;&#20803;&#32534;&#31243;&#24418;&#24335;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive study of Meta Prompting, an innovative technique reshaping the utilization of large language models (LLMs), multi-modal foundation models, and AI systems in problem-solving and data interpretation. Grounded in type theory and category theory, Meta Prompting emphasizes the structure and syntax of information over traditional content-centric methods. The paper explores the formal definitions of Meta Prompting (MP), sets it apart from Few-Shot Prompting, and underlines its effectiveness in various AI applications. A key focus is on extending Meta Prompting to complex reasoning tasks, showing how it effectively deconstructs intricate problems into simpler sub-problems, enhancing token efficiency and enabling more equitable problem-solving comparisons, especially against few-shot example methods. Additionally, the paper introduces Meta Prompting for Prompting Tasks, allowing LLMs to self-generate new prompts in an iterative, metaprogramming-like manner. T
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20419;&#36827;&#38598;&#20307;&#20915;&#31574;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#31649;&#29702;&#23545;&#35805;&#21644;&#24179;&#34913;&#20010;&#20154;&#20559;&#22909;&#26469;&#25552;&#20379;&#28385;&#36275;&#25104;&#21592;&#38656;&#27714;&#30340;&#36873;&#39033;&#65292;&#23454;&#29616;&#39640;&#25928;&#21327;&#35843;&#24182;&#19981;&#26029;&#20248;&#21270;&#31995;&#32479;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.04928</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38598;&#20307;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Collective Decision-Making. (arXiv:2311.04928v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20419;&#36827;&#38598;&#20307;&#20915;&#31574;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#31649;&#29702;&#23545;&#35805;&#21644;&#24179;&#34913;&#20010;&#20154;&#20559;&#22909;&#26469;&#25552;&#20379;&#28385;&#36275;&#25104;&#21592;&#38656;&#27714;&#30340;&#36873;&#39033;&#65292;&#23454;&#29616;&#39640;&#25928;&#21327;&#35843;&#24182;&#19981;&#26029;&#20248;&#21270;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#24037;&#20316;&#29615;&#22659;&#20013;&#65292;&#22914;&#20250;&#35758;&#23433;&#25490;&#12289;&#21512;&#20316;&#21644;&#39033;&#30446;&#35268;&#21010;&#20013;&#65292;&#38598;&#20307;&#20915;&#31574;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#20294;&#30001;&#20110;&#20010;&#20307;&#20559;&#22909;&#22810;&#26679;&#24615;&#12289;&#24037;&#20316;&#28966;&#28857;&#19981;&#21516;&#21644;&#25104;&#21592;&#20043;&#38388;&#30340;&#26435;&#21147;&#21160;&#24577;&#31561;&#22240;&#32032;&#65292;&#24120;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#20419;&#36827;&#32676;&#20307;&#20915;&#31574;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#31649;&#29702;&#23545;&#35805;&#21644;&#24179;&#34913;&#20010;&#20154;&#20559;&#22909;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#26088;&#22312;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;&#20010;&#20307;&#20559;&#22909;&#65292;&#24182;&#25552;&#20986;&#28385;&#36275;&#25104;&#21592;&#20559;&#22909;&#30340;&#36873;&#39033;&#12290;&#25105;&#20204;&#29305;&#21035;&#23558;&#27492;&#31995;&#32479;&#24212;&#29992;&#20110;&#20225;&#19994;&#20250;&#35758;&#23433;&#25490;&#12290;&#25105;&#20204;&#21033;&#29992;LLM&#21019;&#24314;&#20102;&#21512;&#25104;&#21592;&#24037;&#37197;&#32622;&#25991;&#20214;&#65292;&#24182;&#27169;&#25311;&#20102;&#22823;&#35268;&#27169;&#30340;&#23545;&#35805;&#65292;&#36890;&#36807;&#21033;&#29992;LLM&#35780;&#20272;&#31995;&#32479;&#34920;&#29616;&#26469;&#20316;&#20026;&#24320;&#23637;&#29992;&#25143;&#30740;&#31350;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31995;&#32479;&#33021;&#23454;&#29616;&#25104;&#21592;&#19982;LLM&#31995;&#32479;&#20043;&#38388;&#30340;&#39640;&#25928;&#21327;&#35843;&#65292;&#24182;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#23545;&#20854;&#25552;&#20986;&#30340;&#36873;&#39033;&#36827;&#34892;&#25913;&#36827;&#21644;&#23436;&#21892;&#65292;&#30830;&#20445;&#20248;&#21270;&#31995;&#32479;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In various work contexts, such as meeting scheduling, collaborating, and project planning, collective decision-making is essential but often challenging due to diverse individual preferences, varying work focuses, and power dynamics among members. To address this, we propose a system leveraging Large Language Models (LLMs) to facilitate group decision-making by managing conversations and balancing preferences among individuals. Our system aims to extract individual preferences from conversations and suggest options that satisfy the preferences of the members. We specifically apply this system to corporate meeting scheduling. We create synthetic employee profiles and simulate conversations at scale, leveraging LLMs to evaluate the system performance as a novel approach to conducting a user study. Our results indicate efficient coordination with reduced interactions between the members and the LLM-based system. The system refines and improves its proposed options over time, ensuring that
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#32534;&#36753;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36229;&#32593;&#32476;&#26469;&#29983;&#25104;&#21442;&#25968;&#21464;&#21270;&#65292;&#36890;&#36807;&#35299;&#20915;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#26469;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#36890;&#36807;&#23558;&#35745;&#31639;&#20998;&#31163;&#22312;&#36229;&#32593;&#32476;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#65292;&#20351;&#24471;&#21487;&#20197;&#21516;&#26102;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2311.04661</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Massive Editing for Large Language Models via Meta Learning. (arXiv:2311.04661v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#23454;&#29616;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#32534;&#36753;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#36229;&#32593;&#32476;&#26469;&#29983;&#25104;&#21442;&#25968;&#21464;&#21270;&#65292;&#36890;&#36807;&#35299;&#20915;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#26469;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#36890;&#36807;&#23558;&#35745;&#31639;&#20998;&#31163;&#22312;&#36229;&#32593;&#32476;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#65292;&#20351;&#24471;&#21487;&#20197;&#21516;&#26102;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#26550;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#23398;&#20064;&#30693;&#35782;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#21487;&#33021;&#26159;&#22522;&#26412;&#19981;&#27491;&#30830;&#25110;&#36807;&#26102;&#30340;&#65292;&#36825;&#38656;&#35201;&#22312;&#35757;&#32451;&#21518;&#32416;&#27491;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#30693;&#35782;&#12290;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#36229;&#32593;&#32476;&#29983;&#25104;&#21442;&#25968;&#20559;&#31227;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#36229;&#32593;&#32476;&#22312;&#21516;&#27493;&#32534;&#36753;&#25805;&#20316;&#25968;&#37327;&#26041;&#38754;&#23384;&#22312;&#25193;&#23637;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#32593;&#32476;&#65288;MALMEN&#65289;&#65292;&#23427;&#23558;&#21442;&#25968;&#20559;&#31227;&#32858;&#21512;&#24418;&#24335;&#21270;&#20026;&#26368;&#23567;&#20108;&#20056;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#27491;&#35268;&#26041;&#31243;&#26356;&#26032;LM&#21442;&#25968;&#12290;&#20026;&#36866;&#24212;&#22312;&#26377;&#38480;&#20869;&#23384;&#39044;&#31639;&#19979;&#21516;&#26102;&#32534;&#36753;&#22810;&#20010;&#20107;&#23454;&#65292;&#25105;&#20204;&#23558;&#36229;&#32593;&#32476;&#21644;LM&#19978;&#30340;&#35745;&#31639;&#20998;&#31163;&#65292;&#20351;&#24471;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#37117;&#21487;&#20197;&#20855;&#26377;&#20219;&#24847;&#25209;&#37327;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23545;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#30340;LM&#65288;&#20363;&#22914;BERT-base&#65289;&#36827;&#34892;&#39640;&#36798;&#25968;&#21315;&#20010;&#20107;&#23454;&#30340;&#32534;&#36753;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have enabled learning knowledge from the pre-training corpora, the acquired knowledge may be fundamentally incorrect or outdated over time, which necessitates rectifying the knowledge of the language model (LM) after the training. A promising approach involves employing a hyper-network to generate parameter shift, whereas existing hyper-networks suffer from inferior scalability in synchronous editing operation amount. To mitigate the problem, we propose the MAssive Language Model Editing Network (MALMEN), which formulates the parameter shift aggregation as the least square problem, subsequently updating the LM parameters using the normal equation. To accommodate editing multiple facts simultaneously with limited memory budgets, we separate the computation on the hyper-network and LM, enabling arbitrary batch size on both neural networks. Our method is evaluated by editing up to thousands of facts on LMs with different architectures, i.e., BERT-base, G
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#31574;&#30053;&#30340;&#26032;&#22411;Emotion Recognition Network (ERNetCL)&#27169;&#22411;&#65292;&#36890;&#36807;&#31616;&#21270;&#32593;&#32476;&#32467;&#26500;&#24182;&#20805;&#20998;&#24314;&#27169;&#19978;&#19979;&#25991;&#26469;&#39640;&#25928;&#22320;&#25429;&#25417;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#32447;&#32034;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.06450</link><description>&lt;p&gt;
&#31616;&#21333;&#27169;&#22411;&#20063;&#26377;&#25928;&#65306;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#31574;&#30053;&#30340;&#25991;&#26412;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#32593;&#32476;&#30340;&#26032;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Simple Model Also Works: A Novel Emotion Recognition Network in Textual Conversation Based on Curriculum Learning Strategy. (arXiv:2308.06450v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#31574;&#30053;&#30340;&#26032;&#22411;Emotion Recognition Network (ERNetCL)&#27169;&#22411;&#65292;&#36890;&#36807;&#31616;&#21270;&#32593;&#32476;&#32467;&#26500;&#24182;&#20805;&#20998;&#24314;&#27169;&#19978;&#19979;&#25991;&#26469;&#39640;&#25928;&#22320;&#25429;&#25417;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#32447;&#32034;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#33021;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#65288;ERC&#65289;&#24050;&#25104;&#20026;&#23545;&#35805;&#26426;&#22120;&#20154;&#21644;&#38382;&#31572;&#31995;&#32479;&#31561;&#39046;&#22495;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#22914;&#20309;&#39640;&#25928;&#22320;&#33719;&#21462;&#19978;&#19979;&#25991;&#20013;&#30340;&#24773;&#24863;&#32447;&#32034;&#19968;&#30452;&#26159;ERC&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#20043;&#19968;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#24314;&#27169;&#19978;&#19979;&#25991;&#65292;&#24182;&#37319;&#29992;&#22797;&#26434;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#23548;&#33268;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#36807;&#22823;&#32780;&#27809;&#26377;&#23454;&#36136;&#24615;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#23398;&#20064;&#31574;&#30053;&#30340;&#26032;&#22411;Emotion Recognition Network&#65288;ERNetCL&#65289;&#12290;&#35813;&#26041;&#27861;&#20027;&#35201;&#30001;Temporal Encoder&#65288;TE&#65289;&#12289;Spatial Encoder&#65288;SE&#65289;&#21644;Curriculum Learning&#65288;CL&#65289; loss&#32452;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;TE&#21644;SE&#20197;&#31616;&#27905;&#30340;&#26041;&#24335;&#32467;&#21512;&#20102;&#20197;&#21069;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#23545;&#35805;&#20013;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#20026;&#20102;&#27169;&#25311;&#20154;&#31867;&#20174;&#26131;&#21040;&#38590;&#30340;&#35838;&#31243;&#23398;&#20064;&#26041;&#24335;&#65292;&#25105;&#20204;&#23558;CL&#30340;&#24605;&#24819;&#24212;&#29992;&#21040;ERC&#20219;&#21153;&#20013;&#65292;&#36880;&#27493;&#20248;&#21270;&#32593;&#32476;&#26500;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation (ERC) has emerged as a research hotspot in domains such as conversational robots and question-answer systems. How to efficiently and adequately retrieve contextual emotional cues has been one of the key challenges in the ERC task. Existing efforts do not fully model the context and employ complex network structures, resulting in excessive computational resource overhead without substantial performance improvement. In this paper, we propose a novel Emotion Recognition Network based on Curriculum Learning strategy (ERNetCL). The proposed ERNetCL primarily consists of Temporal Encoder (TE), Spatial Encoder (SE), and Curriculum Learning (CL) loss. We utilize TE and SE to combine the strengths of previous methods in a simplistic manner to efficiently capture temporal and spatial contextual information in the conversation. To simulate the way humans learn curriculum from easy to hard, we apply the idea of CL to the ERC task to progressively optimize the ne
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31639;&#26415;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20869;&#37096;&#30340;&#20540;&#31354;&#38388;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.01154</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31639;&#26415;&#36816;&#31639;&#65306;&#20174;&#35760;&#24518;&#21040;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Arithmetic with Language Models: from Memorization to Computation. (arXiv:2308.01154v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31639;&#26415;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20869;&#37096;&#30340;&#20540;&#31354;&#38388;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26356;&#22909;&#22320;&#29702;&#35299;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#24615;&#35745;&#31639;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#23545;&#20110;&#36827;&#19968;&#27493;&#25913;&#36827;&#23427;&#20204;&#24182;&#25299;&#23485;&#20854;&#36866;&#29992;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#20010;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22312;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#25191;&#34892;&#31639;&#26415;&#35745;&#31639;&#12290;&#20108;&#36827;&#21046;&#21152;&#27861;&#21644;&#20056;&#27861;&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#27979;&#35797;&#22522;&#30784;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#19968;&#20010;&#38750;&#24120;&#23567;&#30340;&#35789;&#27719;&#34920;&#65292;&#24182;&#19988;&#22312;&#36755;&#20837;/&#36755;&#20986;&#19978;&#23637;&#31034;&#20102;&#30456;&#20851;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#20351;&#24471;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#24179;&#28369;&#30340;&#36755;&#20837;&#25554;&#20540;&#26080;&#25928;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#35757;&#32451;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#23398;&#20064;&#36825;&#20123;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20854;&#22806;&#25512;&#33021;&#21147;&#21644;&#20869;&#37096;&#20449;&#24687;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25903;&#25345;&#36825;&#26679;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#19968;&#20010;&#32534;&#30721;-&#22238;&#24402;-&#35299;&#30721;&#26426;&#22120;&#65292;&#19968;&#26086;&#23558;&#36755;&#20837;&#26631;&#35760;&#34920;&#31034;&#26144;&#23556;&#21040;&#21512;&#36866;&#30340;&#20869;&#37096;&#20540;&#31354;&#38388;&#65292;&#35745;&#31639;&#23601;&#22312;&#20540;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
A better understanding of the emergent computation and problem-solving capabilities of recent large language models is of paramount importance to further improve them and broaden their applicability. This work investigates how a language model, trained to predict the next token, can perform arithmetic computations generalizing beyond training data. Binary addition and multiplication constitute a good testbed for this purpose, since they require a very small vocabulary and exhibit relevant input/output discontinuities making smooth input interpolation ineffective for novel data. We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing. Our findings support the hypotheses that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input token representation is mapped to an appropriate intern
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;KoBBQ&#65292;&#19968;&#20010;&#38024;&#23545;&#38889;&#22269;&#25991;&#21270;&#30340;&#20559;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#30340;&#25991;&#21270;&#36866;&#24212;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#35843;&#26597;&#25910;&#38598;&#21644;&#39564;&#35777;&#20102;&#21453;&#26144;&#38889;&#22269;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#30340;&#31038;&#20250;&#20559;&#35265;&#21644;&#20559;&#35265;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.16778</link><description>&lt;p&gt;
KoBBQ: &#38024;&#23545;&#38382;&#31572;&#30340;&#38889;&#22269;&#20559;&#35265;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
KoBBQ: Korean Bias Benchmark for Question Answering. (arXiv:2307.16778v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;KoBBQ&#65292;&#19968;&#20010;&#38024;&#23545;&#38889;&#22269;&#25991;&#21270;&#30340;&#20559;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#30340;&#25991;&#21270;&#36866;&#24212;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22823;&#35268;&#27169;&#35843;&#26597;&#25910;&#38598;&#21644;&#39564;&#35777;&#20102;&#21453;&#26144;&#38889;&#22269;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#30340;&#31038;&#20250;&#20559;&#35265;&#21644;&#20559;&#35265;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#35265;&#38382;&#31572;&#30340;&#22522;&#20934;&#65288;BBQ&#65289;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#20294;&#26159;&#23558;&#27492;&#22522;&#20934;&#36866;&#24212;&#20110;&#32654;&#22269;&#20197;&#22806;&#30340;&#25991;&#21270;&#32972;&#26223;&#24182;&#19981;&#31616;&#21333;&#65292;&#22240;&#20026;&#31038;&#20250;&#20559;&#35265;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#25991;&#21270;&#32972;&#26223;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;KoBBQ&#65292;&#19968;&#20010;&#38889;&#22269;&#20559;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#35299;&#20915;&#25968;&#25454;&#38598;&#30340;&#25991;&#21270;&#36866;&#24212;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;BBQ&#25968;&#25454;&#38598;&#20998;&#20026;&#19977;&#31867;&#8212;&#8212;&#31616;&#21333;&#36716;&#25442;&#65288;&#21487;&#20197;&#22312;&#25991;&#21270;&#32763;&#35793;&#21518;&#30452;&#25509;&#20351;&#29992;&#65289;&#12289;&#30446;&#26631;&#20462;&#25913;&#65288;&#38656;&#35201;&#22312;&#30446;&#26631;&#32676;&#20307;&#20013;&#36827;&#34892;&#26412;&#22320;&#21270;&#65289;&#21644;&#26679;&#26412;&#21024;&#38500;&#65288;&#19981;&#36866;&#21512;&#38889;&#22269;&#25991;&#21270;&#65289;&#65292;&#24182;&#28155;&#21152;&#20102;&#22235;&#20010;&#38024;&#23545;&#38889;&#22269;&#25991;&#21270;&#29305;&#23450;&#30340;&#20559;&#35265;&#31867;&#21035;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#35843;&#26597;&#65292;&#25910;&#38598;&#21644;&#39564;&#35777;&#20102;&#21453;&#26144;&#38889;&#22269;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#30340;&#31038;&#20250;&#20559;&#35265;&#21644;&#20559;&#35265;&#30446;&#26631;&#12290;&#26368;&#32456;&#30340;KoBBQ&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;268&#20010;&#27169;&#26495;&#21644;76,048&#20010;&#26679;&#26412;&#65292;&#28085;&#30422;&#20102;12&#20010;&#31038;&#20250;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Bias Benchmark for Question Answering (BBQ) is designed to evaluate social biases of language models (LMs), but it is not simple to adapt this benchmark to cultural contexts other than the US because social biases depend heavily on the cultural context. In this paper, we present KoBBQ, a Korean bias benchmark dataset, and we propose a general framework that addresses considerations for cultural adaptation of a dataset. Our framework includes partitioning the BBQ dataset into three classes--Simply-Transferred (can be used directly after cultural translation), Target-Modified (requires localization in target groups), and Sample-Removed (does not fit Korean culture)-- and adding four new categories of bias specific to Korean culture. We conduct a large-scale survey to collect and validate the social biases and the targets of the biases that reflect the stereotypes in Korean culture. The resulting KoBBQ dataset comprises 268 templates and 76,048 samples across 12 categories of social b
&lt;/p&gt;</description></item><item><title>&#22312;&#26080;&#26465;&#20214;&#35821;&#38899;&#21512;&#25104;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ASGAN&#30340;&#35299;&#32806;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20511;&#37492;&#20102;StyleGAN&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20123;&#26032;&#25216;&#26415;&#12290;&#36890;&#36807;&#23398;&#20064;&#35299;&#32806;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;ASGAN&#33021;&#22815;&#20174;&#28508;&#22312;&#31354;&#38388;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#35821;&#38899;&#65292;&#21363;&#20351;&#22312;&#23567;&#23383;&#20856;&#25968;&#25454;&#38598;&#19978;&#20063;&#33021;&#21462;&#24471;&#26368;&#26032;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.01673</link><description>&lt;p&gt;
&#26080;&#26465;&#20214;&#35821;&#38899;&#21512;&#25104;&#20013;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#20013;&#30340;&#35299;&#32806;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Disentanglement in a GAN for Unconditional Speech Synthesis. (arXiv:2307.01673v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01673
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#26465;&#20214;&#35821;&#38899;&#21512;&#25104;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ASGAN&#30340;&#35299;&#32806;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20511;&#37492;&#20102;StyleGAN&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20123;&#26032;&#25216;&#26415;&#12290;&#36890;&#36807;&#23398;&#20064;&#35299;&#32806;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;ASGAN&#33021;&#22815;&#20174;&#28508;&#22312;&#31354;&#38388;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#35821;&#38899;&#65292;&#21363;&#20351;&#22312;&#23567;&#23383;&#20856;&#25968;&#25454;&#38598;&#19978;&#20063;&#33021;&#21462;&#24471;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#24320;&#21457;&#19968;&#20010;&#27169;&#22411;&#65292;&#33021;&#22815;&#30452;&#25509;&#20174;&#28508;&#22312;&#31354;&#38388;&#21512;&#25104;&#36924;&#30495;&#30340;&#35821;&#38899;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#26465;&#20214;&#65311;&#23613;&#31649;&#36807;&#21435;&#21313;&#24180;&#20013;&#36827;&#34892;&#20102;&#20960;&#27425;&#23581;&#35797;&#65292;&#20043;&#21069;&#30340;&#23545;&#25239;&#24615;&#21644;&#25193;&#25955;&#24615;&#26041;&#27861;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#65292;&#21363;&#20351;&#22312;&#23567;&#23383;&#20856;&#25968;&#25454;&#38598;&#19978;&#20063;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AudioStyleGAN(ASGAN)&#8212;&#8212;&#19968;&#31181;&#29992;&#20110;&#26080;&#26465;&#20214;&#35821;&#38899;&#21512;&#25104;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#35299;&#32806;&#28508;&#22312;&#31354;&#38388;&#12290;&#22312;StyleGAN&#31995;&#21015;&#22270;&#20687;&#21512;&#25104;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;ASGAN&#65292;&#23427;&#23558;&#37319;&#26679;&#22122;&#22768;&#26144;&#23556;&#21040;&#19968;&#20010;&#35299;&#32806;&#28508;&#22312;&#21521;&#37327;&#65292;&#28982;&#21518;&#23558;&#20854;&#26144;&#23556;&#21040;&#19968;&#20010;&#38899;&#39057;&#29305;&#24449;&#24207;&#21015;&#65292;&#20197;&#22312;&#27599;&#19968;&#23618;&#20013;&#25233;&#21046;&#20449;&#21495;&#28151;&#21472;&#12290;&#20026;&#20102;&#25104;&#21151;&#35757;&#32451;ASGAN&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20123;&#26032;&#25216;&#26415;&#65292;&#21253;&#25324;&#23545;&#33258;&#36866;&#24212;&#37492;&#21035;&#22120;&#22686;&#24378;&#30340;&#20462;&#25913;&#65292;&#20351;&#20854;&#20197;&#27010;&#29575;&#26041;&#24335;&#36339;&#36807;&#37492;&#21035;&#22120;&#26356;&#26032;&#12290;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#23567;&#23383;&#20856;&#30340;&#35895;&#27468;&#35821;&#38899;&#21629;&#20196;&#25968;&#23383;&#25968;&#25454;&#38598;&#19978;&#65292;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26080;&#26465;&#20214;&#35821;&#38899;&#21512;&#25104;&#26041;&#38754;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can we develop a model that can synthesize realistic speech directly from a latent space, without explicit conditioning? Despite several efforts over the last decade, previous adversarial and diffusion-based approaches still struggle to achieve this, even on small-vocabulary datasets. To address this, we propose AudioStyleGAN (ASGAN) -- a generative adversarial network for unconditional speech synthesis tailored to learn a disentangled latent space. Building upon the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a disentangled latent vector which is then mapped to a sequence of audio features so that signal aliasing is suppressed at every layer. To successfully train ASGAN, we introduce a number of new techniques, including a modification to adaptive discriminator augmentation which probabilistically skips discriminator updates. We apply it on the small-vocabulary Google Speech Commands digits dataset, where it achieves state-of-the-art results in unconditional
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#23618;&#20013;&#32534;&#30721;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#20063;&#23398;&#20064;&#20102;&#31867;&#20284;&#38899;&#32032;&#30340;&#23376;&#35789;&#21333;&#20803;&#12290;&#19982;&#21333;&#35789;&#30456;&#20851;&#30340;&#20449;&#24687;&#20027;&#35201;&#22312;&#20013;&#38388;&#30340;&#27169;&#22411;&#23618;&#20013;&#65292;&#21516;&#26102;&#19968;&#20123;&#20302;&#32423;&#20449;&#24687;&#22312;&#26356;&#39640;&#30340;&#23618;&#20013;&#20063;&#24471;&#20197;&#20445;&#30041;&#12290;</title><link>http://arxiv.org/abs/2307.00162</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#23545;&#21333;&#35789;&#30340;&#20102;&#35299;&#31243;&#24230;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What do self-supervised speech models know about words?. (arXiv:2307.00162v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00162
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#23618;&#20013;&#32534;&#30721;&#20102;&#19981;&#21516;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#20063;&#23398;&#20064;&#20102;&#31867;&#20284;&#38899;&#32032;&#30340;&#23376;&#35789;&#21333;&#20803;&#12290;&#19982;&#21333;&#35789;&#30456;&#20851;&#30340;&#20449;&#24687;&#20027;&#35201;&#22312;&#20013;&#38388;&#30340;&#27169;&#22411;&#23618;&#20013;&#65292;&#21516;&#26102;&#19968;&#20123;&#20302;&#32423;&#20449;&#24687;&#22312;&#26356;&#39640;&#30340;&#23618;&#20013;&#20063;&#24471;&#20197;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#35768;&#22810;&#33258;&#25105;&#30417;&#30563;&#30340;&#35821;&#38899;&#27169;&#22411;&#65288;S3Ms&#65289;&#34987;&#24341;&#20837;&#65292;&#20026;&#21508;&#31181;&#35821;&#38899;&#20219;&#21153;&#25552;&#20379;&#20102;&#24615;&#33021;&#21644;&#25968;&#25454;&#25928;&#29575;&#30340;&#25913;&#36827;&#12290;&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;S3Ms&#22312;&#19981;&#21516;&#30340;&#23618;&#20013;&#32534;&#30721;&#35821;&#35328;&#20449;&#24687;&#65292;&#32780;&#19988;&#19968;&#20123;S3Ms&#20284;&#20046;&#23398;&#20064;&#20102;&#31867;&#20284;&#20110;&#38899;&#32032;&#30340;&#23376;&#35789;&#21333;&#20803;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#25429;&#25417;&#26356;&#22823;&#30340;&#35821;&#35328;&#21333;&#20803;&#65288;&#22914;&#21333;&#35789;&#65289;&#30340;&#31243;&#24230;&#20197;&#21450;&#21333;&#35789;&#30456;&#20851;&#20449;&#24687;&#30340;&#32534;&#30721;&#20301;&#32622;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#26469;&#33258;&#19977;&#20010;S3Ms&#30340;&#19981;&#21516;&#23618;&#30340;&#21333;&#35789;&#29255;&#27573;&#34920;&#31034;&#36827;&#34892;&#20102;&#22810;&#31181;&#20998;&#26512;&#65306;wav2vec2&#12289;HuBERT&#21644;WavLM&#12290;&#25105;&#20204;&#21033;&#29992;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;&#65288;CCA&#65289;&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#26469;&#34913;&#37327;&#36825;&#20123;&#34920;&#31034;&#19982;&#21333;&#35789;&#32423;&#35821;&#35328;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#26368;&#22823;&#30340;&#21333;&#35789;&#32423;&#35821;&#35328;&#20869;&#23481;&#24448;&#24448;&#20986;&#29616;&#22312;&#20013;&#38388;&#30340;&#27169;&#22411;&#23618;&#65292;&#32780;&#19968;&#20123;&#20302;&#32423;&#20449;&#24687;&#65288;&#22914;&#21457;&#38899;&#65289;&#20063;&#22312;&#26356;&#39640;&#30340;&#23618;&#20013;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many self-supervised speech models (S3Ms) have been introduced over the last few years, producing performance and data efficiency improvements for a variety of speech tasks. Evidence is emerging that different S3Ms encode linguistic information in different layers, and also that some S3Ms appear to learn phone-like sub-word units. However, the extent to which these models capture larger linguistic units, such as words, and where word-related information is encoded, remains unclear. In this study, we conduct several analyses of word segment representations extracted from different layers of three S3Ms: wav2vec2, HuBERT, and WavLM. We employ canonical correlation analysis (CCA), a lightweight analysis tool, to measure the similarity between these representations and word-level linguistic properties. We find that the maximal word-level linguistic content tends to be found in intermediate model layers, while some lower-level information like pronunciation is also retained in higher layers 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#36127;&#26679;&#26412;&#24230;&#37327;&#23398;&#20064;&#26694;&#26550;&#65288;P3M&#65289;&#29992;&#20110;&#20855;&#26377;&#19981;&#23436;&#25972;&#26631;&#27880;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#65292;&#36890;&#36807;&#25289;&#36817;&#23454;&#20307;&#23884;&#20837;&#21644;&#20854;&#23545;&#24212;&#20851;&#31995;&#23884;&#20837;&#30340;&#36317;&#31163;&#65292;&#21516;&#26102;&#20351;&#20854;&#19982;&#38750;&#31867;&#21035;&#20851;&#31995;&#23884;&#20837;&#30340;&#36317;&#31163;&#25512;&#36828;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.14806</link><description>&lt;p&gt;
&#29992;&#20110;&#20855;&#26377;&#19981;&#23436;&#25972;&#26631;&#27880;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#30340;&#27491;&#36127;&#26679;&#26412;&#24230;&#37327;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Positive-Unlabeled Metric Learning Framework for Document-Level Relation Extraction with Incomplete Labeling. (arXiv:2306.14806v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14806
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#36127;&#26679;&#26412;&#24230;&#37327;&#23398;&#20064;&#26694;&#26550;&#65288;P3M&#65289;&#29992;&#20110;&#20855;&#26377;&#19981;&#23436;&#25972;&#26631;&#27880;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#65292;&#36890;&#36807;&#25289;&#36817;&#23454;&#20307;&#23884;&#20837;&#21644;&#20854;&#23545;&#24212;&#20851;&#31995;&#23884;&#20837;&#30340;&#36317;&#31163;&#65292;&#21516;&#26102;&#20351;&#20854;&#19982;&#38750;&#31867;&#21035;&#20851;&#31995;&#23884;&#20837;&#30340;&#36317;&#31163;&#25512;&#36828;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#30340;&#30446;&#26631;&#26159;&#35782;&#21035;&#36328;&#22810;&#20010;&#21477;&#23376;&#30340;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26368;&#36817;&#65292;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#19981;&#23436;&#25972;&#26631;&#27880;&#38382;&#39064;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#19968;&#20123;&#30740;&#31350;&#37319;&#29992;&#27491;&#36127;&#26679;&#26412;&#23398;&#20064;&#31561;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#26679;&#26412;&#22686;&#24378;&#21644;&#27491;&#26679;&#26412;&#28151;&#21512;&#30340;&#27491;&#36127;&#26679;&#26412;&#24230;&#37327;&#23398;&#20064;&#26694;&#26550;&#65288;P3M&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#24418;&#24335;&#21270;&#20026;&#24230;&#37327;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#26088;&#22312;&#25289;&#36817;&#23454;&#20307;&#23545;&#23884;&#20837;&#21644;&#23427;&#20204;&#23545;&#24212;&#30340;&#20851;&#31995;&#23884;&#20837;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#21516;&#26102;&#23558;&#23427;&#20204;&#19982;&#38750;&#31867;&#21035;&#20851;&#31995;&#23884;&#20837;&#30340;&#36317;&#31163;&#25512;&#36828;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#27491;&#36127;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#35813;&#25439;&#22833;&#30446;&#26631;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;dropout&#26469;&#22686;&#21152;&#27491;&#26679;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#27491;-&#38750;&#31867;&#21035;&#28151;&#21512;&#26041;&#27861;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;P3M&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of document-level relation extraction (RE) is to identify relations between entities that span multiple sentences. Recently, incomplete labeling in document-level RE has received increasing attention, and some studies have used methods such as positive-unlabeled learning to tackle this issue, but there is still a lot of room for improvement. Motivated by this, we propose a positive-augmentation and positive-mixup positive-unlabeled metric learning framework (P3M). Specifically, we formulate document-level RE as a metric learning problem. We aim to pull the distance closer between entity pair embedding and their corresponding relation embedding, while pushing it farther away from the none-class relation embedding. Additionally, we adapt the positive-unlabeled learning to this loss objective. In order to improve the generalizability of the model, we use dropout to augment positive samples and propose a positive-none-class mixup method. Extensive experiments show that P3M improve
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31995;&#32479;&#32423;&#21035;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20219;&#21153;&#24230;&#37327;&#35774;&#35745;&#21644;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#20309;&#20351;&#29992;&#21453;&#39304;&#22312;&#20154;&#24037;&#20132;&#20114;&#27969;&#31243;&#20013;&#24418;&#24335;&#21270;&#31995;&#32479;&#32423;&#21035;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#20415;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#21644;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.13588</link><description>&lt;p&gt;
&#31995;&#32479;&#32423;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
System-Level Natural Language Feedback. (arXiv:2306.13588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31995;&#32479;&#32423;&#21035;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20219;&#21153;&#24230;&#37327;&#35774;&#35745;&#21644;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#20309;&#20351;&#29992;&#21453;&#39304;&#22312;&#20154;&#24037;&#20132;&#20114;&#27969;&#31243;&#20013;&#24418;&#24335;&#21270;&#31995;&#32479;&#32423;&#21035;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#20415;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#21644;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#21253;&#21547;&#20102;&#20016;&#23500;&#30340;&#29992;&#25143;&#20307;&#39564;&#20449;&#24687;&#12290;&#29616;&#26377;&#30740;&#31350;&#32858;&#28966;&#20110;&#23454;&#20363;&#32423;&#21035;&#30340;&#26041;&#27861;&#65292;&#21363;&#23558;&#21453;&#39304;&#29992;&#20110;&#32454;&#21270;&#29305;&#23450;&#20363;&#23376;&#65292;&#32780;&#24573;&#30053;&#20102;&#20854;&#31995;&#32479;&#33539;&#22260;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#38145;&#31995;&#32479;&#32423;&#21035;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#21453;&#39304;&#22312;&#20154;&#24037;&#20132;&#20114;&#27969;&#31243;&#20013;&#24418;&#24335;&#21270;&#31995;&#32479;&#32423;&#21035;&#30340;&#35774;&#35745;&#20915;&#31574;&#65292;&#20197;&#20415;&#20135;&#29983;&#26356;&#22909;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#26159;&#36890;&#36807;&#20197;&#19979;&#20004;&#26041;&#38754;&#23454;&#29616;&#30340;&#65306;(i) &#20219;&#21153;&#24230;&#37327;&#35774;&#35745;; (ii) &#29992;&#20110;&#25913;&#36827;&#27169;&#22411;&#21709;&#24212;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#35774;&#35745;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#39033;&#26696;&#20363;&#30740;&#31350;&#65292;&#26469;&#25913;&#36827;&#25628;&#32034;&#26597;&#35810;&#29983;&#25104;&#21644;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#65292;&#23637;&#31034;&#20102;&#20351;&#29992;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#31995;&#32479;&#32423;&#21035;&#21453;&#39304;&#21644;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#30340;&#32452;&#21512;&#24102;&#26469;&#20102;&#36827;&#19968;&#27493;&#30340;&#25910;&#30410;&#65292;&#24182;&#19988;&#30001;&#20154;&#31867;&#25776;&#20889;&#30340;&#23454;&#20363;&#32423;&#21035;&#21453;&#39304;&#23548;&#33268;&#27604;GPT-3.5&#25776;&#20889;&#30340;&#21453;&#39304;&#26356;&#21152;&#25166;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language (NL) feedback contains rich information about the user experience. Existing studies focus on an instance-level approach, where feedback is used to refine specific examples, disregarding its system-wide application. This paper proposes a general framework for unlocking the system-level use of NL feedback. We show how to use feedback to formalize system-level design decisions in a human-in-the-loop-process -- in order to produce better models. In particular this is done through: (i) metric design for tasks; and (ii) language model prompt design for refining model responses. We conduct two case studies of this approach for improving search query generation and dialog response generation, demonstrating the effectiveness of the use of system-level feedback. We show the combination of system-level feedback and instance-level feedback brings further gains, and that human written instance-level feedback results in more grounded refinements than GPT-3.5 written ones, underlying
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21069;&#30651;&#24615;&#30340;&#32479;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#36335;&#32447;&#22270;&#65292;&#36890;&#36807;&#19977;&#20010;&#26694;&#26550;&#65306;&#22686;&#24378;KGs&#30340;LLMs&#65292;&#30693;&#35782;&#22686;&#24378;KGs&#21644;LLMs&#19982;KGs&#30340;&#32852;&#21512;&#25512;&#29702;&#65292;&#32508;&#21512;&#21033;&#29992;&#20004;&#32773;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.08302</link><description>&lt;p&gt;
&#32479;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;: &#19968;&#26465;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Unifying Large Language Models and Knowledge Graphs: A Roadmap. (arXiv:2306.08302v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21069;&#30651;&#24615;&#30340;&#32479;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#36335;&#32447;&#22270;&#65292;&#36890;&#36807;&#19977;&#20010;&#26694;&#26550;&#65306;&#22686;&#24378;KGs&#30340;LLMs&#65292;&#30693;&#35782;&#22686;&#24378;KGs&#21644;LLMs&#19982;KGs&#30340;&#32852;&#21512;&#25512;&#29702;&#65292;&#32508;&#21512;&#21033;&#29992;&#20004;&#32773;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;GPT4&#27491;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#25472;&#36215;&#26032;&#30340;&#28909;&#28526;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#31361;&#29616;&#33021;&#21147;&#21644;&#19968;&#33324;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLM&#26159;&#40657;&#30418;&#27169;&#22411;&#65292;&#24448;&#24448;&#19981;&#33021;&#25429;&#25417;&#21644;&#33719;&#21462;&#23454;&#38469;&#30693;&#35782;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#21326;&#26222;&#21017;&#26159;&#26126;&#30830;&#23384;&#20648;&#20016;&#23500;&#23454;&#38469;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#27169;&#22411;&#12290;KGs&#21487;&#20197;&#36890;&#36807;&#20026;&#25512;&#29702;&#21644;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#22806;&#37096;&#30693;&#35782;&#26469;&#22686;&#24378;LLMs&#12290;&#21516;&#26102;&#65292;KGs&#30340;&#26500;&#24314;&#22256;&#38590;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#28436;&#21270;&#65292;&#36825;&#25361;&#25112;&#20102;&#29616;&#26377;&#30340;KGs&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#20107;&#23454;&#24182;&#34920;&#31034;&#26410;&#35265;&#36807;&#30340;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#32479;&#19968;LLMs&#21644;KGs&#24182;&#21516;&#26102;&#21033;&#29992;&#23427;&#20204;&#30340;&#20248;&#28857;&#26159;&#26377;&#30410;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21069;&#30651;&#24615;&#30340;&#32479;&#19968;LLMs&#21644;KGs&#30340;&#36335;&#32447;&#22270;&#12290;&#25105;&#20204;&#30340;&#36335;&#32447;&#22270;&#21253;&#25324;&#19977;&#20010;&#19968;&#33324;&#26694;&#26550;&#65292;&#21363;1&#65289;&#22686;&#24378;KGs&#30340;LLMs&#65292;&#23427;&#20204;&#23558;&#30693;&#35782;&#34920;&#31034;&#20026;LM&#30340;&#19968;&#37096;&#20998;&#65292;&#20174;&#32780;&#33021;&#22815;&#25429;&#25417;&#20016;&#23500;&#30340;&#23454;&#20307;&#20851;&#31995;&#65292;2&#65289;&#30693;&#35782;&#22686;&#24378;KGs&#65292;&#23427;&#20204;&#23558;LLMs&#29992;&#20316;&#30693;&#35782;&#34920;&#31034;&#23398;&#20064;&#30340;&#20248;&#31168;&#24037;&#20855;&#65292;3&#65289;LLMs&#19982;KGs&#30340;&#32852;&#21512;&#25512;&#29702;&#65292;&#20854;&#20013;LLMs&#21644;KGs&#30456;&#20114;&#22686;&#24378;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorpo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377;&#26694;&#26550;&#20013;&#22686;&#21152;&#19968;&#27493;&#35821;&#20041;&#30693;&#35782;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#33258;&#21160;&#35821;&#38899;&#32763;&#35793;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#32763;&#35793;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#24182;&#20943;&#23569;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#38388;&#38553;(TRFGap)&#12290;</title><link>http://arxiv.org/abs/2306.00789</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#38899;&#32763;&#35793;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Transfer Learning for Low-Resource Speech Translation. (arXiv:2306.00789v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19977;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377;&#26694;&#26550;&#20013;&#22686;&#21152;&#19968;&#27493;&#35821;&#20041;&#30693;&#35782;&#33976;&#39311;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;&#33258;&#21160;&#35821;&#38899;&#32763;&#35793;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#32763;&#35793;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#24182;&#20943;&#23569;&#20102;&#36328;&#35821;&#35328;&#36801;&#31227;&#38388;&#38553;(TRFGap)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#33258;&#21160;&#35821;&#38899;&#32763;&#35793;&#20013;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#23558;&#35821;&#20041;&#30693;&#35782;&#33976;&#39311;&#27493;&#39588;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#20004;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;XLS-R&#20013;&#12290;&#36825;&#19968;&#39069;&#22806;&#30340;&#27493;&#39588;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#35821;&#38899;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#23545;&#22810;&#35821;&#35328;&#35821;&#38899;&#32534;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#20197;&#32534;&#30721;&#35821;&#20041;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#19977;&#27493;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#35299;&#20915;&#20102;XLS-R&#26694;&#26550;&#20013;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#30340;&#22823;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#24046;&#36317;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;CoVoST-2&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#21644;&#27604;&#36739;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#32763;&#35793;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#24182;&#19988;&#36328;&#35821;&#35328;&#36801;&#31227;&#38388;&#38553;(TRFGap)&#26377;&#26126;&#26174;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents a novel three-step transfer learning framework for enhancing cross-lingual transfer from high- to low-resource languages in the downstream application of Automatic Speech Translation. The approach integrates a semantic knowledge-distillation step into the existing two-step cross-lingual transfer learning framework XLS-R. This extra step aims to encode semantic knowledge in the multilingual speech encoder pre-trained via Self-Supervised Learning using unlabeled speech. Our proposed three-step cross-lingual transfer learning framework addresses the large cross-lingual transfer gap (TRFGap) observed in the XLS-R framework between high-resource and low-resource languages. We validate our proposal through extensive experiments and comparisons on the CoVoST-2 benchmark, showing significant improvements in translation performance, especially for low-resource languages, and a notable reduction in the TRFGap.
&lt;/p&gt;</description></item><item><title>OpenPI2.0&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#20307;&#36861;&#36394;&#30340;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#23427;&#21253;&#25324;&#35268;&#33539;&#21270;&#23454;&#20307;&#12289;&#26174;&#33879;&#24615;&#27880;&#37322;&#21644;&#19979;&#28216;&#24212;&#29992;&#35843;&#26597;&#31561;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.14603</link><description>&lt;p&gt;
OpenPI2.0: &#19968;&#31181;&#29992;&#20110;&#23454;&#20307;&#36861;&#36394;&#30340;&#25913;&#36827;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OpenPI2.0: An Improved Dataset for Entity Tracking in Texts. (arXiv:2305.14603v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14603
&lt;/p&gt;
&lt;p&gt;
OpenPI2.0&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#20307;&#36861;&#36394;&#30340;&#25913;&#36827;&#25968;&#25454;&#38598;&#65292;&#23427;&#21253;&#25324;&#35268;&#33539;&#21270;&#23454;&#20307;&#12289;&#26174;&#33879;&#24615;&#27880;&#37322;&#21644;&#19979;&#28216;&#24212;&#29992;&#35843;&#26597;&#31561;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#25991;&#26412;&#34920;&#31034;&#20026;&#23454;&#20307;&#20449;&#24687;&#19968;&#30452;&#34987;&#35748;&#20026;&#22312;&#20107;&#20214;&#25512;&#29702;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;OpenPI2.0&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#36861;&#36394;&#31243;&#24207;&#24615;&#25991;&#26412;&#20013;&#23454;&#20307;&#29366;&#24577;&#30340;&#25913;&#36827;&#25968;&#25454;&#38598;&#12290;OpenPI2.0&#19981;&#20165;&#20855;&#26377;&#35268;&#33539;&#21270;&#23454;&#20307;&#20197;&#20419;&#36827;&#35780;&#20272;&#65292;&#36824;&#21253;&#25324;&#28085;&#30422;&#20154;&#24037;&#26631;&#31614;&#21644;&#33258;&#21160;&#39044;&#27979;&#30340;&#26174;&#33879;&#24615;&#27880;&#37322;&#12290;&#20851;&#20110;&#23454;&#20307;&#26174;&#33879;&#24615;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;&#27880;&#37322;&#20027;&#35266;&#24615;&#12289;&#24314;&#27169;&#21487;&#34892;&#24615;&#20197;&#21450;&#22312;&#38382;&#39064;&#22238;&#31572;&#21644;&#32463;&#20856;&#35745;&#21010;&#31561;&#20219;&#21153;&#20013;&#30340;&#19979;&#28216;&#24212;&#29992;&#30340;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representing texts as information about entities has long been deemed effective in event reasoning. We propose OpenPI2.0, an improved dataset for tracking entity states in procedural texts. OpenPI2.0 features not only canonicalized entities that facilitate evaluation, but also salience annotations including both manual labels and automatic predictions. Regarding entity salience, we provide a survey on annotation subjectivity, modeling feasibility, and downstream applications in tasks such as question answering and classical planning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#65292;&#24182;&#36890;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#20102;&#20854;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#24050;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10818</link><description>&lt;p&gt;
&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Democratized Diffusion Language Model. (arXiv:2305.10818v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#65292;&#24182;&#36890;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#20102;&#20854;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#24050;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26377;&#28508;&#22312;&#22909;&#22788;&#65292;&#20294;&#30446;&#21069;&#20844;&#24320;&#30340;&#23454;&#29616;&#12289;&#35757;&#32451;&#27169;&#22411;&#25110;&#21487;&#37325;&#29616;&#30340;&#35757;&#32451;&#31243;&#24207;&#24182;&#19981;&#23384;&#22312;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;CDCD&#26694;&#26550;&#30340;&#27665;&#20027;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65288;DDLM&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;C4&#25968;&#25454;&#38598;&#31616;&#21270;&#30340;DDLM&#35757;&#32451;&#27969;&#31243;&#65292;&#24182;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#36895;&#24230;&#26356;&#24555;&#30340;&#37319;&#26679;&#30340;&#26032;&#22411;&#26089;&#26399;&#36864;&#20986;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#38024;&#23545;&#20351;&#29992;&#24471;&#20998;&#25554;&#20540;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#30001;&#20110;&#27492;&#21069;&#27809;&#26377;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#39044;&#35757;&#32451;&#25193;&#25955;LM&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#31867;&#20219;&#21153;&#65289;&#65292;&#25105;&#20204;&#22312;GLUE&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20197;&#30740;&#31350;DDLM&#30340;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#12290;&#36890;&#36807;&#26412;&#25991;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20379;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#30340;DDLM&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#20197;&#21450;&#39044;&#20808;&#35757;&#32451;&#30340;DDLM&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#22312;&#26410;&#26469;&#30340;D&#30456;&#20851;&#30340;&#30740;&#31350;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the potential benefits of Diffusion Models for NLP applications, publicly available implementations, trained models, or reproducible training procedures currently need to be publicly available. We present the Democratized Diffusion Language Model (DDLM), based on the Continuous Diffusion for Categorical Data (CDCD) framework, to address these challenges. We propose a simplified training procedure for DDLM using the C4 dataset and perform an in-depth analysis of the trained model's behavior. Furthermore, we introduce a novel early-exiting strategy for faster sampling with models trained with score interpolation. Since no previous works aimed at solving downstream tasks with pre-trained Diffusion LM (e.g., classification tasks), we experimented with GLUE Benchmark to study the ability of DDLM to transfer knowledge. With this paper, we propose available training and evaluation pipelines to other researchers and pre-trained DDLM models, which could be used in future research with D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.04928</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#26377;&#30417;&#30563;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20381;&#36182;&#20110;&#20855;&#26377;&#32473;&#23450;&#21629;&#21517;&#23454;&#20307;&#30340;&#22823;&#37327;&#27880;&#37322;&#25991;&#26412;&#65292;&#20854;&#21019;&#24314;&#21487;&#33021;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#25552;&#21462;&#26032;&#23454;&#20307;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#27880;&#37322;&#20219;&#21153;&#21644;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65288;&#26631;&#35760;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#25110;&#19981;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#65289;&#65292;&#24182;&#22312;&#26356;&#22810;&#30340;&#25968;&#25454;&#38598;&#21644;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#21487;&#23398;&#20064;&#21040;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;9&#31181;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#65292;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;NER&#12289;&#19968;&#27425;&#26679;&#26412;NER&#12289;10&#27425;&#26679;&#26412;NER&#21644;100&#27425;&#26679;&#26412;NER&#19978;&#23454;&#29616;&#20102;&#24179;&#22343;F1&#24471;&#20998;&#20998;&#21035;&#20026;35.44&#65285;&#12289;50.10&#65285;&#12289;69.94&#65285;&#21644;79.51&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised named entity recognition (NER) in the biomedical domain is dependent on large sets of annotated texts with the given named entities, whose creation can be time-consuming and expensive. Furthermore, the extraction of new entities often requires conducting additional annotation tasks and retraining the model. To address these challenges, this paper proposes a transformer-based method for zero- and few-shot NER in the biomedical domain. The method is based on transforming the task of multi-class token classification into binary token classification (token contains the searched entity or does not contain the searched entity) and pre-training on a larger amount of datasets and biomedical entities, from where the method can learn semantic relations between the given and potential classes. We have achieved average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical entities with PubMed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992; ChatGPT &#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616; ChatGPT &#22312;&#26494;&#24347;&#21305;&#37197; F1 &#20998;&#25968;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110; GPT-3&#12290;&#34429;&#28982;&#20854;&#24615;&#33021;&#20173;&#20302;&#20110; BioClinicalBERT &#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#20102; ChatGPT &#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#26377;&#24456;&#22823;&#30340;&#20020;&#24202; NER &#20219;&#21153;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16416</link><description>&lt;p&gt;
&#21033;&#29992;ChatGPT&#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Clinical Entity Recognition using ChatGPT. (arXiv:2303.16416v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992; ChatGPT &#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616; ChatGPT &#22312;&#26494;&#24347;&#21305;&#37197; F1 &#20998;&#25968;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110; GPT-3&#12290;&#34429;&#28982;&#20854;&#24615;&#33021;&#20173;&#20302;&#20110; BioClinicalBERT &#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#20102; ChatGPT &#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#26377;&#24456;&#22823;&#30340;&#20020;&#24202; NER &#20219;&#21153;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;OpenAI&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#22312;2010&#24180;i2b2&#25361;&#25112;&#20013;&#25351;&#23450;&#30340;&#20020;&#24202;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#35774;&#32622;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23558;&#20854;&#24615;&#33021;&#19982;GPT-3&#22312;&#31867;&#20284;&#30340;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20197;&#21450;&#20351;&#29992;MTSamples&#30340;&#19968;&#32452;&#21512;&#25104;&#30340;&#20020;&#24202;&#31508;&#35760;&#23545;BioClinicalBERT&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#24494;&#35843;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#31934;&#30830;&#21305;&#37197;&#21644;&#26494;&#24347;&#21305;&#37197;&#30340;F1&#20998;&#21035;&#20026;0.418&#65288;vs.0.250&#65289;&#21644;0.620&#65288;vs.0.480&#65289;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;GPT-3&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#21478;&#22806;&#65292;&#25552;&#31034;&#31574;&#30053;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;ChatGPT&#30340;&#24615;&#33021;&#65292;&#22312;&#20004;&#31181;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#19979;&#26494;&#24347;&#21305;&#37197;&#30340;F1&#20998;&#21035;&#20026;0.628&#21644;0.541&#12290;&#34429;&#28982;ChatGPT&#30340;&#24615;&#33021;&#20173;&#20302;&#20110;&#21463;&#30417;&#30563;&#30340;BioClinicalBERT&#27169;&#22411;&#65288;&#21363;&#26494;&#24347;&#21305;&#37197;F1&#20998;&#25968;&#20998;&#21035;&#20026;0.628&#21644;0.870&#65289;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#20102;ChatGPT&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20020;&#24202;NER&#20219;&#21153;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigated the potential of ChatGPT, a large language model developed by OpenAI, for the clinical named entity recognition task defined in the 2010 i2b2 challenge, in a zero-shot setting with two different prompt strategies. We compared its performance with GPT-3 in a similar zero-shot setting, as well as a fine-tuned BioClinicalBERT model using a set of synthetic clinical notes from MTSamples. Our findings revealed that ChatGPT outperformed GPT-3 in the zero-shot setting, with F1 scores of 0.418 (vs.0.250) and 0.620 (vs. 0.480) for exact- and relaxed-matching, respectively. Moreover, prompts affected ChatGPT's performance greatly, with relaxed-matching F1 scores of 0.628 vs.0.541 for two different prompt strategies. Although ChatGPT's performance was still lower than that of the supervised BioClinicalBERT model (i.e., relaxed-matching F1 scores of 0.628 vs. 0.870), our study demonstrates the great potential of ChatGPT for clinical NER tasks in a zero-shot setting, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#34913;&#37327;&#20225;&#19994;&#25991;&#21270;&#65292;&#36890;&#36807;&#23545;&#21592;&#24037;&#35780;&#20215;&#36827;&#34892;&#20998;&#31867;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#26679;&#26412;&#22806;&#39044;&#27979;&#20013;&#33021;&#25552;&#39640;17&#21040;30&#20010;&#30334;&#20998;&#28857;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.00509</link><description>&lt;p&gt;
CultureBERT&#65306;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#34913;&#37327;&#20225;&#19994;&#25991;&#21270;
&lt;/p&gt;
&lt;p&gt;
CultureBERT: Measuring Corporate Culture With Transformer-Based Language Models. (arXiv:2212.00509v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#34913;&#37327;&#20225;&#19994;&#25991;&#21270;&#65292;&#36890;&#36807;&#23545;&#21592;&#24037;&#35780;&#20215;&#36827;&#34892;&#20998;&#31867;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#26679;&#26412;&#22806;&#39044;&#27979;&#20013;&#33021;&#25552;&#39640;17&#21040;30&#20010;&#30334;&#20998;&#28857;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#21040;&#25991;&#29486;&#20013;&#65292;&#29992;&#20110;&#34913;&#37327;&#20225;&#19994;&#25991;&#21270;&#30340;&#25991;&#26412;&#25991;&#26723;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#21592;&#24037;&#35780;&#20215;&#25968;&#25454;&#38598;&#65292;&#24182;&#30001;&#20154;&#24037;&#35780;&#20272;&#32773;&#23545;&#36825;&#20123;&#35780;&#20215;&#36827;&#34892;&#26631;&#35760;&#65292;&#20197;&#20102;&#35299;&#36825;&#20123;&#35780;&#20215;&#23545;&#20844;&#21496;&#20225;&#19994;&#25991;&#21270;&#30340;&#25581;&#31034;&#31243;&#24230;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25191;&#34892;&#30456;&#21516;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#22312;&#26679;&#26412;&#22806;&#30340;&#39044;&#27979;&#20013;&#65292;&#25105;&#20204;&#30340;&#35821;&#35328;&#27169;&#22411;&#23558;&#21592;&#24037;&#35780;&#20215;&#19982;&#20154;&#24037;&#35780;&#20272;&#32773;&#30340;&#20998;&#31867;&#19968;&#33268;&#24615;&#27604;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#25552;&#39640;&#20102;17&#21040;30&#20010;&#30334;&#20998;&#28857;&#12290;&#25105;&#20204;&#23558;&#27169;&#22411;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces transformer-based language models to the literature measuring corporate culture from text documents. We compile a unique data set of employee reviews that were labeled by human evaluators with respect to the information the reviews reveal about the firms' corporate culture. Using this data set, we fine-tune state-of-the-art transformer-based language models to perform the same classification task. In out-of-sample predictions, our language models classify 17 to 30 percentage points more of employee reviews in line with human evaluators than traditional approaches of text classification. We make our models publicly available.
&lt;/p&gt;</description></item><item><title>CELLS&#26159;&#29992;&#20110;&#26222;&#36890;&#35821;&#35328;&#29983;&#25104;&#30340;&#26368;&#22823;&#26368;&#24191;&#27867;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#29983;&#25104;&#32972;&#26223;&#35299;&#37322;&#21644;&#31616;&#21270;&#21407;&#22987;&#25688;&#35201;&#26469;&#35299;&#20915;&#26222;&#36890;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2211.03818</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26222;&#36890;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#26816;&#32034;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Retrieval augmentation of large language models for lay language generation. (arXiv:2211.03818v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03818
&lt;/p&gt;
&lt;p&gt;
CELLS&#26159;&#29992;&#20110;&#26222;&#36890;&#35821;&#35328;&#29983;&#25104;&#30340;&#26368;&#22823;&#26368;&#24191;&#27867;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#29983;&#25104;&#32972;&#26223;&#35299;&#37322;&#21644;&#31616;&#21270;&#21407;&#22987;&#25688;&#35201;&#26469;&#35299;&#20915;&#26222;&#36890;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#26222;&#36890;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#21033;&#29992;&#22312;&#24179;&#34892;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#22686;&#21152;&#20102;&#20581;&#24247;&#20449;&#24687;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#21487;&#29992;&#35821;&#26009;&#24211;&#30340;&#35268;&#27169;&#21644;&#20027;&#39064;&#24191;&#24230;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;CELLS&#65292;&#36825;&#26159;&#29992;&#20110;&#26222;&#36890;&#35821;&#35328;&#29983;&#25104;&#30340;&#26368;&#22823;&#65288;63k&#23545;&#65289;&#21644;&#26368;&#24191;&#27867;&#28041;&#21450;&#30340;&#65288;12&#20010;&#26399;&#21002;&#65289;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#25688;&#35201;&#21644;&#30456;&#24212;&#30340;&#26222;&#36890;&#35821;&#35328;&#25688;&#35201;&#30001;&#39046;&#22495;&#19987;&#23478;&#25776;&#20889;&#65292;&#30830;&#20445;&#20102;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#19987;&#23478;&#25776;&#20889;&#30340;&#24120;&#35268;&#35821;&#35328;&#25688;&#35201;&#30340;&#23450;&#24615;&#35780;&#20272;&#25581;&#31034;&#20102;&#32972;&#26223;&#35299;&#37322;&#20316;&#20026;&#22686;&#21152;&#21487;&#35775;&#38382;&#24615;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;&#36825;&#31181;&#35299;&#37322;&#23545;&#20110;&#31070;&#32463;&#27169;&#22411;&#30340;&#29983;&#25104;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#20165;&#26159;&#31616;&#21270;&#65292;&#36824;&#28155;&#21152;&#20102;&#28304;&#25968;&#25454;&#20013;&#32570;&#23569;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#20174;CELLS&#20013;&#34893;&#29983;&#20986;&#20004;&#20010;&#19987;&#38376;&#30340;&#37197;&#23545;&#35821;&#26009;&#24211;&#65292;&#20197;&#35299;&#20915;&#26222;&#36890;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#65306;&#29983;&#25104;&#32972;&#26223;&#35299;&#37322;&#21644;&#31616;&#21270;&#21407;&#22987;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent lay language generation systems have used Transformer models trained on a parallel corpus to increase health information accessibility. However, the applicability of these models is constrained by the limited size and topical breadth of available corpora. We introduce CELLS, the largest (63k pairs) and broadest-ranging (12 journals) parallel corpus for lay language generation. The abstract and the corresponding lay language summary are written by domain experts, assuring the quality of our dataset. Furthermore, qualitative evaluation of expert-authored plain language summaries has revealed background explanation as a key strategy to increase accessibility. Such explanation is challenging for neural models to generate because it goes beyond simplification by adding content absent from the source. We derive two specialized paired corpora from CELLS to address key challenges in lay language generation: generating background explanations and simplifying the original abstract. We ado
&lt;/p&gt;</description></item></channel></rss>