<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08774</link><description>&lt;p&gt;
GPT-4&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Technical Report. (arXiv:2303.08774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08774
&lt;/p&gt;
&lt;p&gt;
GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;GPT-4&#30340;&#24320;&#21457;&#65292;&#23427;&#26159;&#19968;&#20010;&#21487;&#20197;&#25509;&#21463;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#19981;&#22914;&#20154;&#31867;&#65292;&#20294;GPT-4&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#65292;&#25104;&#32489;&#25490;&#21517;&#22312;&#21069;10&#65285;&#24038;&#21491;&#12290;GPT-4&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#25991;&#26723;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#21518;&#35757;&#32451;&#23545;&#40784;&#36807;&#31243;&#25552;&#39640;&#20102;&#20107;&#23454;&#24615;&#21644;&#31526;&#21512;&#26399;&#26395;&#34892;&#20026;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;GPT-4&#30340;&#26576;&#20123;&#24615;&#33021;&#26041;&#38754;&#65292;&#32780;&#36825;&#20123;&#24615;&#33021;&#26159;&#22522;&#20110;&#20351;&#29992;&#19981;&#36229;&#36807;GPT-4&#35745;&#31639;&#33021;&#21147;&#30340;1/1,000&#30340;&#27169;&#22411;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;AI&#39537;&#21160;&#30340;&#35828;&#26381;&#26410;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21253;&#25324;&#31227;&#21160;&#35828;&#26381;&#21147;&#24179;&#34913;&#30340;&#26041;&#24335;&#12289;&#23450;&#21046;&#21270;&#30340;&#35828;&#26381;&#12289;&#34394;&#20551;&#20449;&#24687;&#30340;&#24102;&#21160;&#20197;&#21450;&#25913;&#21464;&#20154;&#31867;&#33258;&#36523;&#35328;&#35770;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#35686;&#21578;&#23384;&#22312;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#21628;&#21505;&#21152;&#24378;&#23545;&#20854;&#24320;&#21457;&#21644;&#20351;&#29992;&#30340;&#30417;&#31649;&#12290;</title><link>http://arxiv.org/abs/2303.08721</link><description>&lt;p&gt;
&#20154;&#24037;&#24433;&#21709;: AI&#39537;&#21160;&#30340;&#35828;&#26381;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Artificial Influence: An Analysis Of AI-Driven Persuasion. (arXiv:2303.08721v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;AI&#39537;&#21160;&#30340;&#35828;&#26381;&#26410;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21253;&#25324;&#31227;&#21160;&#35828;&#26381;&#21147;&#24179;&#34913;&#30340;&#26041;&#24335;&#12289;&#23450;&#21046;&#21270;&#30340;&#35828;&#26381;&#12289;&#34394;&#20551;&#20449;&#24687;&#30340;&#24102;&#21160;&#20197;&#21450;&#25913;&#21464;&#20154;&#31867;&#33258;&#36523;&#35328;&#35770;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#35686;&#21578;&#23384;&#22312;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#21628;&#21505;&#21152;&#24378;&#23545;&#20854;&#24320;&#21457;&#21644;&#20351;&#29992;&#30340;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#26381;&#26159;&#20154;&#31867;&#30340;&#37325;&#35201;&#29305;&#24449;&#20043;&#19968;&#65292;&#26159;&#21830;&#19994;&#12289;&#25919;&#27835;&#31561;&#20107;&#19994;&#30340;&#26680;&#24515;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36827;&#27493;&#24050;&#32463;&#20135;&#29983;&#20102;&#33021;&#22815;&#35828;&#26381;&#20154;&#31867;&#36141;&#20080;&#20135;&#21697;&#12289;&#35266;&#30475;&#35270;&#39057;&#12289;&#28857;&#20987;&#25628;&#32034;&#32467;&#26524;&#31561;&#30340;AI&#31995;&#32479;&#12290;&#21363;&#20351;&#27809;&#26377;&#26126;&#30830;&#35774;&#35745;&#20026;&#35828;&#26381;&#30340;&#31995;&#32479;&#65292;&#22312;&#23454;&#36341;&#20013;&#20063;&#21487;&#33021;&#20250;&#36825;&#26679;&#20570;&#12290;&#26410;&#26469;&#65292;&#36234;&#26469;&#36234;&#20855;&#26377;&#20154;&#24418;&#29305;&#24449;&#30340;AI&#31995;&#32479;&#21487;&#33021;&#20250;&#19982;&#29992;&#25143;&#24418;&#25104;&#25345;&#32493;&#30340;&#20851;&#31995;&#65292;&#25552;&#39640;&#23427;&#20204;&#30340;&#35828;&#26381;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;AI&#31995;&#32479;&#30340;&#35828;&#26381;&#33021;&#21147;&#26410;&#26469;&#12290;&#25105;&#20204;&#32771;&#34385;&#21040;AI&#22914;&#20309;&#22312;&#31227;&#21160;&#35828;&#26381;&#21147;&#24179;&#34913;&#30340;&#22522;&#30784;&#19978;&#65292;&#23454;&#29616;&#23450;&#21046;&#21270;&#30340;&#35828;&#26381;&#65292;&#20026;&#34394;&#20551;&#20449;&#24687;&#24102;&#26469;&#21160;&#21147;&#20197;&#21450;&#25913;&#21464;&#20154;&#31867;&#22609;&#36896;&#33258;&#36523;&#35328;&#35770;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#32771;&#34385;AI&#39537;&#21160;&#30340;&#35828;&#26381;&#26041;&#24335;&#21487;&#33021;&#19982;&#20154;&#31867;&#39537;&#21160;&#30340;&#26041;&#24335;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#35686;&#21578;&#35828;&#65292;&#26222;&#36941;&#23384;&#22312;&#39640;&#24230;&#35828;&#26381;&#21147;&#30340;AI&#31995;&#32479;&#21487;&#33021;&#23545;&#20154;&#31867;&#30340;&#33258;&#20027;&#26435;&#21644;&#31119;&#31049;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#21628;&#21505;&#21152;&#24378;&#20851;&#20110;&#20854;&#24320;&#21457;&#21644;&#20351;&#29992;&#30340;&#23545;&#35805;&#21644;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;
Persuasion is a key aspect of what it means to be human, and is central to business, politics, and other endeavors. Advancements in artificial intelligence (AI) have produced AI systems that are capable of persuading humans to buy products, watch videos, click on search results, and more. Even systems that are not explicitly designed to persuade may do so in practice. In the future, increasingly anthropomorphic AI systems may form ongoing relationships with users, increasing their persuasive power. This paper investigates the uncertain future of persuasive AI systems. We examine ways that AI could qualitatively alter our relationship to and views regarding persuasion by shifting the balance of persuasive power, allowing personalized persuasion to be deployed at scale, powering misinformation campaigns, and changing the way humans can shape their own discourse. We consider ways AI-driven persuasion could differ from human-driven persuasion. We warn that ubiquitous highlypersuasive AI sy
&lt;/p&gt;</description></item><item><title>Mirror &#26159;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#30340;&#24179;&#21488;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#21487;&#20197;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026; SQL&#65292;&#24182;&#33258;&#21160;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340; SQL &#21629;&#20196;&#12290;&#29992;&#25143;&#21487;&#20197;&#39044;&#35272;&#21644;&#32534;&#36753; SQL &#20197;&#20445;&#35777;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#65292;&#36824;&#21487;&#20197;&#33719;&#24471;&#25968;&#25454;&#25688;&#35201;&#21644;&#21487;&#35270;&#21270;&#12290;&#23427;&#36866;&#29992;&#20110;&#21508;&#31181;&#31243;&#24230;&#30340;&#25968;&#25454;&#20998;&#26512;&#20154;&#21592;&#21644;&#38750;&#25216;&#26415;&#19987;&#19994;&#20154;&#21592;&#12290;</title><link>http://arxiv.org/abs/2303.08697</link><description>&lt;p&gt;
Mirror: &#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#29992;&#20110;&#25968;&#25454;&#26597;&#35810;&#12289;&#25688;&#35201;&#21644;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Mirror: A Natural Language Interface for Data Querying, Summarization, and Visualization. (arXiv:2303.08697v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08697
&lt;/p&gt;
&lt;p&gt;
Mirror &#26159;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#30340;&#24179;&#21488;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#21487;&#20197;&#23558;&#38382;&#39064;&#36716;&#21270;&#20026; SQL&#65292;&#24182;&#33258;&#21160;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340; SQL &#21629;&#20196;&#12290;&#29992;&#25143;&#21487;&#20197;&#39044;&#35272;&#21644;&#32534;&#36753; SQL &#20197;&#20445;&#35777;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#65292;&#36824;&#21487;&#20197;&#33719;&#24471;&#25968;&#25454;&#25688;&#35201;&#21644;&#21487;&#35270;&#21270;&#12290;&#23427;&#36866;&#29992;&#20110;&#21508;&#31181;&#31243;&#24230;&#30340;&#25968;&#25454;&#20998;&#26512;&#20154;&#21592;&#21644;&#38750;&#25216;&#26415;&#19987;&#19994;&#20154;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; Mirror&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#21160;&#30340;&#25968;&#25454;&#25506;&#32034;&#21644;&#20998;&#26512;&#24320;&#28304;&#24179;&#21488;&#12290;Mirror &#25552;&#20379;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#29992;&#20110;&#26597;&#35810;&#25968;&#25454;&#24211;&#65292;&#24182;&#33258;&#21160;&#29983;&#25104;&#21487;&#25191;&#34892;&#30340; SQL &#21629;&#20196;&#26469;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#24182;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#21487;&#20197;&#39044;&#35272;&#21644;&#25163;&#21160;&#32534;&#36753;&#29983;&#25104;&#30340; SQL &#21629;&#20196;&#65292;&#20197;&#30830;&#20445;&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#12290;Mirror &#36824;&#29983;&#25104;&#21487;&#35270;&#21270;&#22270;&#34920;&#65292;&#20197;&#20415;&#20102;&#35299;&#25968;&#25454;&#24773;&#20917;&#12290;&#35774;&#35745;&#26102;&#32771;&#34385;&#21040;&#28789;&#27963;&#24615;&#21644;&#20154;&#30340;&#36755;&#20837;&#65292;&#22240;&#27492; Mirror &#36866;&#21512;&#20110;&#32463;&#39564;&#20016;&#23500;&#30340;&#25968;&#25454;&#20998;&#26512;&#24072;&#21644;&#38750;&#25216;&#26415;&#19987;&#19994;&#20154;&#21592;&#20174;&#25968;&#25454;&#20013;&#33719;&#21462;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Mirror, an open-source platform for data exploration and analysis powered by large language models. Mirror offers an intuitive natural language interface for querying databases, and automatically generates executable SQL commands to retrieve relevant data and summarize it in natural language. In addition, users can preview and manually edit the generated SQL commands to ensure the accuracy of their queries. Mirror also generates visualizations to facilitate understanding of the data. Designed with flexibility and human input in mind, Mirror is suitable for both experienced data analysts and non-technical professionals looking to gain insights from their data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#33258;&#21160;&#26597;&#35810;&#29983;&#25104;&#65292;&#21363;&#26681;&#25454;&#20107;&#23454;&#38472;&#36848;&#33258;&#21160;&#29983;&#25104;&#25628;&#32034;&#26597;&#35810;&#12290;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;390&#20010;&#20107;&#23454;&#38472;&#36848;&#21644;&#30456;&#20851;&#25628;&#32034;&#26597;&#35810;&#21644;&#32467;&#26524;&#30340;&#20013;&#31561;&#35268;&#27169;&#35777;&#25454;&#25910;&#38598;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.08652</link><description>&lt;p&gt;
&#33258;&#21160;&#26597;&#35810;&#29983;&#25104;&#29992;&#20110;&#20174;&#32593;&#32476;&#25628;&#32034;&#24341;&#25806;&#20013;&#25910;&#38598;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Query Generation for Evidence Collection from Web Search Engines. (arXiv:2303.08652v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#33258;&#21160;&#26597;&#35810;&#29983;&#25104;&#65292;&#21363;&#26681;&#25454;&#20107;&#23454;&#38472;&#36848;&#33258;&#21160;&#29983;&#25104;&#25628;&#32034;&#26597;&#35810;&#12290;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;390&#20010;&#20107;&#23454;&#38472;&#36848;&#21644;&#30456;&#20851;&#25628;&#32034;&#26597;&#35810;&#21644;&#32467;&#26524;&#30340;&#20013;&#31561;&#35268;&#27169;&#35777;&#25454;&#25910;&#38598;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#20114;&#32852;&#32593;&#19978;&#25628;&#32034;&#20449;&#24687;&#26469;&#39564;&#35777;&#25152;&#35859;&#30340;&#20107;&#23454;&#12290;&#36825;&#20010;&#36807;&#31243;&#38656;&#35201;&#20107;&#23454;&#26680;&#26597;&#21592;&#26681;&#25454;&#20107;&#23454;&#21046;&#23450;&#25628;&#32034;&#26597;&#35810;&#24182;&#21521;&#25628;&#32034;&#24341;&#25806;&#25552;&#20132;&#65292;&#28982;&#21518;&#38656;&#35201;&#22312;&#25628;&#32034;&#32467;&#26524;&#20013;&#35782;&#21035;&#30456;&#20851;&#21644;&#21487;&#20449;&#30340;&#27573;&#33853;&#65292;&#28982;&#21518;&#25165;&#33021;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;&#35768;&#22810;&#26032;&#38395;&#21644;&#23186;&#20307;&#32452;&#32455;&#20013;&#65292;&#36825;&#20010;&#36807;&#31243;&#30001;&#21103;&#32534;&#36753;&#27599;&#22825;&#23436;&#25104;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#38382;&#19968;&#20010;&#38382;&#39064;&#65292;&#37027;&#23601;&#26159;&#26159;&#21542;&#21487;&#33021;&#33258;&#21160;&#21270;&#31532;&#19968;&#27493;&#65292;&#21363;&#26597;&#35810;&#29983;&#25104;&#12290;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#26681;&#25454;&#31867;&#20284;&#20110;&#20154;&#31867;&#19987;&#23478;&#21046;&#23450;&#30340;&#20107;&#23454;&#38472;&#36848;&#33258;&#21160;&#29983;&#25104;&#25628;&#32034;&#26597;&#35810;&#65311;&#25105;&#20204;&#32771;&#34385;&#30456;&#20284;&#24615;&#65292;&#26080;&#35770;&#26159;&#20174;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#35282;&#24230;&#36824;&#26159;&#20174;&#25628;&#32034;&#24341;&#25806;&#36820;&#22238;&#30456;&#20851;&#25991;&#26723;&#30340;&#35282;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#20010;&#20013;&#31561;&#35268;&#27169;&#30340;&#35777;&#25454;&#25910;&#38598;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;390&#20010;&#20107;&#23454;&#38472;&#36848;&#20197;&#21450;&#30456;&#20851;&#30340;&#20154;&#24037;&#29983;&#25104;&#30340;&#25628;&#32034;&#26597;&#35810;&#21644;&#25628;&#32034;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is widely accepted that so-called facts can be checked by searching for information on the Internet. This process requires a fact-checker to formulate a search query based on the fact and to present it to a search engine. Then, relevant and believable passages need to be identified in the search results before a decision is made. This process is carried out by sub-editors at many news and media organisations on a daily basis. Here, we ask the question as to whether it is possible to automate the first step, that of query generation. Can we automatically formulate search queries based on factual statements which are similar to those formulated by human experts? Here, we consider similarity both in terms of textual similarity and with respect to relevant documents being returned by a search engine. First, we introduce a moderate-sized evidence collection dataset which includes 390 factual statements together with associated human-generated search queries and search results. Then, we i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;PG-DRR&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#20837;&#39640;&#26031;&#36807;&#31243;&#23618;&#26469;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#26816;&#32034;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#24615;&#33021;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20302;&#30340;&#26657;&#20934;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2303.08606</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;Polya-Gamma&#22686;&#24378;&#23545;&#20110;&#23545;&#35805;&#26816;&#32034;&#27169;&#22411;&#30340;&#26657;&#20934;&#21644;&#19981;&#30830;&#23450;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Calibration and Uncertainty with P\'{o}lya-Gamma Augmentation for Dialog Retrieval Models. (arXiv:2303.08606v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;PG-DRR&#26694;&#26550;&#65292;&#36890;&#36807;&#21152;&#20837;&#39640;&#26031;&#36807;&#31243;&#23618;&#26469;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#26816;&#32034;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#24615;&#33021;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20302;&#30340;&#26657;&#20934;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#26816;&#32034;&#27169;&#22411;&#24050;&#32463;&#20805;&#20998;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#65292;&#20294;&#20272;&#35745;&#23427;&#20204;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22823;&#22810;&#25968;&#23545;&#35805;&#21709;&#24212;&#26816;&#32034;&#27169;&#22411;&#20026;&#27599;&#20010;&#21709;&#24212;&#36755;&#20986;&#19968;&#20010;&#30456;&#20851;&#38382;&#39064;&#30340;&#21333;&#20010;&#24471;&#20998;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#22351;&#26657;&#20934;&#20250;&#23548;&#33268;&#21508;&#31181;&#19981;&#30830;&#23450;&#24615;&#21333;&#20010;&#24471;&#20998;&#65292;&#20174;&#32780;&#19981;&#21487;&#38752;&#30340;&#39044;&#27979;&#24635;&#26159;&#35823;&#23548;&#29992;&#25143;&#20915;&#31574;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26657;&#20934;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26694;&#26550;PG-DRR&#65292;&#29992;&#20110;&#23545;&#35805;&#21709;&#24212;&#26816;&#32034;&#27169;&#22411;&#65292;&#23427;&#23558;&#19968;&#20010;&#39640;&#26031;&#36807;&#31243;&#23618;&#28155;&#21152;&#21040;&#19968;&#20010;&#30830;&#23450;&#24615;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#24182;&#36890;&#36807;Polya-Gamma&#22686;&#24378;&#24674;&#22797;&#20849;&#36717;&#20197;&#20415;&#24471;&#21040;&#26131;&#22788;&#29702;&#30340;&#21518;&#39564;&#25512;&#26029;&#12290;&#26368;&#21518;&#65292;PG-DRR&#22312;&#22495;&#20869;&#25968;&#25454;&#38598;&#21644;&#20998;&#24067;&#20559;&#31227;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20302;&#30340;&#23454;&#35777;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;$R_{10}@1$&#21644;MAP&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural retrieval models have amply demonstrated their power but estimating the reliability of their predictions remains challenging. Most dialog response retrieval models output a single score for a response on how relevant it is to a given question. However, the bad calibration of deep neural network results in various uncertainty for the single score such that the unreliable predictions always misinform user decisions. To investigate these issues, we present an efficient calibration and uncertainty estimation framework PG-DRR for dialog response retrieval models which adds a Gaussian Process layer to a deterministic deep neural network and recovers conjugacy for tractable posterior inference by P\'{o}lya-Gamma augmentation. Finally, PG-DRR achieves the lowest empirical calibration error (ECE) in the in-domain datasets and the distributional shift task while keeping $R_{10}@1$ and MAP performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;GCRE-GPT, &#21487;&#30452;&#25509;&#20174;&#27604;&#36739;&#25991;&#26412;&#20013;&#25552;&#21462;&#20986;&#39640;&#31934;&#24230;&#30340;&#27604;&#36739;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.08601</link><description>&lt;p&gt;
GCRE-GPT: &#19968;&#31181;&#29992;&#20110;&#27604;&#36739;&#20851;&#31995;&#25552;&#21462;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GCRE-GPT: A Generative Model for Comparative Relation Extraction. (arXiv:2303.08601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08601
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;GCRE-GPT, &#21487;&#30452;&#25509;&#20174;&#27604;&#36739;&#25991;&#26412;&#20013;&#25552;&#21462;&#20986;&#39640;&#31934;&#24230;&#30340;&#27604;&#36739;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#27604;&#36739;&#25991;&#26412;&#65292;&#27604;&#36739;&#20851;&#31995;&#25552;&#21462;&#26088;&#22312;&#25552;&#21462;&#20004;&#20010;&#30446;&#26631;&#65288;&#20363;&#22914;&#20004;&#20010;&#30456;&#26426;&#65289;&#30340;&#27604;&#36739;&#21644;&#23427;&#20204;&#34987;&#27604;&#36739;&#30340;&#26041;&#38754;&#65288;&#20363;&#22914;&#22270;&#20687;&#36136;&#37327;&#65289;&#12290;&#25552;&#21462;&#20986;&#30340;&#27604;&#36739;&#20851;&#31995;&#26159;&#36827;&#19968;&#27493;&#24847;&#35265;&#20998;&#26512;&#30340;&#22522;&#30784;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#23558;&#27492;&#20219;&#21153;&#20316;&#20026;&#19968;&#20010;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#65292;&#20197;&#25552;&#21462;&#30446;&#26631;&#21644;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#33021;&#30452;&#25509;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#27604;&#36739;&#20851;&#31995;&#12290;&#26412;&#25991;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#23637;&#31034;&#20986;&#65292;&#21487;&#20197;&#30452;&#25509;&#20197;&#39640;&#31934;&#24230;&#25552;&#21462;&#20986;&#27604;&#36739;&#20851;&#31995;&#12290;&#22522;&#20110;GPT-2&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#27604;&#36739;&#20851;&#31995;&#25552;&#21462;&#22120;&#65288;GCRE-GPT&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given comparative text, comparative relation extraction aims to extract two targets (\eg two cameras) in comparison and the aspect they are compared for (\eg image quality). The extracted comparative relations form the basis of further opinion analysis.Existing solutions formulate this task as a sequence labeling task, to extract targets and aspects. However, they cannot directly extract comparative relation(s) from text. In this paper, we show that comparative relations can be directly extracted with high accuracy, by generative model. Based on GPT-2, we propose a Generation-based Comparative Relation Extractor (GCRE-GPT). Experiment results show that \modelname achieves state-of-the-art accuracy on two datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#26031;&#36807;&#31243;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#26694;&#26550;GPF-BERT&#29992;&#20110;&#22522;&#20110;BERT&#30340;&#23545;&#35805;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#25490;&#21517;&#22120;&#65292;&#30456;&#36739;&#20110;&#22522;&#26412;&#26657;&#20934;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#32463;&#39564;&#26657;&#20934;&#35823;&#24046;&#21644;&#26356;&#39640;&#30340;&#26816;&#32034;&#24615;&#33021;&#65292;&#22312;&#26102;&#38388;&#19978;&#20855;&#26377;8&#20493;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.08599</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#30340;&#39640;&#25928;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#29992;&#20110;&#21487;&#38752;&#23545;&#35805;&#21709;&#24212;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient Uncertainty Estimation with Gaussian Process for Reliable Dialog Response Retrieval. (arXiv:2303.08599v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#26031;&#36807;&#31243;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#26694;&#26550;GPF-BERT&#29992;&#20110;&#22522;&#20110;BERT&#30340;&#23545;&#35805;&#25628;&#32034;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#25490;&#21517;&#22120;&#65292;&#30456;&#36739;&#20110;&#22522;&#26412;&#26657;&#20934;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#32463;&#39564;&#26657;&#20934;&#35823;&#24046;&#21644;&#26356;&#39640;&#30340;&#26816;&#32034;&#24615;&#33021;&#65292;&#22312;&#26102;&#38388;&#19978;&#20855;&#26377;8&#20493;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#24335;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#34987;&#35777;&#26126;&#26159;&#19981;&#33391;&#26657;&#20934;&#30340;&#12290;&#34429;&#28982;&#20687;&#33945;&#29305;&#21345;&#32599;Dropout&#21644;Ensemble&#36825;&#26679;&#30340;&#22522;&#26412;&#26657;&#20934;&#26041;&#27861;&#21487;&#20197;&#24456;&#22909;&#22320;&#26657;&#20934;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#25110;&#25512;&#29702;&#38454;&#27573;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#26694;&#26550;GPF-BERT&#65292;&#29992;&#20110;&#22522;&#20110;BERT&#30340;&#20250;&#35805;&#25628;&#32034;&#65292;&#23427;&#37319;&#29992;&#39640;&#26031;&#36807;&#31243;&#23618;&#21644;&#28966;&#28857;&#25439;&#22833;&#22312;BERT&#26550;&#26500;&#30340;&#39030;&#37096;&#65292;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#25490;&#21517;&#22120;&#12290;&#22823;&#37327;&#23454;&#39564;&#29992;&#20110;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#19982;&#22522;&#26412;&#26657;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;GPF-BERT&#22312;&#19977;&#20010;&#39046;&#22495;&#20869;&#25968;&#25454;&#38598;&#21644;&#20998;&#24067;&#20559;&#31227;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20302;&#30340;&#32463;&#39564;&#26657;&#20934;&#35823;&#24046;&#65288;ECE&#65289;&#65292;&#21516;&#26102;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20135;&#29983;&#20102;&#26368;&#39640;&#30340;$R_{10}@1$&#21644;MAP&#24615;&#33021;&#12290;&#22312;&#26102;&#38388;&#28040;&#32791;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;GPF-BERT&#20855;&#26377;8&#20493;&#30340;&#21152;&#36895;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have achieved remarkable performance in retrieval-based dialogue systems, but they are shown to be ill calibrated. Though basic calibration methods like Monte Carlo Dropout and Ensemble can calibrate well, these methods are time-consuming in the training or inference stages. To tackle these challenges, we propose an efficient uncertainty calibration framework GPF-BERT for BERT-based conversational search, which employs a Gaussian Process layer and the focal loss on top of the BERT architecture to achieve a high-quality neural ranker. Extensive experiments are conducted to verify the effectiveness of our method. In comparison with basic calibration methods, GPF-BERT achieves the lowest empirical calibration error (ECE) in three in-domain datasets and the distributional shift tasks, while yielding the highest $R_{10}@1$ and MAP performance on most cases. In terms of time consumption, our GPF-BERT has an 8$\times$ speedup.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22343;&#21248;&#36890;&#36947;&#27169;&#22411;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21306;&#20998;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#23558;&#26465;&#20214;&#27010;&#29575;&#36136;&#37327;&#20989;&#25968;&#35270;&#20026;&#31163;&#25955;&#26080;&#35760;&#24518;&#36890;&#36947;&#65292;&#24182;&#36873;&#25321;&#26368;&#21487;&#33021;&#30340;&#22240;&#26524;&#26041;&#21521;&#20351;&#24471;&#26465;&#20214;&#27010;&#29575;&#36136;&#37327;&#20989;&#25968;&#26356;&#25509;&#36817;&#20110;&#22343;&#21248;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.08572</link><description>&lt;p&gt;
&#21306;&#20998;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#65306;&#22343;&#21248;&#36890;&#36947;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Distinguishing Cause from Effect on Categorical Data: The Uniform Channel Model. (arXiv:2303.08572v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#22343;&#21248;&#36890;&#36947;&#27169;&#22411;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21306;&#20998;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#35813;&#26041;&#27861;&#23558;&#26465;&#20214;&#27010;&#29575;&#36136;&#37327;&#20989;&#25968;&#35270;&#20026;&#31163;&#25955;&#26080;&#35760;&#24518;&#36890;&#36947;&#65292;&#24182;&#36873;&#25321;&#26368;&#21487;&#33021;&#30340;&#22240;&#26524;&#26041;&#21521;&#20351;&#24471;&#26465;&#20214;&#27010;&#29575;&#36136;&#37327;&#20989;&#25968;&#26356;&#25509;&#36817;&#20110;&#22343;&#21248;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#20998;&#22240;&#26524;&#20851;&#31995;&#26159;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#26680;&#24515;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#29992;&#20110;&#27492;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21363;&#21152;&#24615;&#22122;&#22768;&#27169;&#22411;&#65288;ANM&#65289;&#65292;&#20165;&#36866;&#29992;&#20110;&#23450;&#37327;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#20998;&#31867;&#21464;&#37327;&#65288;&#23646;&#20110;&#27809;&#26377;&#26377;&#24847;&#20041;&#39034;&#24207;&#30340;&#38598;&#21512;&#65289;&#19978;&#35299;&#20915;&#22240;&#26524;-&#25928;&#24212;&#38382;&#39064;&#30340;&#26631;&#20934;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;&#23558;&#26465;&#20214;&#27010;&#29575;&#36136;&#37327;&#20989;&#25968;&#65288;pmf&#65289;&#35270;&#20026;&#31163;&#25955;&#26080;&#35760;&#24518;&#36890;&#36947;&#12290;&#25105;&#20204;&#36873;&#25321;&#26368;&#21487;&#33021;&#30340;&#22240;&#26524;&#26041;&#21521;&#65292;&#20854;&#20013;&#26465;&#20214;pmf&#26356;&#25509;&#36817;&#22343;&#21248;&#36890;&#36947;&#65288;UC&#65289;&#12290;&#21407;&#29702;&#26159;&#65292;&#22312;UC&#20013;&#65292;&#27491;&#22914;&#22312;ANM&#20013;&#19968;&#26679;&#65292;&#65288;&#32473;&#23450;&#22240;&#26524;&#20851;&#31995;&#30340;&#25928;&#24212;&#65289;&#26465;&#20214;&#29109;&#29420;&#31435;&#20110;&#22240;&#26524;&#20998;&#24067;&#65292;&#31526;&#21512;&#22240;&#26524;&#21644;&#26426;&#21046;&#29420;&#31435;&#24615;&#21407;&#21017;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;&#22343;&#21248;&#36890;&#36947;&#27169;&#22411;&#65288;UCM&#65289;&#65292;&#25193;&#23637;&#20102;ANM&#29702;&#35770;&#21040;&#20998;&#31867;&#21464;&#37327;&#12290;&#20026;&#20102;&#35780;&#20272;&#20174;&#25968;&#25454;&#20272;&#35745;&#30340;&#26465;&#20214;pmf&#19982;UC&#30340;&#25509;&#36817;&#31243;&#24230;&#65292;&#25105;&#20204;&#20351;&#29992;s
&lt;/p&gt;
&lt;p&gt;
Distinguishing cause from effect using observations of a pair of random variables is a core problem in causal discovery. Most approaches proposed for this task, namely additive noise models (ANM), are only adequate for quantitative data. We propose a criterion to address the cause-effect problem with categorical variables (living in sets with no meaningful order), inspired by seeing a conditional probability mass function (pmf) as a discrete memoryless channel. We select as the most likely causal direction the one in which the conditional pmf is closer to a uniform channel (UC). The rationale is that, in a UC, as in an ANM, the conditional entropy (of the effect given the cause) is independent of the cause distribution, in agreement with the principle of independence of cause and mechanism. Our approach, which we call the uniform channel model (UCM), thus extends the ANM rationale to categorical variables. To assess how close a conditional pmf (estimated from data) is to a UC, we use s
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#23384;&#22312;&#20110;&#27491;&#21017;&#34920;&#36798;&#24335;&#36827;&#31243;&#35821;&#20041;&#20013;&#30340;&#21452;&#27169;&#25240;&#21472;&#19981;&#23553;&#38381;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;1-free&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#35299;&#37322;&#20013;&#21457;&#29616;&#20102;&#23545;&#36825;&#31181;&#38590;&#39064;&#30340;&#20851;&#38190;&#21407;&#22240;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;LEE&#23646;&#24615;&#30340;&#29305;&#24449;&#65292;&#35777;&#26126;&#20102;1-free&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#26041;&#31243;&#35777;&#26126;&#31995;&#32479;&#26159;&#23436;&#22791;&#30340;&#65292;&#24182;&#19988;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#20197;&#35299;&#20915;&#35299;&#37322;&#21644;&#36807;&#31243;&#22270;&#21452;&#27169;&#30456;&#20284;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.08553</link><description>&lt;p&gt;
&#20851;&#20110;&#27491;&#21017;&#34920;&#36798;&#24335;&#36827;&#31243;&#35821;&#20041;&#30340;&#21452;&#27169;&#25240;&#21472;&#19981;&#23553;&#38381;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Image of the Process Interpretation of Regular Expressions is Not Closed under Bisimulation Collapse. (arXiv:2303.08553v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08553
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#23384;&#22312;&#20110;&#27491;&#21017;&#34920;&#36798;&#24335;&#36827;&#31243;&#35821;&#20041;&#20013;&#30340;&#21452;&#27169;&#25240;&#21472;&#19981;&#23553;&#38381;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;1-free&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#35299;&#37322;&#20013;&#21457;&#29616;&#20102;&#23545;&#36825;&#31181;&#38590;&#39064;&#30340;&#20851;&#38190;&#21407;&#22240;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;LEE&#23646;&#24615;&#30340;&#29305;&#24449;&#65292;&#35777;&#26126;&#20102;1-free&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#26041;&#31243;&#35777;&#26126;&#31995;&#32479;&#26159;&#23436;&#22791;&#30340;&#65292;&#24182;&#19988;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#20197;&#35299;&#20915;&#35299;&#37322;&#21644;&#36807;&#31243;&#22270;&#21452;&#27169;&#30456;&#20284;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Milner&#25552;&#20986;&#30340;&#27491;&#21017;&#34920;&#36798;&#24335;&#36827;&#31243;&#35821;&#20041;&#30340;&#20844;&#29702;&#21270;&#21644;&#34920;&#36798;&#38382;&#39064;&#22312;&#32771;&#34385;&#27515;&#38145;0&#21644;&#31354;&#27493;&#38271;1&#30340;&#23436;&#25972;&#34920;&#36798;&#24335;&#31867;&#20013;&#21464;&#24471;&#22256;&#38590;&#36215;&#26469;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#24403;0&#23384;&#22312;&#26102;&#28155;&#21152;1&#20250;&#20135;&#29983;&#30340;&#29616;&#35937;&#65292;&#36825;&#23558;&#36825;&#20010;&#22256;&#38590;&#30340;&#20851;&#38190;&#21407;&#22240;&#24102;&#21040;&#20102;&#32858;&#28966;&#28857;&#12290;&#21363;&#65292;&#34429;&#28982;1-free&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#35299;&#37322;&#22312;&#21452;&#27169;&#25240;&#21472;&#19979;&#26159;&#23553;&#38381;&#30340;&#65292;&#20294;&#20219;&#24847;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#35299;&#37322;&#19981;&#26159;&#36825;&#31181;&#24773;&#20917;&#12290;1-free&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#36807;&#31243;&#22270;&#35299;&#37322;&#28385;&#36275;&#24490;&#29615;&#23384;&#22312;&#21644;&#28040;&#38500;&#23646;&#24615;LEE&#65292;&#35813;&#23646;&#24615;&#22312;&#21452;&#27169;&#25240;&#21472;&#19979;&#24471;&#20197;&#20445;&#30041;&#12290;LEE&#30340;&#36825;&#20123;&#29305;&#24449;&#34987;&#29992;&#20110;&#35777;&#26126;1-free&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#26041;&#31243;&#35777;&#26126;&#31995;&#32479;&#26159;&#23436;&#22791;&#30340;&#65292;&#24182;&#19988;&#29992;&#20110;&#21028;&#26029;&#19968;&#20010;1-free&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#35299;&#37322;&#26159;&#21542;&#19982;&#19968;&#20010;&#36807;&#31243;&#22270;&#21452;&#27169;&#30456;&#20284;&#26159;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#21487;&#21028;&#23450;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Axiomatization and expressibility problems for Milner's process semantics (1984) of regular expressions modulo bisimilarity have turned out to be difficult for the full class of expressions with deadlock 0 and empty step~1. We report on a phenomenon that arises from the added presence of 1 when 0 is available, and that brings a crucial reason for this difficulty into focus. To wit, while interpretations of 1-free regular expressions are closed under bisimulation collapse, this is not the case for the interpretations of arbitrary regular expressions.  Process graph interpretations of 1-free regular expressions satisfy the loop existence and elimination property LEE, which is preserved under bisimulation collapse. These features of LEE were applied for showing that an equational proof system for 1-free regular expressions modulo bisimilarity is complete, and that it is decidable in polynomial time whether a process graph is bisimilar to the interpretation of a 1-free regular expression. 
&lt;/p&gt;</description></item><item><title>UPRISE&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26816;&#32034;&#22120;&#65292;&#21487;&#33258;&#21160;&#20026;&#32473;&#23450;&#30340;&#38646;&#26679;&#26412;&#20219;&#21153;&#36755;&#20837;&#26816;&#32034;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#35780;&#20272;&#12290;&#23427;&#36890;&#36807;&#36328;&#20219;&#21153;&#21644;&#36328;&#27169;&#22411;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#36890;&#29992;&#24615;&#21644;&#28508;&#21147;&#65292;&#21516;&#26102;&#34920;&#26126;&#20855;&#26377;&#20943;&#36731;&#24187;&#35273;&#38382;&#39064;&#21644;&#25552;&#39640;LLM&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.08518</link><description>&lt;p&gt;
UPRISE: &#36890;&#29992;&#25552;&#31034;&#26816;&#32034;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
UPRISE: Universal Prompt Retrieval for Improving Zero-Shot Evaluation. (arXiv:2303.08518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08518
&lt;/p&gt;
&lt;p&gt;
UPRISE&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26816;&#32034;&#22120;&#65292;&#21487;&#33258;&#21160;&#20026;&#32473;&#23450;&#30340;&#38646;&#26679;&#26412;&#20219;&#21153;&#36755;&#20837;&#26816;&#32034;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#35780;&#20272;&#12290;&#23427;&#36890;&#36807;&#36328;&#20219;&#21153;&#21644;&#36328;&#27169;&#22411;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#20854;&#36890;&#29992;&#24615;&#21644;&#28508;&#21147;&#65292;&#21516;&#26102;&#34920;&#26126;&#20855;&#26377;&#20943;&#36731;&#24187;&#35273;&#38382;&#39064;&#21644;&#25552;&#39640;LLM&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#32780;&#21463;&#27426;&#36814;&#65292;&#20294;&#38656;&#35201;&#29305;&#23450;&#27169;&#22411;&#30340;&#24494;&#35843;&#25110;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#24037;&#31243;&#21487;&#33021;&#20250;&#38459;&#30861;&#20854;&#19968;&#33324;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;UPRISE&#65288;&#36890;&#29992;&#25552;&#31034;&#26816;&#32034;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#35780;&#20272;&#65289;&#65292;&#35813;&#26041;&#27861;&#35843;&#25972;&#20102;&#36731;&#37327;&#32423;&#21644;&#22810;&#21151;&#33021;&#30340;&#26816;&#32034;&#22120;&#65292;&#20197;&#33258;&#21160;&#26816;&#32034;&#32473;&#23450;&#38646;&#26679;&#26412;&#20219;&#21153;&#36755;&#20837;&#30340;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36328;&#20219;&#21153;&#21644;&#36328;&#27169;&#22411;&#26041;&#26696;&#20013;&#23637;&#31034;&#20102;&#36890;&#29992;&#24615;&#65306;&#26816;&#32034;&#22120;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#22312;&#30475;&#19981;&#35265;&#30340;&#20219;&#21153;&#31867;&#22411;&#19978;&#36827;&#34892;&#27979;&#35797;&#65307;&#25105;&#20204;&#22312;&#19968;&#20010;&#23567;&#22411;&#20923;&#32467;LLM&#8212;&#8212;GPT-Neo-2.7B&#19978;&#35843;&#25972;&#26816;&#32034;&#22120;&#65292;&#20294;&#22312;&#19981;&#21516;&#35268;&#27169;&#30340;LLM&#19978;&#27979;&#35797;&#26816;&#32034;&#22120;&#65292;&#20363;&#22914;BLOOM-7.1B&#12289;OPT-66B&#21644;GPT3-175B&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;UPRISE&#22312;&#25105;&#20204;&#19982;ChatGPT&#30340;&#23454;&#39564;&#20013;&#20943;&#36731;&#20102;&#24187;&#35273;&#38382;&#39064;&#65292;&#34920;&#26126;&#23427;&#26377;&#28508;&#21147;&#25913;&#36827;&#29978;&#33267;&#26159;&#26368;&#24378;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are popular for their impressive abilities, but the need for model-specific fine-tuning or task-specific prompt engineering can hinder their generalization. We propose UPRISE (Universal Prompt Retrieval for Improving zero-Shot Evaluation), which tunes a lightweight and versatile retriever that automatically retrieves prompts for a given zero-shot task input. Specifically, we demonstrate universality in a cross-task and cross-model scenario: the retriever is tuned on a diverse set of tasks, but tested on unseen task types; we use a small frozen LLM, GPT-Neo-2.7B, for tuning the retriever, but test the retriever on different LLMs of much larger scales, such as BLOOM-7.1B, OPT-66B and GPT3-175B. Additionally, we show that UPRISE mitigates the hallucination problem in our experiments with ChatGPT, suggesting its potential to improve even the strongest LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20083;&#33146;&#30284;&#34920;&#22411;&#25552;&#21462;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;BERT&#30340;&#20020;&#24202;NLP&#27169;&#22411;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#24320;&#21457;&#24191;&#20041;&#20020;&#24202;NLP&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.08448</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#30005;&#23376;&#30149;&#21382;&#30340;&#20083;&#33146;&#30284;&#34920;&#22411;NLP&#31639;&#27861;&#36827;&#34892;&#36328;&#26426;&#26500;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Cross-institutional Evaluation on Breast Cancer Phenotyping NLP Algorithms on Electronic Health Records. (arXiv:2303.08448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20083;&#33146;&#30284;&#34920;&#22411;&#25552;&#21462;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;BERT&#30340;&#20020;&#24202;NLP&#27169;&#22411;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#24320;&#21457;&#24191;&#20041;&#20020;&#24202;NLP&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#22312;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#65292;&#36890;&#24120;&#24573;&#30053;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20083;&#33146;&#30284;&#34920;&#22411;&#25552;&#21462;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;&#22522;&#20110;BERT&#30340;&#20020;&#24202;NLP&#27169;&#22411;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26041;&#27861;&#65306;&#20174;&#26126;&#23612;&#33487;&#36798;&#22823;&#23398;&#21644;&#26757;&#22885;&#35786;&#25152;&#30340;&#30005;&#23376;&#30149;&#21382;&#20013;&#25910;&#38598;&#20102;&#20004;&#31181;&#20083;&#33146;&#30284;&#24739;&#32773;&#30340;&#20020;&#24202;&#35821;&#26009;&#24211;&#65292;&#24182;&#25353;&#29031;&#21516;&#19968;&#25351;&#21335;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;NLP&#27169;&#22411;&#65288;&#26465;&#20214;&#38543;&#26426;&#22330;&#12289;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;CancerBERT&#65289;&#65292;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#30284;&#30151;&#34920;&#22411;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#23398;&#20064;&#31574;&#30053;&#65288;&#27169;&#22411;&#36716;&#31227;&#19982;&#26412;&#22320;&#35757;&#32451;&#65289;&#23545;&#27169;&#22411;&#22312;&#19981;&#21516;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#27867;&#21270;&#33021;&#21147;&#35780;&#20272;&#12290;&#35780;&#20272;&#23454;&#20307;&#35206;&#30422;&#29575;&#19982;&#27169;&#22411;&#24615;&#33021;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#12290;&#32467;&#26524;&#65306;&#22312;UMN&#21644;MC&#25163;&#21160;&#27880;&#37322;&#20102;200&#21644;161&#20221;&#20020;&#24202;&#25991;&#26723;&#12290;CancerBERT&#27169;&#22411;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;F1&#20998;&#25968;&#65288;0.896&#65289;&#21644;&#23454;&#20307;&#35206;&#30422;&#29575;&#65288;98.8%&#65289;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#27169;&#22411;&#36716;&#31227;&#26041;&#27861;&#22312;&#20004;&#20010;&#26426;&#26500;&#20013;&#20135;&#29983;&#20102;&#31867;&#20284;&#20110;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#34920;&#26126;&#36328;&#26426;&#26500;&#23384;&#22312;&#28508;&#22312;&#30340;&#27867;&#21270;&#24615;&#12290;&#32467;&#35770;&#65306;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#20013;&#35780;&#20272;NLP&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#24320;&#21457;&#24191;&#20041;&#20020;&#24202;NLP&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: The generalizability of clinical large language models is usually ignored during the model development process. This study evaluated the generalizability of BERT-based clinical NLP models across different clinical settings through a breast cancer phenotype extraction task.  Materials and Methods: Two clinical corpora of breast cancer patients were collected from the electronic health records from the University of Minnesota and the Mayo Clinic, and annotated following the same guideline. We developed three types of NLP models (i.e., conditional random field, bi-directional long short-term memory and CancerBERT) to extract cancer phenotypes from clinical texts. The models were evaluated for their generalizability on different test sets with different learning strategies (model transfer vs. locally trained). The entity coverage score was assessed with their association with the model performances.  Results: We manually annotated 200 and 161 clinical documents at UMN and MC, re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#35821;&#35328;&#22270;&#20687;&#23383;&#24149;&#30340;&#25200;&#21160;&#40065;&#26834;&#24230;&#37327;&#26041;&#27861;&#65288;PR-MCS&#65289;&#65292;&#36890;&#36807;&#23545;CLIP&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#23545;&#25200;&#21160;&#25991;&#26412;&#30340;&#21306;&#20998;&#65292;&#24182;&#22312;&#26032;&#30340;&#32454;&#31890;&#24230;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#27604;&#22522;&#32447;&#25351;&#26631;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#20854;&#36866;&#24212;&#22810;&#31181;&#35821;&#35328;&#30340;&#22270;&#20687;&#23383;&#24149;&#35780;&#20272;&#25351;&#26631;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08389</link><description>&lt;p&gt;
PR-MCS: &#38754;&#21521;&#22810;&#35821;&#35328;&#22270;&#20687;&#23383;&#24149;&#30340;&#25200;&#21160;&#40065;&#26834;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PR-MCS: Perturbation Robust Metric for MultiLingual Image Captioning. (arXiv:2303.08389v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#22810;&#35821;&#35328;&#22270;&#20687;&#23383;&#24149;&#30340;&#25200;&#21160;&#40065;&#26834;&#24230;&#37327;&#26041;&#27861;&#65288;PR-MCS&#65289;&#65292;&#36890;&#36807;&#23545;CLIP&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20102;&#23545;&#25200;&#21160;&#25991;&#26412;&#30340;&#21306;&#20998;&#65292;&#24182;&#22312;&#26032;&#30340;&#32454;&#31890;&#24230;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#27604;&#22522;&#32447;&#25351;&#26631;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#35777;&#26126;&#20102;&#20854;&#36866;&#24212;&#22810;&#31181;&#35821;&#35328;&#30340;&#22270;&#20687;&#23383;&#24149;&#35780;&#20272;&#25351;&#26631;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#22270;&#20687;&#23383;&#24149;&#30340;&#25351;&#26631;&#23545;&#20110;&#35789;&#27719;&#25200;&#21160;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#24369;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Perturbation Robust Multi-Lingual CLIPScore&#65288;PR-MCS&#65289;&#65292;&#23427;&#33021;&#22815;&#34920;&#29616;&#20986;&#23545;&#36825;&#31181;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#26159;&#19968;&#31181;&#26032;&#30340;&#19981;&#38656;&#35201;&#21442;&#29031;&#29289;&#30340;&#36866;&#29992;&#20110;&#22810;&#31181;&#35821;&#35328;&#30340;&#22270;&#20687;&#23383;&#24149;&#35780;&#20272;&#25351;&#26631;&#12290;&#20026;&#20102;&#23454;&#29616;&#25200;&#21160;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#35821;&#35328;&#26080;&#20851;&#26041;&#27861;&#23545;CLIP&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21306;&#20998;&#25200;&#21160;&#25991;&#26412;&#21644;&#21407;&#22987;&#25991;&#26412;&#12290;&#20026;&#20102;&#39564;&#35777;PR-MCS&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#32454;&#31890;&#24230;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20116;&#31181;&#35821;&#35328;&#20013; 3,000 &#20010;&#22270;&#20687;&#30340;&#35814;&#32454;&#23383;&#24149;&#65292;&#20851;&#38190;&#23545;&#35937;&#21644;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;PR-MCS &#22312;&#25429;&#33719;&#25152;&#26377;&#19981;&#21516;&#25200;&#21160;&#31867;&#22411;&#30340;&#35789;&#27719;&#22122;&#22768;&#26041;&#38754;&#26174;&#30528;&#20248;&#20110;&#22522;&#32447;&#25351;&#26631;&#65292;&#35777;&#26126;&#20102; PR-MCS &#23545;&#20110;&#35789;&#27719;&#25200;&#21160;&#20855;&#26377;&#39640;&#24230;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vulnerability to lexical perturbation is a critical weakness of automatic evaluation metrics for image captioning. This paper proposes Perturbation Robust Multi-Lingual CLIPScore(PR-MCS), which exhibits robustness to such perturbations, as a novel reference-free image captioning metric applicable to multiple languages. To achieve perturbation robustness, we fine-tune the text encoder of CLIP with our language-agnostic method to distinguish the perturbed text from the original text. To verify the robustness of PR-MCS, we introduce a new fine-grained evaluation dataset consisting of detailed captions, critical objects, and the relationships between the objects for 3, 000 images in five languages. In our experiments, PR-MCS significantly outperforms baseline metrics in capturing lexical noise of all various perturbation types in all five languages, proving that PR-MCS is highly robust to lexical perturbations.
&lt;/p&gt;</description></item><item><title>FactReranker&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#35780;&#20272;&#22120;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#25688;&#35201;&#19982;&#25918;&#23556;&#23398;&#21457;&#29616;&#23454;&#20917;&#19968;&#33268;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#20107;&#23454;&#24341;&#23548;&#26469;&#26377;&#25928;&#22320;&#36873;&#25321;&#26368;&#20339;&#30340;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2303.08335</link><description>&lt;p&gt;
FactReranker&#65306;&#22522;&#20110;&#20107;&#23454;&#24341;&#23548;&#30340;&#36741;&#21161;&#35780;&#20272;&#22120;&#29992;&#20110;&#24544;&#23454;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
FactReranker: Fact-guided Reranker for Faithful Radiology Report Summarization. (arXiv:2303.08335v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08335
&lt;/p&gt;
&lt;p&gt;
FactReranker&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#35780;&#20272;&#22120;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#25688;&#35201;&#19982;&#25918;&#23556;&#23398;&#21457;&#29616;&#23454;&#20917;&#19968;&#33268;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#20107;&#23454;&#24341;&#23548;&#26469;&#26377;&#25928;&#22320;&#36873;&#25321;&#26368;&#20339;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#20854;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#20445;&#25345;&#25152;&#20135;&#29983;&#30340;&#25688;&#35201;&#21644;&#22320;&#38754;&#23454;&#20917;&#25918;&#23556;&#23398;&#21457;&#29616;&#20043;&#38388;&#30340;&#23454;&#38469;&#20934;&#30830;&#24615;&#12290;&#29616;&#26377;&#30740;&#31350;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#30452;&#25509;&#20248;&#21270;&#27491;&#30830;&#35748;&#30693;&#24230;&#37327;&#25351;&#26631;&#65292;&#22914;CheXBert&#25110;RadGraph&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20351;&#29992;&#36138;&#23146;&#25628;&#32034;&#25110;&#26463;&#25628;&#32034;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#22312;&#36873;&#25321;&#26368;&#20339;&#20505;&#36873;&#39033;&#26102;&#27809;&#26377;&#32771;&#34385;&#20107;&#23454;&#30340;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#23548;&#33268;&#23454;&#38469;&#19968;&#33268;&#24615;&#30340;&#25913;&#21892;&#21463;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31532;&#20108;&#38454;&#27573;&#25688;&#35201;&#26041;&#27861;FactReranker&#65292;&#23427;&#26159;&#31532;&#19968;&#27425;&#23581;&#35797;&#22522;&#20110;&#23427;&#20204;&#20272;&#35745;&#30340;&#23454;&#38469;&#19968;&#33268;&#24615;&#24471;&#20998;&#26469;&#23398;&#20064;&#20174;&#25152;&#26377;&#20505;&#36873;&#39033;&#20013;&#36873;&#25321;&#26368;&#20339;&#25688;&#35201;&#12290;&#25105;&#20204;&#24314;&#35758;&#22522;&#20110;RadGraph&#27169;&#24335;&#25552;&#21462;&#36755;&#20837;&#21307;&#30103;&#25253;&#21578;&#12289;&#20854;&#40644;&#37329;&#25688;&#35201;&#21644;&#20505;&#36873;&#25688;&#35201;&#30340;&#21307;&#30103;&#20107;&#23454;&#65292;&#24182;&#35774;&#35745;&#22522;&#20110;&#20107;&#23454;&#24341;&#23548;&#30340;&#37325;&#26032;&#25490;&#24207;&#22120;&#65292;&#20197;&#26377;&#25928;&#22320;&#32467;&#21512;&#25552;&#21462;&#30340;&#21307;&#30103;&#20107;&#23454;&#26469;&#36873;&#25321;&#26368;&#20339;&#25688;&#35201;&#12290;&#25105;&#20204;&#20998;&#35299;&#20102;&#20107;&#23454;-
&lt;/p&gt;
&lt;p&gt;
Automatic radiology report summarization is a crucial clinical task, whose key challenge is to maintain factual accuracy between produced summaries and ground truth radiology findings. Existing research adopts reinforcement learning to directly optimize factual consistency metrics such as CheXBert or RadGraph score. However, their decoding method using greedy search or beam search considers no factual consistency when picking the optimal candidate, leading to limited factual consistency improvement. To address it, we propose a novel second-stage summarizing approach FactReranker, the first attempt that learns to choose the best summary from all candidates based on their estimated factual consistency score. We propose to extract medical facts of the input medical report, its gold summary, and candidate summaries based on the RadGraph schema and design the fact-guided reranker to efficiently incorporate the extracted medical facts for selecting the optimal summary. We decompose the fact-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25805;&#32437;&#35821;&#38899;&#39118;&#26684;&#38544;&#21464;&#37327;&#36827;&#34892;&#36328;&#35828;&#35805;&#20154;&#24773;&#24863;&#36716;&#31227;&#21644;&#25805;&#32437;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#20174;&#38405;&#35835;&#39118;&#26684;&#30340;&#35821;&#38899;&#29983;&#25104;&#24773;&#24863;&#35821;&#38899;&#65292;&#19988;&#24773;&#24863;&#24378;&#24230;&#21487;&#36890;&#36807;&#26631;&#37327;&#20540;&#36827;&#34892;&#21487;&#25511;&#65292;&#20445;&#30041;&#35828;&#35805;&#20154;&#36523;&#20221;&#12290;</title><link>http://arxiv.org/abs/2303.08329</link><description>&lt;p&gt;
&#36890;&#36807;&#25805;&#32437;&#35821;&#38899;&#39118;&#26684;&#38544;&#21464;&#37327;&#36827;&#34892;&#36328;&#35828;&#35805;&#20154;&#24773;&#24863;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Cross-speaker Emotion Transfer by Manipulating Speech Style Latents. (arXiv:2303.08329v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25805;&#32437;&#35821;&#38899;&#39118;&#26684;&#38544;&#21464;&#37327;&#36827;&#34892;&#36328;&#35828;&#35805;&#20154;&#24773;&#24863;&#36716;&#31227;&#21644;&#25805;&#32437;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#20174;&#38405;&#35835;&#39118;&#26684;&#30340;&#35821;&#38899;&#29983;&#25104;&#24773;&#24863;&#35821;&#38899;&#65292;&#19988;&#24773;&#24863;&#24378;&#24230;&#21487;&#36890;&#36807;&#26631;&#37327;&#20540;&#36827;&#34892;&#21487;&#25511;&#65292;&#20445;&#30041;&#35828;&#35805;&#20154;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24773;&#32490;&#25991;&#26412;&#36716;&#35821;&#38899;&#24050;&#32463;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#24182;&#19981;&#23481;&#26131;&#33719;&#21462;&#12290;&#21363;&#20351;&#26377;&#21487;&#33021;&#33719;&#24471;&#24773;&#24863;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#20173;&#28982;&#23384;&#22312;&#25511;&#21046;&#24773;&#24863;&#24378;&#24230;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25805;&#32437;&#28508;&#22312;&#30340;&#35821;&#38899;&#39118;&#26684;&#31354;&#38388;&#20013;&#30340;&#21521;&#37327;&#31639;&#26415;&#65292;&#23454;&#29616;&#36328;&#35828;&#35805;&#20154;&#24773;&#24863;&#36716;&#31227;&#21644;&#25805;&#32437;&#12290;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65292;&#21487;&#20197;&#20174;&#38405;&#35835;&#39118;&#26684;&#30340;&#35821;&#38899;&#29983;&#25104;&#24773;&#24863;&#35821;&#38899;&#65292;&#21516;&#26102;&#20445;&#30041;&#35828;&#35805;&#20154;&#36523;&#20221;&#12290;&#27492;&#22806;&#65292;&#24773;&#24863;&#24378;&#24230;&#21487;&#20197;&#36890;&#36807;&#26631;&#37327;&#20540;&#36827;&#34892;&#21487;&#25511;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#25805;&#20316;&#35821;&#38899;&#30340;&#26041;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#34920;&#29616;&#21147;&#12289;&#33258;&#28982;&#24230;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#37117;&#26377;&#30528;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20445;&#30041;&#35828;&#35805;&#20154;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, emotional text-to-speech has shown considerable progress. However, it requires a large amount of labeled data, which is not easily accessible. Even if it is possible to acquire an emotional speech dataset, there is still a limitation in controlling emotion intensity. In this work, we propose a novel method for cross-speaker emotion transfer and manipulation using vector arithmetic in latent style space. By leveraging only a few labeled samples, we generate emotional speech from reading-style speech without losing the speaker identity. Furthermore, emotion strength is readily controllable using a scalar value, providing an intuitive way for users to manipulate speech. Experimental results show the proposed method affords superior performance in terms of expressiveness, naturalness, and controllability, preserving speaker identity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#37327;&#21270;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#32467;&#26524;&#21457;&#29616;&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#24456;&#37325;&#35201;&#65292;&#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2303.08302</link><description>&lt;p&gt;
&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#21512;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study on Post-Training Quantization for Large Language Models. (arXiv:2303.08302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#22522;&#20110;&#21518;&#35757;&#32451;&#37327;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#21516;&#37327;&#21270;&#32452;&#20214;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#65292;&#32467;&#26524;&#21457;&#29616;&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#24456;&#37325;&#35201;&#65292;&#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#26159;&#19968;&#31181;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#23384;&#28040;&#32791;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#30340;&#26435;&#34913;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#19981;&#21516;&#37327;&#21270;&#26041;&#26696;&#12289;&#19981;&#21516;&#27169;&#22411;&#26063;&#12289;&#19981;&#21516;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12289;&#19981;&#21516;&#37327;&#21270;&#20301;&#31934;&#24230;&#31561;&#30340;&#24433;&#21709;&#30340;&#20840;&#38754;&#30740;&#31350;&#20173;&#32570;&#22833;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#19975;&#20010;&#38646;-shot&#23454;&#39564;&#23545;&#36825;&#20123;&#32452;&#20214;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;(1)&#32454;&#31890;&#24230;&#37327;&#21270;&#21644;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;(&#32780;&#19981;&#26159;&#26420;&#32032;&#30340;&#26368;&#36817;&#33293;&#20837;&#37327;&#21270;)&#26159;&#23454;&#29616;&#33391;&#22909;&#31934;&#24230;&#30340;&#24517;&#35201;&#26465;&#20214;&#65307;(2) &#29992;&#31895;&#31890;&#24230;&#37327;&#21270;&#30340;&#26356;&#39640;&#20301;&#25968;&#65288;&#22914;5&#20301;&#65289;&#27604;&#29992;&#38750;&#24120;&#32454;&#31890;&#24230;&#30340;&#26356;&#20302;&#20301;&#25968;&#65288;&#22914;4&#20301;&#65289;&#65288;&#20854;&#26377;&#25928;&#20301;&#25968;&#19982;5&#20301;&#30456;&#20284;&#65289;&#26356;&#24378;&#22823;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22914;&#20309;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;\llms&#21033;&#29992;&#37327;&#21270;&#30340;&#24314;&#35758;&#65292;&#24182;&#30041;&#19979;&#26410;&#26469;&#26426;&#20250;&#21644;&#31995;&#32479;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (\ptq) had been recently shown as a compromising method to reduce the memory consumption and/or compute cost for large language models. However, a comprehensive study about the effect of different quantization schemes, different model families, different \ptq methods, different quantization bit precision, etc, is still missing. In this work, we provide an extensive study on those components over tens of thousands of zero-shot experiments. Our results show that (1) Fine-grained quantization and \ptq methods (instead of naive round-to-nearest quantization) are necessary to achieve good accuracy and (2) Higher bits (e.g., 5 bits) with coarse-grained quantization is more powerful than lower bits (e.g., 4 bits) with very fine-grained quantization (whose effective bits is similar to 5-bits). We also present recommendations about how to utilize quantization for \llms with different sizes, and leave suggestions of future opportunities and system work that are not res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#65292;CNN&#22312;&#20581;&#24247;&#35760;&#24405;&#25991;&#26412;&#32534;&#30721;&#26041;&#38754;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#38544;&#21547;&#23618;&#27425;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#32534;&#30721;&#22120;&#26469;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;EHR&#29305;&#24449;&#65292;&#24182;&#22312;&#20020;&#24202;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08290</link><description>&lt;p&gt;
&#37325;&#26032;&#21457;&#29616;CNN&#22312;&#21407;&#22987;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25991;&#26412;&#32534;&#30721;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rediscovery of CNN's Versatility for Text-based Encoding of Raw Electronic Health Records. (arXiv:2303.08290v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#65292;CNN&#22312;&#20581;&#24247;&#35760;&#24405;&#25991;&#26412;&#32534;&#30721;&#26041;&#38754;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#38544;&#21547;&#23618;&#27425;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#32534;&#30721;&#22120;&#26469;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;EHR&#29305;&#24449;&#65292;&#24182;&#22312;&#20020;&#24202;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20805;&#20998;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#20016;&#23500;&#30340;&#20449;&#24687;&#27491;&#36880;&#28176;&#25104;&#20026;&#21307;&#23398;&#39046;&#22495;&#30340;&#37325;&#35201;&#35805;&#39064;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#20854;&#26684;&#24335;&#21644;&#21307;&#23398;&#32534;&#30721;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#23884;&#20837;&#21407;&#22987;EHR&#25968;&#25454;&#30340;&#25972;&#20010;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#35813;&#26694;&#26550;&#20165;&#20391;&#37325;&#20110;&#23545;EHR&#36827;&#34892;&#26368;&#23567;&#30340;&#39044;&#22788;&#29702;&#65292;&#26410;&#32771;&#34385;&#22914;&#20309;&#23398;&#20064;&#39640;&#25928;&#30340;EHR&#34920;&#31034;&#65292;&#21253;&#25324;&#35745;&#31639;&#21644;&#20869;&#23384;&#20351;&#29992;&#31561;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23547;&#25214;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#32534;&#30721;&#22120;&#65292;&#19981;&#20165;&#23558;&#22823;&#37327;&#25968;&#25454;&#32553;&#23567;&#21040;&#21487;&#31649;&#29702;&#30340;&#22823;&#23567;&#65292;&#36824;&#33021;&#24456;&#22909;&#22320;&#20445;&#30041;&#24739;&#32773;&#30340;&#26680;&#24515;&#20449;&#24687;&#65292;&#20197;&#25191;&#34892;&#21508;&#31181;&#20020;&#24202;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#20998;&#23618;&#32467;&#26500;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#65288;&#22914;&#37325;&#24314;&#65292;&#39044;&#27979;&#21644;&#29983;&#25104;&#65289;&#20013;&#32463;&#24120;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21363;&#20351;&#21442;&#25968;&#36739;&#23569;&#19988;&#35757;&#32451;&#26102;&#38388;&#36739;&#30701;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;EHR&#25968;&#25454;&#30340;&#22266;&#26377;&#23618;&#27425;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;CNN&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#20123;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CNN&#30340;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;EHR&#29305;&#24449;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20960;&#31181;&#20020;&#24202;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Making the most use of abundant information in electronic health records (EHR) is rapidly becoming an important topic in the medical domain. Recent work presented a promising framework that embeds entire features in raw EHR data regardless of its form and medical code standards. The framework, however, only focuses on encoding EHR with minimal preprocessing and fails to consider how to learn efficient EHR representation in terms of computation and memory usage. In this paper, we search for a versatile encoder not only reducing the large data into a manageable size but also well preserving the core information of patients to perform diverse clinical tasks. We found that hierarchically structured Convolutional Neural Network (CNN) often outperforms the state-of-the-art model on diverse tasks such as reconstruction, prediction, and generation, even with fewer parameters and less training time. Moreover, it turns out that making use of the inherent hierarchy of EHR data can boost the perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;Transformer&#20013;&#26631;&#35760;&#21487;&#33021;&#24615;&#21644;&#27880;&#24847;&#21147;&#20540;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#25581;&#31034;&#20102;&#22312;&#36935;&#21040;&#24847;&#22806;&#26631;&#35760;&#26102;&#27169;&#22411;&#20851;&#27880;&#36739;&#23569;&#30340;&#20449;&#24687;&#65292;&#23545;&#20110;&#35780;&#20272;LLMs&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#20855;&#26377;&#26377;&#20215;&#20540;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.08288</link><description>&lt;p&gt;
Transformer&#20013;&#30340;&#27880;&#24847;&#21147;-&#21487;&#33021;&#24615;&#20851;&#31995;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Attention-likelihood relationship in transformers. (arXiv:2303.08288v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;Transformer&#20013;&#26631;&#35760;&#21487;&#33021;&#24615;&#21644;&#27880;&#24847;&#21147;&#20540;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#25581;&#31034;&#20102;&#22312;&#36935;&#21040;&#24847;&#22806;&#26631;&#35760;&#26102;&#27169;&#22411;&#20851;&#27880;&#36739;&#23569;&#30340;&#20449;&#24687;&#65292;&#23545;&#20110;&#35780;&#20272;LLMs&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#20855;&#26377;&#26377;&#20215;&#20540;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#34920;&#31034;&#19978;&#19979;&#25991;&#20043;&#22806;&#30340;&#21333;&#35789;&#65292;&#24182;&#35843;&#26597;&#23427;&#20204;&#23545;&#32473;&#23450;&#19978;&#19979;&#25991;&#26469;&#25429;&#25417;&#35821;&#20041;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#30340;&#21487;&#33021;&#24615;&#24341;&#23548;&#30340;&#25991;&#26412;&#25200;&#21160;&#25581;&#31034;&#20102;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#26631;&#35760;&#21487;&#33021;&#24615;&#21644;&#27880;&#24847;&#21147;&#20540;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;&#26356;&#39640;&#23618;&#29305;&#21035;&#26159;&#36935;&#21040;&#24847;&#22806;&#30340;&#26631;&#35760;&#26102;&#65292;&#27169;&#22411;&#20250;&#20851;&#27880;&#36739;&#23569;&#30340;&#26469;&#33258;&#33258;&#36523;&#30340;&#20449;&#24687;&#26469;&#35745;&#31639;&#23427;&#20204;&#30340;&#34920;&#31034;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#35780;&#20272;LLMs&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#31283;&#20581;&#24615;&#20855;&#26377;&#26377;&#20215;&#20540;&#30340;&#24433;&#21709;&#12290;&#22312;https://github.com/Flegyas/AttentionLikelihood&#20013;&#26377;&#23436;&#20840;&#21487;&#37325;&#29616;&#30340;&#20195;&#30721;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyze how large language models (LLMs) represent out-of-context words, investigating their reliance on the given context to capture their semantics. Our likelihood-guided text perturbations reveal a correlation between token likelihood and attention values in transformer-based language models. Extensive experiments reveal that unexpected tokens cause the model to attend less to the information coming from themselves to compute their representations, particularly at higher layers. These findings have valuable implications for assessing the robustness of LLMs in real-world scenarios. Fully reproducible codebase at https://github.com/Flegyas/AttentionLikelihood.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#32467;&#26500;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#20020;&#24202;&#27010;&#24565;&#25552;&#21462;&#21644;&#20851;&#31995;&#25552;&#21462;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#36328;&#26426;&#26500;&#24212;&#29992;&#30340;&#33391;&#22909;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08262</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#20020;&#24202;&#27010;&#24565;&#21450;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Clinical Concept and Relation Extraction Using Prompt-based Machine Reading Comprehension. (arXiv:2303.08262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08262
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#32467;&#26500;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#20020;&#24202;&#27010;&#24565;&#25552;&#21462;&#21644;&#20851;&#31995;&#25552;&#21462;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#36328;&#26426;&#26500;&#24212;&#29992;&#30340;&#33391;&#22909;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#37319;&#29992;&#32479;&#19968;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#32467;&#26500;&#65292;&#24320;&#21457;&#20986;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#65292;&#21487;&#26377;&#25928;&#22320;&#35299;&#20915;&#20020;&#24202;&#27010;&#24565;&#25552;&#21462;&#21644;&#20851;&#31995;&#25552;&#21462;&#65292;&#20855;&#26377;&#36328;&#26426;&#26500;&#24212;&#29992;&#30340;&#33391;&#22909;&#36890;&#29992;&#24615;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20351;&#29992;&#32479;&#19968;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#32467;&#26500;&#26469;&#25552;&#21462;&#20020;&#24202;&#27010;&#24565;&#21644;&#20851;&#31995;&#65292;&#24182;&#25506;&#32034;&#26368;&#20808;&#36827;&#30340;Transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;2018&#24180;&#22269;&#23478;NLP&#20020;&#24202;&#25361;&#25112;&#36187;&#65288;n2c2&#65289;&#24320;&#21457;&#30340;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;&#33647;&#29289;&#21644;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#65289;&#20197;&#21450;2022&#24180;n2c2&#25361;&#25112;&#36187;&#65288;&#20581;&#24247;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#30340;&#20851;&#31995;&#65289;&#23545;&#25105;&#20204;&#30340;MRC&#27169;&#22411;&#19982;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#20851;&#31995;&#25552;&#21462;&#12290;&#25105;&#20204;&#36824;&#22312;&#36328;&#26426;&#26500;&#35774;&#32622;&#20013;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;MRC&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#35823;&#24046;&#20998;&#26512;&#65292;&#24182;&#26816;&#26597;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#22914;&#20309;&#24433;&#21709;MRC&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: To develop a natural language processing system that solves both clinical concept extraction and relation extraction in a unified prompt-based machine reading comprehension (MRC) architecture with good generalizability for cross-institution applications.  Methods: We formulate both clinical concept extraction and relation extraction using a unified prompt-based MRC architecture and explore state-of-the-art transformer models. We compare our MRC models with existing deep learning models for concept extraction and end-to-end relation extraction using two benchmark datasets developed by the 2018 National NLP Clinical Challenges (n2c2) challenge (medications and adverse drug events) and the 2022 n2c2 challenge (relations of social determinants of health [SDoH]). We also evaluate the transfer learning ability of the proposed MRC models in a cross-institution setting. We perform error analyses and examine how different prompting strategies affect the performance of MRC models.  Re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;Transformer&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#24320;&#21457;&#20102;NLP&#31995;&#32479;&#65292;&#21487;&#22312;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#33647;&#29289;&#21450;&#20854;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#22312;&#33647;&#29289;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.08259</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#22312;&#35821;&#22659;&#21270;&#33647;&#29289;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Contextualized Medication Information Extraction Using Transformer-based Deep Learning Architectures. (arXiv:2303.08259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;Transformer&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#24320;&#21457;&#20102;NLP&#31995;&#32479;&#65292;&#21487;&#22312;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#33647;&#29289;&#21450;&#20854;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#22312;&#33647;&#29289;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#24320;&#21457;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#31995;&#32479;&#65292;&#25552;&#21462;&#33647;&#29289;&#21450;&#26377;&#21161;&#20110;&#29702;&#35299;&#33647;&#29289;&#21464;&#21270;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#26041;&#27861;&#65306;&#24320;&#21457;&#20102;&#19977;&#20010;NLP&#31995;&#32479;&#65292;&#21253;&#25324;&#33647;&#29289;&#25552;&#21450;&#25552;&#21462;&#12289;&#20107;&#20214;&#20998;&#31867;(&#25351;&#33647;&#29289;&#21464;&#21270;&#30340;&#35752;&#35770;&#25110;&#26410;&#35752;&#35770;)&#12289;&#20197;&#21450;&#20998;&#31867;&#33647;&#29289;&#21464;&#21270;&#19978;&#19979;&#25991;&#21040;5&#20010;&#19982;&#33647;&#29289;&#21464;&#21270;&#30456;&#20851;&#30340;&#27491;&#20132;&#32500;&#24230;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;6&#20010;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#21253;&#25324;GatorTron&#65292;&#23427;&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#20351;&#29992;&#36229;&#36807;90&#20159;&#20010;&#21333;&#35789;&#30340;&#25991;&#26412;(&#21253;&#25324;&#26469;&#33258;&#20315;&#32599;&#37324;&#36798;&#22823;&#23398;&#20581;&#24247;&#20013;&#24515;&#35782;&#21035;&#30340;2.9&#20159;&#22810;&#20010;&#20020;&#24202;&#31508;&#35760;&#20013;&#30340;&#36229;&#36807;80&#20159;&#20010;&#21333;&#35789;)&#12290;&#25105;&#20204;&#20351;&#29992;2022 n2c2&#30340;&#27880;&#37322;&#25968;&#25454;&#21644;&#35780;&#20272;&#33050;&#26412;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;NLP&#31995;&#32479;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#30340;GatorTron&#27169;&#22411;&#22312;&#33647;&#29289;&#25552;&#21462;&#12289;&#20107;&#20214;&#20998;&#31867;&#21644;&#19978;&#19979;&#25991;&#20998;&#31867;&#26041;&#38754;&#20998;&#21035;&#21462;&#24471;&#20102;&#26368;&#20339;&#30340; F1 &#20998;&#25968;&#65292;&#20998;&#21035;&#20026; 0.9828&#65288;&#25490;&#21517;&#31532;3&#65289;&#12289;0.9379&#65288;&#25490;&#21517;&#31532;1&#65289;&#12289;0.8375&#65288;&#25490;&#21517;&#31532;1&#65289;&#65292;&#36229;&#36807;&#20854;&#20182;5&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#26368;&#20339;&#30340;NLP&#31995;&#32479;&#22312;18&#20010;&#21442;&#36187;&#22242;&#38431;&#20013;&#25490;&#21517;&#31532;&#20108;&#65292;&#33719;&#24471;&#20102;&#24635;&#20307;F1&#24471;&#20998;0.8774 &#12290;&#32467;&#35770;&#65306;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#20307;&#31995;&#32467;&#26500;&#65292;&#22914;GatorTron&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#33647;&#29289;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#22312;&#33647;&#29289;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: To develop a natural language processing (NLP) system to extract medications and contextual information that help understand drug changes. This project is part of the 2022 n2c2 challenge.  Materials and methods: We developed NLP systems for medication mention extraction, event classification (indicating medication changes discussed or not), and context classification to classify medication changes context into 5 orthogonal dimensions related to drug changes. We explored 6 state-of-the-art pretrained transformer models for the three subtasks, including GatorTron, a large language model pretrained using &gt;90 billion words of text (including &gt;80 billion words from &gt;290 million clinical notes identified at the University of Florida Health). We evaluated our NLP systems using annotated data and evaluation scripts provided by the 2022 n2c2 organizers.  Results:Our GatorTron models achieved the best F1-scores of 0.9828 for medication extraction (ranked 3rd), 0.9379 for event classif
&lt;/p&gt;</description></item><item><title>NL4Opt&#27604;&#36187;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20986;&#20248;&#21270;&#38382;&#39064;&#30340;&#21547;&#20041;&#21644;&#34920;&#36848;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#38750;&#19987;&#19994;&#20154;&#22763;&#36827;&#34892;&#20132;&#20114;&#12290;&#31454;&#36187;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;(1) &#35782;&#21035;&#21644;&#26631;&#35760;&#23545;&#24212;&#20110;&#20248;&#21270;&#38382;&#39064;&#32452;&#20214;&#30340;&#35821;&#20041;&#23454;&#20307;;(2)&#20174;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;&#23454;&#20307;&#29983;&#25104;&#24847;&#20041;&#34920;&#31034;(&#21363;&#36923;&#36753;&#24418;&#24335;)&#12290;</title><link>http://arxiv.org/abs/2303.08233</link><description>&lt;p&gt;
NL4Opt &#27604;&#36187;&#65306;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26500;&#24314;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
NL4Opt Competition: Formulating Optimization Problems Based on Their Natural Language Descriptions. (arXiv:2303.08233v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08233
&lt;/p&gt;
&lt;p&gt;
NL4Opt&#27604;&#36187;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20986;&#20248;&#21270;&#38382;&#39064;&#30340;&#21547;&#20041;&#21644;&#34920;&#36848;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#38750;&#19987;&#19994;&#20154;&#22763;&#36827;&#34892;&#20132;&#20114;&#12290;&#31454;&#36187;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;(1) &#35782;&#21035;&#21644;&#26631;&#35760;&#23545;&#24212;&#20110;&#20248;&#21270;&#38382;&#39064;&#32452;&#20214;&#30340;&#35821;&#20041;&#23454;&#20307;;(2)&#20174;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;&#23454;&#20307;&#29983;&#25104;&#24847;&#20041;&#34920;&#31034;(&#21363;&#36923;&#36753;&#24418;&#24335;)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20248;&#21270;&#65288;NL4Opt&#65289;&#31454;&#36187;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#26681;&#25454;&#20248;&#21270;&#38382;&#39064;&#30340;&#25991;&#26412;&#25551;&#36848;&#25552;&#21462;&#20854;&#21547;&#20041;&#21644;&#34920;&#36848;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#31454;&#36187;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20013;&#20171;&#26469;&#20351;&#38750;&#19987;&#19994;&#20154;&#22763;&#33021;&#22815;&#25509;&#21475;&#20351;&#29992;&#20248;&#21270;&#27714;&#35299;&#22120;&#65292;&#20197;&#22686;&#21152;&#20854;&#21487;&#35775;&#38382;&#24615;&#21644;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#25361;&#25112;&#24615;&#30446;&#26631;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;(1)&#35782;&#21035;&#21644;&#26631;&#35760;&#23545;&#24212;&#20110;&#20248;&#21270;&#38382;&#39064;&#32452;&#20214;&#30340;&#35821;&#20041;&#23454;&#20307;;(2)&#20174;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;&#23454;&#20307;&#29983;&#25104;&#24847;&#20041;&#34920;&#31034;(&#21363;&#36923;&#36753;&#24418;&#24335;)&#12290;&#31532;&#19968;&#20010;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#26816;&#27979;&#21644;&#26631;&#35760;&#20248;&#21270;&#38382;&#39064;&#30340;&#23454;&#20307;&#26469;&#20943;&#23569;&#27495;&#20041;&#12290;&#31532;&#20108;&#20010;&#20219;&#21153;&#21019;&#24314;&#20102;&#19968;&#20010;&#32447;&#24615;&#35268;&#21010;(LP)&#38382;&#39064;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#35813;&#20013;&#38388;&#34920;&#31034;&#34987;&#36716;&#25442;&#20026;&#21830;&#29992;&#27714;&#35299;&#22120;&#21487;&#29992;&#30340;&#26684;&#24335;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LP&#21333;&#35789;&#38382;&#39064;&#25968;&#25454;&#38598;&#21644;NL4Opt&#27604;&#36187;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#24182;&#24635;&#32467;&#20102;&#31454;&#36187;&#26465;&#30446;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Natural Language for Optimization (NL4Opt) Competition was created to investigate methods of extracting the meaning and formulation of an optimization problem based on its text description. Specifically, the goal of the competition is to increase the accessibility and usability of optimization solvers by allowing non-experts to interface with them using natural language. We separate this challenging goal into two sub-tasks: (1) recognize and label the semantic entities that correspond to the components of the optimization problem; (2) generate a meaning representation (i.e., a logical form) of the problem from its detected problem entities. The first task aims to reduce ambiguity by detecting and tagging the entities of the optimization problems. The second task creates an intermediate representation of the linear programming (LP) problem that is converted into a format that can be used by commercial solvers. In this report, we present the LP word problem dataset and shared tasks f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;medBERT.de&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#21307;&#23398;&#39046;&#22495;&#30340;BERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#30340;&#35757;&#32451;&#65292;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#23545;&#38271;&#25991;&#26412;&#29305;&#21035;&#26377;&#29992;&#65292;&#32780;&#25968;&#25454;&#21435;&#37325;&#21644;&#26377;&#25928;&#30340;&#20998;&#35789;&#21017;&#21482;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#20102;&#36739;&#23567;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.08179</link><description>&lt;p&gt;
MEDBERT.de&#65306;&#19968;&#20010;&#22522;&#20110;&#24503;&#35821;&#30340;&#12289;&#38024;&#23545;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#20840;&#38754;BERT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MEDBERT.de: A Comprehensive German BERT Model for the Medical Domain. (arXiv:2303.08179v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;medBERT.de&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#21307;&#23398;&#39046;&#22495;&#30340;BERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#30340;&#35757;&#32451;&#65292;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#23545;&#38271;&#25991;&#26412;&#29305;&#21035;&#26377;&#29992;&#65292;&#32780;&#25968;&#25454;&#21435;&#37325;&#21644;&#26377;&#25928;&#30340;&#20998;&#35789;&#21017;&#21482;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#20102;&#36739;&#23567;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;medBERT.de&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#24503;&#35821;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#24050;&#32463;&#22312;470&#19975;&#20221;&#24503;&#35821;&#21307;&#23398;&#25991;&#26723;&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#65292;&#28041;&#21450;&#21508;&#31181;&#23398;&#31185;&#21644;&#21307;&#23398;&#25991;&#29486;&#31867;&#22411;&#12290;&#38500;&#20102;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#22806;&#65292;&#26412;&#25991;&#36824;&#23545;&#20854;&#33021;&#21147;&#36827;&#34892;&#20102;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#21435;&#37325;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20351;&#29992;&#26356;&#26377;&#25928;&#30340;&#20998;&#35789;&#26041;&#27861;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20687;medBERT.de&#36825;&#26679;&#30340;&#39046;&#22495;&#19987;&#29992;&#27169;&#22411;&#29305;&#21035;&#36866;&#29992;&#20110;&#36739;&#38271;&#30340;&#25991;&#26412;&#65292;&#24182;&#19988;&#25968;&#25454;&#21435;&#37325;&#19981;&#19968;&#23450;&#20250;&#23548;&#33268;&#24615;&#33021;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#26377;&#25928;&#30340;&#20998;&#35789;&#21482;&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#21457;&#25381;&#20102;&#36739;&#23567;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#22823;&#22810;&#25968;&#25913;&#36827;&#28304;&#20110;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents medBERT.de, a pre-trained German BERT model specifically designed for the German medical domain. The model has been trained on a large corpus of 4.7 Million German medical documents and has been shown to achieve new state-of-the-art performance on eight different medical benchmarks covering a wide range of disciplines and medical document types. In addition to evaluating the overall performance of the model, this paper also conducts a more in-depth analysis of its capabilities. We investigate the impact of data deduplication on the model's performance, as well as the potential benefits of using more efficient tokenization methods. Our results indicate that domain-specific models such as medBERT.de are particularly useful for longer texts, and that deduplication of training data does not necessarily lead to improved performance. Furthermore, we found that efficient tokenization plays only a minor role in improving model performance, and attribute most of the improved
&lt;/p&gt;</description></item><item><title>WHOOPS!&#26159;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24120;&#35782;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20102;&#22270;&#20687;&#23383;&#24149;&#12289;&#36328;&#27169;&#24577;&#21305;&#37197;&#21644;&#35270;&#35273;&#38382;&#31572;&#31561;&#33509;&#24178;&#20010;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#35299;&#37322;&#29983;&#25104;&#20219;&#21153;&#65292;&#25361;&#25112;&#20102;AI&#27169;&#22411;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#21512;&#24120;&#35268;&#30340;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07274</link><description>&lt;p&gt;
&#25171;&#30772;&#24120;&#35782;&#65306;WHOOPS&#65281;&#19968;&#20010;&#22522;&#20110;&#21512;&#25104;&#21644;&#32452;&#21512;&#22270;&#20687;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images. (arXiv:2303.07274v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07274
&lt;/p&gt;
&lt;p&gt;
WHOOPS!&#26159;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24120;&#35782;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#20102;&#22270;&#20687;&#23383;&#24149;&#12289;&#36328;&#27169;&#24577;&#21305;&#37197;&#21644;&#35270;&#35273;&#38382;&#31572;&#31561;&#33509;&#24178;&#20010;&#20219;&#21153;&#65292;&#24341;&#20837;&#20102;&#35299;&#37322;&#29983;&#25104;&#20219;&#21153;&#65292;&#25361;&#25112;&#20102;AI&#27169;&#22411;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#21512;&#24120;&#35268;&#30340;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22855;&#24618;&#12289;&#24322;&#24120;&#21644;&#31070;&#31192;&#30340;&#22270;&#20687;&#20250;&#24341;&#36215;&#35266;&#23519;&#32773;&#30340;&#22909;&#22855;&#24515;&#65292;&#22240;&#20026;&#23427;&#20204;&#25361;&#25112;&#20102;&#24120;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;WHOOPS&#65281;&#19968;&#20010;&#26032;&#30340;&#35270;&#35273;&#24120;&#35782;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;&#35774;&#35745;&#24072;&#20351;&#29992;Midjourney&#31561;&#20844;&#20849;&#21487;&#29992;&#22270;&#20687;&#29983;&#25104;&#24037;&#20855;&#21046;&#20316;&#65292;&#24182;&#21253;&#21547;&#33509;&#24178;&#20010;&#20219;&#21153;&#12290;&#38500;&#20102;&#22270;&#20687;&#23383;&#24149;&#12289;&#36328;&#27169;&#24577;&#21305;&#37197;&#21644;&#35270;&#35273;&#38382;&#31572;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22256;&#38590;&#30340;&#35299;&#37322;&#29983;&#25104;&#20219;&#21153;&#65292;&#20854;&#20013;&#27169;&#22411;&#24517;&#39035;&#35782;&#21035;&#24182;&#35299;&#37322;&#32473;&#23450;&#22270;&#20687;&#30340;&#24322;&#24120;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weird, unusual, and uncanny images pique the curiosity of observers because they challenge commonsense. For example, an image released during the 2022 world cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo playing chess, which playfully violates our expectation that their competition should occur on the football field. Humans can easily recognize and interpret these unconventional images, but can AI models do the same? We introduce WHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is comprised of purposefully commonsense-defying images created by designers using publicly-available image generation tools like Midjourney. We consider several tasks posed over the dataset. In addition to image captioning, cross-modal matching, and visual question answering, we introduce a difficult explanation generation task, where models must identify and explain why a given image is unusual. Our results show that state-of-the-art models such as GPT3 and BLIP2
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19977;&#31181;&#22810;&#27169;&#24577;&#23545;&#35937;&#35782;&#21035;&#26041;&#27861;&#24182;&#22312;SIMMC 2.1&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#26368;&#20339;&#26041;&#27861;&#26159;&#22522;&#20110;&#22330;&#26223;&#23545;&#35805;&#30340;&#23545;&#40784;&#65292;&#30456;&#27604;&#22522;&#20934;&#27979;&#35797;&#25552;&#39640;&#20102;&#32422;20%&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2302.14680</link><description>&lt;p&gt;
&#20320;&#26159;&#25351;&#21738;&#19968;&#20010;&#65311;&#22522;&#20110;&#24773;&#22659;&#23545;&#35805;&#30340;&#22810;&#27169;&#24577;&#23545;&#35937;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Which One Are You Referring To? Multimodal Object Identification in Situated Dialogue. (arXiv:2302.14680v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19977;&#31181;&#22810;&#27169;&#24577;&#23545;&#35937;&#35782;&#21035;&#26041;&#27861;&#24182;&#22312;SIMMC 2.1&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#26368;&#20339;&#26041;&#27861;&#26159;&#22522;&#20110;&#22330;&#26223;&#23545;&#35805;&#30340;&#23545;&#40784;&#65292;&#30456;&#27604;&#22522;&#20934;&#27979;&#35797;&#25552;&#39640;&#20102;&#32422;20%&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#35805;&#31995;&#32479;&#30340;&#38656;&#27714;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#19981;&#26029;&#19978;&#21319;&#65292;&#36825;&#24378;&#35843;&#20102;&#20174;&#23545;&#35805;&#21644;&#24773;&#22659;&#32972;&#26223;&#20013;&#29702;&#35299;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#22312;&#26368;&#22823;&#30340;&#24773;&#22659;&#23545;&#35805;&#25968;&#25454;&#38598;SIMMC 2.1&#19978;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#22330;&#26223;&#23545;&#35805;&#30340;&#23545;&#40784;&#65292;&#30456;&#27604;SIMMC 2.1&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#25552;&#39640;&#20102;&#32422;20%&#30340;F1&#20998;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20998;&#26512;&#21644;&#35752;&#35770;&#25105;&#20204;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#24037;&#20316;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#22312;https://github.com/holylovenia/multimodal-object-identification&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demand for multimodal dialogue systems has been rising in various domains, emphasizing the importance of interpreting multimodal inputs from conversational and situational contexts. We explore three methods to tackle this problem and evaluate them on the largest situated dialogue dataset, SIMMC 2.1. Our best method, scene-dialogue alignment, improves the performance by ~20% F1-score compared to the SIMMC 2.1 baselines. We provide analysis and discussion regarding the limitation of our methods and the potential directions for future works. Our code is publicly available at https://github.com/holylovenia/multimodal-object-identification.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;Fine-grained Two-stage&#35757;&#32451;&#26694;&#26550;&#65288;FiTs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#30693;&#35782;&#24863;&#30693;&#38382;&#31572;&#65288;KAQA&#65289;&#20013;&#65292;&#20174;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#20013;&#33719;&#24471;&#30340;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#22312;&#34920;&#31034;&#19978;&#30340;&#24046;&#24322;&#21644;&#32852;&#21512;&#25512;&#29702;&#30340;&#22256;&#38590;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.11799</link><description>&lt;p&gt;
FiTs:&#32454;&#31890;&#24230;&#20004;&#38454;&#27573;&#35757;&#32451;&#29992;&#20110;&#30693;&#35782;&#24863;&#30693;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
FiTs: Fine-grained Two-stage Training for Knowledge-aware Question Answering. (arXiv:2302.11799v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;Fine-grained Two-stage&#35757;&#32451;&#26694;&#26550;&#65288;FiTs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#30693;&#35782;&#24863;&#30693;&#38382;&#31572;&#65288;KAQA&#65289;&#20013;&#65292;&#20174;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#20013;&#33719;&#24471;&#30340;&#20004;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#22312;&#34920;&#31034;&#19978;&#30340;&#24046;&#24322;&#21644;&#32852;&#21512;&#25512;&#29702;&#30340;&#22256;&#38590;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24863;&#30693;&#38382;&#31572;&#65288;KAQA&#65289;&#38656;&#35201;&#27169;&#22411;&#22312;&#30693;&#35782;&#24211;&#20013;&#22238;&#31572;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#24320;&#25918;&#22495;QA&#21644;&#29305;&#23450;&#39046;&#22495;QA&#37117;&#26159;&#24517;&#35201;&#30340;&#65292;&#23588;&#20854;&#26159;&#24403;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#25552;&#20379;&#25152;&#38656;&#30340;&#25152;&#26377;&#30693;&#35782;&#26102;&#12290;&#26368;&#36817;KAQA&#31995;&#32479;&#34701;&#21512;&#20102;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#21644;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#33719;&#24471;&#30340;&#35821;&#35328;&#30693;&#35782;&#21644;&#20107;&#23454;&#30693;&#35782;&#20197;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#23384;&#22312;&#22256;&#38590;&#65292;&#21363;&#26377;&#25928;&#22320;&#34701;&#21512;&#26469;&#33258;PLMs&#21644;KGs&#30340;&#34920;&#31034;&#65292;&#22240;&#20026;&#65288;i&#65289;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#35821;&#20041;&#21644;&#20998;&#24067;&#24046;&#24322;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#38590;&#20197;&#32852;&#21512;&#25512;&#29702;&#25552;&#20379;&#30340;&#20004;&#31867;&#30693;&#35782;&#12290;&#38024;&#23545;&#19978;&#36848;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;Fine-grained Two-stage&#35757;&#32451;&#26694;&#26550;&#65288;FiTs&#65289;&#65292;&#26088;&#22312;&#25552;&#39640;KAQA&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#31532;&#19968;&#38454;&#27573;&#26088;&#22312;&#36890;&#36807;&#30693;&#35782;&#36866;&#24212;&#21518;&#35757;&#32451;&#26469;&#23545;&#40784;&#26469;&#33258;PLM&#21644;KG&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#24357;&#21512;&#23427;&#20204;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-aware question answering (KAQA) requires the model to answer questions over a knowledge base, which is essential for both open-domain QA and domain-specific QA, especially when language models alone cannot provide all the knowledge needed. Despite the promising result of recent KAQA systems which tend to integrate linguistic knowledge from pre-trained language models (PLM) and factual knowledge from knowledge graphs (KG) to answer complex questions, a bottleneck exists in effectively fusing the representations from PLMs and KGs because of (i) the semantic and distributional gaps between them, and (ii) the difficulties in joint reasoning over the provided knowledge from both modalities. To address the above two problems, we propose a Fine-grained Two-stage training framework (FiTs) to boost the KAQA system performance: The first stage aims at aligning representations from the PLM and the KG, thus bridging the modality gaps between them, named knowledge adaptive post-training. 
&lt;/p&gt;</description></item><item><title>TRESTLE&#26159;&#19968;&#20010;&#30001;TalkBank&#23384;&#20648;&#24211;&#20013;&#36873;&#21462;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#21487;&#37325;&#22797;&#25191;&#34892;&#24037;&#20855;&#21253;&#12290;&#20854;&#20013;&#65292;&#32769;&#24180;&#30196;&#21574;&#26816;&#27979;&#20026;&#20363;&#23376;&#12290;TRESTLE&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#21644;&#21487;&#37325;&#22797;&#30340;&#23454;&#39564;&#24179;&#21488;&#65292;&#21487;&#20197;&#20351;&#24471;&#30740;&#31350;&#32773;&#21487;&#20197;&#22312;&#20043;&#19978;&#36827;&#34892;&#26356;&#21152;&#39640;&#25928;&#12289;&#20934;&#30830;&#30340;&#27169;&#22411;&#26500;&#24314;&#12290;</title><link>http://arxiv.org/abs/2302.07322</link><description>&lt;p&gt;
TRESTLE&#65306;&#35821;&#38899;&#12289;&#25991;&#26412;&#21644;&#35821;&#35328;&#23454;&#39564;&#30340;&#21487;&#37325;&#22797;&#25191;&#34892;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
TRESTLE: Toolkit for Reproducible Execution of Speech, Text and Language Experiments. (arXiv:2302.07322v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07322
&lt;/p&gt;
&lt;p&gt;
TRESTLE&#26159;&#19968;&#20010;&#30001;TalkBank&#23384;&#20648;&#24211;&#20013;&#36873;&#21462;&#20004;&#20010;&#25968;&#25454;&#38598;&#30340;&#21487;&#37325;&#22797;&#25191;&#34892;&#24037;&#20855;&#21253;&#12290;&#20854;&#20013;&#65292;&#32769;&#24180;&#30196;&#21574;&#26816;&#27979;&#20026;&#20363;&#23376;&#12290;TRESTLE&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#21644;&#21487;&#37325;&#22797;&#30340;&#23454;&#39564;&#24179;&#21488;&#65292;&#21487;&#20197;&#20351;&#24471;&#30740;&#31350;&#32773;&#21487;&#20197;&#22312;&#20043;&#19978;&#36827;&#34892;&#26356;&#21152;&#39640;&#25928;&#12289;&#20934;&#30830;&#30340;&#27169;&#22411;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#21040;&#19981;&#21516;&#35748;&#30693;&#38556;&#30861;&#65288;&#22914;&#32769;&#24180;&#30196;&#21574;&#30151;&#65289;&#20010;&#20307;&#21644;&#35748;&#30693;&#20581;&#24247;&#20010;&#20307;&#25152;&#20135;&#29983;&#30340;&#35821;&#35328;&#20043;&#38388;&#24494;&#22937;&#30340;&#24046;&#24322;&#12290;TalkBank&#31561;&#20844;&#20849;&#25968;&#25454;&#36164;&#28304;&#24050;&#32463;&#20351;&#24471;&#35745;&#31639;&#26426;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#32852;&#21512;&#36215;&#26469;&#65292;&#30456;&#20114;&#23398;&#20064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#30740;&#31350;&#32773;&#20351;&#29992;&#30340;&#26041;&#27861;&#21644;&#25968;&#25454;&#36873;&#21462;&#31574;&#30053;&#30340;&#21464;&#24322;&#24615;&#65292;&#19981;&#21516;&#22242;&#20307;&#33719;&#24471;&#30340;&#32467;&#26524;&#24456;&#38590;&#30452;&#25509;&#36827;&#34892;&#27604;&#36739;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TRESTLE&#65288;\textbf{T}oolkit for \textbf{R}eproducible \textbf{E}xecution of \textbf{S}peech \textbf{T}ext and \textbf{L}anguage \textbf{E}xperiments&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#25918;&#28304;&#30721;&#24179;&#21488;&#65292;&#19987;&#27880;&#20110;&#20174;TalkBank&#23384;&#20648;&#24211;&#20013;&#36873;&#21462;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#38656;&#20197;&#32769;&#24180;&#30196;&#21574;&#26816;&#27979;&#20026;&#20363;&#12290;TRESTLE&#24050;&#32463;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;2019&#24180;&#20449;&#24687;&#26816;&#32034;&#22269;&#38469;&#20250;&#35758;Hackathon / Challenge&#30340;&#27604;&#36187;&#20013;&#65292;&#22312;&#26631;&#20934;&#21270;&#21644;&#21487;&#37325;&#22797;&#23454;&#39564;&#26041;&#38754;&#20026;&#31038;&#21306;&#25552;&#20379;&#25903;&#25345;&#65292;&#20026;&#26816;&#27979;&#35748;&#30693;&#38556;&#30861;&#30340;&#26356;&#20934;&#30830;&#21644;&#20020;&#24202;&#26377;&#25928;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evidence is growing that machine and deep learning methods can learn the subtle differences between the language produced by people with various forms of cognitive impairment such as dementia and cognitively healthy individuals. Valuable public data repositories such as TalkBank have made it possible for researchers in the computational community to join forces and learn from each other to make significant advances in this area. However, due to variability in approaches and data selection strategies used by various researchers, results obtained by different groups have been difficult to compare directly. In this paper, we present TRESTLE (\textbf{T}oolkit for \textbf{R}eproducible \textbf{E}xecution of \textbf{S}peech \textbf{T}ext and \textbf{L}anguage \textbf{E}xperiments), an open source platform that focuses on two datasets from the TalkBank repository with dementia detection as an illustrative domain. Successfully deployed in the hackallenge (Hackathon/Challenge) of the Intern
&lt;/p&gt;</description></item><item><title>&#8220;&#36890;&#36807;&#27979;&#35797;&#22810;&#20010;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;40&#20010;ToM&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-3&#21644;GPT-4&#33021;&#22815;&#35299;&#20915;&#22823;&#37096;&#20998;&#20219;&#21153;&#65292;&#35828;&#26126;&#31867;&#20284;ToM&#30340;&#33021;&#21147;&#21487;&#33021;&#26159;&#35821;&#35328;&#27169;&#22411;&#33258;&#21457;&#20986;&#29616;&#30340;&#38468;&#24102;&#20135;&#29289;&#12290;&#8221;</title><link>http://arxiv.org/abs/2302.02083</link><description>&lt;p&gt;
&#8220;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#33258;&#21457;&#20986;&#29616;&#24515;&#26234;&#29702;&#35770;&#8221;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind May Have Spontaneously Emerged in Large Language Models. (arXiv:2302.02083v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02083
&lt;/p&gt;
&lt;p&gt;
&#8220;&#36890;&#36807;&#27979;&#35797;&#22810;&#20010;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;40&#20010;ToM&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-3&#21644;GPT-4&#33021;&#22815;&#35299;&#20915;&#22823;&#37096;&#20998;&#20219;&#21153;&#65292;&#35828;&#26126;&#31867;&#20284;ToM&#30340;&#33021;&#21147;&#21487;&#33021;&#26159;&#35821;&#35328;&#27169;&#22411;&#33258;&#21457;&#20986;&#29616;&#30340;&#38468;&#24102;&#20135;&#29289;&#12290;&#8221;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#25351;&#33021;&#22815;&#25512;&#29702;&#20182;&#20154;&#20869;&#24515;&#30340;&#19981;&#21487;&#35266;&#23519;&#29366;&#24577;&#65292;&#23545;&#20110;&#20154;&#31867;&#31038;&#20132;&#20114;&#21160;&#12289;&#20132;&#27969;&#12289;&#31227;&#24773;&#12289;&#33258;&#25105;&#24847;&#35782;&#21644;&#36947;&#24503;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;40&#20010;&#24191;&#27867;&#29992;&#20110;&#27979;&#35797;&#20154;&#31867;ToM&#30340;&#32463;&#20856;&#34394;&#20551;&#20449;&#24565;&#20219;&#21153;&#26469;&#27979;&#35797;&#20960;&#20010;&#35821;&#35328;&#27169;&#22411;&#12290;2020&#24180;&#20043;&#21069;&#21457;&#24067;&#30340;&#27169;&#22411;&#22312;&#35299;&#20915;ToM&#20219;&#21153;&#26041;&#38754;&#20960;&#20046;&#27809;&#26377;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;2020&#24180;5&#26376;&#21457;&#24067;&#30340;&#31532;&#19968;&#20010;GPT-3&#29256;&#26412;&#65288;&#8220;davinci-001&#8221;&#65289;&#35299;&#20915;&#20102;&#32422;40&#65285;&#30340;&#34394;&#20551;&#20449;&#24565;&#20219;&#21153;&#65292;&#19982;3.5&#23681;&#30340;&#20799;&#31461;&#30340;&#34920;&#29616;&#30456;&#24403;&#12290;&#23427;&#30340;&#31532;&#20108;&#20010;&#29256;&#26412;&#65288;&#8220;davinci-002&#8221;&#65292;2022&#24180;1&#26376;&#65289;&#35299;&#20915;&#20102;70&#65285;&#30340;&#34394;&#20551;&#20449;&#24565;&#20219;&#21153;&#65292;&#19982;6&#23681;&#20799;&#31461;&#30340;&#34920;&#29616;&#30456;&#24403;&#12290;&#26368;&#26032;&#29256;&#26412;&#30340;GPT-3.5&#65288;&#8220;davinci-003&#8221;&#65292;2022&#24180;11&#26376;&#65289;&#35299;&#20915;&#20102;90&#65285;&#30340;&#34394;&#20551;&#20449;&#24565;&#20219;&#21153;&#65292;&#36798;&#21040;&#20102;7&#23681;&#20799;&#31461;&#27700;&#24179;&#12290;&#20110;2023&#24180;3&#26376;&#21457;&#24067;&#30340;GPT-4&#35299;&#20915;&#20102;&#20960;&#20046;&#25152;&#26377;&#30340;&#20219;&#21153;&#65288;95&#65285;&#65289;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#31867;&#20284;ToM&#30340;&#33021;&#21147;&#65288;&#36804;&#20170;&#34987;&#35748;&#20026;&#26159;&#20154;&#31867;&#29420;&#26377;&#30340;&#65289;&#21487;&#33021;&#26159;&#35821;&#35328;&#30340;&#38468;&#24102;&#20135;&#29289;&#12290;&#8221;
&lt;/p&gt;
&lt;p&gt;
Theory of mind (ToM), or the ability to impute unobservable mental states to others, is central to human social interactions, communication, empathy, self-consciousness, and morality. We tested several language models using 40 classic false-belief tasks widely used to test ToM in humans. The models published before 2020 showed virtually no ability to solve ToM tasks. Yet, the first version of GPT-3 ("davinci-001"), published in May 2020, solved about 40% of false-belief tasks-performance comparable with 3.5-year-old children. Its second version ("davinci-002"; January 2022) solved 70% of false-belief tasks, performance comparable with six-year-olds. Its most recent version, GPT-3.5 ("davinci-003"; November 2022), solved 90% of false-belief tasks, at the level of seven-year-olds. GPT-4 published in March 2023 solved nearly all the tasks (95%). These findings suggest that ToM-like ability (thus far considered to be uniquely human) may have spontaneously emerged as a byproduct of language
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;BabelCode&#26694;&#26550;&#21644;Translating Python Programming Puzzles&#65288;TP3&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#25506;&#35752;&#20102;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;14&#31181;&#32534;&#31243;&#35821;&#35328;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#24179;&#34913;&#20998;&#24067;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2302.01973</link><description>&lt;p&gt;
&#27979;&#37327;&#32534;&#31243;&#35821;&#35328;&#20998;&#24067;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Measuring The Impact Of Programming Language Distribution. (arXiv:2302.01973v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01973
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;BabelCode&#26694;&#26550;&#21644;Translating Python Programming Puzzles&#65288;TP3&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#25506;&#35752;&#20102;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;14&#31181;&#32534;&#31243;&#35821;&#35328;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#24179;&#34913;&#20998;&#24067;&#26377;&#21161;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#29992;&#20110;&#35780;&#20272;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#21482;&#38598;&#20013;&#22312;&#24456;&#23569;&#30340;&#19968;&#37096;&#20998;&#32534;&#31243;&#35821;&#35328;&#19978;&#65292;&#19981;&#21253;&#25324;&#35768;&#22810;&#27969;&#34892;&#30340;&#35821;&#35328;&#65292;&#20363;&#22914;Go&#25110;Rust&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BabelCode&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#25191;&#34892;&#30340;&#35780;&#20272;&#20219;&#20309;&#35821;&#35328;&#20013;&#30340;&#20219;&#20309;&#22522;&#20934;&#27979;&#35797;&#12290;BabelCode&#20351;&#24471;&#21487;&#20197;&#23545;&#27169;&#22411;&#30340;&#20869;&#23384;&#12289;&#36816;&#34892;&#26102;&#38388;&#21644;&#21333;&#20010;&#27979;&#35797;&#26696;&#20363;&#32467;&#26524;&#36827;&#34892;&#26032;&#30340;&#23450;&#24615;&#24615;&#33021;&#35843;&#26597;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;Translating Python Programming Puzzles&#65288;TP3&#65289;&#30340;&#26032;&#20195;&#30721;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;Python Programming Puzzles&#65288;Schuster&#31561;&#20154;&#65292;2021&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#28041;&#21450;&#23558;&#19987;&#23478;&#32423;Python&#20989;&#25968;&#32763;&#35793;&#25104;&#20219;&#20309;&#35821;&#35328;&#12290;&#36890;&#36807;&#23545;BabelCode&#21644;TP3&#22522;&#20934;&#27979;&#35797;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#24179;&#34913;14&#31181;&#35821;&#35328;&#30340;&#20998;&#24067;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#12290;&#22312;&#24179;&#34913;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#24179;&#22343;&#32780;&#35328;&#65292;&#30456;&#23545;&#20110;&#19981;&#24179;&#34913;&#20998;&#24067;&#30340;&#24773;&#20917;&#65292;&#25152;&#26377;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;$pass@k$&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;12.34%&#12290;
&lt;/p&gt;
&lt;p&gt;
Current benchmarks for evaluating neural code models focus on only a small subset of programming languages, excluding many popular languages such as Go or Rust. To ameliorate this issue, we present the BabelCode framework for execution-based evaluation of any benchmark in any language. BabelCode enables new investigations into the qualitative performance of models' memory, runtime, and individual test case results. Additionally, we present a new code translation dataset called Translating Python Programming Puzzles (TP3) from the Python Programming Puzzles (Schuster et al. 2021) benchmark that involves translating expert-level python functions to any language. With both BabelCode and the TP3 benchmark, we investigate if balancing the distributions of 14 languages in a training dataset improves a large language model's performance on low-resource languages. Training a model on a balanced corpus results in, on average, 12.34% higher $pass@k$ across all tasks and languages compared to the
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20174;r/AmITheAsshole Reddit&#31038;&#21306;&#20013;&#30340;&#20010;&#20154;&#21465;&#36848;&#20013;&#37492;&#21035;&#20102;&#20316;&#32773;&#20316;&#20026;&#35282;&#33394;&#21644;&#21465;&#36848;&#32773;&#30340;&#35821;&#35328;&#21644;&#21465;&#20107;&#29305;&#24449;&#65292;&#20197;&#22238;&#31572;&#20160;&#20040;&#26679;&#30340;&#20154;&#29289;&#20250;&#25104;&#20026;&#8220;&#28151;&#34507;&#8221;&#65292;&#20197;&#21450;&#20160;&#20040;&#26679;&#30340;&#21465;&#36848;&#26041;&#24335;&#20250;&#34987;&#35270;&#20026;&#8220;&#28151;&#34507;&#8221;&#12290;</title><link>http://arxiv.org/abs/2301.08104</link><description>&lt;p&gt;
&#20316;&#32773;&#21363;&#35282;&#33394;&#21448;&#21465;&#36848;&#32773;&#65306;&#20174;r/AmITheAsshole Reddit&#31038;&#21306;&#35299;&#26500;&#20010;&#20154;&#21465;&#36848;
&lt;/p&gt;
&lt;p&gt;
Author as Character and Narrator: Deconstructing Personal Narratives from the r/AmITheAsshole Reddit Community. (arXiv:2301.08104v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08104
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20174;r/AmITheAsshole Reddit&#31038;&#21306;&#20013;&#30340;&#20010;&#20154;&#21465;&#36848;&#20013;&#37492;&#21035;&#20102;&#20316;&#32773;&#20316;&#20026;&#35282;&#33394;&#21644;&#21465;&#36848;&#32773;&#30340;&#35821;&#35328;&#21644;&#21465;&#20107;&#29305;&#24449;&#65292;&#20197;&#22238;&#31572;&#20160;&#20040;&#26679;&#30340;&#20154;&#29289;&#20250;&#25104;&#20026;&#8220;&#28151;&#34507;&#8221;&#65292;&#20197;&#21450;&#20160;&#20040;&#26679;&#30340;&#21465;&#36848;&#26041;&#24335;&#20250;&#34987;&#35270;&#20026;&#8220;&#28151;&#34507;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;r/AmITheAsshole&#23376;&#35770;&#22363;&#20013;&#65292;&#20154;&#20204;&#21311;&#21517;&#20998;&#20139;&#21253;&#21547;&#19968;&#20123;&#36947;&#24503;&#22256;&#22659;&#25110;&#20914;&#31361;&#30340;&#31532;&#19968;&#20154;&#31216;&#21465;&#36848;&#65292;&#24182;&#35810;&#38382;&#31038;&#21306;&#21028;&#26029;&#35841;&#26159;&#38169;&#30340;&#65288;&#21363;&#8220;&#28151;&#34507;&#8221;&#65289;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#31532;&#19968;&#20154;&#31216;&#21465;&#36848;&#26159;&#19968;&#20010;&#29420;&#29305;&#30340;&#35762;&#25925;&#20107;&#39046;&#22495;&#65292;&#20854;&#20013;&#20316;&#32773;&#26082;&#26159;&#21465;&#36848;&#32773;&#65288;&#35762;&#36848;&#25925;&#20107;&#30340;&#20154;&#65289;&#65292;&#21448;&#21487;&#20197;&#26159;&#35282;&#33394;&#65288;&#29983;&#27963;&#25925;&#20107;&#30340;&#20154;&#65289;&#65292;&#22240;&#27492;&#20316;&#32773;&#22312;&#25925;&#20107;&#20013;&#26377;&#20004;&#31181;&#19981;&#21516;&#30340;&#22768;&#38899;&#12290;&#26412;&#30740;&#31350;&#35782;&#21035;&#20102;&#19982;&#20316;&#32773;&#20316;&#20026;&#35282;&#33394;&#25110;&#21465;&#36848;&#32773;&#30456;&#20851;&#30340;&#35821;&#35328;&#21644;&#21465;&#20107;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;&#65288;1&#65289;&#26159;&#20160;&#20040;&#35753;&#19968;&#20010;&#35282;&#33394;&#25104;&#20026;&#28151;&#34507;&#65292;&#65288;2&#65289;&#21465;&#36848;&#32773;&#24590;&#26679;&#25165;&#31639;&#26159;&#19968;&#20010;&#28151;&#34507;&#65311;&#25105;&#20204;&#25552;&#21462;&#20102;&#20316;&#32773;&#20316;&#20026;&#35282;&#33394;&#30340;&#29305;&#24449;&#65288;&#20363;&#22914;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12289;&#21465;&#20107;&#20107;&#20214;&#38142;&#21644;&#24773;&#24863;&#24359;&#32447;&#65289;&#21644;&#20316;&#32773;&#20316;&#20026;&#21465;&#36848;&#32773;&#30340;&#29305;&#24449;&#65288;&#21363;&#25972;&#20307;&#25925;&#20107;&#30340;&#39118;&#26684;&#21644;&#24773;&#24863;&#65289;&#65292;&#20197;&#30830;&#23450;&#25925;&#20107;&#30340;&#21738;&#20123;&#26041;&#38754;&#19982;&#26368;&#32456;&#30340;&#36947;&#24503;&#21028;&#26029;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the r/AmITheAsshole subreddit, people anonymously share first person narratives that contain some moral dilemma or conflict and ask the community to judge who is at fault (i.e., who is "the asshole"). In general, first person narratives are a unique storytelling domain where the author is the narrator (the person telling the story) but can also be a character (the person living the story) and, thus, the author has two distinct voices presented in the story. In this study, we identify linguistic and narrative features associated with the author as the character or as a narrator. We use these features to answer the following questions: (1) what makes an asshole character and (2) what makes an asshole narrator? We extract both Author-as-Character features (e.g., demographics, narrative event chain, and emotional arc) and Author-as-Narrator features (i.e., the style and emotion of the story as a whole) in order to identify which aspects of the narrative are correlated with the final mor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#21407;&#22411;&#32593;&#32476;&#65288;DPN&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#20108;&#20998;&#22270;&#21305;&#37197;&#38382;&#39064;&#20998;&#31163;&#24050;&#30693;&#21644;&#26032;&#31867;&#21035;&#26469;&#26377;&#25928;&#22320;&#23454;&#29616;&#19981;&#21516;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#23558;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#24050;&#30693;&#31867;&#21035;&#23545;&#40784;&#65292;&#20197;&#26174;&#24335;&#36716;&#31227;&#31867;&#21035;&#29305;&#23450;&#30340;&#30693;&#35782;&#21644;&#25429;&#33719;&#39640;&#32423;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2211.15115</link><description>&lt;p&gt;
&#22522;&#20110;&#35299;&#32806;&#21407;&#22411;&#32593;&#32476;&#30340;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Generalized Category Discovery with Decoupled Prototypical Network. (arXiv:2211.15115v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#21407;&#22411;&#32593;&#32476;&#65288;DPN&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#20108;&#20998;&#22270;&#21305;&#37197;&#38382;&#39064;&#20998;&#31163;&#24050;&#30693;&#21644;&#26032;&#31867;&#21035;&#26469;&#26377;&#25928;&#22320;&#23454;&#29616;&#19981;&#21516;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#24182;&#23558;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#24050;&#30693;&#31867;&#21035;&#23545;&#40784;&#65292;&#20197;&#26174;&#24335;&#36716;&#31227;&#31867;&#21035;&#29305;&#23450;&#30340;&#30693;&#35782;&#21644;&#25429;&#33719;&#39640;&#32423;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#30340;&#30446;&#26631;&#26159;&#20174;&#19968;&#32452;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#35782;&#21035;&#24050;&#30693;&#21644;&#26032;&#30340;&#31867;&#21035;&#65292;&#22522;&#20110;&#21478;&#19968;&#20010;&#20165;&#24102;&#26377;&#24050;&#30693;&#31867;&#21035;&#30340;&#25968;&#25454;&#38598;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#24573;&#30053;&#24050;&#30693;&#21644;&#26032;&#31867;&#21035;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22312;&#32806;&#21512;&#30340;&#26041;&#24335;&#19979;&#23398;&#20064;&#23427;&#20204;&#65292;&#36825;&#21487;&#33021;&#20250;&#25439;&#23475;&#27169;&#22411;&#30340;&#27867;&#21270;&#21644;&#21306;&#20998;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#32806;&#21512;&#35757;&#32451;&#26041;&#27861;&#38459;&#27490;&#36825;&#20123;&#27169;&#22411;&#26174;&#24335;&#22320;&#20174;&#26631;&#35760;&#25968;&#25454;&#21521;&#26410;&#26631;&#35760;&#25968;&#25454;&#36716;&#31227;&#31867;&#21035;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#36825;&#21487;&#33021;&#20250;&#20002;&#22833;&#39640;&#32423;&#35821;&#20041;&#20449;&#24687;&#24182;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#20943;&#36731;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;&#35299;&#32806;&#21407;&#22411;&#32593;&#32476;&#65288;DPN&#65289;&#12290;&#36890;&#36807;&#20026;&#31867;&#21035;&#21407;&#22411;&#21046;&#23450;&#19968;&#20010;&#20108;&#20998;&#22270;&#21305;&#37197;&#38382;&#39064;&#65292;DPN&#19981;&#20165;&#21487;&#20197;&#35299;&#32806;&#24050;&#30693;&#21644;&#26032;&#30340;&#31867;&#21035;&#20197;&#26377;&#25928;&#22320;&#23454;&#29616;&#19981;&#21516;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#23558;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#24050;&#30693;&#31867;&#21035;&#23545;&#40784;&#65292;&#20197;&#26174;&#24335;&#22320;&#36716;&#31227;&#31867;&#21035;&#29305;&#23450;&#30340;&#30693;&#35782;&#24182;&#25429;&#33719;&#39640;&#27700;&#24179;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized Category Discovery (GCD) aims to recognize both known and novel categories from a set of unlabeled data, based on another dataset labeled with only known categories. Without considering differences between known and novel categories, current methods learn about them in a coupled manner, which can hurt model's generalization and discriminative ability. Furthermore, the coupled training approach prevents these models transferring category-specific knowledge explicitly from labeled data to unlabeled data, which can lose high-level semantic information and impair model performance. To mitigate above limitations, we present a novel model called Decoupled Prototypical Network (DPN). By formulating a bipartite matching problem for category prototypes, DPN can not only decouple known and novel categories to achieve different training targets effectively, but also align known categories in labeled and unlabeled data to transfer category-specific knowledge explicitly and capture high
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#27604;&#20102;BLOOM&#21644;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#22312;&#24615;&#33021;&#12289;&#36328;&#35821;&#35328;&#21644;&#22810;&#35821;&#35328;&#24494;&#35843;&#20197;&#21450;&#22522;&#20110;&#25552;&#31034;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#27602;&#24615;&#20998;&#26512;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;BLOOM&#19982;&#20854;&#20182;LLM&#30340;&#24615;&#33021;&#19981;&#25104;&#27491;&#27604;&#65292;&#20294;&#22312;&#29983;&#25104;&#27602;&#24615;&#20302;&#30340;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.14865</link><description>&lt;p&gt;
&#35299;&#26512;BLOOM&#65306;&#23545;&#21508;&#31181;NLP&#20219;&#21153;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding BLOOM: An empirical study on diverse NLP tasks. (arXiv:2211.14865v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#27604;&#20102;BLOOM&#21644;&#20854;&#20182;&#35821;&#35328;&#27169;&#22411;&#22312;&#24615;&#33021;&#12289;&#36328;&#35821;&#35328;&#21644;&#22810;&#35821;&#35328;&#24494;&#35843;&#20197;&#21450;&#22522;&#20110;&#25552;&#31034;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#27602;&#24615;&#20998;&#26512;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;BLOOM&#19982;&#20854;&#20182;LLM&#30340;&#24615;&#33021;&#19981;&#25104;&#27491;&#27604;&#65292;&#20294;&#22312;&#29983;&#25104;&#27602;&#24615;&#20302;&#30340;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;&#36739;&#23567;&#30340;BLOOM&#27169;&#22411;&#65288;350m/560m&#21644;1b3/1b7&#65289;&#22312;&#22810;&#20010;NLP&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#27969;&#34892;&#25490;&#34892;&#27036;&#19978;&#30340;&#34920;&#29616;&#65292;&#20197;&#20102;&#35299;BLOOM&#21644;&#20854;&#20182;&#20165;&#20351;&#29992;&#35299;&#30721;&#22120;&#30340;LLM&#19982;BERT&#24335;&#32534;&#30721;&#22120;-&#20165;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290; &#25105;&#20204;&#24471;&#20986;&#20197;&#19979;&#35266;&#23519;&#32467;&#26524;:&#65288;1&#65289;BLOOM&#30340;&#24615;&#33021;&#19982;&#21442;&#25968;&#22823;&#23567;&#27809;&#26377;&#27491;&#27604;&#20363;&#20851;&#31995;&#65292;&#19981;&#21516;&#20110;GPT&#21644;BERT&#31561;&#20854;&#20182;LLM&#12290;&#24494;&#35843;BLOOM&#27169;&#22411;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;1b7&#21464;&#20307;&#30456;&#27604;&#65292;560m&#21464;&#20307;&#30340;&#34920;&#29616;&#30456;&#20284;&#25110;&#26356;&#22909;&#12290;&#65288;2&#65289;&#38646;-shot&#20132;&#21449;&#35821;&#35328;&#21644;&#22810;&#35821;&#35328;&#24494;&#35843;&#23454;&#39564;&#34920;&#26126;&#65292;BLOOM&#19982;&#21333;&#35821;&#35328;&#30340;GPT-2&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#24046;&#65292;&#65288;3&#65289;&#20351;&#29992;RealToxicityPrompts&#25968;&#25454;&#38598;&#36827;&#34892;&#22522;&#20110;&#25552;&#31034;&#30340;&#25991;&#26412;&#29983;&#25104;&#27602;&#24615;&#20998;&#26512;&#26174;&#31034;&#65292;BLOOM&#29983;&#25104;&#30340;&#25991;&#26412;&#33267;&#23569;&#27604;GPT-2&#21644;GPT-3&#27169;&#22411;&#27602;&#24615;&#20302;17&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
We view the landscape of large language models (LLMs) through the lens of the recently released BLOOM model to understand the performance of BLOOM and other decoder-only LLMs compared to BERT-style encoder-only models. We achieve this by evaluating the smaller BLOOM model variants (\textit{350m/560m} and \textit{1b3/1b7}) on several NLP benchmark datasets and popular leaderboards. We make the following observations: (1) BLOOM performance does not scale with parameter size, unlike other LLMs like GPT and BERT. Experiments fine-tuning BLOOM models show that the 560m variant performs similarly to or better than the 1b7 variant, (2) Zero-shot cross-lingual and multi-lingual fine-tuning experiments show that BLOOM is at par or worse than monolingual GPT-2 models, and (3) Toxicity analysis of prompt-based text generation using the RealToxicityPrompts dataset shows that the text generated by BLOOM is at least 17\% less toxic than GPT-2 and GPT-3 models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#19968;&#27425;&#24615;&#24207;&#21015;&#21387;&#32553;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25903;&#25345;&#36830;&#32493;&#30340;&#25805;&#20316;&#21387;&#32553;&#29575;&#33539;&#22260;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24179;&#28369;&#30340;&#24615;&#33021;&#25928;&#29575;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2211.02332</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#19968;&#27425;&#24615;&#24207;&#21015;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Once-for-All Sequence Compression for Self-Supervised Speech Models. (arXiv:2211.02332v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#19968;&#27425;&#24615;&#24207;&#21015;&#21387;&#32553;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25903;&#25345;&#36830;&#32493;&#30340;&#25805;&#20316;&#21387;&#32553;&#29575;&#33539;&#22260;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24179;&#28369;&#30340;&#24615;&#33021;&#25928;&#29575;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#22788;&#29702;&#20013;&#65292;&#26102;&#38388;&#36724;&#19978;&#30340;&#24207;&#21015;&#38271;&#24230;&#36890;&#24120;&#26159;&#35745;&#31639;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#20026;&#20102;&#38477;&#20302;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#20943;&#23569;&#24207;&#21015;&#38271;&#24230;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#23545;&#24207;&#21015;&#21387;&#32553;&#26377;&#19981;&#21516;&#30340;&#23481;&#24525;&#24230;&#65292;&#22240;&#27492;&#29983;&#20135;&#22266;&#23450;&#21387;&#32553;&#29575;&#30340;&#27169;&#22411;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#25152;&#26377;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#19968;&#27425;&#24615;&#24207;&#21015;&#21387;&#32553;&#26694;&#26550;&#65292;&#25903;&#25345;&#36830;&#32493;&#30340;&#25805;&#20316;&#21387;&#32553;&#29575;&#33539;&#22260;&#12290;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#19982;&#22266;&#23450;&#21387;&#32553;&#29575;&#21464;&#20307;&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#24179;&#28369;&#30340;&#24615;&#33021;&#25928;&#29575;&#26435;&#34913;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#33258;&#36866;&#24212;&#21387;&#32553;&#29575;&#23398;&#20064;&#65292;&#28436;&#31034;&#20102;&#36873;&#25321;&#20219;&#21153;&#29305;&#23450;&#30340;&#20248;&#20808;&#24103;&#26102;&#26399;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#36827;&#34892;&#32593;&#26684;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sequence length along the time axis is often the dominant factor of the computation in speech processing. Works have been proposed to reduce the sequence length for lowering the computational cost in self-supervised speech models. However, different downstream tasks have different tolerance of sequence compressing, so a model that produces a fixed compressing rate may not fit all tasks. In this work, we introduce a once-for-all (OFA) sequence compression framework for self-supervised speech models that supports a continuous range of operating compressing rates. The framework is evaluated on various tasks, showing marginal degradation compared to the fixed compressing rate variants with a smooth performance-efficiency trade-off. We further explore adaptive compressing rate learning, demonstrating the ability to select task-specific preferred frame periods without needing a grid search.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#21475;&#35821;&#29702;&#35299;&#20013;&#30340;&#38271;&#23614;&#35789;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26641;&#32422;&#26463;&#30340;&#25351;&#38024;&#29983;&#25104;&#22120;TCPGen&#21644;&#27133;&#27010;&#29575;&#20559;&#32622;&#26426;&#21046;SPB&#65292;&#36890;&#36807;&#23545;&#24212;&#30340;&#23454;&#20307;&#21644;&#30701;&#27133;&#21015;&#34920;&#25552;&#21462;&#20559;&#32622;&#21015;&#34920;&#26469;&#20559;&#32622;SLU&#27169;&#22411;&#30340;&#36755;&#20986;&#27133;&#20998;&#24067;&#65292;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.16554</link><description>&lt;p&gt;
&#24102;&#26641;&#32422;&#26463;&#30340;&#25351;&#38024;&#29983;&#25104;&#22120;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
End-to-end Spoken Language Understanding with Tree-constrained Pointer Generator. (arXiv:2210.16554v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#21475;&#35821;&#29702;&#35299;&#20013;&#30340;&#38271;&#23614;&#35789;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26641;&#32422;&#26463;&#30340;&#25351;&#38024;&#29983;&#25104;&#22120;TCPGen&#21644;&#27133;&#27010;&#29575;&#20559;&#32622;&#26426;&#21046;SPB&#65292;&#36890;&#36807;&#23545;&#24212;&#30340;&#23454;&#20307;&#21644;&#30701;&#27133;&#21015;&#34920;&#25552;&#21462;&#20559;&#32622;&#21015;&#34920;&#26469;&#20559;&#32622;SLU&#27169;&#22411;&#30340;&#36755;&#20986;&#27133;&#20998;&#24067;&#65292;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#30340;&#21475;&#35821;&#29702;&#35299;&#31995;&#32479;&#38754;&#20020;&#30528;&#38271;&#23614;&#35789;&#38382;&#39064;&#12290;&#26412;&#25991;&#21033;&#29992;&#19978;&#19979;&#25991;&#20559;&#32622;&#25216;&#26415;&#25552;&#39640;&#20102;&#21475;&#35821;&#29702;&#35299;&#31995;&#32479;&#30340;&#31232;&#26377;&#35789;&#35782;&#21035;&#33021;&#21147;&#65292;&#20855;&#20307;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#26641;&#32422;&#26463;&#25351;&#38024;&#29983;&#25104;&#22120;&#65288;TCPGen&#65289;&#30340;&#24378;&#22823;&#39640;&#25928;&#30340;&#20559;&#32622;&#27169;&#22411;&#32452;&#20214;&#65292;&#35813;&#32452;&#20214;&#21033;&#29992;&#23545;&#24212;&#30340;&#23454;&#20307;&#21644;&#30701;&#27133;&#21015;&#34920;&#25552;&#21462;&#20559;&#32622;&#21015;&#34920;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#20559;&#32622;SLU&#27169;&#22411;&#30340;&#36755;&#20986;&#27133;&#20998;&#24067;&#65292;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#27133;&#27010;&#29575;&#20559;&#32622;&#65288;SPB&#65289;&#26426;&#21046;&#65292;&#20174;TCPGen&#35745;&#31639;&#27133;&#20998;&#24067;&#12290;&#22312;SLURP&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;TCPGen&#21644;SPB&#21487;&#20197;&#25345;&#32493;&#25913;&#21892;SLU-F1&#24471;&#20998;&#65292;&#23588;&#20854;&#26159;&#22312;&#30475;&#19981;&#35265;&#30340;&#23454;&#20307;&#19978;&#12290;&#22312;&#20445;&#30041;5&#20010;&#27133;&#31867;&#22411;&#36827;&#34892;&#27979;&#35797;&#30340;&#26032;&#20998;&#21106;&#19978;&#65292;TCPGen&#21644;SPB&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#22312;SLU-F1&#24471;&#20998;&#19978;&#36229;&#36807;&#20102;50%&#30340;&#22522;&#32447;&#27700;&#24179;&#12290;&#38500;&#20102;&#27133;&#22635;&#20805;&#22806;&#65292;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#24615;&#20063;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end spoken language understanding (SLU) suffers from the long-tail word problem. This paper exploits contextual biasing, a technique to improve the speech recognition of rare words, in end-to-end SLU systems. Specifically, a tree-constrained pointer generator (TCPGen), a powerful and efficient biasing model component, is studied, which leverages a slot shortlist with corresponding entities to extract biasing lists. Meanwhile, to bias the SLU model output slot distribution, a slot probability biasing (SPB) mechanism is proposed to calculate a slot distribution from TCPGen. Experiments on the SLURP dataset showed consistent SLU-F1 improvements using TCPGen and SPB, especially on unseen entities. On a new split by holding out 5 slot types for the test, TCPGen with SPB achieved zero-shot learning with an SLU-F1 score over 50% compared to baselines which can not deal with it. In addition to slot filling, the intent classification accuracy was also improved.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#27169;&#22411;&#29992;&#20110;&#26500;&#24314;&#22768;&#23398;&#35789;&#23884;&#20837;&#65288;AWE&#65289;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#24179;&#22343;&#27719;&#32858;&#30340;HuBERT&#34920;&#31034;&#27861;&#24050;&#32463;&#21487;&#20197;&#19982;&#33521;&#35821;AWE&#30340;&#26368;&#26032;&#34920;&#29616;&#30456;&#23218;&#32654;&#65292;&#32780;&#19988;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#20063;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2210.16043</link><description>&lt;p&gt;
&#20174;&#39044;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#20998;&#26512;&#22768;&#23398;&#35789;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Analyzing Acoustic Word Embeddings from Pre-trained Self-supervised Speech Models. (arXiv:2210.16043v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#27169;&#22411;&#29992;&#20110;&#26500;&#24314;&#22768;&#23398;&#35789;&#23884;&#20837;&#65288;AWE&#65289;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#24179;&#22343;&#27719;&#32858;&#30340;HuBERT&#34920;&#31034;&#27861;&#24050;&#32463;&#21487;&#20197;&#19982;&#33521;&#35821;AWE&#30340;&#26368;&#26032;&#34920;&#29616;&#30456;&#23218;&#32654;&#65292;&#32780;&#19988;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#20063;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#27169;&#22411;&#21462;&#24471;&#20102;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24378;&#22823;&#32467;&#26524;&#65292;&#20294;&#23545;&#20110;&#29992;&#20110;&#20195;&#34920;&#21487;&#21464;&#38271;&#30340;&#21475;&#35821;&#21333;&#35789;&#29255;&#27573;&#30340;&#22768;&#23398;&#35789;&#23884;&#20837;&#65288;AWE&#65289;&#65292;&#21364;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#32034;&#33258;&#30417;&#30563;&#34920;&#31034;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#27719;&#32858;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#20855;&#26377;&#33258;&#30417;&#30563;&#34920;&#31034;&#27861;&#30340;AWE&#12290;&#30001;&#20110;&#33258;&#30417;&#30563;&#34920;&#31034;&#27861;&#20855;&#26377;&#19978;&#19979;&#25991;&#21270;&#30340;&#29305;&#28857;&#65292;&#25105;&#20204;&#20551;&#35774;&#31616;&#21333;&#30340;&#27719;&#32858;&#26041;&#27861;&#65292;&#22914;&#24179;&#22343;&#27861;&#65292;&#21487;&#33021;&#24050;&#32463;&#26377;&#21161;&#20110;&#26500;&#24314;AWE&#12290;&#22312;&#26631;&#20934;&#30340;&#21333;&#35789;&#21306;&#20998;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#20855;&#26377;&#22343;&#20540;&#27719;&#32858;&#30340;HuBERT&#34920;&#31034;&#27861;&#24050;&#32463;&#21487;&#20197;&#19982;&#33521;&#35821;AWE&#30340;&#26368;&#26032;&#34920;&#29616;&#30456;&#23218;&#32654;&#12290;&#26356;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23613;&#31649;&#21482;&#22312;&#33521;&#35821;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#22312;Xitsonga&#12289;&#26222;&#36890;&#35805;&#21644;&#27861;&#35821;&#19978;&#35780;&#20272;&#30340;HuBERT&#34920;&#31034;&#27861;&#22987;&#32456;&#20248;&#20110;&#22810;&#35821;&#35328;&#27169;&#22411;XLSR-53&#65288;&#20197;&#21450;&#22312;&#33521;&#35821;&#19978;&#35757;&#32451;&#30340;Wav2Vec 2.0&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the strong results of self-supervised models on various tasks, there have been surprisingly few studies exploring self-supervised representations for acoustic word embeddings (AWE), fixed-dimensional vectors representing variable-length spoken word segments. In this work, we study several pre-trained models and pooling methods for constructing AWEs with self-supervised representations. Owing to the contextualized nature of self-supervised representations, we hypothesize that simple pooling methods, such as averaging, might already be useful for constructing AWEs. When evaluating on a standard word discrimination task, we find that HuBERT representations with mean-pooling rival the state of the art on English AWEs. More surprisingly, despite being trained only on English, HuBERT representations evaluated on Xitsonga, Mandarin, and French consistently outperform the multilingual model XLSR-53 (as well as Wav2Vec 2.0 trained on English).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#20197;&#26415;&#35821;&#20026;&#20013;&#24515;&#30340;&#29305;&#24449;&#25913;&#36827;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.15551</link><description>&lt;p&gt;
&#26415;&#35821;&#24863;&#30693;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Terminology-aware Medical Dialogue Generation. (arXiv:2210.15551v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#20197;&#26415;&#35821;&#20026;&#20013;&#24515;&#30340;&#29305;&#24449;&#25913;&#36827;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#26088;&#22312;&#26681;&#25454;&#21307;&#29983;&#21644;&#24739;&#32773;&#20043;&#38388;&#30340;&#23545;&#35805;&#21382;&#21490;&#35760;&#24405;&#29983;&#25104;&#21709;&#24212;&#12290;&#19982;&#38754;&#21521;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#29983;&#25104;&#19981;&#21516;&#65292;&#36825;&#35201;&#27714;&#20855;&#26377;&#21307;&#23398;&#39046;&#22495;&#29305;&#23450;&#30340;&#32972;&#26223;&#30693;&#35782;&#12290;&#29616;&#26377;&#30340;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#29983;&#25104;&#26694;&#26550;&#26410;&#33021;&#32435;&#20837;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#21307;&#23398;&#26415;&#35821;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32771;&#34385;&#20197;&#26415;&#35821;&#20026;&#20013;&#24515;&#30340;&#29305;&#24449;&#26469;&#25913;&#36827;&#21307;&#23398;&#23545;&#35805;&#29983;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#26469;&#32435;&#20837;&#26415;&#35821;&#23621;&#20013;&#30340;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#24378;&#21046;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#36741;&#21161;&#26415;&#35821;&#35782;&#21035;&#20219;&#21153;&#23398;&#20064;&#26415;&#35821;&#34920;&#31034;&#26469;&#22635;&#34917;&#21307;&#23398;&#32972;&#26223;&#30693;&#35782;&#21644;&#24120;&#29992;&#35805;&#35821;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#20248;&#20110;SOTA&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical dialogue generation aims to generate responses according to a history of dialogue turns between doctors and patients. Unlike open-domain dialogue generation, this requires background knowledge specific to the medical domain. Existing generative frameworks for medical dialogue generation fall short of incorporating domain-specific knowledge, especially with regard to medical terminology. In this paper, we propose a novel framework to improve medical dialogue generation by considering features centered on domain-specific terminology. We leverage an attention mechanism to incorporate terminologically centred features, and fill in the semantic gap between medical background knowledge and common utterances by enforcing language models to learn terminology representations with an auxiliary terminology recognition task. Experimental results demonstrate the effectiveness of our approach, in which our proposed framework outperforms SOTA language models. Additionally, we provide a new da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Virtuoso&#65292;&#19968;&#31181;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#35821;&#38899;-&#25991;&#23383;&#32852;&#21512;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;TTS&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#35757;&#32451;&#26041;&#26696;&#20351;&#27169;&#22411;&#22312;&#24050;&#30693;&#35821;&#35328;&#21644;&#26410;&#30693;&#35821;&#35328;&#20013;&#22343;&#33021;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2210.15447</link><description>&lt;p&gt;
Virtuoso&#65306;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#35821;&#38899;&#25991;&#23383;&#32852;&#21512;&#21322;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Virtuoso: Massive Multilingual Speech-Text Joint Semi-Supervised Learning for Text-To-Speech. (arXiv:2210.15447v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Virtuoso&#65292;&#19968;&#31181;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#35821;&#38899;-&#25991;&#23383;&#32852;&#21512;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;TTS&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#19981;&#21516;&#35757;&#32451;&#26041;&#26696;&#20351;&#27169;&#22411;&#22312;&#24050;&#30693;&#35821;&#35328;&#21644;&#26410;&#30693;&#35821;&#35328;&#20013;&#22343;&#33021;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;Virtuoso&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;&#35821;&#38899;&#21512;&#25104;&#65288;TTS&#65289;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#35821;&#38899;&#25991;&#23383;&#32852;&#21512;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#12290;&#29616;&#26377;&#30340;&#22810;&#35821;&#35328;TTS&#36890;&#24120;&#25903;&#25345;&#20960;&#21313;&#31181;&#35821;&#35328;&#65292;&#36825;&#21482;&#26159;&#20840;&#29699;&#25968;&#21315;&#31181;&#35821;&#35328;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#23558;&#22810;&#35821;&#35328;TTS&#25193;&#23637;&#21040;&#25968;&#30334;&#31181;&#35821;&#35328;&#30340;&#19968;&#20010;&#22256;&#38590;&#26159;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#12290;&#26412;&#30740;&#31350;&#23558;Maestro&#65288;&#19968;&#31181;&#38754;&#21521;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#35821;&#38899;-&#25991;&#26412;&#32852;&#21512;&#39044;&#35757;&#32451;&#26694;&#26550;&#65289;&#25193;&#23637;&#21040;&#35821;&#38899;&#29983;&#25104;&#20219;&#21153;&#12290;&#20026;&#20102;&#20174;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#25968;&#25454;&#20013;&#35757;&#32451;TTS&#27169;&#22411;&#65292;&#35774;&#35745;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#20197;&#22788;&#29702;&#26377;&#30417;&#30563;&#30340;&#65288;&#37197;&#23545;&#30340;TTS&#21644;ASR&#25968;&#25454;&#65289;&#21644;&#26080;&#30417;&#30563;&#30340;&#65288;&#26410;&#36716;&#24405;&#30340;&#35821;&#38899;&#21644;&#26410;&#21457;&#22768;&#30340;&#25991;&#26412;&#65289;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;1&#65289;&#22312;&#24050;&#30693;&#35821;&#35328;&#20013;&#65292;&#20351;&#29992;Virtuoso&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;TTS&#27169;&#22411;&#21487;&#20197;&#27604;&#22522;&#32447;&#27169;&#22411;&#26174;&#30528;&#25552;&#39640;&#33258;&#28982;&#24230;&#21644;&#21487;&#25026;&#24230;&#65307;2&#65289;&#22312;&#30475;&#19981;&#21040;&#21442;&#32771;&#35821;&#38899;&#26679;&#26412;&#25110;&#36716;&#24405;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#21487;&#20197;&#21512;&#25104;&#26410;&#30693;&#35821;&#35328;&#30340;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes Virtuoso, a massively multilingual speech-text joint semi-supervised learning framework for text-to-speech synthesis (TTS) models. Existing multilingual TTS typically supports tens of languages, which are a small fraction of the thousands of languages in the world. One difficulty to scale multilingual TTS to hundreds of languages is collecting high-quality speech-text paired data in low-resource languages. This study extends Maestro, a speech-text joint pretraining framework for automatic speech recognition (ASR), to speech generation tasks. To train a TTS model from various types of speech and text data, different training schemes are designed to handle supervised (paired TTS and ASR data) and unsupervised (untranscribed speech and unspoken text) datasets. Experimental evaluation shows that 1) multilingual TTS models trained on Virtuoso can achieve significantly better naturalness and intelligibility than baseline ones in seen languages, and 2) they can synthesize 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#22686;&#24378;&#30340;&#33258;&#36866;&#24212;&#34701;&#21512;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#23558;&#20381;&#36182;&#20449;&#24687;&#19982;&#21407;&#22987;&#35821;&#20041;&#20449;&#21495;&#33258;&#36866;&#24212;&#34701;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#35821;&#20041;&#21305;&#37197;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2210.08471</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#33258;&#36866;&#24212;&#34701;&#21512;&#25552;&#39640;&#35821;&#20041;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion. (arXiv:2210.08471v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#22686;&#24378;&#30340;&#33258;&#36866;&#24212;&#34701;&#21512;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#23558;&#20381;&#36182;&#20449;&#24687;&#19982;&#21407;&#22987;&#35821;&#20041;&#20449;&#21495;&#33258;&#36866;&#24212;&#34701;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#35821;&#20041;&#21305;&#37197;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22914;BERT&#65292;&#22312;&#35821;&#20041;&#21477;&#23376;&#21305;&#37197;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#12290;&#21516;&#26102;&#65292;&#20381;&#36182;&#24615;&#20808;&#39564;&#30693;&#35782;&#22312;&#22810;&#20010;NLP&#20219;&#21153;&#20013;&#20063;&#26174;&#31034;&#20986;&#26222;&#36941;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#20381;&#36182;&#24615;&#20808;&#39564;&#32467;&#26500;&#26377;&#25928;&#22320;&#38598;&#25104;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#35821;&#20041;&#21305;&#37197;&#20851;&#31995;&#65292;&#20173;&#26410;&#30830;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DAFA&#30340;&#20381;&#36182;&#22686;&#24378;&#33258;&#36866;&#24212;&#34701;&#21512;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#36825;&#23558;&#20381;&#36182;&#32467;&#26500;&#26126;&#30830;&#22320;&#24341;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#21040;&#35821;&#20041;&#20449;&#24687;&#20013;&#12290;&#20855;&#20307;&#22320;&#65292;DAFA&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#25935;&#24863;&#33539;&#24335;&#26469;&#26500;&#24314;&#19968;&#20010;&#20381;&#36182;&#30697;&#38453;&#65292;&#20197;&#26657;&#20934;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#23427;&#37319;&#29992;&#33258;&#36866;&#24212;&#34701;&#21512;&#27169;&#22359;&#26469;&#38598;&#25104;&#33719;&#21462;&#30340;&#20381;&#36182;&#20449;&#24687;&#21644;&#21407;&#22987;&#35821;&#20041;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;DAFA&#37325;&#26500;&#20102;&#27880;&#24847;&#21147;&#35745;&#31639;&#27969;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pre-trained models like BERT have achieved great progress on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also shown general benefits in multiple NLP tasks. However, how to efficiently integrate dependency prior structure into pre-trained models to better model complex semantic matching relations is still unsettled. In this paper, we propose the \textbf{D}ependency-Enhanced \textbf{A}daptive \textbf{F}usion \textbf{A}ttention (\textbf{DAFA}), which explicitly introduces dependency structure into pre-trained models and adaptively fuses it with semantic information. Specifically, \textbf{\emph{(i)}} DAFA first proposes a structure-sensitive paradigm to construct a dependency matrix for calibrating attention weights. It adopts an adaptive fusion module to integrate the obtained dependency information and the original semantic signals. Moreover, DAFA reconstructs the attention calculation flow and provides better interpretability. By applying it o
&lt;/p&gt;</description></item><item><title>MAPL&#20351;&#29992;&#23545;&#40784;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23398;&#20064;&#21333;&#27169;&#24577;&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#20043;&#38388;&#30340;&#36731;&#37327;&#32423;&#26144;&#23556;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38754;&#21521;&#35270;&#35273;-&#35821;&#35328;&#23569;&#26679;&#26412;&#20219;&#21153;&#30340;&#22522;&#20110;&#21442;&#25968;&#25928;&#29575;&#30340;&#36866;&#24212;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2210.07179</link><description>&lt;p&gt;
MAPL: &#22522;&#20110;&#21442;&#25968;&#25928;&#29575;&#30340;&#21333;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#35270;&#35273;-&#35821;&#35328;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#30340;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting. (arXiv:2210.07179v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07179
&lt;/p&gt;
&lt;p&gt;
MAPL&#20351;&#29992;&#23545;&#40784;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23398;&#20064;&#21333;&#27169;&#24577;&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#20043;&#38388;&#30340;&#36731;&#37327;&#32423;&#26144;&#23556;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38754;&#21521;&#35270;&#35273;-&#35821;&#35328;&#23569;&#26679;&#26412;&#20219;&#21153;&#30340;&#22522;&#20110;&#21442;&#25968;&#25928;&#29575;&#30340;&#36866;&#24212;&#65292;&#24182;&#22312;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21333;&#27169;&#24577;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#21644;&#65288;&#22522;&#20110;&#25552;&#31034;&#30340;&#65289;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MAPL&#65292;&#19968;&#31181;&#31616;&#21333;&#19988;&#21442;&#25968;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#65292;&#23427;&#37325;&#29992;&#20923;&#32467;&#30340;&#21333;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#22330;&#26223;&#20013;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;MAPL&#20351;&#29992;&#23545;&#40784;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23398;&#20064;&#20102;&#21333;&#27169;&#24577;&#27169;&#22411;&#34920;&#31034;&#31354;&#38388;&#20043;&#38388;&#30340;&#36731;&#37327;&#32423;&#26144;&#23556;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#20165;&#26377;&#23569;&#37327;&#19978;&#19979;&#25991;&#31034;&#20363;&#23601;&#25512;&#24191;&#21040;&#30475;&#19981;&#35265;&#30340;VL&#20219;&#21153;&#12290;MAPL&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#24456;&#23569;&#65292;&#20351;&#24471;&#23427;&#22312;&#20302;&#25968;&#25454;&#21644;&#22495;&#20869;&#23398;&#20064;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;MAPL&#30340;&#27169;&#22359;&#21270;&#20351;&#24471;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#21040;&#20854;&#20182;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#22312;&#20960;&#20010;&#35270;&#35273;&#38382;&#31572;&#21644;&#22270;&#20687;&#26631;&#39064;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;MAPL&#30456;&#23545;&#20110;&#31867;&#20284;&#26041;&#27861;&#22312;&#35757;&#32451;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#26102;&#23454;&#29616;&#20102;&#20248;&#36234;&#25110;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;MAPL&#21487;&#20197;&#22312;&#20960;&#23567;&#26102;&#20869;&#20351;&#29992;&#36866;&#24230;&#30340;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained models have proved to be remarkable zero- and (prompt-based) few-shot learners in unimodal vision and language tasks. We propose MAPL, a simple and parameter-efficient method that reuses frozen pre-trained unimodal models and leverages their strong generalization capabilities in multimodal vision-language (VL) settings. MAPL learns a lightweight mapping between the representation spaces of unimodal models using aligned image-text data, and can generalize to unseen VL tasks from just a few in-context examples. The small number of trainable parameters makes MAPL effective at low-data and in-domain learning. Moreover, MAPL's modularity enables easy extension to other pre-trained models. Extensive experiments on several visual question answering and image captioning benchmarks show that MAPL achieves superior or competitive performance compared to similar methods while training orders of magnitude fewer parameters. MAPL can be trained in just a few hours using modest comp
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#22256;&#24785;&#24230;&#25351;&#26631;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#30001;&#20110;&#30701;&#25991;&#26412; PPL &#20540;&#39640;&#20110;&#38271;&#25991;&#26412;&#65292;&#37325;&#22797;&#25991;&#26412;&#27573;&#33853;&#21644;&#26631;&#28857;&#31526;&#21495;&#20063;&#21487;&#20197;&#25439;&#22351;&#25351;&#26631;&#34920;&#29616;&#12290; &#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;&#24212;&#35880;&#24910;&#12290;</title><link>http://arxiv.org/abs/2210.05892</link><description>&lt;p&gt;
PLM &#22256;&#24785;&#24230;&#19981;&#21487;&#38752;&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Perplexity from PLM Is Unreliable for Evaluating Text Quality. (arXiv:2210.05892v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05892
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#22256;&#24785;&#24230;&#25351;&#26631;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#30001;&#20110;&#30701;&#25991;&#26412; PPL &#20540;&#39640;&#20110;&#38271;&#25991;&#26412;&#65292;&#37325;&#22797;&#25991;&#26412;&#27573;&#33853;&#21644;&#26631;&#28857;&#31526;&#21495;&#20063;&#21487;&#20197;&#25439;&#22351;&#25351;&#26631;&#34920;&#29616;&#12290; &#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;&#24212;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24456;&#22810;&#30740;&#31350;&#37117;&#20351;&#29992;&#22256;&#24785;&#24230;&#65288;PPL&#65289;&#26469;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;&#20182;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524; PPL &#30340;&#20540;&#36234;&#23567;&#65292;&#34987;&#35780;&#20272;&#25991;&#26412;&#30340;&#36136;&#37327;&#65288;&#21363;&#27969;&#30021;&#24615;&#65289;&#23601;&#36234;&#22909;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616; PPL &#21452;&#37325;&#38169;&#35823;&#65292;&#19981;&#33021;&#20844;&#27491;&#22320;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#65292;&#21407;&#22240;&#21253;&#25324;&#65306;&#65288;i&#65289;&#30701;&#25991;&#26412;&#30340; PPL &#20540;&#22823;&#20110;&#38271;&#25991;&#26412;&#65292;&#36825;&#19982;&#24120;&#35782;&#30456;&#24726;&#65292;&#65288;ii&#65289;&#37325;&#22797;&#25991;&#26412;&#27573;&#33853;&#29366;&#24577;&#19979;&#21487;&#20197;&#25439;&#22351; PPL&#65292;&#65288;iii&#65289;&#26631;&#28857;&#31526;&#21495;&#21487;&#20197;&#20005;&#37325;&#24433;&#21709; PPL &#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;PPL &#19981;&#33021;&#21487;&#38752;&#22320;&#35780;&#20272;&#32473;&#23450;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25991;&#26412;&#36136;&#37327;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, amounts of works utilize perplexity~(PPL) to evaluate the quality of the generated text. They suppose that if the value of PPL is smaller, the quality(i.e. fluency) of the text to be evaluated is better. However, we find that the PPL referee is unqualified and it cannot evaluate the generated text fairly for the following reasons: (i) The PPL of short text is larger than long text, which goes against common sense, (ii) The repeated text span could damage the performance of PPL, and (iii) The punctuation marks could affect the performance of PPL heavily. Experiments show that the PPL is unreliable for evaluating the quality of given text. Last, we discuss the key problems with evaluating text quality using language models.
&lt;/p&gt;</description></item><item><title>DABERT &#36890;&#36807;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#33258;&#36866;&#24212;&#34701;&#21512;&#27169;&#22359;&#22686;&#24378;&#20102; BERT &#22312;&#25429;&#25417;&#21477;&#23376;&#23545;&#20043;&#38388;&#32454;&#24494;&#24046;&#24322;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.03454</link><description>&lt;p&gt;
DABERT&#65306;&#21452;&#37325;&#27880;&#24847;&#21147;&#22686;&#24378;&#30340;BERT&#35821;&#20041;&#21305;&#37197;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DABERT: Dual Attention Enhanced BERT for Semantic Matching. (arXiv:2210.03454v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03454
&lt;/p&gt;
&lt;p&gt;
DABERT &#36890;&#36807;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#33258;&#36866;&#24212;&#34701;&#21512;&#27169;&#22359;&#22686;&#24378;&#20102; BERT &#22312;&#25429;&#25417;&#21477;&#23376;&#23545;&#20043;&#38388;&#32454;&#24494;&#24046;&#24322;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#22312;&#35821;&#20041;&#21477;&#23376;&#21305;&#37197;&#26041;&#38754;&#21462;&#24471;&#20102;&#26480;&#20986;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#20173;&#28982;&#22312;&#25429;&#25417;&#24494;&#23567;&#24046;&#24322;&#30340;&#33021;&#21147;&#19978;&#23384;&#22312;&#19981;&#36275;&#12290;&#22914;&#21152;&#20837;&#12289;&#21024;&#38500;&#25110;&#20462;&#25913;&#21477;&#23376;&#20013;&#30340;&#19968;&#20010;&#21333;&#35789;&#31561;&#22122;&#22768;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#20986;&#38169;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#27880;&#24847;&#21147;&#22686;&#24378;&#30340;BERT&#27169;&#22411;&#65288;DABERT&#65289;&#65292;&#20197;&#22686;&#24378;BERT&#22312;&#25429;&#25417;&#21477;&#23376;&#23545;&#20043;&#38388;&#32454;&#24494;&#24046;&#24322;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;DABERT&#30001;&#65288;1&#65289;&#21452;&#37325;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#65288;2&#65289;&#33258;&#36866;&#24212;&#34701;&#21512;&#27169;&#22359;&#26500;&#25104;&#12290;&#25105;&#20204;&#22312;&#32463;&#20856;&#30340;&#35821;&#20041;&#21305;&#37197;&#21644;&#40065;&#26834;&#24615;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;DABERT&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pre-trained language models such as BERT have achieved remarkable results in Semantic Sentence Matching. However, existing models still suffer from insufficient ability to capture subtle differences. Minor noise like word addition, deletion, and modification of sentences may cause flipped predictions. To alleviate this problem, we propose a novel Dual Attention Enhanced BERT (DABERT) to enhance the ability of BERT to capture fine-grained differences in sentence pairs. DABERT comprises (1) Dual Attention module, which measures soft word matches by introducing a new dual channel alignment mechanism to model affinity and difference attention. (2) Adaptive Fusion module, this module uses attention to learn the aggregation of difference and affinity features, and generates a vector describing the matching details of sentence pairs. We conduct extensive experiments on well-studied semantic matching and robustness test datasets, and the experimental results show the effectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#20197;&#26816;&#26597;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#23545;&#35805;&#20013;&#20844;&#24179;&#24615;&#30340;&#21547;&#20041;&#12290;&#20316;&#32773;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#36827;&#34892;&#20102;&#23457;&#35745;&#30740;&#31350;&#65292;&#21457;&#29616; GPT-3 &#22312;&#22238;&#24212;&#27668;&#20505;&#21464;&#21270;&#21644;BBL&#36816;&#21160;&#30340;&#25552;&#31034;&#26102;&#23384;&#22312;&#19981;&#20844;&#24179;&#30340;&#34892;&#20026;&#65292;&#24378;&#21270;&#20102;&#21051;&#26495;&#21360;&#35937;&#65292;&#36793;&#32536;&#21270;&#20102;&#26576;&#20123;&#29305;&#23450;&#30340;&#32676;&#20307;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#26377;&#24517;&#35201;&#35299;&#20915;&#36825;&#20123;&#20559;&#35265;&#65292;&#20197;&#38450;&#27490;AI-powered&#26381;&#21153;&#20013;&#36827;&#19968;&#27493;&#24041;&#22266;&#26435;&#21147;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2209.13627</link><description>&lt;p&gt;
GPT-3 &#22914;&#20309;&#22788;&#29702;&#27668;&#20505;&#21464;&#21270;&#21644;&#8220;&#40657;&#20154;&#30340;&#21629;&#20063;&#26159;&#21629;&#8221;&#31561;&#19981;&#21516;&#20844;&#20247;&#30340;&#35805;&#39064;&#65306;&#20851;&#20110;&#23545;&#35805;&#24335; AI &#20844;&#24179;&#24615;&#30340;&#25209;&#21028;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
How GPT-3 responds to different publics on climate change and Black Lives Matter: A critical appraisal of equity in conversational AI. (arXiv:2209.13627v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#20197;&#26816;&#26597;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#23545;&#35805;&#20013;&#20844;&#24179;&#24615;&#30340;&#21547;&#20041;&#12290;&#20316;&#32773;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#36827;&#34892;&#20102;&#23457;&#35745;&#30740;&#31350;&#65292;&#21457;&#29616; GPT-3 &#22312;&#22238;&#24212;&#27668;&#20505;&#21464;&#21270;&#21644;BBL&#36816;&#21160;&#30340;&#25552;&#31034;&#26102;&#23384;&#22312;&#19981;&#20844;&#24179;&#30340;&#34892;&#20026;&#65292;&#24378;&#21270;&#20102;&#21051;&#26495;&#21360;&#35937;&#65292;&#36793;&#32536;&#21270;&#20102;&#26576;&#20123;&#29305;&#23450;&#30340;&#32676;&#20307;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#26377;&#24517;&#35201;&#35299;&#20915;&#36825;&#20123;&#20559;&#35265;&#65292;&#20197;&#38450;&#27490;AI-powered&#26381;&#21153;&#20013;&#36827;&#19968;&#27493;&#24041;&#22266;&#26435;&#21147;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#36825;&#31181;&#27169;&#22411;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#25509;&#36817;&#20154;&#31867;&#30340;&#25991;&#26412;&#12290;&#36825;&#20123;&#27169;&#22411;&#39537;&#21160;&#30528;&#26234;&#33021;&#20581;&#24247;&#12289;&#37329;&#34701;&#21644;&#33258;&#20027;&#39550;&#39542;&#31561;&#39046;&#22495;&#30340;&#27969;&#34892;&#34394;&#25311;&#21161;&#25163;&#12290;&#34429;&#28982;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#27491;&#22312;&#25913;&#36827;&#65292;&#20294;&#20154;&#20204;&#20173;&#28982;&#25285;&#24515;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#19981;&#21516;&#31243;&#24230;&#22320;&#20026;&#31038;&#20250;&#20013;&#30340;&#25152;&#26377;&#23376;&#32676;&#20307;&#25552;&#20379;&#26381;&#21153;&#12290;&#23613;&#31649;&#36328;&#23398;&#31185;&#30340; AI &#20844;&#24179;&#35752;&#35770;&#36234;&#26469;&#36234;&#22810;&#65292;&#20294;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#20013;&#20844;&#24179;&#30340;&#21547;&#20041;&#65292;&#20197;&#21450;&#22914;&#20309;&#35753;&#19981;&#21516;&#30340;&#20154;&#32676;&#21442;&#19982;&#35780;&#20272;&#24490;&#29615;&#12290;&#26412;&#25991;&#22522;&#20110;&#27665;&#20027;&#20915;&#31574;&#29702;&#35770;&#21644;&#31185;&#23398;&#25216;&#26415;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#26469;&#25581;&#31034;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#23545;&#35805;&#20013;&#20844;&#24179;&#24615;&#30340;&#21547;&#20041;&#12290;&#20351;&#29992;&#36825;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23457;&#35745;&#30740;&#31350;&#65292;&#20197;&#26816;&#26597; GPT-3 &#22914;&#20309;&#21709;&#24212;&#20851;&#38190;&#30340;&#31185;&#23398;&#21644;&#31038;&#20250;&#35805;&#39064;&#65306;&#27668;&#20505;&#21464;&#21270;&#21644;&#8220;&#40657;&#20154;&#30340;&#21629;&#20063;&#26159;&#21629;&#8221;&#36816;&#21160;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#21253;&#25324; 480 &#27425;&#25552;&#31034;&#65292;&#20854;&#20013;&#31995;&#32479;&#22320;&#21464;&#21270;&#21457;&#35328;&#20154;&#30340;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#22320;&#29702;&#20301;&#32622;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;&#22238;&#24212;&#27668;&#20505;&#21464;&#21270;&#21644; BLM &#30340;&#25552;&#31034;&#26102;&#65292;GPT-3 &#26377;&#26102;&#20250;&#24378;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#36793;&#32536;&#21270;&#26576;&#20123;&#32676;&#20307;&#65292;&#22914;&#22303;&#33879;&#27665;&#26063;&#12290;&#25105;&#20204;&#35748;&#20026;&#24517;&#39035;&#35299;&#20915;&#36825;&#20123;&#20559;&#35265;&#65292;&#20197;&#38450;&#27490;&#26435;&#21147;&#32467;&#26500;&#22312; AI &#21160;&#21147;&#26381;&#21153;&#20013;&#36827;&#19968;&#27493;&#24041;&#22266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive language models, which use deep learning to produce human-like texts, have become increasingly widespread. Such models are powering popular virtual assistants in areas like smart health, finance, and autonomous driving. While the parameters of these large language models are improving, concerns persist that these models might not work equally for all subgroups in society. Despite growing discussions of AI fairness across disciplines, there lacks systemic metrics to assess what equity means in dialogue systems and how to engage different populations in the assessment loop. Grounded in theories of deliberative democracy and science and technology studies, this paper proposes an analytical framework for unpacking the meaning of equity in human-AI dialogues. Using this framework, we conducted an auditing study to examine how GPT-3 responded to different sub-populations on crucial science and social topics: climate change and the Black Lives Matter (BLM) movement. Our corpus 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GeoSumm&#30340;&#26032;&#22411;&#31995;&#32479;&#65292;&#29992;&#20110;&#25191;&#34892;&#26080;&#30417;&#30563;&#30340;&#25277;&#21462;&#24335;&#24847;&#35265;&#25688;&#35201;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#30340;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#21644;&#36817;&#20284;&#27979;&#22320;&#32447;&#36317;&#31163;&#30340;&#24471;&#20998;&#26426;&#21046;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#30340;&#21516;&#26102;&#40065;&#26834;&#24615;&#20063;&#24471;&#21040;&#20102;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2209.07496</link><description>&lt;p&gt;
&#20351;&#29992;&#36817;&#20284;&#27979;&#22320;&#32447;&#30340;&#26080;&#30417;&#30563;&#24847;&#35265;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Opinion Summarization Using Approximate Geodesics. (arXiv:2209.07496v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GeoSumm&#30340;&#26032;&#22411;&#31995;&#32479;&#65292;&#29992;&#20110;&#25191;&#34892;&#26080;&#30417;&#30563;&#30340;&#25277;&#21462;&#24335;&#24847;&#35265;&#25688;&#35201;&#12290;&#23427;&#20351;&#29992;&#22522;&#20110;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#30340;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#21644;&#36817;&#20284;&#27979;&#22320;&#32447;&#36317;&#31163;&#30340;&#24471;&#20998;&#26426;&#21046;&#65292;&#22312;&#20445;&#35777;&#24615;&#33021;&#30340;&#21516;&#26102;&#40065;&#26834;&#24615;&#20063;&#24471;&#21040;&#20102;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#35265;&#25688;&#35201;&#26159;&#20174;&#29992;&#25143;&#35780;&#35770;&#20013;&#25429;&#33719;&#27969;&#34892;&#35266;&#28857;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Geodesic Summarizer (GeoSumm)&#30340;&#26032;&#22411;&#31995;&#32479;&#65292;&#29992;&#20110;&#25191;&#34892;&#26080;&#30417;&#30563;&#30340;&#25277;&#21462;&#24335;&#24847;&#35265;&#25688;&#35201;&#12290;GeoSumm&#28041;&#21450;&#22522;&#20110;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;&#30340;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#35299;&#30721;&#22120;&#23618;&#19978;&#23545;&#39044;&#35757;&#32451;&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#35789;&#20856;&#23398;&#20064;&#26469;&#29983;&#25104;&#25991;&#26412;&#30340;&#34920;&#31034;&#24418;&#24335;&#20026;&#28508;&#22312;&#35821;&#20041;&#21333;&#20803;&#30340;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#34920;&#31034;&#26469;&#37327;&#21270;&#35780;&#35770;&#21477;&#23376;&#30340;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#36817;&#20284;&#27979;&#22320;&#32447;&#36317;&#31163;&#30340;&#24471;&#20998;&#26426;&#21046;&#12290;&#25105;&#20204;&#20351;&#29992;&#30456;&#20851;&#24615;&#35780;&#20998;&#26469;&#35782;&#21035;&#27969;&#34892;&#35266;&#28857;&#65292;&#20174;&#32780;&#32452;&#25104;&#26222;&#36941;&#24615;&#21644;&#26041;&#38754;&#29305;&#23450;&#30340;&#25688;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;GeoSumm&#22312;&#19977;&#20010;&#24847;&#35265;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#39069;&#22806;&#30340;&#23454;&#39564;&#26469;&#20998;&#26512;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#21151;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#29983;&#25104;&#30340;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opinion summarization is the task of creating summaries capturing popular opinions from user reviews. In this paper, we introduce Geodesic Summarizer (GeoSumm), a novel system to perform unsupervised extractive opinion summarization. GeoSumm involves an encoder-decoder based representation learning model, that generates representations of text as a distribution over latent semantic units. GeoSumm generates these representations by performing dictionary learning over pre-trained text representations at multiple decoder layers. We then use these representations to quantify the relevance of review sentences using a novel approximate geodesic distance based scoring mechanism. We use the relevance scores to identify popular opinions in order to compose general and aspect-specific summaries. Our proposed model, GeoSumm, achieves state-of-the-art performance on three opinion summarization datasets. We perform additional experiments to analyze the functioning of our model and showcase the gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#21512;&#36974;&#34109;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#65292;&#22312;&#36328;&#27169;&#24577;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#25104;&#26524;&#65292;&#24182;&#22312;&#30334;&#19975;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#20869;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.02131</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#36974;&#34109;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Masked Vision and Language Modeling for Multi-modal Representation Learning. (arXiv:2208.02131v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32852;&#21512;&#36974;&#34109;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#65292;&#22312;&#36328;&#27169;&#24577;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#25104;&#26524;&#65292;&#24182;&#22312;&#30334;&#19975;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#20869;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#65288;V + L&#65289;&#34920;&#31034;&#23398;&#20064;&#20013;&#20351;&#29992;&#36974;&#34109;&#20449;&#21495;&#24314;&#27169;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24314;&#31435;&#32852;&#21512;&#36974;&#34109;&#35270;&#35273;&#21644;&#35821;&#35328;&#24314;&#27169;&#65292;&#20854;&#20013;&#19968;&#20010;&#27169;&#24577;&#30340;&#36974;&#34109;&#20449;&#21495;&#22312;&#21478;&#19968;&#20010;&#27169;&#24577;&#30340;&#24110;&#21161;&#19979;&#36827;&#34892;&#37325;&#24314;&#12290;&#36825;&#26159;&#30001;&#22270;&#20687;&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#30340;&#24615;&#36136;&#25152;&#39537;&#21160;&#30340;&#65292;&#22240;&#20026;&#22270;&#20687;&#21644;&#25991;&#26412;&#37117;&#20256;&#36798;&#20960;&#20046;&#30456;&#21516;&#30340;&#20449;&#24687;&#20294;&#20197;&#19981;&#21516;&#30340;&#26684;&#24335;&#21576;&#29616;&#12290;&#19968;&#20010;&#27169;&#24577;&#30340;&#36974;&#34109;&#20449;&#21495;&#37325;&#24314;&#20197;&#21478;&#19968;&#27169;&#24577;&#20026;&#26465;&#20214;&#20063;&#21487;&#20197;&#38544;&#24335;&#22320;&#23398;&#20064;&#35821;&#35328;&#26631;&#35760;&#21644;&#22270;&#20687;&#34917;&#19969;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#23545;&#40784;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;V + L&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#36830;&#21516;&#24120;&#35265;&#30340;V + L&#23545;&#40784;&#25439;&#22833;&#65292;&#22312;&#30334;&#19975;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#20869;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#36229;&#36807;&#20102;&#20854;&#20182;&#31454;&#20105;&#23545;&#25163;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study how to use masked signal modeling in vision and language (V+L) representation learning. Instead of developing masked language modeling (MLM) and masked image modeling (MIM) independently, we propose to build joint masked vision and language modeling, where the masked signal of one modality is reconstructed with the help from another modality. This is motivated by the nature of image-text paired data that both of the image and the text convey almost the same information but in different formats. The masked signal reconstruction of one modality conditioned on another modality can also implicitly learn cross-modal alignment between language tokens and image patches. Our experiments on various V+L tasks show that the proposed method, along with common V+L alignment losses, achieves state-of-the-art performance in the regime of millions of pre-training data. Also, we outperforms the other competitors by a significant margin in limited data scenarios.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38463;&#25289;&#20271;&#35821;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2201.09227</link><description>&lt;p&gt;
&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#20803;&#21270;&#30340;&#38463;&#25289;&#20271;&#35821;&#35821;&#26009;&#24211;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
A Large and Diverse Arabic Corpus for Language Modeling. (arXiv:2201.09227v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.09227
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#38463;&#25289;&#20271;&#35821;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24341;&#20837;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24314;&#27169;&#30340;&#37325;&#22823;&#33539;&#24335;&#36716;&#21464;&#65292;&#20854;&#20013;&#22823;&#22411;&#39044;&#20808;&#35757;&#32451;&#30340;LM&#24050;&#32463;&#25104;&#20026;&#22823;&#22810;&#25968;NLP&#20219;&#21153;&#19981;&#21487;&#20998;&#21106;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;LM&#36275;&#22815;&#26234;&#33021;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#35821;&#35328;&#30340;&#26377;&#29992;&#21644;&#30456;&#20851;&#34920;&#31034;&#12290;&#36825;&#20123;&#27169;&#22411;&#34987;&#29992;&#20110;&#23545;&#24120;&#35268;NLP&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20855;&#26377;&#26174;&#30528;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#30456;&#21453;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#38656;&#35201;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35821;&#26009;&#24211;&#65292;&#36825;&#20010;&#35821;&#26009;&#24211;&#21487;&#20197;&#24456;&#22909;&#22320;&#20195;&#34920;&#38463;&#25289;&#20271;&#35821;&#12290;&#30001;&#20110;&#33521;&#35821;&#35821;&#26009;&#24211;&#21487;&#33719;&#24471;&#22823;&#37327;&#36164;&#28304;&#65292;&#22240;&#27492;&#33521;&#35821;LM&#36890;&#24120;&#27604;&#20854;&#20182;&#35821;&#35328;LM&#34920;&#29616;&#26356;&#22909;&#12290;&#26412;&#25991;&#35814;&#32454;&#25551;&#36848;&#20102;&#19968;&#20010;&#22823;&#22411;&#38463;&#25289;&#20271;&#35821;&#35821;&#26009;&#24211;&#30340;&#35774;&#35745;&#21644;&#24320;&#21457;&#12290;&#23427;&#30001;&#36229;&#36807;500GB&#30340;&#24050;&#21152;&#24037;&#30340;&#38463;&#25289;&#20271;&#25991;&#26412;&#32452;&#25104;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#30693;&#35782;&#21644;&#19979;&#28216;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#35813;&#35821;&#26009;&#24211;&#36824;&#29992;&#20110;&#35757;&#32451;&#22823;&#22411;&#38463;&#25289;&#20271;&#35821;LM&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have introduced a major paradigm shift in Natural Language Processing (NLP) modeling where large pre-trained LMs became integral to most of the NLP tasks. The LMs are intelligent enough to find useful and relevant representations of the language without any supervision. Perhaps, these models are used to fine-tune typical NLP tasks with significantly high accuracy as compared to the traditional approaches. Conversely, the training of these models requires a massively large corpus that is a good representation of the language. English LMs generally perform better than their other language counterparts, due to the availability of massive English corpora. This work elaborates on the design and development of a large Arabic corpus. It consists of over 500 GB of Arabic cleaned text targeted at improving cross-domain knowledge and downstream generalization capability of large-scale language models. Moreover, the corpus is utilized in the training of a large Arabic LM. In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Label Mask &#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65288;LM-MTC&#65289;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#25429;&#25417;&#26631;&#31614;&#20043;&#38388;&#30340;&#38544;&#21547;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#26631;&#31614;&#30340;&#36974;&#30422;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2106.10076</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#26631;&#31614;&#25552;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Label prompt for multi-label text classification. (arXiv:2106.10076v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.10076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; Label Mask &#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65288;LM-MTC&#65289;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#25429;&#25417;&#26631;&#31614;&#20043;&#38388;&#30340;&#38544;&#21547;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#26631;&#31614;&#30340;&#36974;&#30422;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#22914;&#20309;&#21033;&#29992;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#20294;&#26159;&#65292;&#22312;&#19968;&#20010;&#22797;&#26434;&#21644;&#26410;&#30693;&#30340;&#26631;&#31614;&#31354;&#38388;&#20013;&#30452;&#25509;&#24314;&#27169;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181; Label Mask &#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65288;LM-MTC&#65289;&#65292;&#23427;&#21463;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#22635;&#31354;&#38382;&#39064;&#24605;&#24819;&#30340;&#21551;&#21457;&#12290;LM-MTC &#33021;&#22815;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#25429;&#25417;&#26631;&#31614;&#20043;&#38388;&#30340;&#38544;&#21547;&#20851;&#31995;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#28508;&#22312;&#26631;&#31614;&#20998;&#37197;&#19968;&#20010;&#19981;&#21516;&#30340;&#26631;&#35760;&#65292;&#24182;&#20197;&#19968;&#23450;&#27010;&#29575;&#38543;&#26426;&#36974;&#30422;&#35813;&#26631;&#35760;&#65292;&#24314;&#31435;&#20102;&#22522;&#20110;&#26631;&#31614;&#30340;&#36974;&#25513;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#12290;&#25105;&#20204;&#21516;&#26102;&#35757;&#32451; MTC &#21644; MLM&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the key problems in multi-label text classification is how to take advantage of the correlation among labels. However, it is very challenging to directly model the correlations among labels in a complex and unknown label space. In this paper, we propose a Label Mask multi-label text classification model (LM-MTC), which is inspired by the idea of cloze questions of language model. LM-MTC is able to capture implicit relationships among labels through the powerful ability of pre-train language models. On the basis, we assign a different token to each potential label, and randomly mask the token with a certain probability to build a label based Masked Language Model (MLM). We train the MTC and MLM together, further improving the generalization ability of the model. A large number of experiments on multiple datasets demonstrate the effectiveness of our method.
&lt;/p&gt;</description></item></channel></rss>