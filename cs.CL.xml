<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;FEDI&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12289;&#29992;&#25143;&#24773;&#32490;&#21644;&#38544;&#21547;&#21453;&#39304;&#23545;&#20219;&#21153;&#23548;&#21521;&#30340;&#25991;&#26723;&#23545;&#35805;&#36827;&#34892;&#27880;&#37322;&#30340;&#33521;&#25991;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#25968;&#25454;&#26377;&#28508;&#21147;&#25913;&#21892;&#20219;&#21153;&#23436;&#25104;&#24773;&#20917;&#12289;&#29983;&#25104;&#21709;&#24212;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#21644;&#29992;&#25143;&#25509;&#21463;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.09248</link><description>&lt;p&gt;
&#20174;&#24773;&#32490;&#12289;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#21644;&#38544;&#21547;&#29992;&#25143;&#21453;&#39304;&#20013;&#23398;&#20064;&#20219;&#21153;&#23548;&#21521;&#30340;&#25991;&#26723;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Learning from Emotions, Demographic Information and Implicit User Feedback in Task-Oriented Document-Grounded Dialogues. (arXiv:2401.09248v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;FEDI&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12289;&#29992;&#25143;&#24773;&#32490;&#21644;&#38544;&#21547;&#21453;&#39304;&#23545;&#20219;&#21153;&#23548;&#21521;&#30340;&#25991;&#26723;&#23545;&#35805;&#36827;&#34892;&#27880;&#37322;&#30340;&#33521;&#25991;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#25968;&#25454;&#26377;&#28508;&#21147;&#25913;&#21892;&#20219;&#21153;&#23436;&#25104;&#24773;&#20917;&#12289;&#29983;&#25104;&#21709;&#24212;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#21644;&#29992;&#25143;&#25509;&#21463;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#21644;&#25991;&#26723;&#23545;&#35805;&#31995;&#32479;&#30340;&#25104;&#21151;&#21462;&#20915;&#20110;&#29992;&#25143;&#25509;&#21463;&#21644;&#20139;&#21463;&#20351;&#29992;&#23427;&#20204;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#34920;&#26126;&#65292;&#32771;&#34385;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12289;&#29992;&#25143;&#24773;&#32490;&#24182;&#20174;&#20182;&#20204;&#30340;&#35805;&#35821;&#20013;&#23398;&#20064;&#38544;&#21547;&#21453;&#39304;&#30340;&#32452;&#21512;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21457;&#29616;&#23578;&#26410;&#36716;&#31227;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#36825;&#20123;&#25968;&#25454;&#20027;&#35201;&#26159;&#20998;&#21035;&#30740;&#31350;&#30340;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#27809;&#26377;&#36275;&#22815;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21487;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FEDI&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12289;&#29992;&#25143;&#24773;&#32490;&#21644;&#38544;&#21547;&#21453;&#39304;&#23545;&#20219;&#21153;&#23548;&#21521;&#30340;&#25991;&#26723;&#23545;&#35805;&#36827;&#34892;&#27880;&#37322;&#30340;&#33521;&#25991;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;FLAN-T5&#12289;GPT-2&#21644;LLaMA-2&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#25968;&#25454;&#26377;&#28508;&#21147;&#25913;&#21892;&#20219;&#21153;&#23436;&#25104;&#24773;&#20917;&#12289;&#29983;&#25104;&#21709;&#24212;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#21644;&#29992;&#25143;&#25509;&#21463;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of task-oriented and document-grounded dialogue systems depends on users accepting and enjoying using them. To achieve this, recently published work in the field of Human-Computer Interaction suggests that the combination of considering demographic information, user emotions and learning from the implicit feedback in their utterances, is particularly important. However, these findings have not yet been transferred to the field of Natural Language Processing, where these data are primarily studied separately. Accordingly, no sufficiently annotated dataset is available. To address this gap, we introduce FEDI, the first English dialogue dataset for task-oriented document-grounded dialogues annotated with demographic information, user emotions and implicit feedback. Our experiments with FLAN-T5, GPT-2 and LLaMA-2 show that these data have the potential to improve task completion and the factual consistency of the generated responses and user acceptance.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#31038;&#20132;&#23186;&#20307;&#20013;&#36328;&#35821;&#35328;&#20882;&#29359;&#24615;&#35821;&#35328;&#26816;&#27979;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20998;&#26512;&#20102;67&#31687;&#30456;&#20851;&#35770;&#25991;&#65292;&#24182;&#24635;&#32467;&#20102;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#30340;&#19977;&#31181;&#20027;&#35201;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36824;&#25506;&#35752;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2401.09244</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#20882;&#29359;&#24615;&#35821;&#35328;&#26816;&#27979;&#65306;&#25968;&#25454;&#38598;&#12289;&#36716;&#31227;&#26041;&#27861;&#21644;&#25361;&#25112;&#30340;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Offensive Language Detection: A Systematic Review of Datasets, Transfer Approaches and Challenges. (arXiv:2401.09244v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09244
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#31038;&#20132;&#23186;&#20307;&#20013;&#36328;&#35821;&#35328;&#20882;&#29359;&#24615;&#35821;&#35328;&#26816;&#27979;&#30340;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20998;&#26512;&#20102;67&#31687;&#30456;&#20851;&#35770;&#25991;&#65292;&#24182;&#24635;&#32467;&#20102;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#30340;&#19977;&#31181;&#20027;&#35201;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36824;&#25506;&#35752;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#20882;&#29359;&#24615;&#35821;&#35328;&#30340;&#26222;&#21450;&#21644;&#36805;&#36895;&#21457;&#23637;&#22686;&#21152;&#20102;&#26816;&#27979;&#30340;&#22797;&#26434;&#24615;&#65292;&#23588;&#20854;&#31361;&#26174;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#35782;&#21035;&#27492;&#31867;&#20869;&#23481;&#30340;&#25361;&#25112;&#12290;&#26412;&#32508;&#36848;&#31995;&#32479;&#24615;&#12289;&#20840;&#38754;&#22320;&#25506;&#35752;&#20102;&#31038;&#20132;&#23186;&#20307;&#20013;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#65288;CLTL&#65289;&#25216;&#26415;&#22312;&#20882;&#29359;&#24615;&#35821;&#35328;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26159;&#35813;&#39046;&#22495;&#39318;&#20010;&#19987;&#27880;&#20110;&#36328;&#35821;&#35328;&#24773;&#26223;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;67&#31687;&#30456;&#20851;&#35770;&#25991;&#65292;&#24182;&#23545;&#36825;&#20123;&#30740;&#31350;&#36827;&#34892;&#20102;&#21508;&#20010;&#32500;&#24230;&#30340;&#20998;&#31867;&#65292;&#21253;&#25324;&#20351;&#29992;&#30340;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#30340;&#29305;&#24449;&#12289;&#20351;&#29992;&#30340;&#36328;&#35821;&#35328;&#36164;&#28304;&#20197;&#21450;&#23454;&#26045;&#30340;&#20855;&#20307;CLTL&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#26681;&#25454;&#8220;&#20309;&#31181;&#36716;&#31227;&#8221;&#65292;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#19977;&#31181;&#20027;&#35201;&#30340;CLTL&#36716;&#31227;&#26041;&#27861;&#65306;&#23454;&#20363;&#12289;&#29305;&#24449;&#21644;&#21442;&#25968;&#36716;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#35813;&#39046;&#22495;&#24403;&#21069;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26426;&#20250;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing prevalence and rapid evolution of offensive language in social media amplify the complexities of detection, particularly highlighting the challenges in identifying such content across diverse languages. This survey presents a systematic and comprehensive exploration of Cross-Lingual Transfer Learning (CLTL) techniques in offensive language detection in social media. Our study stands as the first holistic overview to focus exclusively on the cross-lingual scenario in this domain. We analyse 67 relevant papers and categorise these studies across various dimensions, including the characteristics of multilingual datasets used, the cross-lingual resources employed, and the specific CLTL strategies implemented. According to "what to transfer", we also summarise three main CLTL transfer approaches: instance, feature, and parameter transfer. Additionally, we shed light on the current challenges and future research opportunities in this field. Furthermore, we have made our survey re
&lt;/p&gt;</description></item><item><title>UniVIE&#26159;&#19968;&#31181;&#23558;VIE&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#20851;&#31995;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#23558;&#19981;&#21516;&#20219;&#21153;&#30340;&#26631;&#31614;&#32479;&#19968;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#26631;&#31614;&#31354;&#38388;&#20013;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#30001;&#31895;&#21040;&#32454;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#34920;&#21333;&#24335;&#25991;&#26723;&#20013;&#30340;&#23618;&#27425;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.09220</link><description>&lt;p&gt;
UniVIE:&#19968;&#31181;&#20174;&#34920;&#21333;&#24335;&#25991;&#26723;&#20013;&#25552;&#21462;&#35270;&#35273;&#20449;&#24687;&#30340;&#32479;&#19968;&#26631;&#31614;&#31354;&#38388;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
UniVIE: A Unified Label Space Approach to Visual Information Extraction from Form-like Documents. (arXiv:2401.09220v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09220
&lt;/p&gt;
&lt;p&gt;
UniVIE&#26159;&#19968;&#31181;&#23558;VIE&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#20851;&#31995;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#23558;&#19981;&#21516;&#20219;&#21153;&#30340;&#26631;&#31614;&#32479;&#19968;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#26631;&#31614;&#31354;&#38388;&#20013;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#30001;&#31895;&#21040;&#32454;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#34920;&#21333;&#24335;&#25991;&#26723;&#20013;&#30340;&#23618;&#27425;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20174;&#34920;&#21333;&#24335;&#25991;&#26723;&#20013;&#25552;&#21462;&#35270;&#35273;&#20449;&#24687;(VIE)&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#35813;&#36807;&#31243;&#20998;&#21106;&#20026;&#29420;&#31435;&#30340;&#23376;&#20219;&#21153;&#65292;&#20363;&#22914;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#12289;&#38190;&#20540;&#23545;&#25552;&#21462;&#21644;&#36873;&#25321;&#32676;&#25552;&#21462;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#20102;&#34920;&#21333;&#25991;&#26723;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#21253;&#25324;&#23618;&#27425;&#38190;&#20540;&#23545;&#21644;&#23618;&#27425;&#36873;&#25321;&#32676;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#23558;VIE&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#20851;&#31995;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#23558;&#19981;&#21516;&#20219;&#21153;&#30340;&#26631;&#31614;&#32479;&#19968;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#26631;&#31614;&#31354;&#38388;&#20013;&#12290;&#36825;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#20801;&#35768;&#23450;&#20041;&#21508;&#31181;&#20851;&#31995;&#31867;&#22411;&#65292;&#24182;&#26377;&#25928;&#22320;&#22788;&#29702;&#34920;&#21333;&#24335;&#25991;&#26723;&#20013;&#30340;&#23618;&#27425;&#20851;&#31995;&#12290;&#26681;&#25454;&#36825;&#20010;&#35266;&#28857;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;UniVIE&#65292;&#23427;&#26159;&#19968;&#20010;&#20840;&#38754;&#35299;&#20915;VIE&#38382;&#39064;&#30340;&#32479;&#19968;&#27169;&#22411;&#12290;UniVIE&#20351;&#29992;&#19968;&#20010;&#30001;&#31895;&#21040;&#32454;&#30340;&#31574;&#30053;&#36827;&#34892;&#25805;&#20316;&#12290;&#23427;&#39318;&#20808;&#36890;&#36807;&#19968;&#20010;&#26641;&#25552;&#26696;&#32593;&#32476;&#29983;&#25104;&#26641;&#25552;&#26696;&#65292;&#28982;&#21518;&#23558;&#20854;&#32454;&#21270;&#20026;&#23618;&#27425;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods for Visual Information Extraction (VIE) from form-like documents typically fragment the process into separate subtasks, such as key information extraction, key-value pair extraction, and choice group extraction. However, these approaches often overlook the hierarchical structure of form documents, including hierarchical key-value pairs and hierarchical choice groups. To address these limitations, we present a new perspective, reframing VIE as a relation prediction problem and unifying labels of different tasks into a single label space. This unified approach allows for the definition of various relation types and effectively tackles hierarchical relationships in form-like documents. In line with this perspective, we present UniVIE, a unified model that addresses the VIE problem comprehensively. UniVIE functions using a coarse-to-fine strategy. It initially generates tree proposals through a tree proposal network, which are subsequently refined into hierarchical trees b
&lt;/p&gt;</description></item><item><title>QAnswer&#26159;&#19968;&#31181;&#38754;&#21521;&#32593;&#31449;&#30340;&#38382;&#31572;&#25628;&#32034;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#30693;&#35782;&#22270;&#35889;&#21644;&#33258;&#30001;&#25991;&#26412;&#30340;QA&#25216;&#26415;&#12290;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#23558;QA&#25216;&#26415;&#29992;&#20110;&#32593;&#31449;&#25628;&#32034;&#30340;&#28508;&#21147;&#65292;&#24182;&#35752;&#35770;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2401.09175</link><description>&lt;p&gt;
QAnswer: &#38754;&#21521;&#32593;&#31449;&#30340;&#38382;&#31572;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
QAnswer: Towards Question Answering Search over Websites. (arXiv:2401.09175v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09175
&lt;/p&gt;
&lt;p&gt;
QAnswer&#26159;&#19968;&#31181;&#38754;&#21521;&#32593;&#31449;&#30340;&#38382;&#31572;&#25628;&#32034;&#31995;&#32479;&#65292;&#32467;&#21512;&#20102;&#30693;&#35782;&#22270;&#35889;&#21644;&#33258;&#30001;&#25991;&#26412;&#30340;QA&#25216;&#26415;&#12290;&#35813;&#31995;&#32479;&#23637;&#31034;&#20102;&#23558;QA&#25216;&#26415;&#29992;&#20110;&#32593;&#31449;&#25628;&#32034;&#30340;&#28508;&#21147;&#65292;&#24182;&#35752;&#35770;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#65288;QA&#65289;&#36234;&#26469;&#36234;&#34987;&#25628;&#32034;&#24341;&#25806;&#29992;&#20110;&#20026;&#29992;&#25143;&#25552;&#20379;&#32467;&#26524;&#65292;&#28982;&#32780;&#30446;&#21069;&#24456;&#23569;&#26377;&#32593;&#31449;&#20351;&#29992;QA&#25216;&#26415;&#36827;&#34892;&#25628;&#32034;&#21151;&#33021;&#12290;&#20026;&#20102;&#23637;&#31034;QA&#25216;&#26415;&#22312;&#32593;&#31449;&#25628;&#32034;&#20013;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;QA&#21644;&#33258;&#30001;&#25991;&#26412;QA&#30340;&#32593;&#32476;&#25628;&#32034;&#65292;&#36825;&#20123;&#36890;&#24120;&#20998;&#21035;&#36827;&#34892;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20004;&#31181;&#26041;&#27861;&#22312;&#32593;&#31449;&#25628;&#32034;&#20013;&#30340;&#19981;&#21516;&#20248;&#32570;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#30001;&#32500;&#22522;&#23186;&#20307;&#22522;&#37329;&#20250;&#25176;&#31649;&#30340;&#32593;&#31449;&#26696;&#20363;&#30740;&#31350;&#65288;&#21363;&#32500;&#22522;&#30334;&#31185;&#21644;&#32500;&#22522;&#25968;&#25454;&#65289;&#12290;&#19982;&#25628;&#32034;&#24341;&#25806;&#65288;&#22914;&#35895;&#27468;&#65292;&#24517;&#24212;&#31561;&#65289;&#19981;&#21516;&#65292;&#25968;&#25454;&#34987;&#23436;&#25972;&#32034;&#24341;&#65292;&#21363;&#25105;&#20204;&#19981;&#20165;&#32034;&#24341;&#23376;&#38598;&#65292;&#32780;&#19988;&#23427;&#20204;&#30340;&#32034;&#24341;&#26159;&#19987;&#23646;&#30340;&#65292;&#21363;&#25105;&#20204;&#20165;&#32034;&#24341;&#30456;&#24212;&#32593;&#31449;&#19978;&#21487;&#29992;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question Answering (QA) is increasingly used by search engines to provide results to their end-users, yet very few websites currently use QA technologies for their search functionality. To illustrate the potential of QA technologies for the website search practitioner, we demonstrate web searches that combine QA over knowledge graphs and QA over free text -- each being usually tackled separately. We also discuss the different benefits and drawbacks of both approaches for web site searches. We use the case studies made of websites hosted by the Wikimedia Foundation (namely Wikipedia and Wikidata). Differently from a search engine (e.g. Google, Bing, etc), the data are indexed integrally, i.e. we do not index only a subset, and they are indexed exclusively, i.e. we index only data available on the corresponding website.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#20302;&#39044;&#31639;&#19979;Fine-tuning QA&#27169;&#22411;&#30340;&#31574;&#30053;&#36827;&#34892;&#20102;&#30740;&#31350;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;Fine-tuning&#65292;&#24182;&#32467;&#21512;&#30446;&#26631;&#25968;&#25454;&#38598;&#21644;SQuAD&#25968;&#25454;&#38598;&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09168</link><description>&lt;p&gt;
&#20302;&#26631;&#27880;&#39044;&#31639;&#32422;&#26463;&#19979;&#22495;&#29305;&#23450;&#38382;&#31572;Fine-tuning&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Strategies for Domain Specific Question Answering under Low Annotation Budget Constraints. (arXiv:2401.09168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09168
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#20302;&#39044;&#31639;&#19979;Fine-tuning QA&#27169;&#22411;&#30340;&#31574;&#30053;&#36827;&#34892;&#20102;&#30740;&#31350;&#20998;&#26512;&#65292;&#21457;&#29616;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;Fine-tuning&#65292;&#24182;&#32467;&#21512;&#30446;&#26631;&#25968;&#25454;&#38598;&#21644;SQuAD&#25968;&#25454;&#38598;&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#39069;&#22806;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;Fine-tuning&#30340;&#36827;&#23637;&#22312;&#22823;&#22810;&#25968;&#19979;&#28216;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#22312;&#20302;&#38382;&#31572;&#26631;&#27880;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#31574;&#30053;&#23545;&#20110;Fine-tuning QA&#27169;&#22411;&#32780;&#35328;&#24182;&#19981;&#26159;&#26368;&#20248;&#30340;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;QA&#25968;&#25454;&#38598;&#19978;&#39034;&#24207;Fine-tuning&#31574;&#30053;&#30340;&#24615;&#33021;&#36827;&#34892;&#35814;&#23613;&#20998;&#26512;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#23545;&#20110;&#20302;&#39044;&#31639;&#35774;&#32622;&#19979;Fine-tuning QA&#27169;&#22411;&#30340;&#26368;&#20339;&#31574;&#30053;&#26159;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#24182;&#23558;PLM&#19982;&#30446;&#26631;&#25968;&#25454;&#38598;&#21644;SQuAD&#25968;&#25454;&#38598;&#36827;&#34892;Fine-tuning&#12290;&#22312;&#27809;&#26377;&#39069;&#22806;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#31574;&#30053;&#20248;&#20110;&#26631;&#20934;&#30340;baseline&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress introduced by pre-trained language models and their fine-tuning has resulted in significant improvements in most downstream NLP tasks. The unsupervised training of a language model combined with further target task fine-tuning has become the standard QA fine-tuning procedure. In this work, we demonstrate that this strategy is sub-optimal for fine-tuning QA models, especially under a low QA annotation budget, which is a usual setting in practice due to the extractive QA labeling cost. We draw our conclusions by conducting an exhaustive analysis of the performance of the alternatives of the sequential fine-tuning strategy on different QA datasets. Based on the experiments performed, we observed that the best strategy to fine-tune the QA model in low-budget settings is taking a pre-trained language model (PLM) and then fine-tuning PLM with a dataset composed of the target dataset and SQuAD dataset. With zero extra annotation effort, the best strategy outperforms the standard 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#33258;&#21160;&#21270;&#23398;&#26415;&#35770;&#25991;&#35299;&#35835;&#31995;&#32479;(MMAPIS)&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#21319;&#21151;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#30340;&#22788;&#29702;&#38454;&#27573;&#65292;&#24182;&#38024;&#23545;&#29616;&#26377;&#27169;&#22411;&#38754;&#20020;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#65292;&#38271;&#25991;&#26412;&#25688;&#35201;&#21644;&#22810;&#26679;&#21270;&#29992;&#25143;&#30028;&#38754;&#12290;</title><link>http://arxiv.org/abs/2401.09150</link><description>&lt;p&gt;
&#26550;&#36215;&#30740;&#31350;&#19982;&#35835;&#32773;&#20043;&#38388;&#30340;&#26725;&#26753;&#65306;&#19968;&#20010;&#22810;&#27169;&#24577;&#33258;&#21160;&#21270;&#23398;&#26415;&#35770;&#25991;&#35299;&#35835;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Bridging Research and Readers: A Multi-Modal Automated Academic Papers Interpretation System. (arXiv:2401.09150v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09150
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#33258;&#21160;&#21270;&#23398;&#26415;&#35770;&#25991;&#35299;&#35835;&#31995;&#32479;(MMAPIS)&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#21319;&#21151;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#30340;&#22788;&#29702;&#38454;&#27573;&#65292;&#24182;&#38024;&#23545;&#29616;&#26377;&#27169;&#22411;&#38754;&#20020;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25968;&#25454;&#22788;&#29702;&#65292;&#38271;&#25991;&#26412;&#25688;&#35201;&#21644;&#22810;&#26679;&#21270;&#29992;&#25143;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20195;&#20449;&#24687;&#26102;&#20195;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#26174;&#33879;&#21152;&#24555;&#20102;&#31185;&#23398;&#25991;&#29486;&#30340;&#20256;&#25773;&#36895;&#24230;&#65292;&#31185;&#23398;&#25991;&#29486;&#30340;&#22686;&#38271;&#36798;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#27700;&#24179;&#12290;&#30740;&#31350;&#20154;&#21592;&#36843;&#20999;&#38656;&#35201;&#39640;&#25928;&#30340;&#24037;&#20855;&#26469;&#38405;&#35835;&#21644;&#24635;&#32467;&#23398;&#26415;&#35770;&#25991;&#65292;&#21457;&#29616;&#37325;&#35201;&#30340;&#31185;&#23398;&#25991;&#29486;&#65292;&#24182;&#36816;&#29992;&#22810;&#31181;&#35299;&#37322;&#26041;&#27861;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#36843;&#20999;&#38656;&#27714;&#65292;&#33258;&#21160;&#21270;&#31185;&#23398;&#25991;&#29486;&#35299;&#35835;&#31995;&#32479;&#30340;&#20316;&#29992;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21830;&#19994;&#21644;&#24320;&#28304;&#27169;&#22411;&#37117;&#38754;&#20020;&#30528;&#19968;&#20123;&#22256;&#38590;&#65306;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#38590;&#20197;&#24635;&#32467;&#36807;&#38271;&#30340;&#25991;&#26412;&#65292;&#24182;&#32570;&#20047;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#30028;&#38754;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#27169;&#24577;&#33258;&#21160;&#21270;&#23398;&#26415;&#35770;&#25991;&#35299;&#35835;&#31995;&#32479;&#65288;MMAPIS&#65289;&#65292;&#23427;&#20855;&#26377;&#19977;&#20010;&#27493;&#39588;&#30340;&#22788;&#29702;&#38454;&#27573;&#65292;&#24182;&#20351;&#29992;LLMs&#26469;&#22686;&#24378;&#20854;&#21151;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#39318;&#20808;&#21033;&#29992;&#28151;&#21512;&#27169;&#24577;&#39044;&#22788;&#29702;&#21644;&#23545;&#40784;&#27169;&#22359;&#25552;&#21462;&#32431;&#25991;&#26412;&#65292;
&lt;/p&gt;
&lt;p&gt;
In the contemporary information era, significantly accelerated by the advent of Large-scale Language Models, the proliferation of scientific literature is reaching unprecedented levels. Researchers urgently require efficient tools for reading and summarizing academic papers, uncovering significant scientific literature, and employing diverse interpretative methodologies. To address this burgeoning demand, the role of automated scientific literature interpretation systems has become paramount. However, prevailing models, both commercial and open-source, confront notable challenges: they often overlook multimodal data, grapple with summarizing over-length texts, and lack diverse user interfaces. In response, we introduce an open-source multi-modal automated academic paper interpretation system (MMAPIS) with three-step process stages, incorporating LLMs to augment its functionality. Our system first employs the hybrid modality preprocessing and alignment module to extract plain text, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24322;&#27493;Local-SGD&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#24322;&#27493;&#26356;&#26032;&#26356;&#39057;&#32321;&#65292;&#20294;&#20854;&#25910;&#25947;&#25152;&#38656;&#30340;&#36845;&#20195;&#27425;&#25968;&#22810;&#20110;&#21516;&#27493;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24310;&#36831;&#30340;Nesterov&#21160;&#37327;&#26356;&#26032;&#36827;&#34892;&#35843;&#25972;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#24322;&#27493;&#26356;&#26032;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.09135</link><description>&lt;p&gt;
&#24322;&#27493;Local-SGD&#35757;&#32451;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Asynchronous Local-SGD Training for Language Modeling. (arXiv:2401.09135v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24322;&#27493;Local-SGD&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#24322;&#27493;&#26356;&#26032;&#26356;&#39057;&#32321;&#65292;&#20294;&#20854;&#25910;&#25947;&#25152;&#38656;&#30340;&#36845;&#20195;&#27425;&#25968;&#22810;&#20110;&#21516;&#27493;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24310;&#36831;&#30340;Nesterov&#21160;&#37327;&#26356;&#26032;&#36827;&#34892;&#35843;&#25972;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#24322;&#27493;&#26356;&#26032;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Local&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(Local-SGD)&#65292;&#20063;&#31216;&#20026;&#32852;&#37030;&#24179;&#22343;&#65292;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#35774;&#22791;&#22312;&#36890;&#20449;&#20013;&#25191;&#34892;&#22810;&#20010;SGD&#26356;&#26032;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#24322;&#27493;Local-SGD&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#39564;&#35777;&#30740;&#31350;&#65307;&#21363;&#65292;&#27599;&#20010;&#24037;&#20316;&#33410;&#28857;&#22312;&#23436;&#25104;&#20854;SGD&#27493;&#39588;&#21518;&#31435;&#21363;&#26356;&#26032;&#20840;&#23616;&#21442;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#23519;&#24037;&#20316;&#33410;&#28857;&#30828;&#20214;&#24322;&#26500;&#24615;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#24037;&#20316;&#33410;&#28857;&#25968;&#37327;&#21644;&#20248;&#21270;&#22120;&#31561;&#22240;&#32032;&#23545;&#23398;&#20064;&#24615;&#33021;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#26356;&#39057;&#32321;&#22320;&#26356;&#26032;&#65288;&#20840;&#23616;&#65289;&#27169;&#22411;&#21442;&#25968;&#65292;&#20294;&#24322;&#27493;Local-SGD&#27604;&#20854;&#21516;&#27493;&#23545;&#24212;&#29289;&#38656;&#35201;&#26356;&#22810;&#36845;&#20195;&#25165;&#33021;&#25910;&#25947;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#24037;&#20316;&#33410;&#28857;&#26799;&#24230;&#38472;&#26087;&#26102;&#20840;&#23616;&#21442;&#25968;&#30340;&#21160;&#37327;&#21152;&#36895;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24310;&#36831;&#30340;Nesterov&#21160;&#37327;&#26356;&#26032;&#65292;&#26681;&#25454;&#24037;&#20316;&#33410;&#28857;&#30340;&#26412;&#22320;&#35757;&#32451;&#27493;&#39588;&#36827;&#34892;&#35843;&#25972;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Local stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to distributed optimization where each device performs more than one SGD update per communication. This work presents an empirical study of {\it asynchronous} Local-SGD for training language models; that is, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a comprehensive investigation by examining how worker hardware heterogeneity, model size, number of workers, and optimizer could impact the learning performance. We find that with naive implementations, asynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite updating the (global) model parameters more frequently. We identify momentum acceleration on the global parameters when worker gradients are stale as a key challenge. We propose a novel method that utilizes a delayed Nesterov momentum update and adjusts the workers' local training steps based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20197;&#23562;&#37325;&#20026;&#35270;&#35282;&#35780;&#20272;&#19982;&#35821;&#35328;&#20195;&#29702;&#30340;&#20132;&#20114;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#20851;&#27880;&#20851;&#31995;&#21644;&#24773;&#22659;&#22240;&#32032;&#30340;&#20262;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;LLM&#25216;&#26415;&#34920;&#29616;&#24471;&#8220;&#22909;&#8221;</title><link>http://arxiv.org/abs/2401.09082</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#8220;&#22909;&#8221;&#30340;&#31038;&#20132;&#34892;&#20026;&#32773;&#65311;&#20197;&#23562;&#37325;&#20026;&#35270;&#35282;&#35780;&#20272;&#19982;&#35821;&#35328;&#20195;&#29702;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
What makes for a 'good' social actor? Using respect as a lens to evaluate interactions with language agents. (arXiv:2401.09082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20197;&#23562;&#37325;&#20026;&#35270;&#35282;&#35780;&#20272;&#19982;&#35821;&#35328;&#20195;&#29702;&#30340;&#20132;&#20114;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#20851;&#27880;&#20851;&#31995;&#21644;&#24773;&#22659;&#22240;&#32032;&#30340;&#20262;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;LLM&#25216;&#26415;&#34920;&#29616;&#24471;&#8220;&#22909;&#8221;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#35805;&#20195;&#29702;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22914;&#20309;&#30830;&#20445;&#23427;&#20204;&#30340;&#34892;&#20026;&#36947;&#24503;&#21644;&#36866;&#24403;&#24615;&#24050;&#32463;&#24341;&#36215;&#20102;&#32039;&#24613;&#20851;&#27880;&#12290;&#20174;&#8220;HHH&#8221;&#26631;&#20934;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20027;&#35201;&#20307;&#29616;&#22312;&#35753;&#36755;&#20986;&#26356;&#26377;&#24110;&#21161;&#21644;&#35802;&#23454;&#65292;&#24182;&#36991;&#20813;&#26377;&#23475;&#65288;&#26377;&#20559;&#35265;&#12289;&#26377;&#27602;&#25110;&#19981;&#20934;&#30830;&#65289;&#30340;&#38472;&#36848;&#12290;&#34429;&#28982;&#36825;&#31181;&#35821;&#20041;&#28966;&#28857;&#23545;&#20110;&#23558;LLM&#20195;&#29702;&#35270;&#20026;&#32431;&#31929;&#30340;&#20449;&#24687;&#23186;&#20171;&#26159;&#26377;&#29992;&#30340;&#65292;&#20294;&#23427;&#26410;&#33021;&#32771;&#34385;&#21040;&#22312;&#19981;&#21516;&#31038;&#20132;&#24773;&#22659;&#20013;&#65292;&#21516;&#26679;&#30340;&#35805;&#35821;&#21487;&#33021;&#20250;&#26174;&#24471;&#26356;&#25110;&#32773;&#26356;&#23569;&#20882;&#29359;&#25110;&#19981;&#24471;&#20307;&#30340;&#23454;&#38469;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#20851;&#27880;&#20851;&#31995;&#21644;&#24773;&#22659;&#22240;&#32032;&#30340;&#20262;&#29702;&#26041;&#27861;&#65292;&#25506;&#35752;&#20316;&#20026;&#31038;&#20132;&#34892;&#20026;&#32773;&#30340;&#31995;&#32479;&#22914;&#20309;&#22312;&#20132;&#20114;&#20013;&#20197;&#23562;&#37325;&#30340;&#26041;&#24335;&#23545;&#24453;&#20010;&#20307;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39044;&#35265;&#20102;&#22312;&#24773;&#22659;&#20132;&#20114;&#23618;&#38754;&#19978;&#19968;&#31995;&#21015;&#23578;&#26410;&#34987;&#25506;&#32034;&#30340;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#29992;&#24314;&#35758;&#65292;&#20197;&#24110;&#21161;LLM&#25216;&#26415;&#34920;&#29616;&#24471;&#8220;&#22909;&#8221;
&lt;/p&gt;
&lt;p&gt;
With the growing popularity of dialogue agents based on large language models (LLMs), urgent attention has been drawn to finding ways to ensure their behaviour is ethical and appropriate. These are largely interpreted in terms of the 'HHH' criteria: making outputs more helpful and honest, and avoiding harmful (biased, toxic, or inaccurate) statements. Whilst this semantic focus is useful from the perspective of viewing LLM agents as mere mediums for information, it fails to account for pragmatic factors that can make the same utterance seem more or less offensive or tactless in different social situations. We propose an approach to ethics that is more centred on relational and situational factors, exploring what it means for a system, as a social actor, to treat an individual respectfully in a (series of) interaction(s). Our work anticipates a set of largely unexplored risks at the level of situated interaction, and offers practical suggestions to help LLM technologies behave as 'good'
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09074</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20195;&#30721;&#27169;&#25311;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Code Simulation Challenges for Large Language Models. (arXiv:2401.09074v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09074
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#65292;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#65292;&#23427;&#20204;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#65292;&#20294;&#23545;&#20110;&#22797;&#26434;&#30340;&#31243;&#24207;&#65292;&#29305;&#21035;&#26159;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#65292;&#27169;&#25311;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#25191;&#34892;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27169;&#25311;&#35745;&#31639;&#26426;&#20195;&#30721;&#21644;&#31639;&#27861;&#25191;&#34892;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#30452;&#32447;&#31243;&#24207;&#65292;&#24182;&#23637;&#31034;&#20102;&#24403;&#21069;LLMs&#22312;&#22788;&#29702;&#36825;&#26679;&#31616;&#21333;&#30340;&#31243;&#24207;&#26102;&#34920;&#29616;&#20986;&#30340;&#24615;&#33021;&#36739;&#24046;&#8212;&#8212;&#24615;&#33021;&#38543;&#30528;&#20195;&#30721;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#36805;&#36895;&#19979;&#38477;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#27169;&#25311;&#21253;&#21547;&#20851;&#38190;&#36335;&#24452;&#21644;&#20887;&#20313;&#25351;&#20196;&#30340;&#31243;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25490;&#24207;&#31639;&#27861;&#21644;&#23884;&#22871;&#24490;&#29615;&#36229;&#36234;&#20102;&#30452;&#32447;&#31243;&#24207;&#30340;&#27169;&#25311;&#65292;&#24182;&#23637;&#31034;&#20102;&#31243;&#24207;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#30452;&#25509;&#24433;&#21709;LLMs&#27169;&#25311;&#20854;&#25191;&#34892;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#21482;&#26377;&#22312;&#22788;&#29702;&#30701;&#31243;&#24207;&#25110;&#26631;&#20934;&#36807;&#31243;&#26102;&#25165;&#33021;&#20197;&#20302;&#38169;&#35823;&#29575;&#25353;&#39034;&#24207;&#25191;&#34892;&#25351;&#20196;&#12290;LLMs&#30340;&#20195;&#30721;&#27169;&#25311;&#19982;&#23427;&#20204;&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#35760;&#24518;&#33021;&#21147;&#23384;&#22312;&#30683;&#30462;&#65306;&#22312;&#35760;&#24518;&#23545;&#20219;&#21153;&#26377;&#23475;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#36880;&#34892;&#27169;&#25311;&#20195;&#30721;&#30340;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the extent to which Large Language Models (LLMs) can simulate the execution of computer code and algorithms. We begin by looking straight line programs, and show that current LLMs demonstrate poor performance even with such simple programs -- performance rapidly degrades with the length of code. We then investigate the ability of LLMs to simulate programs that contain critical paths and redundant instructions. We also go beyond straight line program simulation with sorting algorithms and nested loops, and we show the computational complexity of a routine directly affects the ability of an LLM to simulate its execution. We observe that LLMs execute instructions sequentially and with a low error margin only for short programs or standard procedures. LLMs' code simulation is in tension with their pattern recognition and memorisation capabilities: on tasks where memorisation is detrimental, we propose a novel prompting method to simulate code execution line by line. Empirica
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#22810;&#31181;&#26368;&#20808;&#36827;&#30340;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#34920;&#29616;&#27424;&#20339;&#65292;&#36825;&#25361;&#25112;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#24120;&#35782;&#35268;&#21010;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09042</link><description>&lt;p&gt;
LLM&#23545;&#20110;&#20851;&#31995;&#25512;&#29702;&#30340;&#23454;&#29616;&#31243;&#24230;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
LLMs for Relational Reasoning: How Far are We?. (arXiv:2401.09042v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#22810;&#31181;&#26368;&#20808;&#36827;&#30340;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#34920;&#29616;&#27424;&#20339;&#65292;&#36825;&#25361;&#25112;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;&#24120;&#35782;&#35268;&#21010;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#22312;&#24191;&#27867;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#38761;&#26032;&#20102;&#35768;&#22810;&#39046;&#22495;&#65288;&#20363;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#36719;&#20214;&#24037;&#31243;&#31561;&#65289;&#12290;&#20026;&#20102;&#23454;&#29616;&#24378;&#22823;&#19988;&#36890;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#20154;&#20204;&#23545;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#20135;&#29983;&#20102;&#27987;&#21402;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30740;&#31350;&#37319;&#29992;&#30340;&#25991;&#26412;&#21644;&#25968;&#20540;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#30456;&#23545;&#27973;&#26174;&#31616;&#21333;&#65292;&#20165;&#20165;&#36890;&#36807;&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#21462;&#24471;&#31215;&#26497;&#32467;&#26524;&#38590;&#20197;&#24471;&#20986;LLM&#20855;&#26377;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#30340;&#32467;&#35770;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21162;&#21147;&#36890;&#36807;&#22312;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;LLM&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;LLM&#22312;&#35299;&#20915;&#38656;&#35201;&#24120;&#35782;&#35268;&#21010;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#24402;&#32435;&#36923;&#36753;&#32534;&#31243;&#65288;ILP&#65289;&#22522;&#20934;&#27979;&#35797;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#28145;&#20837;&#35780;&#20272;&#65292;&#36825;&#19968;&#22522;&#20934;&#27979;&#35797;&#34987;&#24191;&#27867;&#35748;&#21487;&#20026;&#23545;&#25512;&#29702;&#33021;&#21147;&#30340;&#20195;&#34920;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized many areas (e.g. natural language processing, software engineering, etc.) by achieving state-of-the-art performance on extensive downstream tasks. Aiming to achieve robust and general artificial intelligence, there has been a surge of interest in investigating the reasoning ability of the LLMs. Whereas the textual and numerical reasoning benchmarks adopted by previous works are rather shallow and simple, it is hard to conclude that the LLMs possess strong reasoning ability by merely achieving positive results on these benchmarks. Recent efforts have demonstrated that the LLMs are poor at solving sequential decision-making problems that require common-sense planning by evaluating their performance on the reinforcement learning benchmarks. In this work, we conduct an in-depth assessment of several state-of-the-art LLMs' reasoning ability based on the inductive logic programming (ILP) benchmark, which is broadly recognized as a representati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#65292;&#29992;&#20110;&#24635;&#32467;&#23398;&#26415;&#35770;&#25991;&#20013;&#30340;&#21442;&#32771;&#25991;&#29486;&#38598;&#21512;&#65292;&#25193;&#23637;&#20102;&#20043;&#21069;&#20851;&#20110;&#24635;&#32467;&#28040;&#36153;&#21697;&#38598;&#21512;&#30340;&#24037;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09041</link><description>&lt;p&gt;
&#22823;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#25688;&#35201;&#65306;&#26397;&#30528;&#19968;&#20010;&#36890;&#29992;&#26041;&#27861;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Textual Summarisation of Large Sets: Towards a General Approach. (arXiv:2401.09041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#65292;&#29992;&#20110;&#24635;&#32467;&#23398;&#26415;&#35770;&#25991;&#20013;&#30340;&#21442;&#32771;&#25991;&#29486;&#38598;&#21512;&#65292;&#25193;&#23637;&#20102;&#20043;&#21069;&#20851;&#20110;&#24635;&#32467;&#28040;&#36153;&#21697;&#38598;&#21512;&#30340;&#24037;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27491;&#22312;&#24320;&#21457;&#19968;&#31181;&#29983;&#25104;&#23545;&#35937;&#38598;&#21512;&#25688;&#35201;&#25551;&#36848;&#30340;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#21017;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#65292;&#29992;&#20110;&#24635;&#32467;&#23398;&#26415;&#35770;&#25991;&#20013;&#30340;&#21442;&#32771;&#25991;&#29486;&#38598;&#21512;&#12290;&#36825;&#25193;&#23637;&#20102;&#25105;&#20204;&#20043;&#21069;&#20851;&#20110;&#24635;&#32467;&#28040;&#36153;&#21697;&#38598;&#21512;&#30340;&#24037;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#36825;&#20004;&#20010;&#38750;&#24120;&#19981;&#21516;&#30340;&#39046;&#22495;&#20013;&#36827;&#34892;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are developing techniques to generate summary descriptions of sets of objects. In this paper, we present and evaluate a rule-based NLG technique for summarising sets of bibliographical references in academic papers. This extends our previous work on summarising sets of consumer products and shows how our model generalises across these two very different domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;mExCB&#65292;&#29992;&#20110;&#20174;&#28151;&#21512;&#35821;&#35328;&#20013;&#33258;&#21160;&#26816;&#27979;&#32593;&#32476;&#38712;&#20940;&#65292;&#24182;&#24341;&#20837;&#20102;&#35299;&#37322;&#24615;&#32593;&#32476;&#38712;&#20940;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;BullyExplain&#12290;</title><link>http://arxiv.org/abs/2401.09023</link><description>&lt;p&gt;
&#35299;&#37322;&#20320;&#33258;&#24049;&#21543;&#65292;&#38712;&#20940;&#32773;&#65306;&#20351;&#29992;&#24773;&#24863;&#36741;&#21161;&#30340;&#24102;&#35299;&#37322;&#30340;&#32593;&#32476;&#38712;&#20940;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Explain Thyself Bully: Sentiment Aided Cyberbullying Detection with Explanation. (arXiv:2401.09023v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;mExCB&#65292;&#29992;&#20110;&#20174;&#28151;&#21512;&#35821;&#35328;&#20013;&#33258;&#21160;&#26816;&#27979;&#32593;&#32476;&#38712;&#20940;&#65292;&#24182;&#24341;&#20837;&#20102;&#35299;&#37322;&#24615;&#32593;&#32476;&#38712;&#20940;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;BullyExplain&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#32593;&#32476;&#21644;&#22312;&#32447;&#36890;&#35759;&#24212;&#29992;&#30340;&#26222;&#21450;&#65292;&#32593;&#32476;&#38712;&#20940;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#22823;&#38382;&#39064;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#24456;&#22810;&#20851;&#20110;&#21333;&#35821;&#35328;&#32593;&#32476;&#38712;&#20940;&#26816;&#27979;&#27169;&#22411;&#30340;&#30740;&#31350;&#65292;&#20294;&#22312;&#28151;&#21512;&#35821;&#35328;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#21364;&#24456;&#23569;&#12290;&#26368;&#36817;&#30340;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;&#31561;&#27861;&#24459;&#25512;&#21160;&#20102;&#20197;&#35299;&#37322;&#24615;&#27169;&#22411;&#20026;&#37325;&#28857;&#30340;&#30740;&#31350;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#24615;&#33021;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;mExCB&#65292;&#29992;&#20110;&#20174;&#28151;&#21512;&#35821;&#35328;&#20013;&#33258;&#21160;&#26816;&#27979;&#32593;&#32476;&#38712;&#20940;&#65292;&#21516;&#26102;&#35299;&#20915;&#22810;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#32593;&#32476;&#38712;&#20940;&#26816;&#27979;&#12289;&#35299;&#37322;/&#29702;&#30001;&#35782;&#21035;&#12289;&#30446;&#26631;&#32676;&#20307;&#26816;&#27979;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35299;&#37322;&#24615;&#32593;&#32476;&#38712;&#20940;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;BullyExplain&#12290;BullyExplain&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#24086;&#23376;&#37117;&#34987;&#27880;&#37322;&#20102;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Cyberbullying has become a big issue with the popularity of different social media networks and online communication apps. While plenty of research is going on to develop better models for cyberbullying detection in monolingual language, there is very little research on the code-mixed languages and explainability aspect of cyberbullying. Recent laws like "right to explanations" of General Data Protection Regulation, have spurred research in developing interpretable models rather than focusing on performance. Motivated by this we develop the first interpretable multi-task model called {\em mExCB} for automatic cyberbullying detection from code-mixed languages which can simultaneously solve several tasks, cyberbullying detection, explanation/rationale identification, target group detection and sentiment analysis. We have introduced {\em BullyExplain}, the first benchmark dataset for explainable cyberbullying detection in code-mixed language. Each post in {\em BullyExplain} dataset is ann
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09003</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;&#26469;&#22686;&#24378;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22312;&#19981;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#20173;&#28982;&#23545;&#24320;&#28304;LLMs&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MMIQC&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#28151;&#21512;&#22788;&#29702;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#21512;&#25104;&#38382;&#39064;-&#21709;&#24212;&#23545;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#20379;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;MMIQC&#19978;&#23545;Mistral-7B(arXiv:2310.06825)&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#30340;&#27169;&#22411;Mistral-7B-MMIQC&#65292;&#22312;MATH(arXiv:2103.03874)&#19978;&#36798;&#21040;&#20102;36.0%&#30340;&#20934;&#30830;&#29575;&#65292;&#27604;&#20043;&#21069;(model size $\sim$7B)&#30340;&#26368;&#20339;&#32467;&#26524;&#39640;&#20986;5.8%&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#24402;&#21151;&#20110;&#25105;&#20204;&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;IQC(&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;)&#65292;&#20854;&#20013;&#25105;&#20204;&#36845;&#20195;&#22320;&#35201;&#27714;LLM&#20174;&#32473;&#23450;&#30340;&#31181;&#23376;&#38382;&#39064;&#20013;&#32452;&#21512;&#26032;&#38382;&#39064;&#65292;&#24182;&#20174;&#21478;&#19968;&#20010;LLM&#20013;&#36827;&#34892;&#25298;&#32477;&#25277;&#26679;&#12290;MMIQC&#29616;&#24050;&#22312;https://huggingface.co/datasets/Vivacem/MMIQC&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#25928;&#26524;&#65292;&#24341;&#20837;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#21644;&#32454;&#33268;&#30340;&#35780;&#20272;&#35282;&#24230;&#65292;&#24182;&#24320;&#21457;&#20102;&#19987;&#38376;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#20316;&#20026;&#22522;&#20934;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#24314;&#31435;&#20102;&#22522;&#30784;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.09002</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#25928;&#26524;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models. (arXiv:2401.09002v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#25928;&#26524;&#65292;&#24341;&#20837;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#21644;&#32454;&#33268;&#30340;&#35780;&#20272;&#35282;&#24230;&#65292;&#24182;&#24320;&#21457;&#20102;&#19987;&#38376;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#20316;&#20026;&#22522;&#20934;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#24314;&#31435;&#20102;&#22522;&#30784;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21019;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#36234;&#29425;&#25915;&#20987;&#25928;&#26524;&#30340;&#26032;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;&#20581;&#22766;&#24615;&#35780;&#20272;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#35780;&#20272;&#26694;&#26550;&#65306;&#31895;&#31890;&#24230;&#35780;&#20272;&#21644;&#32454;&#31890;&#24230;&#35780;&#20272;&#12290;&#27599;&#20010;&#26694;&#26550;&#37117;&#20351;&#29992;&#20174;0&#21040;1&#30340;&#35780;&#20998;&#33539;&#22260;&#65292;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#21644;&#32454;&#33268;&#22320;&#35780;&#20272;&#25915;&#20987;&#25928;&#26524;&#65292;&#24182;&#24110;&#21161;&#25915;&#20987;&#32773;&#26356;&#22909;&#22320;&#20248;&#21270;&#25915;&#20987;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#36234;&#29425;&#20219;&#21153;&#30340;&#20840;&#38754;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#19981;&#20165;&#26159;&#25105;&#20204;&#24403;&#21069;&#30740;&#31350;&#30340;&#20851;&#38190;&#22522;&#20934;&#65292;&#20063;&#20026;&#26410;&#26469;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#30784;&#36164;&#28304;&#65292;&#21487;&#20197;&#22312;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#36827;&#34892;&#19968;&#33268;&#21644;&#27604;&#36739;&#30340;&#20998;&#26512;&#12290;&#36890;&#36807;&#19982;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#30340;&#31934;&#24515;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#19982;&#20043;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
In our research, we pioneer a novel approach to evaluate the effectiveness of jailbreak attacks on Large Language Models (LLMs), such as GPT-4 and LLaMa2, diverging from traditional robustness-focused binary evaluations. Our study introduces two distinct evaluation frameworks: a coarse-grained evaluation and a fine-grained evaluation. Each framework, using a scoring range from 0 to 1, offers a unique perspective, enabling a more comprehensive and nuanced evaluation of attack effectiveness and empowering attackers to refine their attack prompts with greater understanding. Furthermore, we have developed a comprehensive ground truth dataset specifically tailored for jailbreak tasks. This dataset not only serves as a crucial benchmark for our current study but also establishes a foundational resource for future research, enabling consistent and comparative analyses in this evolving field. Upon meticulous comparison with traditional evaluation methods, we discovered that our evaluation alig
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#27969;&#24335;&#22810;&#35821;&#35328;ASR&#20013;&#37319;&#29992;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36866;&#37197;&#22120;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23545;&#36793;&#32536;&#35821;&#31181;&#30340;&#25903;&#25345;&#12290;&#36866;&#37197;&#22120;&#20165;&#21344;&#27599;&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#30340;0.4%&#12290;&#35813;&#26041;&#27861;&#22312;&#32423;&#32852;&#30340;Conformer&#36716;&#24405;&#22120;&#26694;&#26550;&#19979;&#65292;&#36890;&#36807;&#25945;&#24072;&#20266;&#26631;&#31614;&#22686;&#24378;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08992</link><description>&lt;p&gt;
&#27969;&#24335;&#22810;&#35821;&#35328;ASR&#20013;&#38024;&#23545;&#36793;&#32536;&#35821;&#31181;&#30340;&#39640;&#25928;&#36866;&#37197;&#22120;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Efficient Adapter Finetuning for Tail Languages in Streaming Multilingual ASR. (arXiv:2401.08992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#27969;&#24335;&#22810;&#35821;&#35328;ASR&#20013;&#37319;&#29992;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36866;&#37197;&#22120;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#23545;&#36793;&#32536;&#35821;&#31181;&#30340;&#25903;&#25345;&#12290;&#36866;&#37197;&#22120;&#20165;&#21344;&#27599;&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#30340;0.4%&#12290;&#35813;&#26041;&#27861;&#22312;&#32423;&#32852;&#30340;Conformer&#36716;&#24405;&#22120;&#26694;&#26550;&#19979;&#65292;&#36890;&#36807;&#25945;&#24072;&#20266;&#26631;&#31614;&#22686;&#24378;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27969;&#24335;&#22810;&#35821;&#35328;&#24773;&#26223;&#20013;&#65292;&#20154;&#20204;&#36890;&#24120;&#24076;&#26395;&#20351;&#29992;&#31471;&#21040;&#31471;ASR&#27169;&#22411;&#65292;&#22240;&#20026;&#36825;&#26679;&#26356;&#23481;&#26131;&#37096;&#32626;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#27169;&#22411;&#20013;&#21463;&#30410;&#65292;&#20363;&#22914;&#24378;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19981;&#21516;&#35821;&#35328;&#30340;&#24322;&#36136;&#24615;&#21644;&#25968;&#25454;&#20016;&#23500;&#24230;&#30340;&#19981;&#24179;&#34913;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#21516;&#35821;&#35328;&#30340;&#24615;&#33021;&#20986;&#29616;&#24322;&#27493;&#23792;&#20540;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36793;&#32536;&#35821;&#31181;&#12290;&#26377;&#26102;&#65292;&#25968;&#25454;&#26412;&#36523;&#29978;&#33267;&#21487;&#33021;&#22240;&#20026;&#21152;&#24378;&#30340;&#38544;&#31169;&#20445;&#25252;&#32780;&#19981;&#21487;&#29992;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#20542;&#21521;&#20110;&#26174;&#33879;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#25110;&#23398;&#20064;&#35821;&#35328;&#29305;&#23450;&#30340;&#35299;&#30721;&#22120;&#26469;&#21333;&#29420;&#36866;&#24212;&#27599;&#31181;&#35821;&#35328;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;Language-Dependent Adapter (LDA)&#24494;&#35843;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36816;&#29992;&#20102;&#32423;&#32852;&#30340;Conformer&#36716;&#24405;&#22120;&#26694;&#26550;&#65292;&#36890;&#36807;&#25945;&#24072;&#20266;&#26631;&#31614;&#22686;&#24378;&#20102;&#23545;&#27969;&#24335;&#22810;&#35821;&#35328;ASR&#20013;&#30340;&#36793;&#32536;&#35821;&#31181;&#30340;&#25903;&#25345;&#12290;&#36866;&#37197;&#22120;&#20165;&#21344;&#27599;&#31181;&#35821;&#35328;&#30340;&#23436;&#25972;&#27169;&#22411;&#30340;0.4%&#12290;&#23427;&#34987;&#25554;&#20837;&#21040;&#20923;&#32467;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The end-to-end ASR model is often desired in the streaming multilingual scenario since it is easier to deploy and can benefit from pre-trained speech models such as powerful foundation models. Meanwhile, the heterogeneous nature and imbalanced data abundance of different languages may cause performance degradation, leading to asynchronous peak performance for different languages during training, especially on tail ones. Sometimes even the data itself may become unavailable as a result of the enhanced privacy protection. Existing work tend to significantly increase the model size or learn language-specific decoders to accommodate each language separately. In this study, we explore simple yet effective Language-Dependent Adapter (LDA) finetuning under a cascaded Conformer transducer framework enhanced by teacher pseudo-labeling for tail languages in the streaming multilingual ASR. The adapter only accounts for 0.4% of the full model per language. It is plugged into the frozen foundation 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;OCTO + &#30340;&#22871;&#20214;&#65292;&#29992;&#20110;&#22312;&#28151;&#21512;&#29616;&#23454;&#20013;&#33258;&#21160;&#23558;&#34394;&#25311;&#29289;&#20307;&#25918;&#32622;&#22312;&#21512;&#36866;&#30340;&#20301;&#32622;&#12290;&#36890;&#36807;&#20351;&#29992;&#26368;&#26032;&#30340;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#35780;&#20272;&#26631;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#65292;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#22686;&#24378;&#29616;&#23454;&#20013;&#34394;&#25311;&#29289;&#20307;&#25918;&#32622;&#30340;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.08973</link><description>&lt;p&gt;
OCTO + &#65306;&#33258;&#21160;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#25918;&#32622;&#22312;&#28151;&#21512;&#29616;&#23454;&#20013;&#30340;&#22871;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
OCTO+: A Suite for Automatic Open-Vocabulary Object Placement in Mixed Reality. (arXiv:2401.08973v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;OCTO + &#30340;&#22871;&#20214;&#65292;&#29992;&#20110;&#22312;&#28151;&#21512;&#29616;&#23454;&#20013;&#33258;&#21160;&#23558;&#34394;&#25311;&#29289;&#20307;&#25918;&#32622;&#22312;&#21512;&#36866;&#30340;&#20301;&#32622;&#12290;&#36890;&#36807;&#20351;&#29992;&#26368;&#26032;&#30340;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#35780;&#20272;&#26631;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#65292;&#36229;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#22686;&#24378;&#29616;&#23454;&#20013;&#34394;&#25311;&#29289;&#20307;&#25918;&#32622;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#29616;&#23454;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23558;&#34394;&#25311;&#20869;&#23481;&#25918;&#32622;&#22312;&#33258;&#28982;&#20301;&#32622;&#19978;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#33258;&#21160;&#21270;&#25216;&#26415;&#21482;&#33021;&#20351;&#29992;&#23553;&#38381;&#35789;&#27719;&#12289;&#22266;&#23450;&#30340;&#29289;&#20307;&#38598;&#21512;&#26469;&#24037;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#20351;&#29992;&#26368;&#26032;&#36827;&#23637;&#30340;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#29289;&#20307;&#25918;&#32622;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22810;&#26041;&#38754;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;OCTO +&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#22686;&#24378;&#29616;&#23454;&#20013;&#34394;&#25311;&#29289;&#20307;&#25918;&#32622;&#30340;&#22522;&#20934;&#65292;&#20943;&#36731;&#20102;&#38656;&#35201;&#26114;&#36149;&#29992;&#25143;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#36825;&#20010;&#22522;&#20934;&#65292;&#38500;&#20102;&#20154;&#31867;&#35780;&#20272;&#20043;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;OCTO + &#26377;&#25928;&#22320;&#23558;&#29289;&#20307;&#25918;&#32622;&#22312;&#19968;&#20010;&#21512;&#29702;&#30340;&#21306;&#22495;&#20869;&#36229;&#36807;70&#65285;&#30340;&#26102;&#38388;&#65292;&#22312;&#21508;&#31181;&#25351;&#26631;&#19978;&#36229;&#36234;&#20102;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One key challenge in Augmented Reality is the placement of virtual content in natural locations. Most existing automated techniques can only work with a closed-vocabulary, fixed set of objects. In this paper, we introduce and evaluate several methods for automatic object placement using recent advances in open-vocabulary vision-language models. Through a multifaceted evaluation, we identify a new state-of-the-art method, OCTO+. We also introduce a benchmark for automatically evaluating the placement of virtual objects in augmented reality, alleviating the need for costly user studies. Through this, in addition to human evaluations, we find that OCTO+ places objects in a valid region over 70% of the time, outperforming other methods on a range of metrics.
&lt;/p&gt;</description></item><item><title>ReFT&#26159;&#19968;&#31181;&#21152;&#24378;&#25512;&#29702;&#33021;&#21147;&#30340;&#24378;&#21270;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#30340;&#25512;&#29702;&#36335;&#24452;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.08967</link><description>&lt;p&gt;
ReFT: &#21152;&#24378;&#24378;&#21270;&#24494;&#35843;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ReFT: Reasoning with Reinforced Fine-Tuning. (arXiv:2401.08967v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08967
&lt;/p&gt;
&lt;p&gt;
ReFT&#26159;&#19968;&#31181;&#21152;&#24378;&#25512;&#29702;&#33021;&#21147;&#30340;&#24378;&#21270;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26356;&#22810;&#30340;&#25512;&#29702;&#36335;&#24452;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#65288;CoT&#65289;&#27880;&#37322;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#27867;&#21270;&#33021;&#21147;&#19978;&#24182;&#19981;&#21313;&#20998;&#24378;&#22823;&#65292;&#22240;&#20026;&#35757;&#32451;&#20165;&#20381;&#36182;&#20110;&#32473;&#23450;&#30340;CoT&#25968;&#25454;&#12290;&#20363;&#22914;&#65292;&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#20013;&#36890;&#24120;&#21482;&#26377;&#19968;&#20010;&#27880;&#37322;&#30340;&#25512;&#29702;&#36335;&#24452;&#29992;&#20110;&#27599;&#20010;&#38382;&#39064;&#12290;&#30452;&#35266;&#26469;&#35828;&#65292;&#35753;&#31639;&#27861;&#20174;&#32473;&#23450;&#30340;&#38382;&#39064;&#20013;&#23398;&#20064;&#22810;&#20010;&#27880;&#37322;&#30340;&#25512;&#29702;&#36335;&#24452;&#20250;&#26356;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21152;&#24378;&#24378;&#21270;&#24494;&#35843;&#65288;ReFT&#65289;&#65292;&#20197;&#22686;&#24378;&#23398;&#20064;LLMs&#36827;&#34892;&#25512;&#29702;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20026;&#20363;&#12290;ReFT&#39318;&#20808;&#20351;&#29992;SFT&#23545;&#27169;&#22411;&#36827;&#34892;&#28909;&#36523;&#65292;&#28982;&#21518;&#37319;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#26412;&#25991;&#20013;&#26159;&#20351;&#29992;PPO&#31639;&#27861;&#65289;&#36827;&#19968;&#27493;&#24494;&#35843;&#27169;&#22411;&#65292;&#20854;&#20013;&#26681;&#25454;&#38382;&#39064;&#33258;&#21160;&#37319;&#26679;&#20102;&#22823;&#37327;&#30340;&#25512;&#29702;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question
&lt;/p&gt;</description></item><item><title>&#37096;&#20998;&#38899;&#26631;&#21270;&#26159;&#36873;&#25321;&#26631;&#35760;&#37096;&#20998;&#23383;&#31526;&#26469;&#25552;&#39640;&#38405;&#35835;&#21487;&#35835;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#19978;&#19979;&#25991;&#23545;&#27604;&#30340;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;CCPD&#65289;&#38598;&#25104;&#20102;&#29616;&#26377;&#30340;&#38463;&#25289;&#20271;&#38899;&#26631;&#21270;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#34913;&#37327;&#37096;&#20998;&#38899;&#26631;&#21270;&#30340;&#26032;&#25351;&#26631;&#26469;&#21028;&#26029;&#38656;&#35201;&#26631;&#35760;&#21738;&#20123;&#23383;&#31526;&#12290;</title><link>http://arxiv.org/abs/2401.08919</link><description>&lt;p&gt;
&#37096;&#20998;&#38899;&#26631;&#21270;&#65306;&#19968;&#31181;&#19978;&#19979;&#25991;&#23545;&#27604;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Partial Diacritization: A Context-Contrastive Inference Approach. (arXiv:2401.08919v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08919
&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#38899;&#26631;&#21270;&#26159;&#36873;&#25321;&#26631;&#35760;&#37096;&#20998;&#23383;&#31526;&#26469;&#25552;&#39640;&#38405;&#35835;&#21487;&#35835;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#19978;&#19979;&#25991;&#23545;&#27604;&#30340;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;CCPD&#65289;&#38598;&#25104;&#20102;&#29616;&#26377;&#30340;&#38463;&#25289;&#20271;&#38899;&#26631;&#21270;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#34913;&#37327;&#37096;&#20998;&#38899;&#26631;&#21270;&#30340;&#26032;&#25351;&#26631;&#26469;&#21028;&#26029;&#38656;&#35201;&#26631;&#35760;&#21738;&#20123;&#23383;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#26631;&#21270;&#22312;&#25552;&#39640;&#38463;&#25289;&#20271;&#25991;&#26412;&#21487;&#35835;&#24615;&#21644;&#28040;&#38500;&#27495;&#20041;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30446;&#21069;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#26631;&#35760;&#27599;&#20010;&#31526;&#21512;&#26465;&#20214;&#30340;&#23383;&#31526;&#65288;&#20840;&#38899;&#26631;&#21270;&#65289;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;PD&#65289;&#26159;&#36873;&#25321;&#26631;&#35760;&#23376;&#38598;&#20197;&#22312;&#24517;&#35201;&#26102;&#25552;&#20379;&#24110;&#21161;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36807;&#22810;&#30340;&#38899;&#26631;&#31526;&#21495;&#20250;&#22952;&#30861;&#29087;&#32451;&#35835;&#32773;&#65292;&#38477;&#20302;&#38405;&#35835;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#34892;&#20026;&#23454;&#39564;&#65292;&#24182;&#26174;&#31034;&#20986;&#37096;&#20998;&#26631;&#35760;&#30340;&#25991;&#26412;&#36890;&#24120;&#27604;&#23436;&#20840;&#26631;&#35760;&#30340;&#25991;&#26412;&#26356;&#23481;&#26131;&#38405;&#35835;&#65292;&#26377;&#26102;&#29978;&#33267;&#27604;&#32431;&#25991;&#26412;&#26356;&#23481;&#26131;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19978;&#19979;&#25991;&#23545;&#27604;&#30340;&#37096;&#20998;&#38899;&#26631;&#21270;&#65288;CCPD&#65289;-&#19968;&#31181;&#19982;&#29616;&#26377;&#38463;&#25289;&#20271;&#38899;&#26631;&#21270;&#31995;&#32479;&#26080;&#32541;&#38598;&#25104;&#30340;&#26032;&#26041;&#27861;&#12290;CCPD&#23545;&#27599;&#20010;&#21333;&#35789;&#36827;&#34892;&#20004;&#27425;&#22788;&#29702;&#65292;&#19968;&#27425;&#26377;&#19978;&#19979;&#25991;&#65292;&#19968;&#27425;&#27809;&#26377;&#65292;&#24182;&#19988;&#21482;&#23545;&#20004;&#27425;&#25512;&#29702;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#30340;&#23383;&#31526;&#36827;&#34892;&#38899;&#26631;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#34913;&#37327;&#37096;&#20998;&#38899;&#26631;&#21270;&#30340;&#26032;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diacritization plays a pivotal role in improving readability and disambiguating the meaning of Arabic texts. Efforts have so far focused on marking every eligible character (Full Diacritization). Comparatively overlooked, Partial Diacritzation (PD) is the selection of a subset of characters to be marked to aid comprehension where needed. Research has indicated that excessive diacritic marks can hinder skilled readers--reducing reading speed and accuracy. We conduct a behavioral experiment and show that partially marked text is often easier to read than fully marked text, and sometimes easier than plain text. In this light, we introduce Context-Contrastive Partial Diacritization (CCPD)--a novel approach to PD which integrates seamlessly with existing Arabic diacritization systems. CCPD processes each word twice, once with context and once without, and diacritizes only the characters with disparities between the two inferences. Further, we introduce novel indicators for measuring partial
&lt;/p&gt;</description></item><item><title>NOTSOFAR-1&#25361;&#25112;&#26088;&#22312;&#35299;&#20915;&#36828;&#36317;&#31163;&#20250;&#35758;&#36716;&#24405;&#20013;&#30340;&#36828;&#36317;&#31163;&#21457;&#35328;&#20154;&#21010;&#20998;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38382;&#39064;&#65292;&#22312;&#19968;&#20010;&#26032;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.08887</link><description>&lt;p&gt;
NOTSOFAR-1&#25361;&#25112;&#65306;&#36828;&#36317;&#31163;&#20250;&#35758;&#36716;&#24405;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#22522;&#20934;&#32447;&#21644;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
NOTSOFAR-1 Challenge: New Datasets, Baseline, and Tasks for Distant Meeting Transcription. (arXiv:2401.08887v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08887
&lt;/p&gt;
&lt;p&gt;
NOTSOFAR-1&#25361;&#25112;&#26088;&#22312;&#35299;&#20915;&#36828;&#36317;&#31163;&#20250;&#35758;&#36716;&#24405;&#20013;&#30340;&#36828;&#36317;&#31163;&#21457;&#35328;&#20154;&#21010;&#20998;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#38382;&#39064;&#65292;&#22312;&#19968;&#20010;&#26032;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#33258;&#28982;&#21150;&#20844;&#23460;&#35848;&#35805;&#32773;&#22312;&#36828;&#22330;&#38899;&#39057;&#35760;&#24405;&#29615;&#22659;&#20013;&#65288;&#8220;NOTSOFAR-1&#8221;&#65289;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#31995;&#32479;&#12290;&#35813;&#25361;&#25112;&#20027;&#35201;&#20851;&#27880;&#36828;&#22330;&#20250;&#35758;&#20013;&#30340;&#36828;&#36317;&#31163;&#21457;&#35328;&#20154;&#21010;&#20998;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;DASR&#65289;&#65292;&#21253;&#25324;&#21333;&#22768;&#36947;&#21644;&#24050;&#30693;&#20960;&#20309;&#22810;&#36890;&#36947;&#30340;&#36712;&#36947;&#65292;&#24182;&#19988;&#20316;&#20026;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#30340;&#21457;&#24067;&#24179;&#21488;&#65306;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;315&#20010;&#20250;&#35758;&#65292;&#27599;&#20010;&#20250;&#35758;&#24179;&#22343;6&#20998;&#38047;&#65292;&#25429;&#25417;&#21040;&#24191;&#27867;&#30340;&#30495;&#23454;&#19990;&#30028;&#22768;&#23398;&#29615;&#22659;&#21644;&#23545;&#35805;&#21160;&#24577;&#12290;&#23427;&#26159;&#22312;30&#20010;&#20250;&#35758;&#23460;&#20013;&#24405;&#21046;&#30340;&#65292;&#27599;&#20010;&#20250;&#35758;&#23460;&#26377;4-8&#20010;&#19982;&#20250;&#32773;&#21644;&#20849;&#35745;35&#20010;&#19981;&#21516;&#30340;&#21457;&#35328;&#32773;&#12290;&#31532;&#20108;&#20010;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;1000&#23567;&#26102;&#30340;&#27169;&#25311;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;&#22686;&#24378;&#30340;&#30495;&#23454;&#24615;&#20197;&#23454;&#29616;&#30495;&#23454;&#19990;&#30028;&#30340;&#27867;&#21270;&#65292;&#21253;&#25324;15000&#20010;&#30495;&#23454;&#22768;&#23398;&#36716;&#31227;&#20989;&#25968;&#12290;&#20219;&#21153;&#38598;&#20013;&#22312;&#21333;&#35774;&#22791;DASR&#19978;&#65292;&#20854;&#20013;&#22810;&#36890;&#36947;&#35774;&#22791;&#22987;&#32456;&#20849;&#20139;&#30456;&#21516;&#24050;&#30693;&#30340;&#20960;&#20309;&#24418;&#29366;&#12290;&#36825;&#19982;&#23454;&#38469;&#20250;&#35758;&#23460;&#30340;&#24120;&#35265;&#35774;&#32622;&#26159;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the first Natural Office Talkers in Settings of Far-field Audio Recordings (``NOTSOFAR-1'') Challenge alongside datasets and baseline system. The challenge focuses on distant speaker diarization and automatic speech recognition (DASR) in far-field meeting scenarios, with single-channel and known-geometry multi-channel tracks, and serves as a launch platform for two new datasets: First, a benchmarking dataset of 315 meetings, averaging 6 minutes each, capturing a broad spectrum of real-world acoustic conditions and conversational dynamics. It is recorded across 30 conference rooms, featuring 4-8 attendees and a total of 35 unique speakers. Second, a 1000-hour simulated training dataset, synthesized with enhanced authenticity for real-world generalization, incorporating 15,000 real acoustic transfer functions. The tasks focus on single-device DASR, where multi-channel devices always share the same known geometry. This is aligned with common setups in actual conference rooms,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;i&#21521;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36827;&#34892;&#36328;&#20027;&#20307;&#36328;&#20250;&#35805;EEG&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#31561;&#25928;&#30340;&#20027;&#20307;&#30456;&#20851;&#27169;&#22411;&#21462;&#24471;&#20102;18%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20027;&#20307;&#26080;&#20851;&#27169;&#22411;&#22312;&#26032;&#20027;&#20307;&#19978;&#30340;&#31454;&#20105;&#21147;&#21644;&#39069;&#22806;&#20027;&#20307;&#25968;&#25454;&#30340;&#22686;&#21152;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#25913;&#21892;&#65292;&#34920;&#26126;&#26377;&#25928;&#30340;&#35748;&#30693;&#36127;&#33655;&#30830;&#23450;&#19981;&#38656;&#35201;&#20027;&#20307;&#30456;&#20851;&#30340;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2401.08851</link><description>&lt;p&gt;
&#20351;&#29992;i&#21521;&#37327;&#36827;&#34892;&#20027;&#20307;&#26080;&#20851;&#30340;&#36328;&#20250;&#35805;EEG&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using i-vectors for subject-independent cross-session EEG transfer learning. (arXiv:2401.08851v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;i&#21521;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36827;&#34892;&#36328;&#20027;&#20307;&#36328;&#20250;&#35805;EEG&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#31561;&#25928;&#30340;&#20027;&#20307;&#30456;&#20851;&#27169;&#22411;&#21462;&#24471;&#20102;18%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#20027;&#20307;&#26080;&#20851;&#27169;&#22411;&#22312;&#26032;&#20027;&#20307;&#19978;&#30340;&#31454;&#20105;&#21147;&#21644;&#39069;&#22806;&#20027;&#20307;&#25968;&#25454;&#30340;&#22686;&#21152;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#25913;&#21892;&#65292;&#34920;&#26126;&#26377;&#25928;&#30340;&#35748;&#30693;&#36127;&#33655;&#30830;&#23450;&#19981;&#38656;&#35201;&#20027;&#20307;&#30456;&#20851;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#36127;&#33655;&#20998;&#31867;&#26159;&#26681;&#25454;&#29983;&#29702;&#27979;&#37327;&#22914;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#33258;&#21160;&#30830;&#23450;&#20010;&#20307;&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#21033;&#29992;&#24037;&#20316;&#35760;&#24518;&#36164;&#28304;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#37319;&#29992;&#20102;&#36328;&#23398;&#31185;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#38899;&#22788;&#29702;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#30340;&#35821;&#26009;&#24211;&#20316;&#20026;&#31532;&#19968;&#27425;&#34987;&#21160;&#33041;&#26426;&#25509;&#21475;&#31454;&#36187;&#30340;&#19968;&#37096;&#20998;&#65292;&#22312;2021&#24180;&#20844;&#24320;&#21457;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#22522;&#20110;i&#21521;&#37327;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36827;&#34892;&#36328;&#20027;&#20307;&#36328;&#20250;&#35805;EEG&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#31561;&#25928;&#30340;&#20027;&#20307;&#30456;&#20851;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;18%&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#25253;&#36947;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#26174;&#31034;&#25105;&#20204;&#30340;&#20027;&#20307;&#26080;&#20851;&#27169;&#22411;&#22312;&#26032;&#20027;&#20307;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#38543;&#30528;&#39069;&#22806;&#20027;&#20307;&#25968;&#25454;&#30340;&#22686;&#21152;&#32780;&#25913;&#21892;&#65292;&#34920;&#26126;&#26377;&#25928;&#30340;&#35748;&#30693;&#36127;&#33655;&#30830;&#23450;&#19981;&#38656;&#35201;&#20027;&#20307;&#30456;&#20851;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cognitive load classification is the task of automatically determining an individual's utilization of working memory resources during performance of a task based on physiologic measures such as electroencephalography (EEG). In this paper, we follow a cross-disciplinary approach, where tools and methodologies from speech processing are used to tackle this problem. The corpus we use was released publicly in 2021 as part of the first passive brain-computer interface competition on cross-session workload estimation. We present our approach which used i-vector-based neural network classifiers to accomplish inter-subject cross-session EEG transfer learning, achieving 18% relative improvement over equivalent subject-dependent models. We also report experiments showing how our subject-independent models perform competitively on held-out subjects and improve with additional subject data, suggesting that subject-dependent training is not required for effective cognitive load determination.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#23548;&#27880;&#24847;&#21147;&#30340;&#36741;&#21161;&#35757;&#32451;&#25439;&#22833;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#36827;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#19978;&#19979;&#25991;&#20559;&#24046;&#38382;&#39064;&#65292;&#19981;&#24341;&#20837;&#39069;&#22806;&#21442;&#25968;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#22312;&#20559;&#24046;&#30701;&#35821;&#25968;&#37327;&#22686;&#21152;&#26102;&#20173;&#28982;&#20445;&#25345;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08835</link><description>&lt;p&gt;
&#20351;&#29992;&#24341;&#23548;&#27880;&#24847;&#21147;&#25913;&#36827;ASR&#30340;&#19978;&#19979;&#25991;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Improving ASR Contextual Biasing with Guided Attention. (arXiv:2401.08835v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#23548;&#27880;&#24847;&#21147;&#30340;&#36741;&#21161;&#35757;&#32451;&#25439;&#22833;&#26041;&#27861;&#65292;&#21487;&#20197;&#25913;&#36827;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#19978;&#19979;&#25991;&#20559;&#24046;&#38382;&#39064;&#65292;&#19981;&#24341;&#20837;&#39069;&#22806;&#21442;&#25968;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#22312;&#20559;&#24046;&#30701;&#35821;&#25968;&#37327;&#22686;&#21152;&#26102;&#20173;&#28982;&#20445;&#25345;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#23548;&#27880;&#24847;&#21147;&#65288;GA&#65289;&#30340;&#36741;&#21161;&#35757;&#32451;&#25439;&#22833;&#65292;&#21487;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#19978;&#19979;&#25991;&#20559;&#24046;&#30340;&#25928;&#26524;&#21644;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#21442;&#25968;&#12290;&#22312;&#20043;&#21069;&#30340;&#25991;&#29486;&#20013;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#26159;&#38543;&#30528;&#20559;&#24046;&#30701;&#35821;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#19978;&#19979;&#25991;&#20559;&#24046;&#24102;&#26469;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#25928;&#26524;&#20250;&#20943;&#24369;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#38500;&#20102;&#20351;&#29992;&#36716;&#24405;&#25439;&#22833;&#22806;&#65292;&#36824;&#37319;&#29992;&#20102;GA&#25439;&#22833;&#20316;&#20026;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#25152;&#25552;&#20986;&#30340;GA&#25439;&#22833;&#26088;&#22312;&#25945;&#20250;&#20132;&#21449;&#27880;&#24847;&#21147;&#22914;&#20309;&#23558;&#20559;&#24046;&#30701;&#35821;&#19982;&#25991;&#26412;&#26631;&#35760;&#25110;&#38899;&#39057;&#24103;&#23545;&#40784;&#12290;&#19982;&#20855;&#26377;&#31867;&#20284;&#21160;&#26426;&#30340;&#20854;&#20182;&#30740;&#31350;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#30452;&#25509;&#20316;&#29992;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#26435;&#37325;&#65292;&#26356;&#23481;&#26131;&#23454;&#29616;&#12290;&#36890;&#36807;&#22522;&#20110;&#24102;&#19978;&#19979;&#25991;&#36866;&#37197;&#22120;&#30340;Conformer Transducer&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#23548;&#33268;&#26356;&#20302;&#30340;WER&#65292;&#32780;&#19988;&#22312;&#20559;&#24046;&#30701;&#35821;&#25968;&#37327;&#22686;&#21152;&#26102;&#20173;&#28982;&#20445;&#25345;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a Guided Attention (GA) auxiliary training loss, which improves the effectiveness and robustness of automatic speech recognition (ASR) contextual biasing without introducing additional parameters. A common challenge in previous literature is that the word error rate (WER) reduction brought by contextual biasing diminishes as the number of bias phrases increases. To address this challenge, we employ a GA loss as an additional training objective besides the Transducer loss. The proposed GA loss aims to teach the cross attention how to align bias phrases with text tokens or audio frames. Compared to studies with similar motivations, the proposed loss operates directly on the cross attention weights and is easier to implement. Through extensive experiments based on Conformer Transducer with Contextual Adapter, we demonstrate that the proposed method not only leads to a lower WER but also retains its effectiveness as the number of bias phrases increases. Specifical
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35821;&#38899;&#34920;&#24449;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#20351;&#29992;&#20114;&#20449;&#24687;&#26469;&#24230;&#37327;&#34920;&#24449;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#25506;&#27979;&#22120;&#35780;&#20272;&#20102;&#34920;&#24449;&#19982;&#30446;&#26631;&#20449;&#24687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#20026;&#27169;&#22411;&#35774;&#35745;&#21644;&#36873;&#25321;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.08833</link><description>&lt;p&gt;
&#20174;&#20114;&#20449;&#24687;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35821;&#38899;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Revisiting Self-supervised Learning of Speech Representation from a Mutual Information Perspective. (arXiv:2401.08833v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35821;&#38899;&#34920;&#24449;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#20351;&#29992;&#20114;&#20449;&#24687;&#26469;&#24230;&#37327;&#34920;&#24449;&#36136;&#37327;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32447;&#24615;&#25506;&#27979;&#22120;&#35780;&#20272;&#20102;&#34920;&#24449;&#19982;&#30446;&#26631;&#20449;&#24687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#20026;&#27169;&#22411;&#35774;&#35745;&#21644;&#36873;&#25321;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#24449;&#23398;&#20064;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24320;&#21457;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#24182;&#24212;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20110;&#19981;&#21516;&#24212;&#29992;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#36136;&#37327;&#24448;&#24448;&#26159;&#36890;&#36807;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#26469;&#34913;&#37327;&#30340;&#12290;&#26377;&#20851;&#34920;&#24449;&#22914;&#20309;&#35775;&#38382;&#25152;&#38656;&#20449;&#24687;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#20449;&#24687;&#35770;&#30340;&#35282;&#24230;&#23545;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#26041;&#27861;&#36827;&#34892;&#20102;&#26356;&#28145;&#20837;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#26088;&#22312;&#20351;&#29992;&#20114;&#20449;&#24687;&#26469;&#24320;&#21457;&#24230;&#37327;&#26631;&#20934;&#65292;&#24110;&#21161;&#35299;&#20915;&#27169;&#22411;&#35774;&#35745;&#21644;&#36873;&#25321;&#31561;&#23454;&#38469;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#32447;&#24615;&#25506;&#27979;&#22120;&#20272;&#35745;&#30446;&#26631;&#20449;&#24687;&#19982;&#23398;&#20064;&#21040;&#30340;&#34920;&#24449;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#23637;&#31034;&#20102;&#20174;&#35821;&#38899;&#34920;&#24449;&#20013;&#35775;&#38382;&#30446;&#26631;&#20449;&#24687;&#30340;&#21478;&#19968;&#31181;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#33258;&#30417;&#30563;&#26041;&#24335;&#35780;&#20272;&#34920;&#24449;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20272;&#35745;&#19981;&#21516;&#25968;&#25454;&#37096;&#20998;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing studies on self-supervised speech representation learning have focused on developing new training methods and applying pre-trained models for different applications. However, the quality of these models is often measured by the performance of different downstream tasks. How well the representations access the information of interest is less studied. In this work, we take a closer look into existing self-supervised methods of speech from an information-theoretic perspective. We aim to develop metrics using mutual information to help practical problems such as model design and selection. We use linear probes to estimate the mutual information between the target information and learned representations, showing another insight into the accessibility to the target information from speech representations. Further, we explore the potential of evaluating representations in a self-supervised fashion, where we estimate the mutual information between different parts of the data without u
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#26426;&#22120;&#29983;&#25104;&#30340;&#31038;&#20132;&#23186;&#20307;&#39184;&#21381;&#35780;&#35770;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;AiGen-FoodReview&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#37492;&#21035;&#30495;&#23454;&#21644;&#34394;&#20551;&#30340;&#35780;&#35770;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#35835;&#24615;&#21644;&#25668;&#24433;&#29702;&#35770;&#30340;&#23646;&#24615;&#35780;&#20998;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#23646;&#24615;&#20316;&#20026;&#25163;&#24037;&#29305;&#24449;&#22312;&#21487;&#20280;&#32553;&#21644;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08825</link><description>&lt;p&gt;
AiGen-FoodReview:&#19968;&#31181;&#22810;&#27169;&#24577;&#30340;&#26426;&#22120;&#29983;&#25104;&#30340;&#31038;&#20132;&#23186;&#20307;&#39184;&#21381;&#35780;&#35770;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
AiGen-FoodReview: A Multimodal Dataset of Machine-Generated Restaurant Reviews and Images on Social Media. (arXiv:2401.08825v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#26426;&#22120;&#29983;&#25104;&#30340;&#31038;&#20132;&#23186;&#20307;&#39184;&#21381;&#35780;&#35770;&#21644;&#22270;&#20687;&#25968;&#25454;&#38598;AiGen-FoodReview&#65292;&#24182;&#24320;&#21457;&#20102;&#30456;&#24212;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#37492;&#21035;&#30495;&#23454;&#21644;&#34394;&#20551;&#30340;&#35780;&#35770;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#35835;&#24615;&#21644;&#25668;&#24433;&#29702;&#35770;&#30340;&#23646;&#24615;&#35780;&#20998;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#23646;&#24615;&#20316;&#20026;&#25163;&#24037;&#29305;&#24449;&#22312;&#21487;&#20280;&#32553;&#21644;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#35780;&#35770;&#20316;&#20026;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#65288;UGC&#65289;&#26174;&#33879;&#24433;&#21709;&#28040;&#36153;&#32773;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#19981;&#20165;&#20154;&#20026;&#21046;&#36896;&#30340;&#34394;&#20551;&#20869;&#23481;&#65292;&#36824;&#26377;&#26426;&#22120;&#29983;&#25104;&#30340;&#20869;&#23481;&#37117;&#25361;&#25112;&#20102;UGC&#30340;&#21487;&#38752;&#24615;&#12290;&#20511;&#21161;&#20110;OpenAI&#30340;GPT-4-Turbo&#21644;DALL-E-2&#27169;&#22411;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;AiGen-FoodReview&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;20,144&#20010;&#39184;&#21381;&#35780;&#35770;-&#22270;&#20687;&#23545;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#20998;&#20026;&#30495;&#23454;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#37096;&#20998;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#22312;FLAVA&#19978;&#23454;&#29616;&#20102;99.80%&#30340;&#22810;&#27169;&#24577;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#21487;&#35835;&#24615;&#21644;&#25668;&#24433;&#29702;&#35770;&#30340;&#23646;&#24615;&#20998;&#21035;&#23545;&#35780;&#35770;&#21644;&#22270;&#20687;&#36827;&#34892;&#35780;&#20998;&#65292;&#35777;&#26126;&#20102;&#23427;&#20204;&#20316;&#20026;&#25163;&#24037;&#29305;&#24449;&#22312;&#21487;&#20280;&#32553;&#21644;&#21487;&#35299;&#37322;&#30340;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#23454;&#29992;&#24615;&#65292;&#20854;&#24615;&#33021;&#19982;&#23545;&#27604;&#30456;&#24403;&#12290;&#26412;&#25991;&#36890;&#36807;&#24320;&#28304;&#25968;&#25454;&#38598;&#21644;&#21457;&#24067;&#34394;&#20551;&#35780;&#35770;&#26816;&#27979;&#22120;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#25512;&#33616;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#20197;&#22686;&#24378;&#21487;&#38752;&#30340;UGC&#12290;
&lt;/p&gt;
&lt;p&gt;
Online reviews in the form of user-generated content (UGC) significantly impact consumer decision-making. However, the pervasive issue of not only human fake content but also machine-generated content challenges UGC's reliability. Recent advances in Large Language Models (LLMs) may pave the way to fabricate indistinguishable fake generated content at a much lower cost. Leveraging OpenAI's GPT-4-Turbo and DALL-E-2 models, we craft AiGen-FoodReview, a multi-modal dataset of 20,144 restaurant review-image pairs divided into authentic and machine-generated. We explore unimodal and multimodal detection models, achieving 99.80% multimodal accuracy with FLAVA. We use attributes from readability and photographic theories to score reviews and images, respectively, demonstrating their utility as hand-crafted features in scalable and interpretable detection models, with comparable performance. The paper contributes by open-sourcing the dataset and releasing fake review detectors, recommending its
&lt;/p&gt;</description></item><item><title>HuixiangDou&#26159;&#19968;&#31181;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25216;&#26415;&#21161;&#25163;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#24320;&#28304;&#31639;&#27861;&#39033;&#30446;&#30456;&#20851;&#38382;&#39064;&#30340;&#28145;&#20837;&#22238;&#31572;&#26469;&#21327;&#21161;&#31639;&#27861;&#24320;&#21457;&#20154;&#21592;&#12290;&#35813;&#21161;&#25163;&#24050;&#34987;&#25104;&#21151;&#25972;&#21512;&#21040;&#32676;&#32842;&#24037;&#20855;&#20013;&#65292;&#26377;&#25928;&#22320;&#22238;&#31572;&#29992;&#25143;&#30340;&#25216;&#26415;&#38382;&#39064;&#65292;&#24182;&#20855;&#22791;&#35780;&#20998;&#33021;&#21147;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#38271;&#19978;&#19979;&#25991;&#31561;&#20851;&#38190;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.08772</link><description>&lt;p&gt;
HuixiangDou&#65306;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#25216;&#26415;&#21161;&#25163;&#20811;&#26381;&#32676;&#32842;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
HuixiangDou: Overcoming Group Chat Scenarios with LLM-based Technical Assistance. (arXiv:2401.08772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08772
&lt;/p&gt;
&lt;p&gt;
HuixiangDou&#26159;&#19968;&#31181;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25216;&#26415;&#21161;&#25163;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#24320;&#28304;&#31639;&#27861;&#39033;&#30446;&#30456;&#20851;&#38382;&#39064;&#30340;&#28145;&#20837;&#22238;&#31572;&#26469;&#21327;&#21161;&#31639;&#27861;&#24320;&#21457;&#20154;&#21592;&#12290;&#35813;&#21161;&#25163;&#24050;&#34987;&#25104;&#21151;&#25972;&#21512;&#21040;&#32676;&#32842;&#24037;&#20855;&#20013;&#65292;&#26377;&#25928;&#22320;&#22238;&#31572;&#29992;&#25143;&#30340;&#25216;&#26415;&#38382;&#39064;&#65292;&#24182;&#20855;&#22791;&#35780;&#20998;&#33021;&#21147;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#38271;&#19978;&#19979;&#25991;&#31561;&#20851;&#38190;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#30340;&#25216;&#26415;&#21161;&#25163;HuixiangDou&#12290;&#35813;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#23545;&#19982;&#24320;&#28304;&#31639;&#27861;&#39033;&#30446;&#30456;&#20851;&#30340;&#38382;&#39064;&#25552;&#20379;&#28145;&#20837;&#30340;&#22238;&#31572;&#65292;&#22914;&#26469;&#33258;OpenMMLab&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#28145;&#24230;&#23398;&#20064;&#39033;&#30446;&#65292;&#26469;&#21327;&#21161;&#31639;&#27861;&#24320;&#21457;&#20154;&#21592;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#23558;&#35813;&#21161;&#25163;&#25972;&#21512;&#21040;&#21363;&#26102;&#28040;&#24687;&#24037;&#20855;&#65288;&#22914;&#24494;&#20449;&#21644;Lark&#65289;&#30340;&#32676;&#32842;&#20013;&#12290;&#36890;&#36807;&#20960;&#27425;&#36845;&#20195;&#25913;&#36827;&#21644;&#35797;&#39564;&#65292;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#25216;&#26415;&#32842;&#22825;&#21161;&#25163;&#65292;&#33021;&#22815;&#22312;&#19981;&#36896;&#25104;&#28040;&#24687;&#27867;&#28389;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#22238;&#31572;&#29992;&#25143;&#30340;&#25216;&#26415;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#21253;&#25324;: 1) &#20026;&#32676;&#32842;&#22330;&#26223;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#27969;&#27700;&#32447;; 2) &#39564;&#35777;&#20102;&#25991;&#26412;2&#21521;&#37327;&#22312;&#20219;&#21153;&#25298;&#32477;&#20013;&#30340;&#21487;&#38752;&#24615;&#33021;; 3) &#30830;&#23450;&#20102;&#25216;&#26415;&#21161;&#25163;&#20135;&#21697;&#20013;LLM&#30340;&#19977;&#20010;&#20851;&#38190;&#35201;&#27714;&#65292;&#21363;&#35780;&#20998;&#33021;&#21147;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#38271;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#24050;&#32463;&#23436;&#25104;&#20102;&#36825;&#20123;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
In this work, we present HuixiangDou, a technical assistant powered by Large Language Models (LLM). This system is designed to assist algorithm developers by providing insightful responses to questions related to open-source algorithm projects, such as computer vision and deep learning projects from OpenMMLab. We further explore the integration of this assistant into the group chats of instant messaging (IM) tools such as WeChat and Lark. Through several iterative improvements and trials, we have developed a sophisticated technical chat assistant capable of effectively answering users' technical questions without causing message flooding. This paper's contributions include: 1) Designing an algorithm pipeline specifically for group chat scenarios; 2) Verifying the reliable performance of text2vec in task rejection; 3) Identifying three critical requirements for LLMs in technical-assistant-like products, namely scoring ability, In-Context Learning (ICL), and Long Context. We have made th
&lt;/p&gt;</description></item><item><title>MMToM-QA&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23545;&#20110;&#20154;&#30340;&#24515;&#26234;&#29702;&#35770;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;BIP-ALM&#29992;&#20110;&#23454;&#29616;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.08743</link><description>&lt;p&gt;
MMToM-QA: &#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
MMToM-QA: Multimodal Theory of Mind Question Answering. (arXiv:2401.08743v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08743
&lt;/p&gt;
&lt;p&gt;
MMToM-QA&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#23545;&#20110;&#20154;&#30340;&#24515;&#26234;&#29702;&#35770;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;BIP-ALM&#29992;&#20110;&#23454;&#29616;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20154;&#20204;&#30340;&#24515;&#26234;&#26159;&#24320;&#21457;&#20855;&#26377;&#20154;&#31867;&#27700;&#24179;&#31038;&#20132;&#26234;&#33021;&#30340;&#26426;&#22120;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20284;&#20046;&#23637;&#29616;&#20986;&#26576;&#20123;&#24515;&#26234;&#29702;&#35299;&#30340;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24515;&#26234;&#29702;&#35770;&#22522;&#20934;&#20351;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#38598;-&#25110;&#32773;&#35270;&#39057;&#25110;&#32773;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#24515;&#26234;&#29702;&#35770;&#19981;&#20165;&#20165;&#26159;&#35270;&#39057;&#25110;&#25991;&#26412;&#29702;&#35299;&#12290;&#20154;&#20204;&#21487;&#20197;&#26681;&#25454;&#20174;&#20219;&#20309;&#21487;&#29992;&#25968;&#25454;&#20013;&#25552;&#21462;&#30340;&#27010;&#24565;&#34920;&#31034;&#65288;&#20363;&#22914;&#30446;&#26631;&#65292;&#20449;&#24565;&#65292;&#35745;&#21010;&#65289;&#28789;&#27963;&#22320;&#25512;&#29702;&#21478;&#19968;&#20010;&#20154;&#30340;&#24515;&#26234;&#65292;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#21253;&#25324;&#35270;&#35273;&#32447;&#32034;&#65292;&#35821;&#35328;&#21465;&#20107;&#25110;&#20004;&#32773;&#20860;&#26377;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#24515;&#26234;&#29702;&#35770;&#38382;&#31572;&#65288;MMToM-QA&#65289;&#22522;&#20934;&#12290;MMToM-QA&#22312;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#20851;&#20110;&#19968;&#20010;&#20154;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#30340;&#27963;&#21160;&#30340;&#19981;&#21516;&#31181;&#31867;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#19978;&#20840;&#38754;&#35780;&#20272;&#26426;&#22120;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#20026;&#20102;&#23454;&#29616;&#22810;&#27169;&#24577;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;BIP-ALM&#65288;&#36125;&#21494;&#26031;&#36870;&#21521;&#35268;&#21010;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind (ToM), the ability to understand people's minds, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets - either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person's mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data, which can include visual cues, linguistic narratives, or both. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person's activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Plannin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#32622;&#20449;&#24230;&#24341;&#23548;&#21644;&#22522;&#20110;&#26679;&#26412;&#30340;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#35823;&#20449;&#24687;&#28040;&#38500;&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#28151;&#21512;&#26694;&#26550;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2401.08694</link><description>&lt;p&gt;
&#32467;&#21512;&#32622;&#20449;&#24230;&#24341;&#23548;&#21644;&#22522;&#20110;&#26679;&#26412;&#30340;&#26041;&#27861;&#29992;&#20110;&#28040;&#38500;&#35823;&#20449;&#24687;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation. (arXiv:2401.08694v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#32622;&#20449;&#24230;&#24341;&#23548;&#21644;&#22522;&#20110;&#26679;&#26412;&#30340;&#26041;&#27861;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#35823;&#20449;&#24687;&#28040;&#38500;&#20013;&#30340;&#24187;&#35273;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#28151;&#21512;&#26694;&#26550;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#35823;&#20449;&#24687;&#28040;&#38500;&#30340;&#20027;&#35201;&#20505;&#36873;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#24187;&#35273;&#21644;&#36807;&#24230;&#33258;&#20449;&#30340;&#39044;&#27979;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#30452;&#25509;&#32622;&#20449;&#24230;&#24341;&#23548;&#21644;&#22522;&#20110;&#26679;&#26412;&#30340;&#19968;&#33268;&#24615;&#26041;&#27861;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35823;&#20449;&#24687;&#28040;&#38500;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#22522;&#20110;&#26679;&#26412;&#19968;&#33268;&#24615;&#26041;&#27861;&#30340;&#26657;&#20934;&#24615;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26679;&#26412;&#35268;&#27169;&#21644;&#38543;&#26426;&#27700;&#24179;&#30340;&#19968;&#33268;&#24615;&#30340;&#19981;&#21516;&#29305;&#24449;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#40065;&#26834;&#30340;&#25968;&#23383;&#21270;&#21475;&#22836;&#25552;&#31034;&#22312;&#21333;&#27493;&#21644;&#20004;&#27493;&#32622;&#20449;&#24230;&#24341;&#23548;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#21644;&#20998;&#24067;&#21464;&#21270;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#30456;&#21516;&#25552;&#31034;&#22312;&#19981;&#21516;&#29256;&#26412;&#30340;GPT&#21644;&#19981;&#21516;&#25968;&#23383;&#23610;&#24230;&#19979;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32467;&#21512;&#22522;&#20110;&#26679;&#26412;&#19968;&#33268;&#24615;&#21644;&#25968;&#23383;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#20026;GPT&#27169;&#22411;&#25552;&#20379;&#26356;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have emerged as prime candidates to tackle misinformation mitigation. However, existing approaches struggle with hallucinations and overconfident predictions. We propose an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions. We first investigate the calibration of sample-based consistency methods that exploit distinct features of consistency across sample sizes and stochastic levels. Next, we evaluate the performance and distributional shift of a robust numeric verbalization prompt across single vs. two-step confidence elicitation procedure. We also compare the performance of the same prompt with different versions of GPT and different numerical scales. Finally, we combine the sample-based consistency and verbalized methods to propose a hybrid framework that yields a better uncertainty estimation for GPT models. Overal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#25991;&#26412;&#30456;&#20284;&#24615;&#36827;&#34892;&#33258;&#21160;&#31572;&#26696;&#39564;&#35777;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#65292;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#20013;&#20248;&#20110;&#31070;&#32463;&#26041;&#27861;&#12290;&#36890;&#36807;&#27604;&#36739;&#25105;&#20204;&#30340;&#30417;&#30563;&#27169;&#22411;&#19982;&#20854;&#20182;&#25991;&#26412;&#30456;&#20284;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08688</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#30456;&#20284;&#24615;&#36827;&#34892;&#33258;&#21160;&#31572;&#26696;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Automated Answer Validation using Text Similarity. (arXiv:2401.08688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08688
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#25991;&#26412;&#30456;&#20284;&#24615;&#36827;&#34892;&#33258;&#21160;&#31572;&#26696;&#39564;&#35777;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#19968;&#20010;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#65292;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#20013;&#20248;&#20110;&#31070;&#32463;&#26041;&#27861;&#12290;&#36890;&#36807;&#27604;&#36739;&#25105;&#20204;&#30340;&#30417;&#30563;&#27169;&#22411;&#19982;&#20854;&#20182;&#25991;&#26412;&#30456;&#20284;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31572;&#26696;&#39564;&#35777;&#21487;&#20197;&#36890;&#36807;&#20026;&#23398;&#20064;&#32773;&#25552;&#20379;&#36866;&#24403;&#30340;&#21453;&#39304;&#65292;&#20351;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#21644;&#22312;&#32447;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#26356;&#24191;&#27867;&#21487;&#29992;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#12290;&#22312;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#26041;&#38754;&#24050;&#32463;&#26377;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#22312;&#36825;&#20010;&#38382;&#39064;&#30340;&#22810;&#39033;&#36873;&#25321;&#29256;&#26412;&#20013;&#20248;&#20110;&#31070;&#32463;&#26041;&#27861;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#23402;&#29983;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#30417;&#30563;&#27169;&#22411;&#19982;&#20854;&#20182;&#22522;&#20110;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated answer validation can help improve learning outcomes by providing appropriate feedback to learners, and by making question answering systems and online learning solutions more widely available. There have been some works in science question answering which show that information retrieval methods outperform neural methods, especially in the multiple choice version of this problem. We implement Siamese neural network models and produce a generalised solution to this problem. We compare our supervised model with other text similarity based solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#38024;&#23545;&#25945;&#32946;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#20889;&#20316;&#12289;&#32534;&#31243;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#65292;&#26088;&#22312;&#25506;&#32034;&#20854;&#22312;&#26500;&#24314;&#19979;&#19968;&#20195;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.08664</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#25945;&#32946;&#65306;&#22522;&#26412;&#33021;&#21147;&#12289;&#28508;&#21147;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges. (arXiv:2401.08664v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#38024;&#23545;&#25945;&#32946;&#33021;&#21147;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#20889;&#20316;&#12289;&#32534;&#31243;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#65292;&#26088;&#22312;&#25506;&#32034;&#20854;&#22312;&#26500;&#24314;&#19979;&#19968;&#20195;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25945;&#32946;&#24179;&#21488;&#21033;&#29992;&#20114;&#32852;&#32593;&#20998;&#21457;&#25945;&#32946;&#36164;&#28304;&#65292;&#26088;&#22312;&#25552;&#20379;&#20415;&#25463;&#30340;&#25945;&#32946;&#65292;&#20294;&#24448;&#24448;&#22312;&#19982;&#23398;&#29983;&#30340;&#23454;&#26102;&#20132;&#27969;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#30001;&#20110;&#38656;&#35201;&#35299;&#20915;&#23398;&#29983;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#22810;&#26679;&#21270;&#38556;&#30861;&#30340;&#25361;&#25112;&#65292;&#23427;&#20204;&#32463;&#24120;&#38590;&#20197;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#25945;&#32946;&#36164;&#28304;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#25552;&#20379;&#20102;&#36890;&#36807;&#29702;&#35299;&#20010;&#20307;&#35831;&#27714;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;&#12290;&#34429;&#28982;LLMs&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22522;&#20110;LLM&#30340;&#25945;&#32946;&#31995;&#32479;&#30340;&#26500;&#24314;&#20173;&#28982;&#38754;&#20020;&#30528;&#24191;&#27867;&#30340;&#25945;&#32946;&#25216;&#33021;&#35201;&#27714;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#19982;&#25945;&#32946;&#33021;&#21147;&#30456;&#20851;&#30340;&#36817;&#26399;&#20986;&#29616;&#30340;LLM&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#23398;&#12289;&#20889;&#20316;&#12289;&#32534;&#31243;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#65292;&#26088;&#22312;&#25506;&#32034;&#23427;&#20204;&#22312;&#26500;&#24314;&#19979;&#19968;&#20195;&#26234;&#33021;&#25945;&#32946;&#31995;&#32479;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online education platforms, leveraging the internet to distribute education resources, seek to provide convenient education but often fall short in real-time communication with students. They often struggle to offer personalized education resources due to the challenge of addressing the diverse obstacles students encounter throughout their learning journey. Recently, the emergence of large language models (LLMs), such as ChatGPT, offers the possibility for resolving this issue by comprehending individual requests. Although LLMs have been successful in various fields, creating an LLM-based education system is still challenging for the wide range of educational skills required. This paper reviews the recently emerged LLM researches related to educational capabilities, including mathematics, writing, programming, reasoning, and knowledge-based question answering, with the aim to explore their potential in constructing the next-generation intelligent education system. Based on the current 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;Gemini Pro&#21644;GPT-4V&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4V&#22312;&#35780;&#20998;&#20934;&#30830;&#24230;&#21644;Quadratic Weighted Kappa&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;Gemini Pro&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;GPT-4V&#33021;&#22815;&#22788;&#29702;&#22270;&#20687;&#20013;&#30340;&#32454;&#33268;&#25991;&#26412;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#25972;&#20307;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.08660</link><description>&lt;p&gt;
Gemini Pro&#34987;GPT-4V&#20987;&#36133;&#65306;&#26469;&#33258;&#25945;&#32946;&#39046;&#22495;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Gemini Pro Defeated by GPT-4V: Evidence from Education. (arXiv:2401.08660v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;Gemini Pro&#21644;GPT-4V&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4V&#22312;&#35780;&#20998;&#20934;&#30830;&#24230;&#21644;Quadratic Weighted Kappa&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;Gemini Pro&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;GPT-4V&#33021;&#22815;&#22788;&#29702;&#22270;&#20687;&#20013;&#30340;&#32454;&#33268;&#25991;&#26412;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#25972;&#20307;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#27604;&#36739;&#20102;Gemini Pro&#21644;GPT-4V&#22312;&#20998;&#31867;&#24615;&#33021;&#19978;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;&#20351;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#20004;&#20010;&#27169;&#22411;&#22312;&#38405;&#35835;&#22522;&#20110;&#25991;&#26412;&#30340;&#35780;&#20998;&#26631;&#20934;&#24182;&#33258;&#21160;&#35780;&#20998;&#31185;&#23398;&#25945;&#32946;&#20013;&#23398;&#29983;&#32472;&#21046;&#30340;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#65292;&#24182;&#20351;&#29992;&#20102;&#20174;&#23398;&#29983;&#32472;&#21046;&#30340;&#31185;&#23398;&#27169;&#22411;&#20013;&#24471;&#20986;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;NERIF&#65288;Notation-Enhanced Rubrics for Image Feedback&#65289;&#25552;&#38382;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4V&#22312;&#35780;&#20998;&#20934;&#30830;&#24230;&#21644;Quadratic Weighted Kappa&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;Gemini Pro&#12290;&#23450;&#24615;&#20998;&#26512;&#26174;&#31034;&#65292;&#36825;&#31181;&#24046;&#24322;&#21487;&#33021;&#28304;&#20110;&#27169;&#22411;&#22312;&#22270;&#20687;&#20013;&#22788;&#29702;&#32454;&#33268;&#25991;&#26412;&#21644;&#25972;&#20307;&#22270;&#20687;&#20998;&#31867;&#24615;&#33021;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#21363;&#20351;&#36890;&#36807;&#36827;&#19968;&#27493;&#32553;&#23567;&#36755;&#20837;&#22270;&#20687;&#30340;NERIF&#26041;&#27861;&#26469;&#36866;&#24212;Gemini Pro&#65292;&#20854;&#24615;&#33021;&#20173;&#19981;&#22914;GPT-4V&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;GPT-4V&#22312;&#22788;&#29702;&#22797;&#26434;&#22810;&#26679;&#30340;&#23398;&#29983;&#32472;&#21046;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#26356;&#24378;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study compared the classification performance of Gemini Pro and GPT-4V in educational settings. Employing visual question answering (VQA) techniques, the study examined both models' abilities to read text-based rubrics and then automatically score student-drawn models in science education. We employed both quantitative and qualitative analyses using a dataset derived from student-drawn scientific models and employing NERIF (Notation-Enhanced Rubrics for Image Feedback) prompting methods. The findings reveal that GPT-4V significantly outperforms Gemini Pro in terms of scoring accuracy and Quadratic Weighted Kappa. The qualitative analysis reveals that the differences may be due to the models' ability to process fine-grained texts in images and overall image classification performance. Even adapting the NERIF approach by further de-sizing the input images, Gemini Pro seems not able to perform as well as GPT-4V. The findings suggest GPT-4V's superior capability in handling complex mu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#30340;&#20845;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#25361;&#25112;&#20013;&#25152;&#21462;&#24471;&#36827;&#23637;&#30340;&#23454;&#35777;&#21457;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#23545;&#24179;&#34892;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#24182;&#25193;&#23637;&#32763;&#35793;&#25991;&#26723;&#30340;&#38271;&#24230;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#19981;&#21305;&#37197;&#21644;&#32597;&#35265;&#35789;&#39044;&#27979;&#20173;&#28982;&#26159;&#38656;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.08350</link><description>&lt;p&gt;
&#21521;&#32463;&#20856;&#33268;&#25964;&#65306;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#26426;&#22120;&#32763;&#35793;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models. (arXiv:2401.08350v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#30340;&#20845;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#25361;&#25112;&#20013;&#25152;&#21462;&#24471;&#36827;&#23637;&#30340;&#23454;&#35777;&#21457;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#23545;&#24179;&#34892;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#24182;&#25193;&#23637;&#32763;&#35793;&#25991;&#26723;&#30340;&#38271;&#24230;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#19981;&#21305;&#37197;&#21644;&#32597;&#35265;&#35789;&#39044;&#27979;&#20173;&#28982;&#26159;&#38656;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793; (NMT) &#30340;&#21457;&#23637;&#21463;&#21040;&#20845;&#20010;&#26680;&#24515;&#25361;&#25112;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#36825;&#20123;&#25361;&#25112;&#20026;&#36825;&#20010;&#39046;&#22495;&#30340;&#36827;&#23637;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;&#26412;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#20808;&#36827;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#32972;&#26223;&#19979;&#65292;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#25361;&#25112;&#25345;&#32493;&#30456;&#20851;&#24615;&#30340;&#28145;&#20837;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;LLM&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#23545;&#24179;&#34892;&#25968;&#25454;&#30340;&#20381;&#36182;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20027;&#35201;&#35821;&#35328;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;LLM&#30340;&#32763;&#35793;&#31995;&#32479;&#26174;&#33879;&#25552;&#39640;&#20102;&#32763;&#35793;&#32422;80&#20010;&#21333;&#35789;&#30340;&#38271;&#21477;&#23376;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#33021;&#22815;&#32763;&#35793;&#38271;&#36798;512&#20010;&#21333;&#35789;&#30340;&#25991;&#26723;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#39046;&#22495;&#19981;&#21305;&#37197;&#21644;&#32597;&#35265;&#35789;&#39044;&#27979;&#20173;&#28982;&#26159;&#25361;&#25112;&#12290;&#22312;&#35299;&#20915;&#21333;&#35789;&#23545;&#40784;&#21644;&#20122;&#26368;&#20248;&#25628;&#32034;&#30340;&#25361;&#25112;&#26041;&#38754;&#65292;LLM&#20173;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution of Neural Machine Translation (NMT) has been significantly influenced by six core challenges (Koehn and Knowles, 2017), which have acted as benchmarks for progress in this field. This study revisits these challenges, offering insights into their ongoing relevance in the context of advanced Large Language Models (LLMs): domain mismatch, amount of parallel data, rare word prediction, translation of long sentences, attention model as word alignment, and sub-optimal beam search. Our empirical findings indicate that LLMs effectively lessen the reliance on parallel data for major languages in the pretraining phase. Additionally, the LLM-based translation system significantly enhances the translation of long sentences that contain approximately 80 words and shows the capability to translate documents of up to 512 words. However, despite these significant improvements, the challenges of domain mismatch and prediction of rare words persist. While the challenges of word alignment a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#36827;&#34892;&#39640;&#26031;&#22122;&#22768;&#37319;&#26679;&#65292;&#23454;&#29616;&#20102;&#23545;&#30693;&#35782;&#32534;&#36753;&#30340;&#19978;&#19979;&#25991;&#19968;&#33268;&#24615;&#36827;&#34892;&#26356;&#22909;&#30340;&#25512;&#24191;&#65292;&#25552;&#39640;&#20102;&#32534;&#36753;&#30340;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.07544</link><description>&lt;p&gt;
&#30475;&#35265;&#26410;&#35265;&#65306;&#36890;&#36807;&#22122;&#22768;&#23454;&#29616;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#19968;&#33268;&#24615;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
See the Unseen: Better Context-Consistent Knowledge-Editing by Noises. (arXiv:2401.07544v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.07544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#36827;&#34892;&#39640;&#26031;&#22122;&#22768;&#37319;&#26679;&#65292;&#23454;&#29616;&#20102;&#23545;&#30693;&#35782;&#32534;&#36753;&#30340;&#19978;&#19979;&#25991;&#19968;&#33268;&#24615;&#36827;&#34892;&#26356;&#22909;&#30340;&#25512;&#24191;&#65292;&#25552;&#39640;&#20102;&#32534;&#36753;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#32534;&#36753;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#30693;&#35782;&#65292;&#24182;&#20026;LLM&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24212;&#29992;&#20570;&#20986;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#24212;&#29992;&#26159;&#19978;&#19979;&#25991;&#19968;&#33268;&#30340;&#65306;LLM&#21487;&#20197;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#22238;&#24518;&#30456;&#21516;&#30340;&#30693;&#35782;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#24573;&#35270;&#20102;&#36825;&#19968;&#23646;&#24615;&#65292;&#24182;&#19988;&#32534;&#36753;&#32570;&#20047;&#26222;&#36866;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#19981;&#21516;&#19978;&#19979;&#25991;&#23545;&#20110;LLM&#22312;&#22238;&#24518;&#30456;&#21516;&#30693;&#35782;&#26102;&#30340;&#24433;&#21709;&#31526;&#21512;&#39640;&#26031;&#20998;&#24067;&#12290;&#25105;&#20204;&#38543;&#21518;&#23545;&#39640;&#26031;&#22122;&#22768;&#36827;&#34892;&#37319;&#26679;&#65292;&#27169;&#25311;&#22312;&#26356;&#26032;LLM&#26102;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#36825;&#26679;&#30340;&#26041;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;LLM&#30475;&#21040;&#23558;&#24212;&#29992;&#32534;&#36753;&#21518;&#30693;&#35782;&#30340;&#26410;&#35265;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#25913;&#21892;&#32534;&#36753;&#30340;&#25512;&#24191;&#24615;&#12290;&#23545;&#19977;&#20010;LLM&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#36890;&#36807;&#22122;&#22768;&#24494;&#35843;LLM&#30340;&#20854;&#20182;&#26041;&#27861;&#21306;&#20998;&#24320;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-editing updates knowledge of large language models (LLMs) and contributes to the interpretability and application of LLMs. However, knowledge applying is context-consistent: LLMs can recall the same knowledge in different contexts. Existing works ignore this property and the editing lacks generalization. In this paper, we empirically find that the effects of different contexts upon LLMs in recalling the same knowledge follow a Gaussian-like distribution. We then sample Gaussian noises to simulate the effects of different contexts when updating LLMs. By such, we can make LLMs see the unseen contexts where the edited knowledge will be applied, therefore improving the editing generalization. Experimental results on three LLMs demonstrate the effectiveness of our methods and also distinguish our methods from the others of fine-tuning LLMs by noises.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#33258;&#21160;&#32454;&#31890;&#24230;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#32508;&#21512;&#20998;&#31867;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#22823;&#37096;&#20998;&#24187;&#35273;&#23646;&#20110;&#23569;&#26377;&#30340;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#32773;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#26816;&#27979;&#21644;&#32416;&#27491;&#24187;&#35273;&#12290;</title><link>http://arxiv.org/abs/2401.06855</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#31890;&#24230;&#24187;&#35273;&#26816;&#27979;&#19982;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Hallucination Detection and Editing for Language Models. (arXiv:2401.06855v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06855
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#33258;&#21160;&#32454;&#31890;&#24230;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#32508;&#21512;&#20998;&#31867;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#22823;&#37096;&#20998;&#24187;&#35273;&#23646;&#20110;&#23569;&#26377;&#30340;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#32773;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#26816;&#27979;&#21644;&#32416;&#27491;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#20250;&#29983;&#25104;&#22810;&#26679;&#30340;&#20107;&#23454;&#19981;&#27491;&#30830;&#30340;&#38472;&#36848;&#65292;&#34987;&#24191;&#27867;&#31216;&#20026;&#24187;&#35273;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#31895;&#31890;&#24230;&#30340;&#33258;&#21160;&#24187;&#35273;&#26816;&#27979;&#25110;&#32534;&#36753;&#19978;&#65292;&#24573;&#35270;&#20102;&#32454;&#24494;&#30340;&#38169;&#35823;&#32423;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#8212;&#8212;&#33258;&#21160;&#32454;&#31890;&#24230;&#24187;&#35273;&#26816;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20845;&#20010;&#23618;&#27425;&#20998;&#26126;&#30340;&#24187;&#35273;&#31867;&#22411;&#30340;&#32508;&#21512;&#20998;&#31867;&#27861;&#12290;&#20026;&#20102;&#20415;&#20110;&#35780;&#20272;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#20004;&#20010;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#22312;&#21508;&#20010;&#39046;&#22495;&#19978;&#36827;&#34892;&#32454;&#31890;&#24230;&#20154;&#24037;&#21028;&#26029;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;ChatGPT&#21644;Llama 2-Chat&#30340;&#36755;&#20986;&#20013;&#26377;60%&#21644;75%&#30340;&#24187;&#35273;&#65292;&#20854;&#20013;&#22810;&#25968;&#24187;&#35273;&#23646;&#20110;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#31867;&#21035;&#12290;&#20316;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#21021;&#22987;&#27493;&#39588;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;FAVA&#65292;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26469;&#26816;&#27979;&#21644;&#32416;&#27491;&#32454;&#31890;&#24230;&#24187;&#35273;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) are prone to generate diverse factually incorrect statements, which are widely called hallucinations. Current approaches predominantly focus on coarse-grained automatic hallucination detection or editing, overlooking nuanced error levels. In this paper, we propose a novel task -- automatic fine-grained hallucination detection -- and present a comprehensive taxonomy encompassing six hierarchically defined types of hallucination. To facilitate evaluation, we introduce a new benchmark that includes fine-grained human judgments on two LM outputs across various domains. Our analysis reveals that ChatGPT and Llama 2-Chat exhibit hallucinations in 60% and 75% of their outputs, respectively, and a majority of these hallucinations fall into categories that have been underexplored. As an initial step to address this, we train FAVA, a retrieval-augmented LM by carefully designing synthetic data generations to detect and correct fine-grained hallucinations. On our bench
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;AGI&#65288;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65289;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#24635;&#32467;&#20102;&#24403;&#21069;&#23545;AGI&#24187;&#35273;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2401.06792</link><description>&lt;p&gt;
LightHouse: AGI&#24187;&#35273;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
LightHouse: A Survey of AGI Hallucination. (arXiv:2401.06792v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06792
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;AGI&#65288;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65289;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#24635;&#32467;&#20102;&#24403;&#21069;&#23545;AGI&#24187;&#35273;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#22823;&#35268;&#27169;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#26234;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#22823;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#25104;&#20026;&#20102;&#38459;&#30861;AI&#30740;&#31350;&#21457;&#23637;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#23454;&#29616;&#24378;&#20154;&#24037;&#26234;&#33021;&#65292;&#22823;&#37327;&#30340;&#30740;&#31350;&#24037;&#20316;&#27491;&#22312;&#25237;&#20837;&#21040;AGI&#65288;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65289;&#24187;&#35273;&#30740;&#31350;&#20013;&#12290;&#20197;&#24448;&#30340;&#25506;&#32034;&#20027;&#35201;&#38598;&#20013;&#22312;&#30740;&#31350;LLM&#65288;&#22823;&#35821;&#35328;&#27169;&#22411;&#65289;&#20013;&#30340;&#24187;&#35273;&#65292;&#32780;&#23545;&#20110;&#22810;&#27169;&#24577;AGI&#65292;&#24187;&#35273;&#30740;&#31350;&#20173;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25512;&#21160;&#24187;&#35273;&#29616;&#35937;&#39046;&#22495;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;AGI&#24187;&#35273;&#30340;&#24635;&#35272;&#65292;&#24635;&#32467;&#20102;&#24403;&#21069;&#23545;AGI&#24187;&#35273;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of artificial intelligence, large-scale models have become increasingly intelligent. However, numerous studies indicate that hallucinations within these large models are a bottleneck hindering the development of AI research. In the pursuit of achieving strong artificial intelligence, a significant volume of research effort is being invested in the AGI (Artificial General Intelligence) hallucination research. Previous explorations have been conducted in researching hallucinations within LLMs (Large Language Models). As for multimodal AGI, research on hallucinations is still in an early stage. To further the progress of research in the domain of hallucinatory phenomena, we present a bird's eye view of hallucinations in AGI, summarizing the current work on AGI hallucinations and proposing some directions for future research.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#21313;&#20010;&#36136;&#37327;&#21644;&#35821;&#35328;&#35782;&#21035;&#36807;&#28388;&#22120;&#23545;&#19981;&#21516;&#31038;&#20132;&#32500;&#24230;&#21464;&#21270;&#30340;&#32593;&#39029;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#21457;&#29616;&#22312;&#25968;&#25454;&#31579;&#36873;&#36807;&#31243;&#20013;&#23384;&#22312;&#38544;&#21547;&#30340;&#20559;&#22909;&#65292;&#19968;&#20123;&#36136;&#37327;&#20998;&#31867;&#22120;&#31867;&#20284;&#20110;&#20027;&#39064;&#36807;&#28388;&#22120;&#65292;&#32780;&#35821;&#35328;&#35782;&#21035;&#21487;&#33021;&#20250;&#24573;&#35270;&#26576;&#20123;&#22320;&#21306;&#30340;&#33521;&#35821;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#20419;&#36827;&#26356;&#20844;&#27491;&#21644;&#20840;&#38754;&#30340;&#27169;&#22411;&#24320;&#21457;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2401.06408</link><description>&lt;p&gt;
&#20851;&#20110;&#25105;&#65306;&#20351;&#29992;&#33258;&#25105;&#25551;&#36848;&#30340;&#32593;&#39029;&#26469;&#35760;&#24405;&#33521;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#36807;&#28388;&#22120;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters. (arXiv:2401.06408v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06408
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#21313;&#20010;&#36136;&#37327;&#21644;&#35821;&#35328;&#35782;&#21035;&#36807;&#28388;&#22120;&#23545;&#19981;&#21516;&#31038;&#20132;&#32500;&#24230;&#21464;&#21270;&#30340;&#32593;&#39029;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#21457;&#29616;&#22312;&#25968;&#25454;&#31579;&#36873;&#36807;&#31243;&#20013;&#23384;&#22312;&#38544;&#21547;&#30340;&#20559;&#22909;&#65292;&#19968;&#20123;&#36136;&#37327;&#20998;&#31867;&#22120;&#31867;&#20284;&#20110;&#20027;&#39064;&#36807;&#28388;&#22120;&#65292;&#32780;&#35821;&#35328;&#35782;&#21035;&#21487;&#33021;&#20250;&#24573;&#35270;&#26576;&#20123;&#22320;&#21306;&#30340;&#33521;&#35821;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#20419;&#36827;&#26356;&#20844;&#27491;&#21644;&#20840;&#38754;&#30340;&#27169;&#22411;&#24320;&#21457;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#26469;&#28304;&#20110;&#23427;&#20204;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#27169;&#22411;&#30340;&#24320;&#21457;&#22987;&#20110;&#25968;&#25454;&#30340;&#31579;&#36873;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#21021;&#27493;&#38454;&#27573;&#20915;&#23450;&#20445;&#30041;&#21738;&#20123;&#25968;&#25454;&#25110;&#31227;&#38500;&#21738;&#20123;&#25968;&#25454;&#30340;&#20915;&#31574;&#24120;&#24120;&#27809;&#26377;&#34987;&#20805;&#20998;&#23457;&#26597;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#32593;&#39029;&#25991;&#26412;&#19982;&#20854;&#31038;&#20132;&#21644;&#22320;&#29702;&#32972;&#26223;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1030&#19975;&#20010;&#32593;&#39029;&#21019;&#24314;&#32773;&#30340;&#33258;&#25105;&#25551;&#36848;&#65292;&#24182;&#25552;&#21462;&#20102;&#20851;&#20110;&#20182;&#20204;&#30340;&#20010;&#20154;&#20449;&#24687;&#20197;&#21450;&#20182;&#20204;&#26469;&#33258;&#21738;&#37324;&#30340;&#20449;&#24687;&#65306;&#20182;&#20204;&#30340;&#20852;&#36259;&#39046;&#22495;&#12289;&#31038;&#20132;&#35282;&#33394;&#21644;&#22320;&#29702;&#24402;&#23646;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#39033;&#30740;&#31350;&#65292;&#35843;&#26597;&#20102;&#21313;&#20010;&#8220;&#36136;&#37327;&#8221;&#21644;&#33521;&#35821;&#35821;&#35328;&#35782;&#21035;&#65288;langID&#65289;&#36807;&#28388;&#22120;&#23545;&#36825;&#20123;&#31038;&#20132;&#32500;&#24230;&#21464;&#21270;&#30340;&#32593;&#39029;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#25968;&#25454;&#31579;&#36873;&#20013;&#19968;&#31995;&#21015;&#38544;&#21547;&#30340;&#20559;&#22909;&#65306;&#25105;&#20204;&#23637;&#31034;&#20986;&#19968;&#20123;&#36136;&#37327;&#20998;&#31867;&#22120;&#30340;&#20316;&#29992;&#31867;&#20284;&#20110;&#20027;&#39064;&#39046;&#22495;&#36807;&#28388;&#22120;&#65292;&#32780;langID&#21487;&#33021;&#20250;&#24573;&#35270;&#19990;&#30028;&#26576;&#20123;&#22320;&#21306;&#30340;&#33521;&#35821;&#20869;&#23481;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#22815;&#25552;&#20379;&#23545;&#25968;&#25454;&#31579;&#36873;&#20013;&#38544;&#21547;&#20559;&#22909;&#30340;&#27934;&#23519;&#65292;&#20197;&#20419;&#36827;&#26356;&#20844;&#27491;&#21644;&#20840;&#38754;&#30340;&#27169;&#22411;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models' (LLMs) abilities are drawn from their pretraining data, and model development begins with data curation. However, decisions around what data is retained or removed during this initial stage is under-scrutinized. In our work, we ground web text, which is a popular pretraining data source, to its social and geographic contexts. We create a new dataset of 10.3 million self-descriptions of website creators, and extract information about who they are and where they are from: their topical interests, social roles, and geographic affiliations. Then, we conduct the first study investigating how ten "quality" and English language identification (langID) filters affect webpages that vary along these social dimensions. Our experiments illuminate a range of implicit preferences in data curation: we show that some quality classifiers act like topical domain filters, and langID can overlook English content from some regions of the world. Overall, we hope that our work will enc
&lt;/p&gt;</description></item><item><title>AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05268</link><description>&lt;p&gt;
AUTOACT&#65306;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#23454;&#29616;&#30340;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05268
&lt;/p&gt;
&lt;p&gt;
AUTOACT&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#20027;&#35268;&#21010;&#21512;&#25104;&#36712;&#36857;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#26356;&#22909;&#25110;&#31867;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#19981;&#26029;&#30340;&#25506;&#32034;&#65292;&#20294;&#29616;&#26377;&#30340;&#35821;&#35328;&#20195;&#29702;&#31995;&#32479;&#20173;&#28982;&#38754;&#20020;&#26114;&#36149;&#12289;&#19981;&#21487;&#37325;&#22797;&#30340;&#25968;&#25454;&#20381;&#36182;&#38382;&#39064;&#65292;&#24182;&#19988;&#38754;&#20020;&#23558;&#21333;&#19968;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20010;&#21151;&#33021;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoAct&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#20195;&#29702;&#23398;&#20064;&#26694;&#26550;&#65292;&#19981;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#21644;&#26469;&#33258;&#38381;&#28304;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#30340;&#21512;&#25104;&#36712;&#36857;&#12290;&#32473;&#23450;&#26377;&#38480;&#30340;&#25968;&#25454;&#21644;&#24037;&#20855;&#24211;&#65292;AutoAct&#39318;&#20808;&#33258;&#21160;&#21512;&#25104;&#35268;&#21010;&#36712;&#36857;&#65292;&#19981;&#38656;&#35201;&#20154;&#31867;&#25110;&#24378;&#38381;&#28304;&#27169;&#22411;&#30340;&#20219;&#20309;&#36741;&#21161;&#12290;&#28982;&#21518;&#65292;AutoAct&#21033;&#29992;&#20998;&#24037;&#31574;&#30053;&#65292;&#26681;&#25454;&#30446;&#26631;&#20219;&#21153;&#20449;&#24687;&#21644;&#21512;&#25104;&#36712;&#36857;&#33258;&#21160;&#21306;&#20998;&#65292;&#20135;&#29983;&#19968;&#20010;&#23376;&#20195;&#29702;&#32452;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#31181;LLMs&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;AutoAct&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#25110;&#19982;&#20854;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language agents have achieved considerable performance on various complex tasks. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework that does not rely on large-scale annotated data and synthetic trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to var
&lt;/p&gt;</description></item><item><title>MLLM-Protektor&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#25915;&#20987;&#12290;&#36890;&#36807;&#35299;&#20915;&#22270;&#20687;&#19982;&#25991;&#26412;&#23545;&#40784;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;MLLM&#30340;&#26377;&#25928;&#20445;&#25252;&#65292;&#38450;&#27490;&#20854;&#20135;&#29983;&#26377;&#23475;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2401.02906</link><description>&lt;p&gt;
MLLM-Protektor: &#30830;&#20445;MLLM&#30340;&#23433;&#20840;&#24615;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance. (arXiv:2401.02906v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02906
&lt;/p&gt;
&lt;p&gt;
MLLM-Protektor&#26159;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20813;&#21463;&#24694;&#24847;&#25915;&#20987;&#12290;&#36890;&#36807;&#35299;&#20915;&#22270;&#20687;&#19982;&#25991;&#26412;&#23545;&#40784;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;MLLM&#30340;&#26377;&#25928;&#20445;&#25252;&#65292;&#38450;&#27490;&#20854;&#20135;&#29983;&#26377;&#23475;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLM)&#30340;&#37096;&#32626;&#24102;&#26469;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#33030;&#24369;&#24615;&#65306;&#23545;&#35270;&#35273;&#36755;&#20837;&#30340;&#24694;&#24847;&#25915;&#20987;&#26131;&#21463;&#25915;&#20987;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20445;&#25252;MLLM&#20813;&#21463;&#27492;&#31867;&#25915;&#20987;&#30340;&#26032;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22270;&#20687;&#20316;&#20026;&#19968;&#31181;&#8220;&#22806;&#35821;&#8221;&#22312;&#23545;&#40784;&#36807;&#31243;&#20013;&#27809;&#26377;&#34987;&#32771;&#34385;&#21040;&#65292;&#36825;&#21487;&#33021;&#20351;MLLM&#26356;&#23481;&#26131;&#20135;&#29983;&#26377;&#23475;&#30340;&#21709;&#24212;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#19982;&#25991;&#26412;&#20013;&#25152;&#32771;&#34385;&#30340;&#31163;&#25955;&#26631;&#35760;&#19981;&#21516;&#65292;&#22270;&#20687;&#20449;&#21495;&#30340;&#36830;&#32493;&#24615;&#36136;&#22312;&#23545;&#40784;&#36807;&#31243;&#20013;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#20174;&#32780;&#20351;&#35206;&#30422;&#21487;&#33021;&#24773;&#26223;&#21464;&#24471;&#22256;&#38590;&#12290;&#36825;&#31181;&#33030;&#24369;&#24615;&#21152;&#21095;&#20102;&#19968;&#20010;&#20107;&#23454;&#65292;&#21363;&#24320;&#28304;&#30340;MLLM&#20027;&#35201;&#22312;&#26377;&#38480;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#36828;&#36828;&#23567;&#20110;&#24191;&#27867;&#30340;&#25991;&#26412;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#36825;&#20351;&#24471;MLLM&#22312;&#26126;&#30830;&#23545;&#40784;&#35843;&#25972;&#36807;&#31243;&#20013;&#26356;&#23481;&#26131;&#36951;&#24536;&#20854;&#21407;&#22987;&#33021;&#21147;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MLLM-Protektor&#65292;&#19968;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;
&lt;/p&gt;
&lt;p&gt;
The deployment of multimodal large language models (MLLMs) has brought forth a unique vulnerability: susceptibility to malicious attacks through visual inputs. We delve into the novel challenge of defending MLLMs against such attacks. We discovered that images act as a "foreign language" that is not considered during alignment, which can make MLLMs prone to producing harmful responses. Unfortunately, unlike the discrete tokens considered in text-based LLMs, the continuous nature of image signals presents significant alignment challenges, which poses difficulty to thoroughly cover the possible scenarios. This vulnerability is exacerbated by the fact that open-source MLLMs are predominantly fine-tuned on limited image-text pairs that is much less than the extensive text-based pretraining corpus, which makes the MLLMs more prone to catastrophic forgetting of their original abilities during explicit alignment tuning. To tackle these challenges, we introduce MLLM-Protector, a plug-and-play 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22768;&#35843;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#35821;&#38899;&#27169;&#24577;&#26469;&#26356;&#22909;&#22320;&#24314;&#27169;&#21475;&#35821;&#23545;&#35805;&#30340;&#35821;&#35328;&#20869;&#23481;&#21644;&#22768;&#35843;&#35821;&#35328;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.15316</link><description>&lt;p&gt;
&#22686;&#24378;&#21475;&#35821;&#23545;&#35805;&#30340;&#22768;&#35843;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue. (arXiv:2312.15316v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15316
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#22768;&#35843;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65292;&#21033;&#29992;&#25991;&#26412;&#21644;&#35821;&#38899;&#27169;&#24577;&#26469;&#26356;&#22909;&#22320;&#24314;&#27169;&#21475;&#35821;&#23545;&#35805;&#30340;&#35821;&#35328;&#20869;&#23481;&#21644;&#22768;&#35843;&#35821;&#35328;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#32842;&#22825;&#12289;&#25512;&#29702;&#21644;&#38382;&#31572;&#31561;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340; LLM &#21487;&#33021;&#24573;&#35270;&#20102;&#20851;&#38190;&#30340;&#22768;&#35843;&#35821;&#35328;&#20449;&#24687;&#65292;&#20363;&#22914;&#24773;&#24863;&#12289;&#24773;&#32490;&#21644;&#35821;&#35328;&#39118;&#26684;&#65292;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#23454;&#29616;&#33258;&#28982;&#30340;&#12289;&#31867;&#20284;&#20154;&#31867;&#30340;&#21475;&#35821;&#23545;&#35805;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#24403;&#36825;&#20123;&#20449;&#24687;&#36890;&#36807;&#22768;&#23398;&#32447;&#32034;&#20256;&#36798;&#26102;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#22768;&#35843;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120; (ParalinGPT)&#65292;&#23427;&#21033;&#29992;&#25991;&#26412;&#21644;&#35821;&#38899;&#27169;&#24577;&#26469;&#26356;&#22909;&#22320;&#24314;&#27169;&#21475;&#35821;&#23545;&#35805;&#30340;&#35821;&#35328;&#20869;&#23481;&#21644;&#22768;&#35843;&#35821;&#35328;&#23646;&#24615;&#12290;&#35813;&#27169;&#22411;&#23558;&#25991;&#26412;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#12289;&#35821;&#38899;&#23884;&#20837;&#21644;&#22768;&#35843;&#35821;&#35328;&#23646;&#24615;&#20316;&#20026;&#36755;&#20837;&#25552;&#31034;&#65292;&#25918;&#22312;&#19968;&#20010;&#20018;&#34892;&#22810;&#20219;&#21153;&#22810;&#27169;&#24577;&#26694;&#26550;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25353;&#29031;&#24403;&#21069;&#22768;&#35843;&#35821;&#35328;&#23646;&#24615;&#39044;&#27979;&#12289;&#22238;&#24212;&#22768;&#35843;&#35821;&#35328;&#23646;&#24615;&#39044;&#27979;&#21644;&#22238;&#24212;&#25991;&#26412;&#29983;&#25104;&#30340;&#39034;&#24207;&#36827;&#34892;&#24207;&#21015;&#21270;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated superior abilities in tasks such as chatting, reasoning, and question-answering. However, standard LLMs may ignore crucial paralinguistic information, such as sentiment, emotion, and speaking style, which are essential for achieving natural, human-like spoken conversation, especially when such information is conveyed by acoustic cues. We therefore propose Paralinguistics-enhanced Generative Pretrained Transformer (ParalinGPT), an LLM that utilizes text and speech modalities to better model the linguistic content and paralinguistic attributes of spoken dialogue. The model takes the conversational context of text, speech embeddings, and paralinguistic attributes as input prompts within a serialized multitasking multimodal framework. Specifically, our framework serializes tasks in the order of current paralinguistic attribute prediction, response paralinguistic attribute prediction, and response text generation with autoregressive conditionin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;SpiNNaker 2&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#19978;&#23454;&#29616;&#35821;&#35328;&#24314;&#27169;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20107;&#20214;&#30340;&#26550;&#26500;&#21644;&#22823;&#35268;&#27169;&#24322;&#27493;&#22788;&#29702;&#30340;&#30828;&#20214;&#65292;&#35813;&#26041;&#27861;&#26377;&#26395;&#22312;&#20943;&#23569;&#33021;&#32791;&#30340;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.09084</link><description>&lt;p&gt;
&#22312;SpiNNaker 2&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#19978;&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Language Modeling on a SpiNNaker 2 Neuromorphic Chip. (arXiv:2312.09084v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.09084
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;SpiNNaker 2&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#19978;&#23454;&#29616;&#35821;&#35328;&#24314;&#27169;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20107;&#20214;&#30340;&#26550;&#26500;&#21644;&#22823;&#35268;&#27169;&#24322;&#27493;&#22788;&#29702;&#30340;&#30828;&#20214;&#65292;&#35813;&#26041;&#27861;&#26377;&#26395;&#22312;&#20943;&#23569;&#33021;&#32791;&#30340;&#21516;&#26102;&#20445;&#25345;&#31454;&#20105;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#36805;&#36895;&#22686;&#38271;&#65292;&#25152;&#38656;&#30340;&#35745;&#31639;&#33021;&#21147;&#20063;&#22312;&#22686;&#21152;&#12290;&#22522;&#20110;&#31070;&#32463;&#24418;&#24577;&#35774;&#22791;&#19978;&#30340;&#20107;&#20214;&#39537;&#21160;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#31181;&#26174;&#33879;&#38477;&#20302;&#25512;&#29702;&#33021;&#32791;&#30340;&#28508;&#22312;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#22823;&#22810;&#25968;&#21487;&#20197;&#22312;&#31070;&#32463;&#24418;&#24577;&#30828;&#20214;&#19978;&#36816;&#34892;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#32593;&#32476;&#65292;&#21253;&#25324;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNN)&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#20219;&#21153;&#24615;&#33021;&#29978;&#33267;&#19981;&#33021;&#19982;LSTM&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#22240;&#27492;&#65292;&#22312;&#31070;&#32463;&#24418;&#24577;&#35774;&#22791;&#19978;&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;&#20284;&#20046;&#26159;&#19968;&#20010;&#36965;&#36828;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#22312;&#31070;&#32463;&#24418;&#24577;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411; - &#20855;&#20307;&#26469;&#35828;&#26159;&#22522;&#20110;&#26368;&#36817;&#21457;&#24067;&#30340;&#21517;&#20026;EGRU&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#26550;&#26500;&#30340;SpiNNaker 2&#33455;&#29255;&#12290;SpiNNaker 2&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#22823;&#35268;&#27169;&#24322;&#27493;&#22788;&#29702;&#30340;&#20247;&#26680;&#31070;&#32463;&#24418;&#24577;&#33455;&#29255;&#65292;&#32780;EGRU&#26159;&#20026;&#20102;&#22312;&#20445;&#25345;&#31454;&#20105;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#39640;&#25928;&#21033;&#29992;&#36825;&#31181;&#30828;&#20214;&#32780;&#35774;&#35745;&#30340;&#12290;&#36825;&#20010;&#23454;&#29616;&#26631;&#24535;&#30528;&#22312;&#31070;&#32463;&#24418;&#24577;&#35774;&#22791;&#19978;&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;&#30340;&#31532;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
As large language models continue to scale in size rapidly, so too does the computational power required to run them. Event-based networks on neuromorphic devices offer a potential way to reduce energy consumption for inference significantly. However, to date, most event-based networks that can run on neuromorphic hardware, including spiking neural networks (SNNs), have not achieved task performance even on par with LSTM models for language modeling. As a result, language modeling on neuromorphic devices has seemed a distant prospect. In this work, we demonstrate the first-ever implementation of a language model on a neuromorphic device - specifically the SpiNNaker 2 chip based on a recently published event-based architecture called the EGRU. SpiNNaker 2 is a many-core neuromorphic chip designed for large-scale asynchronous processing, while the EGRU is architected to leverage such hardware efficiently while maintaining competitive task performance. This implementation marks the firs
&lt;/p&gt;</description></item><item><title>TiMix&#26159;&#19968;&#31181;&#23558;&#25991;&#26412;&#24863;&#30693;&#30340;&#22270;&#20687;&#28151;&#21512;&#25216;&#26415;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#24182;&#20174;&#20114;&#20449;&#24687;&#30340;&#35282;&#24230;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#24182;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.08846</link><description>&lt;p&gt;
TiMix: &#25991;&#26412;&#24863;&#30693;&#22270;&#20687;&#28151;&#21512;&#29992;&#20110;&#26377;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training. (arXiv:2312.08846v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08846
&lt;/p&gt;
&lt;p&gt;
TiMix&#26159;&#19968;&#31181;&#23558;&#25991;&#26412;&#24863;&#30693;&#30340;&#22270;&#20687;&#28151;&#21512;&#25216;&#26415;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#24182;&#20174;&#20114;&#20449;&#24687;&#30340;&#35282;&#24230;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#24182;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#65288;SMCL&#65289;&#36890;&#36807;&#23545;&#40784;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#65292;&#26174;&#33879;&#25512;&#36827;&#20102;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32593;&#32476;&#25910;&#38598;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#20013;&#23384;&#22312;&#22122;&#22768;&#65292;&#25193;&#22823;SMCL&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#38754;&#20020;&#30528;&#30456;&#24403;&#22823;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#25552;&#39640;VLP&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#24863;&#30693;&#22270;&#20687;&#28151;&#21512;&#65288;TiMix&#65289;&#65292;&#23558;&#22522;&#20110;&#28151;&#21512;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#38598;&#25104;&#21040;SMCL&#20013;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#20174;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#30340;&#35282;&#24230;&#23545;TiMix&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#34920;&#26126;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#30340;&#28151;&#21512;&#25968;&#25454;&#26679;&#26412;&#38544;&#24335;&#22320;&#20316;&#20026;&#23545;&#27604;&#25439;&#22833;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#20351;&#29992;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;TiMix&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Multi-modal Contrastive Learning (SMCL) remarkably advances modern Vision-Language Pre-training (VLP) models by aligning visual and linguistic modalities. Due to noises in web-harvested text-image pairs, however, scaling up training data volume in SMCL presents considerable obstacles in terms of computational cost and data inefficiency. To improve data efficiency in VLP, we propose Text-aware Image Mixing (TiMix), which integrates mix-based data augmentation techniques into SMCL, yielding significant performance improvements without significantly increasing computational overhead. We provide a theoretical analysis of TiMixfrom a mutual information (MI) perspective, showing that mixed data samples for cross-modal contrastive learning implicitly serve as a regularizer for the contrastive loss. The experimental results demonstrate that TiMix exhibits a comparable performance on downstream tasks, even with a reduced amount of training data and shorter training time, when be
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#20219;&#21153;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;CLadder&#65292;&#24182;&#21033;&#29992;oracle&#22240;&#26524;&#25512;&#29702;&#24341;&#25806;&#23558;&#31526;&#21495;&#38382;&#39064;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22810;&#20010;LLMs&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#38142;&#24335;&#25512;&#29702;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2312.04350</link><description>&lt;p&gt;
CLadder: &#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models. (arXiv:2312.04350v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.04350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#20219;&#21153;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;CLadder&#65292;&#24182;&#21033;&#29992;oracle&#22240;&#26524;&#25512;&#29702;&#24341;&#25806;&#23558;&#31526;&#21495;&#38382;&#39064;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22810;&#20010;LLMs&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#38142;&#24335;&#25512;&#29702;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#30340;&#33021;&#21147;&#34987;&#24191;&#27867;&#35270;&#20026;&#26234;&#33021;&#30340;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#21542;&#36830;&#36143;&#22320;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#35780;&#20272;LLMs&#20013;&#30340;&#24120;&#35782;&#22240;&#26524;&#25512;&#29702;&#65292;&#26410;&#33021;&#35780;&#20272;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25353;&#29031;&#19968;&#32452;&#26126;&#30830;&#23450;&#20041;&#30340;&#24418;&#24335;&#35268;&#21017;&#25191;&#34892;&#22240;&#26524;&#25512;&#26029;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;NLP&#20219;&#21153;&#65292;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#22240;&#26524;&#25512;&#26029;&#65292;&#21463;&#21040;Judea Pearl&#31561;&#20154;&#25552;&#20986;&#30340;&#8220;&#22240;&#26524;&#25512;&#26029;&#24341;&#25806;&#8221;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;10K&#20010;&#26679;&#26412;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;CLadder&#65292;&#36890;&#36807;&#19968;&#31181;oracle&#22240;&#26524;&#25512;&#29702;&#24341;&#25806;&#65292;&#22522;&#20110;&#19968;&#32452;&#22240;&#26524;&#22270;&#21644;&#26597;&#35810;(&#32852;&#21512;&#12289;&#24178;&#39044;&#21644;&#21453;&#20107;&#23454;)&#65292;&#24471;&#21040;&#31526;&#21495;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#24182;&#23558;&#20854;&#32763;&#35793;&#20026;&#33258;&#28982;&#35821;&#35328;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#19978;&#30340;&#22810;&#20010;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#24341;&#20837;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#23450;&#21046;&#30340;&#38142;&#24335;&#25512;&#29702;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the "causal inference engine" postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#25925;&#38556;&#33410;&#28857;&#65292;&#24182;&#21033;&#29992;&#21069;&#21518;&#26102;&#38388;&#27573;&#20869;&#33410;&#28857;&#30340;&#29366;&#24577;&#26469;&#36741;&#21161;&#24403;&#21069;&#25925;&#38556;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20934;&#30830;&#21487;&#38752;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25925;&#38556;&#33410;&#28857;&#20256;&#25773;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2311.16522</link><description>&lt;p&gt;
&#30005;&#21147;&#31995;&#32479;&#20013;&#21160;&#24577;&#25925;&#38556;&#29305;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dynamic Fault Characteristics Evaluation in Power Grid. (arXiv:2311.16522v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#25925;&#38556;&#33410;&#28857;&#65292;&#24182;&#21033;&#29992;&#21069;&#21518;&#26102;&#38388;&#27573;&#20869;&#33410;&#28857;&#30340;&#29366;&#24577;&#26469;&#36741;&#21161;&#24403;&#21069;&#25925;&#38556;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20934;&#30830;&#21487;&#38752;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25925;&#38556;&#33410;&#28857;&#20256;&#25773;&#30340;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#24378;&#36816;&#32500;&#30340;&#26234;&#33021;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#36827;&#34892;&#25925;&#38556;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#21644;&#30693;&#35782;&#22270;&#35889;&#26469;&#35782;&#21035;&#25925;&#38556;&#33410;&#28857;&#12290;&#36890;&#36807;&#24341;&#20837;&#26102;&#38388;&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21069;&#21518;&#26102;&#38388;&#27573;&#20869;&#33410;&#28857;&#30340;&#29366;&#24577;&#26469;&#36741;&#21161;&#24403;&#21069;&#25925;&#38556;&#26816;&#27979;&#12290;&#20026;&#20102;&#39564;&#35777;&#33410;&#28857;&#29305;&#24449;&#30340;&#26377;&#25928;&#24615;&#65292;&#36824;&#36827;&#34892;&#20102;&#27599;&#20010;&#33410;&#28857;&#36755;&#20986;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#20998;&#26512;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20223;&#30495;&#22330;&#26223;&#20013;&#20934;&#30830;&#22320;&#23450;&#20301;&#25925;&#38556;&#33410;&#28857;&#65292;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#24314;&#27169;&#21487;&#20197;&#23450;&#24615;&#22320;&#32771;&#23519;&#25925;&#38556;&#22914;&#20309;&#22312;&#33410;&#28857;&#38388;&#20256;&#25773;&#65292;&#20026;&#20998;&#26512;&#25925;&#38556;&#33410;&#28857;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enhance the intelligence degree in operation and maintenance, a novel method for fault detection in power grids is proposed. The proposed GNN-based approach first identifies fault nodes through a specialized feature extraction method coupled with a knowledge graph. By incorporating temporal data, the method leverages the status of nodes from preceding and subsequent time periods to help current fault detection. To validate the effectiveness of the node features, a correlation analysis of the output features from each node was conducted. The results from experiments show that this method can accurately locate fault nodes in simulation scenarios with a remarkable accuracy. Additionally, the graph neural network based feature modeling allows for a qualitative examination of how faults spread across nodes, which provides valuable insights for analyzing fault nodes.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21464;&#30005;&#31449;&#21160;&#24577;&#25925;&#38556;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#36890;&#36807;&#38544;&#34255;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#65292;&#21033;&#29992;Neo4j&#22270;&#25968;&#25454;&#24211;&#21019;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#23454;&#29616;&#23545;&#21464;&#30005;&#31449;&#20013;&#38544;&#34255;&#21361;&#38505;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2311.13708</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21464;&#30005;&#31449;&#21160;&#24577;&#25925;&#38556;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Dynamic Fault Analysis in Substations Based on Knowledge Graphs. (arXiv:2311.13708v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13708
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#21464;&#30005;&#31449;&#21160;&#24577;&#25925;&#38556;&#20998;&#26512;&#26041;&#27861;&#65292;&#21033;&#29992;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#36890;&#36807;&#38544;&#34255;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#65292;&#21033;&#29992;Neo4j&#22270;&#25968;&#25454;&#24211;&#21019;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#23454;&#29616;&#23545;&#21464;&#30005;&#31449;&#20013;&#38544;&#34255;&#21361;&#38505;&#30340;&#21487;&#35270;&#21270;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#35782;&#21035;&#21464;&#30005;&#31449;&#38544;&#34255;&#21361;&#38505;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#20998;&#26512;&#26041;&#27861;&#12290;&#39318;&#20808;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#65292;&#28982;&#21518;&#21033;&#29992;&#22522;&#20110;Elastic-Search&#26500;&#24314;&#30340;&#28789;&#27963;&#20998;&#24067;&#24335;&#25628;&#32034;&#24341;&#25806;&#22788;&#29702;&#25968;&#25454;&#12290;&#25509;&#19979;&#26469;&#65292;&#20351;&#29992;&#38544;&#34255;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#26469;&#35757;&#32451;&#24341;&#25806;&#20013;&#30340;&#25968;&#25454;&#12290;&#32500;&#29305;&#27604;&#31639;&#27861;&#34987;&#25972;&#21512;&#36827;&#26469;&#35299;&#23494;&#38544;&#34255;&#29366;&#24577;&#24207;&#21015;&#65292;&#20415;&#20110;&#23545;&#19982;&#38544;&#34255;&#21361;&#38505;&#30456;&#20851;&#30340;&#23454;&#20307;&#36827;&#34892;&#20998;&#21106;&#21644;&#26631;&#27880;&#12290;&#26368;&#21518;&#65292;&#20351;&#29992;Neo4j&#22270;&#25968;&#25454;&#24211;&#21160;&#24577;&#21019;&#24314;&#30693;&#35782;&#22270;&#35889;&#26469;&#21487;&#35270;&#21270;&#21464;&#30005;&#31449;&#20013;&#30340;&#38544;&#34255;&#21361;&#38505;&#12290;&#36890;&#36807;&#23545;&#25991;&#26412;&#35760;&#24405;&#20013;&#25581;&#31034;&#30340;&#20855;&#20307;&#21464;&#30005;&#31449;&#30340;&#38544;&#34255;&#21361;&#38505;&#36827;&#34892;&#26696;&#20363;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the challenge of identifying hidden danger in substations from unstructured text, a novel dynamic analysis method is proposed. We first extract relevant information from the unstructured text, and then leverages a flexible distributed search engine built on Elastic-Search to handle the data. Following this, the hidden Markov model is employed to train the data within the engine. The Viterbi algorithm is integrated to decipher the hidden state sequences, facilitating the segmentation and labeling of entities related to hidden dangers. The final step involves using the Neo4j graph database to dynamically create a knowledge graph that visualizes hidden dangers in the substation. The effectiveness of the proposed method is demonstrated through a case analysis from a specific substation with hidden dangers revealed in the text records.
&lt;/p&gt;</description></item><item><title>LQ-LoRA&#26159;&#19968;&#31181;&#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#20869;&#23384;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#23558;&#27599;&#20010;&#39044;&#35757;&#32451;&#30697;&#38453;&#20998;&#35299;&#20026;&#39640;&#31934;&#24230;&#20302;&#31209;&#37096;&#20998;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#37327;&#21270;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#37197;&#32622;&#37327;&#21270;&#21442;&#25968;&#20197;&#21450;&#23545;&#37325;&#26500;&#30446;&#26631;&#36827;&#34892;&#21152;&#26435;&#30340;&#20248;&#21270;&#65292;&#24182;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;QLoRA&#21644;GPTQ-LoRA&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.12023</link><description>&lt;p&gt;
LQ-LoRA: &#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#29992;&#20110;&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning. (arXiv:2311.12023v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.12023
&lt;/p&gt;
&lt;p&gt;
LQ-LoRA&#26159;&#19968;&#31181;&#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#20869;&#23384;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#12290;&#23427;&#36890;&#36807;&#23558;&#27599;&#20010;&#39044;&#35757;&#32451;&#30697;&#38453;&#20998;&#35299;&#20026;&#39640;&#31934;&#24230;&#20302;&#31209;&#37096;&#20998;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#37327;&#21270;&#37096;&#20998;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#37197;&#32622;&#37327;&#21270;&#21442;&#25968;&#20197;&#21450;&#23545;&#37325;&#26500;&#30446;&#26631;&#36827;&#34892;&#21152;&#26435;&#30340;&#20248;&#21270;&#65292;&#24182;&#22312;&#24494;&#35843;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#20110;QLoRA&#21644;GPTQ-LoRA&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20869;&#23384;&#39640;&#25928;&#30340;&#33258;&#36866;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#36845;&#20195;&#31639;&#27861;&#23558;&#27599;&#20010;&#39044;&#35757;&#32451;&#30697;&#38453;&#20998;&#35299;&#20026;&#39640;&#31934;&#24230;&#20302;&#31209;&#37096;&#20998;&#21644;&#20869;&#23384;&#39640;&#25928;&#30340;&#37327;&#21270;&#37096;&#20998;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#37327;&#21270;&#37096;&#20998;&#20445;&#25345;&#22266;&#23450;&#65292;&#21482;&#26377;&#20302;&#31209;&#37096;&#20998;&#34987;&#26356;&#26032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#37327;&#21270;&#37096;&#20998;&#30340;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#34920;&#36798;&#65292;&#21487;&#20197;&#26681;&#25454;&#24635;&#20307;&#20869;&#23384;&#39044;&#31639;&#21160;&#24577;&#37197;&#32622;&#37327;&#21270;&#21442;&#25968;&#65288;&#20363;&#22914;&#27604;&#29305;&#23485;&#24230;&#12289;&#22359;&#22823;&#23567;&#65289;&#32473;&#23450;&#27599;&#20010;&#30697;&#38453;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#25968;&#25454;&#24863;&#30693;&#29256;&#26412;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#36817;&#20284;&#26469;&#21152;&#26435;&#30697;&#38453;&#20998;&#35299;&#36807;&#31243;&#20013;&#30340;&#37325;&#26500;&#30446;&#26631;&#12290;&#22312;RoBERTa&#21644;LLaMA-2&#65288;7B&#21644;70B&#65289;&#30340;&#24494;&#35843;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#20302;&#31209;&#21152;&#37327;&#21270;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#65288;LQ-LoRA&#65289;&#20248;&#20110;&#24378;&#22522;&#32447;&#26041;&#27861;QLoRA&#21644;GPTQ-LoRA&#65292;&#24182;&#23454;&#29616;&#20102;&#28608;&#36827;&#30340;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22885;&#20857;&#39764;&#26415;&#24072;&#27169;&#25311;&#26410;&#26469;&#35760;&#24518;&#21644;&#20855;&#26377;&#38887;&#24615;&#30340;&#23450;&#20301;&#36890;&#20449;&#65292;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#25506;&#32034;&#19982;&#31038;&#20132;&#36741;&#21161;&#26426;&#22120;&#20154;&#30340;&#27807;&#36890;&#35760;&#24518;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2311.05268</link><description>&lt;p&gt;
&#36890;&#36807;&#22885;&#20857;&#39764;&#26415;&#24072;&#27169;&#25311;&#26410;&#26469;&#35760;&#24518;&#21644;&#20855;&#26377;&#38887;&#24615;&#30340;&#23450;&#20301;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Modelling prospective memory and resilient situated communications via Wizard of Oz. (arXiv:2311.05268v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.05268
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22885;&#20857;&#39764;&#26415;&#24072;&#27169;&#25311;&#26410;&#26469;&#35760;&#24518;&#21644;&#20855;&#26377;&#38887;&#24615;&#30340;&#23450;&#20301;&#36890;&#20449;&#65292;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#25506;&#32034;&#19982;&#31038;&#20132;&#36741;&#21161;&#26426;&#22120;&#20154;&#30340;&#27807;&#36890;&#35760;&#24518;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25688;&#35201;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#23478;&#24237;&#29615;&#22659;&#20013;&#28041;&#21450;&#32769;&#24180;&#20154;&#21644;&#26426;&#22120;&#20154;&#30340;&#20154;&#26426;&#34892;&#21160;&#24773;&#26223;&#12290;&#35813;&#24773;&#26223;&#26088;&#22312;&#25506;&#32034;&#23545;&#20110;&#19982;&#31038;&#20132;&#36741;&#21161;&#26426;&#22120;&#20154;&#65288;SAR&#65289;&#36827;&#34892;&#27807;&#36890;&#30340;&#35760;&#24518;&#24314;&#27169;&#12290;&#35813;&#24773;&#26223;&#23558;&#33021;&#22815;&#25910;&#38598;&#20851;&#20110;&#35821;&#38899;&#25216;&#26415;&#25925;&#38556;&#21644;&#20154;&#26426;&#27807;&#36890;&#20013;&#28041;&#21450;&#20849;&#20139;&#35760;&#24518;&#30340;&#25968;&#25454;&#65292;&#36825;&#20123;&#25925;&#38556;&#21487;&#33021;&#21457;&#29983;&#22312;&#35832;&#22914;&#38899;&#20048;&#27427;&#36175;&#27963;&#21160;&#31561;&#26085;&#24120;&#27963;&#21160;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This abstract presents a scenario for human-robot action in a home setting involving an older adult and a robot. The scenario is designed to explore the envisioned modelling of memory for communication with a socially assistive robots (SAR). The scenario will enable the gathering of data on failures of speech technology and human-robot communication involving shared memory that may occur during daily activities such as a music-listening activity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DistilWhisper&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#29305;&#23450;&#19987;&#23478;&#36827;&#34892;&#36731;&#37327;&#32423;&#27169;&#22359;&#21270;ASR&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#25104;&#21151;&#24357;&#21512;&#20102;&#22810;&#20219;&#21153;&#35821;&#38899;&#27169;&#22411;&#22312;&#23569;&#25968;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#22810;&#20219;&#21153;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2311.01070</link><description>&lt;p&gt;
DistilWhisper&#65306;&#36890;&#36807;&#35821;&#35328;&#29305;&#23450;&#19987;&#23478;&#39640;&#25928;&#21387;&#32553;&#22810;&#20219;&#21153;&#35821;&#38899;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DistilWhisper: Efficient Distillation of Multi-task Speech Models via Language-Specific Experts. (arXiv:2311.01070v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DistilWhisper&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#29305;&#23450;&#19987;&#23478;&#36827;&#34892;&#36731;&#37327;&#32423;&#27169;&#22359;&#21270;ASR&#24494;&#35843;&#21644;&#30693;&#35782;&#33976;&#39311;&#65292;&#25104;&#21151;&#24357;&#21512;&#20102;&#22810;&#20219;&#21153;&#35821;&#38899;&#27169;&#22411;&#22312;&#23569;&#25968;&#35821;&#35328;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#22810;&#20219;&#21153;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Whisper&#26159;&#19968;&#20010;&#22810;&#20219;&#21153;&#21644;&#22810;&#35821;&#35328;&#30340;&#35821;&#38899;&#27169;&#22411;&#65292;&#28085;&#30422;99&#31181;&#35821;&#35328;&#12290;&#23427;&#22312;&#20854;&#28085;&#30422;&#30340;&#37096;&#20998;&#35821;&#35328;&#20013;&#33719;&#24471;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#32467;&#26524;&#65292;&#20294;&#22312;&#19968;&#20123;&#25968;&#37327;&#21487;&#35266;&#30340;&#23569;&#25968;&#35821;&#35328;&#20013;&#65292;&#35813;&#27169;&#22411;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#65292;&#23588;&#20854;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#29256;&#26412;&#20013;&#34920;&#29616;&#26356;&#20026;&#20005;&#37325;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DistilWhisper&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;ASR&#26041;&#38754;&#24357;&#21512;&#36825;&#20123;&#35821;&#35328;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21516;&#26102;&#20445;&#30041;&#22810;&#20219;&#21153;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#20248;&#21183;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#31574;&#30053;&#65306;&#20351;&#29992;&#35821;&#35328;&#29305;&#23450;&#19987;&#23478;&#23545;whisper-small&#36827;&#34892;&#36731;&#37327;&#32423;&#27169;&#22359;&#21270;ASR&#24494;&#35843;&#65292;&#24182;&#20174;whisper-large-v2&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#12290;&#36825;&#31181;&#21452;&#37325;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#20445;&#25345;&#22810;&#20219;&#21153;&#21644;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#30340;&#21516;&#26102;&#26377;&#25928;&#25552;&#21319;ASR&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#26631;&#20934;&#24494;&#35843;&#25110;LoRA&#36866;&#37197;&#22120;&#26356;&#26377;&#25928;&#65292;&#22312;&#30446;&#26631;&#35821;&#35328;&#20013;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whisper is a multitask and multilingual speech model covering 99 languages. It yields commendable automatic speech recognition (ASR) results in a subset of its covered languages, but the model still under-performs on a non-negligible number of under-represented languages, a problem exacerbated in smaller model versions. In this work, we propose DistilWhisper, an approach able to bridge the performance gap in ASR for these languages while retaining the advantages of multitask and multilingual capabilities. Our approach involves two key strategies: lightweight modular ASR fine-tuning of whisper-small using language-specific experts, and knowledge distillation from whisper-large-v2. This dual approach allows us to effectively boost ASR performance while keeping the robustness inherited from the multitask and multilingual pre-training. Results demonstrate that our approach is more effective than standard fine-tuning or LoRA adapters, boosting performance in the targeted languages for both 
&lt;/p&gt;</description></item><item><title>DialogueLLM&#26159;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#24773;&#24863;&#23545;&#35805;&#24494;&#35843;&#30340;&#19978;&#19979;&#25991;&#21644;&#24773;&#24863;&#30693;&#35782;&#35843;&#26657;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.11374</link><description>&lt;p&gt;
DialogueLLM: &#23545;&#24773;&#24863;&#35782;&#21035;&#20013;&#23545;&#35805;&#36827;&#34892;&#24773;&#22659;&#21644;&#24773;&#24863;&#30693;&#35782;&#35843;&#26657;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DialogueLLM: Context and Emotion Knowledge-Tuned Large Language Models for Emotion Recognition in Conversations. (arXiv:2310.11374v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11374
&lt;/p&gt;
&lt;p&gt;
DialogueLLM&#26159;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22810;&#27169;&#24577;&#24773;&#24863;&#23545;&#35805;&#24494;&#35843;&#30340;&#19978;&#19979;&#25991;&#21644;&#24773;&#24863;&#30693;&#35782;&#35843;&#26657;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21450;&#20854;&#21464;&#31181;&#22312;&#20247;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#38750;&#20961;&#30340;&#25928;&#26524;&#65292;&#36825;&#20026;NLP&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#26032;&#30340;&#35270;&#37326;&#12290;&#23613;&#31649;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#26041;&#38754;&#30340;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#32570;&#20047;&#23545;&#24773;&#24863;&#29702;&#35299;&#39046;&#22495;&#30340;&#26126;&#30830;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;LLMs&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#21487;&#33021;&#23548;&#33268;&#31934;&#24230;&#19981;&#20339;&#21644;&#19981;&#20805;&#20998;&#12290;&#21478;&#19968;&#20010;LLMs&#30340;&#38480;&#21046;&#26159;&#23427;&#20204;&#36890;&#24120;&#22312;&#27809;&#26377;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DialogueLLM&#65292;&#19968;&#31181;&#32463;&#36807;&#32454;&#35843;&#30340;&#19978;&#19979;&#25991;&#21644;&#24773;&#24863;&#30693;&#35782;&#35843;&#26657;&#30340;LLM&#65292;&#36890;&#36807;&#20351;&#29992;13,638&#20010;&#22810;&#27169;&#24577;&#65288;&#25991;&#26412;&#21644;&#35270;&#39057;&#65289;&#24773;&#24863;&#23545;&#35805;&#30340;LLaMA&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#12290;&#35270;&#35273;&#20449;&#24687;&#34987;&#35270;&#20026;&#26500;&#24314;&#39640;&#36136;&#37327;&#25351;&#20196;&#30340;&#34917;&#20805;&#30693;&#35782;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#19977;&#20010;&#24773;&#24863;&#35782;&#21035;&#23545;&#35805;&#65288;ERC&#65289;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) and their variants have shown extraordinary efficacy across numerous downstream natural language processing (NLP) tasks, which has presented a new vision for the development of NLP. Despite their remarkable performance in natural language generating (NLG), LLMs lack a distinct focus on the emotion understanding domain. As a result, using LLMs for emotion recognition may lead to suboptimal and inadequate precision. Another limitation of LLMs is that they are typical trained without leveraging multi-modal information. To overcome these limitations, we propose DialogueLLM, a context and emotion knowledge tuned LLM that is obtained by fine-tuning LLaMA models with 13,638 multi-modal (i.e., texts and videos) emotional dialogues. The visual information is considered as the supplementary knowledge to construct high-quality instructions. We offer a comprehensive evaluation of our proposed model on three benchmarking emotion recognition in conversations (ERC) datase
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#32771;&#23519;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#24615;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.16042</link><description>&lt;p&gt;
&#12298;&#35821;&#35328;&#27169;&#22411;&#20013;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#30340;&#26368;&#20339;&#23454;&#36341;&#65306;&#24230;&#37327;&#21644;&#26041;&#27861;&#12299;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Towards Best Practices of Activation Patching in Language Models: Metrics and Methods. (arXiv:2309.16042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#32771;&#23519;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#24615;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#26800;&#35299;&#37322;&#24615;&#26088;&#22312;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#20854;&#20013;&#23450;&#20301;-&#35782;&#21035;&#37325;&#35201;&#30340;&#27169;&#22411;&#32452;&#20214;&#26159;&#20851;&#38190;&#27493;&#39588;&#12290;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#65292;&#20063;&#31216;&#20026;&#22240;&#26524;&#36861;&#36394;&#25110;&#20132;&#25442;&#24178;&#39044;&#65292;&#26159;&#23436;&#25104;&#36825;&#19968;&#20219;&#21153;&#30340;&#26631;&#20934;&#25216;&#26415;&#65292;&#20294;&#25991;&#29486;&#20013;&#23384;&#22312;&#35768;&#22810;&#21464;&#20307;&#65292;&#23545;&#36229;&#21442;&#25968;&#25110;&#26041;&#27861;&#36873;&#25321;&#32570;&#20047;&#19968;&#33268;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#32771;&#23519;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#20013;&#30340;&#26041;&#27861;&#32454;&#33410;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#35780;&#20272;&#25351;&#26631;&#21644;&#25439;&#22351;&#26041;&#27861;&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#20301;&#21644;&#30005;&#36335;&#21457;&#29616;&#30340;&#20960;&#31181;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#35299;&#37322;&#32467;&#26524;&#12290;&#36890;&#36807;&#32463;&#39564;&#35266;&#23519;&#25903;&#25345;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20026;&#20160;&#20040;&#26576;&#20123;&#25351;&#26631;&#25110;&#26041;&#27861;&#21487;&#33021;&#26356;&#21463;&#27426;&#36814;&#30340;&#27010;&#24565;&#24615;&#35770;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28608;&#27963;&#36335;&#24452;&#20462;&#22797;&#30340;&#26368;&#20339;&#23454;&#36341;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mechanistic interpretability seeks to understand the internal mechanisms of machine learning models, where localization -- identifying the important model components -- is a key step. Activation patching, also known as causal tracing or interchange intervention, is a standard technique for this task (Vig et al., 2020), but the literature contains many variants with little consensus on the choice of hyperparameters or methodology. In this work, we systematically examine the impact of methodological details in activation patching, including evaluation metrics and corruption methods. In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparate interpretability results. Backed by empirical observations, we give conceptual arguments for why certain metrics or methods may be preferred. Finally, we provide recommendations for the best practices of activation patching going forwards.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#23481;&#23457;&#26597;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#22522;&#20110;&#35268;&#21017;&#30340;&#31038;&#21306;&#23457;&#26597;&#21644;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23545;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#30340;&#25913;&#36827;&#25928;&#26524;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.14517</link><description>&lt;p&gt;
&#27880;&#24847;&#35328;&#36766;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20869;&#23481;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Watch Your Language: Large Language Models and Content Moderation. (arXiv:2309.14517v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#23481;&#23457;&#26597;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#22522;&#20110;&#35268;&#21017;&#30340;&#31038;&#21306;&#23457;&#26597;&#21644;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23545;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#30340;&#25913;&#36827;&#25928;&#26524;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21464;&#24471;&#38750;&#24120;&#21463;&#27426;&#36814;&#12290;&#22522;&#20110;&#25991;&#26412;&#30340;&#20869;&#23481;&#23457;&#26597;&#26159;&#20854;&#20013;&#19968;&#20010;&#21463;&#21040;&#36817;&#26399;&#28909;&#24773;&#20851;&#27880;&#30340;LLM&#24212;&#29992;&#26696;&#20363;&#65292;&#28982;&#32780;&#65292;&#40092;&#26377;&#30740;&#31350;&#35843;&#26597;LLMs&#22312;&#20869;&#23481;&#23457;&#26597;&#35774;&#32622;&#20013;&#30340;&#34920;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#22871;&#29616;&#20195;&#12289;&#21830;&#19994;&#21270;&#30340;LLMs&#65288;GPT-3&#12289;GPT-3.5&#12289;GPT-4&#65289;&#22312;&#20004;&#20010;&#24120;&#35265;&#30340;&#20869;&#23481;&#23457;&#26597;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65306;&#22522;&#20110;&#35268;&#21017;&#30340;&#31038;&#21306;&#23457;&#26597;&#21644;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#12290;&#23545;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#31038;&#21306;&#23457;&#26597;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;95&#20010;LLM&#23457;&#26597;&#24341;&#25806;&#65292;&#24182;&#20351;&#29992;95&#20010;Reddit&#23376;&#31038;&#21306;&#30340;&#35268;&#21017;&#36827;&#34892;&#25351;&#23548;&#65292;&#21457;&#29616;LLMs&#22312;&#35768;&#22810;&#31038;&#21306;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#23457;&#26597;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#20013;&#20301;&#25968;&#20934;&#30830;&#29575;&#20026;64%&#21644;&#20013;&#20301;&#25968;&#31934;&#30830;&#24230;&#20026;83%&#12290;&#22312;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#21830;&#19994;&#21487;&#29992;&#30340;&#26377;&#23475;&#24615;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#26368;&#36817;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#23545;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#20960;&#20046;&#27809;&#26377;&#24102;&#26469;&#26126;&#26174;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exploded in popularity due to their ability to perform a wide array of natural language tasks. Text-based content moderation is one LLM use case that has received recent enthusiasm, however, there is little research investigating how LLMs perform in content moderation settings. In this work, we evaluate a suite of modern, commercial LLMs (GPT-3, GPT-3.5, GPT-4) on two common content moderation tasks: rule-based community moderation and toxic content detection. For rule-based community moderation, we construct 95 LLM moderation-engines prompted with rules from 95 Reddit subcommunities and find that LLMs can be effective at rule-based moderation for many communities, achieving a median accuracy of 64% and a median precision of 83%. For toxicity detection, we find that LLMs significantly outperform existing commercially available toxicity classifiers. However, we also find that recent increases in model size add only marginal benefit to toxicity detection
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;GPT&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#20219;&#21153;&#12290;&#36890;&#36807;&#32467;&#21512;&#33258;&#27965;&#25512;&#29702;&#21644;&#22522;&#20110;&#35821;&#35328;&#30693;&#35782;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#30340;&#38169;&#35823;&#29575;&#27604;&#39030;&#32423;&#35268;&#33539;&#21270;&#31995;&#32479;&#20302;&#32422;40&#65285;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#38169;&#35823;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#20219;&#21153;&#35774;&#35745;&#23384;&#22312;&#20851;&#38190;&#38480;&#21046;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#20197;&#35782;&#21035;&#22522;&#20110;GPT&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#36825;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2309.13426</link><description>&lt;p&gt;
&#38386;&#35848;&#20196;&#20154;&#26080;&#32842;&#30340;&#38382;&#39064;&#65306;&#30740;&#31350;&#22522;&#20110;GPT&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;
&lt;/p&gt;
&lt;p&gt;
A Chat About Boring Problems: Studying GPT-based text normalization. (arXiv:2309.13426v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;GPT&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#20219;&#21153;&#12290;&#36890;&#36807;&#32467;&#21512;&#33258;&#27965;&#25512;&#29702;&#21644;&#22522;&#20110;&#35821;&#35328;&#30693;&#35782;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#30340;&#38169;&#35823;&#29575;&#27604;&#39030;&#32423;&#35268;&#33539;&#21270;&#31995;&#32479;&#20302;&#32422;40&#65285;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#23545;&#38169;&#35823;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#20219;&#21153;&#35774;&#35745;&#23384;&#22312;&#20851;&#38190;&#38480;&#21046;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#20197;&#35782;&#21035;&#22522;&#20110;GPT&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#36825;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#35268;&#33539;&#21270;-&#23558;&#25991;&#26412;&#20174;&#20070;&#38754;&#24418;&#24335;&#36716;&#21270;&#20026;&#21475;&#35821;&#24418;&#24335;-&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#19981;&#23436;&#21892;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19982;&#20043;&#30456;&#21453;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#23569;&#26679;&#26412;&#24773;&#22659;&#19979;&#36827;&#34892;&#25991;&#26412;&#35268;&#33539;&#21270;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#32467;&#21512;&#33258;&#27965;&#25512;&#29702;&#21644;&#22522;&#20110;&#35821;&#35328;&#30693;&#35782;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;LLM&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#30340;&#38169;&#35823;&#29575;&#27604;&#39030;&#32423;&#35268;&#33539;&#21270;&#31995;&#32479;&#20302;&#32422;40&#65285;&#12290;&#36827;&#19968;&#27493;&#30340;&#38169;&#35823;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#20256;&#32479;&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#20219;&#21153;&#35774;&#35745;&#30340;&#20851;&#38190;&#38480;&#21046;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;GPT-3.5-Turbo&#21644;GPT-4.0&#30340;&#32467;&#26524;&#20013;&#12290;&#36890;&#36807;&#36825;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#35782;&#21035;&#20986;&#22522;&#20110;GPT&#30340;&#25991;&#26412;&#35268;&#33539;&#21270;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text normalization - the conversion of text from written to spoken form - is traditionally assumed to be an ill-formed task for language models. In this work, we argue otherwise. We empirically show the capacity of Large-Language Models (LLM) for text normalization in few-shot scenarios. Combining self-consistency reasoning with linguistic-informed prompt engineering, we find LLM based text normalization to achieve error rates around 40\% lower than top normalization systems. Further, upon error analysis, we note key limitations in the conventional design of text normalization tasks. We create a new taxonomy of text normalization errors and apply it to results from GPT-3.5-Turbo and GPT-4.0. Through this new framework, we can identify strengths and weaknesses of GPT-based TN, opening opportunities for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#30452;&#25509;&#39044;&#27979;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#24471;&#21040;&#30340;&#30456;&#20284;&#24615;&#26356;&#21152;&#31526;&#21512;&#25105;&#20204;&#23545;&#40065;&#26834;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#39044;&#26399;&#12290;</title><link>http://arxiv.org/abs/2309.12697</link><description>&lt;p&gt;
&#35821;&#20041;&#30456;&#20284;&#24615;&#39044;&#27979;&#20248;&#20110;&#20854;&#20182;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity prediction is better than other semantic similarity measures. (arXiv:2309.12697v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#30452;&#25509;&#39044;&#27979;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#19982;&#20854;&#20182;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#24471;&#21040;&#30340;&#30456;&#20284;&#24615;&#26356;&#21152;&#31526;&#21512;&#25105;&#20204;&#23545;&#40065;&#26834;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#36890;&#24120;&#36890;&#36807;&#26816;&#26597;&#23376;&#24207;&#21015;&#30340;&#37325;&#21472;&#65288;&#20363;&#22914;BLEU&#65289;&#25110;&#20351;&#29992;&#23884;&#20837;&#65288;&#20363;&#22914;BERTScore&#65292;S-BERT&#65289;&#26469;&#34913;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#25105;&#20204;&#20165;&#23545;&#34913;&#37327;&#35821;&#20041;&#30456;&#20284;&#24615;&#24863;&#20852;&#36259;&#26102;&#65292;&#30452;&#25509;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#26469;&#39044;&#27979;&#30456;&#20284;&#24615;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#24494;&#35843;&#30340;STS-B&#27169;&#22411;&#65292;&#23450;&#20041;&#20102;STSScore&#26041;&#27861;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#25152;&#24471;&#21040;&#30340;&#30456;&#20284;&#24615;&#19982;&#25105;&#20204;&#23545;&#40065;&#26834;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#30340;&#39044;&#26399;&#26356;&#21152;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity between natural language texts is typically measured either by looking at the overlap between subsequences (e.g., BLEU) or by using embeddings (e.g., BERTScore, S-BERT). Within this paper, we argue that when we are only interested in measuring the semantic similarity, it is better to directly predict the similarity using a fine-tuned model for such a task. Using a fine-tuned model for the STS-B from the GLUE benchmark, we define the STSScore approach and show that the resulting similarity is better aligned with our expectations on a robust semantic similarity measure than other approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;JuDec&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#20102;&#33258;&#25105;&#21028;&#26029;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#20316;&#20026;&#33258;&#20027;&#20915;&#31574;&#32773;&#23454;&#29616;&#33258;&#20027;&#21028;&#26029;&#21644;&#20915;&#31574;&#25506;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;JuDec&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#25552;&#39640;&#20102;&#36890;&#36807;&#29575;&#24182;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.12519</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#20027;&#20915;&#31574;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as Autonomous Decision Maker. (arXiv:2308.12519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;JuDec&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#20102;&#33258;&#25105;&#21028;&#26029;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#20316;&#20026;&#33258;&#20027;&#20915;&#31574;&#32773;&#23454;&#29616;&#33258;&#20027;&#21028;&#26029;&#21644;&#20915;&#31574;&#25506;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;JuDec&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#25552;&#39640;&#20102;&#36890;&#36807;&#29575;&#24182;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#26102;&#20173;&#20005;&#37325;&#20381;&#36182;&#20110;&#19987;&#23478;&#30693;&#35782;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#21457;&#25381;LLMs&#20316;&#20026;&#33258;&#20027;&#20915;&#31574;&#32773;&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;JuDec&#30340;&#26041;&#27861;&#65292;&#36171;&#20104;LLMs&#33258;&#25105;&#21028;&#26029;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#23454;&#29616;&#33258;&#20027;&#21028;&#26029;&#21644;&#20915;&#31574;&#25506;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;JuDec&#20013;&#65292;&#35774;&#35745;&#20102;&#22522;&#20110;Elo&#30340;&#33258;&#25105;&#21028;&#26029;&#26426;&#21046;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#37197;&#23545;&#27604;&#36739;&#65292;&#20026;&#20915;&#31574;&#27493;&#39588;&#20998;&#37197;Elo&#20998;&#25968;&#65292;&#20197;&#21028;&#26029;&#23427;&#20204;&#30340;&#20215;&#20540;&#21644;&#25928;&#29992;&#65292;&#24182;&#30456;&#24212;&#22320;&#24341;&#23548;&#20915;&#31574;&#25628;&#32034;&#36807;&#31243;&#26397;&#21521;&#26368;&#20248;&#35299;&#12290;&#22312;ToolBench&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;JuDec&#30456;&#23545;&#20110;&#22522;&#20934;&#27169;&#22411;&#20855;&#26377;&#20248;&#21183;&#65292;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#36890;&#36807;&#29575;&#25552;&#39640;&#20102;10%&#20197;&#19978;&#12290;&#23427;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#38477;&#20302;&#20102;&#25104;&#26412;(ChatGPT API&#35843;&#29992;)&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) exhibit impressive language understanding and in-context learning abilities, their decision-making ability still heavily relies on the guidance of task-specific expert knowledge when solving real-world tasks. To unleash the potential of LLMs as autonomous decision makers, this paper presents an approach JuDec to endow LLMs with the self-judgment ability, enabling LLMs to achieve autonomous judgment and exploration for decision making. Specifically, in JuDec, Elo-based Self-Judgment Mechanism is designed to assign Elo scores to decision steps to judge their values and utilities via pairwise comparisons between two solutions and then guide the decision-searching process toward the optimal solution accordingly. Experimental results on the ToolBench dataset demonstrate JuDec's superiority over baselines, achieving over 10% improvement in Pass Rate on diverse tasks. It offers higher-quality solutions and reduces costs (ChatGPT API calls), highlighting its 
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#65292;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#26469;&#33258;&#22622;&#23572;&#32500;&#20122;&#35799;&#27468;&#30340;&#26032;&#39062;&#25991;&#23398;&#38544;&#21947;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01497</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#23545;&#26032;&#39062;&#25991;&#23398;&#38544;&#21947;&#30340;&#35299;&#37322;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Displays Emergent Ability to Interpret Novel Literary Metaphors. (arXiv:2308.01497v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01497
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;GPT-4&#65292;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#26469;&#33258;&#22622;&#23572;&#32500;&#20122;&#35799;&#27468;&#30340;&#26032;&#39062;&#25991;&#23398;&#38544;&#21947;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#26041;&#38754;&#30340;&#36827;&#23637;&#24341;&#21457;&#20102;&#20851;&#20110;&#36825;&#31181;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26159;&#21542;&#33021;&#22815;&#22312;&#36275;&#22815;&#30340;&#35757;&#32451;&#19979;&#23637;&#29616;&#20986;&#39640;&#27700;&#24179;&#20154;&#31867;&#33021;&#21147;&#30340;&#20105;&#35770;&#12290;&#23613;&#31649;LLMs&#22312;&#28041;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#29702;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#23427;&#20204;&#30340;&#33021;&#21147;&#26159;&#21542;&#24310;&#20280;&#21040;&#26356;&#20855;&#21019;&#36896;&#21147;&#30340;&#20154;&#31867;&#33021;&#21147;&#23384;&#22312;&#20005;&#37325;&#20998;&#27495;&#12290;&#20854;&#20013;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#35299;&#37322;&#26032;&#39062;&#38544;&#21947;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#29992;&#20110;&#35757;&#32451;LLMs&#30340;&#24222;&#22823;&#19988;&#38750;&#31574;&#21010;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#65292;&#35774;&#35745;&#27979;&#35797;&#30340;&#19968;&#20010;&#20005;&#37325;&#38556;&#30861;&#23601;&#26159;&#38656;&#35201;&#25214;&#21040;&#26032;&#39062;&#20294;&#39640;&#36136;&#37327;&#30340;&#38544;&#21947;&#65292;&#36825;&#20123;&#38544;&#21947;&#19981;&#22826;&#21487;&#33021;&#20986;&#29616;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-4&#65292;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#26469;&#33258;&#22622;&#23572;&#32500;&#20122;&#35799;&#27468;&#24182;&#32763;&#35793;&#20026;&#33521;&#35821;&#30340;&#26032;&#39062;&#25991;&#23398;&#38544;&#21947;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in the performance of large language models (LLMs) have sparked debate over whether, given sufficient training, high-level human abilities emerge in such generic forms of artificial intelligence (AI). Despite the exceptional performance of LLMs on a wide range of tasks involving natural language processing and reasoning, there has been sharp disagreement as to whether their abilities extend to more creative human abilities. A core example is the ability to interpret novel metaphors. Given the enormous and non-curated text corpora used to train LLMs, a serious obstacle to designing tests is the requirement of finding novel yet high-quality metaphors that are unlikely to have been included in the training data. Here we assessed the ability of GPT-4, a state-of-the-art large language model, to provide natural-language interpretations of novel literary metaphors drawn from Serbian poetry and translated into English. Despite exhibiting no signs of having been exposed to thes
&lt;/p&gt;</description></item><item><title>Sumformer&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26102;&#38388;&#20195;&#26367;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#29992;&#24635;&#32467;&#28151;&#21512;&#26469;&#22788;&#29702;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.07421</link><description>&lt;p&gt;
Sumformer: &#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#32447;&#24615;&#22797;&#26434;&#24230;&#20195;&#26367;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sumformer: A Linear-Complexity Alternative to Self-Attention for Speech Recognition. (arXiv:2307.07421v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07421
&lt;/p&gt;
&lt;p&gt;
Sumformer&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26102;&#38388;&#20195;&#26367;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#29992;&#24635;&#32467;&#28151;&#21512;&#26469;&#22788;&#29702;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20381;&#36182;&#20110;&#33258;&#27880;&#24847;&#21147;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#36827;&#34892;&#20196;&#29260;&#28151;&#21512;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#35821;&#38899;&#35821;&#21477;&#30340;&#38271;&#24230;&#21576;&#20108;&#27425;&#20851;&#31995;&#65292;&#23548;&#33268;&#25512;&#29702;&#12289;&#35757;&#32451;&#21644;&#20869;&#23384;&#21344;&#29992;&#36895;&#24230;&#21464;&#24930;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#27604;&#33258;&#27880;&#24847;&#21147;&#26356;&#20415;&#23452;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#20294;&#24456;&#38590;&#20445;&#35777;&#36798;&#21040;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#27700;&#24179;&#12290;&#23454;&#38469;&#19978;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;&#35821;&#38899;&#35782;&#21035;&#22120;&#30340;&#33258;&#27880;&#24847;&#21147;&#26435;&#37325;&#22312;&#26102;&#38388;&#19978;&#21576;&#20840;&#23616;&#24179;&#22343;&#21270;&#30340;&#24418;&#24335;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#32447;&#24615;&#26102;&#38388;&#26367;&#20195;&#33258;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#12290;&#23427;&#29992;&#25152;&#26377;&#26102;&#38388;&#27493;&#38271;&#30340;&#21521;&#37327;&#30340;&#24179;&#22343;&#20540;&#26469;&#24635;&#32467;&#25972;&#20010;&#35821;&#21477;&#12290;&#28982;&#21518;&#23558;&#36825;&#20010;&#21333;&#19968;&#30340;&#24635;&#32467;&#19982;&#29305;&#23450;&#26102;&#38388;&#30340;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#8220;&#24635;&#32467;&#28151;&#21512;&#8221;&#12290;&#22312;&#26368;&#20808;&#36827;&#30340;ASR&#27169;&#22411;&#20013;&#24341;&#20837;&#24635;&#32467;&#28151;&#21512;&#65292;&#21487;&#20197;&#22312;&#38477;&#20302;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#22810;&#36798;27%&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#25110;&#36229;&#36807;&#20808;&#21069;&#30340;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern speech recognition systems rely on self-attention. Unfortunately, token mixing with self-attention takes quadratic time in the length of the speech utterance, slowing down inference as well as training and increasing memory consumption. Cheaper alternatives to self-attention for ASR have been developed, but fail to consistently reach the same level of accuracy. In practice, however, the self-attention weights of trained speech recognizers take the form of a global average over time. This paper, therefore, proposes a linear-time alternative to self-attention for speech recognition. It summarises a whole utterance with the mean over vectors for all time steps. This single summary is then combined with time-specific information. We call this method ``Summary Mixing''. Introducing Summary Mixing in state-of-the-art ASR models makes it feasible to preserve or exceed previous speech recognition performance while lowering the training and inference times by up to 27% and reducing the m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#65292;&#36890;&#36807;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#24182;&#22312;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26041;&#38754;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36798;&#21040;&#20102;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13649</link><description>&lt;p&gt;
GKD&#65306;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#30340;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models. (arXiv:2306.13649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#65292;&#36890;&#36807;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#24182;&#22312;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26041;&#38754;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36798;&#21040;&#20102;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#36890;&#24120;&#29992;&#20110;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#38024;&#23545;&#33258;&#22238;&#24402;&#27169;&#22411;&#65288;&#22914;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#33976;&#39311;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#35757;&#32451;&#26399;&#38388;&#36755;&#20986;&#24207;&#21015;&#21644;&#37096;&#32626;&#26102;&#30001;&#23398;&#29983;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#20043;&#38388;&#20998;&#24067;&#19981;&#21305;&#37197;&#65292;&#65288;2&#65289;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#23398;&#29983;&#27169;&#22411;&#21487;&#33021;&#19981;&#22815;&#34920;&#36798;&#32769;&#24072;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#30693;&#35782;&#33976;&#39311;&#65288;GKD&#65289;&#12290;GKD&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#20174;&#23398;&#29983;&#20013;&#37319;&#26679;&#36755;&#20986;&#24207;&#21015;&#26469;&#32531;&#35299;&#20998;&#24067;&#19981;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;GKD&#36890;&#36807;&#20248;&#21270;&#26367;&#20195;KL&#31561;&#31163;&#25955;&#24230;&#26469;&#22788;&#29702;&#27169;&#22411;&#27424;&#35268;&#33539;&#65292;&#36825;&#20123;&#31163;&#25955;&#24230;&#38598;&#20013;&#20110;&#29983;&#25104;&#21487;&#33021;&#31526;&#21512;&#32769;&#24072;&#20998;&#24067;&#30340;&#23398;&#29983;&#26679;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#65292;GKD&#20248;&#20110;&#24120;&#29992;&#30340;LLM&#33976;&#39311;&#26041;&#27861;&#65292;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is commonly used for compressing neural networks to reduce their inference cost and memory footprint. However, current distillation methods for auto-regressive models, such as generative language models (LMs), suffer from two key issues: (1) distribution mismatch between output sequences during training and the sequences generated by the student during its deployment, and (2) model under-specification, where the student model may not be expressive enough to fit the teacher's distribution. To address these issues, we propose Generalized Knowledge Distillation (GKD). GKD mitigates distribution mismatch by sampling output sequences from the student during training. Furthermore, GKD handles model under-specification by optimizing alternative divergences, such as reverse KL, that focus on generating samples from the student that are likely under the teacher's distribution. We demonstrate that GKD outperforms commonly-used approaches for distilling LLMs on summarizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#25991;&#26412;&#30456;&#20851;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#34429;&#28982;&#22312;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23545;&#21333;&#20010;&#23383;&#31526;&#24418;&#29366;&#30340;&#24863;&#30693;&#26377;&#38480;&#65292;&#23545;&#22270;&#20687;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#26816;&#27979;&#33021;&#21147;&#20063;&#19981;&#36275;&#65292;&#19981;&#33021;&#19982;&#20256;&#32479;&#39046;&#22495;&#29305;&#23450;&#26041;&#27861;&#30456;&#21305;&#37197;&#65292;&#24182;&#20173;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#23427;&#20204;&#22312;OCR&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.07895</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;OCR&#30340;&#38544;&#31192;&#20043;&#35868;
&lt;/p&gt;
&lt;p&gt;
On the Hidden Mystery of OCR in Large Multimodal Models. (arXiv:2305.07895v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#25991;&#26412;&#30456;&#20851;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#34429;&#28982;&#22312;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23545;&#21333;&#20010;&#23383;&#31526;&#24418;&#29366;&#30340;&#24863;&#30693;&#26377;&#38480;&#65292;&#23545;&#22270;&#20687;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#26816;&#27979;&#33021;&#21147;&#20063;&#19981;&#36275;&#65292;&#19981;&#33021;&#19982;&#20256;&#32479;&#39046;&#22495;&#29305;&#23450;&#26041;&#27861;&#30456;&#21305;&#37197;&#65292;&#24182;&#20173;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#23427;&#20204;&#22312;OCR&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22823;&#22411;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#20013;&#25198;&#28436;&#30528;&#25903;&#37197;&#24615;&#30340;&#35282;&#33394;&#12290;&#20851;&#20110;&#23427;&#20204;&#22312;&#25991;&#26412;&#30456;&#20851;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#26377;&#25928;&#24615;&#30340;&#25506;&#32034;&#20173;&#19981;&#22815;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#25991;&#26412;&#35782;&#21035;&#12289;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#35273;&#38382;&#31572;&#21644;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#20248;&#21155;&#21183;&#65292;&#23427;&#20204;&#20027;&#35201;&#20381;&#36182;&#20110;&#35821;&#20041;&#29702;&#35299;&#26469;&#35782;&#21035;&#21333;&#35789;&#65292;&#24182;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#23545;&#21333;&#20010;&#23383;&#31526;&#24418;&#29366;&#30340;&#24863;&#30693;&#12290;&#23427;&#20204;&#23545;&#25991;&#26412;&#38271;&#24230;&#28448;&#19981;&#20851;&#24515;&#65292;&#22312;&#26816;&#27979;&#22270;&#20687;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#26041;&#38754;&#20855;&#26377;&#26377;&#38480;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#24403;&#21069;&#26368;&#24378;&#22823;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20063;&#26080;&#27861;&#19982;&#20256;&#32479;&#25991;&#26412;&#20219;&#21153;&#30340;&#39046;&#22495;&#29305;&#23450;&#26041;&#27861;&#30456;&#21305;&#37197;&#65292;&#24182;&#22312;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#20013;&#38754;&#20020;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#30340;&#22522;&#32447;&#32467;&#26524;&#25581;&#31034;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;OCR&#30340;&#38544;&#31192;&#20043;&#35868;&#65292;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large models have recently played a dominant role in natural language processing and multimodal vision-language learning. It remains less explored about their efficacy in text-related visual tasks. We conducted a comprehensive study of existing publicly available multimodal models, evaluating their performance in text recognition, text-based visual question answering, and key information extraction. Our findings reveal strengths and weaknesses in these models, which primarily rely on semantic understanding for word recognition and exhibit inferior perception of individual character shapes. They also display indifference towards text length and have limited capabilities in detecting fine-grained features in images. Consequently, these results demonstrate that even the current most powerful large multimodal models cannot match domain-specific methods in traditional text tasks and face greater challenges in more complex tasks. Most importantly, the baseline results showcased in this study
&lt;/p&gt;</description></item><item><title>CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.00969</link><description>&lt;p&gt;
CryCeleb: &#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00969
&lt;/p&gt;
&lt;p&gt;
CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;Ubenwa CryCeleb&#25968;&#25454;&#38598;&#8212;&#8212;&#19968;&#20010;&#26631;&#35760;&#30340;&#23156;&#20799;&#21741;&#22768;&#25910;&#38598;&#65292;&#20197;&#21450;&#38468;&#24102;&#30340;CryCeleb 2023&#20219;&#21153;&#8212;&#8212;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#20844;&#20849;&#35828;&#35805;&#20154;&#39564;&#35777;&#25361;&#25112;&#12290;&#25105;&#20204;&#37322;&#25918;&#20986;786&#21517;&#26032;&#29983;&#20799;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#20197;&#40723;&#21169;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the Ubenwa CryCeleb dataset - a labeled collection of infant cries, and the accompanying CryCeleb 2023 task - a public speaker verification challenge based on infant cry sounds. We release for academic usage more than 6 hours of manually segmented cry sounds from 786 newborns to encourage research in infant cry analysis.
&lt;/p&gt;</description></item><item><title>BiomedCLIP&#26159;&#19968;&#20010;&#20174;1500&#19975;&#31185;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;&#22823;&#35268;&#27169;&#30340;PMC-15M&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#25968;&#25454;&#38598;&#27604;&#29616;&#26377;&#30340;&#29983;&#29289;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#22823;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#30340;&#26816;&#32034;&#12289;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2303.00915</link><description>&lt;p&gt;
BiomedCLIP&#65306;&#19968;&#31181;&#20174;&#19968;&#21315;&#20116;&#30334;&#19975;&#31185;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. (arXiv:2303.00915v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00915
&lt;/p&gt;
&lt;p&gt;
BiomedCLIP&#26159;&#19968;&#20010;&#20174;1500&#19975;&#31185;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#29983;&#29289;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;&#22823;&#35268;&#27169;&#30340;PMC-15M&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#25968;&#25454;&#38598;&#27604;&#29616;&#26377;&#30340;&#29983;&#29289;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#22823;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#30340;&#26816;&#32034;&#12289;&#20998;&#31867;&#21644;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#26412;&#36136;&#19978;&#26159;&#22810;&#27169;&#24577;&#30340;&#65292;&#21253;&#25324;&#29289;&#29702;&#27979;&#37327;&#21644;&#33258;&#28982;&#35821;&#35328;&#21465;&#36848;&#12290;&#19968;&#20010;&#36890;&#29992;&#30340;&#29983;&#29289;&#21307;&#23398;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#38656;&#35201;&#21516;&#26102;&#22788;&#29702;&#19981;&#21516;&#30340;&#25968;&#25454;&#27169;&#24577;&#65292;&#21253;&#25324;&#25991;&#26412;&#21644;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#35757;&#32451;&#19968;&#20010;&#26377;&#25928;&#30340;&#36890;&#29992;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#20363;&#22914;&#24179;&#34892;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;PMC-15M&#65292;&#27604;&#29616;&#26377;&#30340;&#29983;&#29289;&#21307;&#23398;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65288;&#22914;MIMIC-CXR&#65289;&#22823;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#28085;&#30422;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#31867;&#22411;&#12290;PMC-15M&#21253;&#21547;&#20102;&#26469;&#33258;440&#19975;&#31185;&#23398;&#35770;&#25991;&#30340;1500&#19975;&#20010;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12290;&#22522;&#20110;PMC-15M&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;BiomedCLIP&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#33258;&#36866;&#24212;&#65292;&#20197;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#35270;&#35273;-&#35821;&#35328;&#22788;&#29702;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#65292;&#20174;&#26816;&#32034;&#21040;&#20998;&#31867;&#21040;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#65288;VQA&#65289;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical data is inherently multimodal, comprising physical measurements and natural language narratives. A generalist biomedical AI model needs to simultaneously process different modalities of data, including text and images. Therefore, training an effective generalist biomedical model requires high-quality multimodal data, such as parallel image-text pairs. Here, we present PMC-15M, a novel dataset that is two orders of magnitude larger than existing biomedical multimodal datasets such as MIMIC-CXR, and spans a diverse range of biomedical image types. PMC-15M contains 15 million biomedical image-text pairs collected from 4.4 million scientific articles. Based on PMC-15M, we have pretrained BiomedCLIP, a multimodal foundation model, with domain-specific adaptations tailored to biomedical vision-language processing. We conducted extensive experiments and ablation studies on standard biomedical imaging tasks from retrieval to classification to visual question-answering (VQA). BiomedC
&lt;/p&gt;</description></item></channel></rss>