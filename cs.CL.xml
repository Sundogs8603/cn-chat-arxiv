<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01586</link><description>&lt;p&gt;
TrustAgent: &#36890;&#36807;&#20195;&#29702;&#26500;&#25104;&#23454;&#29616;&#23433;&#20840;&#21487;&#20449;&#36182;&#30340;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
TrustAgent: Towards Safe and Trustworthy LLM-based Agents through Agent Constitution
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#39044;&#20808;&#35268;&#21010;&#12289;&#35268;&#21010;&#36807;&#31243;&#20013;&#21644;&#35745;&#21010;&#21518;&#26816;&#26597;&#19977;&#31181;&#31574;&#30053;&#26469;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20197;&#21450;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#19982;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#20854;&#21487;&#20449;&#24230;&#20173;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#30001;&#20110;&#20195;&#29702;&#21487;&#20197;&#30452;&#25509;&#19982;&#29289;&#29702;&#29615;&#22659;&#20132;&#20114;&#65292;&#20854;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#26500;&#25104;&#30340;&#20195;&#29702;&#26694;&#26550;TrustAgent&#65292;&#23545;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#32500;&#24230;&#36827;&#34892;&#20102;&#21021;&#27493;&#30740;&#31350;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#31181;&#31574;&#30053;&#65306;&#39044;&#20808;&#35268;&#21010;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#20043;&#21069;&#21521;&#27169;&#22411;&#27880;&#20837;&#23433;&#20840;&#30693;&#35782;&#65307;&#35268;&#21010;&#36807;&#31243;&#20013;&#31574;&#30053;&#65292;&#22312;&#29983;&#25104;&#35745;&#21010;&#26102;&#22686;&#24378;&#23433;&#20840;&#24615;&#65307;&#35745;&#21010;&#21518;&#26816;&#26597;&#31574;&#30053;&#65292;&#36890;&#36807;&#35745;&#21010;&#21518;&#26816;&#26597;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#36890;&#36807;&#35782;&#21035;&#21644;&#39044;&#38450;&#28508;&#22312;&#21361;&#38505;&#26377;&#25928;&#25552;&#39640;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#23433;&#20840;&#24615;&#19982;&#20351;&#29992;&#32773;&#28385;&#24847;&#24230;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#65292;&#20197;&#21450;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#19982;&#20854;&#25928;&#29575;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of LLM-based agents has garnered considerable attention, yet their trustworthiness remains an under-explored area. As agents can directly interact with the physical environment, their reliability and safety is critical. This paper presents an Agent-Constitution-based agent framework, TrustAgent, an initial investigation into improving the safety dimension of trustworthiness in LLM-based agents. This framework consists of threefold strategies: pre-planning strategy which injects safety knowledge to the model prior to plan generation, in-planning strategy which bolsters safety during plan generation, and post-planning strategy which ensures safety by post-planning inspection. Through experimental analysis, we demonstrate how these approaches can effectively elevate an LLM agent's safety by identifying and preventing potential dangers. Furthermore, we explore the intricate relationships between safety and helpfulness, and between the model's reasoning ability and its efficac
&lt;/p&gt;</description></item><item><title>&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01109</link><description>&lt;p&gt;
&#30123;&#33495;&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Vaccine: Perturbation-aware Alignment for Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01109
&lt;/p&gt;
&lt;p&gt;
&#30123;&#33495;&#26159;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24178;&#25200;&#24863;&#30693;&#23545;&#40784;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#28155;&#21152;&#25200;&#21160;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24494;&#35843;&#21363;&#26381;&#21153;&#33539; paradigm&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20026;&#29992;&#25143;&#19978;&#20256;&#30340;&#19968;&#23567;&#37096;&#20998;&#26377;&#23475;&#25968;&#25454;&#25552;&#20379;&#20102;&#26032;&#30340;&#25915;&#20987;&#38754;&#65292;&#36825;&#20123;&#25968;&#25454;&#24456;&#23481;&#26131;&#27450;&#39575;&#24494;&#35843;&#36807;&#31243;&#20174;&#32780;&#20135;&#29983;&#23545;&#40784;&#22833;&#25928;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#21487;&#33021;&#23548;&#33268;&#23545;&#40784;&#22833;&#25928;&#30340;&#26377;&#23475;&#23884;&#20837;&#28418;&#31227;&#29616;&#35937;&#12290;&#21463;&#21040;&#25105;&#20204;&#30340;&#21457;&#29616;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30123;&#33495; (Vaccine) &#65292;&#19968;&#31181;&#38024;&#23545;&#24178;&#25200;&#24863;&#30693;&#30340;&#23545;&#40784;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#29992;&#25143;&#24494;&#35843;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#30123;&#33495;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#22312;&#23545;&#40784;&#38454;&#27573;&#36880;&#28176;&#28155;&#21152;&#31934;&#24515;&#35774;&#35745;&#30340;&#25200;&#21160;&#65292;&#20135;&#29983;&#19981;&#21464;&#30340;&#38544;&#34255;&#23884;&#20837;&#65292;&#20174;&#32780;&#20351;&#23884;&#20837;&#33021;&#22815;&#25269;&#24481;&#26469;&#33258;&#26410;&#32463;&#28040;&#27602;&#30340;&#29992;&#25143;&#25968;&#25454;&#30340;&#26377;&#23475;&#25200;&#21160;&#12290;&#25105;&#20204;&#22312;&#24320;&#28304;&#20027;&#27969;LLM&#65288;&#22914;Llama2&#65292;Opt&#65292;Vicuna&#65289;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30123;&#33495;&#33021;&#22815;&#25552;&#39640;&#23545;&#25239;&#26377;&#23475;&#25552;&#31034;&#24341;&#36215;&#30340;&#23884;&#20837;&#28418;&#31227;&#30340;&#23545;&#40784;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#23545;&#33391;&#24615;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The new paradigm of finetuning-as-a-service introduces a new attack surface for Large Language Models (LLMs): a few harmful data uploaded by users can easily trick the finetuning to produce an alignment-broken model. We conduct an empirical analysis and uncover a \textit{harmful embedding drift} phenomenon, showing a probable cause of the alignment-broken effect. Inspired by our findings, we propose Vaccine, a perturbation-aware alignment technique to mitigate the security risk of users finetuning. The core idea of Vaccine is to produce invariant hidden embeddings by progressively adding crafted perturbation to them in the alignment phase. This enables the embeddings to withstand harmful perturbation from un-sanitized user data in the finetuning phase. Our results on open source mainstream LLMs (e.g., Llama2, Opt, Vicuna) demonstrate that Vaccine can boost the robustness of alignment against harmful prompts induced embedding drift while reserving reasoning ability towards benign prompt
&lt;/p&gt;</description></item><item><title>Sequoia&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#31283;&#20581;&#19988;&#30828;&#20214;&#24863;&#30693;&#30340;&#25512;&#27979;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#20248;&#21270;&#26631;&#35760;&#26641;&#32467;&#26500;&#12289;&#37319;&#29992;&#26032;&#39062;&#30340;&#37319;&#26679;&#21644;&#39564;&#35777;&#26041;&#27861;&#23454;&#29616;&#31283;&#20581;&#24615;&#33021;&#20197;&#21450;&#30828;&#20214;&#24863;&#30693;&#30340;&#26641;&#20248;&#21270;&#22120;&#26368;&#22823;&#21270;&#25512;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12374</link><description>&lt;p&gt;
Sequoia: &#21487;&#25193;&#23637;&#12289;&#31283;&#20581;&#19988;&#30828;&#20214;&#24863;&#30693;&#30340;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Sequoia: Scalable, Robust, and Hardware-aware Speculative Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12374
&lt;/p&gt;
&lt;p&gt;
Sequoia&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#12289;&#31283;&#20581;&#19988;&#30828;&#20214;&#24863;&#30693;&#30340;&#25512;&#27979;&#35299;&#30721;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#20248;&#21270;&#26631;&#35760;&#26641;&#32467;&#26500;&#12289;&#37319;&#29992;&#26032;&#39062;&#30340;&#37319;&#26679;&#21644;&#39564;&#35777;&#26041;&#27861;&#23454;&#29616;&#31283;&#20581;&#24615;&#33021;&#20197;&#21450;&#30828;&#20214;&#24863;&#30693;&#30340;&#26641;&#20248;&#21270;&#22120;&#26368;&#22823;&#21270;&#25512;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20351;&#29992;&#22686;&#22810;&#65292;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#25512;&#29702;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#34429;&#28982;&#26368;&#36817;&#25512;&#27979;&#35299;&#30721;&#24050;&#32463;&#25104;&#20026;&#21152;&#36895;&#25512;&#29702;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#22312;&#25193;&#23637;&#21040;&#36739;&#22823;&#30340;&#25512;&#27979;&#39044;&#31639;&#12289;&#36866;&#24212;&#19981;&#21516;&#36229;&#21442;&#25968;&#21644;&#30828;&#20214;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Sequoia&#65292;&#19968;&#20010;&#21487;&#25193;&#23637;&#12289;&#31283;&#20581;&#19988;&#30828;&#20214;&#24863;&#30693;&#30340;&#29992;&#20110;&#25512;&#27979;&#35299;&#30721;&#30340;&#31639;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;Sequoia&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#26469;&#25214;&#21040;&#29992;&#20110;&#34987;&#25512;&#27979;&#26631;&#35760;&#30340;&#26368;&#20339;&#26641;&#32467;&#26500;&#12290;&#20026;&#20102;&#23454;&#29616;&#31283;&#20581;&#30340;&#25512;&#27979;&#24615;&#33021;&#65292;Sequoia&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37319;&#26679;&#21644;&#39564;&#35777;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#35299;&#30721;&#28201;&#24230;&#19979;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;Sequoia&#24341;&#20837;&#20102;&#19968;&#31181;&#30828;&#20214;&#24863;&#30693;&#30340;&#26641;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#32473;&#23450;&#24773;&#20917;&#19979;&#30340;&#26631;&#35760;&#26641;&#22823;&#23567;&#21644;&#28145;&#24230;&#26469;&#26368;&#22823;&#21270;&#25512;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12374v1 Announce Type: new  Abstract: As the usage of large language models (LLMs) grows, performing efficient inference with these models becomes increasingly important. While speculative decoding has recently emerged as a promising direction for speeding up inference, existing methods are limited in their ability to scale to larger speculation budgets, and adapt to different hyperparameters and hardware. This paper introduces Sequoia, a scalable, robust, and hardware-aware algorithm for speculative decoding. To attain better scalability, Sequoia introduces a dynamic programming algorithm to find the optimal tree structure for the speculated tokens. To achieve robust speculative performance, Sequoia uses a novel sampling and verification method that outperforms prior work across different decoding temperatures. Finally, Sequoia introduces a hardware-aware tree optimizer that maximizes speculative performance by automatically selecting the token tree size and depth for a giv
&lt;/p&gt;</description></item><item><title>&#25361;&#25112;BTM&#24037;&#20855;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#24212;&#29992;&#30340;&#21487;&#38752;&#24615;&#65292;&#36890;&#36807;&#36328;&#35821;&#26009;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#24402;&#19968;&#21270;&#24037;&#20855;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12372</link><description>&lt;p&gt;
&#22312;&#36328;&#35821;&#26009;&#35780;&#20272;&#20013;&#23545;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#24402;&#19968;&#21270;&#24037;&#20855;&#30340;HunFlair2
&lt;/p&gt;
&lt;p&gt;
HunFlair2 in a cross-corpus evaluation of named entity recognition and normalization tools
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12372
&lt;/p&gt;
&lt;p&gt;
&#25361;&#25112;BTM&#24037;&#20855;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#24212;&#29992;&#30340;&#21487;&#38752;&#24615;&#65292;&#36890;&#36807;&#36328;&#35821;&#26009;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#24402;&#19968;&#21270;&#24037;&#20855;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#21629;&#31185;&#23398;&#25991;&#29486;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#65288;BTM&#65289;&#24050;&#25104;&#20026;&#21152;&#36895;&#20174;&#20986;&#29256;&#29289;&#20013;&#25552;&#21462;&#35265;&#35299;&#30340;&#22522;&#26412;&#25216;&#26415;&#12290;&#22312;BTM&#27969;&#31243;&#20013;&#65292;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#65288;&#20363;&#22914;&#30142;&#30149;&#12289;&#33647;&#29289;&#25110;&#22522;&#22240;&#65289;&#20197;&#21450;&#23558;&#20854;&#38142;&#25509;&#21040;&#21442;&#32771;&#30693;&#35782;&#24211;&#26159;&#20851;&#38190;&#27493;&#39588;&#65292;&#20197;&#20415;&#20174;&#19981;&#21516;&#25991;&#26723;&#20013;&#21551;&#29992;&#20449;&#24687;&#32858;&#21512;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#36825;&#20004;&#20010;&#27493;&#39588;&#30340;&#24037;&#20855;&#24456;&#23569;&#22312;&#24320;&#21457;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#20013;&#24212;&#29992;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#34987;&#24212;&#29992;&#22312;&#37326;&#22806;&#65292;&#21363;&#22312;&#24212;&#29992;&#30456;&#20851;&#30340;&#25991;&#26412;&#38598;&#21512;&#19978;&#65292;&#19981;&#21516;&#20110;&#29992;&#20110;&#24037;&#20855;&#35757;&#32451;&#30340;&#25991;&#26412;&#65292;&#20363;&#22914;&#22312;&#28966;&#28857;&#12289;&#20307;&#35009;&#12289;&#39118;&#26684;&#21644;&#25991;&#26412;&#31867;&#22411;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#26159;&#21542;&#21487;&#20197;&#20449;&#20219;&#25253;&#21578;&#30340;BTM&#24037;&#20855;&#24615;&#33021;&#65292;&#29992;&#20110;&#19979;&#28216;&#24212;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#38024;&#23545;&#21629;&#21517;&#23454;&#20307;&#25552;&#21462;&#30340;&#31934;&#24515;&#35774;&#35745;&#30340;&#36328;&#35821;&#26009;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#65292;&#24037;&#20855;&#34987;&#24212;&#29992;&#21040;&#31995;&#32479;&#20013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12372v1 Announce Type: new  Abstract: With the exponential growth of the life science literature, biomedical text mining (BTM) has become an essential technology for accelerating the extraction of insights from publications. Identifying named entities (e.g., diseases, drugs, or genes) in texts and their linkage to reference knowledge bases are crucial steps in BTM pipelines to enable information aggregation from different documents. However, tools for these two steps are rarely applied in the same context in which they were developed. Instead, they are applied in the wild, i.e., on application-dependent text collections different from those used for the tools' training, varying, e.g., in focus, genre, style, and text type. This raises the question of whether the reported performance of BTM tools can be trusted for downstream applications. Here, we report on the results of a carefully designed cross-corpus benchmark for named entity extraction, where tools were applied system
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;ANALOBENCH&#22522;&#20934;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#31867;&#27604;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25193;&#23637;LMs&#35268;&#27169;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#38271;&#22330;&#26223;&#25110;&#30456;&#20851;&#32463;&#39564;&#22238;&#24518;&#30340;&#31867;&#27604;&#26102;&#24102;&#26469;&#30340;&#24615;&#33021;&#25552;&#21319;&#36739;&#23567;&#12290;</title><link>https://arxiv.org/abs/2402.12370</link><description>&lt;p&gt;
AnaloBench&#65306;&#35780;&#20272;&#25277;&#35937;&#21644;&#38271;&#19978;&#19979;&#25991;&#31867;&#27604;&#35782;&#21035;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12370
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;ANALOBENCH&#22522;&#20934;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#31867;&#27604;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#25193;&#23637;LMs&#35268;&#27169;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#38271;&#22330;&#26223;&#25110;&#30456;&#20851;&#32463;&#39564;&#22238;&#24518;&#30340;&#31867;&#27604;&#26102;&#24102;&#26469;&#30340;&#24615;&#33021;&#25552;&#21319;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#32463;&#24120;&#36827;&#34892;&#31867;&#27604;&#24605;&#32500;&#65292;&#23558;&#20010;&#20154;&#32463;&#39564;&#19982;&#24403;&#21069;&#24773;&#20917;&#32852;&#31995;&#36215;&#26469;&#65288;$X$&#31867;&#20284;&#20110;$Y$&#26159;&#22240;&#20026;$Z$&#65289;&#12290;&#31867;&#27604;&#24605;&#32500;&#20351;&#20154;&#31867;&#33021;&#22815;&#29992;&#21019;&#36896;&#24615;&#26041;&#24335;&#35299;&#20915;&#38382;&#39064;&#65292;&#29702;&#35299;&#22256;&#38590;&#27010;&#24565;&#65292;&#26356;&#26377;&#25928;&#22320;&#34920;&#36798;&#24819;&#27861;&#12290;&#33021;&#21542;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20063;&#33021;&#20570;&#21040;&#36825;&#19968;&#28857;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ANALOBENCH&#65292;&#19968;&#20010;&#29992;&#20110;&#30830;&#23450;LMs&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#26041;&#27861;&#19987;&#27880;&#20110;&#20154;&#31867;&#20043;&#38388;&#20849;&#21516;&#30340;&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#65306;&#65288;i&#65289;&#20174;&#22823;&#37327;&#20449;&#24687;&#20013;&#22238;&#24518;&#30456;&#20851;&#32463;&#39564;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#23558;&#31867;&#27604;&#25512;&#29702;&#24212;&#29992;&#20110;&#22797;&#26434;&#21644;&#38271;&#24230;&#36739;&#38271;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#22823;&#37327;&#19987;&#26377;&#27169;&#22411;&#65288;&#20363;&#22914;&#65292;GPT&#31995;&#21015;&#65292;Claude V2&#65289;&#21644;&#24320;&#28304;&#27169;&#22411;&#65292;&#22914;LLaMA2&#12290;&#19982;&#20808;&#21069;&#30340;&#32467;&#26524;&#19968;&#26679;&#65292;&#25193;&#23637;LMs&#20250;&#24102;&#26469;&#19968;&#20123;&#24615;&#33021;&#25552;&#21319;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#31867;&#27604;&#28041;&#21450;&#38271;&#22330;&#26223;&#25110;&#22238;&#24518;&#30456;&#20851;&#32463;&#39564;&#26102;&#65292;&#35268;&#27169;&#30340;&#25552;&#21319;&#24102;&#26469;&#30340;&#22686;&#30410;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12370v1 Announce Type: cross  Abstract: Humans regularly engage in analogical thinking, relating personal experiences to current situations ($X$ is analogous to $Y$ because of $Z$). Analogical thinking allows humans to solve problems in creative ways, grasp difficult concepts, and articulate ideas more effectively. Can language models (LMs) do the same? To answer this question, we propose ANALOBENCH, a benchmark to determine analogical reasoning ability in LMs. Our benchmarking approach focuses on aspects of this ability that are common among humans: (i) recalling related experiences from a large amount of information, and (ii) applying analogical reasoning to complex and lengthy scenarios. We test a broad collection of proprietary models (e.g., GPT family, Claude V2) and open source models such as LLaMA2. As in prior results, scaling up LMs results in some performance boosts. Surprisingly, scale offers minimal gains when, (i) analogies involve lengthy scenarios, or (ii) rec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#39046;&#22495;&#21644;&#38271;&#24230;&#30340;&#21512;&#25104;NLI&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;NLI&#27169;&#22411;&#22312;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12368</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;NLI&#27169;&#22411;&#39046;&#22495;&#27867;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A synthetic data approach for domain generalization of NLI models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#39046;&#22495;&#21644;&#38271;&#24230;&#30340;&#21512;&#25104;NLI&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;NLI&#27169;&#22411;&#22312;&#39046;&#22495;&#27867;&#21270;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20173;&#28982;&#26159;LLMs&#30340;&#19968;&#20010;&#37325;&#35201;&#22522;&#20934;&#20219;&#21153;&#12290; NLI&#25968;&#25454;&#38598;&#26159;&#36801;&#31227;&#23398;&#20064;&#21040;&#20854;&#20182;&#35821;&#20041;&#20219;&#21153;&#30340;&#36339;&#26495;&#65292;&#32780;NLI&#27169;&#22411;&#26159;&#35782;&#21035;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#24544;&#23454;&#24615;&#30340;&#26631;&#20934;&#24037;&#20855;&#12290; &#20170;&#22825;&#26377;&#20960;&#20010;&#22823;&#35268;&#27169;&#30340;NLI&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;&#36825;&#20123;&#38598;&#21512;&#19978;&#36827;&#34892;&#29228;&#22369;&#65292;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290; &#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#20998;&#24067;/&#39046;&#22495;&#25968;&#25454;&#19978;&#30340;&#23454;&#38469;&#24615;&#33021;&#23578;&#19981;&#24456;&#28165;&#26970;&#12290; &#25105;&#20204;&#23545;NLI&#27169;&#22411;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#38271;&#24230;&#29983;&#25104;&#21512;&#25104;NLI&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#36825;&#20123;&#25968;&#25454;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#34987;&#29616;&#26377;&#35757;&#32451;&#38598;&#35206;&#30422;&#12290; &#29983;&#25104;&#30340;&#31034;&#20363;&#20855;&#26377;&#26377;&#24847;&#20041;&#30340;&#21069;&#25552;&#65292;&#20551;&#35774;&#20197;&#21019;&#36896;&#24615;&#30340;&#26041;&#24335;&#24418;&#25104;&#65292;&#32780;&#19981;&#26159;&#31616;&#21333;&#22320;&#23545;&#20960;&#20010;&#21069;&#25552;&#26631;&#35760;&#36827;&#34892;&#32534;&#36753;&#65292;&#26631;&#31614;&#30340;&#20934;&#30830;&#29575;&#24456;&#39640;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65288;685K&#20010;&#21512;&#25104;&#31034;&#20363;&#65289;&#20855;&#26377;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12368v1 Announce Type: new  Abstract: Natural Language Inference (NLI) remains an important benchmark task for LLMs. NLI datasets are a springboard for transfer learning to other semantic tasks, and NLI models are standard tools for identifying the faithfulness of model-generated text. There are several large scale NLI datasets today, and models have improved greatly by hill-climbing on these collections. Yet their realistic performance on out-of-distribution/domain data is less well-understood. We present an in-depth exploration of the problem of domain generalization of NLI models. We demonstrate a new approach for generating synthetic NLI data in diverse domains and lengths, so far not covered by existing training sets. The resulting examples have meaningful premises, the hypotheses are formed in creative ways rather than simple edits to a few premise tokens, and the labels have high accuracy. We show that models trained on this data ($685$K synthetic examples) have the b
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36136;&#30097;&#22797;&#26434;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;AI&#21453;&#39304;&#20013;&#30340;&#24517;&#35201;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;&#26356;&#24378;&#30340;&#25945;&#24072;&#27169;&#22411;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#21487;&#20197;&#36229;&#36234;&#29616;&#26377;&#30340;RLAIF&#31649;&#36947;&#12290;</title><link>https://arxiv.org/abs/2402.12366</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;AI&#21453;&#39304;&#30340;&#20851;&#38190;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Critical Evaluation of AI Feedback for Aligning Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12366
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36136;&#30097;&#22797;&#26434;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;AI&#21453;&#39304;&#20013;&#30340;&#24517;&#35201;&#24615;&#65292;&#34920;&#26126;&#20351;&#29992;&#26356;&#24378;&#30340;&#25945;&#24072;&#27169;&#22411;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#21487;&#20197;&#36229;&#36234;&#29616;&#26377;&#30340;RLAIF&#31649;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#19982;AI&#21453;&#39304;&#65288;RLAIF&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#25552;&#39640;&#24378;&#22823;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#30340;&#27969;&#34892;&#33539;&#24335;&#12290; RLAIF&#39318;&#20808;&#20351;&#29992;&#26469;&#33258;&#25945;&#24072;&#27169;&#22411;&#30340;&#31034;&#33539;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#65292;&#28982;&#21518;&#20877;&#20351;&#29992;&#26469;&#33258;&#35780;&#35770;&#27169;&#22411;&#30340;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#36827;&#19968;&#27493;&#24494;&#35843;&#27169;&#22411;&#12290;&#23613;&#31649;&#26368;&#36817;&#27969;&#34892;&#30340;&#24320;&#28304;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#20102;&#20174;RL&#27493;&#39588;&#20013;&#33719;&#24471;&#30340;&#24615;&#33021;&#26174;&#30528;&#25552;&#39640;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#26159;&#21542;&#22797;&#26434;&#30340;RL&#27493;&#39588;&#30495;&#27491;&#26377;&#24517;&#35201;&#20026;AI&#21453;&#39304;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RL&#27493;&#39588;&#30340;&#25913;&#36827;&#20960;&#20046;&#23436;&#20840;&#26159;&#22240;&#20026;&#20351;&#29992;&#36739;&#24369;&#30340;&#25945;&#24072;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-3.5&#65289;&#29992;&#20110;SFT&#25968;&#25454;&#25910;&#38598;&#32780;&#19981;&#26159;&#29992;&#20110;AI&#21453;&#39304;&#29983;&#25104;&#30340;&#35780;&#35770;&#32773;&#65288;&#20363;&#22914;GPT-4&#65289;&#30340;&#24191;&#27867;&#23454;&#36341;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#20197;GPT-4&#20316;&#20026;&#25945;&#24072;&#30340;&#30417;&#30563;&#24494;&#35843;&#20248;&#20110;&#29616;&#26377;&#30340;RLAIF&#31649;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12366v1 Announce Type: cross  Abstract: Reinforcement learning with AI feedback (RLAIF) is a popular paradigm for improving the instruction-following abilities of powerful pre-trained language models. RLAIF first performs supervised fine-tuning (SFT) using demonstrations from a teacher model and then further fine-tunes the model with reinforcement learning (RL), using feedback from a critic model. While recent popular open-source models have demonstrated substantial improvements in performance from the RL step, in this paper we question whether the complexity of this RL step is truly warranted for AI feedback. We show that the improvements of the RL step are virtually entirely due to the widespread practice of using a weaker teacher model (e.g. GPT-3.5) for SFT data collection than the critic (e.g., GPT-4) used for AI feedback generation. Specifically, we show that simple supervised fine-tuning with GPT-4 as the teacher outperforms existing RLAIF pipelines. More generally, w
&lt;/p&gt;</description></item><item><title>&#35748;&#30693;&#39537;&#21160;&#30340;&#35821;&#35328;&#27169;&#22411;&#26174;&#31034;&#20986;&#21487;&#20197;&#35299;&#37322;&#35768;&#22810;&#35789;&#24207;&#26222;&#36941;&#35268;&#24459;&#30340;&#20248;&#21183;</title><link>https://arxiv.org/abs/2402.12363</link><description>&lt;p&gt;
&#20174;&#35748;&#30693;&#39537;&#21160;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#24471;&#20986;&#30340;&#35789;&#24207;&#26222;&#36941;&#35268;&#24459;
&lt;/p&gt;
&lt;p&gt;
Emergent Word Order Universals from Cognitively-Motivated Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12363
&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#39537;&#21160;&#30340;&#35821;&#35328;&#27169;&#22411;&#26174;&#31034;&#20986;&#21487;&#20197;&#35299;&#37322;&#35768;&#22810;&#35789;&#24207;&#26222;&#36941;&#35268;&#24459;&#30340;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#19978;&#30340;&#35821;&#35328;&#34920;&#29616;&#20986;&#26576;&#20123;&#25152;&#35859;&#30340;&#31867;&#22411;&#23398;&#25110;&#34164;&#21547;&#35268;&#24459;&#65307;&#20363;&#22914;&#65292;&#20027;-&#23486;-&#35859;&#65288;SOV&#65289;&#30340;&#35789;&#24207;&#36890;&#24120;&#20351;&#29992;&#21518;&#32622;&#35789;&#12290;&#35299;&#37322;&#36825;&#20123;&#20559;&#22909;&#30340;&#26469;&#28304;&#26159;&#35821;&#35328;&#23398;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#35748;&#30693;&#20559;&#24046;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#35745;&#31639;&#27169;&#25311;&#30740;&#31350;&#35789;&#24207;&#26222;&#36941;&#35268;&#24459;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20855;&#26377;&#31867;&#22411;&#23398;&#20856;&#22411;&#35789;&#24207;&#30340;&#35821;&#35328;&#20542;&#21521;&#20110;&#20855;&#26377;&#30001;&#20855;&#26377;&#35748;&#30693;&#21512;&#29702;&#20559;&#24046;&#30340;LMs&#20272;&#35745;&#30340;&#36739;&#20302;&#22256;&#24785;&#24230;&#65306;&#21477;&#27861;&#20559;&#24046;&#12289;&#29305;&#23450;&#30340;&#35299;&#26512;&#31574;&#30053;&#21644;&#35760;&#24518;&#38480;&#21046;&#12290;&#36825;&#34920;&#26126;&#65292;&#36825;&#20123;&#35748;&#30693;&#20559;&#24046;&#21644;&#21487;&#39044;&#27979;&#24615;&#65288;&#22256;&#24785;&#24230;&#65289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21487;&#20197;&#35299;&#37322;&#35789;&#24207;&#26222;&#36941;&#35268;&#24459;&#30340;&#35768;&#22810;&#26041;&#38754;&#12290;&#36825;&#20063;&#23637;&#31034;&#20102;&#35748;&#30693;&#39537;&#21160;LMs&#30340;&#20248;&#21183;&#65292;&#22312;&#35745;&#31639;&#27169;&#25311;&#35821;&#35328;&#26222;&#36941;&#35268;&#24459;&#26102;&#36890;&#24120;&#29992;&#20110;&#35748;&#30693;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12363v1 Announce Type: new  Abstract: The world's languages exhibit certain so-called typological or implicational universals; for example, Subject-Object-Verb (SOV) word order typically employs postpositions. Explaining the source of such biases is a key goal in linguistics. We study the word-order universals through a computational simulation with language models (LMs). Our experiments show that typologically typical word orders tend to have lower perplexity estimated by LMs with cognitively plausible biases: syntactic biases, specific parsing strategies, and memory limitations. This suggests that the interplay of these cognitive biases and predictability (perplexity) can explain many aspects of word-order universals. This also showcases the advantage of cognitively-motivated LMs, which are typically employed in cognitive modeling, in the computational simulation of language universals.
&lt;/p&gt;</description></item><item><title>LoRA+&#36890;&#36807;&#35774;&#32622;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#25913;&#36827;&#21407;&#22987;LoRA&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#24494;&#35843;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.12354</link><description>&lt;p&gt;
LoRA+: &#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#39640;&#25928;&#20302;&#31209;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
LoRA+: Efficient Low Rank Adaptation of Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12354
&lt;/p&gt;
&lt;p&gt;
LoRA+&#36890;&#36807;&#35774;&#32622;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#25913;&#36827;&#21407;&#22987;LoRA&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#65292;&#22312;&#20445;&#25345;&#35745;&#31639;&#25104;&#26412;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#21644;&#24494;&#35843;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26368;&#21021;&#30001;&#32993;&#31561;&#20154;&#65288;2021&#24180;&#65289;&#24341;&#20837;&#65292;&#23548;&#33268;&#23545;&#20855;&#26377;&#22823;&#23485;&#24230;&#65288;&#23884;&#20837;&#32500;&#24230;&#65289;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26102;&#34920;&#29616;&#20122;&#20248;&#12290;&#36825;&#26159;&#22240;&#20026;LoRA&#20013;&#30340;&#36866;&#37197;&#22120;&#30697;&#38453;A&#21644;B&#20351;&#29992;&#30456;&#21516;&#30340;&#23398;&#20064;&#29575;&#36827;&#34892;&#26356;&#26032;&#12290;&#36890;&#36807;&#23545;&#22823;&#23485;&#24230;&#32593;&#32476;&#36827;&#34892;&#32553;&#25918;&#21442;&#25968;&#30340;&#35770;&#35777;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#36866;&#37197;&#22120;&#30697;&#38453;A&#21644;B&#20351;&#29992;&#30456;&#21516;&#30340;&#23398;&#20064;&#29575;&#19981;&#21033;&#20110;&#26377;&#25928;&#30340;&#29305;&#24449;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;LoRA&#30340;&#36825;&#31181;&#27425;&#20248;&#24615;&#21487;&#20197;&#31616;&#21333;&#22320;&#36890;&#36807;&#20026;LoRA&#36866;&#37197;&#22120;&#30697;&#38453;A&#21644;B&#35774;&#32622;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#20197;&#21450;&#19968;&#20010;&#31934;&#24515;&#36873;&#25321;&#30340;&#27604;&#29575;&#26469;&#36827;&#34892;&#26657;&#27491;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#25552;&#20986;&#30340;&#31639;&#27861;&#31216;&#20026;LoRA$+$&#12290;&#22312;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;LoRA$+$&#22312;&#30456;&#21516;&#35745;&#31639;&#25104;&#26412;&#19979;&#25552;&#39640;&#20102;&#24615;&#33021;&#65288;1-2&#65285;&#30340;&#25913;&#36827;&#65289;&#21644;&#24494;&#35843;&#36895;&#24230;&#65288;&#26368;&#22810;&#25552;&#36895;&#32422;2&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12354v1 Announce Type: cross  Abstract: In this paper, we show that Low Rank Adaptation (LoRA) as originally introduced in Hu et al. (2021) leads to suboptimal finetuning of models with large width (embedding dimension). This is due to the fact that adapter matrices A and B in LoRA are updated with the same learning rate. Using scaling arguments for large width networks, we demonstrate that using the same learning rate for A and B does not allow efficient feature learning. We then show that this suboptimality of LoRA can be corrected simply by setting different learning rates for the LoRA adapter matrices A and B with a well-chosen ratio. We call this proposed algorithm LoRA$+$. In our extensive experiments, LoRA$+$ improves performance (1-2 $\%$ improvements) and finetuning speed (up to $\sim$ 2X SpeedUp), at the same computational cost as LoRA.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;Retrieval Augmented Generation (RAG)&#30340;&#22270;&#26816;&#32034;&#22120;&#34987;&#25552;&#35758;&#29992;&#26469;&#20811;&#26381;LLMs&#30340;&#23616;&#38480;&#65292;&#36890;&#36807;&#20174;&#22806;&#37096;&#25968;&#25454;&#38598;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;&#25552;&#31034;&#20449;&#24687;&#65292;&#20174;&#32780;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38271;&#23614;&#30693;&#35782;&#30340;&#25429;&#33719;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12352</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26816;&#32034;&#22120;&#25429;&#25417;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#30340;&#38271;&#23614;
&lt;/p&gt;
&lt;p&gt;
Graph-Based Retriever Captures the Long Tail of Biomedical Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12352
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Retrieval Augmented Generation (RAG)&#30340;&#22270;&#26816;&#32034;&#22120;&#34987;&#25552;&#35758;&#29992;&#26469;&#20811;&#26381;LLMs&#30340;&#23616;&#38480;&#65292;&#36890;&#36807;&#20174;&#22806;&#37096;&#25968;&#25454;&#38598;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;&#25552;&#31034;&#20449;&#24687;&#65292;&#20174;&#32780;&#35299;&#20915;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38271;&#23614;&#30693;&#35782;&#30340;&#25429;&#33719;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large language models, LLMs)&#27491;&#22312;&#25913;&#21464;&#20449;&#24687;&#26816;&#32034;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#24635;&#32467;&#21644;&#23637;&#31034;&#22823;&#37327;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;LLMs&#20542;&#21521;&#20110;&#31361;&#20986;&#35757;&#32451;&#38598;&#20013;&#26368;&#24120;&#35265;&#30340;&#20449;&#24687;&#29255;&#27573;&#65292;&#24182;&#24573;&#35270;&#32597;&#35265;&#30340;&#20449;&#24687;&#12290;&#22312;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#39046;&#22495;&#65292;&#26368;&#26032;&#30340;&#21457;&#29616;&#23545;&#20110;&#23398;&#26415;&#21644;&#24037;&#19994;&#30028;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#24448;&#24448;&#34987;&#22823;&#37327;&#19981;&#26029;&#22686;&#38271;&#30340;&#25991;&#29486;&#39046;&#22495;&#25152;&#25513;&#30422;(&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;)&#12290;&#21033;&#29992;LLMs&#23637;&#29616;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#20043;&#38388;&#30340;&#26032;&#20851;&#32852;&#65292;&#22914;&#33647;&#29289;&#12289;&#22522;&#22240;&#12289;&#30142;&#30149;&#65292;&#25104;&#20026;&#25429;&#25417;&#29983;&#29289;&#21307;&#23398;&#31185;&#23398;&#29983;&#20135;&#30340;&#38271;&#23614;&#30693;&#35782;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12352v1 Announce Type: new  Abstract: Large language models (LLMs) are transforming the way information is retrieved with vast amounts of knowledge being summarized and presented via natural language conversations. Yet, LLMs are prone to highlight the most frequently seen pieces of information from the training set and to neglect the rare ones. In the field of biomedical research, latest discoveries are key to academic and industrial actors and are obscured by the abundance of an ever-increasing literature corpus (the information overload problem). Surfacing new associations between biomedical entities, e.g., drugs, genes, diseases, with LLMs becomes a challenge of capturing the long-tail knowledge of the biomedical scientific production. To overcome this challenge, Retrieval Augmented Generation (RAG) has been proposed to alleviate some of the shortcomings of LLMs by augmenting the prompts with context retrieved from external datasets. RAG methods typically select the conte
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#20219;&#21153;&#35780;&#20272;&#20102;LLMs&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35266;&#23519;&#21040;LLMs&#22312;&#19981;&#21516;&#28216;&#25103;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#19981;&#21516;&#34892;&#20026;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#25112;&#30053;&#25512;&#29702;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12348</link><description>&lt;p&gt;
&#36890;&#36807;&#21338;&#24328;&#35770;&#35780;&#20272;&#25581;&#31034;LLM&#30340;&#25112;&#30053;&#25512;&#29702;&#23616;&#38480;&#24615;&#30340;GTBench
&lt;/p&gt;
&lt;p&gt;
GTBench: Uncovering the Strategic Reasoning Limitations of LLMs via Game-Theoretic Evaluations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12348
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#20219;&#21153;&#35780;&#20272;&#20102;LLMs&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35266;&#23519;&#21040;LLMs&#22312;&#19981;&#21516;&#28216;&#25103;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#19981;&#21516;&#34892;&#20026;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#25112;&#30053;&#25512;&#29702;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#25972;&#21512;&#21040;&#20851;&#38190;&#30340;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#23427;&#20204;&#30340;&#25112;&#30053;&#21644;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;&#26412;&#25991;&#36890;&#36807;&#21338;&#24328;&#35770;&#20219;&#21153;&#35780;&#20272;LLMs&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20363;&#22914;&#65292;&#38656;&#35201;&#32431;&#36923;&#36753;&#21644;&#25112;&#30053;&#25512;&#29702;&#26469;&#19982;&#23545;&#25163;&#31454;&#20105;&#30340;&#26827;&#30424;&#28216;&#25103;&#21644;&#32440;&#29260;&#28216;&#25103;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;GTBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#35821;&#35328;&#39537;&#21160;&#30340;&#29615;&#22659;&#65292;&#21253;&#25324;10&#20010;&#24191;&#27867;&#35748;&#21487;&#30340;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#20840;&#38754;&#30340;&#28216;&#25103;&#20998;&#31867;&#27861;&#65306;&#23436;&#25972;&#20449;&#24687;&#19982;&#19981;&#23436;&#25972;&#20449;&#24687;&#65292;&#21160;&#24577;&#19982;&#38745;&#24577;&#65292;&#20197;&#21450;&#27010;&#29575;&#19982;&#30830;&#23450;&#24615;&#22330;&#26223;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#34920;&#24449;LLMs&#30340;&#21338;&#24328;&#35770;&#25512;&#29702;&#65307;&#65288;2&#65289;LLM&#23545;&#25239;LLM&#30340;&#27604;&#36187;&#20316;&#20026;&#25512;&#29702;&#35780;&#20272;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65288;1&#65289;LLMs&#22312;&#21508;&#31181;&#28216;&#25103;&#22330;&#26223;&#19979;&#26377;&#19981;&#21516;&#30340;&#34892;&#20026;&#65307;&#20363;&#22914;&#65292;LLMs&#22312;&#23436;&#25972;&#21644;&#30830;&#23450;&#24615;&#28216;&#25103;&#20013;&#22833;&#36133;&#65292;&#20294;&#23427;&#20204;&#22312;&#27010;&#29575;&#28216;&#25103;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12348v1 Announce Type: cross  Abstract: As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we investigate two key problems: (1) Characterizing game-theoretic reasoning of LLMs; (2) LLM-vs-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming
&lt;/p&gt;</description></item><item><title>&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12343</link><description>&lt;p&gt;
&#27169;&#25311;&#22833;&#35843;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#23545;&#40784;&#21487;&#33021;&#20250;&#36866;&#24471;&#20854;&#21453;&#65281;
&lt;/p&gt;
&lt;p&gt;
Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12343
&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#20250;&#36890;&#36807;&#27169;&#25311;&#22833;&#35843;&#26694;&#26550;&#65292;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#20135;&#29983;&#21361;&#38505;&#32467;&#26524;&#65292;&#23545;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21452;&#20493;&#26377;&#23475;&#24615;&#65292;&#39640;&#20110;&#24378;&#22522;&#32447;&#65292;&#24378;&#35843;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#20063;&#38656;&#35201;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#36827;&#34892;&#23433;&#20840;&#23545;&#40784;&#65292;&#20197;&#30830;&#20445;&#19982;&#20154;&#31867;&#36827;&#34892;&#23433;&#20840;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#25512;&#29702;&#26102;&#25915;&#20987;&#26694;&#26550;&#65292;&#34920;&#26126;&#23433;&#20840;&#23545;&#40784;&#20063;&#21487;&#33021;&#22312;&#23545;&#25239;&#24615;&#25805;&#32437;&#19979;&#26080;&#24847;&#20013;&#20419;&#25104;&#26377;&#23475;&#32467;&#26524;&#12290;&#36825;&#20010;&#26694;&#26550;&#34987;&#21629;&#21517;&#20026;&#27169;&#25311;&#22833;&#35843;&#65288;ED&#65289;&#65292;&#22312;&#36755;&#20986;&#31354;&#38388;&#20013;&#19981;&#33391;&#22320;&#32452;&#21512;&#20102;&#19968;&#23545;&#24320;&#28304;&#39044;&#35757;&#32451;&#21644;&#23433;&#20840;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20135;&#29983;&#20102;&#19968;&#20010;&#26377;&#23475;&#30340;&#35821;&#35328;&#27169;&#22411;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;ED&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;&#27169;&#22411;&#31995;&#21015;&#65288;Llama-1&#12289;Llama-2&#12289;Mistral&#21644;Alpaca&#65289;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ED&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#23475;&#24615;&#22686;&#21152;&#20102;&#19968;&#20493;&#65292;&#24182;&#32988;&#36807;&#24378;&#22522;&#32447;&#65292;&#20197;&#36739;&#22823;&#20248;&#21183;&#22312;48&#20010;&#35780;&#20272;&#23376;&#38598;&#20013;&#30340;43&#20010;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;&#26377;&#23475;&#29575;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;&#21363;&#20351;&#22312;&#23433;&#20840;&#23545;&#40784;&#21518;&#65292;&#37325;&#26032;&#35780;&#20272;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#23454;&#36341;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12343v1 Announce Type: new  Abstract: Large language models (LLMs) need to undergo safety alignment to ensure safe conversations with humans. However, in this work, we introduce an inference-time attack framework, demonstrating that safety alignment can also unintentionally facilitate harmful outcomes under adversarial manipulation. This framework, named Emulated Disalignment (ED), adversely combines a pair of open-source pre-trained and safety-aligned language models in the output space to produce a harmful language model without any training. Our experiments with ED across three datasets and four model families (Llama-1, Llama-2, Mistral, and Alpaca) show that ED doubles the harmfulness of pre-trained models and outperforms strong baselines, achieving the highest harmful rate in 43 out of 48 evaluation subsets by a large margin. Crucially, our findings highlight the importance of reevaluating the practice of open-sourcing language models even after safety alignment.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19977;&#37325;&#32534;&#30721;&#22120;&#35745;&#31639;&#35805;&#35821;&#28151;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#27169;&#22411;&#30340;&#26174;&#30528;&#25913;&#36827;&#21644;&#38646;-shot&#27867;&#21270;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.12332</link><description>&lt;p&gt;
&#19977;&#37325;&#32534;&#30721;&#22120;&#65306;&#20849;&#21516;&#28608;&#27963;&#30340;&#34920;&#31034;&#19968;&#36215;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
Triple-Encoders: Representations That Fire Together, Wire Together
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12332
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19977;&#37325;&#32534;&#30721;&#22120;&#35745;&#31639;&#35805;&#35821;&#28151;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;&#27169;&#22411;&#30340;&#26174;&#30528;&#25913;&#36827;&#21644;&#38646;-shot&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25628;&#32034;&#23548;&#21521;&#30340;&#23545;&#35805;&#27169;&#22411;&#36890;&#24120;&#22312;&#27599;&#20010;&#36718;&#27425;&#37325;&#26032;&#23545;&#23545;&#35805;&#21382;&#21490;&#36827;&#34892;&#32534;&#30721;&#65292;&#36896;&#25104;&#39640;&#26114;&#30340;&#25104;&#26412;&#12290;&#26354;&#29575;&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#26368;&#36817;&#23637;&#31034;&#20986;&#23545;&#35805;&#24314;&#27169;&#36828;&#36229;&#25928;&#29575;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21452;&#32534;&#30721;&#22120;&#23558;&#35805;&#35821;&#20043;&#38388;&#30340;&#30456;&#23545;&#36317;&#31163;&#32534;&#30721;&#21040;&#23884;&#20837;&#31354;&#38388;&#20013;&#12290;&#39640;&#25928;&#29575;&#26159;&#36890;&#36807;&#29420;&#31435;&#32534;&#30721;&#35805;&#35821;&#23454;&#29616;&#30340;&#65292;&#28982;&#32780;&#36825;&#24573;&#30053;&#20102;&#19978;&#19979;&#25991;&#21270;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19977;&#37325;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;Hebbian&#21551;&#21457;&#30340;&#20849;&#23384;&#23398;&#20064;&#30446;&#26631;&#65292;&#20174;&#36825;&#20123;&#29420;&#31435;&#32534;&#30721;&#30340;&#35805;&#35821;&#20013;&#39640;&#25928;&#35745;&#31639;&#20998;&#24067;&#24335;&#35805;&#35821;&#28151;&#21512;&#65292;&#32780;&#19981;&#20351;&#29992;&#20219;&#20309;&#26435;&#37325;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#19977;&#37325;&#32534;&#30721;&#22120;&#22312;&#27604;&#32534;&#30721;&#22120;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#29978;&#33267;&#27604;&#21333;&#21521;&#37327;&#34920;&#31034;&#27169;&#22411;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;-shot&#27867;&#21270;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12332v1 Announce Type: new  Abstract: Search-based dialog models typically re-encode the dialog history at every turn, incurring high cost. Curved Contrastive Learning, a representation learning method that encodes relative distances between utterances into the embedding space via a bi-encoder, has recently shown promising results for dialog modeling at far superior efficiency. While high efficiency is achieved through independently encoding utterances, this ignores the importance of contextualization. To overcome this issue, this study introduces triple-encoders, which efficiently compute distributed utterance mixtures from these independently encoded utterances through a novel hebbian inspired co-occurrence learning objective without using any weights. Empirically, we find that triple-encoders lead to a substantial improvement over bi-encoders, and even to better zero-shot generalization than single-vector representation models without requiring re-encoding. Our code/model
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36828;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340; API &#35775;&#38382;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#20197;&#26356;&#39640;&#27010;&#29575;&#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#38750;&#20165;&#20165;&#22522;&#20110;&#27169;&#22411;&#20043;&#38388;&#30340;&#36716;&#31227;&#24615;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.12329</link><description>&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Query-Based Adversarial Prompt Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36828;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340; API &#35775;&#38382;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#20197;&#26356;&#39640;&#27010;&#29575;&#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#38750;&#20165;&#20165;&#22522;&#20110;&#27169;&#22411;&#20043;&#38388;&#30340;&#36716;&#31227;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#23548;&#33268;&#19968;&#20010;&#23545;&#20854;&#36827;&#34892;&#20102;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#23383;&#31526;&#20018;&#25110;&#25191;&#34892;&#26377;&#23475;&#34892;&#20026;&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#35201;&#20040;&#22312;&#30333;&#30418;&#35774;&#32622;&#20013;&#65288;&#23436;&#20840;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#65289;&#65292;&#35201;&#20040;&#36890;&#36807;&#21487;&#36716;&#31227;&#24615;&#65306;&#19968;&#31181;&#29616;&#35937;&#65292;&#21363;&#22312;&#19968;&#20010;&#27169;&#22411;&#19978;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#36890;&#24120;&#22312;&#20854;&#20182;&#27169;&#22411;&#19978;&#20173;&#28982;&#26377;&#25928;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#25913;&#36827;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#21033;&#29992; API &#35775;&#38382;&#36828;&#31243;&#35821;&#35328;&#27169;&#22411;&#26469;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#20197;&#65288;&#26126;&#26174;&#65289;&#26356;&#39640;&#30340;&#27010;&#29575;&#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#19981;&#33021;&#20165;&#20165;&#20351;&#29992;&#36716;&#31227;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312; GPT-3.5 &#21644; OpenAI &#30340;&#23433;&#20840;&#20998;&#31867;&#22120;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#65307;&#25105;&#20204;&#33021;&#22815;&#35753; GPT-3.5 &#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#30446;&#21069;&#30340;&#36716;&#31227;&#25915;&#20987;&#22833;&#36133;&#20102;&#65292;&#24182;&#19988;&#25105;&#20204;&#20960;&#20046;&#20197; 100% &#30340;&#27010;&#29575;&#35268;&#36991;&#20102;&#23433;&#20840;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12329v1 Announce Type: cross  Abstract: Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#29978;&#33267;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#20063;&#33021;&#33258;&#21457;&#24418;&#25104;&#21512;&#20316;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#39564;&#35777;&#20102;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#30340;&#24895;&#26223;&#65292;&#34920;&#26126;LLM&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20114;&#21160;&#65292;&#21253;&#25324;&#33258;&#21457;&#21512;&#20316;&#30340;&#20114;&#21160;&#65292;&#20026;&#31038;&#20250;&#29616;&#35937;&#25552;&#20379;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2402.12327</link><description>&lt;p&gt;
&#25105;&#20204;&#24212;&#35813;&#20132;&#27969;&#21527;&#65306;&#25506;&#32034;&#31454;&#20105;LLM&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#21457;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12327
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#29978;&#33267;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#20063;&#33021;&#33258;&#21457;&#24418;&#25104;&#21512;&#20316;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#39564;&#35777;&#20102;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#30340;&#24895;&#26223;&#65292;&#34920;&#26126;LLM&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20114;&#21160;&#65292;&#21253;&#25324;&#33258;&#21457;&#21512;&#20316;&#30340;&#20114;&#21160;&#65292;&#20026;&#31038;&#20250;&#29616;&#35937;&#25552;&#20379;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#20195;&#29702;&#20855;&#26377;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#21644;&#31038;&#20250;&#21160;&#24577;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23578;&#26410;&#30740;&#31350;LLM&#20195;&#29702;&#22312;&#27809;&#26377;&#26126;&#30830;&#25351;&#20196;&#30340;&#24773;&#20917;&#19979;&#33258;&#21457;&#24314;&#31435;&#21512;&#20316;&#20851;&#31995;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19977;&#39033;&#26696;&#20363;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;LLM&#20195;&#29702;&#29978;&#33267;&#22312;&#31454;&#20105;&#29615;&#22659;&#20013;&#20063;&#33021;&#33258;&#21457;&#24418;&#25104;&#21512;&#20316;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#36825;&#19968;&#21457;&#29616;&#19981;&#20165;&#23637;&#31034;&#20102;LLM&#20195;&#29702;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20013;&#31454;&#20105;&#19982;&#21512;&#20316;&#30340;&#33021;&#21147;&#65292;&#20063;&#39564;&#35777;&#20102;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#24895;&#26223;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#34920;&#26126;LLM&#20195;&#29702;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20154;&#31867;&#31038;&#20250;&#20114;&#21160;&#65292;&#21253;&#25324;&#37027;&#20123;&#33258;&#21457;&#21512;&#20316;&#30340;&#20114;&#21160;&#65292;&#20174;&#32780;&#25552;&#20379;&#23545;&#31038;&#20250;&#29616;&#35937;&#30340;&#27934;&#23519;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/wuzengqing001225/SABM_ShallWe &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12327v1 Announce Type: new  Abstract: Recent advancements have shown that agents powered by large language models (LLMs) possess capabilities to simulate human behaviors and societal dynamics. However, the potential for LLM agents to spontaneously establish collaborative relationships in the absence of explicit instructions has not been studied. To address this gap, we conduct three case studies, revealing that LLM agents are capable of spontaneously forming collaborations even within competitive settings. This finding not only demonstrates the capacity of LLM agents to mimic competition and cooperation in human societies but also validates a promising vision of computational social science. Specifically, it suggests that LLM agents could be utilized to model human social interactions, including those with spontaneous collaborations, thus offering insights into social phenomena. The source codes for this study are available at https://github.com/wuzengqing001225/SABM_ShallWe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PsychoGAT&#65288;&#24515;&#29702;&#28216;&#25103;&#20195;&#29702;&#65289;&#20197;&#23454;&#29616;&#24515;&#29702;&#35780;&#20272;&#30340;&#36890;&#29992;&#28216;&#25103;&#21270;&#65292;&#36890;&#36807;&#23558;&#24378;&#22823;&#30340;LLM&#20195;&#29702;&#32435;&#20837;&#35282;&#33394;&#65292;&#23558;&#26631;&#20934;&#37327;&#34920;&#36716;&#21270;&#20026;&#20010;&#24615;&#21270;&#19988;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#20114;&#21160;&#23567;&#35828;&#28216;&#25103;&#12290;</title><link>https://arxiv.org/abs/2402.12326</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#24515;&#29702;&#23398;&#26234;&#33021;&#20195;&#29702;&#65306;&#19968;&#39033;&#20851;&#20110;&#28216;&#25103;&#21270;&#35780;&#20272;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LLM Agents for Psychology: A Study on Gamified Assessments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PsychoGAT&#65288;&#24515;&#29702;&#28216;&#25103;&#20195;&#29702;&#65289;&#20197;&#23454;&#29616;&#24515;&#29702;&#35780;&#20272;&#30340;&#36890;&#29992;&#28216;&#25103;&#21270;&#65292;&#36890;&#36807;&#23558;&#24378;&#22823;&#30340;LLM&#20195;&#29702;&#32435;&#20837;&#35282;&#33394;&#65292;&#23558;&#26631;&#20934;&#37327;&#34920;&#36716;&#21270;&#20026;&#20010;&#24615;&#21270;&#19988;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#20114;&#21160;&#23567;&#35828;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29702;&#27979;&#37327;&#23545;&#20110;&#31934;&#31070;&#20581;&#24247;&#12289;&#33258;&#25105;&#29702;&#35299;&#21644;&#20010;&#20154;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#33258;&#25105;&#25253;&#21578;&#37327;&#34920;&#21644;&#24515;&#29702;&#23398;&#23478;&#35775;&#35848;&#65292;&#24120;&#24120;&#38754;&#20020;&#21442;&#19982;&#24230;&#21644;&#21487;&#33719;&#24471;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#24050;&#32463;&#25506;&#35752;&#20102;&#22522;&#20110;&#28216;&#25103;&#21644;LLM&#30340;&#24037;&#20855;&#26469;&#25552;&#39640;&#29992;&#25143;&#20852;&#36259;&#24182;&#33258;&#21160;&#21270;&#35780;&#20272;&#65292;&#20294;&#23427;&#20204;&#38590;&#20197;&#24179;&#34913;&#21442;&#19982;&#24230;&#21644;&#26222;&#36866;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PsychoGAT&#65288;&#24515;&#29702;&#28216;&#25103;&#20195;&#29702;&#65289;&#65292;&#20197;&#23454;&#29616;&#24515;&#29702;&#35780;&#20272;&#30340;&#36890;&#29992;&#28216;&#25103;&#21270;&#12290;&#20027;&#35201;&#27934;&#23519;&#26159;&#24378;&#22823;&#30340;LLM&#26082;&#21487;&#20197;&#20805;&#24403;&#29087;&#32451;&#30340;&#24515;&#29702;&#23398;&#23478;&#65292;&#20063;&#21487;&#20197;&#26159;&#21019;&#26032;&#30340;&#28216;&#25103;&#35774;&#35745;&#24072;&#12290;&#36890;&#36807;&#23558;LLM&#20195;&#29702;&#32435;&#20837;&#25351;&#23450;&#35282;&#33394;&#24182;&#31934;&#24515;&#31649;&#29702;&#23427;&#20204;&#30340;&#20114;&#21160;&#65292;PsychoGAT&#21487;&#20197;&#23558;&#20219;&#20309;&#26631;&#20934;&#37327;&#34920;&#36716;&#21270;&#20026;&#20010;&#24615;&#21270;&#19988;&#20855;&#26377;&#21560;&#24341;&#21147;&#30340;&#20114;&#21160;&#23567;&#35828;&#28216;&#25103;&#12290;&#20026;&#39564;&#35777;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#24515;&#29702;&#24230;&#37327;&#35780;&#20272;&#20197;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#65292;&#24182;&#20351;&#29992;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12326v1 Announce Type: new  Abstract: Psychological measurement is essential for mental health, self-understanding, and personal development. Traditional methods, such as self-report scales and psychologist interviews, often face challenges with engagement and accessibility. While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability. In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment. The main insight is that powerful LLMs can function both as adept psychologists and innovative game designers. By incorporating LLM agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games. To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ huma
&lt;/p&gt;</description></item><item><title>ARKS&#26159;&#19968;&#31181;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#20808;&#36827;&#31574;&#30053;&#65292;&#36890;&#36807;&#27963;&#36291;&#26816;&#32034;&#21644;&#25972;&#21512;&#21508;&#31181;&#20449;&#24687;&#28304;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20026;&#35299;&#20915;&#19982;&#39057;&#32321;&#26356;&#26032;&#30340;&#24211;&#21644;&#38271;&#23614;&#32534;&#31243;&#35821;&#35328;&#30456;&#20851;&#30340;&#29616;&#23454;&#32534;&#31243;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12317</link><description>&lt;p&gt;
ARKS&#65306;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#30693;&#35782;&#27748;&#20013;&#30340;&#27963;&#36291;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
ARKS: Active Retrieval in Knowledge Soup for Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12317
&lt;/p&gt;
&lt;p&gt;
ARKS&#26159;&#19968;&#31181;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#20808;&#36827;&#31574;&#30053;&#65292;&#36890;&#36807;&#27963;&#36291;&#26816;&#32034;&#21644;&#25972;&#21512;&#21508;&#31181;&#20449;&#24687;&#28304;&#65292;&#33021;&#22815;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20026;&#35299;&#20915;&#19982;&#39057;&#32321;&#26356;&#26032;&#30340;&#24211;&#21644;&#38271;&#23614;&#32534;&#31243;&#35821;&#35328;&#30456;&#20851;&#30340;&#29616;&#23454;&#32534;&#31243;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#33539;&#24335;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#23558;&#22806;&#37096;&#30693;&#35782;&#25972;&#21512;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#65292;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#35757;&#32451;&#12290;&#23613;&#31649;&#22312;&#33258;&#28982;&#35821;&#35328;&#24212;&#29992;&#20013;&#24471;&#21040;&#24191;&#27867;&#25506;&#35752;&#65292;&#20294;&#23427;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#21033;&#29992;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#27748;&#20013;&#30340;&#27963;&#36291;&#26816;&#32034;(ARKS)&#30340;&#20808;&#36827;&#31574;&#30053;&#65292;&#29992;&#20110;&#27867;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#20195;&#30721;&#12290;&#19982;&#20381;&#38752;&#21333;&#19968;&#26469;&#28304;&#19981;&#21516;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#23558;&#32593;&#39029;&#25628;&#32034;&#12289;&#25991;&#26723;&#12289;&#25191;&#34892;&#21453;&#39304;&#21644;&#36827;&#21270;&#20195;&#30721;&#29255;&#27573;&#25972;&#21512;&#22312;&#19968;&#36215;&#30340;&#30693;&#35782;&#27748;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#31215;&#26497;&#30340;&#26816;&#32034;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36845;&#20195;&#22320;&#20248;&#21270;&#26597;&#35810;&#24182;&#26356;&#26032;&#30693;&#35782;&#27748;&#12290;&#20026;&#20102;&#35780;&#20272;ARKS&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#19982;&#39057;&#32321;&#26356;&#26032;&#30340;&#24211;&#21644;&#38271;&#23614;&#32534;&#31243;&#35821;&#35328;&#30456;&#20851;&#30340;&#29616;&#23454;&#32534;&#31243;&#38382;&#39064;&#12290;&#22312;ChatGPT&#21644;CodeLlama&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ARKS&#27604;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#20135;&#29983;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12317v1 Announce Type: cross  Abstract: Recently the retrieval-augmented generation (RAG) paradigm has raised much attention for its potential in incorporating external knowledge into large language models (LLMs) without further training. While widely explored in natural language applications, its utilization in code generation remains under-explored. In this paper, we introduce Active Retrieval in Knowledge Soup (ARKS), an advanced strategy for generalizing large language models for code. In contrast to relying on a single source, we construct a knowledge soup integrating web search, documentation, execution feedback, and evolved code snippets. We employ an active retrieval strategy that iteratively refines the query and updates the knowledge soup. To assess the performance of ARKS, we compile a new benchmark comprising realistic coding problems associated with frequently updated libraries and long-tail programming languages. Experimental results on ChatGPT and CodeLlama de
&lt;/p&gt;</description></item><item><title>TILP&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#21487;&#24494;&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#21463;&#38480;&#38543;&#26426;&#28216;&#36208;&#26426;&#21046;&#21644;&#24341;&#20837;&#26102;&#38388;&#25805;&#20316;&#31526;&#65292;&#26377;&#25928;&#24314;&#27169;&#20102;&#26102;&#38388;&#29305;&#24449;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12309</link><description>&lt;p&gt;
TILP&#65306;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#23398;&#20064;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#21487;&#24494;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TILP: Differentiable Learning of Temporal Logical Rules on Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12309
&lt;/p&gt;
&lt;p&gt;
TILP&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#21487;&#24494;&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#21463;&#38480;&#38543;&#26426;&#28216;&#36208;&#26426;&#21046;&#21644;&#24341;&#20837;&#26102;&#38388;&#25805;&#20316;&#31526;&#65292;&#26377;&#25928;&#24314;&#27169;&#20102;&#26102;&#38388;&#29305;&#24449;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#38745;&#24577;&#30693;&#35782;&#22270;&#35889;&#30456;&#27604;&#65292;&#33021;&#22815;&#25429;&#25417;&#20449;&#24687;&#38543;&#26102;&#38388;&#28436;&#21464;&#21644;&#21464;&#21270;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#65288;tKG&#65289;&#26356;&#21152;&#29616;&#23454;&#21644;&#36890;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#27010;&#24565;&#24341;&#20837;&#21040;&#35268;&#21017;&#23398;&#20064;&#20013;&#25152;&#24102;&#26469;&#30340;&#22797;&#26434;&#24615;&#65292;&#22914;&#20934;&#30830;&#30340;&#22270;&#25512;&#29702;&#65292;&#20363;&#22914;&#39044;&#27979;&#23454;&#20307;&#20043;&#38388;&#30340;&#26032;&#38142;&#25509;&#65292;&#20173;&#28982;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TILP&#65292;&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#21487;&#24494;&#26694;&#26550;&#12290;&#36890;&#36807;&#35774;&#35745;&#21463;&#38480;&#38543;&#26426;&#28216;&#36208;&#26426;&#21046;&#21644;&#24341;&#20837;&#26102;&#38388;&#25805;&#20316;&#31526;&#65292;&#25105;&#20204;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;tKG&#20013;&#24314;&#27169;&#26102;&#38388;&#29305;&#24449;&#65292;&#20363;&#22914;&#22797;&#21457;&#24615;&#12289;&#26102;&#38388;&#39034;&#24207;&#12289;&#20851;&#31995;&#23545;&#20043;&#38388;&#30340;&#38388;&#38548;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#25105;&#20204;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;TILP&#19982;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#25913;&#21892;&#22522;&#32447;&#26041;&#27861;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12309v1 Announce Type: new  Abstract: Compared with static knowledge graphs, temporal knowledge graphs (tKG), which can capture the evolution and change of information over time, are more realistic and general. However, due to the complexity that the notion of time introduces to the learning of the rules, an accurate graph reasoning, e.g., predicting new links between entities, is still a difficult problem. In this paper, we propose TILP, a differentiable framework for temporal logical rules learning. By designing a constrained random walk mechanism and the introduction of temporal operators, we ensure the efficiency of our model. We present temporal features modeling in tKG, e.g., recurrence, temporal order, interval between pair of relations, and duration, and incorporate it into our learning process. We compare TILP with state-of-the-art methods on two benchmark datasets. We show that our proposed framework can improve upon the performance of baseline methods while provid
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#21830;&#19994;&#27169;&#22411;GPT-3.5 Turbo&#21644;GPT-4&#19982;&#24320;&#28304;&#27169;&#22411;Mistral-7B&#12289;Mixtral-8x7B&#12289;Llama2-13B&#12289;Llama2-70B&#12289;QWEN1.5-72B&#20197;&#21450;CheXbert&#21644;CheXpert-labeler&#22312;&#20934;&#30830;&#26631;&#35760;X&#23556;&#32447;&#25991;&#26412;&#25253;&#21578;&#20013;&#22810;&#21457;&#29616;&#23384;&#22312;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.12298</link><description>&lt;p&gt;
&#24320;&#28304;&#21040;&#24213;&#22914;&#20309;&#20102;&#65311;&#20851;&#20110;&#21830;&#19994;&#21644;&#24320;&#28304;LLM&#22312;&#26631;&#35760;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#33021;&#21147;&#26041;&#38754;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Is Open-Source There Yet? A Comparative Study on Commercial and Open-Source LLMs in Their Ability to Label Chest X-Ray Reports
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12298
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#27604;&#36739;&#20102;&#21830;&#19994;&#27169;&#22411;GPT-3.5 Turbo&#21644;GPT-4&#19982;&#24320;&#28304;&#27169;&#22411;Mistral-7B&#12289;Mixtral-8x7B&#12289;Llama2-13B&#12289;Llama2-70B&#12289;QWEN1.5-72B&#20197;&#21450;CheXbert&#21644;CheXpert-labeler&#22312;&#20934;&#30830;&#26631;&#35760;X&#23556;&#32447;&#25991;&#26412;&#25253;&#21578;&#20013;&#22810;&#21457;&#29616;&#23384;&#22312;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#28044;&#29616;&#20102;&#35768;&#22810;&#26032;&#30340;&#24320;&#28304;&#27169;&#22411;&#21644;&#21830;&#19994;&#27169;&#22411;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#20986;&#29256;&#29289;&#25506;&#35752;&#20102;GPT-4&#22312;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#24863;&#20852;&#36259;&#20449;&#24687;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#20294;&#23578;&#26410;&#23545;GPT-4&#19982;&#19981;&#21516;&#39046;&#20808;&#30340;&#24320;&#28304;&#27169;&#22411;&#36827;&#34892;&#23454;&#38469;&#27604;&#36739;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#29420;&#31435;&#25968;&#25454;&#38598;&#12290;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#20102;2019&#24180;7&#26376;&#33267;2021&#24180;7&#26376;&#22312;&#39532;&#33832;&#35832;&#22622;&#24030;&#32508;&#21512;&#21307;&#38498;&#21019;&#24314;&#30340;540&#20221;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#12290;&#31532;&#20108;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26469;&#33258;ImaGenome&#25968;&#25454;&#38598;&#30340;500&#20221;&#33016;&#37096;X&#23556;&#32447;&#25253;&#21578;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21830;&#19994;&#27169;&#22411;GPT-3.5 Turbo&#21644;GPT-4&#19982;&#24320;&#28304;&#27169;&#22411;Mistral-7B&#12289;Mixtral-8x7B&#12289;Llama2-13B&#12289;Llama2-70B&#12289;QWEN1.5-72B&#20197;&#21450;CheXbert&#21644;CheXpert-labeler&#22312;&#20934;&#30830;&#26631;&#35760;X&#23556;&#32447;&#25991;&#26412;&#25253;&#21578;&#20013;&#22810;&#21457;&#29616;&#23384;&#22312;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12298v1 Announce Type: cross  Abstract: Introduction: With the rapid advances in large language models (LLMs), there have been numerous new open source as well as commercial models. While recent publications have explored GPT-4 in its application to extracting information of interest from radiology reports, there has not been a real-world comparison of GPT-4 to different leading open-source models.   Materials and Methods: Two different and independent datasets were used. The first dataset consists of 540 chest x-ray reports that were created at the Massachusetts General Hospital between July 2019 and July 2021. The second dataset consists of 500 chest x-ray reports from the ImaGenome dataset. We then compared the commercial models GPT-3.5 Turbo and GPT-4 from OpenAI to the open-source models Mistral-7B, Mixtral-8x7B, Llama2-13B, Llama2-70B, QWEN1.5-72B and CheXbert and CheXpert-labeler in their ability to accurately label the presence of multiple findings in x-ray text repo
&lt;/p&gt;</description></item><item><title>KARL&#26159;&#19968;&#31181;&#22522;&#20110;DKT&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#21033;&#29992;&#26816;&#32034;&#21644;BERT&#23884;&#20837;&#26469;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#23398;&#29983;&#35760;&#24518;&#39044;&#27979;&#65292;&#22312;AUC&#21644;&#26657;&#20934;&#35823;&#24046;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#25945;&#23398;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.12291</link><description>&lt;p&gt;
KARL: &#30693;&#35782;&#24863;&#30693;&#26816;&#32034;&#21644;&#34920;&#31034;&#24110;&#21161;&#23398;&#29983;&#20445;&#25345;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
KARL: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12291
&lt;/p&gt;
&lt;p&gt;
KARL&#26159;&#19968;&#31181;&#22522;&#20110;DKT&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#21033;&#29992;&#26816;&#32034;&#21644;BERT&#23884;&#20837;&#26469;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#23398;&#29983;&#35760;&#24518;&#39044;&#27979;&#65292;&#22312;AUC&#21644;&#26657;&#20934;&#35823;&#24046;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#25945;&#23398;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Flashcard&#35843;&#24230;&#22120;&#26159;&#20381;&#36182;&#20110;&#23398;&#29983;&#27169;&#22411;&#26469;&#39044;&#27979;&#23398;&#29983;&#25484;&#25569;&#30340;&#21333;&#35789;&#21345;&#65292;&#24182;&#20351;&#29992;&#25945;&#23398;&#31574;&#30053;&#26681;&#25454;&#36825;&#20123;&#39044;&#27979;&#23433;&#25490;&#35789;&#21345;&#30340;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#23398;&#29983;&#27169;&#22411;&#20165;&#20351;&#29992;&#21333;&#35789;&#21345;&#32423;&#21035;&#30340;&#29305;&#24449;&#65292;&#27604;&#22914;&#23398;&#29983;&#30340;&#36807;&#21435;&#22238;&#31572;&#65292;&#24573;&#30053;&#20102;&#21333;&#35789;&#21345;&#20043;&#38388;&#30340;&#35821;&#20041;&#32852;&#31995;&#12290;&#28145;&#24230;&#30693;&#35782;&#36319;&#36394;&#65288;DKT&#65289;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#25429;&#25417;&#35821;&#20041;&#20851;&#31995;&#65292;&#20294;&#25928;&#29575;&#20302;&#19979;&#65292;&#32570;&#20047;&#20869;&#23481;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#65292;&#24182;&#38656;&#35201;&#31283;&#20581;&#30340;&#25945;&#23398;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;KARL&#65292;&#36825;&#26159;&#21463;DKT&#21551;&#21457;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#21033;&#29992;&#26816;&#32034;&#21644;BERT&#23884;&#20837;&#20197;&#23454;&#29616;&#39640;&#25928;&#20934;&#30830;&#30340;&#23398;&#29983;&#35760;&#24518;&#39044;&#27979;&#12290;&#20026;&#20102;&#27979;&#35797;KARL&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#24191;&#27867;&#23398;&#20064;&#21382;&#21490;&#20851;&#20110;&#29712;&#20107;&#38382;&#39064;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;KARL&#22312;AUC&#21644;&#26657;&#20934;&#35823;&#24046;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25945;&#23398;&#31574;&#30053;&#65292;&#21033;&#29992;DKT&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#22312;&#32447;&#37096;&#32626;KARL&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12291v1 Announce Type: new  Abstract: Flashcard schedulers are tools that rely on 1) student models to predict the flashcards a student knows; and 2) teaching policies to schedule cards based on these predictions. Existing student models, however, only use flashcard-level features, like the student's past responses, ignoring the semantic ties of flashcards. Deep Knowledge Tracing (DKT) models can capture semantic relations with language models, but are inefficient, lack content-rich datasets for evaluation, and require robust teaching policies. To address these issues, we design KARL, a DKT-inspired student model that uses retrieval and BERT embeddings for efficient and accurate student recall predictions. To test KARL, we collect a new dataset of diverse study history on trivia questions. KARL bests existing student models in AUC and calibration error. Finally, we propose a novel teaching policy that exploits the predictive power of DKT models to deploy KARL online. Based o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#22686;&#24378;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#21477;&#23376;&#32423;&#20027;&#24352;&#26816;&#27979;&#65292;&#22312;&#23567;&#22411;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#28155;&#21152;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#21644;&#26412;&#20307;&#23884;&#20837;&#26377;&#21161;&#20110;&#25913;&#21892;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12282</link><description>&lt;p&gt;
&#22522;&#20110;&#26412;&#20307;&#22686;&#24378;&#30340;&#20027;&#24352;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ontology Enhanced Claim Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#22686;&#24378;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#21477;&#23376;&#32423;&#20027;&#24352;&#26816;&#27979;&#65292;&#22312;&#23567;&#22411;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#28155;&#21152;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#21644;&#26412;&#20307;&#23884;&#20837;&#26377;&#21161;&#20110;&#25913;&#21892;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26412;&#20307;&#22686;&#24378;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#21477;&#23376;&#30340;&#20027;&#24352;&#26816;&#27979;&#12290;&#25105;&#20204;&#23558;&#26469;&#33258;&#30693;&#35782;&#24211;&#30340;&#26412;&#20307;&#23884;&#20837;&#19982;BERT&#21477;&#23376;&#23884;&#20837;&#34701;&#21512;&#65292;&#20197;&#25191;&#34892;ClaimBuster&#21644;NewsClaims&#25968;&#25454;&#38598;&#30340;&#20027;&#24352;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#26412;&#20307;&#22686;&#24378;&#26041;&#27861;&#22312;&#36825;&#20123;&#23567;&#22411;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#32467;&#26524;&#65292;&#19982;&#20854;&#20182;&#32479;&#35745;&#21644;&#31070;&#32463;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#28155;&#21152;&#39046;&#22495;&#29305;&#23450;&#29305;&#24449;&#65288;&#20363;&#22914;&#35757;&#32451;&#30340;&#35789;&#23884;&#20837;&#25110;&#30693;&#35782;&#22270;&#20803;&#25968;&#25454;&#65289;&#21487;&#20197;&#25913;&#21892;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20197;&#26412;&#20307;&#23884;&#20837;&#30340;&#24418;&#24335;&#28155;&#21152;&#39046;&#22495;&#30693;&#35782;&#26377;&#21161;&#20110;&#36991;&#20813;&#22312;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#20013;&#36935;&#21040;&#30340;&#20559;&#35265;&#65292;&#20363;&#22914;&#25105;&#20204;&#23567;&#35821;&#26009;&#24211;&#20013;&#32431;BERT&#27169;&#22411;&#20559;&#21521;&#20110;&#26356;&#22823;&#31867;&#21035;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12282v1 Announce Type: new  Abstract: We propose an ontology enhanced model for sentence based claim detection. We fused ontology embeddings from a knowledge base with BERT sentence embeddings to perform claim detection for the ClaimBuster and the NewsClaims datasets. Our ontology enhanced approach showed the best results with these small-sized unbalanced datasets, compared to other statistical and neural machine learning models. The experiments demonstrate that adding domain specific features (either trained word embeddings or knowledge graph metadata) can improve traditional ML methods. In addition, adding domain knowledge in the form of ontology embeddings helps avoid the bias encountered in neural network based models, for example the pure BERT model bias towards larger classes in our small corpus.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#39592;&#26550;&#22270;&#35299;&#30721;&#65288;SGD&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#23376;&#38382;&#39064;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#20449;&#24687;&#36716;&#21457;&#65292;&#25913;&#21892;&#21709;&#24212;&#36136;&#37327;&#19988;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12280</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#39592;&#26550;&#22270;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Adaptive Skeleton Graph Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12280
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#39592;&#26550;&#22270;&#35299;&#30721;&#65288;SGD&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#23376;&#38382;&#39064;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#36827;&#34892;&#20449;&#24687;&#36716;&#21457;&#65292;&#25913;&#21892;&#21709;&#24212;&#36136;&#37327;&#19988;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#25104;&#21151;&#24402;&#22240;&#20110;&#22823;&#37327;&#30340;&#27169;&#22411;&#21442;&#25968;&#65288;&#20363;&#22914;&#65292;70&#20159;+&#65289;&#65307;&#28982;&#32780;&#65292;LLM&#25512;&#26029;&#20250;&#20135;&#29983;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#24182;&#34892;&#35299;&#30721;&#31574;&#30053;&#65292;&#20363;&#22914;&#8220;&#24605;&#24819;&#39592;&#26550;&#8221;&#65288;SoT&#65289;&#65292;&#36890;&#36807;&#23558;&#25552;&#31034;&#20998;&#35299;&#20026;&#21487;&#20197;&#24182;&#34892;&#35299;&#30721;&#30340;&#23376;&#38382;&#39064;&#26469;&#25913;&#21892;&#24615;&#33021;&#65307;&#20294;&#26159;&#65292;&#23427;&#20204;&#24448;&#24448;&#22312;&#21709;&#24212;&#36136;&#37327;&#19978;&#36973;&#21463;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#22312;&#29983;&#25104;&#23376;&#38382;&#39064;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#35831;&#27714;&#39069;&#22806;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#20381;&#36182;&#20851;&#31995;&#21644;&#38590;&#24230;&#65292;&#20197;&#25552;&#39640;&#21709;&#24212;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39592;&#26550;&#22270;&#35299;&#30721;&#65288;SGD&#65289;&#65292;&#21033;&#29992;&#23376;&#38382;&#39064;&#20043;&#38388;&#26292;&#38706;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25903;&#25345;&#20381;&#36182;&#23376;&#38382;&#39064;&#20043;&#38388;&#30340;&#20449;&#24687;&#36716;&#21457;&#65292;&#20197;&#25552;&#39640;&#36136;&#37327;&#65292;&#21516;&#26102;&#26292;&#38706;&#29420;&#31435;&#23376;&#38382;&#39064;&#35299;&#30721;&#30340;&#24182;&#34892;&#21270;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12280v1 Announce Type: cross  Abstract: Large language models (LLMs) have seen significant adoption for natural language tasks, owing their success to massive numbers of model parameters (e.g., 70B+); however, LLM inference incurs significant computation and memory costs. Recent approaches propose parallel decoding strategies, such as Skeleton-of-Thought (SoT), to improve performance by breaking prompts down into sub-problems that can be decoded in parallel; however, they often suffer from reduced response quality. Our key insight is that we can request additional information, specifically dependencies and difficulty, when generating the sub-problems to improve both response quality and performance. In this paper, we propose Skeleton Graph Decoding (SGD), which uses dependencies exposed between sub-problems to support information forwarding between dependent sub-problems for improved quality while exposing parallelization opportunities for decoding independent sub-problems. 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24494;&#35843;&#23398;&#20064;&#29575;&#21487;&#20197;&#32531;&#35299;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29983;&#25104;&#20013;&#20197;&#38169;&#35823;&#35821;&#35328;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#26159;&#24456;&#24378;&#22823;&#30340;&#22522;&#32447;&#65292;mBART&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#31867;&#20284;&#20110;&#21516;&#31561;&#22823;&#23567;&#30340;mT5&#12290;</title><link>https://arxiv.org/abs/2402.12279</link><description>&lt;p&gt;
&#26377;&#25928;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Key ingredients for effective zero-shot cross-lingual knowledge transfer in generative tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12279
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24494;&#35843;&#23398;&#20064;&#29575;&#21487;&#20197;&#32531;&#35299;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29983;&#25104;&#20013;&#20197;&#38169;&#35823;&#35821;&#35328;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20840;&#38754;&#24494;&#35843;&#27169;&#22411;&#26159;&#24456;&#24378;&#22823;&#30340;&#22522;&#32447;&#65292;mBART&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#31867;&#20284;&#20110;&#21516;&#31561;&#22823;&#23567;&#30340;mT5&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29983;&#25104;&#24847;&#21619;&#30528;&#22312;&#19968;&#20010;&#35821;&#35328;&#19978;&#24494;&#35843;&#22810;&#35821;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20110;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#36827;&#34892;&#27492;&#20219;&#21153;&#30340;&#39044;&#27979;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#25351;&#20986;&#19968;&#20010;&#32463;&#24120;&#20986;&#29616;&#30340;&#38382;&#39064;&#26159;&#20197;&#38169;&#35823;&#30340;&#35821;&#35328;&#29983;&#25104;&#65292;&#24182;&#25552;&#20986;&#20102;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#24120;&#20351;&#29992;mT5&#20316;&#20026;&#20027;&#24178;&#27169;&#22411;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#32479;&#19968;&#30340;&#35774;&#32622;&#20013;&#27604;&#36739;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#36824;&#21253;&#25324;&#26367;&#20195;&#24615;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#21363;mBART&#21644;NLLB-200&#12290;&#25105;&#20204;&#39318;&#20808;&#24378;&#35843;&#20102;&#24494;&#35843;&#25152;&#20351;&#29992;&#30340;&#23398;&#20064;&#29575;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#26377;&#21161;&#20110;&#22823;&#22823;&#32531;&#35299;&#20197;&#38169;&#35823;&#35821;&#35328;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#32454;&#33268;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#65292;&#23545;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#24494;&#35843;&#20316;&#20026;&#38750;&#24120;&#24378;&#22823;&#30340;&#22522;&#32447;&#65292;&#26367;&#20195;&#26041;&#27861;&#21482;&#24102;&#26469;&#24494;&#23567;&#30340;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;mBART&#19982;&#21516;&#31561;&#22823;&#23567;&#30340;mT5&#34920;&#29616;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12279v1 Announce Type: cross  Abstract: Zero-shot cross-lingual generation implies finetuning of the multilingual pretrained language model on a generation task in one language and then using it to make predictions for this task in other languages. Previous works notice a frequent problem of generation in a wrong language and propose approaches to address it, usually using mT5 as a backbone model. In this work we compare various approaches proposed from the literature in unified settings, also including alternative backbone models, namely mBART and NLLB-200. We first underline the importance of tuning learning rate used for finetuning, which helps to substantially alleviate the problem of generation in the wrong language. Then, we show that with careful learning rate tuning, the simple full finetuning of the model acts as a very strong baseline and alternative approaches bring only marginal improvements. Finally, we find that mBART performs similarly to mT5 of the same size,
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32534;&#20889;&#20195;&#30721;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;LLM&#20195;&#29702;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#20248;&#20110;&#28145;&#24230;RL&#65292;&#24182;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20248;&#20110;ReAct&#39118;&#26684;&#30340;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.12275</link><description>&lt;p&gt;
WorldCoder&#65292;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;LLM&#20195;&#29702;&#65306;&#36890;&#36807;&#32534;&#20889;&#20195;&#30721;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
WorldCoder, a Model-Based LLM Agent: Building World Models by Writing Code and Interacting with the Environment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12275
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32534;&#20889;&#20195;&#30721;&#21644;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#26500;&#24314;&#19990;&#30028;&#27169;&#22411;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;LLM&#20195;&#29702;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#20248;&#20110;&#28145;&#24230;RL&#65292;&#24182;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#20248;&#20110;ReAct&#39118;&#26684;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26500;&#24314;&#20195;&#34920;&#20854;&#23545;&#19990;&#30028;&#30693;&#35782;&#30340;Python&#31243;&#24207;&#12290;&#35813;&#19990;&#30028;&#27169;&#22411;&#35797;&#22270;&#35299;&#37322;&#20854;&#20132;&#20114;&#65292;&#21516;&#26102;&#23545;&#33258;&#24049;&#33021;&#22815;&#33719;&#24471;&#30340;&#22870;&#21169;&#25345;&#20048;&#35266;&#24577;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#25193;&#23637;LLM&#30340;&#31243;&#24207;&#21512;&#25104;&#24037;&#20316;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#22312;&#32593;&#26684;&#19990;&#30028;&#19978;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#20195;&#29702;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#26412;&#25928;&#29575;&#19978;&#27604;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26356;&#39640;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#27604;ReAct&#39118;&#26684;&#30340;&#20195;&#29702;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12275v1 Announce Type: new  Abstract: We give a model-based agent that builds a Python program representing its knowledge of the world based on its interactions with the environment. The world model tries to explain its interactions, while also being optimistic about what reward it can achieve. We do this by extending work on program synthesis via LLMs. We study our agent on gridworlds, finding our approach is more sample-efficient compared to deep RL, and more compute-efficient compared to ReAct-style agents.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20005;&#37325;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#39046;&#22495;&#30340;&#25968;&#25454;&#25991;&#26412;&#29983;&#25104;&#20013;&#36731;&#26494;&#21047;&#26032;&#26368;&#26032;&#21457;&#23637;&#27700;&#24179;&#65292;&#23637;&#29616;&#20986;&#24357;&#21512;&#24615;&#33021;&#24046;&#36317;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.12267</link><description>&lt;p&gt;
&#38024;&#23545;&#20005;&#37325;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#25991;&#26412;&#29983;&#25104;&#19982;&#21363;&#25554;&#21363;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
High-quality Data-to-Text Generation for Severely Under-Resourced Languages with Out-of-the-box Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12267
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20005;&#37325;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#39046;&#22495;&#30340;&#25968;&#25454;&#25991;&#26412;&#29983;&#25104;&#20013;&#36731;&#26494;&#21047;&#26032;&#26368;&#26032;&#21457;&#23637;&#27700;&#24179;&#65292;&#23637;&#29616;&#20986;&#24357;&#21512;&#24615;&#33021;&#24046;&#36317;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#26041;&#27861;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#27700;&#24179;&#26159;&#26080;&#27861;&#26399;&#26395;&#22312;&#20005;&#37325;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#39046;&#22495;&#24471;&#21040;&#21305;&#37197;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#29233;&#23572;&#20848;&#35821;&#12289;&#23041;&#23572;&#22763;&#35821;&#12289;&#24067;&#21015;&#22612;&#23612;&#35821;&#21644;&#39532;&#32819;&#20182;&#35821;&#30340;&#25968;&#25454;&#25991;&#26412;&#29983;&#25104;&#20026;&#20363;&#65292;&#25506;&#35752;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36825;&#26041;&#38754;&#33021;&#22815;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#21644;&#33521;&#35821;&#19978;&#27979;&#35797;&#20102;LLMs&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLMs&#20197;&#26126;&#26174;&#36739;&#22823;&#30340;&#20248;&#21183;&#36731;&#26494;&#22320;&#21047;&#26032;&#20102;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#30340;&#26368;&#26032;&#21457;&#23637;&#27700;&#24179;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#33258;&#21160;&#35780;&#20272;&#36824;&#26159;&#20154;&#24037;&#35780;&#20272;&#12290;&#23545;&#20110;&#25105;&#20204;&#25152;&#26377;&#30340;&#35821;&#35328;&#65292;&#20154;&#24037;&#35780;&#20272;&#26174;&#31034;&#25105;&#20204;&#26368;&#20339;&#31995;&#32479;&#30340;&#24615;&#33021;&#19982;&#20154;&#31867;&#25345;&#24179;&#65292;&#20294;&#19982;&#33521;&#35821;&#30456;&#27604;&#65292;BLEU&#20998;&#25968;&#19979;&#38477;&#65292;&#36825;&#23545;&#20110;&#35780;&#20272;&#38750;&#20219;&#21153;&#29305;&#23450;&#31995;&#32479;&#30340;&#25351;&#26631;&#30340;&#36866;&#29992;&#24615;&#25552;&#20986;&#20102;&#30097;&#38382;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;&#24357;&#21512;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#24615;&#33021;&#24046;&#36317;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12267v1 Announce Type: new  Abstract: The performance of NLP methods for severely under-resourced languages cannot currently hope to match the state of the art in NLP methods for well resourced languages. We explore the extent to which pretrained large language models (LLMs) can bridge this gap, via the example of data-to-text generation for Irish, Welsh, Breton and Maltese. We test LLMs on these under-resourced languages and English, in a range of scenarios. We find that LLMs easily set the state of the art for the under-resourced languages by substantial margins, as measured by both automatic and human evaluations. For all our languages, human evaluation shows on-a-par performance with humans for our best systems, but BLEU scores collapse compared to English, casting doubt on the metric's suitability for evaluating non-task-specific systems. Overall, our results demonstrate the great potential of LLMs to bridge the performance gap for under-resourced languages.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LoRA&#38598;&#25104;&#22312;&#31934;&#35843;LLMs&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#22495;&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#20998;&#26512;&#65292;&#25512;&#27979;&#20102;&#27169;&#22411;&#23545;&#29305;&#23450;&#26550;&#26500;&#38590;&#20197;&#23398;&#20064;&#30340;&#25968;&#25454;&#39046;&#22495;&#30340;&#20449;&#21495;&#12290;</title><link>https://arxiv.org/abs/2402.12264</link><description>&lt;p&gt;
&#20351;&#29992;LoRA&#38598;&#25104;&#22312;&#31934;&#35843;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification in fine-tuned LLMs using LoRA ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12264
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LoRA&#38598;&#25104;&#22312;&#31934;&#35843;LLMs&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#25968;&#25454;&#22495;&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#20998;&#26512;&#65292;&#25512;&#27979;&#20102;&#27169;&#22411;&#23545;&#29305;&#23450;&#26550;&#26500;&#38590;&#20197;&#23398;&#20064;&#30340;&#25968;&#25454;&#39046;&#22495;&#30340;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#23545;&#20110;&#31934;&#35843;&#27169;&#22411;&#23398;&#21040;&#20102;&#20160;&#20040;&#12289;&#36951;&#24536;&#20102;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#20449;&#20219;&#20854;&#39044;&#27979;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#19968;&#33324;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#23545;&#31934;&#35843;LLMs&#36827;&#34892;&#22522;&#20110;&#21518;&#39564;&#36924;&#36817;&#30340;&#21407;&#21017;&#24615;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;Mistral-7b&#30340;&#20302;&#31209;&#36866;&#24212;&#38598;&#25104;&#20998;&#26512;&#20102;&#19977;&#20010;&#24120;&#35265;&#30340;&#22810;&#39033;&#36873;&#25321;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20854;&#22312;&#31934;&#35843;&#36807;&#31243;&#20013;&#21644;&#20043;&#21518;&#23545;&#19981;&#21516;&#30446;&#26631;&#39046;&#22495;&#30340;&#24863;&#30693;&#22797;&#26434;&#24615;&#21644;&#27169;&#22411;&#25928;&#33021;&#36827;&#34892;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#30340;&#32467;&#35770;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;&#25968;&#20540;&#23454;&#39564;&#25903;&#25345;&#65292;&#25105;&#20204;&#23545;&#37027;&#20123;&#23545;&#20110;&#32473;&#23450;&#26550;&#26500;&#38590;&#20197;&#23398;&#20064;&#30340;&#25968;&#25454;&#39046;&#22495;&#30340;&#29109;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#25552;&#20986;&#20102;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12264v1 Announce Type: cross  Abstract: Fine-tuning large language models can improve task specific performance, although a general understanding of what the fine-tuned model has learned, forgotten and how to trust its predictions is still missing. We derive principled uncertainty quantification for fine-tuned LLMs with posterior approximations using computationally efficient low-rank adaptation ensembles. We analyze three common multiple-choice datasets using low-rank adaptation ensembles based on Mistral-7b, and draw quantitative and qualitative conclusions on their perceived complexity and model efficacy on the different target domains during and after fine-tuning. In particular, backed by the numerical experiments, we hypothesise about signals from entropic uncertainty measures for data domains that are inherently difficult for a given architecture to learn.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#26368;&#26032;&#33521;&#35821;&#26032;&#35789;&#36164;&#28304;&#65292;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#26032;&#35789;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#23545;&#26032;&#35789;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#26426;&#22120;&#32763;&#35793;&#20013;&#27169;&#22411;&#24615;&#33021;&#20250;&#22240;&#24341;&#20837;&#26032;&#35789;&#32780;&#20960;&#20046;&#20943;&#21322;&#12290;</title><link>https://arxiv.org/abs/2402.12261</link><description>&lt;p&gt;
NEO-BENCH&#65306;&#20351;&#29992;&#26032;&#35789;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
NEO-BENCH: Evaluating Robustness of Large Language Models with Neologisms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#26368;&#26032;&#33521;&#35821;&#26032;&#35789;&#36164;&#28304;&#65292;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#26032;&#35789;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#23545;&#26032;&#35789;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#26426;&#22120;&#32763;&#35793;&#20013;&#27169;&#22411;&#24615;&#33021;&#20250;&#22240;&#24341;&#20837;&#26032;&#35789;&#32780;&#20960;&#20046;&#20943;&#21322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs)&#30340;&#34920;&#29616;&#20250;&#22240;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#19982;&#25512;&#29702;&#36807;&#31243;&#20013;&#30475;&#21040;&#30340;&#26032;&#25991;&#26412;&#20043;&#38388;&#30340;&#26102;&#38388;&#28418;&#31227;&#32780;&#36864;&#21270;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23548;&#33268;&#25968;&#25454;&#28418;&#31227;&#30340;&#35821;&#35328;&#21464;&#21270;&#20013;&#19968;&#20010;&#19981;&#22826;&#34987;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#21363;&#38543;&#30528;&#26102;&#38388;&#25512;&#31227;&#32780;&#20986;&#29616;&#30340;&#26032;&#35789;&#24418;&#24335;&#8212;&#8212;&#26032;&#35789;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20960;&#31181;&#27969;&#34892;&#30340;&#25910;&#38598;&#26041;&#27861;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#26368;&#26032;&#33521;&#35821;&#26032;&#35789;&#36164;&#28304;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;&#21253;&#21547;&#26032;&#35789;&#30340;&#21477;&#23376;&#19982;&#23558;&#26032;&#35789;&#26367;&#25442;&#20026;&#29616;&#26377;&#26367;&#20195;&#35789;&#30340;&#20960;&#20046;&#30456;&#21516;&#30340;&#21477;&#23376;&#26469;&#20998;&#26512;&#26032;&#35789;&#23545;&#26102;&#38388;&#28418;&#31227;&#30340;&#24433;&#21709;&#12290;&#22312;&#21477;&#23376;&#20013;&#24341;&#20837;&#21333;&#20010;&#26032;&#35789;&#26102;&#65292;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#20960;&#20046;&#20943;&#21322;&#12290;&#21463;&#21040;&#36825;&#20123;&#32467;&#26524;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#35780;&#20272;LLMs&#23545;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#21644;&#27169;&#22411;&#22256;&#24785;&#24230;&#20013;&#26032;&#35789;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21518;&#26399;&#30693;&#35782;&#25130;&#27490;&#26085;&#26399;&#30340;&#27169;&#22411;&#20135;&#29983;&#36739;&#20302;&#30340;&#22256;&#24785;&#24230;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12261v1 Announce Type: new  Abstract: The performance of Large Language Models (LLMs) degrades from the temporal drift between data used for model training and newer text seen during inference. One understudied avenue of language change causing data drift is the emergence of neologisms -- new word forms -- over time. We create a diverse resource of recent English neologisms by using several popular collection methods. We analyze temporal drift using neologisms by comparing sentences containing new words with near-identical sentences that replace neologisms with existing substitute words. Model performance is nearly halved in machine translation when a single neologism is introduced in a sentence. Motivated by these results, we construct a benchmark to evaluate LLMs' ability to generalize to neologisms with various natural language understanding tasks and model perplexity. Models with later knowledge cutoff dates yield lower perplexities and perform better in downstream tasks
&lt;/p&gt;</description></item><item><title>GPT-4&#22312;&#25903;&#25345;&#20154;&#31867;&#36827;&#34892;&#22836;&#33041;&#39118;&#26292;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#27809;&#26377;&#20154;&#31867;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#20570;&#21040;&#23545;&#30456;&#20851;&#20316;&#21697;&#36827;&#34892;&#35814;&#32454;&#30340;&#21512;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.12255</link><description>&lt;p&gt;
GPT&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#30693;&#35782;&#27973;&#23618;&#21512;&#25104;&#65306;&#33258;&#21160;&#30456;&#20851;&#24037;&#20316;&#25776;&#20889;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Shallow Synthesis of Knowledge in GPT-Generated Texts: A Case Study in Automatic Related Work Composition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12255
&lt;/p&gt;
&lt;p&gt;
GPT-4&#22312;&#25903;&#25345;&#20154;&#31867;&#36827;&#34892;&#22836;&#33041;&#39118;&#26292;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#27809;&#26377;&#20154;&#31867;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#20570;&#21040;&#23545;&#30456;&#20851;&#20316;&#21697;&#36827;&#34892;&#35814;&#32454;&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#24320;&#21457;&#20986;&#35768;&#22810;AI&#36741;&#21161;&#23398;&#26415;&#24212;&#29992;&#31243;&#24207;&#26469;&#24110;&#21161;&#30740;&#31350;&#36807;&#31243;&#30340;&#19981;&#21516;&#38454;&#27573;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20351;&#29992;&#25105;&#20204;&#26500;&#24314;&#30340;&#24037;&#20855;ScholaCite&#29983;&#25104;&#30340;AI&#36741;&#21161;&#23398;&#26415;&#20889;&#20316;&#36827;&#34892;&#20998;&#26512;&#12290;&#35813;&#24037;&#20855;&#26088;&#22312;&#32452;&#32455;&#25991;&#29486;&#24182;&#25776;&#20889;&#23398;&#26415;&#35770;&#25991;&#30340;&#30456;&#20851;&#24037;&#20316;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#20391;&#37325;&#20110;&#20998;&#26512;&#24341;&#25991;&#22270;&#65292;&#20197;&#35780;&#20272;&#25991;&#26412;&#20013;&#24341;&#25991;&#30340;&#32467;&#26500;&#22797;&#26434;&#24615;&#21644;&#20114;&#32852;&#24615;&#65292;&#24182;&#28041;&#21450;&#23545;&#65288;1&#65289;&#21407;&#22987;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#65292;&#65288;2&#65289;&#32431;&#31929;&#30001;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#20197;&#21450;&#65288;3&#65289;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#19977;&#26041;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-4&#33021;&#22815;&#29983;&#25104;&#21512;&#29702;&#30340;&#31895;&#31890;&#24230;&#24341;&#25991;&#20998;&#32452;&#65292;&#20197;&#25903;&#25345;&#20154;&#31867;&#29992;&#25143;&#36827;&#34892;&#22836;&#33041;&#39118;&#26292;&#65292;&#20294;&#22312;&#27809;&#26377;&#20154;&#31867;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#25191;&#34892;&#30456;&#20851;&#20316;&#21697;&#30340;&#35814;&#32454;&#21512;&#25104;&#12290;&#25105;&#20204;&#24314;&#35758;&#26410;&#26469;&#30340;&#20889;&#20316;&#21161;&#25163;&#24037;&#20855;&#19981;&#24212;&#29420;&#31435;&#20110;&#20154;&#31867;&#20316;&#32773;&#36215;&#33609;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12255v1 Announce Type: new  Abstract: Numerous AI-assisted scholarly applications have been developed to aid different stages of the research process. We present an analysis of AI-assisted scholarly writing generated with ScholaCite, a tool we built that is designed for organizing literature and composing Related Work sections for academic papers. Our evaluation method focuses on the analysis of citation graphs to assess the structural complexity and inter-connectedness of citations in texts and involves a three-way comparison between (1) original human-written texts, (2) purely GPT-generated texts, and (3) human-AI collaborative texts. We find that GPT-4 can generate reasonable coarse-grained citation groupings to support human users in brainstorming, but fails to perform detailed synthesis of related works without human intervention. We suggest that future writing assistant tools should not be used to draft text independently of the human author.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;Levenshtein Transformer&#65288;LevT&#65289;&#30340;&#35299;&#30721;&#22120;&#65292;&#25506;&#35752;&#20102;&#35299;&#30721;&#32467;&#26524;&#38271;&#24230;&#12289;&#23376;&#35789;&#29983;&#25104;&#21644;&#21024;&#38500;&#27169;&#22359;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25214;&#20986;&#35299;&#30721;&#22120;&#30340;&#24369;&#28857;&#20197;&#20415;&#26410;&#26469;&#25913;&#36827;&#12290;&#21516;&#26102;&#27604;&#36739;&#20102;&#21407;&#22987;LevT&#12289;&#30693;&#35782;&#33976;&#39311;LevT&#12289;&#24102;&#26377;&#32763;&#35793;&#35760;&#24518;&#30340;LevT&#20197;&#21450;&#24102;&#26377;&#32763;&#35793;&#35760;&#24518;&#30340;&#30693;&#35782;&#33976;&#39311;LevT&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12249</link><description>&lt;p&gt;
Levenshtein Transformer&#35299;&#30721;&#22120;&#21450;&#20854;&#21464;&#20307;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Levenshtein Transformer's Decoder and Its Variants
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;Levenshtein Transformer&#65288;LevT&#65289;&#30340;&#35299;&#30721;&#22120;&#65292;&#25506;&#35752;&#20102;&#35299;&#30721;&#32467;&#26524;&#38271;&#24230;&#12289;&#23376;&#35789;&#29983;&#25104;&#21644;&#21024;&#38500;&#27169;&#22359;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25214;&#20986;&#35299;&#30721;&#22120;&#30340;&#24369;&#28857;&#20197;&#20415;&#26410;&#26469;&#25913;&#36827;&#12290;&#21516;&#26102;&#27604;&#36739;&#20102;&#21407;&#22987;LevT&#12289;&#30693;&#35782;&#33976;&#39311;LevT&#12289;&#24102;&#26377;&#32763;&#35793;&#35760;&#24518;&#30340;LevT&#20197;&#21450;&#24102;&#26377;&#32763;&#35793;&#35760;&#24518;&#30340;&#30693;&#35782;&#33976;&#39311;LevT&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Levenshtein Transformer (LevT)&#26159;&#19968;&#31181;&#38750;&#33258;&#22238;&#24402;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#35299;&#30721;&#21644;&#21487;&#27604;&#30340;BLEU&#24471;&#20998;&#32763;&#35793;&#36136;&#37327;&#65292;&#36825;&#24402;&#21151;&#20110;&#20854;&#24182;&#34892;&#35299;&#30721;&#21644;&#36845;&#20195;&#32454;&#21270;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#20851;&#27880;LevT&#30340;&#35299;&#30721;&#22120;&#65292;&#20998;&#26512;&#35299;&#30721;&#32467;&#26524;&#38271;&#24230;&#12289;&#23376;&#35789;&#29983;&#25104;&#21644;&#21024;&#38500;&#27169;&#22359;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#25214;&#20986;&#35299;&#30721;&#22120;&#30340;&#24369;&#28857;&#20197;&#20415;&#26410;&#26469;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#21407;&#22987;LevT&#12289;&#30693;&#35782;&#33976;&#39311;LevT&#12289;&#24102;&#26377;&#32763;&#35793;&#35760;&#24518;&#30340;LevT&#20197;&#21450;&#24102;&#26377;&#32763;&#35793;&#35760;&#24518;&#30340;&#30693;&#35782;&#33976;&#39311;LevT&#30340;&#32763;&#35793;&#32467;&#26524;&#65292;&#20197;&#25506;&#35752;&#30693;&#35782;&#33976;&#39311;&#21644;&#32763;&#35793;&#35760;&#24518;&#23545;&#32763;&#35793;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12249v1 Announce Type: new  Abstract: Levenshtein transformer (LevT) is a non-autoregressive machine translation model with high decoding efficiency and comparable translation quality in terms of bleu score, due to its parallel decoding and iterative refinement procedure. Are there any deficiencies of its translations and what improvements could be made? In this report, we focus on LevT's decoder and analyse the decoding results length, subword generation, and deletion module's capability. We hope to identify weaknesses of the decoder for future improvements.   We also compare translations of the original LevT, knowledge-distilled LevT, LevT with translation memory, and the KD-LevT with translation memory to see how KD and translation memory can help.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#25991;&#26412;&#21040;SQL&#39046;&#22495;&#20013;&#30340;&#22122;&#22768;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;BIRD-Bench&#22522;&#20934;&#27979;&#35797;&#20013;&#23384;&#22312;&#22823;&#37327;&#38382;&#39064;&#21644;&#26631;&#20934;&#26597;&#35810;&#20013;&#30340;&#22122;&#22768;&#65292;&#36825;&#20250;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12243</link><description>&lt;p&gt;
&#29702;&#35299;&#25991;&#26412;&#21040;SQL&#20013;&#22122;&#22768;&#30340;&#24433;&#21709;&#65306;&#23545;BIRD-Bench&#22522;&#20934;&#27979;&#35797;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effects of Noise in Text-to-SQL: An Examination of the BIRD-Bench Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12243
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#25991;&#26412;&#21040;SQL&#39046;&#22495;&#20013;&#30340;&#22122;&#22768;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#22312;BIRD-Bench&#22522;&#20934;&#27979;&#35797;&#20013;&#23384;&#22312;&#22823;&#37327;&#38382;&#39064;&#21644;&#26631;&#20934;&#26597;&#35810;&#20013;&#30340;&#22122;&#22768;&#65292;&#36825;&#20250;&#26174;&#33879;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Text-to-SQL&#28041;&#21450;&#23558;&#33258;&#28982;&#35821;&#35328;&#32763;&#35793;&#20026;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65288;SQL&#65289;&#65292;&#23545;&#20110;&#20351;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#21487;&#20197;&#22312;&#27809;&#26377;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#24191;&#27867;&#35775;&#38382;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#38024;&#23545;&#36825;&#20123;&#20219;&#21153;&#30340;&#27169;&#22411;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#21407;&#22240;&#21253;&#25324;&#23384;&#22312;&#8220;&#22122;&#22768;&#8221;&#65292;&#22914;&#27169;&#31946;&#38382;&#39064;&#21644;&#35821;&#27861;&#38169;&#35823;&#12290;&#35813;&#30740;&#31350;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;BIRD-Bench&#22522;&#20934;&#27979;&#35797;&#20013;&#22122;&#22768;&#30340;&#20998;&#24067;&#21644;&#31867;&#22411;&#20197;&#21450;&#22122;&#22768;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#34429;&#28982;BIRD-Bench&#26088;&#22312;&#27169;&#25311;&#33039;&#20081;&#21644;&#22024;&#26434;&#30340;&#25968;&#25454;&#24211;&#20540;&#65292;&#20294;&#24182;&#26410;&#21253;&#21547;&#38382;&#39064;&#21644;&#26631;&#20934;&#26597;&#35810;&#20013;&#30340;&#22122;&#22768;&#21644;&#38169;&#35823;&#12290;&#25105;&#20204;&#21457;&#29616;&#25968;&#25454;&#38598;&#20013;&#38382;&#39064;&#21644;&#26631;&#20934;&#26597;&#35810;&#20013;&#30340;&#22122;&#22768;&#26222;&#36941;&#23384;&#22312;&#65292;&#36328;&#39046;&#22495;&#23384;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#22122;&#22768;&#65292;&#24182;&#19988;&#22122;&#22768;&#31867;&#22411;&#20043;&#38388;&#20998;&#24067;&#19981;&#22343;&#21248;&#12290;&#23384;&#22312;&#19981;&#27491;&#30830;&#30340;&#26631;&#20934;SQL&#26597;&#35810;&#65292;&#36827;&#32780;&#29983;&#25104;&#19981;&#27491;&#30830;&#30340;&#26631;&#20934;&#31572;&#26696;&#65292;&#23545;&#22522;&#20934;&#27979;&#35797;&#30340;&#24433;&#21709;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12243v1 Announce Type: new  Abstract: Text-to-SQL, which involves translating natural language into Structured Query Language (SQL), is crucial for enabling broad access to structured databases without expert knowledge. However, designing models for such tasks is challenging due to numerous factors, including the presence of 'noise,' such as ambiguous questions and syntactical errors. This study provides an in-depth analysis of the distribution and types of noise in the widely used BIRD-Bench benchmark and the impact of noise on models. While BIRD-Bench was created to model dirty and noisy database values, it was not created to contain noise and errors in the questions and gold queries. We found that noise in questions and gold queries are prevalent in the dataset, with varying amounts across domains, and with an uneven distribution between noise types. The presence of incorrect gold SQL queries, which then generate incorrect gold answers, has a significant impact on the ben
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#30830;&#23450;&#24615;&#25191;&#34892;&#19994;&#21153;&#36923;&#36753;&#30340;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#26500;&#24314;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#26174;&#31034;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20351;&#29992;&#35813;&#31995;&#32479;&#24320;&#21457;&#32842;&#22825;&#26426;&#22120;&#20154;&#38656;&#35201;&#30340;&#24037;&#20316;&#37327;&#26126;&#26174;&#20943;&#23569;&#65292;&#24182;&#19988;&#20855;&#26377;&#22788;&#29702;&#22797;&#26434;&#23545;&#35805;&#21644;&#25193;&#23637;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#21040;&#22823;&#37327;&#20219;&#21153;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.12234</link><description>&lt;p&gt;
&#20855;&#26377;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38754;&#21521;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Task-Oriented Dialogue with In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12234
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#30830;&#23450;&#24615;&#25191;&#34892;&#19994;&#21153;&#36923;&#36753;&#30340;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#26500;&#24314;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#26174;&#31034;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20351;&#29992;&#35813;&#31995;&#32479;&#24320;&#21457;&#32842;&#22825;&#26426;&#22120;&#20154;&#38656;&#35201;&#30340;&#24037;&#20316;&#37327;&#26126;&#26174;&#20943;&#23569;&#65292;&#24182;&#19988;&#20855;&#26377;&#22788;&#29702;&#22797;&#26434;&#23545;&#35805;&#21644;&#25193;&#23637;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#21040;&#22823;&#37327;&#20219;&#21153;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21644;&#30830;&#23450;&#24615;&#25191;&#34892;&#19994;&#21153;&#36923;&#36753;&#30340;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#26500;&#24314;&#31995;&#32479;&#12290;LLM&#29992;&#20110;&#22312;&#34920;&#38754;&#24418;&#24335;&#30340;&#23545;&#35805;&#21644;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#65288;DSL&#65289;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#65292;&#21518;&#32773;&#29992;&#20110;&#25512;&#36827;&#19994;&#21153;&#36923;&#36753;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#24403;&#21069;&#24037;&#19994;&#20013;&#20027;&#35201;&#20351;&#29992;&#30340;&#22522;&#20110;&#24847;&#22270;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#31995;&#32479;&#24320;&#21457;&#32842;&#22825;&#26426;&#22120;&#20154;&#38656;&#35201;&#30340;&#24037;&#20316;&#37327;&#26126;&#26174;&#23569;&#20110;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22815;&#25104;&#21151;&#22320;&#22788;&#29702;&#23545;&#22522;&#20110;NLU&#31995;&#32479;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#22797;&#26434;&#23545;&#35805;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#31995;&#32479;&#20855;&#26377;&#21487;&#25193;&#23637;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#21040;&#22823;&#37327;&#20219;&#21153;&#30340;&#29702;&#24819;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20379;&#25105;&#20204;&#30340;&#23454;&#29616;&#20379;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12234v1 Announce Type: new  Abstract: We describe a system for building task-oriented dialogue systems combining the in-context learning abilities of large language models (LLMs) with the deterministic execution of business logic. LLMs are used to translate between the surface form of the conversation and a domain-specific language (DSL) which is used to progress the business logic. We compare our approach to the intent-based NLU approach predominantly used in industry today. Our experiments show that developing chatbots with our system requires significantly less effort than established approaches, that these chatbots can successfully navigate complex dialogues which are extremely challenging for NLU-based systems, and that our system has desirable properties for scaling task-oriented dialogue systems to a large number of tasks. We make our implementation available for use and further study.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#21644;&#24494;&#35843;&#20219;&#21153;&#36827;&#34892;&#27604;&#36739;&#65292;&#36827;&#19968;&#27493;&#29702;&#35299;&#20102;&#22312;&#36716;&#25442;&#22120;&#21069;&#39304;&#23618;&#20013;&#26356;&#26032;&#20851;&#38190;&#25110;&#20540;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12233</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#36716;&#25442;&#22120;&#21069;&#39304;&#23618;&#20013;&#20851;&#38190;-&#20540;&#35760;&#24518;&#26356;&#26032;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Empirical Study on Updating Key-Value Memories in Transformer Feed-forward Layers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12233
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#21644;&#24494;&#35843;&#20219;&#21153;&#36827;&#34892;&#27604;&#36739;&#65292;&#36827;&#19968;&#27493;&#29702;&#35299;&#20102;&#22312;&#36716;&#25442;&#22120;&#21069;&#39304;&#23618;&#20013;&#26356;&#26032;&#20851;&#38190;&#25110;&#20540;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#65306;&#21464;&#21387;&#22120;&#20013;&#30340;&#21069;&#39304;&#32593;&#32476;&#65288;FFNs&#65289;&#34987;&#35748;&#20026;&#26159;&#19968;&#32452;&#20851;&#38190;-&#20540;&#31070;&#32463;&#35760;&#24518;&#65292;&#29992;&#20110;&#24674;&#22797;&#25277;&#35937;&#30340;&#39640;&#23618;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#26356;&#26032;&#38190;&#65288;FFNs&#23618;&#20013;&#30340;&#31532;&#19968;&#23618;&#65289;&#25110;&#20540;&#65288;FFNs&#23618;&#20013;&#30340;&#31532;&#20108;&#23618;&#65289;&#36827;&#34892;&#20102;&#23454;&#35777;&#28040;&#34701;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21508;&#31181;&#30693;&#35782;&#32534;&#36753;&#21644;&#24494;&#35843;&#20219;&#21153;&#20013;&#27604;&#36739;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;FFNs&#30340;&#35265;&#35299;&#12290;&#20195;&#30721;&#21487;&#22312; $\href{https://github.com/qiuzh20/Tuning-keys-v.s.-values}{&#27492;&#23384;&#20648;&#24211;}$ &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12233v1 Announce Type: new  Abstract: The feed-forward networks (FFNs) in transformers are recognized as a group of key-value neural memories to restore abstract high-level knowledge. In this work, we conduct an empirical ablation study on updating keys (the 1st layer in the FFNs layer) or values (the 2nd layer in the FFNs layer). We compare those two methods in various knowledge editing and fine-tuning tasks of large language models to draw insights to understand FFNs further. Code is available at $\href{https://github.com/qiuzh20/Tuning-keys-v.s.-values}{this\,repo}$.
&lt;/p&gt;</description></item><item><title>AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12226</link><description>&lt;p&gt;
AnyGPT&#65306;&#32479;&#19968;&#30340;&#22810;&#27169;&#24335;&#31163;&#25955;&#24207;&#21015;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12226
&lt;/p&gt;
&lt;p&gt;
AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; AnyGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20219;&#24847;&#22810;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#31163;&#25955;&#34920;&#31034;&#32479;&#19968;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#38899;&#20048;&#12290;AnyGPT &#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#26080;&#38656;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26550;&#26500;&#25110;&#35757;&#32451;&#33539;&#24335;&#36827;&#34892;&#20219;&#20309;&#25913;&#21160;&#12290;&#30456;&#21453;&#65292;&#23427;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#32423;&#39044;&#22788;&#29702;&#65292;&#20419;&#36827;&#20102;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#38598;&#25104;&#21040;LLM&#20013;&#65292;&#31867;&#20284;&#20110;&#26032;&#35821;&#35328;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24335;&#25991;&#26412;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22810;&#27169;&#24335;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#21512;&#25104;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#20219;&#24847;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#25324;108k&#20010;&#22810;&#36718;&#23545;&#35805;&#31034;&#20363;&#65292;&#31934;&#32454;&#22320;&#20132;&#32455;&#21508;&#31181;&#27169;&#24577;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20219;&#24847;&#32452;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AnyGPT&#33021;&#22815;&#20419;&#36827;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
&lt;/p&gt;</description></item><item><title>CovRL&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35206;&#30422;&#21453;&#39304;&#65292;&#36890;&#36807;&#26500;&#24314;&#21152;&#26435;&#35206;&#30422;&#26144;&#23556;&#24182;&#24212;&#29992;&#20110;&#22522;&#20110;LLM&#30340;&#21464;&#24322;&#22120;&#65292;&#20197;&#25552;&#21319;&#23545;JavaScript&#24341;&#25806;&#30340;&#27169;&#31946;&#27979;&#35797;&#25928;&#26524;</title><link>https://arxiv.org/abs/2402.12222</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#35206;&#30422;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;LLM&#22522;&#30784;&#21464;&#24322;&#36827;&#34892;JavaScript&#24341;&#25806;&#27169;&#31946;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CovRL: Fuzzing JavaScript Engines with Coverage-Guided Reinforcement Learning for LLM-based Mutation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12222
&lt;/p&gt;
&lt;p&gt;
CovRL&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35206;&#30422;&#21453;&#39304;&#65292;&#36890;&#36807;&#26500;&#24314;&#21152;&#26435;&#35206;&#30422;&#26144;&#23556;&#24182;&#24212;&#29992;&#20110;&#22522;&#20110;LLM&#30340;&#21464;&#24322;&#22120;&#65292;&#20197;&#25552;&#21319;&#23545;JavaScript&#24341;&#25806;&#30340;&#27169;&#31946;&#27979;&#35797;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#31946;&#27979;&#35797;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#21457;&#29616;&#38169;&#35823;&#30340;&#25216;&#26415;&#65292;&#20294;&#22312;&#20687;&#38656;&#35201;&#31934;&#30830;&#35821;&#27861;&#36755;&#20837;&#30340;JavaScript&#24341;&#25806;&#36825;&#26679;&#30340;&#22797;&#26434;&#31995;&#32479;&#20013;&#24456;&#38590;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#23545;&#27169;&#31946;&#27979;&#35797;&#20013;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#21464;&#24322;&#36827;&#34892;&#20102;&#25913;&#36827;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25216;&#26415;&#22312;&#21033;&#29992;&#35206;&#30422;&#24341;&#23548;&#36827;&#34892;&#27169;&#31946;&#27979;&#35797;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#36890;&#24120;&#20197;&#40657;&#30418;&#26041;&#24335;&#25191;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CovRL&#65288;&#22522;&#20110;&#35206;&#30422;&#24341;&#23548;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20174;&#35206;&#30422;&#21453;&#39304;&#20013;&#24471;&#21040;&#30340;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#27169;&#31946;&#22120;CovRL-Fuzz&#36890;&#36807;&#21033;&#29992;&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;TF-IDF&#65289;&#26041;&#27861;&#23558;&#35206;&#30422;&#21453;&#39304;&#30452;&#25509;&#38598;&#25104;&#21040;LLM&#20013;&#65292;&#26500;&#24314;&#21152;&#26435;&#35206;&#30422;&#26144;&#23556;&#12290;&#35813;&#26144;&#23556;&#22312;&#35745;&#31639;&#27169;&#31946;&#27979;&#35797;&#22870;&#21169;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28982;&#21518;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#22522;&#20110;LLM&#30340;&#21464;&#24322;&#22120;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;CovRL-Fuzz&#23454;&#29616;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12222v1 Announce Type: cross  Abstract: Fuzzing is an effective bug-finding technique but it struggles with complex systems like JavaScript engines that demand precise grammatical input. Recently, researchers have adopted language models for context-aware mutation in fuzzing to address this problem. However, existing techniques are limited in utilizing coverage guidance for fuzzing, which is rather performed in a black-box manner. This paper presents a novel technique called CovRL (Coverage-guided Reinforcement Learning) that combines Large Language Models (LLMs) with reinforcement learning from coverage feedback. Our fuzzer, CovRL-Fuzz, integrates coverage feedback directly into the LLM by leveraging the Term Frequency-Inverse Document Frequency (TF-IDF) method to construct a weighted coverage map. This map is key in calculating the fuzzing reward, which is then applied to the LLM-based mutator through reinforcement learning. CovRL-Fuzz, through this approach, enables the g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReAlign&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#25351;&#23548;&#25968;&#25454;&#30340;&#21709;&#24212;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.12219</link><description>&lt;p&gt;
&#37325;&#26032;&#26684;&#24335;&#21270;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Reformatted Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ReAlign&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#25351;&#23548;&#25968;&#25454;&#30340;&#21709;&#24212;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#24494;&#35843;&#25968;&#25454;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#30340;&#26041;&#27861;&#35201;&#20040;&#32791;&#26102;&#36153;&#21147;&#65292;&#35201;&#20040;&#23481;&#26131;&#21463;&#21040;LLM&#24187;&#35273;&#24341;&#36215;&#30340;&#20107;&#23454;&#38169;&#35823;&#24433;&#21709;&#12290;&#26412;&#25991;&#25506;&#35752;&#25552;&#21319;&#29616;&#26377;&#25351;&#23548;&#25968;&#25454;&#36136;&#37327;&#20197;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;ReAlign&#30340;&#31616;&#21333;&#26377;&#25928;&#26041;&#27861;&#65292;&#23427;&#23558;&#25351;&#23548;&#25968;&#25454;&#30340;&#21709;&#24212;&#37325;&#26032;&#26684;&#24335;&#21270;&#20026;&#26356;&#31526;&#21512;&#39044;&#20808;&#24314;&#31435;&#26631;&#20934;&#21644;&#32534;&#35793;&#35777;&#25454;&#30340;&#26684;&#24335;&#12290;&#35813;&#26041;&#27861;&#26368;&#23567;&#21270;&#20102;&#20154;&#31867;&#27880;&#37322;&#12289;&#24187;&#35273;&#21644;&#25193;&#23637;&#22256;&#38590;&#65292;&#19982;&#29616;&#26377;&#23545;&#40784;&#25216;&#26415;&#27491;&#20132;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ReAlign&#26174;&#33879;&#25552;&#21319;&#20102;LLMs&#30340;&#25972;&#20307;&#23545;&#40784;&#33021;&#21147;&#12289;&#25968;&#23398;&#25512;&#29702;&#12289;&#20107;&#23454;&#24615;&#21644;&#21487;&#35835;&#24615;&#12290;&#20196;&#20154;&#40723;&#33310;&#30340;&#26159;&#65292;&#22312;&#19981;&#24341;&#20837;&#20219;&#20309;&#39069;&#22806;&#25968;&#25454;&#25110;&#20808;&#36827;&#35757;&#32451;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#36890;&#36807;&#37325;&#26032;&#26684;&#24335;&#21270;&#21709;&#24212;&#65292;LLaMA-2-13
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12219v1 Announce Type: cross  Abstract: The quality of finetuning data is crucial for aligning large language models (LLMs) with human values. Current methods to improve data quality are either labor-intensive or prone to factual errors caused by LLM hallucinations. This paper explores elevating the quality of existing instruction data to better align with human values, introducing a simple and effective approach named ReAlign, which reformats the responses of instruction data into a format that better aligns with pre-established criteria and the collated evidence. This approach minimizes human annotation, hallucination, and the difficulty in scaling, remaining orthogonal to existing alignment techniques. Experimentally, ReAlign significantly boosts the general alignment ability, math reasoning, factuality, and readability of the LLMs.   Encouragingly, without introducing any additional data or advanced training techniques, and merely by reformatting the response, LLaMA-2-13
&lt;/p&gt;</description></item><item><title>&#22312;&#22238;&#22768;&#23460;&#29615;&#22659;&#20013;&#65292;&#22522;&#20110;ChatGPT&#30340;&#33258;&#20027;&#29983;&#25104;AI&#20195;&#29702;&#24448;&#24448;&#20250;&#21457;&#29983;&#26497;&#21270;&#65292;&#36825;&#23545;&#20110;&#20102;&#35299;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;</title><link>https://arxiv.org/abs/2402.12212</link><description>&lt;p&gt;
&#33258;&#20027;&#29983;&#25104;AI&#20195;&#29702;&#22312;&#22238;&#22768;&#23460;&#20013;&#30340;&#26497;&#21270;
&lt;/p&gt;
&lt;p&gt;
Polarization of Autonomous Generative AI Agents Under Echo Chambers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12212
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22238;&#22768;&#23460;&#29615;&#22659;&#20013;&#65292;&#22522;&#20110;ChatGPT&#30340;&#33258;&#20027;&#29983;&#25104;AI&#20195;&#29702;&#24448;&#24448;&#20250;&#21457;&#29983;&#26497;&#21270;&#65292;&#36825;&#23545;&#20110;&#20102;&#35299;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#36890;&#24120;&#20250;&#20135;&#29983;&#22238;&#22768;&#23460;&#65292;&#20351;&#20154;&#20204;&#21482;&#21548;&#21040;&#19982;&#20854;&#20449;&#24565;&#30456;&#31526;&#30340;&#35266;&#28857;&#12290;&#22238;&#38899;&#23460;&#32463;&#24120;&#20250;&#20135;&#29983;&#26497;&#21270;&#65292;&#23548;&#33268;&#30001;&#25345;&#26377;&#28608;&#36827;&#35266;&#28857;&#30340;&#20154;&#24341;&#36215;&#30340;&#20914;&#31361;&#65292;&#27604;&#22914;2021&#24180;1&#26376;6&#26085;&#23545;&#32654;&#22269;&#22269;&#20250;&#22823;&#21414;&#30340;&#34989;&#20987;&#12290;&#22238;&#38899;&#23460;&#34987;&#35270;&#20026;&#19968;&#31181;&#20154;&#31867;&#29305;&#26377;&#30340;&#38382;&#39064;&#65292;&#20294;&#38543;&#30528;&#35832;&#22914;ChatGPT&#20043;&#31867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#31038;&#20132;&#33021;&#21147;&#65292;&#36825;&#31181;&#38544;&#21547;&#30340;&#20551;&#35774;&#21464;&#24471;&#19981;&#22826;&#21512;&#29702;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#32452;&#33258;&#20027;AI&#20195;&#29702;&#22312;&#22238;&#22768;&#23460;&#29615;&#22659;&#20013;&#21457;&#29983;&#26497;&#21270;&#30340;&#28508;&#22312;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#35753;AI&#20195;&#29702;&#35752;&#35770;&#29305;&#23450;&#20027;&#39064;&#65292;&#24182;&#20998;&#26512;&#20102;&#38543;&#30528;&#35752;&#35770;&#30340;&#36827;&#34892;&#65292;&#32676;&#20307;&#24847;&#35265;&#22914;&#20309;&#21464;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;ChatGPT&#30340;&#19968;&#32452;&#20195;&#29702;&#22312;&#22238;&#22768;&#23460;&#29615;&#22659;&#20013;&#24448;&#24448;&#20250;&#21464;&#24471;&#26497;&#21270;&#12290;&#24847;&#35265;&#36716;&#21464;&#30340;&#20998;&#26512;&#26174;&#31034;&#20102;&#36825;&#19968;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12212v1 Announce Type: new  Abstract: Online social networks often create echo chambers where people only hear opinions reinforcing their beliefs. An echo chamber often generates polarization, leading to conflicts caused by people with radical opinions, such as the January 6, 2021, attack on the US Capitol. The echo chamber has been viewed as a human-specific problem, but this implicit assumption is becoming less reasonable as large language models, such as ChatGPT, acquire social abilities. In response to this situation, we investigated the potential for polarization to occur among a group of autonomous AI agents based on generative language models in an echo chamber environment. We had AI agents discuss specific topics and analyzed how the group's opinions changed as the discussion progressed. As a result, we found that the group of agents based on ChatGPT tended to become polarized in echo chamber environments. The analysis of opinion transitions shows that this result is
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#33976;&#39311;&#20174;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#36827;&#34892;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12204</link><description>&lt;p&gt;
&#36890;&#36807;&#20174;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#36827;&#34892;&#33258;&#33976;&#39311;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multilingual Capabilities of Large Language Models through Self-Distillation from Resource-Rich Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12204
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#33976;&#39311;&#20174;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#36827;&#34892;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#35821;&#35328;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#28982;&#33853;&#21518;&#20110;&#23569;&#25968;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#26469;&#33258;&#36164;&#28304;&#20016;&#23500;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#32763;&#35793;&#25104;&#20854;&#20182;&#35821;&#35328;&#65292;&#28982;&#21518;&#32487;&#32493;&#35757;&#32451;&#65292;&#20294;&#26159;&#20165;&#20381;&#36182;&#32763;&#35793;&#33719;&#24471;&#30340;&#25968;&#25454;&#65292;&#32780;&#24573;&#30053;&#20102;LLMs&#36328;&#35821;&#35328;&#30340;&#21407;&#22987;&#33021;&#21147;&#65292;&#19981;&#24635;&#26159;&#26377;&#25928;&#30340;&#65292;&#25105;&#20204;&#23637;&#31034;&#23558;&#38480;&#21046;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SDRRL&#65292;&#19968;&#31181;&#22522;&#20110;&#20174;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#36827;&#34892;&#33258;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;LLMs&#22312;&#36164;&#28304;&#20016;&#23500;&#35821;&#35328;&#19978;&#30340;&#20869;&#22312;&#33021;&#21147;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#22810;&#35821;&#35328;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#30340;LLMs&#65288;LLaMA-2&#21644;SeaLLM&#65289;&#21644;&#28304;&#35821;&#35328;&#19978;&#35780;&#20272;&#21508;&#31181;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;SDRRL
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12204v1 Announce Type: new  Abstract: While large language models (LLMs) have been pre-trained on multilingual corpora, their performance still lags behind in most languages compared to a few resource-rich languages. One common approach to mitigate this issue is to translate training data from resource-rich languages into other languages and then continue training. However, using the data obtained solely relying on translation while ignoring the original capabilities of LLMs across languages is not always effective, which we show will limit the performance of cross-lingual knowledge transfer. In this work, we propose SDRRL, a method based on Self-Distillation from Resource-Rich Languages that effectively improve multilingual performance by leveraging the internal capabilities of LLMs on resource-rich languages. We evaluate on different LLMs (LLaMA-2 and SeaLLM) and source languages across various comprehension and generation tasks, experimental results demonstrate that SDRRL
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38646;-shot&#20998;&#31867;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#22914;&#24694;&#24847;&#27169;&#22240;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2402.12198</link><description>&lt;p&gt;
&#38646;-shot &#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#65306;&#25105;&#20204;&#24050;&#32463;&#21040;&#36798;&#30446;&#26631;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Zero shot VLMs for hate meme detection: Are we there yet?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38646;-shot&#20998;&#31867;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#22914;&#24694;&#24847;&#27169;&#22240;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#20854;&#20013;&#27169;&#22240;&#20316;&#20026;&#19968;&#31181;&#29420;&#29305;&#24418;&#24335;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#19968;&#20123;&#24694;&#24847;&#29992;&#25143;&#21033;&#29992;&#27169;&#22240;&#38024;&#23545;&#20010;&#20154;&#25110;&#26131;&#21463;&#25915;&#20987;&#30340;&#31038;&#21306;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#35782;&#21035;&#21644;&#35299;&#20915;&#27492;&#31867;&#24694;&#24847;&#27169;&#22240;&#12290;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#24320;&#21457;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#20010;&#26174;&#33879;&#23616;&#38480;&#24615;&#26159;&#38656;&#35201;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#25165;&#33021;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#30028;&#35265;&#35777;&#20102;&#20960;&#31181;&#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#36825;&#20123;&#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#35832;&#22914;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#31561;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#35774;&#32622;&#26469;&#19987;&#27880;&#20110;&#23545;&#24694;&#24847;/&#26377;&#23475;&#27169;&#22240;&#30340;&#38646;-shot &#20998;&#31867;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12198v1 Announce Type: new  Abstract: Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification. Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection. We use various prompt settings to focus on zero-shot classification of hateful/harmful memes. Through our analysis, we o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#33539;&#24335;"&#27983;&#35272;&#21644;&#38598;&#20013;"&#65292;&#36890;&#36807;&#22312;&#23558;&#29305;&#24449;&#36755;&#20837;LLMs&#20043;&#21069;&#36827;&#34892;&#28145;&#20837;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#20869;&#23481;&#29702;&#35299;&#20013;&#30340; prior-LLM &#27169;&#24577;&#38548;&#31163;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.12195</link><description>&lt;p&gt;
&#36890;&#36807; prior-LLM &#19978;&#19979;&#25991;&#34701;&#21512;&#26469;&#29702;&#35299;&#22810;&#27169;&#24577;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Browse and Concentrate: Comprehending Multimodal Content via prior-LLM Context Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12195
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#33539;&#24335;"&#27983;&#35272;&#21644;&#38598;&#20013;"&#65292;&#36890;&#36807;&#22312;&#23558;&#29305;&#24449;&#36755;&#20837;LLMs&#20043;&#21069;&#36827;&#34892;&#28145;&#20837;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#20869;&#23481;&#29702;&#35299;&#20013;&#30340; prior-LLM &#27169;&#24577;&#38548;&#31163;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#36817;&#26399;&#23558;LLMs&#19982;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#21508;&#31181;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#19978;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#29702;&#35299;&#28041;&#21450;&#22810;&#24352;&#22270;&#29255;&#30340;&#19978;&#19979;&#25991;&#26041;&#38754;&#20173;&#26377;&#19981;&#36275;&#12290;&#36825;&#19968;&#32570;&#38519;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#65292;&#22312;&#23558;&#35270;&#35273;&#29305;&#24449;&#36755;&#20837;LLM&#20027;&#24178;&#20043;&#21069;&#65292;&#27599;&#24352;&#22270;&#29255;&#30340;&#35270;&#35273;&#29305;&#24449;&#37117;&#26159;&#30001;&#20923;&#32467;&#30340;&#32534;&#30721;&#22120;&#21333;&#29420;&#32534;&#30721;&#30340;&#65292;&#32570;&#20047;&#23545;&#20854;&#20182;&#22270;&#29255;&#21644;&#22810;&#27169;&#24577;&#25351;&#20196;&#30340;&#24847;&#35782;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#38382;&#39064;&#31216;&#20026; prior-LLM &#27169;&#24577;&#38548;&#31163;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#33539;&#24335;&#65292;&#21363;&#8220;&#27983;&#35272;&#21644;&#38598;&#20013;&#8221;&#65292;&#20197;&#23454;&#29616;&#22312;&#23558;&#29305;&#24449;&#36755;&#20837;LLMs&#20043;&#21069;&#36827;&#34892;&#28145;&#20837;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#34701;&#21512;&#12290;&#36825;&#31181;&#33539;&#24335;&#26368;&#21021;&#8220;&#27983;&#35272;&#8221;&#36755;&#20837;&#20197;&#33719;&#21462;&#20851;&#38190;&#35265;&#35299;&#65292;&#28982;&#21518;&#20877;&#27425;&#22238;&#39038;&#36755;&#20837;&#8220;&#38598;&#20013;&#8221;&#20110;&#20851;&#38190;&#32454;&#33410;&#65292;&#36890;&#36807;&#36825;&#20123;&#35265;&#35299;&#30340;&#25351;&#23548;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#22810;&#27169;&#24577;&#20869;&#23481;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12195v1 Announce Type: new  Abstract: With the bloom of Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) that incorporate LLMs with pre-trained vision models have recently demonstrated impressive performance across diverse vision-language tasks. However, they fall short to comprehend context involving multiple images. A primary reason for this shortcoming is that the visual features for each images are encoded individually by frozen encoders before feeding into the LLM backbone, lacking awareness of other images and the multimodal instructions. We term this issue as prior-LLM modality isolation and propose a two phase paradigm, browse-and-concentrate, to enable in-depth multimodal context fusion prior to feeding the features into LLMs. This paradigm initially "browses" through the inputs for essential insights, and then revisits the inputs to "concentrate" on crucial details, guided by these insights, to achieve a more comprehensive understanding of the
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;LLMs&#23433;&#20840;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#30340;&#23433;&#20840;&#35780;&#20272;&#26631;&#20934;&#65292;&#20197;&#21450;&#25193;&#23637;&#20102;&#20004;&#31181;&#22330;&#26223;&#29992;&#20110;&#35782;&#21035;&#26377;&#39118;&#38505;&#25552;&#31034;&#25298;&#32477;&#30340;&#34394;&#38420;&#36127;&#38754;&#21644;&#38169;&#35823;&#32943;&#23450;&#31034;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.12193</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23433;&#20840;&#26426;&#21046;&#30340;&#20013;&#25991;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Chinese Dataset for Evaluating the Safeguards in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12193
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;LLMs&#23433;&#20840;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#32454;&#31890;&#24230;&#30340;&#23433;&#20840;&#35780;&#20272;&#26631;&#20934;&#65292;&#20197;&#21450;&#25193;&#23637;&#20102;&#20004;&#31181;&#22330;&#26223;&#29992;&#20110;&#35782;&#21035;&#26377;&#39118;&#38505;&#25552;&#31034;&#25298;&#32477;&#30340;&#34394;&#38420;&#36127;&#38754;&#21644;&#38169;&#35823;&#32943;&#23450;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#20135;&#29983;&#26377;&#23475;&#21709;&#24212;&#65292;&#22312;LLMs&#37096;&#32626;&#26102;&#20351;&#29992;&#25143;&#38754;&#20020;&#24847;&#22806;&#39118;&#38505;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20851;&#20110;LLMs&#24341;&#21457;&#39118;&#38505;&#30340;&#32508;&#21512;&#20998;&#31867;&#27861;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#25552;&#31034;&#65292;&#21487;&#29992;&#20110;&#26816;&#26597;LLMs&#30340;&#23433;&#20840;&#26426;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20960;&#20046;&#23436;&#20840;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#65292;&#20854;&#20182;&#35821;&#35328;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20013;&#25991;LLMs&#23433;&#20840;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#23558;&#20854;&#25193;&#23637;&#21040;&#21478;&#22806;&#20004;&#31181;&#24773;&#26223;&#65292;&#21487;&#29992;&#20110;&#26356;&#22909;&#22320;&#35782;&#21035;&#20851;&#20110;&#26377;&#39118;&#38505;&#25552;&#31034;&#25298;&#32477;&#30340;&#34394;&#38420;&#36127;&#38754;&#21644;&#38169;&#35823;&#32943;&#23450;&#31034;&#20363;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#38024;&#23545;&#27599;&#31181;&#39118;&#38505;&#31867;&#22411;&#25552;&#20986;&#19968;&#32452;&#32454;&#31890;&#24230;&#30340;&#23433;&#20840;&#35780;&#20272;&#26631;&#20934;&#65292;&#20419;&#36827;&#20154;&#24037;&#26631;&#27880;&#21644;&#33258;&#21160;&#35780;&#20272;LLM&#21709;&#24212;&#26377;&#23475;&#24615;&#12290;&#25105;&#20204;&#23545;&#20116;&#20010;LLM&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#29305;&#23450;&#20110;&#22320;&#21306;&#30340;&#39118;&#38505;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12193v1 Announce Type: new  Abstract: Many studies have demonstrated that large language models (LLMs) can produce harmful responses, exposing users to unexpected risks when LLMs are deployed. Previous studies have proposed comprehensive taxonomies of the risks posed by LLMs, as well as corresponding prompts that can be used to examine the safety mechanisms of LLMs. However, the focus has been almost exclusively on English, and little has been explored for other languages. Here we aim to bridge this gap. We first introduce a dataset for the safety evaluation of Chinese LLMs, and then extend it to two other scenarios that can be used to better identify false negative and false positive examples in terms of risky prompt rejections. We further present a set of fine-grained safety assessment criteria for each risk type, facilitating both manual annotation and automatic evaluation in terms of LLM response harmfulness. Our experiments on five LLMs show that region-specific risks a
&lt;/p&gt;</description></item><item><title>&#25915;&#20987;&#32773;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;LM&#36827;&#34892;&#23545;&#25239;&#24494;&#35843;&#65292;&#20197;&#25918;&#22823;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#26333;&#20809;&#65292;&#37319;&#29992;&#20266;&#26631;&#31614;&#21644;&#26426;&#22120;&#29983;&#25104;&#27010;&#29575;&#26469;&#21152;&#24378;LM&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#20445;&#30041;&#12290;</title><link>https://arxiv.org/abs/2402.12189</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20266;&#26631;&#31614;&#25104;&#21592;&#36164;&#26684;&#36827;&#34892;&#24494;&#35843;&#26469;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#26333;&#20809;
&lt;/p&gt;
&lt;p&gt;
Amplifying Training Data Exposure through Fine-Tuning with Pseudo-Labeled Memberships
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12189
&lt;/p&gt;
&lt;p&gt;
&#25915;&#20987;&#32773;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;LM&#36827;&#34892;&#23545;&#25239;&#24494;&#35843;&#65292;&#20197;&#25918;&#22823;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#26333;&#20809;&#65292;&#37319;&#29992;&#20266;&#26631;&#31614;&#21644;&#26426;&#22120;&#29983;&#25104;&#27010;&#29575;&#26469;&#21152;&#24378;LM&#23545;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;(LMs)&#30001;&#20110;&#25968;&#25454;&#35760;&#24518;&#32780;&#23481;&#26131;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#25552;&#21462;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#22330;&#26223;&#65292;&#22312;&#36825;&#31181;&#22330;&#26223;&#20013;&#65292;&#25915;&#20987;&#32773;&#23545;&#39044;&#35757;&#32451;LM&#36827;&#34892;&#23545;&#25239;&#24494;&#35843;&#65292;&#20197;&#25918;&#22823;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#26333;&#20809;&#12290;&#35813;&#31574;&#30053;&#19981;&#21516;&#20110;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#20854;&#30446;&#30340;&#26159;&#21152;&#24378;LM&#23545;&#20854;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#20445;&#30041;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25915;&#20987;&#32773;&#38656;&#35201;&#25910;&#38598;&#19982;&#39044;&#35757;&#32451;&#25968;&#25454;&#23494;&#20999;&#30456;&#20851;&#30340;&#29983;&#25104;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#30693;&#35782;&#65292;&#34913;&#37327;&#29983;&#25104;&#25991;&#26412;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#37327;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#30446;&#26631;LM&#30340;&#26426;&#22120;&#29983;&#25104;&#27010;&#29575;&#25152;&#34920;&#31034;&#30340;&#25104;&#21592;&#36817;&#20284;&#20540;&#20026;&#36825;&#20123;&#29983;&#25104;&#25991;&#26412;&#20351;&#29992;&#20266;&#26631;&#31614;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24494;&#35843;LM&#20197;&#25903;&#25345;&#37027;&#20123;&#26356;&#26377;&#21487;&#33021;&#28304;&#33258;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#29983;&#25104;&#25991;&#26412;&#65292;&#26681;&#25454;&#20854;&#25104;&#21592;&#36164;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12189v1 Announce Type: new  Abstract: Neural language models (LMs) are vulnerable to training data extraction attacks due to data memorization. This paper introduces a novel attack scenario wherein an attacker adversarially fine-tunes pre-trained LMs to amplify the exposure of the original training data. This strategy differs from prior studies by aiming to intensify the LM's retention of its pre-training dataset. To achieve this, the attacker needs to collect generated texts that are closely aligned with the pre-training data. However, without knowledge of the actual dataset, quantifying the amount of pre-training data within generated texts is challenging. To address this, we propose the use of pseudo-labels for these generated texts, leveraging membership approximations indicated by machine-generated probabilities from the target LM. We subsequently fine-tune the LM to favor generations with higher likelihoods of originating from the pre-training data, based on their memb
&lt;/p&gt;</description></item><item><title>Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12177</link><description>&lt;p&gt;
Mafin: &#29992;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#26469;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12177
&lt;/p&gt;
&lt;p&gt;
Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#32463;&#25104;&#20026;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;RAG&#20013;&#30340;&#26816;&#32034;&#38454;&#27573;&#36890;&#24120;&#28041;&#21450;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#26597;&#35810;&#21644;&#27573;&#33853;&#36716;&#25442;&#20026;&#21521;&#37327;&#20197;&#25429;&#33719;&#23427;&#20204;&#30340;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#26102;&#65292;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#21487;&#33021;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#65292;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20165;&#33021;&#20174;&#40657;&#30418;&#27169;&#22411;&#33719;&#21462;&#23884;&#20837;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#65288;Mafin&#65289;--&#19968;&#31181;&#36890;&#36807;&#29992;&#21487;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;&#27169;&#22411;&#26469;&#36827;&#34892;&#24494;&#35843;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Mafin&#20165;&#38656;&#35201;&#35757;&#32451;&#19968;&#20010;&#23567;&#30340;&#22686;&#24378;&#27169;&#22411;&#23601;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#40657;&#30418;&#23884;&#20837;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
&lt;/p&gt;</description></item><item><title>BIDER&#36890;&#36807;&#30693;&#35782;&#32508;&#21512;&#21644;&#20559;&#22909;&#23545;&#40784;&#65292;&#23558;&#26816;&#32034;&#25991;&#26723;&#36716;&#21270;&#20026;&#20851;&#38190;&#25903;&#25345;&#35777;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;LLMs&#30340;&#31572;&#26696;&#36136;&#37327;&#24182;&#20943;&#23569;&#20102;&#36755;&#20837;&#20869;&#23481;&#38271;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.12174</link><description>&lt;p&gt;
BIDER: &#36890;&#36807;&#20851;&#38190;&#25903;&#25345;&#35777;&#25454;&#24357;&#21512;&#26377;&#25928;&#26816;&#32034;&#22686;&#24378;&#30340;LLMs&#20013;&#30340;&#30693;&#35782;&#19981;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
BIDER: Bridging Knowledge Inconsistency for Efficient Retrieval-Augmented LLMs via Key Supporting Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12174
&lt;/p&gt;
&lt;p&gt;
BIDER&#36890;&#36807;&#30693;&#35782;&#32508;&#21512;&#21644;&#20559;&#22909;&#23545;&#40784;&#65292;&#23558;&#26816;&#32034;&#25991;&#26723;&#36716;&#21270;&#20026;&#20851;&#38190;&#25903;&#25345;&#35777;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;LLMs&#30340;&#31572;&#26696;&#36136;&#37327;&#24182;&#20943;&#23569;&#20102;&#36755;&#20837;&#20869;&#23481;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24320;&#25918;&#39046;&#22495;QA&#31561;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#26356;&#26032;&#21644;&#20107;&#23454;&#19981;&#36275;&#31561;&#22266;&#26377;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#26816;&#32034;&#30693;&#35782;&#19982;LLMs&#25152;&#38656;&#30693;&#35782;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#23548;&#33268;&#20102;LLMs&#31572;&#26696;&#36136;&#37327;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BIDER&#65292;&#19968;&#31181;&#36890;&#36807;&#30693;&#35782;&#32508;&#21512;&#12289;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#20559;&#22909;&#23545;&#40784;&#65292;&#23558;&#26816;&#32034;&#25991;&#26723;&#32454;&#21270;&#20026;&#20851;&#38190;&#25903;&#25345;&#35777;&#25454;&#65288;KSE&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#21046;&#23450;KSE&#26469;&#35757;&#32451;BIDER&#65292;&#21516;&#26102;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23558;&#20854;&#36755;&#20986;&#26368;&#22823;&#21270;&#65292;&#20197;&#20351;&#20854;&#19982;LLMs&#30340;&#20449;&#24687;&#33719;&#21462;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;BIDER&#25552;&#39640;&#20102;LLMs&#31572;&#26696;&#36136;&#37327;7&#65285;&#65292;&#21516;&#26102;&#23558;&#26816;&#32034;&#25991;&#26723;&#30340;&#36755;&#20837;&#20869;&#23481;&#38271;&#24230;&#20943;&#23569;&#20102;80&#65285;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;KSE&#27169;&#25311;&#26377;&#25928;&#22320;&#20026;LLMs&#25552;&#20379;&#20102;&#24517;&#35201;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12174v1 Announce Type: new  Abstract: Retrieval-augmented large language models (LLMs) have demonstrated efficacy in knowledge-intensive tasks such as open-domain QA, addressing inherent challenges in knowledge update and factual inadequacy. However, inconsistencies between retrieval knowledge and the necessary knowledge for LLMs, leading to a decline in LLM's answer quality. This paper introduces BIDER, an approach that refines retrieval documents into Key Supporting Evidence (KSE) through knowledge synthesis, supervised fine-tuning (SFT), and preference alignment. We train BIDER by learning from crafting KSE, while maximizing its output to align with LLM's information acquisition preferences through reinforcement learning. Evaluations across five datasets show BIDER boosts LLMs' answer quality by 7% while reducing input content length in retrieval documents by 80%, outperforming existing methods. The proposed KSE simulation effectively equips LLMs with essential informatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25991;&#26723;&#65292;&#23454;&#29616;&#22312;&#26032;&#39046;&#22495;&#22238;&#31572;&#38382;&#39064;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.12170</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Unsupervised LLM Adaptation for Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12170
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#30446;&#26631;&#39046;&#22495;&#30340;&#26410;&#26631;&#35760;&#25991;&#26723;&#65292;&#23454;&#29616;&#22312;&#26032;&#39046;&#22495;&#22238;&#31572;&#38382;&#39064;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#23398;&#20064;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#26679;&#21270;&#30693;&#35782;&#12290;&#25509;&#30528;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;&#65292;LLM&#33021;&#22815;&#36820;&#22238;&#22810;&#26679;&#38382;&#39064;&#30340;&#27491;&#30830;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#39044;&#35757;&#32451;&#30340;LLM&#35843;&#25972;&#21040;&#26032;&#30340;&#30446;&#26631;&#39046;&#22495;&#65292;&#22914;&#19981;&#21516;&#32452;&#32455;&#25110;&#26102;&#26399;&#65292;&#29992;&#20110;&#38382;&#31572;&#20219;&#21153;&#20250;&#20135;&#29983;&#24456;&#39640;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#21363;&#26080;&#30417;&#30563;LLM&#36866;&#24212;&#38382;&#31572;&#20219;&#21153;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#12289;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;&#28304;&#25968;&#25454;&#65289;&#21644;&#30446;&#26631;&#22495;&#30340;&#26410;&#26631;&#35760;&#25991;&#26723;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;LLM&#65292;&#20351;&#20854;&#33021;&#22815;&#22238;&#31572;&#20851;&#20110;&#30446;&#26631;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#22312;&#28304;&#25968;&#25454;&#21644;&#30446;&#26631;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#65292;&#24182;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#35265;&#35299;&#65307;&#65288;i&#65289;&#24494;&#35843;&#27169;&#22411;&#23637;&#31034;&#20102;&#25552;&#20379;&#27491;&#30830;&#31572;&#26696;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12170v1 Announce Type: cross  Abstract: Large language models (LLM) learn diverse knowledge present in the large-scale training dataset via self-supervised training. Followed by instruction-tuning, LLM acquires the ability to return correct information for diverse questions. However, adapting these pre-trained LLMs to new target domains, such as different organizations or periods, for the question-answering (QA) task incurs a substantial annotation cost. To tackle this challenge, we propose a novel task, unsupervised LLM adaptation for question answering. In this task, we leverage a pre-trained LLM, a publicly available QA dataset (source data), and unlabeled documents from the target domain. Our goal is to learn LLM that can answer questions about the target domain. We introduce one synthetic and two real datasets to evaluate models fine-tuned on the source and target data, and reveal intriguing insights; (i) fine-tuned models exhibit the ability to provide correct answers 
&lt;/p&gt;</description></item><item><title>Transformer-based&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#38544;&#34255;&#31354;&#38388;&#20869;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#26469;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#36825;&#31181;&#32858;&#31867;&#36807;&#31243;&#22312;&#23398;&#20064;&#20013;&#21160;&#24577;&#28436;&#21464;&#65292;&#24182;&#26377;&#21161;&#20110;&#22788;&#29702;&#26410;&#35265;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.12151</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Causal Language Models Perform Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12151
&lt;/p&gt;
&lt;p&gt;
Transformer-based&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22312;&#38544;&#34255;&#31354;&#38388;&#20869;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#26469;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#36825;&#31181;&#32858;&#31867;&#36807;&#31243;&#22312;&#23398;&#20064;&#20013;&#21160;&#24577;&#28436;&#21464;&#65292;&#24182;&#26377;&#21161;&#20110;&#22788;&#29702;&#26410;&#35265;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#65292;LLM&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#33021;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#39069;&#22806;&#35757;&#32451;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#24050;&#32463;&#26174;&#31034;&#20986;&#24456;&#22823;&#25913;&#36827;&#65292;&#28982;&#32780;&#65292;&#23548;&#33268;&#26377;&#25928;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#30340;&#26426;&#21046;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#29702;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#38598;&#20998;&#26512;&#20102;&#22522;&#20110;Transformer&#30340;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#27169;&#22411;&#36890;&#36807;&#22312;&#20854;&#38544;&#34255;&#31354;&#38388;&#20869;&#23545;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#32780;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#20449;&#24687;&#65292;&#36825;&#31181;&#32858;&#31867;&#36807;&#31243;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21160;&#24577;&#28436;&#21464;&#12290;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#22914;&#20309;&#24110;&#21161;&#27169;&#22411;&#22788;&#29702;&#26410;&#35265;&#23454;&#20363;&#65292;&#24182;&#22312;&#26356;&#29616;&#23454;&#30340;&#29615;&#22659;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12151v1 Announce Type: cross  Abstract: Even though large language models (LLMs) have demonstrated remarkable capability in solving various natural language tasks, the capability of an LLM to follow human instructions is still a concern. Recent works have shown great improvements in the instruction-following capability via additional training for instruction-following tasks. However, the mechanisms responsible for effective instruction-following capabilities remain inadequately understood. Here, we introduce a simplified instruction-following task and use synthetic datasets to analyze a Transformer-based causal language model. Our findings suggest that the model learns task-specific information by clustering data within its hidden space, with this clustering process evolving dynamically during learning. We also demonstrate how this phenomenon assists the model in handling unseen instances and validate our results in a more realistic setting.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#29305;&#23450;&#35282;&#33394;&#20197;&#34920;&#36798;&#22810;&#26679;&#35266;&#28857;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;FairThinking&#27969;&#27700;&#32447;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#34920;&#36798;&#12290;</title><link>https://arxiv.org/abs/2402.12150</link><description>&lt;p&gt;
&#24744;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26263;&#20013;&#25903;&#25345;&#20844;&#24179;&#65292;&#24744;&#24212;&#35813;&#20687;&#23545;&#24453;&#19968;&#20010;&#20844;&#24179;&#32773;&#37027;&#26679;&#25552;&#31034;&#23427;
&lt;/p&gt;
&lt;p&gt;
Your Large Language Model is Secretly a Fairness Proponent and You Should Prompt it Like One
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12150
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#29305;&#23450;&#35282;&#33394;&#20197;&#34920;&#36798;&#22810;&#26679;&#35266;&#28857;&#30340;&#26041;&#27861;&#65292;&#24182;&#24320;&#21457;&#20102;FairThinking&#27969;&#27700;&#32447;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#20984;&#26174;&#20102;&#30830;&#20445;&#20854;&#20844;&#24179;&#24615;&#30340;&#36843;&#20999;&#38656;&#35201;&#12290;&#28982;&#32780;&#65292;LLMs&#32463;&#24120;&#23637;&#31034;&#25903;&#37197;&#24615;&#35266;&#28857;&#65292;&#21516;&#26102;&#24573;&#35270;&#26469;&#33258;&#23569;&#25968;&#27966;&#30340;&#26367;&#20195;&#35266;&#28857;&#65292;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#20559;&#35265;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#20123;&#36829;&#21453;&#20844;&#24179;&#30340;&#34892;&#20026;&#21457;&#29983;&#26159;&#22240;&#20026;LLMs&#20351;&#29992;&#20195;&#34920;&#35757;&#32451;&#25968;&#25454;&#22823;&#22810;&#25968;&#30340;&#20154;&#31867;&#20010;&#24615;&#26469;&#34920;&#36798;&#20182;&#20204;&#30340;&#35266;&#28857;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#39564;&#35777;&#25552;&#31034;LLMs&#20351;&#29992;&#29305;&#23450;&#35282;&#33394;&#21487;&#20197;&#20351;LLMs&#34920;&#36798;&#22810;&#26679;&#35266;&#28857;&#12290;&#22522;&#20110;&#36825;&#19968;&#27934;&#23519;&#21644;&#35266;&#23519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;FairThinking&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#33021;&#35753;LLMs&#34920;&#36798;&#22810;&#26679;&#35266;&#28857;&#20197;&#23454;&#29616;&#20844;&#24179;&#34920;&#36798;&#30340;&#27969;&#27700;&#32447;&#12290;&#20026;&#20102;&#35780;&#20272;FairThinking&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#19977;&#20010;&#19982;&#20844;&#24179;&#30456;&#20851;&#20027;&#39064;&#30340;&#19968;&#21315;&#20010;&#39033;&#30446;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;GPT-3.5&#65292;GPT-4&#65292;Llama2&#21644;Mistral&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12150v1 Announce Type: cross  Abstract: The widespread adoption of large language models (LLMs) underscores the urgent need to ensure their fairness. However, LLMs frequently present dominant viewpoints while ignoring alternative perspectives from minority parties, resulting in potential biases. We hypothesize that these fairness-violating behaviors occur because LLMs express their viewpoints using a human personality that represents the majority of training data. In response to this, we validate that prompting LLMs with specific roles can allow LLMs to express diverse viewpoints. Building on this insight and observation, we develop FairThinking, a pipeline designed to automatically generate roles that enable LLMs to articulate diverse perspectives for fair expressions. To evaluate FairThinking, we create a dataset with a thousand items covering three fairness-related topics and conduct experiments on GPT-3.5, GPT-4, Llama2, and Mistral to demonstrate its superior performanc
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Factiverse AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#36328;&#35821;&#35328;&#30340;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20026;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.12147</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#35268;&#27169;&#30340;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;
&lt;/p&gt;
&lt;p&gt;
End-to-end multilingual fact-checking at scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12147
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Factiverse AI&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#36328;&#35821;&#35328;&#30340;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;&#65292;&#24182;&#19988;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#20026;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;Factiverse AI&#27169;&#22411;&#22312;100&#22810;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#31471;&#21040;&#31471;&#20107;&#23454;&#26680;&#26597;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23454;&#39564;&#24615;&#22522;&#20934;&#27979;&#35797;&#23637;&#31034;&#65292;&#20026;&#20107;&#23454;&#26680;&#26597;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#32988;&#36807;GPT-4&#12289;GPT-3.5-Turbo&#21644;Mistral-7b&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12147v1 Announce Type: cross  Abstract: In this article, we describe how you can perform end-to-end fact-checking in over 100 languages using Factiverse AI models. We also show through an experimental benchmark that fine-tuned models tailored for fact-checking tasks outperform Large Language Models such as GPT-4, GPT-3.5-Turbo, and Mistral-7b.
&lt;/p&gt;</description></item><item><title>Meta Ranking&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#26469;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12146</link><description>&lt;p&gt;
Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement
&lt;/p&gt;
&lt;p&gt;
Meta Ranking: Less Capable Language Models are Capable for Single Response Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12146
&lt;/p&gt;
&lt;p&gt;
Meta Ranking&#26041;&#27861;&#36890;&#36807;&#27604;&#36739;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#26469;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24191;&#27867;&#20219;&#21153;&#20013;&#23637;&#29616;&#24378;&#22823;&#24615;&#33021;&#65292;&#20294;&#20173;&#38754;&#20020;&#24187;&#35273;&#31561;&#21487;&#38752;&#24615;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20687;GPT-4&#36825;&#26679;&#39640;&#33021;&#21147;&#30340;LLMs&#22312;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#32780;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#36890;&#24120;&#34987;&#35843;&#25972;&#26469;&#35780;&#20272;&#23545;&#30456;&#21516;&#26597;&#35810;&#30340;&#21709;&#24212;&#30340;&#30456;&#23545;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#20351;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#26377;&#25928;&#22320;&#35780;&#20272;&#21333;&#20010;&#21709;&#24212;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;$\textit{Meta}$ $\textit{Ranking}$&#65288;MR&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20808;&#21069;&#30452;&#25509;&#35780;&#20272;&#21709;&#24212;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#30446;&#26631;&#26597;&#35810;-&#21709;&#24212;&#23545;&#19982;&#21442;&#32771;&#26597;&#35810;-&#21709;&#24212;&#23545;&#36827;&#34892;&#27604;&#36739;&#26469;&#23454;&#29616;&#21028;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#25512;&#29702;&#20219;&#21153;&#30340;LLM&#21709;&#24212;&#30340;&#38169;&#35823;&#26816;&#27979;&#20013;&#65292;MR&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;&#19981;&#20855;&#22791;&#33021;&#21147;&#30340;LLMs&#20063;&#33021;&#32988;&#36807;&#24378;&#22522;&#32447;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;MR&#21487;&#20197;&#34987;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12146v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have demonstrated strong performance on a wide range of tasks, they still face reliability challenges such as hallucination. Previous studies reveal that highly capable LLMs like GPT-4 are effective in judging the reliability of individual responses, while less capable ones are often tuned to evaluate the relative reliability of responses to the same query. To enable less capable LLMs to effectively judge the reliability of individual responses, we propose a novel method named $\textit{Meta}$ $\textit{Ranking}$ (MR). Unlike previous methods, which assess the response directly, we achieve the judgement by comparing the target query-response pair with reference query-response pairs. We found its remarkable effectiveness in error detection for LLM responses on reasoning tasks, where less capable LLMs could outperform strong baselines, even without fine-tuning. We further demonstrate that MR can be use
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25490;&#21517;&#30456;&#20851;&#20998;&#26512;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#22312;&#29983;&#25104;&#22270;&#20687;&#35780;&#20215;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12121</link><description>&lt;p&gt;
&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#35780;&#20215;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Image Review Ability of Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25490;&#21517;&#30456;&#20851;&#20998;&#26512;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#22312;&#29983;&#25104;&#22270;&#20687;&#35780;&#20215;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#26159;&#33021;&#22815;&#36890;&#36807;&#21333;&#20010;&#27169;&#22411;&#22788;&#29702;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;LVLM&#29983;&#25104;&#22270;&#20687;&#35780;&#20215;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;LVLM&#23545;&#22270;&#20687;&#30340;&#35780;&#20215;&#33021;&#21147;&#23578;&#26410;&#23436;&#20840;&#34987;&#29702;&#35299;&#65292;&#31361;&#26174;&#20102;&#23545;&#20854;&#35780;&#20215;&#33021;&#21147;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;&#19982;&#22270;&#20687;&#26631;&#39064;&#19981;&#21516;&#65292;&#35780;&#20215;&#25991;&#26412;&#21487;&#20197;&#20174;&#22270;&#20687;&#26500;&#22270;&#21644;&#26333;&#20809;&#31561;&#19981;&#21516;&#35270;&#35282;&#25776;&#20889;&#12290;&#36825;&#31181;&#35780;&#20215;&#35282;&#24230;&#30340;&#22810;&#26679;&#24615;&#20351;&#24471;&#38590;&#20197;&#21807;&#19968;&#30830;&#23450;&#22270;&#20687;&#30340;&#27491;&#30830;&#35780;&#20215;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#21517;&#30456;&#20851;&#20998;&#26512;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#31867;&#21644;LVLM&#23545;&#35780;&#20215;&#25991;&#26412;&#36827;&#34892;&#25490;&#21517;&#65292;&#28982;&#21518;&#27979;&#37327;&#36825;&#20123;&#25490;&#21517;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#26368;&#26032;LVLM&#22270;&#20687;&#35780;&#20215;&#33021;&#21147;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#26469;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12121v1 Announce Type: cross  Abstract: Large-scale vision language models (LVLMs) are language models that are capable of processing images and text inputs by a single model. This paper explores the use of LVLMs to generate review texts for images. The ability of LVLMs to review images is not fully understood, highlighting the need for a methodical evaluation of their review abilities. Unlike image captions, review texts can be written from various perspectives such as image composition and exposure. This diversity of review perspectives makes it difficult to uniquely determine a single correct review for an image. To address this challenge, we introduce an evaluation method based on rank correlation analysis, in which review texts are ranked by humans and LVLMs, then, measures the correlation between these rankings. We further validate this approach by creating a benchmark dataset aimed at assessing the image review ability of recent LVLMs. Our experiments with the dataset
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30830;&#20445;&#24402;&#19968;&#21270;&#23545;&#24207;&#21015;&#38271;&#24230;&#19981;&#21464;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#31227;&#38500;&#24322;&#24120;&#20540;&#30340;&#21516;&#26102;&#20419;&#36827;&#20102;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#39044;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.12102</link><description>&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#31227;&#38500;&#24322;&#24120;&#20540;&#26159;&#21542;&#26377;&#20854;&#30410;&#22788;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is It a Free Lunch for Removing Outliers during Pretraining?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12102
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30830;&#20445;&#24402;&#19968;&#21270;&#23545;&#24207;&#21015;&#38271;&#24230;&#19981;&#21464;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#31227;&#38500;&#24322;&#24120;&#20540;&#30340;&#21516;&#26102;&#20419;&#36827;&#20102;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#19981;&#26029;&#22686;&#38271;&#65292;&#37327;&#21270;&#30340;&#20316;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#26435;&#37325;&#25110;&#28608;&#27963;&#20013;&#23384;&#22312;&#30340;&#24322;&#24120;&#20540;&#26126;&#26174;&#24433;&#21709;&#20102;&#37327;&#21270;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;qtransformer &#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#20197;&#26080;&#24322;&#24120;&#20540;&#26041;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26032;&#22411; softmax &#20989;&#25968;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23427;&#20204;&#36866;&#29992;&#20110;&#37327;&#21270;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20840;&#31934;&#24230;&#24615;&#33021;&#30340;&#19979;&#38477;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#36890;&#36807;&#30830;&#20445;&#20854;&#24402;&#19968;&#21270;&#23545;&#24207;&#21015;&#38271;&#24230;&#19981;&#21464;&#26469;&#22686;&#24378;&#35813;&#26041;&#27861;&#65292;&#36825;&#26159;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#24357;&#21512;&#24046;&#36317;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#25913;&#36827;&#30340;&#26041;&#27861;&#36824;&#20419;&#36827;&#20102;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12102v1 Announce Type: cross  Abstract: With the growing size of large language models, the role of quantization becomes increasingly significant. However, outliers present in weights or activations notably influence the performance of quantized models. Recently, \citet{qtransformer} introduced a novel softmax function aimed at pretraining models in an outlier-free manner, thereby enhancing their suitability for quantization. Interestingly, we observed that such an approach leads to performance degradation in full precision. Building on this insight, we enhance the method by ensuring its normalization is invariant to sequence length, a crucial factor for bridging the gap between pretraining and fine-tuning. Moreover, this improved method also facilitates successful pretraining of causal language models.
&lt;/p&gt;</description></item><item><title>Groot&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#22522;&#20110;&#26641;&#30340;&#35821;&#20041;&#36716;&#25442;&#36827;&#34892;&#23545;&#25239;&#27979;&#35797;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;93.66%&#12290;</title><link>https://arxiv.org/abs/2402.12100</link><description>&lt;p&gt;
Groot: &#20351;&#29992;&#22522;&#20110;&#26641;&#30340;&#35821;&#20041;&#36716;&#25442;&#23545;&#29983;&#25104;&#24335;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Groot: Adversarial Testing for Generative Text-to-Image Models with Tree-based Semantic Transformation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12100
&lt;/p&gt;
&lt;p&gt;
Groot&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#22522;&#20110;&#26641;&#30340;&#35821;&#20041;&#36716;&#25442;&#36827;&#34892;&#23545;&#25239;&#27979;&#35797;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;93.66%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#26222;&#21450;&#65292;&#20854;&#23433;&#20840;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#25239;&#24615;&#27979;&#35797;&#25216;&#26415;&#24050;&#34987;&#24320;&#21457;&#29992;&#20110;&#25506;&#27979;&#36825;&#31867;&#27169;&#22411;&#26159;&#21542;&#33021;&#34987;&#28608;&#21457;&#20135;&#29983;&#19981;&#23433;&#20840;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#38754;&#20020;&#30528;&#20302;&#25104;&#21151;&#29575;&#21644;&#20302;&#25928;&#29575;&#31561;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; Groot&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#22522;&#20110;&#26641;&#30340;&#35821;&#20041;&#36716;&#25442;&#36827;&#34892;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23545;&#25239;&#27979;&#35797;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#12290;Groot &#32467;&#21512;&#35821;&#20041;&#20998;&#35299;&#21644;&#25935;&#24863;&#20803;&#32032;&#28153;&#27809;&#31574;&#30053;&#65292;&#32467;&#21512; LLMs &#26469;&#31995;&#32479;&#22320;&#20248;&#21270;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35780;&#20272;&#39564;&#35777;&#20102; Groot &#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#19981;&#20165;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#22312;&#39046;&#20808;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#22914; DALL-E 3 &#21644; Midjourney &#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#29575;&#65288;93.66%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12100v1 Announce Type: cross  Abstract: With the prevalence of text-to-image generative models, their safety becomes a critical concern. adversarial testing techniques have been developed to probe whether such models can be prompted to produce Not-Safe-For-Work (NSFW) content. However, existing solutions face several challenges, including low success rate and inefficiency. We introduce Groot, the first automated framework leveraging tree-based semantic transformation for adversarial testing of text-to-image models. Groot employs semantic decomposition and sensitive element drowning strategies in conjunction with LLMs to systematically refine adversarial prompts. Our comprehensive evaluation confirms the efficacy of Groot, which not only exceeds the performance of current state-of-the-art approaches but also achieves a remarkable success rate (93.66%) on leading text-to-image models such as DALL-E 3 and Midjourney.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#24182;&#19981;&#30495;&#27491;&#29702;&#35299;&#36923;&#36753;&#35268;&#21017;&#65292;&#32780;&#26159;&#36890;&#36807;&#35821;&#22659;&#23398;&#20064;&#22686;&#24378;&#20102;&#27169;&#22411;&#21040;&#36798;&#32467;&#35770;&#30340;&#21487;&#33021;&#24615;</title><link>https://arxiv.org/abs/2402.12091</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30495;&#27491;&#29702;&#35299;&#36923;&#36753;&#36824;&#26159;&#20165;&#20165;&#27169;&#20223;&#35821;&#22659;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models Understand Logic or Just Mimick Context?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12091
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#24182;&#19981;&#30495;&#27491;&#29702;&#35299;&#36923;&#36753;&#35268;&#21017;&#65292;&#32780;&#26159;&#36890;&#36807;&#35821;&#22659;&#23398;&#20064;&#22686;&#24378;&#20102;&#27169;&#22411;&#21040;&#36798;&#32467;&#35770;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23427;&#20204;&#22312;&#22797;&#26434;&#22330;&#26223;&#65288;&#22914;&#36923;&#36753;&#25512;&#29702;&#21644;&#31526;&#21495;&#25512;&#29702;&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#26159;&#22312;&#35821;&#22659;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#35821;&#22659;&#25512;&#29702;&#30340;&#36825;&#31181;&#27169;&#22411;&#25104;&#21151;&#32972;&#21518;&#30340;&#21407;&#22240;&#23578;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#12290;LLMs&#26159;&#21542;&#20102;&#35299;&#36923;&#36753;&#35268;&#21017;&#20197;&#36827;&#34892;&#25512;&#29702;&#65292;&#36824;&#26159;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#27010;&#29575;&#26144;&#23556;&#36890;&#36807;&#35821;&#22659;&#8220;&#29468;&#27979;&#8221;&#31572;&#26696;&#65311;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#21453;&#20107;&#23454;&#26041;&#27861;&#22312;&#20004;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#30740;&#31350;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#26367;&#25442;&#35821;&#22659;&#25991;&#26412;&#24182;&#20462;&#25913;&#36923;&#36753;&#27010;&#24565;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;LLMs&#24182;&#19981;&#30495;&#27491;&#29702;&#35299;&#36923;&#36753;&#35268;&#21017;&#65307;&#30456;&#21453;&#65292;&#22312;&#35821;&#22659;&#23398;&#20064;&#31616;&#21333;&#22686;&#24378;&#20102;&#36825;&#20123;&#27169;&#22411;&#21040;&#36798;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12091v1 Announce Type: cross  Abstract: Over the past few years, the abilities of large language models (LLMs) have received extensive attention, which have performed exceptionally well in complicated scenarios such as logical reasoning and symbolic inference. A significant factor contributing to this progress is the benefit of in-context learning and few-shot prompting. However, the reasons behind the success of such models using contextual reasoning have not been fully explored. Do LLMs have understand logical rules to draw inferences, or do they ``guess'' the answers by learning a type of probabilistic mapping through context? This paper investigates the reasoning capabilities of LLMs on two logical reasoning datasets by using counterfactual methods to replace context text and modify logical concepts. Based on our analysis, it is found that LLMs do not truly understand logical rules; rather, in-context learning has simply enhanced the likelihood of these models arriving a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#8220;&#24402;&#32435;&#23398;&#20064;&#8221;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#30340;SLM&#32593;&#32476;&#26469;&#25552;&#21319;SLM&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#26725;&#26753;&#20154;&#31867;&#19982;LLM&#20043;&#38388;&#30340;&#36923;&#36753;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.12080</link><description>&lt;p&gt;
LLM&#33021;&#21542;&#29992;&#29702;&#30001;&#36827;&#34892;&#35745;&#31639;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Compute with Reasons?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12080
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#8220;&#24402;&#32435;&#23398;&#20064;&#8221;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#30340;SLM&#32593;&#32476;&#26469;&#25552;&#21319;SLM&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#26725;&#26753;&#20154;&#31867;&#19982;LLM&#20043;&#38388;&#30340;&#36923;&#36753;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#22312;&#22788;&#29702;&#22797;&#26434;&#25968;&#23398;&#20219;&#21153;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#23481;&#26131;&#22240;&#20381;&#36182;&#32479;&#35745;&#27169;&#24335;&#32780;&#8220;&#20135;&#29983;&#24187;&#35273;&#8221;&#65292;&#32473;&#20986;&#38169;&#35823;&#31572;&#26696;&#12290;&#36825;&#19968;&#23616;&#38480;&#22312;&#24179;&#22343;&#19978;&#19979;&#25991;&#21644;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#20013;&#36827;&#19968;&#27493;&#25918;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#24402;&#32435;&#23398;&#20064;&#8221;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#24067;&#24335;&#30340;SLM&#32593;&#32476;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#22522;&#20110;&#38169;&#35823;&#30340;&#23398;&#20064;&#21644;&#25552;&#31034;&#34701;&#21512;&#26469;&#25552;&#21319;SLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20010;&#26694;&#26550;&#65292;&#36171;&#20104;SLM&#25509;&#36817;&#39640;&#21442;&#25968;&#27169;&#22411;&#25152;&#23454;&#29616;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#24212;&#29992;&#27700;&#24179;&#30340;&#33021;&#21147;&#65292;&#28508;&#22312;&#22320;&#20351;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#21463;&#30410;&#12290;&#26368;&#32456;&#65292;&#36825;&#19968;&#26032;&#39062;&#27010;&#24565;&#20026;&#36328;&#21508;&#39046;&#22495;&#20013;&#20154;&#31867;&#19982;LLM&#20043;&#38388;&#36923;&#36753;&#24046;&#36317;&#30340;&#24357;&#21512;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12080v1 Announce Type: new  Abstract: Large language models (LLMs) often struggle with complex mathematical tasks, prone to "hallucinating" incorrect answers due to their reliance on statistical patterns. This limitation is further amplified in average Small LangSLMs with limited context and training data. To address this challenge, we propose an "Inductive Learning" approach utilizing a distributed network of SLMs. This network leverages error-based learning and hint incorporation to refine the reasoning capabilities of SLMs. Our goal is to provide a framework that empowers SLMs to approach the level of logic-based applications achieved by high-parameter models, potentially benefiting any language model. Ultimately, this novel concept paves the way for bridging the logical gap between humans and LLMs across various fields.
&lt;/p&gt;</description></item><item><title>LVChat &#25552;&#20986;&#20102; Frame-Scalable &#32534;&#30721;&#21644; Interleaved Frame Encoding&#65288;IFE&#65289;&#26469;&#35299;&#20915;&#38271;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12079</link><description>&lt;p&gt;
LVCHAT: &#20419;&#36827;&#38271;&#35270;&#39057;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
LVCHAT: Facilitating Long Video Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12079
&lt;/p&gt;
&lt;p&gt;
LVChat &#25552;&#20986;&#20102; Frame-Scalable &#32534;&#30721;&#21644; Interleaved Frame Encoding&#65288;IFE&#65289;&#26469;&#35299;&#20915;&#38271;&#35270;&#39057;&#29702;&#35299;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21551;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35835;&#21462;&#35270;&#39057;&#23545;&#20110;&#22810;&#27169;&#24335;LLMs&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#20316;&#21697;&#23637;&#31034;&#20102;&#23545;&#30701;&#35270;&#39057;&#30340;&#24076;&#26395;&#65292;&#32780;&#38271;&#35270;&#39057;&#65288;&#36229;&#36807;1&#20998;&#38047;&#65289;&#30340;&#29702;&#35299;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#35270;&#39057;&#30340;&#36807;&#24230;&#21387;&#32553;&#65292;&#21363;&#32534;&#30721;&#35270;&#39057;&#34920;&#31034;&#19981;&#36275;&#20197;&#20195;&#34920;&#25972;&#20010;&#35270;&#39057;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Long Video Chat&#65288;LVChat&#65289;&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;Frame-Scalable&#32534;&#30721;&#65288;FSE&#65289;&#65292;&#20197;&#21160;&#24577;&#35843;&#25972;&#23884;&#20837;&#25968;&#37327;&#65292;&#19982;&#35270;&#39057;&#25345;&#32493;&#26102;&#38388;&#23545;&#40784;&#65292;&#30830;&#20445;&#38271;&#35270;&#39057;&#19981;&#20250;&#34987;&#36807;&#24230;&#21387;&#32553;&#20026;&#23569;&#25968;&#23884;&#20837;&#12290;&#20026;&#20102;&#22788;&#29702;&#38271;&#24230;&#36229;&#20986;&#35757;&#32451;&#36807;&#31243;&#20013;&#25152;&#35265;&#35270;&#39057;&#30340;&#38271;&#35270;&#39057;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Interleaved Frame Encoding&#65288;IFE&#65289;&#65292;&#37325;&#22797;&#20301;&#32622;&#23884;&#20837;&#21644;&#20132;&#38169;&#22810;&#32452;&#35270;&#39057;&#65292;&#20197;&#23454;&#29616;&#38271;&#35270;&#39057;&#36755;&#20837;&#65292;&#36991;&#20813;&#22240;&#35270;&#39057;&#36807;&#38271;&#32780;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;LVChat sig
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12079v1 Announce Type: cross  Abstract: Enabling large language models (LLMs) to read videos is vital for multimodal LLMs. Existing works show promise on short videos whereas long video (longer than e.g.~1 minute) comprehension remains challenging. The major problem lies in the over-compression of videos, i.e., the encoded video representations are not enough to represent the whole video. To address this issue, we propose Long Video Chat (LVChat), where Frame-Scalable Encoding (FSE) is introduced to dynamically adjust the number of embeddings in alignment with the duration of the video to ensure long videos are not overly compressed into a few embeddings. To deal with long videos whose length is beyond videos seen during training, we propose Interleaved Frame Encoding (IFE), repeating positional embedding and interleaving multiple groups of videos to enable long video input, avoiding performance degradation due to overly long videos. Experimental results show that LVChat sig
&lt;/p&gt;</description></item><item><title>EmoBench&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#29702;&#35770;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#26234;&#33021;&#65292;&#21253;&#25324;&#24773;&#24863;&#29702;&#35299;&#21644;&#24773;&#24863;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.12071</link><description>&lt;p&gt;
EmoBench: &#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
EmoBench: Evaluating the Emotional Intelligence of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12071
&lt;/p&gt;
&lt;p&gt;
EmoBench&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#29702;&#35770;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#26234;&#33021;&#65292;&#21253;&#25324;&#24773;&#24863;&#29702;&#35299;&#21644;&#24773;&#24863;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20984;&#26174;&#20102;&#38656;&#35201;&#31283;&#20581;&#12289;&#20840;&#38754;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#23427;&#20204;&#30340;&#24773;&#24863;&#26234;&#33021;&#65288;EI&#65289;&#36827;&#34892;&#35780;&#20272;&#30340;&#30740;&#31350;&#30456;&#24403;&#26377;&#38480;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#39318;&#20808;&#65292;&#23427;&#20204;&#20027;&#35201;&#20851;&#27880;&#24773;&#24863;&#35782;&#21035;&#65292;&#24573;&#35270;&#20102;&#24773;&#24863;&#35843;&#33410;&#31561;&#37325;&#35201;&#30340;&#24773;&#24863;&#26234;&#33021;&#33021;&#21147;&#65292;&#32780;&#24773;&#24863;&#29702;&#35299;&#21017;&#20419;&#36827;&#24773;&#24863;; &#20854;&#27425;&#65292;&#23427;&#20204;&#20027;&#35201;&#22522;&#20110;&#29616;&#26377;&#25968;&#25454;&#38598;&#26500;&#24314;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;&#39057;&#32321;&#27169;&#24335;&#12289;&#26126;&#30830;&#20449;&#24687;&#21644;&#27880;&#37322;&#38169;&#35823;&#65292;&#23548;&#33268;&#35780;&#20272;&#19981;&#21487;&#38752;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EmoBench&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#20511;&#37492;&#20102;&#24050;&#24314;&#31435;&#30340;&#24515;&#29702;&#29702;&#35770;&#65292;&#24182;&#20026;&#26426;&#22120;EI&#25552;&#20986;&#20102;&#32508;&#21512;&#23450;&#20041;&#65292;&#21253;&#25324;&#24773;&#24863;&#29702;&#35299;&#21644;&#24773;&#24863;&#24212;&#29992;&#12290;EmoBench&#21253;&#25324;&#19968;&#32452;400&#20010;&#29992;&#33521;&#35821;&#21644;&#20013;&#25991;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#65292;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#38656;&#35201;&#28145;&#20837;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12071v1 Announce Type: cross  Abstract: Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion regulation and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;WKVQuant&#65292;&#19968;&#31181;&#19987;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#26435;&#37325;&#21644;&#38190;&#20540;&#32531;&#23384;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12065</link><description>&lt;p&gt;
WKVQuant&#65306;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#26435;&#37325;&#21644;&#38190;&#20540;&#32531;&#23384;&#20197;&#25552;&#39640;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12065
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;WKVQuant&#65292;&#19968;&#31181;&#19987;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#26435;&#37325;&#21644;&#38190;&#20540;&#32531;&#23384;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38754;&#20020;&#30528;&#37096;&#32626;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#20869;&#23384;&#38656;&#27714;&#21644;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#30340;&#35745;&#31639;&#38656;&#27714;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#27880;LLMs&#30340;&#37327;&#21270;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#37327;&#21270;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#27169;&#22411;&#21442;&#25968;&#21644;&#28608;&#27963;&#36716;&#25442;&#20026;&#20302;&#27604;&#29305;&#25972;&#25968;&#26469;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#37327;&#21270;&#26041;&#27861;&#65292;&#35782;&#21035;&#20986;&#23427;&#20204;&#22312;&#24179;&#34913;&#37327;&#21270;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#36229;&#36234;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WKVQuant&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#37327;&#21270;LLMs&#30340;&#21442;&#25968;&#26435;&#37325;&#21644;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#32780;&#35774;&#35745;&#30340;PTQ&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20165;&#32771;&#34385;&#36807;&#21435;&#30340;&#37327;&#21270;&#20197;&#25913;&#21892;&#27880;&#24847;&#21147;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#20108;&#32500;&#37327;&#21270;&#31574;&#30053;&#26469;&#22788;&#29702;KV&#32531;&#23384;&#30340;&#20998;&#24067;&#65292;&#20197;&#21450;&#19968;&#31181;&#36328;&#22359;&#37325;&#24314;&#27491;&#21017;&#21270;&#26041;&#27861;&#20197;&#24110;&#21161;&#27169;&#22411;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12065v1 Announce Type: cross  Abstract: Large Language Models (LLMs) face significant deployment challenges due to their substantial memory requirements and the computational demands of auto-regressive text generation process. This paper addresses these challenges by focusing on the quantization of LLMs, a technique that reduces memory consumption by converting model parameters and activations into low-bit integers. We critically analyze the existing quantization approaches, identifying their limitations in balancing the accuracy and efficiency of the quantized LLMs. To advance beyond these limitations, we propose WKVQuant, a PTQ framework especially designed for quantizing weights and the key/value (KV) cache of LLMs. Specifically, we incorporates past-only quantization to improve the computation of attention. Additionally, we introduce two-dimensional quantization strategy to handle the distribution of KV cache, along with a cross-block reconstruction regularization for pa
&lt;/p&gt;</description></item><item><title>LONDI&#26694;&#26550;&#21487;&#20197;&#22312;&#38656;&#35201;&#22797;&#26434;&#20915;&#31574;&#21644;&#25512;&#29702;&#30340;&#22320;&#26041;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#36164;&#28304;&#28040;&#32791;&#12290;</title><link>https://arxiv.org/abs/2402.12061</link><description>&lt;p&gt;
&#25152;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23567;&#37117;&#19968;&#26679;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
All Language Models Large and Small
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12061
&lt;/p&gt;
&lt;p&gt;
LONDI&#26694;&#26550;&#21487;&#20197;&#22312;&#38656;&#35201;&#22797;&#26434;&#20915;&#31574;&#21644;&#25512;&#29702;&#30340;&#22320;&#26041;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#36164;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39046;&#20808;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#35757;&#32451;&#21644;&#25191;&#34892;&#36807;&#31243;&#20013;&#20351;&#29992;&#39640;&#24378;&#24230;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#23545;&#20110;&#38477;&#20302;&#37096;&#32626;&#36164;&#28304;&#25104;&#26412;&#21644;&#26356;&#24555;&#25191;&#34892;&#20915;&#31574;&#20219;&#21153;&#31561;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#35328;&#20248;&#21270;&#32593;&#32476;&#20998;&#24067;&#65288;LONDI&#65289;&#26694;&#26550;&#30340;&#26032;&#22411;&#21363;&#25554;&#21363;&#29992;LM&#26694;&#26550;&#12290; LONDI&#23398;&#20250;&#20102;&#22312;&#38656;&#35201;&#36827;&#34892;&#22797;&#26434;&#20915;&#31574;&#21644;&#25512;&#29702;&#30340;&#22320;&#26041;&#36873;&#25321;&#24615;&#22320;&#20351;&#29992;&#22823;&#30340;LM&#65292;&#32780;&#22312;&#20854;&#20182;&#22320;&#26041;&#20351;&#29992;&#20302;&#36164;&#28304;&#30340;LM&#12290; LONDI&#30001;&#20004;&#20010;&#65288;&#31163;&#32447;&#65289;&#31574;&#30053;&#32593;&#32476;&#31995;&#32479;&#12289;&#19968;&#20010;LM&#12289;&#19968;&#20010;&#22823;&#30340;LM&#65288;LLM)&#21644;&#19968;&#20010;&#20351;&#29992;&#24320;&#20851;&#25511;&#21046;&#24555;&#36895;&#23398;&#20064;&#20309;&#26102;&#35843;&#29992;LLM&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22359;&#32452;&#25104;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;LLM&#35843;&#29992;&#21644;&#36164;&#28304;&#20351;&#29992;&#26041;&#38754;&#20445;&#25345;&#39044;&#31639;&#32422;&#26463;&#30340;LONDI&#21464;&#20307;&#12290; &#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LONDI&#23398;&#20064;&#28608;&#27963;&#25152;&#38656;&#35299;&#20915;&#20219;&#21153;&#30340;LLM&#30340;&#31995;&#32479;&#29366;&#24577;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12061v1 Announce Type: cross  Abstract: Many leading language models (LMs) use high-intensity computational resources both during training and execution. This poses the challenge of lowering resource costs for deployment and faster execution of decision-making tasks among others. We introduce a novel plug-and-play LM framework named Language Optimising Network Distribution (LONDI) framework. LONDI learns to selectively employ large LMs only where complex decision-making and reasoning are required while using low-resource LMs everywhere else. LONDI consists of a system of two (off-)policy networks, an LM, a large LM (LLM), and a reinforcement learning module that uses switching controls to quickly learn which system states to call the LLM. We then introduce a variant of LONDI that maintains budget constraints on LLM calls and hence its resource usage. Theoretically, we prove LONDI learns the subset of system states to activate the LLM required to solve the task. We then prove
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#25645;&#24314;&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;&#22270;&#20687;&#20013;&#21472;&#21152;&#28857;&#38453;&#20316;&#20026;&#35270;&#35273;&#20449;&#24687;&#38170;&#28857;&#65292;&#24182;&#21033;&#29992;&#22810;&#32500;&#22352;&#26631;&#20316;&#20026;&#25991;&#26412;&#20301;&#32622;&#21442;&#32771;&#65292;&#20174;&#32780;&#20419;&#36827;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#35270;&#35273;-&#35821;&#35328;&#21327;&#35843;&#12290;</title><link>https://arxiv.org/abs/2402.12058</link><description>&lt;p&gt;
&#36890;&#36807;&#25645;&#24314;&#22352;&#26631;&#26469;&#20419;&#36827;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#35270;&#35273;-&#35821;&#35328;&#21327;&#35843;
&lt;/p&gt;
&lt;p&gt;
Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12058
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#25645;&#24314;&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;&#22270;&#20687;&#20013;&#21472;&#21152;&#28857;&#38453;&#20316;&#20026;&#35270;&#35273;&#20449;&#24687;&#38170;&#28857;&#65292;&#24182;&#21033;&#29992;&#22810;&#32500;&#22352;&#26631;&#20316;&#20026;&#25991;&#26412;&#20301;&#32622;&#21442;&#32771;&#65292;&#20174;&#32780;&#20419;&#36827;&#22823;&#22411;&#22810;&#27169;&#22411;&#30340;&#35270;&#35273;-&#35821;&#35328;&#21327;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#22411;(LMMs)&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#26497;&#39640;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#23427;&#20204;&#21151;&#33021;&#20808;&#36827;&#65292;&#20294;&#22312;&#38656;&#35201;&#22797;&#26434;&#25512;&#29702;&#21644;&#22810;&#20010;&#23618;&#27425;&#30340;&#35270;&#35273;&#20449;&#24687;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#65292;LMMs&#30340;&#24615;&#33021;&#20173;&#28982;&#21463;&#38480;&#12290;&#29616;&#26377;&#30340;LMMs&#25552;&#31034;&#25216;&#26415;&#35201;&#20040;&#19987;&#27880;&#20110;&#25913;&#36827;&#25991;&#26412;&#25512;&#29702;&#65292;&#35201;&#20040;&#21033;&#29992;&#22270;&#20687;&#39044;&#22788;&#29702;&#24037;&#20855;&#65292;&#32570;&#20047;&#19968;&#31181;&#31616;&#21333;&#19988;&#36890;&#29992;&#30340;&#35270;&#35273;&#25552;&#31034;&#26041;&#26696;&#65292;&#20197;&#20419;&#36827;LMMs&#20013;&#30340;&#35270;&#35273;-&#35821;&#35328;&#21327;&#35843;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25645;&#24314;&#25552;&#31034;&#65292;&#36890;&#36807;&#25645;&#24314;&#22352;&#26631;&#26469;&#20419;&#36827;&#35270;&#35273;-&#35821;&#35328;&#21327;&#35843;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25645;&#24314;&#25552;&#31034;&#22312;&#22270;&#20687;&#20013;&#21472;&#21152;&#19968;&#20010;&#28857;&#38453;&#20316;&#20026;&#35270;&#35273;&#20449;&#24687;&#38170;&#28857;&#65292;&#24182;&#21033;&#29992;&#22810;&#32500;&#22352;&#26631;&#20316;&#20026;&#25991;&#26412;&#20301;&#32622;&#21442;&#32771;&#12290;&#23545;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25645;&#24314;&#25552;&#31034;&#30456;&#23545;&#20110;GPT-4V&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12058v1 Announce Type: cross  Abstract: State-of-the-art Large Multi-Modal Models (LMMs) have demonstrated exceptional capabilities in vision-language tasks. Despite their advanced functionalities, the performances of LMMs are still limited in challenging scenarios that require complex reasoning with multiple levels of visual information. Existing prompting techniques for LMMs focus on either improving textual reasoning or leveraging tools for image preprocessing, lacking a simple and general visual prompting scheme to promote vision-language coordination in LMMs. In this work, we propose Scaffold prompting that scaffolds coordinates to promote vision-language coordination. Specifically, Scaffold overlays a dot matrix within the image as visual information anchors and leverages multi-dimensional coordinates as textual positional references. Extensive experiments on a wide range of challenging vision-language tasks demonstrate the superiority of Scaffold over GPT-4V with the 
&lt;/p&gt;</description></item><item><title>LLMs&#22312;NLG&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23384;&#22312;&#28151;&#28102;&#19981;&#21516;&#35780;&#20272;&#26631;&#20934;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#20998;&#31867;&#31995;&#32479;&#21644;&#38024;&#23545;&#19981;&#21516;LLMs&#35780;&#20272;&#34892;&#20026;&#30340;&#25200;&#21160;&#25915;&#20987;&#65292;&#25581;&#31034;&#20102;LLMs&#22266;&#26377;&#30340;&#28151;&#28102;&#38382;&#39064;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.12055</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#26159;&#21542;&#28151;&#28102;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#36136;&#37327;&#26631;&#20934;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are LLM-based Evaluators Confusing NLG Quality Criteria?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12055
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;NLG&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23384;&#22312;&#28151;&#28102;&#19981;&#21516;&#35780;&#20272;&#26631;&#20934;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#20998;&#31867;&#31995;&#32479;&#21644;&#38024;&#23545;&#19981;&#21516;LLMs&#35780;&#20272;&#34892;&#20026;&#30340;&#25200;&#21160;&#25915;&#20987;&#65292;&#25581;&#31034;&#20102;LLMs&#22266;&#26377;&#30340;&#28151;&#28102;&#38382;&#39064;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#22312;&#19981;&#21516;&#20219;&#21153;&#30340;NLG&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#20284;&#20046;&#28151;&#28102;&#20102;&#19981;&#21516;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#39564;&#35777;&#65292;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#36991;&#20813;&#29616;&#26377;NLG&#36136;&#37327;&#26631;&#20934;&#20013;&#19981;&#19968;&#33268;&#27010;&#24565;&#21270;&#21644;&#27169;&#31946;&#34920;&#36798;&#30340;&#38382;&#39064;&#26412;&#36523;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19968;&#20010;&#28165;&#26224;&#30340;&#23618;&#27425;&#20998;&#31867;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;&#20808;&#21069;&#30740;&#31350;&#30340;11&#20010;&#24120;&#35265;&#26041;&#38754;&#30340;&#30456;&#24212;&#19981;&#21516;&#26631;&#20934;&#12290;&#21463;&#34892;&#20026;&#27979;&#35797;&#21551;&#21457;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;18&#31181;&#38024;&#23545;&#19981;&#21516;LLMs&#35780;&#20272;&#34892;&#20026;&#30340;&#26041;&#38754;&#23450;&#21521;&#25200;&#21160;&#25915;&#20987;&#65292;&#20197;&#36827;&#34892;&#32454;&#31890;&#24230;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#36229;&#20986;&#20998;&#31867;&#31995;&#32479;&#25351;&#23548;&#33539;&#22260;&#30340;&#20154;&#31867;&#27880;&#37322;&#65292;&#20197;&#39564;&#35777;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#22266;&#26377;&#30340;&#28151;&#28102;&#38382;&#39064;&#65292;&#20197;&#21450;&#20854;&#20182;&#20540;&#24471;&#20851;&#27880;&#30340;&#29616;&#35937;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12055v1 Announce Type: new  Abstract: Some prior work has shown that LLMs perform well in NLG evaluation for different tasks. However, we discover that LLMs seem to confuse different evaluation criteria, which reduces their reliability. For further verification, we first consider avoiding issues of inconsistent conceptualization and vague expression in existing NLG quality criteria themselves. So we summarize a clear hierarchical classification system for 11 common aspects with corresponding different criteria from previous studies involved. Inspired by behavioral testing, we elaborately design 18 types of aspect-targeted perturbation attacks for fine-grained analysis of the evaluation behaviors of different LLMs. We also conduct human annotations beyond the guidance of the classification system to validate the impact of the perturbations. Our experimental results reveal confusion issues inherent in LLMs, as well as other noteworthy phenomena, and necessitate further researc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#20316;&#26041;&#27861;SlimPLM&#65292;&#36890;&#36807;&#31934;&#31616;&#20195;&#29702;&#27169;&#22411;&#26816;&#27979;LLMs&#20013;&#32570;&#22833;&#30340;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#30693;&#35782;&#33719;&#21462;&#36807;&#31243;</title><link>https://arxiv.org/abs/2402.12052</link><description>&lt;p&gt;
&#23567;&#27169;&#22411;&#65292;&#22823;&#35265;&#35299;&#65306;&#21033;&#29992;&#31934;&#31616;&#20195;&#29702;&#27169;&#22411;&#30830;&#23450;LLMs&#20309;&#26102;&#20197;&#21450;&#20026;&#20309;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#20316;&#26041;&#27861;SlimPLM&#65292;&#36890;&#36807;&#31934;&#31616;&#20195;&#29702;&#27169;&#22411;&#26816;&#27979;LLMs&#20013;&#32570;&#22833;&#30340;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#30693;&#35782;&#33719;&#21462;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12052v1 &#20844;&#21578;&#31867;&#22411;:&#26032;&#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#25628;&#32034;&#24341;&#25806;&#30340;&#25972;&#21512;&#20195;&#34920;&#20102;&#30693;&#35782;&#33719;&#21462;&#26041;&#27861;&#30340;&#37325;&#35201;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;LLM&#24050;&#32463;&#20855;&#22791;&#30340;&#30693;&#35782;&#21644;&#38656;&#35201;&#25628;&#32034;&#24341;&#25806;&#24110;&#21161;&#30340;&#30693;&#35782;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;LLM&#26412;&#36523;&#39044;&#22788;&#29702;&#31572;&#26696;&#25110;&#25512;&#29702;&#30340;&#32467;&#26524;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#36825;&#24102;&#26469;&#20102;&#36807;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#20316;&#26041;&#27861;&#65292;&#21363;SlimPLM&#65292;&#36890;&#36807;&#31934;&#31616;&#20195;&#29702;&#27169;&#22411;&#26816;&#27979;LLMs&#20013;&#32570;&#22833;&#30340;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#30693;&#35782;&#33719;&#21462;&#36807;&#31243;&#12290;&#25105;&#20204;&#37319;&#29992;&#21442;&#25968;&#36828;&#36828;&#26356;&#23569;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#31572;&#26696;&#35270;&#20026;&#21551;&#21457;&#24335;&#31572;&#26696;&#12290;&#28982;&#21518;&#21033;&#29992;&#21551;&#21457;&#24335;&#31572;&#26696;&#26469;&#39044;&#27979;&#22238;&#31572;&#29992;&#25143;&#38382;&#39064;&#25152;&#38656;&#30340;&#30693;&#35782;&#65292;&#20197;&#21450;LLM&#20013;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#21482;&#20026;&#26410;&#30693;&#30693;&#35782;&#36827;&#34892;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12052v1 Announce Type: new  Abstract: The integration of large language models (LLMs) and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an LLM already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or reasoning done by the LLM itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM's knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM. We only conduct retrieval for the mis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Model Tailor &#30340;&#21518;&#35757;&#32451;&#35843;&#25972;&#26041;&#27861;&#65292;&#22312;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36890;&#36807;&#20445;&#30041;&#39044;&#35757;&#32451;&#21442;&#25968;&#24182;&#26367;&#25442;&#23569;&#37327;&#24494;&#35843;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#21407;&#22987;&#20219;&#21153;&#32422;99%&#30340;&#25928;&#26524;&#21644;&#23545;&#26032;&#20219;&#21153;&#32422;97%&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12048</link><description>&lt;p&gt;
&#27169;&#22411;&#23450;&#21046;&#65306;&#22312;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Model Tailor &#30340;&#21518;&#35757;&#32451;&#35843;&#25972;&#26041;&#27861;&#65292;&#22312;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#32531;&#35299;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#36890;&#36807;&#20445;&#30041;&#39044;&#35757;&#32451;&#21442;&#25968;&#24182;&#26367;&#25442;&#23569;&#37327;&#24494;&#35843;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#23545;&#21407;&#22987;&#20219;&#21153;&#32422;99%&#30340;&#25928;&#26524;&#21644;&#23545;&#26032;&#20219;&#21153;&#32422;97%&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24494;&#35843;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#26102;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#20986;&#29616;&#20026;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#36825;&#37324;&#20248;&#21270;&#26410;&#30693;&#20219;&#21153;&#30340;&#24615;&#33021;&#24448;&#24448;&#20250;&#23548;&#33268;&#21407;&#22987;&#20219;&#21153;&#30340;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#23545;MLLMs&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Model Tailor&#30340;&#21518;&#35757;&#32451;&#35843;&#25972;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#21442;&#25968;&#65292;&#21516;&#26102;&#26367;&#25442;&#20102;&#23569;&#37327;&#65288;$\leq$ 10\%&#65289;&#24494;&#35843;&#21442;&#25968;&#65292;&#22312;&#21407;&#22987;&#20219;&#21153;&#19978;&#20445;&#25345;&#20102;&#19982;&#39044;&#35757;&#32451;&#26102;&#32422;99\%&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#19978;&#30456;&#27604;&#26631;&#20934;&#24494;&#35843;&#23454;&#29616;&#20102;&#32422;97\%&#30340;&#25928;&#26524;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31232;&#30095;&#25513;&#30721;&#26469;&#35782;&#21035;&#8220;&#27169;&#22411;&#20462;&#34917;&#8221;&#65292;&#22522;&#20110;&#34701;&#21512;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#25972;&#21512;&#20102;&#26174;&#33879;&#24615;&#21644;&#25935;&#24863;&#24615;&#20998;&#26512;&#12290;&#38543;&#21518;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#34917;&#20607;&#26426;&#21046;&#26469;&#8220;&#35013;&#39280;&#20462;&#34917;&#8221;&#65292;&#22686;&#24378;&#27169;&#22411;&#22312;&#30446;&#26631;&#20219;&#21153;&#21644;&#21407;&#22987;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12048v1 Announce Type: new  Abstract: Catastrophic forgetting emerges as a critical challenge when fine-tuning multi-modal large language models (MLLMs), where improving performance on unseen tasks often leads to a significant performance drop on the original tasks. This paper presents a comprehensive analysis of catastrophic forgetting in MLLMs and introduces a post-training adjustment method called Model Tailor. Our method primarily preserves the pre-trained parameters while replacing a small number ($\leq$ 10\%) of fine-tuned parameters, maintaining $\sim$ 99\% effectiveness on original tasks versus pre-training, and achieving $\sim$ 97\% on new tasks compared to standard fine-tuning. Specifically, we derive a sparse mask to identify the "model patch", based on a fusion strategy that integrates salience and sensitivity analysis. Subsequently, a compensation mechanism is introduced to "decorate the patch", enhancing the model's performance on both target and original tasks
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#23398;&#26415;&#39046;&#22495;&#22312;43&#24180;&#38388;&#24341;&#29992;&#36739;&#26087;&#20316;&#21697;&#30340;&#36235;&#21183;&#65292;&#21457;&#29616;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#24341;&#25991;&#24180;&#40836;&#34928;&#36864;&#26368;&#20026;&#26126;&#26174;&#65292;&#27492;&#36235;&#21183;&#24182;&#38750;&#30001;&#20986;&#29256;&#36895;&#29575;&#22686;&#38271;&#20027;&#23548;&#12290;</title><link>https://arxiv.org/abs/2402.12046</link><description>&lt;p&gt;
&#24341;&#25991;&#36951;&#24536;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20854;&#20182;&#23398;&#26415;&#39046;&#22495;&#27491;&#22788;&#20110;&#24341;&#25991;&#24180;&#40836;&#34928;&#36864;&#26399;
&lt;/p&gt;
&lt;p&gt;
Citation Amnesia: NLP and Other Academic Fields Are in a Citation Age Recession
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12046
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#23398;&#26415;&#39046;&#22495;&#22312;43&#24180;&#38388;&#24341;&#29992;&#36739;&#26087;&#20316;&#21697;&#30340;&#36235;&#21183;&#65292;&#21457;&#29616;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#24341;&#25991;&#24180;&#40836;&#34928;&#36864;&#26368;&#20026;&#26126;&#26174;&#65292;&#27492;&#36235;&#21183;&#24182;&#38750;&#30001;&#20986;&#29256;&#36895;&#29575;&#22686;&#38271;&#20027;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#32771;&#23519;&#20102;&#22312;43&#24180;&#30340;&#26102;&#38388;&#36328;&#24230;&#65288;1980-2023&#24180;&#65289;&#20869;&#65292;&#22312;20&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#20542;&#21521;&#20110;&#24341;&#29992;&#36739;&#26087;&#20316;&#21697;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20542;&#21521;&#20110;&#24341;&#29992;&#36739;&#26087;&#20316;&#21697;&#30340;&#29305;&#24615;&#25918;&#22312;&#20854;&#20182;20&#20010;&#39046;&#22495;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#25506;&#35752;NLP&#26159;&#21542;&#23637;&#29616;&#20986;&#19982;&#20854;&#20182;&#39046;&#22495;&#38543;&#26102;&#38388;&#20986;&#29616;&#31867;&#20284;&#30340;&#24341;&#25991;&#27169;&#24335;&#65292;&#25110;&#32773;&#26159;&#21542;&#21487;&#20197;&#35266;&#23519;&#21040;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#22522;&#20110;&#32422;2.4&#20159;&#31687;&#35770;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;&#19968;&#20010;&#26356;&#24191;&#27867;&#30340;&#31185;&#23398;&#36235;&#21183;&#65306;&#35768;&#22810;&#39046;&#22495;&#22312;&#24341;&#29992;&#36739;&#26087;&#20316;&#21697;&#26041;&#38754;&#26126;&#26174;&#19979;&#38477;&#65288;&#20363;&#22914;&#24515;&#29702;&#23398;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#19979;&#38477;&#31216;&#20026;&#8220;&#24341;&#25991;&#24180;&#40836;&#34928;&#36864;&#8221;&#65292;&#31867;&#20284;&#20110;&#32463;&#27982;&#23398;&#23478;&#22914;&#20309;&#23450;&#20041;&#20943;&#23569;&#32463;&#27982;&#27963;&#21160;&#30340;&#26102;&#26399;&#12290;&#36825;&#19968;&#36235;&#21183;&#22312;NLP&#21644;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#26368;&#20026;&#26174;&#33879;&#65288;&#24341;&#25991;&#24180;&#40836;&#20174;&#20808;&#21069;&#39640;&#23792;&#19979;&#38477;&#20102;12.8%&#21644;5.5%&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#26356;&#36817;&#26399;&#20316;&#21697;&#30340;&#24341;&#29992;&#24182;&#38750;&#30452;&#25509;&#21463;&#21040;&#20986;&#29256;&#36895;&#29575;&#22686;&#38271;&#30340;&#25512;&#21160;&#65288;&#36328;&#39046;&#22495;&#19979;&#38477;&#20102;3.4%&#65292;&#20154;&#25991;&#23398;&#31185;&#19979;&#38477;&#20102;5.2%&#65292;&#24418;&#24335;&#31185;&#23398;&#19979;&#38477;&#20102;5.5%&#65289;--&#21363;&#20351;&#22312;&#25511;&#21046;&#20102;&#21457;&#34920;&#25968;&#37327;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12046v1 Announce Type: cross  Abstract: This study examines the tendency to cite older work across 20 fields of study over 43 years (1980--2023). We put NLP's propensity to cite older work in the context of these 20 other fields to analyze whether NLP shows similar temporal citation patterns to these other fields over time or whether differences can be observed. Our analysis, based on a dataset of approximately 240 million papers, reveals a broader scientific trend: many fields have markedly declined in citing older works (e.g., psychology, computer science). We term this decline a 'citation age recession', analogous to how economists define periods of reduced economic activity. The trend is strongest in NLP and ML research (-12.8% and -5.5% in citation age from previous peaks). Our results suggest that citing more recent works is not directly driven by the growth in publication rates (-3.4% across fields; -5.2% in humanities; -5.5% in formal sciences) -- even when controlli
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25490;&#21517;&#20851;&#38190;&#35789;&#24182;&#25351;&#23548;&#23631;&#34109;&#36807;&#31243;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20307;&#35009;&#21644;&#20027;&#39064;&#20449;&#24687;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#19987;&#19994;&#39046;&#22495;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12036</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#20307;&#35009;&#21644;&#20027;&#39064;&#29305;&#24449;&#30340;&#36873;&#25321;&#24615;&#23631;&#34109;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#19987;&#19994;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Language Model Adaptation to Specialized Domains through Selective Masking based on Genre and Topical Characteristics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12036
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25490;&#21517;&#20851;&#38190;&#35789;&#24182;&#25351;&#23548;&#23631;&#34109;&#36807;&#31243;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20307;&#35009;&#21644;&#20027;&#39064;&#20449;&#24687;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#19987;&#19994;&#39046;&#22495;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#24314;&#27169;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#30340;&#35789;&#23631;&#34109;&#26500;&#25104;&#20102;&#20687;BERT&#36825;&#26679;&#30340;&#26550;&#26500;&#20013;&#35821;&#35328;&#24314;&#27169;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#35789;&#23631;&#34109;&#26041;&#27861;&#20381;&#36182;&#20110;&#38543;&#26426;&#36873;&#25321;&#65292;&#21487;&#33021;&#24573;&#35270;&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#35328;&#23646;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#23631;&#34109;&#26041;&#27861;&#65292;&#21033;&#29992;&#20307;&#35009;&#21644;&#20027;&#39064;&#20449;&#24687;&#26469;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#20197;&#36866;&#24212;&#19987;&#19994;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;&#25490;&#21517;&#36807;&#31243;&#65292;&#26681;&#25454;&#21333;&#35789;&#30340;&#37325;&#35201;&#24615;&#23545;&#20854;&#36827;&#34892;&#20248;&#20808;&#32423;&#25490;&#24207;&#65292;&#38543;&#21518;&#24341;&#23548;&#23631;&#34109;&#36807;&#31243;&#12290;&#25105;&#20204;&#36827;&#34892;&#30340;&#23454;&#39564;&#20351;&#29992;&#27861;&#24459;&#39046;&#22495;&#20869;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33521;&#35821;LegalGLUE&#22522;&#20934;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20195;&#30721;&#21487;&#20813;&#36153;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12036v1 Announce Type: new  Abstract: Recent advances in pre-trained language modeling have facilitated significant progress across various natural language processing (NLP) tasks. Word masking during model training constitutes a pivotal component of language modeling in architectures like BERT. However, the prevalent method of word masking relies on random selection, potentially disregarding domain-specific linguistic attributes. In this article, we introduce an innovative masking approach leveraging genre and topicality information to tailor language models to specialized domains. Our method incorporates a ranking process that prioritizes words based on their significance, subsequently guiding the masking procedure. Experiments conducted using continual pre-training within the legal domain have underscored the efficacy of our approach on the LegalGLUE benchmark in the English language. Pre-trained language models and code are freely available for use.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#36890;&#29992;logit&#33976;&#39311; (ULD) &#25439;&#22833;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#26550;&#26500;&#21644;&#20998;&#35789;&#22120;&#27169;&#22411;&#20043;&#38388;&#33976;&#39311;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2402.12030</link><description>&lt;p&gt;
&#36328;&#20998;&#35789;&#22120;&#33976;&#39311;&#65306;&#29992;&#20110;LLM&#30340;&#36890;&#29992;logit&#33976;&#39311;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12030
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#36890;&#29992;logit&#33976;&#39311; (ULD) &#25439;&#22833;&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#26550;&#26500;&#21644;&#20998;&#35789;&#22120;&#27169;&#22411;&#20043;&#38388;&#33976;&#39311;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#20960;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#22312;&#22823;&#22810;&#25968;&#24037;&#19994;&#24212;&#29992;&#20013;&#21487;&#33021;&#24182;&#19981;&#20999;&#23454;&#38469;&#65292;&#21407;&#22240;&#26159;&#35832;&#22914;&#25104;&#26412;&#12289;&#24310;&#36831;&#38480;&#21046;&#21644;&#30828;&#20214;&#21487;&#35775;&#38382;&#24615;&#31561;&#32422;&#26463;&#12290;&#30693;&#35782;&#33976;&#39311; (KD) &#36890;&#36807;&#23558;&#36164;&#28304;&#23494;&#38598;&#22411;&#22823;&#27169;&#22411;&#30340;&#30693;&#35782;&#21387;&#32553;&#21040;&#36739;&#23567;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#35299;&#20915;&#26041;&#26696;&#12290;&#23384;&#22312;&#22810;&#31181;&#31574;&#30053;&#65292;&#19968;&#20123;&#20381;&#36182;&#20110;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#24182;&#21487;&#36873;&#25321;&#24615;&#22320;&#21033;&#29992;&#20854;logits&#26469;&#22686;&#24378;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;logits&#30340;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#35201;&#27714;&#25945;&#24072;&#21644;&#23398;&#29983;&#27169;&#22411;&#20849;&#20139;&#30456;&#21516;&#30340;&#20998;&#35789;&#22120;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;LLM&#31995;&#21015;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#36890;&#29992;logit&#33976;&#39311; (ULD) &#25439;&#22833;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;ULD&#25439;&#22833;&#22312;&#21551;&#29992;&#19981;&#21516;&#26550;&#26500;&#21644;&#20998;&#35789;&#22120;&#30340;&#27169;&#22411;&#20043;&#38388;&#30340;&#33976;&#39311;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12030v1 Announce Type: new  Abstract: Deploying large language models (LLMs) of several billion parameters can be impractical in most industrial use cases due to constraints such as cost, latency limitations, and hardware accessibility. Knowledge distillation (KD) offers a solution by compressing knowledge from resource-intensive large models to smaller ones. Various strategies exist, some relying on the text generated by the teacher model and optionally utilizing his logits to enhance learning. However, these methods based on logits often require both teacher and student models to share the same tokenizer, limiting their applicability across different LLM families. In this paper, we introduce Universal Logit Distillation (ULD) loss, grounded in optimal transport, to address this limitation. Our experimental results demonstrate the effectiveness of ULD loss in enabling distillation across models with different architectures and tokenizers, paving the way to a more widespread
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#39057;&#29575;&#31354;&#38388;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#20302;&#31209;&#36866;&#24212;&#65288;MuScleLoRA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12026</link><description>&lt;p&gt;
&#20174;&#21518;&#38376;&#27602;&#21270;&#25968;&#25454;&#38598;&#20013;&#36890;&#36807;&#38477;&#39057;&#31354;&#38388;&#33719;&#21462;&#28165;&#27905;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12026
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#39057;&#29575;&#31354;&#38388;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#20302;&#31209;&#36866;&#24212;&#65288;MuScleLoRA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;LMs&#30340;&#21487;&#38752;&#24615;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#22312;&#27602;&#21270;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;LMs&#26102;&#20943;&#36731;&#21518;&#38376;&#23398;&#20064;&#65292;&#20294;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#25269;&#24481;&#22797;&#26434;&#30340;&#21518;&#38376;&#25915;&#20987;&#26102;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20613;&#37324;&#21494;&#20998;&#26512;&#30740;&#31350;&#20102;&#39057;&#29575;&#31354;&#38388;&#20013;&#21518;&#38376;LMs&#30340;&#23398;&#20064;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#27602;&#21270;&#25968;&#25454;&#38598;&#19978;&#21576;&#29616;&#30340;&#21518;&#38376;&#26144;&#23556;&#30456;&#27604;&#28165;&#27905;&#26144;&#23556;&#26356;&#20542;&#21521;&#20110;&#36739;&#20302;&#39057;&#29575;&#65292;&#23548;&#33268;&#21518;&#38376;&#26144;&#23556;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#20302;&#31209;&#36866;&#24212;&#65288;MuScleLoRA&#65289;&#65292;&#23427;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#37096;&#32626;&#22810;&#20010;&#24452;&#21521;&#32553;&#25918;&#65292;&#20302;&#31209;&#36866;&#24212;&#30446;&#26631;&#27169;&#22411;&#65292;&#24182;&#22312;&#26356;&#26032;&#21442;&#25968;&#26102;&#36827;&#19968;&#27493;&#35843;&#25972;&#26799;&#24230;&#12290;&#36890;&#36807;&#38477;&#39057;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12026v1 Announce Type: cross  Abstract: Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscal
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#35821;&#38899;&#32763;&#35793;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#20026;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#25552;&#20379;&#20102;&#26032;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#20294;&#30446;&#21069;&#21508;&#31181;&#35780;&#20272;&#26041;&#27861;&#21644;&#35774;&#32622;&#22810;&#26679;&#24615;&#38459;&#30861;&#20102;&#30830;&#23450;&#27599;&#20010;&#26550;&#26500;&#26500;&#24314;&#22359;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.12025</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#38899;&#32763;&#35793;&#65306;&#23384;&#22312;&#21644;&#32570;&#22833;&#30340;&#20869;&#23481;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Speech Translation with Speech Foundation Models and Large Language Models: What is There and What is Missing?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12025
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#35821;&#38899;&#32763;&#35793;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#23558;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#20026;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#25552;&#20379;&#20102;&#26032;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#20294;&#30446;&#21069;&#21508;&#31181;&#35780;&#20272;&#26041;&#27861;&#21644;&#35774;&#32622;&#22810;&#26679;&#24615;&#38459;&#30861;&#20102;&#30830;&#23450;&#27599;&#20010;&#26550;&#26500;&#26500;&#24314;&#22359;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#26368;&#36817;&#21457;&#29983;&#20102;&#19968;&#22330;&#21464;&#38761;&#24615;&#30340;&#36716;&#21464;&#65292;&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#24443;&#24213;&#25913;&#21464;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;NLP&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#36825;&#31181;&#33539;&#24335;&#24050;&#32463;&#25193;&#23637;&#21040;&#20854;&#20182;&#24418;&#24335;&#65292;&#21253;&#25324;&#35821;&#38899;&#65292;&#22312;&#37027;&#37324;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#31215;&#26497;&#25506;&#32034;&#23558;&#35821;&#38899;&#22522;&#30784;&#27169;&#22411;&#65288;SFMs&#65289;&#21644;LLMs&#32467;&#21512;&#25104;&#21333;&#19968;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#26412;&#25991;&#30528;&#37325;&#20110;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#65288;ST&#65289;&#12290;&#36890;&#36807;&#23457;&#26597;&#35813;&#20027;&#39064;&#19978;&#21457;&#34920;&#30340;&#35770;&#25991;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36804;&#20170;&#20026;&#27490;&#25552;&#20986;&#30340;&#26550;&#26500;&#35299;&#20915;&#26041;&#26696;&#21644;&#35757;&#32451;&#31574;&#30053;&#30340;&#32479;&#19968;&#35266;&#28857;&#65292;&#24378;&#35843;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#21644;&#24046;&#24322;&#20043;&#22788;&#12290;&#22522;&#20110;&#36825;&#19968;&#30740;&#31350;&#65292;&#25105;&#20204;&#19981;&#20165;&#25972;&#29702;&#20102;&#25152;&#23398;&#21040;&#30340;&#32463;&#39564;&#25945;&#35757;&#65292;&#36824;&#23637;&#31034;&#20102;&#22810;&#26679;&#21270;&#30340;&#35774;&#32622;&#21644;&#35780;&#20272;&#26041;&#27861;&#22914;&#20309;&#38459;&#30861;&#23545;&#27599;&#20010;&#26550;&#26500;&#26500;&#24314;&#22359;&#30340;&#26368;&#20339;&#24615;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12025v1 Announce Type: new  Abstract: The field of natural language processing (NLP) has recently witnessed a transformative shift with the emergence of foundation models, particularly Large Language Models (LLMs) that have revolutionized text-based NLP. This paradigm has extended to other modalities, including speech, where researchers are actively exploring the combination of Speech Foundation Models (SFMs) and LLMs into single, unified models capable of addressing multimodal tasks. Among such tasks, this paper focuses on speech-to-text translation (ST). By examining the published papers on the topic, we propose a unified view of the architectural solutions and training strategies presented so far, highlighting similarities and differences among them. Based on this examination, we not only organize the lessons learned but also show how diverse settings and evaluation approaches hinder the identification of the best-performing solution for each architectural building block 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#27169;&#22411;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23558;LLMs&#30340;&#33021;&#21147;&#21387;&#32553;&#21040; TAG &#23398;&#20064;&#30340;&#26412;&#22320;&#22270;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#23427;&#20204;&#20043;&#38388;&#30340;&#22266;&#26377;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.12022</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#29992;&#20110;&#25991;&#26412;&#23646;&#24615;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distilling Large Language Models for Text-Attributed Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#27169;&#22411;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#23558;LLMs&#30340;&#33021;&#21147;&#21387;&#32553;&#21040; TAG &#23398;&#20064;&#30340;&#26412;&#22320;&#22270;&#27169;&#22411;&#20013;&#65292;&#35299;&#20915;&#23427;&#20204;&#20043;&#38388;&#30340;&#22266;&#26377;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#26159;&#36830;&#25509;&#30340;&#25991;&#26412;&#25991;&#26723;&#22270;&#12290;&#22270;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#23398;&#20064;TAGs&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#26631;&#31614;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#36825;&#20123;&#26631;&#31614;&#24456;&#23569;&#25110;&#29978;&#33267;&#19981;&#21487;&#29992;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;TAG&#23398;&#20064;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#21487;&#20280;&#32553;&#24615;&#12289;&#25104;&#26412;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36890;&#36807;&#23558;LLMs&#30340;&#33021;&#21147;&#20256;&#25480;&#32473;TAG&#23398;&#20064;&#20013;&#30340;&#26412;&#22320;&#22270;&#27169;&#22411;&#65292;&#20174;&#32780;&#21327;&#21516;LLMs&#21644;&#22270;&#27169;&#22411;&#30340;&#20114;&#34917;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12022v1 Announce Type: new  Abstract: Text-Attributed Graphs (TAGs) are graphs of connected textual documents. Graph models can efficiently learn TAGs, but their training heavily relies on human-annotated labels, which are scarce or even unavailable in many applications. Large language models (LLMs) have recently demonstrated remarkable capabilities in few-shot and zero-shot TAG learning, but they suffer from scalability, cost, and privacy issues. Therefore, in this work, we focus on synergizing LLMs and graph models with their complementary strengths by distilling the power of LLMs to a local graph model on TAG learning. To address the inherent gaps between LLMs (generative models for texts) and graph models (discriminative models for graphs), we propose first to let LLMs teach an interpreter with rich textual rationale and then let a student model mimic the interpreter's reasoning without LLMs' textual rationale. Extensive experiments validate the efficacy of our proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#21644;&#26041;&#27861;&#65292;&#23558;LSC&#38382;&#39064;&#20998;&#35299;&#20026;&#19981;&#21516;&#32423;&#21035;&#30340;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#20843;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;APD&#22312;GCD&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;XL-LEXEME&#22312;WiC&#12289;WSI&#21644;GCD&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#65292;&#24182;&#19988;&#19982;GPT-4&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2402.12011</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#30340;&#31995;&#32479;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#27169;&#22411;&#21644;&#26041;&#27861;&#65292;&#23558;LSC&#38382;&#39064;&#20998;&#35299;&#20026;&#19981;&#21516;&#32423;&#21035;&#30340;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#20843;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;APD&#22312;GCD&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;XL-LEXEME&#22312;WiC&#12289;WSI&#21644;GCD&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#65292;&#24182;&#19988;&#19982;GPT-4&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#23884;&#20837;&#26159;&#24314;&#27169;&#35789;&#27719;&#35821;&#20041;&#21464;&#21270;&#65288;LSC&#65289;&#30340;&#39318;&#36873;&#24037;&#20855;&#12290;&#24403;&#21069;&#30340;&#35780;&#20272;&#36890;&#24120;&#19987;&#27880;&#20110;&#31216;&#20026;&#20998;&#32423;&#21464;&#21270;&#26816;&#27979;&#65288;GCD&#65289;&#30340;&#29305;&#23450;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#20381;&#36182;&#20110;&#19981;&#21516;&#35774;&#32622;&#65292;&#36328;&#20316;&#21697;&#30340;&#24615;&#33021;&#27604;&#36739;&#32463;&#24120;&#20855;&#26377;&#35823;&#23548;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#35780;&#20272;&#20102;GCD&#30340;&#26368;&#26032;&#27169;&#22411;&#21644;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;LSC&#38382;&#39064;&#20998;&#35299;&#20026;&#19978;&#19979;&#25991;&#20013;&#30340;&#21333;&#35789;&#65288;WiC&#65289;&#21644;&#35789;&#20041;&#24402;&#32435;&#65288;WSI&#65289;&#20219;&#21153;&#65292;&#24182;&#27604;&#36739;&#36825;&#20123;&#19981;&#21516;&#32423;&#21035;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#21487;&#29992;&#30340;LSC&#22522;&#20934;&#27979;&#35797;&#20013;&#36328;&#19981;&#21516;&#35821;&#35328;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;APD&#22312;GCD&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65307;&#65288;ii&#65289;XL-LEXEME&#22312;WiC&#12289;WSI&#21644;GCD&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#19978;&#19979;&#25991;&#21270;&#27169;&#22411;&#65292;&#21516;&#26102;&#19982;GPT-4&#30456;&#24403;&#65307;&#65288;iii&#65289;&#26126;&#26174;&#38656;&#35201;&#25913;&#36827;&#23545;&#35789;&#20041;&#24314;&#27169;&#20197;&#21450;&#20851;&#27880;&#36825;&#20123;&#24847;&#20041;&#20309;&#26102;&#12289;&#22914;&#20309;&#21644;&#20026;&#20309;&#21464;&#21270;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12011v1 Announce Type: new  Abstract: Contextualized embeddings are the preferred tool for modeling Lexical Semantic Change (LSC). Current evaluations typically focus on a specific task known as Graded Change Detection (GCD). However, performance comparison across work are often misleading due to their reliance on diverse settings. In this paper, we evaluate state-of-the-art models and approaches for GCD under equal conditions. We further break the LSC problem into Word-in-Context (WiC) and Word Sense Induction (WSI) tasks, and compare models across these different levels. Our evaluation is performed across different languages on eight available benchmarks for LSC, and shows that (i) APD outperforms other approaches for GCD; (ii) XL-LEXEME outperforms other contextualized models for WiC, WSI, and GCD, while being comparable to GPT-4; (iii) there is a clear need for improving the modeling of word meanings, as well as focus on how, when, and why these meanings change, rather t
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#26102;&#38388;&#20449;&#24687;&#21644;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#65292;&#38381;&#28304;&#27169;&#22411;&#21487;&#33021;&#26263;&#31034;&#20102;&#19981;&#30830;&#23450;&#24615;&#35748;&#35782;&#19982;&#38169;&#35823;&#22238;&#24212;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.11997</link><description>&lt;p&gt;
&#22238;&#24518;&#37027;&#19968;&#24180;&#21457;&#29983;&#30340;&#20107;&#20214;&#65311;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#21644;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11997
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#26102;&#38388;&#20449;&#24687;&#21644;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#65292;&#38381;&#28304;&#27169;&#22411;&#21487;&#33021;&#26263;&#31034;&#20102;&#19981;&#30830;&#23450;&#24615;&#35748;&#35782;&#19982;&#38169;&#35823;&#22238;&#24212;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#25512;&#29702;&#21644;&#20445;&#30041;&#26102;&#38388;&#20449;&#24687;&#30340;&#33021;&#21147;&#20173;&#28982;&#26377;&#38480;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29702;&#35299;&#20107;&#20214;&#30340;&#39034;&#24207;&#24615;&#23545;&#20851;&#38190;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25991;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#35268;&#27169;&#26102;&#38388;&#25968;&#25454;&#38598;\textbf{TempUN}&#19978;&#23545;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#26102;&#38388;&#20445;&#30041;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26174;&#33879;&#38480;&#21046;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#38381;&#28304;&#27169;&#22411;&#26356;&#39057;&#32321;&#22320;&#26174;&#31034;&#20986;&#30693;&#35782;&#24046;&#36317;&#65292;&#21487;&#33021;&#26263;&#31034;&#20102;&#19981;&#30830;&#23450;&#24615;&#35748;&#35782;&#21644;&#38169;&#35823;&#22238;&#24212;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#25506;&#32034;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;&#24182;&#27809;&#26377;&#24102;&#26469;&#20027;&#35201;&#24615;&#33021;&#25913;&#36827;&#12290;&#30456;&#20851;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#33719;&#24471;&#65288;https://github.com/lingoiitgn/TempUN&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11997v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are increasingly becoming ubiquitous, yet their ability to reason about and retain temporal information remains limited. This hinders their application in real-world scenarios where understanding the sequential nature of events is crucial. This paper experiments with state-of-the-art models on a novel, large-scale temporal dataset, \textbf{TempUN}, to reveal significant limitations in temporal retention and reasoning abilities. Interestingly, closed-source models indicate knowledge gaps more frequently, potentially suggesting a trade-off between uncertainty awareness and incorrect responses. Further, exploring various fine-tuning approaches yielded no major performance improvements. The associated dataset and code are available at the following URL (https://github.com/lingoiitgn/TempUN).
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;COmpressive Memory-Enhanced Dialogue sYstems&#65288;COMEDY&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#8220;&#19968;&#23545;&#22810;&#8221;&#26041;&#27861;&#21033;&#29992;&#21333;&#19968;&#35821;&#35328;&#27169;&#22411;&#31649;&#29702;&#35760;&#24518;&#29983;&#25104;&#12289;&#21387;&#32553;&#21644;&#21709;&#24212;&#29983;&#25104;&#65292;&#26680;&#24515;&#27010;&#24565;&#26159;&#21387;&#32553;&#35760;&#24518;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#20013;&#25991;&#25351;&#23548;&#35843;&#20248;&#25968;&#25454;&#38598;Dolphin&#65292;&#27604;&#36739;&#35780;&#20272;&#35777;&#26126;&#20102;COMEDY&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11975</link><description>&lt;p&gt;
&#21387;&#32553;&#20197;&#24341;&#20154;&#27880;&#30446;&#65306;&#37322;&#25918;&#21387;&#32553;&#35760;&#24518;&#22312;&#29616;&#23454;&#19990;&#30028;&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Compress to Impress: Unleashing the Potential of Compressive Memory in Real-World Long-Term Conversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11975
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;COmpressive Memory-Enhanced Dialogue sYstems&#65288;COMEDY&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#8220;&#19968;&#23545;&#22810;&#8221;&#26041;&#27861;&#21033;&#29992;&#21333;&#19968;&#35821;&#35328;&#27169;&#22411;&#31649;&#29702;&#35760;&#24518;&#29983;&#25104;&#12289;&#21387;&#32553;&#21644;&#21709;&#24212;&#29983;&#25104;&#65292;&#26680;&#24515;&#27010;&#24565;&#26159;&#21387;&#32553;&#35760;&#24518;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#20013;&#25991;&#25351;&#23548;&#35843;&#20248;&#25968;&#25454;&#38598;Dolphin&#65292;&#27604;&#36739;&#35780;&#20272;&#35777;&#26126;&#20102;COMEDY&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26041;&#27861;&#22312;&#32500;&#25252;&#38271;&#26399;&#23545;&#35805;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#35760;&#24518;&#25968;&#25454;&#24211;&#31649;&#29702;&#21644;&#20934;&#30830;&#30340;&#35760;&#24518;&#26816;&#32034;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#21160;&#24577;&#12289;&#30495;&#23454;&#19990;&#30028;&#20114;&#21160;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;COmpressive Memory-Enhanced Dialogue sYstems&#65288;COMEDY&#65289;&#65292;&#23427;&#25682;&#24323;&#20102;&#20256;&#32479;&#30340;&#26816;&#32034;&#27169;&#22359;&#21644;&#35760;&#24518;&#25968;&#25454;&#24211;&#12290;&#30456;&#21453;&#65292;COMEDY&#37319;&#29992;&#20102;&#8220;&#19968;&#23545;&#22810;&#8221;&#26041;&#27861;&#65292;&#21033;&#29992;&#21333;&#19968;&#35821;&#35328;&#27169;&#22411;&#26469;&#31649;&#29702;&#35760;&#24518;&#29983;&#25104;&#12289;&#21387;&#32553;&#21644;&#21709;&#24212;&#29983;&#25104;&#12290;&#36825;&#19968;&#26694;&#26550;&#30340;&#26680;&#24515;&#27010;&#24565;&#26159;&#21387;&#32553;&#35760;&#24518;&#65292;&#23427;&#23558;&#20250;&#35805;&#29305;&#23450;&#25688;&#35201;&#12289;&#29992;&#25143;-&#26426;&#22120;&#20154;&#21160;&#24577;&#21644;&#36807;&#21435;&#20107;&#20214;&#25972;&#21512;&#21040;&#31616;&#27905;&#30340;&#35760;&#24518;&#26684;&#24335;&#20013;&#12290;&#20026;&#20102;&#25903;&#25345;COMEDY&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20013;&#25991;&#25351;&#23548;&#35843;&#20248;&#25968;&#25454;&#38598;Dolphin&#65292;&#20174;&#30495;&#23454;&#29992;&#25143;-&#32842;&#22825;&#26426;&#22120;&#20154;&#20114;&#21160;&#20013;&#24471;&#20986;&#12290;&#27604;&#36739;&#35780;&#20272;&#34920;&#26126;COMEDY&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11975v1 Announce Type: new  Abstract: Existing retrieval-based methods have made significant strides in maintaining long-term conversations. However, these approaches face challenges in memory database management and accurate memory retrieval, hindering their efficacy in dynamic, real-world interactions. This study introduces a novel framework, COmpressive Memory-Enhanced Dialogue sYstems (COMEDY), which eschews traditional retrieval modules and memory databases. Instead, COMEDY adopts a ''One-for-All'' approach, utilizing a single language model to manage memory generation, compression, and response generation. Central to this framework is the concept of compressive memory, which intergrates session-specific summaries, user-bot dynamics, and past events into a concise memory format. To support COMEDY, we curated a large-scale Chinese instruction-tuning dataset, Dolphin, derived from real user-chatbot interactions. Comparative evaluations demonstrate COMEDY's superiority ove
&lt;/p&gt;</description></item><item><title>&#24503;&#35821;&#26041;&#35328;&#21644;&#21306;&#22495;&#35821;&#35328;&#35828;&#35805;&#32773;&#26356;&#20542;&#21521;&#20110;&#25903;&#25345;&#33021;&#22815;&#22788;&#29702;&#26041;&#35328;&#36755;&#20837;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#65292;&#27604;&#22914;&#34394;&#25311;&#21161;&#25163;&#65292;&#23545;&#20110;&#20135;&#29983;&#26041;&#35328;&#36755;&#20986;&#30340;&#24212;&#29992;&#21017;&#25903;&#25345;&#31243;&#24230;&#30456;&#23545;&#36739;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.11968</link><description>&lt;p&gt;
&#26041;&#35328;&#35828;&#35805;&#32773;&#26377;&#20160;&#20040;&#38656;&#27714;&#65311;&#24503;&#35821;&#26041;&#35328;&#35821;&#35328;&#25216;&#26415;&#24577;&#24230;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
What Do Dialect Speakers Want? A Survey of Attitudes Towards Language Technology for German Dialects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11968
&lt;/p&gt;
&lt;p&gt;
&#24503;&#35821;&#26041;&#35328;&#21644;&#21306;&#22495;&#35821;&#35328;&#35828;&#35805;&#32773;&#26356;&#20542;&#21521;&#20110;&#25903;&#25345;&#33021;&#22815;&#22788;&#29702;&#26041;&#35328;&#36755;&#20837;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#65292;&#27604;&#22914;&#34394;&#25311;&#21161;&#25163;&#65292;&#23545;&#20110;&#20135;&#29983;&#26041;&#35328;&#36755;&#20986;&#30340;&#24212;&#29992;&#21017;&#25903;&#25345;&#31243;&#24230;&#30456;&#23545;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20027;&#35201;&#38598;&#20013;&#20110;&#23545;&#26631;&#20934;&#21270;&#35821;&#35328;&#36827;&#34892;&#24314;&#27169;&#12290;&#26368;&#36817;&#65292;&#20851;&#27880;&#28857;&#36880;&#28176;&#36716;&#21521;&#20102;&#26412;&#22320;&#12289;&#38750;&#26631;&#20934;&#21270;&#35821;&#35328;&#21644;&#26041;&#35328;&#12290;&#28982;&#32780;&#65292;&#19982;NLP&#24037;&#20855;&#30456;&#20851;&#30340;&#35828;&#35805;&#32773;&#32676;&#20307;&#30340;&#38656;&#27714;&#21644;&#24895;&#26395;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#26410;&#30693;&#30340;&#12290;&#26412;&#25991;&#20851;&#27880;&#24503;&#35821;&#26041;&#35328;&#21644;&#21306;&#22495;&#35821;&#35328;&#65292;&#36825;&#26159;&#19968;&#32452;&#22312;&#22768;&#26395;&#21644;&#26631;&#20934;&#21270;&#26041;&#38754;&#24322;&#36136;&#24615;&#30340;&#35821;&#35328;&#21464;&#20307;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#35821;&#35328;&#21464;&#20307;&#30340;&#35828;&#35805;&#32773;&#36827;&#34892;&#20102;&#35843;&#26597;&#65288;N=327&#65289;&#65292;&#24182;&#20171;&#32461;&#20102;&#20182;&#20204;&#23545;&#20110;&#20182;&#20204;&#26041;&#35328;&#30340;&#20551;&#24819;&#35821;&#35328;&#25216;&#26415;&#30340;&#24847;&#35265;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#21463;&#35775;&#32773;&#23376;&#32676;&#20307;&#20043;&#38388;&#30340;&#24577;&#24230;&#26377;&#25152;&#19981;&#21516;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#21463;&#35775;&#32773;&#29305;&#21035;&#25903;&#25345;&#33021;&#22815;&#22788;&#29702;&#26041;&#35328;&#36755;&#20837;&#30340;&#28508;&#22312;NLP&#24037;&#20855;&#65288;&#29305;&#21035;&#26159;&#38899;&#39057;&#36755;&#20837;&#65289;&#65292;&#27604;&#22914;&#34394;&#25311;&#21161;&#25163;&#65292;&#32780;&#23545;&#20110;&#20135;&#29983;&#26041;&#35328;&#36755;&#20986;&#30340;&#24212;&#29992;&#65292;&#27604;&#22914;&#26426;&#22120;&#32763;&#35793;&#25110;&#25340;&#20889;&#26816;&#26597;&#31243;&#24207;&#65292;&#25903;&#25345;&#31243;&#24230;&#35201;&#20302;&#19968;&#20123;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11968v1 Announce Type: new  Abstract: Natural language processing (NLP) has largely focused on modelling standardized languages. More recently, attention has increasingly shifted to local, non-standardized languages and dialects. However, the relevant speaker populations' needs and wishes with respect to NLP tools are largely unknown. In this paper, we focus on dialects and regional languages related to German -- a group of varieties that is heterogeneous in terms of prestige and standardization. We survey speakers of these varieties (N=327) and present their opinions on hypothetical language technologies for their dialects. Although attitudes vary among subgroups of our respondents, we find that respondents are especially in favour of potential NLP tools that work with dialectal input (especially audio input) such as virtual assistants, and less so for applications that produce dialectal output such as machine translation or spellcheckers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DB-LLM&#30340;&#26032;&#39062;&#21452;&#20108;&#20540;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28789;&#27963;&#21452;&#20108;&#20540;&#21270;(FDB)&#26469;&#24179;&#34913;2&#20301;&#23485;&#24230;&#30340;&#31934;&#24230;&#20248;&#21183;&#21644;&#20108;&#20540;&#21270;&#30340;&#25928;&#29575;&#20248;&#21183;&#65292;&#20174;&#32780;&#22312;&#25552;&#39640;LLMs&#30340;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11960</link><description>&lt;p&gt;
DB-LLM: &#39640;&#25928;LLM&#30340;&#20934;&#30830;&#21452;&#20108;&#20540;&#21270;
&lt;/p&gt;
&lt;p&gt;
DB-LLM: Accurate Dual-Binarization for Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DB-LLM&#30340;&#26032;&#39062;&#21452;&#20108;&#20540;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28789;&#27963;&#21452;&#20108;&#20540;&#21270;(FDB)&#26469;&#24179;&#34913;2&#20301;&#23485;&#24230;&#30340;&#31934;&#24230;&#20248;&#21183;&#21644;&#20108;&#20540;&#21270;&#30340;&#25928;&#29575;&#20248;&#21183;&#65292;&#20174;&#32780;&#22312;&#25552;&#39640;LLMs&#30340;&#35745;&#31639;&#25928;&#29575;&#30340;&#21516;&#26102;&#20445;&#25345;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26174;&#33879;&#25512;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#28982;&#32780;&#39640;&#26114;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#37327;&#21270;&#25104;&#20026;&#25913;&#21892;LLMs&#35745;&#31639;&#25928;&#29575;&#30340;&#26368;&#26377;&#25928;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36229;&#20302;&#27604;&#29305;&#37327;&#21270;&#24635;&#26159;&#23548;&#33268;&#20005;&#37325;&#30340;&#31934;&#24230;&#19979;&#38477;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#32531;&#35299;&#20102;&#36229;&#20302;&#27604;&#29305;&#37327;&#21270;&#30340;&#24494;&#35266;&#21644;&#23439;&#35266;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLMs&#21452;&#20108;&#20540;&#21270;&#26041;&#27861;&#65292;&#21363;DB-LLM&#12290;&#23545;&#20110;&#24494;&#35266;&#23618;&#38754;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;2&#20301;&#23485;&#24230;&#30340;&#20934;&#30830;&#24615;&#20248;&#21183;&#21644;&#20108;&#20540;&#21270;&#30340;&#25928;&#29575;&#20248;&#21183;&#65292;&#24341;&#20837;&#20102;&#28789;&#27963;&#21452;&#20108;&#20540;&#21270;(FDB)&#12290;&#36890;&#36807;&#23558;2&#20301;&#37327;&#21270;&#26435;&#37325;&#20998;&#20026;&#20004;&#32452;&#29420;&#31435;&#30340;&#20108;&#36827;&#21046;&#25968;&#38598;&#65292;FDB&#30830;&#20445;&#20102;&#34920;&#31034;&#30340;&#20934;&#30830;&#24615;&#24182;&#24341;&#20837;&#20102;&#28789;&#27963;&#24615;&#65292;&#21033;&#29992;&#20108;&#20540;&#21270;&#30340;&#39640;&#25928;&#20301;&#25805;&#20316;&#21516;&#26102;&#20445;&#30041;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11960v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly advanced the field of natural language processing, while the expensive memory and computation consumption impede their practical deployment. Quantization emerges as one of the most effective methods for improving the computational efficiency of LLMs. However, existing ultra-low-bit quantization always causes severe accuracy drops. In this paper, we empirically relieve the micro and macro characteristics of ultra-low bit quantization and present a novel Dual-Binarization method for LLMs, namely DB-LLM. For the micro-level, we take both the accuracy advantage of 2-bit-width and the efficiency advantage of binarization into account, introducing Flexible Dual Binarization (FDB). By splitting 2-bit quantized weights into two independent sets of binaries, FDB ensures the accuracy of representations and introduces flexibility, utilizing the efficient bitwise operations of binarization while reta
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LLMs&#33258;&#21160;&#35780;&#20272;&#24515;&#29702;&#21672;&#35810;&#23545;&#35805;&#20013;&#30340;&#24037;&#20316;&#32852;&#30431;&#65292;&#32467;&#26524;&#26174;&#31034;&#19982;&#20154;&#24037;&#35780;&#20272;&#39640;&#24230;&#19968;&#33268;&#65292;&#24182;&#25552;&#20379;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.11958</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#33258;&#21160;&#35780;&#20272;&#24515;&#29702;&#20581;&#24247;&#21672;&#35810;
&lt;/p&gt;
&lt;p&gt;
Automatic Evaluation for Mental Health Counseling using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11958
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LLMs&#33258;&#21160;&#35780;&#20272;&#24515;&#29702;&#21672;&#35810;&#23545;&#35805;&#20013;&#30340;&#24037;&#20316;&#32852;&#30431;&#65292;&#32467;&#26524;&#26174;&#31034;&#19982;&#20154;&#24037;&#35780;&#20272;&#39640;&#24230;&#19968;&#33268;&#65292;&#24182;&#25552;&#20379;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#24515;&#29702;&#21672;&#35810;&#23545;&#20840;&#29699;&#24515;&#29702;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#65292;&#21450;&#26102;&#35780;&#20272;&#23545;&#30830;&#20445;&#20854;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#21672;&#35810;&#20250;&#35805;&#33719;&#21462;&#19987;&#19994;&#35780;&#20272;&#26082;&#26114;&#36149;&#21448;&#20855;&#25361;&#25112;&#24615;&#12290;&#20381;&#36182;&#33258;&#25105;&#25110;&#31532;&#19977;&#26041;&#25163;&#21160;&#25253;&#21578;&#26469;&#35780;&#20272;&#21672;&#35810;&#36136;&#37327;&#30340;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#20027;&#35266;&#20559;&#35265;&#21644;&#32791;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#39640;&#25928;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#35780;&#20272;&#21672;&#35810;&#23545;&#35805;&#20013;&#30340;&#24037;&#20316;&#32852;&#30431;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#21672;&#35810;&#25968;&#25454;&#38598;&#65292;&#24182;&#22522;&#20110;&#27835;&#30103;&#20851;&#31995;&#29702;&#35770;&#36827;&#34892;&#20102;&#22810;&#26041;&#35780;&#20272;&#12290;&#25105;&#20204;&#22522;&#20110;LLMs&#30340;&#35780;&#20272;&#32467;&#21512;&#25105;&#20204;&#30340;&#25351;&#21335;&#65292;&#19982;&#20154;&#24037;&#35780;&#20272;&#39640;&#24230;&#19968;&#33268;&#65292;&#24182;&#20026;&#21672;&#35810;&#33050;&#26412;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#36825;&#31361;&#26174;&#20102;LLMs&#20316;&#20026;&#30417;&#30563;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11958v1 Announce Type: new  Abstract: High-quality psychological counseling is crucial for mental health worldwide, and timely evaluation is vital for ensuring its effectiveness. However, obtaining professional evaluation for each counseling session is expensive and challenging. Existing methods that rely on self or third-party manual reports to assess the quality of counseling suffer from subjective biases and limitations of time-consuming.   To address above challenges, this paper proposes an innovative and efficient automatic approach using large language models (LLMs) to evaluate the working alliance in counseling conversations. We collected a comprehensive counseling dataset and conducted multiple third-party evaluations based on therapeutic relationship theory. Our LLM-based evaluation, combined with our guidelines, shows high agreement with human evaluations and provides valuable insights into counseling scripts. This highlights the potential of LLMs as supervisory to
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;SEASON&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#27010;&#25324;&#24615;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#19982;BART&#12289;PEGASUS&#21644;ProphetNet&#31561;&#27169;&#22411;&#36827;&#34892;&#23545;&#27604;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#30528;&#37325;&#20998;&#26512;&#20102;&#36130;&#32463;&#25968;&#25454;&#38598;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11955</link><description>&lt;p&gt;
&#22810;&#39046;&#22495;&#27010;&#25324;&#24615;&#25688;&#35201;&#29983;&#25104;&#30340;&#20851;&#38190;&#24615;&#20998;&#37197;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Multidomain Abstractive Summarization Using Salience Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11955
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;SEASON&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#27010;&#25324;&#24615;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#19982;BART&#12289;PEGASUS&#21644;ProphetNet&#31561;&#27169;&#22411;&#36827;&#34892;&#23545;&#27604;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#30528;&#37325;&#20998;&#26512;&#20102;&#36130;&#32463;&#25968;&#25454;&#38598;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;SEASON&#65288;Salience Allocation as Guidance for Abstractive SummarizatiON&#65289;&#25216;&#26415;&#30340;&#35270;&#35282;&#25506;&#35752;&#20102;&#27010;&#25324;&#24615;&#25991;&#26412;&#25688;&#35201;&#29983;&#25104;&#30340;&#39046;&#22495;&#65292;&#35813;&#27169;&#22411;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#20851;&#38190;&#24615;&#20998;&#37197;&#25216;&#26415;&#26469;&#22686;&#24378;&#25688;&#35201;&#29983;&#25104;&#12290;&#30740;&#31350;&#36890;&#36807;&#19982;BART&#12289;PEGASUS&#21644;ProphetNet&#31561;&#30693;&#21517;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#36825;&#20123;&#27169;&#22411;&#22343;&#38024;&#23545;&#21508;&#31181;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#26469;&#35780;&#20272;SEASON&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;&#35780;&#20272;&#20351;&#29992;&#21253;&#25324;CNN/Dailymail&#12289;SAMSum&#21644;&#22522;&#20110;&#36130;&#32463;&#26032;&#38395;&#30340;Event-Driven Trading (EDT)&#22312;&#20869;&#30340;&#22810;&#31181;&#25968;&#25454;&#38598;&#36827;&#34892;&#65292;&#29305;&#21035;&#20851;&#27880;&#21253;&#21547;&#22823;&#37327;&#26032;&#38395;&#25991;&#31456;&#30340;&#36130;&#32463;&#25968;&#25454;&#38598;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;2020/03/01&#33267;2021/05/06&#12290;&#26412;&#25991;&#37319;&#29992;&#22810;&#31181;&#35780;&#20272;&#25351;&#26631;&#65288;&#22914;ROUGE&#12289;METEOR&#12289;BERTScore&#21644;MoverScore&#65289;&#26469;&#35780;&#20272;&#36825;&#20123;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#29983;&#25104;&#27010;&#25324;&#24615;&#25688;&#35201;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#25351;&#26631;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#20248;&#21155;&#30340;&#28145;&#20837;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11955v1 Announce Type: cross  Abstract: This paper explores the realm of abstractive text summarization through the lens of the SEASON (Salience Allocation as Guidance for Abstractive SummarizatiON) technique, a model designed to enhance summarization by leveraging salience allocation techniques. The study evaluates SEASON's efficacy by comparing it with prominent models like BART, PEGASUS, and ProphetNet, all fine-tuned for various text summarization tasks. The assessment is conducted using diverse datasets including CNN/Dailymail, SAMSum, and Financial-news based Event-Driven Trading (EDT), with a specific focus on a financial dataset containing a substantial volume of news articles from 2020/03/01 to 2021/05/06. This paper employs various evaluation metrics such as ROUGE, METEOR, BERTScore, and MoverScore to evaluate the performance of these models fine-tuned for generating abstractive summaries. The analysis of these metrics offers a thorough insight into the strengths a
&lt;/p&gt;</description></item><item><title>LVLM-Enhanced Multimodal Misinformation Detection with LEMMA proposes a solution for detecting complex misinformation by enhancing the reasoning capability of Large Vision Language Models.</title><link>https://arxiv.org/abs/2402.11943</link><description>&lt;p&gt;
LEMMA: &#25903;&#25345;&#22806;&#37096;&#30693;&#35782;&#22686;&#24378;&#30340;LVLM&#22686;&#24378;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LEMMA: Towards LVLM-Enhanced Multimodal Misinformation Detection with External Knowledge Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11943
&lt;/p&gt;
&lt;p&gt;
LVLM-Enhanced Multimodal Misinformation Detection with LEMMA proposes a solution for detecting complex misinformation by enhancing the reasoning capability of Large Vision Language Models.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#24179;&#21488;&#19978;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#30340;&#20852;&#36215;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#37117;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#19982;&#25991;&#26412;&#34394;&#20551;&#20449;&#24687;&#30456;&#27604;&#65292;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#20855;&#26377;&#26356;&#39640;&#30340;&#21487;&#20449;&#24230;&#21644;&#26356;&#24191;&#27867;&#30340;&#24433;&#21709;&#65292;&#20351;&#24471;&#26816;&#27979;&#21464;&#24471;&#22797;&#26434;&#65292;&#38656;&#35201;&#36328;&#36234;&#19981;&#21516;&#23186;&#20307;&#31867;&#22411;&#36827;&#34892;&#24378;&#22823;&#30340;&#25512;&#29702;&#65292;&#24182;&#20855;&#22791;&#20934;&#30830;&#39564;&#35777;&#30340;&#28145;&#21051;&#30693;&#35782;&#12290;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#30340;&#20986;&#29616;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#28508;&#22312;&#26041;&#26696;&#12290;&#21033;&#29992;LVLM&#22312;&#22788;&#29702;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#26041;&#38754;&#30340;&#29087;&#32451;&#33021;&#21147;&#65292;LVLM&#22312;&#35782;&#21035;&#22797;&#26434;&#20449;&#24687;&#21644;&#23637;&#29616;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#23637;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;LVLM&#22312;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;LVLM&#30340;&#24615;&#33021;&#20248;&#20110;LLMs&#65292;&#20294;&#20854;&#28145;&#21051;&#25512;&#29702;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#35777;&#25454;&#32780;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#25928;&#21147;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEMMA&#65306;LVLM&#22686;&#24378;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11943v1 Announce Type: new  Abstract: The rise of multimodal misinformation on social platforms poses significant challenges for individuals and societies. Its increased credibility and broader impact compared to textual misinformation make detection complex, requiring robust reasoning across diverse media types and profound knowledge for accurate verification. The emergence of Large Vision Language Model (LVLM) offers a potential solution to this problem. Leveraging their proficiency in processing visual and textual information, LVLM demonstrates promising capabilities in recognizing complex information and exhibiting strong reasoning skills. In this paper, we first investigate the potential of LVLM on multimodal misinformation detection. We find that even though LVLM has a superior performance compared to LLMs, its profound reasoning may present limited power with a lack of evidence. Based on these observations, we propose LEMMA: LVLM-Enhanced Multimodal Misinformation Det
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20840;&#38754;&#35748;&#30693;LLM&#20195;&#29702;&#65292;&#36890;&#36807;&#20840;&#38754;&#29615;&#22659;&#24863;&#30693;&#21644;&#26465;&#20214;&#21160;&#20316;&#39044;&#27979;&#20004;&#31181;&#26032;&#26041;&#27861;&#31995;&#32479;&#24615;&#25552;&#39640;GUI&#33258;&#21160;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11941</link><description>&lt;p&gt;
&#26234;&#33021;&#25163;&#26426;GUI&#33258;&#21160;&#21270;&#30340;&#20840;&#38754;&#35748;&#30693;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Cognitive LLM Agent for Smartphone GUI Automation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11941
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20840;&#38754;&#35748;&#30693;LLM&#20195;&#29702;&#65292;&#36890;&#36807;&#20840;&#38754;&#29615;&#22659;&#24863;&#30693;&#21644;&#26465;&#20214;&#21160;&#20316;&#39044;&#27979;&#20004;&#31181;&#26032;&#26041;&#27861;&#31995;&#32479;&#24615;&#25552;&#39640;GUI&#33258;&#21160;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#26174;&#31034;&#20986;&#20316;&#20026;&#20154;&#31867;&#33324;&#33258;&#20027;&#35821;&#35328;&#20195;&#29702;&#19982;&#29616;&#23454;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#26174;&#33879;&#28508;&#21147;&#65292;&#23588;&#20854;&#22312;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;(GUI)&#33258;&#21160;&#21270;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;GUI&#20195;&#29702;&#38656;&#35201;&#20840;&#38754;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#21253;&#25324;&#35814;&#23613;&#30340;&#24863;&#30693;&#21644;&#21487;&#38752;&#30340;&#21160;&#20316;&#21709;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#38754;&#35748;&#30693;LLM&#20195;&#29702;&#65292;CoCo-Agent&#65292;&#37319;&#29992;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65292;&#20840;&#38754;&#29615;&#22659;&#24863;&#30693;(CEP)&#21644;&#26465;&#20214;&#21160;&#20316;&#39044;&#27979;(CAP)&#65292;&#20197;&#31995;&#32479;&#24615;&#22320;&#25552;&#39640;GUI&#33258;&#21160;&#21270;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;CEP&#36890;&#36807;&#19981;&#21516;&#26041;&#38754;&#21644;&#31890;&#24230;&#20419;&#36827;GUI&#24863;&#30693;&#65292;&#21253;&#25324;&#23631;&#24149;&#25130;&#22270;&#21644;&#29992;&#20110;&#35270;&#35273;&#36890;&#36947;&#30340;&#34917;&#20805;&#35814;&#32454;&#24067;&#23616;&#65292;&#20197;&#21450;&#29992;&#20110;&#25991;&#26412;&#36890;&#36947;&#30340;&#21382;&#21490;&#21160;&#20316;&#12290;&#20854;&#27425;&#65292;CAP&#23558;&#21160;&#20316;&#39044;&#27979;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65306;&#21160;&#20316;&#31867;&#22411;&#39044;&#27979;&#21644;&#26465;&#20214;&#21270;&#20110;&#21160;&#20316;&#31867;&#22411;&#30340;&#21160;&#20316;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11941v1 Announce Type: new  Abstract: Large language models (LLMs) have shown remarkable potential as human-like autonomous language agents to interact with real-world environments, especially for graphical user interface (GUI) automation. However, those GUI agents require comprehensive cognition ability including exhaustive perception and reliable action response. We propose \underline{Co}mprehensive \underline{Co}gnitive LLM \underline{Agent}, CoCo-Agent, with two novel approaches, comprehensive environment perception (CEP) and conditional action prediction (CAP), to systematically improve the GUI automation performance. First, CEP facilitates the GUI perception through different aspects and granularity, including screenshots and complementary detailed layouts for the visual channel and historical actions for the textual channel. Second, CAP decomposes the action prediction into sub-problems: action type prediction and action target conditioned on the action type. With our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22242;&#38431;QUST&#22312;SemEval-2024&#20219;&#21153;8&#20013;&#30340;&#21442;&#19982;&#24773;&#20917;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#28165;&#27927;&#25552;&#39640;&#20102;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#22312;&#21333;&#35821;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#22810;&#31181;&#26041;&#27861;&#24182;&#26368;&#32456;&#37319;&#29992;&#22534;&#21472;&#38598;&#25104;&#27169;&#22411;&#65292;&#26368;&#32456;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2402.11934</link><description>&lt;p&gt;
Team QUST&#22312;SemEval-2024&#20219;&#21153;8&#20013;&#30340;&#30740;&#31350;&#65306;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#26041;&#27861;&#23545;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Team QUST at SemEval-2024 Task 8: A Comprehensive Study of Monolingual and Multilingual Approaches for Detecting AI-generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22242;&#38431;QUST&#22312;SemEval-2024&#20219;&#21153;8&#20013;&#30340;&#21442;&#19982;&#24773;&#20917;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#28165;&#27927;&#25552;&#39640;&#20102;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#65292;&#22312;&#21333;&#35821;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#22810;&#31181;&#26041;&#27861;&#24182;&#26368;&#32456;&#37319;&#29992;&#22534;&#21472;&#38598;&#25104;&#27169;&#22411;&#65292;&#26368;&#32456;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22242;&#38431;QUST&#22312;SemEval 2024&#20219;&#21153;8&#20013;&#30340;&#21442;&#19982;&#24773;&#20917;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22686;&#24378;&#21644;&#28165;&#27927;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#35757;&#32451;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#22312;&#21333;&#35821;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20256;&#32479;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12289;&#22810;&#23610;&#24230;&#27491;&#36127;&#26080;&#26631;&#35760;&#26694;&#26550;&#65288;MPU&#65289;&#12289;&#24494;&#35843;&#12289;&#36866;&#37197;&#22120;&#21644;&#38598;&#25104;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#21333;&#35821;&#27169;&#22411;&#20013;&#36873;&#25321;&#20102;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#23376;&#20219;&#21153;A&#21644;B&#20013;&#23545;&#23427;&#20204;&#36827;&#34892;&#35780;&#20272;&#12290;&#26368;&#32456;&#27169;&#22411;&#37319;&#29992;&#20102;&#23558;&#24494;&#35843;&#19982;MPU&#30456;&#32467;&#21512;&#30340;&#22534;&#21472;&#38598;&#25104;&#12290;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#30340;&#23376;&#20219;&#21153;A&#20013;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#23448;&#26041;&#27979;&#35797;&#38598;&#20013;&#21462;&#24471;&#20102;&#31532;8&#21517;&#65288;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#25490;&#21517;&#31532;13&#65289;&#12290;&#25105;&#20204;&#22312;https://github.com/warmth27/SemEval2024_QUST&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11934v1 Announce Type: cross  Abstract: This paper presents the participation of team QUST in Task 8 SemEval 2024. We first performed data augmentation and cleaning on the dataset to enhance model training efficiency and accuracy. In the monolingual task, we evaluated traditional deep-learning methods, multiscale positive-unlabeled framework (MPU), fine-tuning, adapters and ensemble methods. Then, we selected the top-performing models based on their accuracy from the monolingual models and evaluated them in subtasks A and B. The final model construction employed a stacking ensemble that combined fine-tuning with MPU. Our system achieved 8th (scored 8th in terms of accuracy, officially ranked 13th) place in the official test set in multilingual settings of subtask A. We release our system code at:https://github.com/warmth27/SemEval2024_QUST
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32534;&#36753;HotpotQA&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#30693;&#35782;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;LLM MHQA&#35780;&#20272;&#22522;&#20934;&#65292;&#21516;&#26102;&#27880;&#37322;&#21644;&#35780;&#20272;&#20102;&#25512;&#29702;&#38142;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;MHQA&#22522;&#20934;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.11924</link><description>&lt;p&gt;
MRKE&#65306;&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#23545;LLMs&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MRKE: The Multi-hop Reasoning Evaluation of LLMs by Knowledge Edition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11924
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32534;&#36753;HotpotQA&#25968;&#25454;&#38598;&#20013;&#30340;&#26032;&#30693;&#35782;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;LLM MHQA&#35780;&#20272;&#22522;&#20934;&#65292;&#21516;&#26102;&#27880;&#37322;&#21644;&#35780;&#20272;&#20102;&#25512;&#29702;&#38142;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;MHQA&#22522;&#20934;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#65288;MHQA&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30495;&#27491;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#26377;&#24453;&#25506;&#35752;&#12290;&#30446;&#21069;&#30340;LLM QA&#35780;&#20272;&#22522;&#20934;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#21253;&#25324;1&#65289;&#25968;&#25454;&#27745;&#26579;&#65292;&#35780;&#20272;&#25968;&#25454;&#21487;&#33021;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#26292;&#38706;&#32473;LLMs&#65307;&#20197;&#21450;2&#65289;&#24573;&#35270;&#25512;&#29702;&#38142;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;LLM MHQA&#35780;&#20272;&#22522;&#20934;&#65292;&#36825;&#26159;&#22522;&#20110;&#32534;&#36753;&#29616;&#25104;HotpotQA&#25968;&#25454;&#38598;&#19978;&#30340;&#26032;&#12289;&#21069;&#25152;&#26410;&#26377;&#30340;&#30693;&#35782;&#30340;&#31532;&#19968;&#20010;QA&#22522;&#20934;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27880;&#37322;&#21644;&#35780;&#20272;&#20102;&#25512;&#29702;&#38142;&#65292;&#20197;&#23376;&#38382;&#39064;&#21644;&#20013;&#38388;&#31572;&#26696;&#30340;&#24418;&#24335;&#23545;&#24212;&#20110;&#22810;&#36339;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#26681;&#25454;&#35266;&#23519;&#32467;&#26524;&#65292;1&#65289;LLMs&#22312;&#21407;&#22987;HotpotQA&#21644;&#25105;&#20204;&#32534;&#36753;&#30340;&#25968;&#25454;&#20043;&#38388;&#26174;&#31034;&#24615;&#33021;&#24046;&#36317;&#65292;&#35748;&#20026;&#24403;&#21069;&#30340;MHQA&#22522;&#20934;&#21487;&#33021;&#23384;&#22312;&#25968;&#25454;&#27745;&#26579;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#38590;&#20197;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11924v1 Announce Type: new  Abstract: Although Large Language Models (LLMs) have shown strong performance in Multi-hop Question Answering (MHQA) tasks, their real reasoning ability remains exploration. Current LLM QA evaluation benchmarks have shown limitations, including 1) data contamination, the evaluation data are potentially exposed to LLMs during the pretraining stage; and 2) ignoration of the reasoning chain evaluation. Thus we introduce an LLM MHQA evaluation benchmark, the first QA benchmark based on the new, unprecedented knowledge by editing the off-the-shelf HotpotQA dataset; Besides, we also annotate and evaluate the reasoning chain in the form of sub-questions and intermediate answers corresponding to the multi-hop questions. Specifically, based on the observation, 1) LLMs show a performance gap between the original HotpotQA and our edited data, deeming that current MHQA benchmarks have the potential risk of data contamination that hard to evaluate LLMs' perfor
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#25552;&#31034;&#33258;&#25105;&#22870;&#21169;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;&#65292;&#26080;&#38656;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#30340;&#20559;&#22909;&#25968;&#25454;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;RLHF&#12290;</title><link>https://arxiv.org/abs/2402.11907</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#22870;&#21169;&#23545;&#27604;&#25552;&#31034;&#31934;&#28860;&#30452;&#25509;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Direct Large Language Model Alignment Through Self-Rewarding Contrastive Prompt Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11907
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#25552;&#31034;&#33258;&#25105;&#22870;&#21169;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;&#65292;&#26080;&#38656;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#30340;&#20559;&#22909;&#25968;&#25454;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;RLHF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#25552;&#31034;&#23545;&#21709;&#24212;&#23545;&#30340;&#36755;&#20986;&#27010;&#29575;&#36827;&#34892;&#35780;&#20272;&#65292;&#20174;&#32780;&#22312;LLaMA2-7B&#21644;LLaMA2-13B&#19978;&#23454;&#29616;&#20102;&#27604;RLAIF&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;&#65292;&#21363;&#30452;&#25509;&#22823;&#22411;&#27169;&#22411;&#23545;&#40784;&#65288;DLMA&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#25552;&#31034;&#23545;&#33258;&#21160;&#29983;&#25104;&#30340;&#20559;&#22909;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32487;&#32493;&#20351;&#29992;&#23545;&#27604;&#25552;&#31034;&#23545;&#29983;&#25104;&#30340;&#20559;&#22909;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#24182;&#35745;&#31639;&#33258;&#25105;&#22870;&#21169;&#20998;&#25968;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;DPO&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#36825;&#31181;&#33258;&#25105;&#22870;&#21169;&#20998;&#25968;&#26469;&#26377;&#25928;&#22320;&#23545;&#40784;LLMs&#12290;&#22312;&#23454;&#39564;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;DLMA&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#20154;&#24037;&#27880;&#37322;&#30340;&#20559;&#22909;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;RLHF&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11907v1 Announce Type: new  Abstract: Aligning large language models (LLMs) with human expectations without human-annotated preference data is an important problem. In this paper, we propose a method to evaluate the response preference by using the output probabilities of response pairs under contrastive prompt pairs, which could achieve better performance on LLaMA2-7B and LLaMA2-13B compared to RLAIF. Based on this, we propose an automatic alignment method, Direct Large Model Alignment (DLMA). First, we use contrastive prompt pairs to automatically generate preference data. Then, we continue to evaluate the generated preference data using contrastive prompt pairs and calculate a self-rewarding score. Finally, we use the DPO algorithm to effectively align LLMs by combining this self-rewarding score. In the experimental stage, our DLMA method could surpass the \texttt{RLHF} method without relying on human-annotated preference data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Learning to Edit&#65288;LTE&#65289;&#30340;&#26694;&#26550;&#65292;&#25945;&#23548;LLMs&#23558;&#26356;&#26032;&#21518;&#30340;&#30693;&#35782;&#24212;&#29992;&#20110;&#36755;&#20837;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#40784;&#38454;&#27573;&#21644;&#25512;&#29702;&#38454;&#27573;&#23454;&#29616;&#21487;&#38752;&#30340;&#12289;&#33539;&#22260;&#20869;&#30340;&#25991;&#26412;&#32534;&#36753;&#12290;</title><link>https://arxiv.org/abs/2402.11905</link><description>&lt;p&gt;
&#23398;&#20064;&#32534;&#20889;&#65306;&#23558;LLMs&#19982;&#30693;&#35782;&#32534;&#36753;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Learning to Edit: Aligning LLMs with Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11905
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Learning to Edit&#65288;LTE&#65289;&#30340;&#26694;&#26550;&#65292;&#25945;&#23548;LLMs&#23558;&#26356;&#26032;&#21518;&#30340;&#30693;&#35782;&#24212;&#29992;&#20110;&#36755;&#20837;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#40784;&#38454;&#27573;&#21644;&#25512;&#29702;&#38454;&#27573;&#23454;&#29616;&#21487;&#38752;&#30340;&#12289;&#33539;&#22260;&#20869;&#30340;&#25991;&#26412;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#26088;&#22312;&#39640;&#25928;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#23569;&#37327;&#30693;&#35782;&#65292;&#32780;&#19981;&#20250;&#23545;&#20854;&#20182;&#36755;&#20837;&#30340;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#35760;&#24518;&#26356;&#26032;&#21518;&#30340;&#30693;&#35782;&#65292;&#38459;&#30861;&#20102;LLMs&#26377;&#25928;&#22320;&#23558;&#26032;&#30693;&#35782;&#19982;&#20854;&#22266;&#26377;&#30693;&#35782;&#30456;&#32467;&#21512;&#20197;&#22238;&#31572;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#23398;&#20064;&#32534;&#20889;&#8221;&#65288;LTE&#65289;&#30340;&#26694;&#26550;&#65292;&#37325;&#28857;&#25945;&#23548;LLMs&#23558;&#26356;&#26032;&#21518;&#30340;&#30693;&#35782;&#24212;&#29992;&#20110;&#36755;&#20837;&#38382;&#39064;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#8220;&#25480;&#20154;&#20197;&#40060;&#19981;&#22914;&#25480;&#20154;&#20197;&#28180;&#8221;&#30340;&#29702;&#24565;&#12290;LTE&#20855;&#26377;&#20004;&#38454;&#27573;&#36807;&#31243;&#65306;&#65288;i&#65289;&#23545;&#40784;&#38454;&#27573;&#65292;&#36890;&#36807;&#22312;&#31934;&#24515;&#31579;&#36873;&#30340;&#24179;&#34892;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;LLMs&#65292;&#20351;&#20854;&#33021;&#22815;&#36827;&#34892;&#21487;&#38752;&#30340;&#12289;&#33539;&#22260;&#20869;&#30340;&#32534;&#36753;&#65292;&#21516;&#26102;&#20445;&#30041;&#33539;&#22260;&#22806;&#20449;&#24687;&#21644;&#35821;&#35328;&#33021;&#21147;&#65307;&#65288;ii&#65289;&#25512;&#29702;&#38454;&#27573;&#65292;&#37319;&#29992;&#22522;&#20110;&#26816;&#32034;&#30340;&#26426;&#21046;&#36827;&#34892;&#23454;&#26102;&#21644;&#22823;&#35268;&#27169;&#30693;&#35782;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11905v1 Announce Type: new  Abstract: Knowledge editing techniques, aiming to efficiently modify a minor proportion of knowledge in large language models (LLMs) without negatively impacting performance across other inputs, have garnered widespread attention. However, existing methods predominantly rely on memorizing the updated knowledge, impeding LLMs from effectively combining the new knowledge with their inherent knowledge when answering questions. To this end, we propose a Learning to Edit (LTE) framework, focusing on teaching LLMs to apply updated knowledge into input questions, inspired by the philosophy of "Teach a man to fish." LTE features a two-phase process: (i) the Alignment Phase, which fine-tunes LLMs on a meticulously curated parallel dataset to make reliable, in-scope edits while preserving out-of-scope information and linguistic proficiency; and (ii) the Inference Phase, which employs a retrieval-based mechanism for real-time and mass knowledge editing. By c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#22120;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#22312;LLM&#20013;&#24341;&#20837;&#27714;&#35299;&#22120;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;</title><link>https://arxiv.org/abs/2402.11903</link><description>&lt;p&gt;
SoLA: &#20026;&#20102;&#26356;&#22909;&#30340;&#36923;&#36753;&#25512;&#29702;&#32780;&#23545;LLM&#36827;&#34892;&#27714;&#35299;&#23618;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
SoLA: Solver-Layer Adaption of LLM for Better Logic Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11903
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#22120;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#22312;LLM&#20013;&#24341;&#20837;&#27714;&#35299;&#22120;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36923;&#36753;&#25512;&#29702;&#19978;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#35797;&#22270;&#36890;&#36807;&#24037;&#20855;&#23398;&#20064;&#26469;&#25913;&#21464;&#38382;&#39064;&#27714;&#35299;&#12290;&#34429;&#28982;&#22312;&#23567;&#35268;&#27169;&#38382;&#39064;&#19978;&#24050;&#32463;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#35268;&#27169;&#24222;&#22823;&#19988;&#34920;&#36798;&#22797;&#26434;&#65292;&#35299;&#20915;&#24037;&#19994;&#26696;&#20363;&#20173;&#28982;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27714;&#35299;&#23618;&#36866;&#24212;&#65288;SoLA&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;LLM&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#27714;&#35299;&#22120;&#20316;&#20026;&#26032;&#23618;&#65292;&#19981;&#21516;&#22320;&#24341;&#23548;&#35299;&#20915;&#26041;&#26696;&#26397;&#21521;&#21487;&#28385;&#36275;&#24615;&#12290;&#22312;SoLA&#20013;&#65292;LLM&#26088;&#22312;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#35782;&#21035;&#26368;&#39640;&#36136;&#37327;&#30340;&#23616;&#37096;&#35299;&#65292;&#32780;&#27714;&#35299;&#22120;&#23618;&#21017;&#19987;&#27880;&#20110;&#21021;&#22987;&#35299;&#19981;&#28385;&#36275;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#20511;&#21161;MaxSAT&#20316;&#20026;&#26725;&#26753;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#21069;&#21521;&#21644;&#21518;&#21521;&#20256;&#36882;&#26799;&#24230;&#65292;&#20351;&#26368;&#32456;&#27169;&#22411;&#33021;&#22815;&#25910;&#25947;&#21040;&#19968;&#20010;&#28385;&#36275;&#30340;&#35299;&#25110;&#35777;&#26126;&#19981;&#21487;&#28385;&#36275;&#24615;&#12290;&#21518;&#38376;&#29702;&#35770;&#30830;&#20445;SoLA&#33021;&#22815;&#33719;&#24471;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11903v1 Announce Type: cross  Abstract: Considering the challenges faced by large language models (LLMs) on logical reasoning, prior efforts have sought to transform problem-solving through tool learning. While progress has been made on small-scale problems, solving industrial cases remains difficult due to their large scale and intricate expressions. In this paper, we propose a novel solver-layer adaptation (SoLA) method, where we introduce a solver as a new layer of the LLM to differentially guide solutions towards satisfiability. In SoLA, LLM aims to comprehend the search space described in natural language and identify local solutions of the highest quality, while the solver layer focuses solely on constraints not satisfied by the initial solution. Leveraging MaxSAT as a bridge, we define forward and backward transfer gradients, enabling the final model to converge to a satisfied solution or prove unsatisfiability. The backdoor theory ensures that SoLA can obtain accurat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#35843;&#26597;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21033;&#29992;&#20107;&#23454;&#24555;&#25463;&#26041;&#24335;&#36827;&#34892;&#22810;&#36339;&#20107;&#23454;&#25512;&#29702;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#31181;&#24555;&#25463;&#26041;&#24335;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2402.11900</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#32534;&#36753;&#20013;&#25506;&#32034;&#22810;&#36339;&#20107;&#23454;&#24555;&#25463;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#35843;&#26597;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21033;&#29992;&#20107;&#23454;&#24555;&#25463;&#26041;&#24335;&#36827;&#34892;&#22810;&#36339;&#20107;&#23454;&#25512;&#29702;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#31181;&#24555;&#25463;&#26041;&#24335;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#24518;&#30693;&#35782;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#23558;&#36825;&#20004;&#31181;&#33021;&#21147;&#32467;&#21512;&#21040;&#36890;&#36807;&#22810;&#36339;&#20107;&#23454;&#25512;&#29702;&#20013;&#23578;&#26410;&#34987;&#24191;&#27867;&#25506;&#32034;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;LLMs&#21033;&#29992;&#22522;&#20110;&#22810;&#36339;&#30693;&#35782;&#30340;&#21021;&#22987;&#21644;&#32456;&#31471;&#23454;&#20307;&#20043;&#38388;&#30452;&#25509;&#36830;&#25509;&#30340;&#24555;&#25463;&#26041;&#24335;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;Knowledge Neurons&#25506;&#32034;&#20102;&#20107;&#23454;&#24555;&#25463;&#26041;&#24335;&#30340;&#23384;&#22312;&#65292;&#25581;&#31034;&#20986;&#65306;(i)&#24555;&#25463;&#26041;&#24335;&#30340;&#24378;&#24230;&#19982;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#21021;&#22987;&#21644;&#32456;&#31471;&#23454;&#20307;&#30340;&#20849;&#29616;&#39057;&#29575;&#39640;&#24230;&#30456;&#20851;&#65307;&#65288;ii&#65289;&#23569;&#37327;&#25552;&#31034;&#22312;&#22238;&#31572;&#22810;&#36339;&#38382;&#39064;&#26102;&#21033;&#29992;&#26356;&#22810;&#30340;&#24555;&#25463;&#26041;&#24335;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#24605;&#32500;&#38142;&#25552;&#31034;&#26356;&#22810;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#22810;&#36339;&#30693;&#35782;&#32534;&#36753;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#20107;&#23454;&#24555;&#25463;&#26041;&#24335;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#22823;&#32422;&#26377;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11900v1 Announce Type: new  Abstract: Recent work has showcased the powerful capability of large language models (LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored. This paper systematically investigates the possibilities for LLMs to utilize shortcuts based on direct connections between the initial and terminal entities of multi-hop knowledge. We first explore the existence of factual shortcuts through Knowledge Neurons, revealing that: (i) the strength of factual shortcuts is highly correlated with the frequency of co-occurrence of initial and terminal entities in the pre-training corpora; (ii) few-shot prompting leverage more shortcuts in answering multi-hop questions compared to chain-of-thought prompting. Then, we analyze the risks posed by factual shortcuts from the perspective of multi-hop knowledge editing. Analysis shows that approximatel
&lt;/p&gt;</description></item><item><title>SIBO&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22686;&#24378;&#22120;&#26469;&#22686;&#24378;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;Transformer-based LLMs&#20013;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.11896</link><description>&lt;p&gt;
SIBO&#65306;&#19968;&#20010;&#31616;&#21333;&#30340;&#22686;&#24378;&#22120;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
SIBO: A Simple Booster for Parameter-Efficient Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11896
&lt;/p&gt;
&lt;p&gt;
SIBO&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22686;&#24378;&#22120;&#26469;&#22686;&#24378;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;Transformer-based LLMs&#20013;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25152;&#26377;&#21442;&#25968;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#36739;&#38271;&#26102;&#38388;&#12290;&#26368;&#26032;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#65292;&#22914;&#36866;&#37197;&#22120;&#24494;&#35843;&#21644;LoRA&#65292;&#20801;&#35768;&#21482;&#35843;&#25972;&#36825;&#20123;LLMs&#30340;&#19968;&#23567;&#37096;&#20998;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#20154;&#20204;&#27880;&#24847;&#21040;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#21066;&#24369;&#20102;&#22522;&#20110;Transformer&#30340;LLMs&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SIBO&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22686;&#24378;&#22120;&#65292;&#36890;&#36807;&#27880;&#20837;&#21021;&#22987;&#27531;&#24046;&#26469;&#22686;&#24378;PEFT&#12290;SIBO&#30452;&#35266;&#26131;&#25026;&#65292;&#24182;&#19988;&#24456;&#23481;&#26131;&#25193;&#23637;&#21040;&#19968;&#31995;&#21015;&#26368;&#26032;&#30340;PEFT&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#23545;22&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;SIBO&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#24378;&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#27604;&#29616;&#26377;&#30340;PEFT&#25216;&#26415;&#25552;&#39640;&#20102;&#39640;&#36798;15.7%&#21644;23.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11896v1 Announce Type: new  Abstract: Fine-tuning all parameters of large language models (LLMs) necessitates substantial computational power and extended time. Latest advancements in parameter-efficient fine-tuning (PEFT) techniques, such as Adapter tuning and LoRA, allow for adjustments to only a minor fraction of the parameters of these LLMs. Concurrently, it has been noted that the issue of over-smoothing diminishes the effectiveness of these Transformer-based LLMs, resulting in suboptimal performances in downstream tasks. In this paper, we present SIBO, which is a SImple BOoster to enhance PEFT, by injecting an initial residual. SIBO is straight-forward and readily extensible to a range of state-of-the-art PEFT techniques to alleviate over-smoothing and enhance performance. Extensive experiments on 22 benchmark datasets demonstrate that SIBO significantly enhances the performance of various strong baselines, achieving up to 15.7% and 23.5% improvement over existing PEFT
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#22242;&#20307;&#20114;&#21160;&#23545;&#23447;&#25945;&#26497;&#31471;&#21270;&#30340;&#24433;&#21709;&#65292;&#22312;&#21360;&#24230; Twitter &#29992;&#25143;&#20013;&#21457;&#29616;&#65292;&#25919;&#27835;&#21644;&#31038;&#20250;&#20107;&#20214;&#30340;&#22242;&#20307;&#38388;&#20114;&#21160;&#21487;&#20197;&#20943;&#23569;&#26497;&#31471;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.11895</link><description>&lt;p&gt;
&#19982;&#22242;&#20307;&#20114;&#21160;&#65306;&#23545;&#23447;&#25945;&#26497;&#31471;&#21270;&#24433;&#21709;&#30340;&#32852;&#31995;&#19982;&#30772;&#35010;
&lt;/p&gt;
&lt;p&gt;
Bridging or Breaking: Impact of Intergroup Interactions on Religious Polarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11895
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#22242;&#20307;&#20114;&#21160;&#23545;&#23447;&#25945;&#26497;&#31471;&#21270;&#30340;&#24433;&#21709;&#65292;&#22312;&#21360;&#24230; Twitter &#29992;&#25143;&#20013;&#21457;&#29616;&#65292;&#25919;&#27835;&#21644;&#31038;&#20250;&#20107;&#20214;&#30340;&#22242;&#20307;&#38388;&#20114;&#21160;&#21487;&#20197;&#20943;&#23569;&#26497;&#31471;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#25509;&#35302;&#19981;&#21516;&#35266;&#28857;&#21487;&#33021;&#20250;&#20943;&#23569;&#26497;&#31471;&#21270;&#65292;&#20294;&#24403;&#35752;&#35770;&#23545;&#25239;&#24615;&#26102;&#65292;&#20063;&#21487;&#33021;&#20135;&#29983;&#21453;&#25928;&#24212;&#24182;&#21152;&#21095;&#26497;&#31471;&#21270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22260;&#32469;&#37325;&#35201;&#20107;&#20214;&#30340;&#22242;&#20307;&#38388;&#20114;&#21160;&#26159;&#21542;&#24433;&#21709;&#31038;&#20132;&#32593;&#32476;&#20013;&#22810;&#25968;&#32676;&#20307;&#21644;&#23569;&#25968;&#32676;&#20307;&#20043;&#38388;&#30340;&#26497;&#31471;&#21270;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#32422; 70 &#19975;&#21517;&#21360;&#24230; Twitter &#29992;&#25143;&#22312; 2020 &#24180;&#21442;&#19982;&#19982; COVID-19 &#30456;&#20851;&#35805;&#39064;&#35752;&#35770;&#26102;&#30340;&#23447;&#25945;&#36523;&#20221;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#25512;&#25991;&#25991;&#26412;&#30340;&#24773;&#22659;&#23884;&#20837;&#30340;&#26032;&#37327;&#24230;&#65292;&#29992;&#20110;&#24110;&#21161;&#25105;&#20204;&#35780;&#20272;&#23447;&#25945;&#32676;&#20307;&#20043;&#38388;&#30340;&#26497;&#31471;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20803;&#23398;&#20064;&#26694;&#26550;&#26469;&#30740;&#31350;&#22260;&#32469;&#20849;&#21516;&#12289;&#25919;&#27835;&#21644;&#31038;&#20250;&#32463;&#27982;&#20107;&#20214;&#30340;&#24322;&#36136;&#22788;&#29702;&#25928;&#26524;&#23545;&#20010;&#20307;&#32676;&#20307;&#31526;&#21512;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25919;&#27835;&#21644;&#31038;&#20250;&#20107;&#20214;&#26041;&#38754;&#65292;&#22242;&#20307;&#38388;&#20114;&#21160;&#20250;&#20943;&#23569;&#26497;&#31471;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11895v1 Announce Type: cross  Abstract: While exposure to diverse viewpoints may reduce polarization, it can also have a backfire effect and exacerbate polarization when the discussion is adversarial. Here, we examine the question whether intergroup interactions around important events affect polarization between majority and minority groups in social networks. We compile data on the religious identity of nearly 700,000 Indian Twitter users engaging in COVID-19-related discourse during 2020. We introduce a new measure for an individual's group conformity based on contextualized embeddings of tweet text, which helps us assess polarization between religious groups. We then use a meta-learning framework to examine heterogeneous treatment effects of intergroup interactions on an individual's group conformity in the light of communal, political, and socio-economic events. We find that for political and social events, intergroup interactions reduce polarization. This decline is we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#33258;&#21160;&#21270;&#25968;&#25454;&#38598;&#26356;&#26032;&#31574;&#30053;&#65292;&#21033;&#29992;&#20004;&#31181;&#31995;&#32479;&#39564;&#35777;&#36807;&#30340;&#31574;&#30053;&#29983;&#25104;&#30475;&#19981;&#35265;&#21644;&#39640;&#36136;&#37327;&#30340;&#27979;&#35797;&#26679;&#26412;&#65292;&#20197;&#32531;&#35299;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#24182;&#23454;&#29616;&#35780;&#20272;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11894</link><description>&lt;p&gt;
&#20320;&#35265;&#36807;&#25105;&#21527;&#65311;&#33258;&#21160;&#21270;&#25968;&#25454;&#38598;&#26356;&#26032;&#20197;&#23454;&#29616;&#21487;&#38752;&#21450;&#21450;&#26102;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Have Seen Me Before? Automating Dataset Updates Towards Reliable and Timely Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#33258;&#21160;&#21270;&#25968;&#25454;&#38598;&#26356;&#26032;&#31574;&#30053;&#65292;&#21033;&#29992;&#20004;&#31181;&#31995;&#32479;&#39564;&#35777;&#36807;&#30340;&#31574;&#30053;&#29983;&#25104;&#30475;&#19981;&#35265;&#21644;&#39640;&#36136;&#37327;&#30340;&#27979;&#35797;&#26679;&#26412;&#65292;&#20197;&#32531;&#35299;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#24182;&#23454;&#29616;&#35780;&#20272;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#19981;&#26029;&#25193;&#22823;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#22686;&#21152;&#65292;LLMs&#38754;&#20020;&#30528;&#26085;&#30410;&#20005;&#37325;&#30340;&#35780;&#20272;&#25361;&#25112;&#12290;&#19968;&#26041;&#38754;&#65292;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#23548;&#33268;&#23545;&#29616;&#26377;&#22522;&#20934;&#30340;&#36807;&#24230;&#20272;&#35745;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23450;&#26399;&#25163;&#21160;&#25972;&#29702;&#25968;&#25454;&#38598;&#25104;&#26412;&#39640;&#26114;&#12290;&#26412;&#25991;&#25552;&#20986;&#33258;&#21160;&#21270;&#25968;&#25454;&#38598;&#26356;&#26032;&#20197;&#23454;&#29616;&#21487;&#38752;&#21450;&#21450;&#26102;&#30340;&#35780;&#20272;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#22522;&#20110;&#29616;&#26377;&#26679;&#26412;&#29983;&#25104;&#30475;&#19981;&#35265;&#30340;&#39640;&#36136;&#37327;&#27979;&#35797;&#26679;&#26412;&#20197;&#20943;&#36731;&#27844;&#28431;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#24182;&#36827;&#34892;&#20102;&#31995;&#32479;&#39564;&#35777;&#12290;&#31532;&#19968;&#31181;&#26159;&#27169;&#20223;&#31574;&#30053;&#65292;&#21033;&#29992;LLMs&#21019;&#24314;&#31867;&#20284;&#29616;&#26377;&#26679;&#26412;&#30340;&#26032;&#26679;&#26412;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#20445;&#30041;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#39118;&#26684;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#23427;&#22312;&#22810;&#27425;&#23454;&#20363;&#20013;&#30340;&#35780;&#20272;&#31283;&#23450;&#24615;&#20197;&#21450;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#22788;&#29702;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11894v1 Announce Type: new  Abstract: Due to the expanding capabilities and pre-training data, Large Language Models (LLMs) are facing increasingly serious evaluation challenges. On one hand, the data leakage issue cause over-estimation on existing benchmarks. On the other hand, periodically curating datasets manually is costly. In this paper, we propose to automate dataset updates for reliable and timely evaluation. The basic idea is to generate unseen and high-quality testing samples based on existing ones to mitigate leakage issues. In specific, we propose two strategies with systematically verification. First, the mimicking strategy employs LLMs to create new samples resembling existing ones, to the maximum extent preserving the stylistic of the original dataset. Our experiments demonstrate its evaluation stability across multiple instantiations and its effectiveness in dealing with data leakage issues in most cases. Second, for the cases that mimicking dataset works poo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FeB4RAG&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#20869;&#30340;&#32852;&#21512;&#25628;&#32034;&#35774;&#35745;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.11891</link><description>&lt;p&gt;
&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#29615;&#22659;&#20013;&#35780;&#20272;&#32852;&#21512;&#25628;&#32034;&#31995;&#32479;&#65288;FeB4RAG&#65289;
&lt;/p&gt;
&lt;p&gt;
FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11891
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FeB4RAG&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#20869;&#30340;&#32852;&#21512;&#25628;&#32034;&#35774;&#35745;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#25628;&#32034;&#31995;&#32479;&#32858;&#21512;&#26469;&#33258;&#22810;&#20010;&#25628;&#32034;&#24341;&#25806;&#30340;&#32467;&#26524;&#65292;&#36873;&#25321;&#21512;&#36866;&#30340;&#26469;&#28304;&#20197;&#22686;&#24378;&#32467;&#26524;&#36136;&#37327;&#24182;&#19982;&#29992;&#25143;&#24847;&#22270;&#20445;&#25345;&#19968;&#33268;&#12290;&#38543;&#30528;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31649;&#36947;&#30340;&#26085;&#30410;&#27969;&#34892;&#65292;&#32852;&#21512;&#25628;&#32034;&#21487;&#20197;&#22312;&#36328;&#24322;&#26500;&#25968;&#25454;&#28304;&#20013;&#25628;&#32034;&#30456;&#20851;&#20449;&#24687;&#20197;&#29983;&#25104;&#26126;&#26234;&#30340;&#21709;&#24212;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#65288;&#22914;&#36807;&#21435;&#30340;TREC FedWeb&#36319;&#36394;&#20013;&#24320;&#21457;&#30340;&#25968;&#25454;&#38598;&#65289;&#36807;&#26102;&#20102;&#65292;&#32570;&#20047;&#23545;&#29616;&#20195;&#20449;&#24687;&#26816;&#32034;&#25361;&#25112;&#30340;&#20195;&#34920;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FeB4RAG&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;RAG&#26694;&#26550;&#20869;&#30340;&#32852;&#21512;&#25628;&#32034;&#32780;&#35774;&#35745;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#28304;&#33258;&#24191;&#27867;&#20351;&#29992;&#30340;beir&#22522;&#20934;&#38598;&#30340;16&#20010;&#23376;&#38598;&#21512;&#65292;&#21253;&#25324;&#20102;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#24212;&#29992;&#37327;&#36523;&#23450;&#21046;&#30340;790&#20010;&#20449;&#24687;&#35831;&#27714;&#65288;&#31867;&#20284;&#20110;&#23545;&#35805;&#26597;&#35810;&#65289;&#65292;&#20197;&#21450;&#27599;&#20010;&#36164;&#28304;&#36820;&#22238;&#30340;&#26368;&#20339;&#32467;&#26524;&#20197;&#21450;&#30456;&#20851;&#30340;LLM-der
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11891v1 Announce Type: cross  Abstract: Federated search systems aggregate results from multiple search engines, selecting appropriate sources to enhance result quality and align with user intent. With the increasing uptake of Retrieval-Augmented Generation (RAG) pipelines, federated search can play a pivotal role in sourcing relevant information across heterogeneous data sources to generate informed responses. However, existing datasets, such as those developed in the past TREC FedWeb tracks, predate the RAG paradigm shift and lack representation of modern information retrieval challenges. To bridge this gap, we present FeB4RAG, a novel dataset specifically designed for federated search within RAG frameworks. This dataset, derived from 16 sub-collections of the widely used \beir benchmarking collection, includes 790 information requests (akin to conversational queries) tailored for chatbot applications, along with top results returned by each resource and associated LLM-der
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#25945;&#23398;&#26041;&#27861;&#65288;ATKD&#65289;&#65292;&#20197;&#25913;&#21892;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#24110;&#21161;&#21508;&#31181;&#22522;&#32447;KD&#26041;&#27861;&#22312;&#25152;&#26377;&#27169;&#22411;&#31867;&#22411;&#21644;&#35268;&#27169;&#19978;&#23454;&#29616;&#19968;&#33268;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.11890</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Revisiting Knowledge Distillation for Autoregressive Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11890
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#33258;&#36866;&#24212;&#25945;&#23398;&#26041;&#27861;&#65288;ATKD&#65289;&#65292;&#20197;&#25913;&#21892;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#65292;&#24110;&#21161;&#21508;&#31181;&#22522;&#32447;KD&#26041;&#27861;&#22312;&#25152;&#26377;&#27169;&#22411;&#31867;&#22411;&#21644;&#35268;&#27169;&#19978;&#23454;&#29616;&#19968;&#33268;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#35757;&#32451;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#26469;&#21387;&#32553;&#25945;&#24072;&#27169;&#22411;&#65292;&#20197;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#21644;&#20869;&#23384;&#21344;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#20013;&#21457;&#29616;&#36739;&#22823;&#30340;&#25945;&#24072;LMs&#21487;&#33021;&#20250;&#23548;&#33268;&#36739;&#24046;&#30340;&#23398;&#29983;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#19981;&#21516;&#35760;&#21495;&#20855;&#26377;&#19981;&#21516;&#30340;&#25945;&#23398;&#26041;&#24335;&#65292;&#24573;&#35270;&#36825;&#19968;&#28857;&#23558;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#25945;&#23398;&#26041;&#27861;&#65288;ATKD&#65289;&#26469;&#25913;&#21892;KD&#12290;ATKD&#30340;&#26680;&#24515;&#26159;&#20943;&#23569;&#27515;&#35760;&#30828;&#32972;&#30340;&#23398;&#20064;&#65292;&#20351;&#25945;&#23398;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#28789;&#27963;&#12290;&#23545;8&#20010;LM&#20219;&#21153;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;ATKD&#30340;&#24110;&#21161;&#19979;&#65292;&#21508;&#31181;&#22522;&#32447;KD&#26041;&#27861;&#22312;&#25152;&#26377;&#27169;&#22411;&#31867;&#22411;&#21644;&#35268;&#27169;&#19978;&#22343;&#21487;&#20197;&#23454;&#29616;&#19968;&#33268;&#19988;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65288;&#26368;&#39640;&#21487;&#36798;+3.04%&#30340;&#24179;&#22343;&#20998;&#25968;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11890v1 Announce Type: new  Abstract: Knowledge distillation (KD) is a common approach to compress a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, in the context of autoregressive language models (LMs), we empirically find that larger teacher LMs might dramatically result in a poorer student. In response to this problem, we conduct a series of analyses and reveal that different tokens have different teaching modes, neglecting which will lead to performance degradation. Motivated by this, we propose a simple yet effective adaptive teaching approach (ATKD) to improve the KD. The core of ATKD is to reduce rote learning and make teaching more diverse and flexible. Extensive experiments on 8 LM tasks show that, with the help of ATKD, various baseline KD methods can achieve consistent and significant performance gains (up to +3.04% average score) across all model types and sizes. More encouragingly, ATKD can improve 
&lt;/p&gt;</description></item><item><title>ROSE&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#19981;&#21463;&#27426;&#36814;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#26399;&#26395;&#30340;&#23433;&#20840;&#36755;&#20986;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#30452;&#25509;&#25552;&#21319;&#29616;&#26377;&#35843;&#25972;&#25351;&#20196;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11889</link><description>&lt;p&gt;
ROSE &#19981;&#36825;&#26679;&#20570;&#65306;&#20351;&#29992;&#21453;&#21521;&#25552;&#31034;&#23545;&#27604;&#35299;&#30721;&#25552;&#21319;&#35843;&#25972;&#25351;&#20196;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
ROSE Doesn't Do That: Boosting the Safety of Instruction-Tuned Large Language Models with Reverse Prompt Contrastive Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11889
&lt;/p&gt;
&lt;p&gt;
ROSE&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#19981;&#21463;&#27426;&#36814;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#26399;&#26395;&#30340;&#23433;&#20840;&#36755;&#20986;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#30452;&#25509;&#25552;&#21319;&#29616;&#26377;&#35843;&#25972;&#25351;&#20196;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35843;&#25972;&#25351;&#20196;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#25552;&#39640;LLMs&#30340;&#23433;&#20840;&#24615;&#21464;&#24471;&#26356;&#21152;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#23558;LLMs&#36755;&#20986;&#19982;&#39044;&#26399;&#23433;&#20840;&#24615;&#23545;&#40784;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#24037;&#20316;&#65292;&#20363;&#22914;&#39640;&#36136;&#37327;&#30340;&#23433;&#20840;&#25968;&#25454;&#21644;&#26114;&#36149;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#20123;&#37117;&#26159;&#26114;&#36149;&#19988;&#20302;&#25928;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#21521;&#25552;&#31034;&#23545;&#27604;&#35299;&#30721;&#65288;ROSE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#30452;&#25509;&#25552;&#21319;&#29616;&#26377;&#35843;&#25972;&#25351;&#20196;LLMs&#30340;&#23433;&#20840;&#24615;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;ROSE&#30340;&#21407;&#21017;&#26159;&#36890;&#36807;&#25233;&#21046;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#21453;&#21521;&#25552;&#31034;&#35825;&#23548;&#30340;&#19981;&#21463;&#27426;&#36814;&#30340;&#36755;&#20986;&#65292;&#25552;&#39640;&#26399;&#26395;&#30340;&#23433;&#20840;&#36755;&#20986;&#30340;&#27010;&#29575;&#12290;&#22312;6&#20010;&#23433;&#20840;&#20219;&#21153;&#21644;2&#20010;&#36890;&#29992;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;ROSE&#19981;&#20165;&#22312;5&#31181;&#31867;&#22411;&#30340;&#35843;&#25972;&#25351;&#20196;LLMs&#19978;&#24102;&#26469;&#20102;&#19968;&#33268;&#19988;&#26174;&#33879;&#30340;&#23433;&#20840;&#24615;&#25913;&#36827;&#65288;&#39640;&#36798;+13.8%&#30340;&#23433;&#20840;&#20998;&#25968;&#65289;&#65292;&#32780;&#19988;&#23545;&#36890;&#29992;&#20219;&#21153;&#20063;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11889v1 Announce Type: new  Abstract: With the development of instruction-tuned large language models (LLMs), improving the safety of LLMs has become more critical. However, the current approaches for aligning the LLMs output with expected safety usually require substantial training efforts, e.g., high-quality safety data and expensive computational resources, which are costly and inefficient. To this end, we present reverse prompt contrastive decoding (ROSE), a simple-yet-effective method to directly boost the safety of existing instruction-tuned LLMs without any additional training. The principle of ROSE is to improve the probability of desired safe output via suppressing the undesired output induced by the carefully-designed reverse prompts. Experiments on 6 safety and 2 general-purpose tasks show that, our ROSE not only brings consistent and significant safety improvements (up to +13.8% safety score) upon 5 types of instruction-tuned LLMs, but also benefits the general-p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#21644;&#25913;&#36827;LLM&#20316;&#20026;&#37239;&#20799;&#38738;&#23569;&#24180;&#24773;&#24863;&#25903;&#25345;&#32773;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;LLM&#19982;&#37239;&#20799;&#30456;&#20851;&#20869;&#23481;&#30340;&#20114;&#21160;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26631;&#20934;&#37327;&#34920;&#12290;</title><link>https://arxiv.org/abs/2402.11886</link><description>&lt;p&gt;
LLM&#30340;&#22810;&#24425;&#26410;&#26469;&#65306;&#35780;&#20272;&#21644;&#25913;&#36827;LLM&#20316;&#20026;&#37239;&#20799;&#38738;&#23569;&#24180;&#24773;&#24863;&#25903;&#25345;&#32773;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Colorful Future of LLMs: Evaluating and Improving LLMs as Emotional Supporters for Queer Youth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#21644;&#25913;&#36827;LLM&#20316;&#20026;&#37239;&#20799;&#38738;&#23569;&#24180;&#24773;&#24863;&#25903;&#25345;&#32773;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;LLM&#19982;&#37239;&#20799;&#30456;&#20851;&#20869;&#23481;&#30340;&#20114;&#21160;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26631;&#20934;&#37327;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37239;&#20799;&#38738;&#23569;&#24180;&#38754;&#20020;&#30528;&#22686;&#21152;&#30340;&#24515;&#29702;&#20581;&#24247;&#39118;&#38505;&#65292;&#22914;&#25233;&#37057;&#12289;&#28966;&#34385;&#21644;&#33258;&#26432;&#24847;&#24565;&#12290;&#21463;&#36127;&#38754;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#20182;&#20204;&#32463;&#24120;&#36991;&#20813;&#23547;&#27714;&#24110;&#21161;&#65292;&#20381;&#36182;&#22312;&#32447;&#36164;&#28304;&#65292;&#36825;&#21487;&#33021;&#25552;&#20379;&#19981;&#30456;&#23481;&#30340;&#20449;&#24687;&#12290;&#34429;&#28982;&#33719;&#24471;&#25903;&#25345;&#29615;&#22659;&#21644;&#21487;&#38752;&#20449;&#24687;&#26159;&#26080;&#20215;&#30340;&#65292;&#20294;&#20840;&#29699;&#35768;&#22810;&#37239;&#20799;&#38738;&#23569;&#24180;&#26080;&#27861;&#33719;&#24471;&#36825;&#31181;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24555;&#36895;&#37319;&#29992;&#65292;&#24773;&#20917;&#21487;&#33021;&#24456;&#24555;&#21457;&#29983;&#21464;&#21270;&#12290;&#26412;&#25991;&#26088;&#22312;&#20840;&#38754;&#25506;&#35752;LLM&#25913;&#21464;&#37239;&#20799;&#24773;&#24863;&#25903;&#25345;&#30340;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;LLM&#19982;&#37239;&#20799;&#30456;&#20851;&#20869;&#23481;&#30340;&#20114;&#21160;&#36827;&#34892;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;&#20026;&#20102;&#35780;&#20272;&#22238;&#24212;&#36136;&#37327;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21463;&#24515;&#29702;&#26631;&#20934;&#21644;&#19987;&#23478;&#24847;&#35265;&#21551;&#21457;&#30340;&#26032;&#39062;&#21313;&#20010;&#38382;&#39064;&#37327;&#34920;&#12290;&#25105;&#20204;&#23558;&#35813;&#37327;&#34920;&#24212;&#29992;&#20110;&#35780;&#20998;&#20960;&#20010;LLM&#21644;&#20154;&#31867;&#35780;&#35770;&#65292;&#36825;&#20123;&#35780;&#35770;&#26159;&#37239;&#20799;&#38738;&#23569;&#24180;&#23547;&#27714;&#24314;&#35758;&#21644;&#20998;&#20139;&#26102;&#21457;&#34920;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11886v1 Announce Type: cross  Abstract: Queer youth face increased mental health risks, such as depression, anxiety, and suicidal ideation. Hindered by negative stigma, they often avoid seeking help and rely on online resources, which may provide incompatible information. Although access to a supportive environment and reliable information is invaluable, many queer youth worldwide have no access to such support. However, this could soon change due to the rapid adoption of Large Language Models (LLMs) such as ChatGPT. This paper aims to comprehensively explore the potential of LLMs to revolutionize emotional support for queers. To this end, we conduct a qualitative and quantitative analysis of LLM's interactions with queer-related content. To evaluate response quality, we develop a novel ten-question scale that is inspired by psychological standards and expert input. We apply this scale to score several LLMs and human comments to posts where queer youth seek advice and share 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;VDG&#27169;&#22411;&#30340;&#24187;&#35273;&#29616;&#35937;&#21644;&#38170;&#26631;&#35760;&#30340;&#24046;&#24322;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;M2K-VDG&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#22686;&#24378;&#22810;&#27169;&#24577;&#30693;&#35782;&#38170;&#20197;&#20943;&#23569;&#24187;&#35273;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11875</link><description>&lt;p&gt;
M2K-VDG: &#27169;&#22411;&#33258;&#36866;&#24212;&#22810;&#27169;&#24577;&#30693;&#35782;&#38170;&#22686;&#24378;&#35270;&#39057;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
M2K-VDG: Model-Adaptive Multimodal Knowledge Anchor Enhanced Video-grounded Dialogue Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11875
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;VDG&#27169;&#22411;&#30340;&#24187;&#35273;&#29616;&#35937;&#21644;&#38170;&#26631;&#35760;&#30340;&#24046;&#24322;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;M2K-VDG&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#22686;&#24378;&#22810;&#27169;&#24577;&#30693;&#35782;&#38170;&#20197;&#20943;&#23569;&#24187;&#35273;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#23545;&#35805;&#29983;&#25104;&#65288;VDG&#65289;&#35201;&#27714;&#31995;&#32479;&#22522;&#20110;&#22810;&#27169;&#24577;&#30693;&#35782;&#29983;&#25104;&#27969;&#30021;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#22810;&#27169;&#24577;&#30693;&#35782;&#21033;&#29992;&#30340;&#22256;&#38590;&#32473;VDG&#27169;&#22411;&#24102;&#26469;&#20102;&#20005;&#37325;&#30340;&#24187;&#35273;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#20197;&#21508;&#31181;&#26041;&#24335;&#32531;&#35299;&#20102;&#24187;&#35273;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#27880;&#24847;&#22810;&#27169;&#24577;&#30693;&#35782;&#38170;&#31572;&#26696;&#26631;&#35760;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22256;&#24785;&#24230;&#25581;&#31034;&#20102;&#19981;&#21516;VDG&#27169;&#22411;&#32463;&#21382;&#19981;&#21516;&#24187;&#35273;&#24182;&#23637;&#31034;&#19981;&#21516;&#30340;&#38170;&#26631;&#35760;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M2K-VDG&#65292;&#19968;&#31181;&#27169;&#22411;&#33258;&#36866;&#24212;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#38170;&#22686;&#24378;&#26694;&#26550;&#29992;&#20110;&#20943;&#23569;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#25928;&#24212;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#38170;&#26631;&#35760;&#26816;&#27979;&#12290;&#19977;&#20010;&#27969;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#20943;&#23569;&#24187;&#35273;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11875v1 Announce Type: new  Abstract: Video-grounded dialogue generation (VDG) requires the system to generate a fluent and accurate answer based on multimodal knowledge. However, the difficulty in multimodal knowledge utilization brings serious hallucinations to VDG models in practice. Although previous works mitigate the hallucination in a variety of ways, they hardly take notice of the importance of the multimodal knowledge anchor answer tokens. In this paper, we reveal via perplexity that different VDG models experience varying hallucinations and exhibit diverse anchor tokens. Based on this observation, we propose M2K-VDG, a model-adaptive multimodal knowledge anchor enhancement framework for hallucination reduction. Furthermore, we introduce the counterfactual effect for more accurate anchor token detection. The experimental results on three popular benchmarks exhibit the superiority of our approach over state-of-the-art methods, demonstrating its effectiveness in reduc
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#35299;&#37322;&#25552;&#20986;&#20102;&#20840;&#38754;&#12289;&#22810;&#35282;&#24230;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#24544;&#23454;&#24230;&#12289;&#24378;&#20581;&#24615;&#21644;&#25928;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.11863</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#35299;&#37322;&#30340;&#21487;&#35299;&#37322;&#24615;&#26377;&#22810;&#39640;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Interpretable are Reasoning Explanations from Prompting Large Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11863
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#35299;&#37322;&#25552;&#20986;&#20102;&#20840;&#38754;&#12289;&#22810;&#35282;&#24230;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#24544;&#23454;&#24230;&#12289;&#24378;&#20581;&#24615;&#21644;&#25928;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt Engineering&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#21487;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;Chain-of-Thought&#31561;&#25216;&#26415;&#19981;&#20165;&#22686;&#24378;&#20102;&#20219;&#21153;&#24615;&#33021;&#65292;&#36824;&#25551;&#32472;&#20102;&#28165;&#26224;&#30340;&#25512;&#29702;&#27493;&#39588;&#36712;&#36857;&#65292;&#20026;&#35266;&#20247;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24418;&#30340;&#35299;&#37322;&#24418;&#24335;&#12290;&#25105;&#20204;&#23545;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#22810;&#35282;&#24230;&#30340;&#35780;&#20272;&#65292;&#19981;&#20165;&#32771;&#34385;&#20102;&#24544;&#23454;&#24230;&#65292;&#36824;&#32771;&#34385;&#20102;&#22312;&#22810;&#20010;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24378;&#20581;&#24615;&#21644;&#25928;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11863v1 Announce Type: new  Abstract: Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability ali
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#27169;&#22359;&#21270;&#32593;&#32476;&#21033;&#29992;LoRA&#27169;&#22359;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#29615;&#22659;&#19979;&#36827;&#34892;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.11845</link><description>&lt;p&gt;
&#27169;&#22359;&#21270;&#32593;&#32476;&#29992;&#20110;&#23569;&#26679;&#26412;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Modularized Networks for Few-shot Hateful Meme Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11845
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#27169;&#22359;&#21270;&#32593;&#32476;&#21033;&#29992;LoRA&#27169;&#22359;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#29615;&#22659;&#19979;&#36827;&#34892;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#26816;&#27979;&#20167;&#24680;&#27169;&#22240;&#30340;&#25361;&#25112;&#65292;&#35813;&#29615;&#22659;&#19979;&#21482;&#26377;&#23569;&#37327;&#26631;&#35760;&#31034;&#20363;&#21487;&#29992;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#25216;&#26415;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#30340;&#32452;&#21512;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#36873;&#25321;&#30340;&#19982;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#30456;&#20851;&#30340;&#20219;&#21153;&#19978;&#21033;&#29992;LoRA&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20174;&#32780;&#29983;&#25104;&#19968;&#22871;LoRA&#27169;&#22359;&#12290;&#36825;&#20123;&#27169;&#22359;&#20855;&#26377;&#23545;&#20110;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#30340;&#22522;&#26412;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#21487;&#29992;&#30340;&#26631;&#35760;&#26679;&#26412;&#26469;&#35757;&#32451;&#19968;&#20010;&#27169;&#22359;&#32452;&#21512;&#22120;&#65292;&#23427;&#26681;&#25454;&#27169;&#22359;&#30340;&#30456;&#20851;&#24615;&#20998;&#37197;&#26435;&#37325;&#32473;LoRA&#27169;&#22359;&#12290;&#27169;&#22411;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#19982;LoRA&#27169;&#22359;&#30340;&#25968;&#37327;&#25104;&#27491;&#27604;&#12290;&#36825;&#31181;&#30001;LLMs&#25903;&#25745;&#24182;&#19988;&#22686;&#24378;&#20102;LoRA&#27169;&#22359;&#30340;&#27169;&#22359;&#21270;&#32593;&#32476;&#65292;&#22312;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#30340;&#24773;&#22659;&#20013;&#34920;&#29616;&#20986;&#22686;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#28085;&#30422;&#20102;&#19977;&#20010;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11845v1 Announce Type: new  Abstract: In this paper, we address the challenge of detecting hateful memes in the low-resource setting where only a few labeled examples are available. Our approach leverages the compositionality of Low-rank adaptation (LoRA), a widely used parameter-efficient tuning technique. We commence by fine-tuning large language models (LLMs) with LoRA on selected tasks pertinent to hateful meme detection, thereby generating a suite of LoRA modules. These modules are capable of essential reasoning skills for hateful meme detection. We then use the few available annotated samples to train a module composer, which assigns weights to the LoRA modules based on their relevance. The model's learnable parameters are directly proportional to the number of LoRA modules. This modularized network, underpinned by LLMs and augmented with LoRA modules, exhibits enhanced generalization in the context of hateful meme detection. Our evaluation spans three datasets designe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;RetPO&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#23545;&#25628;&#32034;&#26597;&#35810;&#36827;&#34892;&#37325;&#26500;&#65292;&#20197;&#31526;&#21512;&#30446;&#26631;&#26816;&#32034;&#31995;&#32479;&#30340;&#20559;&#22909;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;RF Collection&#65292;&#29992;&#20110;&#25910;&#38598;&#26816;&#32034;&#32467;&#26524;&#20316;&#20026;&#26816;&#32034;&#22120;&#30340;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.11827</link><description>&lt;p&gt;
&#35810;&#38382;&#26368;&#20339;&#38382;&#39064;&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#26816;&#32034;&#22120;&#20559;&#22909;&#22312;&#20250;&#35805;&#25628;&#32034;&#20013;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Ask Optimal Questions: Aligning Large Language Models with Retriever's Preference in Conversational Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11827
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;RetPO&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#23545;&#25628;&#32034;&#26597;&#35810;&#36827;&#34892;&#37325;&#26500;&#65292;&#20197;&#31526;&#21512;&#30446;&#26631;&#26816;&#32034;&#31995;&#32479;&#30340;&#20559;&#22909;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;RF Collection&#65292;&#29992;&#20110;&#25910;&#38598;&#26816;&#32034;&#32467;&#26524;&#20316;&#20026;&#26816;&#32034;&#22120;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#25628;&#32034;&#19982;&#21333;&#36718;&#26816;&#32034;&#20219;&#21153;&#19981;&#21516;&#65292;&#38656;&#35201;&#29702;&#35299;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#30340;&#24403;&#21069;&#38382;&#39064;&#12290;&#24120;&#35265;&#30340;&#8220;&#37325;&#20889;-&#28982;&#21518;&#26816;&#32034;&#8221;&#30340;&#26041;&#27861;&#26088;&#22312;&#23558;&#38382;&#39064;&#21435;&#19978;&#19979;&#25991;&#21270;&#65292;&#20351;&#20854;&#23545;&#29616;&#25104;&#30340;&#26816;&#32034;&#22120;&#33258;&#32473;&#33258;&#36275;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#33021;&#21147;&#26377;&#38480;&#32780;&#20135;&#29983;&#27425;&#20248;&#30340;&#26597;&#35810;&#37325;&#20889;&#65292;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#26469;&#33258;&#26816;&#32034;&#32467;&#26524;&#30340;&#20449;&#21495;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;RetPO&#65288;&#26816;&#32034;&#22120;&#20559;&#22909;&#20248;&#21270;&#65289;&#65292;&#26088;&#22312;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20197;&#31526;&#21512;&#30446;&#26631;&#26816;&#32034;&#31995;&#32479;&#30340;&#37325;&#20889;&#25628;&#32034;&#26597;&#35810;&#30340;&#20559;&#22909;&#12290;&#35813;&#36807;&#31243;&#22987;&#20110;&#25552;&#31034;&#22823;&#22411;LM&#29983;&#25104;&#21508;&#31181;&#28508;&#22312;&#37325;&#20889;&#65292;&#28982;&#21518;&#25910;&#38598;&#36825;&#20123;&#37325;&#20889;&#30340;&#26816;&#32034;&#24615;&#33021;&#20316;&#20026;&#26816;&#32034;&#22120;&#30340;&#20559;&#22909;&#12290;&#36890;&#36807;&#35813;&#36807;&#31243;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;RF&#22609;&#38598;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#23545;&#36229;&#36807;410K&#20010;&#26597;&#35810;&#30340;&#26816;&#32034;&#22120;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11827v1 Announce Type: cross  Abstract: Conversational search, unlike single-turn retrieval tasks, requires understanding the current question within a dialogue context. The common approach of rewrite-then-retrieve aims to decontextualize questions to be self-sufficient for off-the-shelf retrievers, but most existing methods produce sub-optimal query rewrites due to the limited ability to incorporate signals from the retrieval results. To overcome this limitation, we present a novel framework RetPO (Retriever's Preference Optimization), which is designed to optimize a language model (LM) for reformulating search queries in line with the preferences of the target retrieval systems. The process begins by prompting a large LM to produce various potential rewrites and then collects retrieval performance for these rewrites as the retrievers' preferences. Through the process, we construct a large-scale dataset called RF collection, containing Retrievers' Feedback on over 410K quer
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.11821</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#24494;&#32467;&#26500;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Microstructures and Accuracy of Graph Recall by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22270;&#24418;&#21484;&#22238;&#30340;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65292;&#25506;&#35752;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#24322;&#21516;&#20197;&#21450;&#23545;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#25968;&#25454;&#23545;&#35768;&#22810;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#24456;&#22810;&#25968;&#25454;&#20197;&#25991;&#26412;&#26684;&#24335;&#25551;&#36848;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#20934;&#30830;&#22320;&#21484;&#22238;&#21644;&#32534;&#30721;&#20808;&#21069;&#25991;&#26412;&#20013;&#25551;&#36848;&#30340;&#22270;&#24418;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38656;&#35201;&#23637;&#31034;&#30340;&#22522;&#26412;&#20294;&#20851;&#38190;&#33021;&#21147;&#65292;&#20197;&#25191;&#34892;&#28041;&#21450;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#20154;&#31867;&#22312;&#22270;&#24418;&#21484;&#22238;&#26041;&#38754;&#30340;&#34920;&#29616;&#24050;&#34987;&#35748;&#30693;&#31185;&#23398;&#23478;&#30740;&#31350;&#20102;&#20960;&#21313;&#24180;&#65292;&#21457;&#29616;&#20854;&#32463;&#24120;&#21576;&#29616;&#19982;&#20154;&#31867;&#22788;&#29702;&#31038;&#20250;&#20851;&#31995;&#19968;&#33268;&#30340;&#26576;&#20123;&#32467;&#26500;&#24615;&#20559;&#35265;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#25105;&#20204;&#24456;&#23569;&#20102;&#35299;LLMs&#22312;&#31867;&#20284;&#22270;&#24418;&#21484;&#22238;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65306;&#23427;&#20204;&#21484;&#22238;&#30340;&#22270;&#24418;&#26159;&#21542;&#20063;&#21576;&#29616;&#26576;&#20123;&#20559;&#35265;&#27169;&#24335;&#65292;&#22914;&#26524;&#26159;&#65292;&#23427;&#20204;&#19982;&#20154;&#31867;&#30340;&#34920;&#29616;&#26377;&#20309;&#19981;&#21516;&#24182;&#22914;&#20309;&#24433;&#21709;&#20854;&#20182;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#65311;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#23545;LLMs&#36827;&#34892;&#22270;&#24418;&#21484;&#22238;&#30340;&#31995;&#32479;&#30740;&#31350;&#65292;&#30740;&#31350;&#20854;&#20934;&#30830;&#24615;&#21644;&#20559;&#35265;&#24494;&#32467;&#26500;&#65288;&#23616;&#37096;&#32467;&#26500;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11821v1 Announce Type: cross  Abstract: Graphs data is crucial for many applications, and much of it exists in the relations described in textual format. As a result, being able to accurately recall and encode a graph described in earlier text is a basic yet pivotal ability that LLMs need to demonstrate if they are to perform reasoning tasks that involve graph-structured information. Human performance at graph recall by has been studied by cognitive scientists for decades, and has been found to often exhibit certain structural patterns of bias that align with human handling of social relationships. To date, however, we know little about how LLMs behave in analogous graph recall tasks: do their recalled graphs also exhibit certain biased patterns, and if so, how do they compare with humans and affect other graph reasoning tasks? In this work, we perform the first systematical study of graph recall by LLMs, investigating the accuracy and biased microstructures (local structura
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22836;&#37096;&#20849;&#20139;&#27880;&#24847;&#21147;&#30340;&#35266;&#28857;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#27880;&#24847;&#21147;&#22836;&#20043;&#38388;&#20849;&#20139;&#21442;&#25968;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#24040;&#22823;&#23548;&#33268;&#37096;&#32626;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11819</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#29992;&#20110;&#22836;&#37096;&#20849;&#20139;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Head-wise Shareable Attention for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22836;&#37096;&#20849;&#20139;&#27880;&#24847;&#21147;&#30340;&#35266;&#28857;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#27880;&#24847;&#21147;&#22836;&#20043;&#38388;&#20849;&#20139;&#21442;&#25968;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#24040;&#22823;&#23548;&#33268;&#37096;&#32626;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30001;&#20110;&#21442;&#25968;&#25968;&#37327;&#24040;&#22823;&#21463;&#21040;&#38480;&#21046;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#12290;&#21442;&#25968;&#20849;&#20139;&#26159;&#19968;&#31181;&#26377;&#21033;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#40723;&#21169;&#26435;&#37325;&#37325;&#29992;&#65292;&#26377;&#25928;&#22320;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#37327;&#24182;&#38477;&#20302;&#24615;&#33021;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21442;&#25968;&#20849;&#20139;&#25216;&#26415;&#20027;&#35201;&#19987;&#27880;&#20110;&#20687;BERT&#36825;&#26679;&#30340;&#23567;&#35268;&#27169;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#31895;&#31890;&#24230;&#30340;&#20849;&#20139;&#35268;&#21017;&#65292;&#20363;&#22914;&#36880;&#23618;&#20849;&#20139;&#12290;&#37492;&#20110;LLMs&#30340;&#26222;&#21450;&#65292;&#36825;&#21464;&#24471;&#26377;&#38480;&#65292;&#24182;&#19988;&#20849;&#20139;&#25972;&#20010;&#23618;&#25110;&#22359;&#26174;&#28982;&#38477;&#20302;&#20102;&#21442;&#25968;&#20849;&#20139;&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textbf{&#38754;&#21521;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22836;&#37096;&#20849;&#20139;&#27880;&#24847;&#21147;}$&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#31181;&#22312;&#27880;&#24847;&#21147;&#22836;&#20043;&#38388;&#20849;&#20139;&#21442;&#25968;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;LLMs&#12290;&#23427;&#20204;&#37117;&#20351;&#29992;&#30456;&#21516;&#30340;&#21160;&#24577;&#31574;&#30053;&#36873;&#25321;&#20849;&#20139;&#30340;&#21442;&#25968;&#30697;&#38453;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#30452;&#25509;&#37325;&#22797;&#20351;&#29992;&#39044;&#35757;&#32451;&#26435;&#37325;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11819v1 Announce Type: new  Abstract: Large Language Models (LLMs) suffer from huge number of parameters, which restricts their deployment on edge devices. Weight sharing is one promising solution that encourages weight reuse, effectively reducing memory usage with less performance drop. However, current weight sharing techniques primarily focus on small-scale models like BERT and employ coarse-grained sharing rules, e.g., layer-wise. This becomes limiting given the prevalence of LLMs and sharing an entire layer or block obviously diminishes the flexibility of weight sharing. In this paper, we present a perspective on $\textit{$\textbf{head-wise shareable attention for large language models}$}$. We further propose two memory-efficient methods that share parameters across attention heads, with a specific focus on LLMs. Both of them use the same dynamic strategy to select the shared weight matrices. The first method directly reuses the pre-trained weights without retraining, d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NewsSerow&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#29615;&#22659;&#20445;&#25252;&#20869;&#23481;&#65292;&#24182;&#19988;&#22312;&#23612;&#27850;&#23572;&#35821;&#20013;&#20351;&#29992;&#23569;&#20110;10&#20010;&#31034;&#20363;&#26032;&#38395;&#25991;&#31456;&#26102;&#65292;NewsSerow&#26174;&#30528;&#20248;&#20110;&#20854;&#20182;&#23569;&#26679;&#26412;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11818</link><description>&lt;p&gt;
&#22312;&#30495;&#27491;&#37325;&#35201;&#30340;&#22320;&#26041;&#65306;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#23569;&#26679;&#26412;&#29615;&#22659;&#20445;&#25252;&#23186;&#20307;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
Where It Really Matters: Few-Shot Environmental Conservation Media Monitoring for Low-Resource Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11818
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NewsSerow&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#29615;&#22659;&#20445;&#25252;&#20869;&#23481;&#65292;&#24182;&#19988;&#22312;&#23612;&#27850;&#23572;&#35821;&#20013;&#20351;&#29992;&#23569;&#20110;10&#20010;&#31034;&#20363;&#26032;&#38395;&#25991;&#31456;&#26102;&#65292;NewsSerow&#26174;&#30528;&#20248;&#20110;&#20854;&#20182;&#23569;&#26679;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#20445;&#32452;&#32455;&#24120;&#35268;&#30417;&#27979;&#26377;&#21487;&#33021;&#23545;&#29615;&#22659;&#20135;&#29983;&#24433;&#21709;&#30340;&#20445;&#25252;&#21306;&#23186;&#20307;&#20869;&#23481;&#65292;&#20197;&#20445;&#25345;&#23545;&#21487;&#33021;&#21457;&#23637;&#30340;&#24773;&#20917;&#30340;&#35748;&#35782;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#21270;&#23186;&#20307;&#30417;&#27979;&#31995;&#32479;&#38656;&#35201;&#30001;&#39046;&#22495;&#19987;&#23478;&#26631;&#35760;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#36825;&#22312;&#33521;&#35821;&#31561;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35268;&#27169;&#19978;&#25165;&#26159;&#21487;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#24037;&#20855;&#22312;&#20840;&#29699;&#21335;&#26041;&#26368;&#38656;&#35201;&#65292;&#22312;&#37027;&#37324;&#24863;&#20852;&#36259;&#30340;&#26032;&#38395;&#20027;&#35201;&#26159;&#29992;&#26412;&#22320;&#20302;&#36164;&#28304;&#35821;&#35328;&#21457;&#24067;&#30340;&#65292;&#21487;&#25345;&#32493;&#22320;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#27880;&#37322;&#30340;&#19987;&#23478;&#20063;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;NewsSerow&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35782;&#21035;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#29615;&#22659;&#20445;&#25252;&#20869;&#23481;&#12290;NewsSerow&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24635;&#32467;&#12289;&#19978;&#19979;&#25991;&#23569;&#26679;&#26412;&#20998;&#31867;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#27969;&#31243;&#12290;&#22312;&#23612;&#27850;&#23572;&#35821;&#20013;&#20351;&#29992;&#26368;&#22810;10&#20010;&#26032;&#38395;&#31034;&#20363;&#25991;&#31456;&#65292;NewsSerow&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#23569;&#26679;&#26412;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11818v1 Announce Type: cross  Abstract: Environmental conservation organizations routinely monitor news content on conservation in protected areas to maintain situational awareness of developments that can have an environmental impact. Existing automated media monitoring systems require large amounts of data labeled by domain experts, which is only feasible at scale for high-resource languages like English. However, such tools are most needed in the global south where news of interest is mainly in local low-resource languages, and far fewer experts are available to annotate datasets sustainably. In this paper, we propose NewsSerow, a method to automatically recognize environmental conservation content in low-resource languages. NewsSerow is a pipeline of summarization, in-context few-shot classification, and self-reflection using large language models (LLMs). Using at most 10 demonstration example news articles in Nepali, NewsSerow significantly outperforms other few-shot me
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#23454;&#29616;&#19982;&#22522;&#32447;&#30456;&#24403;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.11815</link><description>&lt;p&gt;
HU&#22312;SemEval-2024&#20219;&#21153;8A&#20013;&#30340;&#34920;&#29616;&#65306;&#23545;&#27604;&#23398;&#20064;&#33021;&#21542;&#23398;&#20064;&#23884;&#20837;&#20197;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#65311;
&lt;/p&gt;
&lt;p&gt;
HU at SemEval-2024 Task 8A: Can Contrastive Learning Learn Embeddings to Detect Machine-Generated Text?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11815
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#23454;&#29616;&#19982;&#22522;&#32447;&#30456;&#24403;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#20026;SemEval-2024&#20219;&#21153;8&#8220;&#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#8221;&#24320;&#21457;&#30340;&#31995;&#32479;&#12290;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#34394;&#20551;&#25991;&#26412;&#29983;&#25104;&#12289;&#32593;&#32476;&#38035;&#40060;&#12289;&#32771;&#35797;&#20316;&#24330;&#29978;&#33267;&#25220;&#34989;&#29256;&#26435;&#26448;&#26009;&#20013;&#30340;&#20351;&#29992;&#65292;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19968;&#30452;&#26159;&#20027;&#35201;&#20851;&#27880;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#35768;&#22810;&#31995;&#32479;&#24050;&#32463;&#34987;&#24320;&#21457;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#20999;&#23454;&#38469;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#36890;&#24120;&#19981;&#21487;&#33021;&#30693;&#36947;&#29992;&#25143;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#20855;&#20307;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#20854;&#20351;&#29992;&#22522;&#32447;&#21442;&#25968;&#30340;&#22823;&#32422;40%&#65288;149M&#27604;355M&#65289;&#65292;&#20294;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#21487;&#27604;&#30340;&#24615;&#33021;&#65288;&#22312;137&#20010;&#21442;&#19982;&#32773;&#20013;&#25490;&#21517;&#31532;21&#65289;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21457;&#29616;&#26159;&#65292;&#21363;&#20351;&#27809;&#26377;&#22810;&#20010;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11815v1 Announce Type: cross  Abstract: This paper describes our system developed for SemEval-2024 Task 8, "Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection." Machine-generated texts have been one of the main concerns due to the use of large language models (LLM) in fake text generation, phishing, cheating in exams, or even plagiarizing copyright materials. A lot of systems have been developed to detect machine-generated text. Nonetheless, the majority of these systems rely on the text-generating model, a limitation that is impractical in real-world scenarios, as it's often impossible to know which specific model the user has used for text generation. In this work, we propose a single model based on contrastive learning, which uses ~40% of the baseline's parameters (149M vs. 355M) but shows a comparable performance on the test dataset (21st out of 137 participants). Our key finding is that even without an ensemble of multiple models, a
&lt;/p&gt;</description></item><item><title>FIPO&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#30001;&#24418;&#24335;&#25351;&#23548;&#30340;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20559;&#22909;&#25968;&#25454;&#38598;&#21644;&#27169;&#22359;&#21270;&#24494;&#35843;&#27169;&#24335;&#65292;&#37325;&#26032;&#26500;&#24605;&#20102;&#20248;&#21270;&#36807;&#31243;&#24182;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#20219;&#21153;&#25552;&#31034;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.11811</link><description>&lt;p&gt;
FIPO&#65306;&#22522;&#20110;&#33258;&#30001;&#24418;&#24335;&#25351;&#23548;&#30340;&#25552;&#31034;&#20248;&#21270;&#19982;&#20559;&#22909;&#25968;&#25454;&#38598;&#21644;&#27169;&#22359;&#21270;&#24494;&#35843;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
FIPO: Free-form Instruction-oriented Prompt Optimization with Preference Dataset and Modular Fine-tuning Schema
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11811
&lt;/p&gt;
&lt;p&gt;
FIPO&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#30001;&#24418;&#24335;&#25351;&#23548;&#30340;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#65292;&#32467;&#21512;&#20559;&#22909;&#25968;&#25454;&#38598;&#21644;&#27169;&#22359;&#21270;&#24494;&#35843;&#27169;&#24335;&#65292;&#37325;&#26032;&#26500;&#24605;&#20102;&#20248;&#21270;&#36807;&#31243;&#24182;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#20219;&#21153;&#25552;&#31034;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20419;&#36827;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#26368;&#32456;&#29992;&#25143;-&#26426;&#22120;&#20154;&#20132;&#20114;&#20013;&#30340;&#28145;&#24230;&#26234;&#33021;&#26041;&#38754;&#65292;&#25552;&#31034;&#21019;&#20316;&#30340;&#33402;&#26415;&#34987;&#35270;&#20026;&#26222;&#36890;&#29992;&#25143;&#30340;&#19968;&#39033;&#20851;&#38190;&#20294;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#19982;&#20043;&#21069;&#22522;&#20110;&#27169;&#22411;&#32780;&#19981;&#32771;&#34385;&#25351;&#23548;&#30340;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26041;&#27861;&#24418;&#25104;&#23545;&#27604;&#65292;&#36825;&#20123;&#26041;&#27861;&#20026;&#39044;&#23450;&#20041;&#30446;&#26631;&#27169;&#22411;&#20135;&#29983;&#20102;&#20809;&#28369;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#20351;&#29992;&#24320;&#31665;&#21363;&#29992;&#27169;&#22411;&#26102;&#23481;&#26131;&#24555;&#36895;&#36864;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#30001;&#24418;&#24335;&#25351;&#23548;&#30340;&#25552;&#31034;&#20248;&#21270;&#65288;FIPO&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#24471;&#21040;&#25105;&#20204;&#30340;&#22823;&#35268;&#27169;&#25552;&#31034;&#20559;&#22909;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#65292;&#24182;&#37319;&#29992;&#27169;&#22359;&#21270;&#24494;&#35843;&#27169;&#24335;&#12290;FIPO&#27169;&#24335;&#37325;&#26032;&#26500;&#24605;&#20102;&#20248;&#21270;&#36807;&#31243;&#65292;&#23558;&#20854;&#20998;&#35299;&#20026;&#21487;&#31649;&#29702;&#30340;&#27169;&#22359;&#65292;&#20197;&#21160;&#24577;&#35843;&#25972;&#20869;&#23481;&#30340;&#20803;&#25552;&#31034;&#20026;&#38170;&#28857;&#12290;&#36825;&#20801;&#35768;&#28789;&#27963;&#25972;&#21512;&#21407;&#22987;&#20219;&#21153;&#25351;&#23548;&#12289;&#21487;&#36873;&#25351;&#23548;&#21709;&#24212;&#21644;&#21487;&#36873;&#30495;&#23454;&#20540;&#65292;&#20197;&#29983;&#25104;&#32463;&#36807;&#31934;&#24515;&#20248;&#21270;&#30340;&#20219;&#21153;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11811v1 Announce Type: new  Abstract: In the quest to facilitate the deep intelligence of Large Language Models (LLMs) accessible in final-end user-bot interactions, the art of prompt crafting emerges as a critical yet complex task for the average user. Contrast to previous model-oriented yet instruction-agnostic Automatic Prompt Optimization methodologies, yielding polished results for predefined target models while suffering rapid degradation with out-of-box models, we present Free-form Instruction-oriented Prompt Optimization (FIPO). This approach is supported by our large-scale prompt preference dataset and employs a modular fine-tuning schema. The FIPO schema reimagines the optimization process into manageable modules, anchored by a meta prompt that dynamically adapts content. This allows for the flexible integration of the raw task instruction, the optional instruction response, and the optional ground truth to produce finely optimized task prompts. The FIPO preference
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Smart Parallel Auto-Correct Decoding (SPACE)&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21152;&#36895;&#21644;&#24182;&#34892;&#29983;&#25104;&#39564;&#35777;&#20196;&#29260;&#30340;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11809</link><description>&lt;p&gt;
&#26234;&#33021;&#24182;&#34892;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#65306;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generation Meets Verification: Accelerating Large Language Model Inference with Smart Parallel Auto-Correct Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Smart Parallel Auto-Correct Decoding (SPACE)&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21152;&#36895;&#21644;&#24182;&#34892;&#29983;&#25104;&#39564;&#35777;&#20196;&#29260;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#21152;&#36895;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26234;&#33021;&#24182;&#34892;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#8221;&#65288;SPACE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;LLMs&#30340;&#26080;&#25439;&#21152;&#36895;&#12290;&#36890;&#36807;&#38598;&#25104;&#21322;&#33258;&#22238;&#24402;&#25512;&#29702;&#21644;&#29468;&#27979;&#35299;&#30721;&#33021;&#21147;&#65292;SPACE&#29420;&#29305;&#22320;&#20351;&#33258;&#22238;&#24402;LLMs&#33021;&#22815;&#24182;&#34892;&#29983;&#25104;&#21644;&#39564;&#35777;&#20196;&#29260;&#12290;&#36825;&#26159;&#36890;&#36807;&#19987;&#38376;&#30340;&#21322;&#33258;&#22238;&#24402;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#30340;&#65292;&#35813;&#36807;&#31243;&#20351;&#29616;&#26377;LLMs&#20855;&#26377;&#21516;&#26102;&#39044;&#27979;&#22810;&#20010;&#20196;&#29260;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#19968;&#31181;&#33258;&#21160;&#32416;&#38169;&#35299;&#30721;&#31639;&#27861;&#20419;&#36827;&#20102;&#21333;&#20010;&#27169;&#22411;&#35843;&#29992;&#20869;&#20196;&#29260;&#24207;&#21015;&#30340;&#21516;&#26102;&#29983;&#25104;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;&#19968;&#31995;&#21015;LLMs&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;SPACE&#22312;HumanEval-X&#19978;&#34920;&#29616;&#20986;2.7&#20493;&#33267;4.0&#20493;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11809v1 Announce Type: cross  Abstract: This research aims to accelerate the inference speed of large language models (LLMs) with billions of parameters. We propose \textbf{S}mart \textbf{P}arallel \textbf{A}uto-\textbf{C}orrect d\textbf{E}coding (SPACE), an innovative approach designed for achieving lossless acceleration of LLMs. By integrating semi-autoregressive inference and speculative decoding capabilities, SPACE uniquely enables autoregressive LLMs to parallelize token generation and verification. This is realized through a specialized semi-autoregressive supervised fine-tuning process that equips existing LLMs with the ability to simultaneously predict multiple tokens. Additionally, an auto-correct decoding algorithm facilitates the simultaneous generation and verification of token sequences within a single model invocation. Through extensive experiments on a range of LLMs, SPACE has demonstrated inference speedup ranging from 2.7x-4.0x on HumanEval-X while maintaini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#22270;&#24418;&#32467;&#26500;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#35265;&#35299;&#65292;&#23454;&#29616;&#20102;&#22312;&#20219;&#24847;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;&#30340;&#39640;&#36890;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11804</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#25552;&#31034;&#22120;&#65306;&#22312;&#20219;&#24847;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
LLM as Prompter: Low-resource Inductive Reasoning on Arbitrary Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#22270;&#24418;&#32467;&#26500;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#35265;&#35299;&#65292;&#23454;&#29616;&#20102;&#22312;&#20219;&#24847;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;&#30340;&#39640;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#65288;KG&#65289;&#24402;&#32435;&#25512;&#29702;&#26088;&#22312;&#25512;&#26029;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;KG&#20013;&#32570;&#22833;&#30340;&#20107;&#23454;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;KG&#24402;&#32435;&#25512;&#29702;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22788;&#29702;&#22312;&#25991;&#26412;&#21644;&#32467;&#26500;&#26041;&#38754;&#37117;&#31232;&#32570;&#30340;&#20302;&#36164;&#28304;&#22330;&#26223;&#12290;&#26412;&#25991;&#23581;&#35797;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;LLMs&#29983;&#25104;&#22270;&#24418;&#32467;&#26500;&#25552;&#31034;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#65292;&#20174;&#32780;&#20026;KG&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;&#24102;&#26469;&#26032;&#30340;&#26041;&#27861;&#35770;&#35265;&#35299;&#65292;&#20197;&#21450;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#26222;&#36866;&#24615;&#12290;&#22312;&#26041;&#27861;&#35770;&#26041;&#38754;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#21644;&#25552;&#31034;&#26694;&#26550;ProLINK&#65292;&#26088;&#22312;&#22312;&#20219;&#24847;KG&#19978;&#36827;&#34892;&#20302;&#36164;&#28304;&#24402;&#32435;&#25512;&#29702;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;&#22312;&#23454;&#36341;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312;36&#20010;&#20302;&#36164;&#28304;&#25968;&#25454;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11804v1 Announce Type: new  Abstract: Knowledge Graph (KG) inductive reasoning, which aims to infer missing facts from new KGs that are not seen during training, has been widely adopted in various applications. One critical challenge of KG inductive reasoning is handling low-resource scenarios with scarcity in both textual and structural aspects. In this paper, we attempt to address this challenge with Large Language Models (LLMs). Particularly, we utilize the state-of-the-art LLMs to generate a graph-structural prompt to enhance the pre-trained Graph Neural Networks (GNNs), which brings us new methodological insights into the KG inductive reasoning methods, as well as high generalizability in practice. On the methodological side, we introduce a novel pretraining and prompting framework ProLINK, designed for low-resource inductive reasoning across arbitrary KGs without requiring additional training. On the practical side, we experimentally evaluate our approach on 36 low-res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#27880;&#24847;&#21147;&#31934;&#28860;&#30340;&#25104;&#21151;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#30340;&#25351;&#26631;.</title><link>https://arxiv.org/abs/2402.11794</link><description>&lt;p&gt;
&#25581;&#31034;&#39764;&#27861;&#65306;&#25506;&#31350;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#27880;&#24847;&#21147;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Magic: Investigating Attention Distillation in Retrieval-augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#30340;&#27880;&#24847;&#21147;&#31934;&#28860;&#30340;&#25104;&#21151;&#26426;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#30340;&#25351;&#26631;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#21487;&#20197;&#36890;&#36807;&#23454;&#26102;&#30693;&#35782;&#26356;&#26032;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23616;&#38480;&#65292;&#20197;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#22312;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#30340;&#35757;&#32451;&#38454;&#27573;&#20013;&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#26159;&#27880;&#24847;&#21147;&#31934;&#28860;&#65292;&#23427;&#20351;&#29992;&#27880;&#24847;&#21147;&#20998;&#25968;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65292;&#32780;&#19981;&#26159;&#25163;&#21160;&#27880;&#37322;&#30340;&#26597;&#35810;&#25991;&#26723;&#23545;&#12290;&#23613;&#31649;&#27880;&#24847;&#21147;&#31934;&#28860;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#22312;&#23427;&#25104;&#21151;&#32972;&#21518;&#30340;&#35814;&#32454;&#26426;&#21046;&#20173;&#26410;&#34987;&#25506;&#32034;&#65292;&#29305;&#21035;&#26159;&#23427;&#21033;&#29992;&#20197;&#21463;&#30410;&#20110;&#35757;&#32451;&#30340;&#20855;&#20307;&#27169;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27880;&#24847;&#21147;&#31934;&#28860;&#24037;&#20316;&#27969;&#31243;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#35782;&#21035;&#24433;&#21709;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#36136;&#37327;&#30340;&#20851;&#38190;&#22240;&#32032;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20248;&#21270;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#21644;&#36991;&#20813;&#20302;&#25928;&#35757;&#32451;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11794v1 Announce Type: new  Abstract: Retrieval-augmented generation framework can address the limitations of large language models by enabling real-time knowledge updates for more accurate answers. An efficient way in the training phase of retrieval-augmented models is attention distillation, which uses attention scores as a supervision signal instead of manually annotated query-document pairs. Despite its growing popularity, the detailed mechanisms behind the success of attention distillation remain unexplored, particularly the specific patterns it leverages to benefit training. In this paper, we address this gap by conducting a comprehensive review of attention distillation workflow and identifying key factors influencing the learning quality of retrieval-augmented language models. We further propose indicators for optimizing models' training methods and avoiding ineffective training.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314; ConflictingQA &#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#25935;&#24863;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#26102;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#32593;&#31449;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#20154;&#31867;&#35748;&#20026;&#37325;&#35201;&#30340;&#25991;&#26412;&#39118;&#26684;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.11782</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#35748;&#20026;&#21738;&#20123;&#35777;&#25454;&#20196;&#20154;&#20449;&#26381;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Evidence Do Language Models Find Convincing?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11782
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314; ConflictingQA &#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#25935;&#24863;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#26102;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#32593;&#31449;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#20154;&#31867;&#35748;&#20026;&#37325;&#35201;&#30340;&#25991;&#26412;&#39118;&#26684;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#36171;&#20104;&#20027;&#35266;&#12289;&#26377;&#20105;&#35758;&#21644;&#30683;&#30462;&#30340;&#26597;&#35810;&#20219;&#21153;&#65292;&#22914;&#8220;&#38463;&#26031;&#24052;&#29980;&#26159;&#21542;&#19982;&#30284;&#30151;&#26377;&#20851;&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#27169;&#31946;&#30340;&#26597;&#35810;&#65292;&#25105;&#20204;&#24517;&#39035;&#25628;&#32034;&#22823;&#37327;&#32593;&#31449;&#65292;&#24182;&#32771;&#34385;&#8220;&#25105;&#35748;&#20026;&#21738;&#20123;&#35777;&#25454;&#26159;&#20196;&#20154;&#20449;&#26381;&#30340;&#65311;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#22914;&#20309;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#30340;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026; ConflictingQA &#30340;&#25968;&#25454;&#38598;&#65292;&#23558;&#26377;&#20105;&#35758;&#30340;&#26597;&#35810;&#19982;&#19968;&#31995;&#21015;&#21253;&#21547;&#19981;&#21516;&#20107;&#23454;&#65288;&#22914;&#23450;&#37327;&#32467;&#26524;&#65289;&#12289;&#35770;&#35777;&#39118;&#26684;&#65288;&#22914;&#26435;&#23041;&#21628;&#22768;&#65289;&#21644;&#31572;&#26696;&#65288;&#26159;&#25110;&#21542;&#65289;&#30340;&#30495;&#23454;&#19990;&#30028;&#35777;&#25454;&#25991;&#26723;&#37197;&#23545;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#25935;&#24863;&#24615;&#21644;&#21453;&#20107;&#23454;&#20998;&#26512;&#65292;&#25506;&#35752;&#21738;&#20123;&#25991;&#26412;&#29305;&#24449;&#26368;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27169;&#22411;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#32593;&#31449;&#19982;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#20154;&#31867;&#35748;&#20026;&#37325;&#35201;&#30340;&#39118;&#26684;&#29305;&#24449;&#65292;&#27604;&#22914;&#25991;&#26412;&#26159;&#21542;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11782v1 Announce Type: new  Abstract: Retrieval-augmented language models are being increasingly tasked with subjective, contentious, and conflicting queries such as "is aspartame linked to cancer". To resolve these ambiguous queries, one must search through a large range of websites and consider "which, if any, of this evidence do I find convincing?". In this work, we study how LLMs answer this question. In particular, we construct ConflictingQA, a dataset that pairs controversial queries with a series of real-world evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No). We use this dataset to perform sensitivity and counterfactual analyses to explore which text features most affect LLM predictions. Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;ETHICS Utilitarianism&#20219;&#21153;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#38544;&#21547;&#20102;&#23545;&#20154;&#31867;&#31119;&#31049;&#30340;&#29702;&#35299;&#65292;&#19988;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#26102;&#65292;&#20934;&#30830;&#29575;&#21576;&#38750;&#19979;&#38477;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.11777</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#20013;&#25581;&#31034;&#28508;&#22312;&#30340;&#20154;&#31867;&#31119;&#31049;
&lt;/p&gt;
&lt;p&gt;
Uncovering Latent Human Wellbeing in Language Model Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;ETHICS Utilitarianism&#20219;&#21153;&#21457;&#29616;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#38544;&#21547;&#20102;&#23545;&#20154;&#31867;&#31119;&#31049;&#30340;&#29702;&#35299;&#65292;&#19988;&#27169;&#22411;&#35268;&#27169;&#22686;&#21152;&#26102;&#65292;&#20934;&#30830;&#29575;&#21576;&#38750;&#19979;&#38477;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#38544;&#21547;&#22320;&#23398;&#20064;&#20102;&#20154;&#31867;&#31119;&#31049;&#30340;&#27010;&#24565;&#65311;&#25105;&#20204;&#36890;&#36807;ETHICS&#21151;&#21033;&#20027;&#20041;&#20219;&#21153;&#36827;&#34892;&#25506;&#35752;&#65292;&#35780;&#20272;&#32553;&#25918;&#26159;&#21542;&#22686;&#24378;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#21457;&#29616;&#26174;&#31034;&#65292;&#26080;&#38656;&#20219;&#20309;&#25552;&#31034;&#24037;&#31243;&#25110;&#24494;&#35843;&#65292;OpenAI&#30340;text-embedding-ada-002&#30340;&#20027;&#25104;&#20998;&#36798;&#21040;73.9%&#30340;&#20934;&#30830;&#29575;&#12290;&#36825;&#19982;&#22312;&#25972;&#20010;ETHICS&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;BERT-large&#27169;&#22411;&#30340;74.6%&#20934;&#30830;&#29575;&#38750;&#24120;&#25509;&#36817;&#65292;&#34920;&#26126;&#39044;&#35757;&#32451;&#20256;&#36798;&#20102;&#23545;&#20154;&#31867;&#31119;&#31049;&#30340;&#26576;&#31181;&#29702;&#35299;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22235;&#31181;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#35266;&#23519;&#21151;&#21033;&#20027;&#20041;&#20934;&#30830;&#29575;&#38543;&#21442;&#25968;&#22686;&#21152;&#32780;&#21464;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#36275;&#22815;&#25968;&#37327;&#30340;&#20027;&#25104;&#20998;&#26102;&#65292;&#24615;&#33021;&#38543;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#32780;&#38750;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11777v1 Announce Type: cross  Abstract: Do language models implicitly learn a concept of human wellbeing? We explore this through the ETHICS Utilitarianism task, assessing if scaling enhances pretrained models' representations. Our initial finding reveals that, without any prompt engineering or finetuning, the leading principal component from OpenAI's text-embedding-ada-002 achieves 73.9% accuracy. This closely matches the 74.6% of BERT-large finetuned on the entire ETHICS dataset, suggesting pretraining conveys some understanding about human wellbeing. Next, we consider four language model families, observing how Utilitarianism accuracy varies with increased parameters. We find performance is nondecreasing with increased model size when using sufficient numbers of principal components.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#32467;&#26500;&#21270;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#29983;&#25104;&#20869;&#23481;&#30456;&#20851;&#30340;&#38382;&#31572;&#23545;&#35805;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#31243;&#24207;&#23545;&#22522;&#30784;&#25991;&#26723;&#30340;&#24544;&#35802;&#24230;&#65292;&#35757;&#32451;&#24378;&#22823;&#30340;&#23545;&#35805;&#38382;&#31572;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.11770</link><description>&lt;p&gt;
&#38754;&#21521;&#23569;&#26679;&#26412;&#29983;&#25104;&#30340;&#20869;&#23481;&#30456;&#20851;&#38382;&#31572;&#23545;&#35805;&#32467;&#26500;&#21270;&#24605;&#32500;&#21551;&#21457;
&lt;/p&gt;
&lt;p&gt;
Structured Chain-of-Thought Prompting for Few-Shot Generation of Content-Grounded QA Conversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11770
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#21270;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#29983;&#25104;&#20869;&#23481;&#30456;&#20851;&#30340;&#38382;&#31572;&#23545;&#35805;&#65292;&#25552;&#39640;&#20102;&#20195;&#29702;&#31243;&#24207;&#23545;&#22522;&#30784;&#25991;&#26723;&#30340;&#24544;&#35802;&#24230;&#65292;&#35757;&#32451;&#24378;&#22823;&#30340;&#23545;&#35805;&#38382;&#31572;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#24605;&#32500;&#38142;&#65288;SCoT&#65289;&#25552;&#31034;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#22522;&#20110;&#20869;&#23481;&#30340;&#22810;&#36718;&#38382;&#31572;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#30340;&#26680;&#24515;&#26159;&#23558;&#22797;&#26434;&#20219;&#21153;&#32467;&#26500;&#21270;&#20998;&#35299;&#20026;&#29366;&#24577;&#26426;&#20013;&#30340;&#22810;&#20010;&#29366;&#24577;&#65292;&#20197;&#20415;&#25191;&#34892;&#23545;&#24212;&#20110;&#21508;&#31181;&#23376;&#20219;&#21153;&#65288;&#20363;&#22914;&#20869;&#23481;&#38405;&#35835;&#21644;&#35805;&#35821;&#29983;&#25104;&#65289;&#30340;&#21160;&#20316;&#12290;&#27599;&#20010;&#29366;&#24577;&#21033;&#29992;&#19968;&#32452;&#29420;&#29305;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#25552;&#31034;&#21644;&#65288;&#21487;&#36873;&#65289;&#39069;&#22806;&#24037;&#20855;&#20197;&#22686;&#24378;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#24187;&#35273;&#20943;&#36731;&#65292;&#20351;&#29992;&#20855;&#26377;&#25351;&#23450;&#29366;&#24577;&#30340;SCoT&#25552;&#31034;&#21487;&#20197;&#20351;&#23545;&#25509;&#22320;&#25991;&#26723;&#30340;&#20195;&#29702;&#24544;&#35802;&#24230;&#25552;&#39640;&#39640;&#36798;16.8&#65285;&#12290;&#24403;&#29992;&#20316;&#35757;&#32451;&#25968;&#25454;&#26102;&#65292;&#20165;&#20174;6&#20010;&#22522;&#20110;&#32500;&#22522;&#30334;&#31185;&#30340;&#31181;&#23376;&#31034;&#33539;&#21512;&#25104;&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#35757;&#32451;&#20986;&#24378;&#22823;&#30340;&#23545;&#35805;&#38382;&#31572;&#20195;&#29702;&#31243;&#24207;&#65307;&#22312;&#39046;&#22495;&#22806;&#35780;&#20272;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11770v1 Announce Type: new  Abstract: We introduce a structured chain-of-thought (SCoT) prompting approach to generating content-grounded multi-turn question-answer conversations using a pre-trained large language model (LLM). At the core of our proposal is a structured breakdown of the complex task into a number of states in a state machine, so that actions corresponding to various subtasks, e.g., content reading and utterance generation, can be executed in their own dedicated states. Each state leverages a unique set of resources including prompts and (optionally) additional tools to augment the generation process. Our experimental results show that SCoT prompting with designated states for hallucination mitigation increases agent faithfulness to grounding documents by up to 16.8%. When used as training data, our open-domain conversations synthesized from only 6 Wikipedia-based seed demonstrations train strong conversational QA agents; in out-of-domain evaluation, for exam
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ChatGPT&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#26469;&#22686;&#24378;LLMs&#21435;&#20559;&#35265;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#21435;&#38500;&#24050;&#30693;&#20559;&#35265;&#24182;&#36328;&#36234;&#19981;&#21516;&#31867;&#21035;&#36827;&#34892;&#21435;&#20559;&#35265;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.11764</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#29992;&#20110;&#25913;&#21892;LLMs&#30340;&#21442;&#25968;&#39640;&#25928;&#21435;&#20559;&#35265;&#21270;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Based Data Augmentation for Improved Parameter-Efficient Debiasing of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;ChatGPT&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#26469;&#22686;&#24378;LLMs&#21435;&#20559;&#35265;&#21270;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#21435;&#38500;&#24050;&#30693;&#20559;&#35265;&#24182;&#36328;&#36234;&#19981;&#21516;&#31867;&#21035;&#36827;&#34892;&#21435;&#20559;&#35265;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34429;&#28982;&#21151;&#33021;&#24378;&#22823;&#65292;&#20294;&#23384;&#22312;&#26377;&#23475;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#30001;&#20110;&#35745;&#31639;&#25104;&#26412;&#12289;&#25968;&#25454;&#32422;&#26463;&#21644;&#21487;&#33021;&#38477;&#20302;&#22810;&#20219;&#21153;&#35821;&#35328;&#33021;&#21147;&#65292;&#21435;&#20559;&#35265;&#21270;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;ChatGPT&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;LLMs&#30340;&#21435;&#20559;&#35265;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#65306;&#30446;&#26631;&#25552;&#31034;&#65292;&#23545;&#24050;&#30693;&#20559;&#35265;&#25552;&#20379;&#26377;&#25928;&#30340;&#21435;&#20559;&#35265;&#21270;&#65292;&#20294;&#38656;&#35201;&#20107;&#20808;&#25351;&#23450;&#38382;&#39064;&#20013;&#30340;&#20559;&#35265;; &#19968;&#33324;&#25552;&#31034;&#65292;&#34429;&#28982;&#25928;&#26524;&#31245;&#36874;&#65292;&#20294;&#33021;&#22815;&#36328;&#21508;&#31181;&#31867;&#21035;&#36827;&#34892;&#21435;&#20559;&#35265;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;&#36866;&#37197;&#22120;&#35843;&#25972;&#26469;&#23454;&#29616;&#36164;&#28304;&#39640;&#25928;&#30340;LLM&#21435;&#20559;&#35265;&#21270;&#65292;&#24182;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#21512;&#25104;&#25968;&#25454;&#19982;&#29616;&#26377;&#21435;&#20559;&#35265;&#21270;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;ChatGPT&#21487;&#20197;&#39640;&#25928;&#22320;&#29983;&#25104;&#29992;&#20110;&#21435;&#20559;&#35265;&#21270;&#20854;&#20182;LLMs&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#65307;&#65288;2&#65289;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#25968;&#25454;&#36229;&#36234;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#21435;&#20559;&#35265;&#21270;&#19978;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11764v1 Announce Type: cross  Abstract: Large Language models (LLMs), while powerful, exhibit harmful social biases. Debiasing is often challenging due to computational costs, data constraints, and potential degradation of multi-task language capabilities. This work introduces a novel approach utilizing ChatGPT to generate synthetic training data, aiming to enhance the debiasing of LLMs. We propose two strategies: Targeted Prompting, which provides effective debiasing for known biases but necessitates prior specification of bias in question; and General Prompting, which, while slightly less effective, offers debiasing across various categories. We leverage resource-efficient LLM debiasing using adapter tuning and compare the effectiveness of our synthetic data to existing debiasing datasets. Our results reveal that: (1) ChatGPT can efficiently produce high-quality training data for debiasing other LLMs; (2) data produced via our approach surpasses existing datasets in debias
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#35789;&#24178;&#25552;&#21462;&#30340;&#26377;&#21069;&#36884;&#30340;&#24819;&#27861;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#27599;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#25104;&#26412;&#12289;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#20855;&#26377;&#19981;&#21516;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.11757</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#35789;&#24178;&#25552;&#21462;&#65306;&#25215;&#35834;&#12289;&#39118;&#38505;&#21644;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Stemming: Promises, Pitfalls and Failures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#35789;&#24178;&#25552;&#21462;&#30340;&#26377;&#21069;&#36884;&#30340;&#24819;&#27861;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#27599;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#25104;&#26412;&#12289;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#20855;&#26377;&#19981;&#21516;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#35789;&#24178;&#25552;&#21462;&#26159;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;&#21333;&#35789;&#32553;&#20943;&#20026;&#20854;&#22522;&#26412;&#24418;&#24335;&#65292;&#20063;&#31216;&#20026;&#26681;&#24418;&#24335;&#12290;&#20256;&#32479;&#30340;&#35789;&#24178;&#25552;&#21462;&#26041;&#27861;&#20165;&#20851;&#27880;&#21333;&#20010;&#26415;&#35821;&#65292;&#24573;&#35270;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#20016;&#23500;&#24615;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#21033;&#29992;&#20854;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#26469;&#25552;&#21462;&#35789;&#24178;&#30340;&#26377;&#21069;&#36884;&#30340;&#24819;&#27861;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26435;&#34913;&#26041;&#26696;&#65306;&#65288;1&#65289;&#20351;&#29992;LLMs&#23545;&#38598;&#21512;&#30340;&#35789;&#27719;&#36827;&#34892;&#25552;&#21462;&#65292;&#21363;&#20986;&#29616;&#22312;&#38598;&#21512;&#20013;&#30340;&#21807;&#19968;&#21333;&#35789;&#38598;&#21512;&#65288;&#35789;&#27719;&#25552;&#21462;&#65289;&#65292;&#65288;2&#65289;&#23558;LLMs&#29992;&#20110;&#21333;&#29420;&#35789;&#26681;&#25552;&#21462; (&#19978;&#19979;&#25991;&#25552;&#21462;)&#65292;(3) &#20351;&#29992;LLMs&#20174;&#27599;&#20010;&#25991;&#26723;&#20013;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11757v1 Announce Type: cross  Abstract: Text stemming is a natural language processing technique that is used to reduce words to their base form, also known as the root form. The use of stemming in IR has been shown to often improve the effectiveness of keyword-matching models such as BM25. However, traditional stemming methods, focusing solely on individual terms, overlook the richness of contextual information. Recognizing this gap, in this paper, we investigate the promising idea of using large language models (LLMs) to stem words by leveraging its capability of context understanding. With this respect, we identify three avenues, each characterised by different trade-offs in terms of computational cost, effectiveness and robustness : (1) use LLMs to stem the vocabulary for a collection, i.e., the set of unique words that appear in the collection (vocabulary stemming), (2) use LLMs to stem each document separately (contextual stemming), and (3) use LLMs to extract from eac
&lt;/p&gt;</description></item><item><title>MARS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#20989;&#25968;MARS&#65292;&#32771;&#34385;&#20102;&#29983;&#25104;&#24207;&#21015;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#36129;&#29486;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11756</link><description>&lt;p&gt;
MARS&#65306;&#29992;&#20110;&#29983;&#25104;&#24335;LLMs&#20013;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#24847;&#20041;&#24863;&#30693;&#21709;&#24212;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11756
&lt;/p&gt;
&lt;p&gt;
MARS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#20989;&#25968;MARS&#65292;&#32771;&#34385;&#20102;&#29983;&#25104;&#24207;&#21015;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#36129;&#29486;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#21331;&#36234;&#34920;&#29616;&#32780;&#34987;&#24191;&#27867;&#21033;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20135;&#29983;&#19981;&#20934;&#30830;&#25110;&#35823;&#23548;&#24615;&#36755;&#20986;&#30340;&#20542;&#21521;&#21487;&#33021;&#24102;&#26469;&#28508;&#22312;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#12290;&#22240;&#27492;&#65292;&#20272;&#35745;&#29983;&#25104;&#24335;LLM&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#26159;&#22686;&#24378;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29983;&#25104;&#24335;LLMs&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65288;UE&#65289;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#20854;&#20013;SOTA&#22522;&#20110;&#27010;&#29575;&#30340;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#38271;&#24230;&#26631;&#20934;&#21270;&#35780;&#20998;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24847;&#20041;&#24863;&#30693;&#21709;&#24212;&#35780;&#20998;&#65288;MARS&#65289;&#30340;&#26367;&#20195;&#38271;&#24230;&#26631;&#20934;&#21270;&#35780;&#20998;&#30340;UE&#26041;&#27861;&#12290;MARS&#26159;&#19968;&#31181;&#32771;&#34385;&#22312;&#38382;&#39064;&#30340;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#24207;&#21015;&#20013;&#27599;&#20010;&#26631;&#35760;&#30340;&#35821;&#20041;&#36129;&#29486;&#30340;&#26032;&#22411;&#35780;&#20998;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#23558;MARS&#25972;&#21512;&#21040;UE&#26041;&#27861;&#20013;&#20250;&#22312;UE&#24615;&#33021;&#19978;&#24102;&#26469;&#26222;&#36941;&#21644;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#38381;&#21367;&#24335;&#38382;&#31572;&#26469;&#36827;&#34892;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11756v1 Announce Type: new  Abstract: Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book questi
&lt;/p&gt;</description></item><item><title>SPML&#26159;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#25552;&#31034;&#24182;&#30417;&#25511;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#36755;&#20837;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#29992;&#20110;&#38450;&#24481;&#24694;&#24847;&#25915;&#20987;&#24182;&#20248;&#21270;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.11755</link><description>&lt;p&gt;
SPML: &#19968;&#31181;&#29992;&#20110;&#38450;&#24481;&#35821;&#35328;&#27169;&#22411;&#21463;&#21040;&#25552;&#31034;&#25915;&#20987;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
SPML: A DSL for Defending Language Models Against Prompt Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11755
&lt;/p&gt;
&lt;p&gt;
SPML&#26159;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#25552;&#31034;&#24182;&#30417;&#25511;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#36755;&#20837;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#29992;&#20110;&#38450;&#24481;&#24694;&#24847;&#25915;&#20987;&#24182;&#20248;&#21270;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#28145;&#21051;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#24212;&#29992;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#20110;&#22522;&#20110;&#25351;&#20196;&#30340;&#23450;&#20041;&#26469;&#35774;&#35745;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#37096;&#32626;&#21518;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#23450;&#20041;&#26159;&#22266;&#23450;&#30340;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#29992;&#25143;&#30340;&#25915;&#20987;&#65292;&#31361;&#20986;&#20102;&#38450;&#27490;&#19981;&#36947;&#24503;&#24212;&#29992;&#21644;&#36130;&#21153;&#25439;&#22833;&#30340;&#38656;&#35201;&#12290;&#29616;&#26377;&#30740;&#31350;&#25506;&#35752;&#20102;&#29992;&#25143;&#25552;&#31034;&#23545;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24433;&#21709;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#21253;&#21547;&#24212;&#29992;&#29305;&#23450;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25915;&#20987;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31995;&#32479;&#25552;&#31034;&#20803;&#35821;&#35328;&#65288;SPML&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20248;&#21270;&#25552;&#31034;&#24182;&#30417;&#25511;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#36755;&#20837;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#12290;SPML&#20027;&#21160;&#26816;&#26597;&#25915;&#20987;&#25552;&#31034;&#65292;&#30830;&#20445;&#29992;&#25143;&#36755;&#20837;&#19982;&#32842;&#22825;&#26426;&#22120;&#20154;&#23450;&#20041;&#30456;&#31526;&#65292;&#38450;&#27490;&#22312;LLM&#20027;&#24178;&#19978;&#23545;&#20854;&#36827;&#34892;&#24694;&#24847;&#25191;&#34892;&#65292;&#20248;&#21270;&#25104;&#26412;&#12290;&#23427;&#20063;&#36890;&#36807;&#32534;&#31243;&#35821;&#35328;&#33021;&#21147;&#31616;&#21270;&#20102;&#32842;&#22825;&#26426;&#22120;&#20154;&#23450;&#20041;&#30340;&#21046;&#20316;&#65292;&#20811;&#26381;&#20102;&#33258;&#28982;&#35821;&#35328;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11755v1 Announce Type: cross  Abstract: Large language models (LLMs) have profoundly transformed natural language applications, with a growing reliance on instruction-based definitions for designing chatbots. However, post-deployment the chatbot definitions are fixed and are vulnerable to attacks by malicious users, emphasizing the need to prevent unethical applications and financial losses. Existing studies explore user prompts' impact on LLM-based chatbots, yet practical methods to contain attacks on application-specific chatbots remain unexplored. This paper presents System Prompt Meta Language (SPML), a domain-specific language for refining prompts and monitoring the inputs to the LLM-based chatbots. SPML actively checks attack prompts, ensuring user inputs align with chatbot definitions to prevent malicious execution on the LLM backbone, optimizing costs. It also streamlines chatbot definition crafting with programming language capabilities, overcoming natural language 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#38750;&#32431;&#35821;&#20041;&#25552;&#31034;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934;&#25361;&#25112;&#12290;&#20116;&#20010;SOTA LLMs&#22312;&#35782;&#21035;ASCII&#33402;&#26415;&#25552;&#31034;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2402.11753</link><description>&lt;p&gt;
ArtPrompt: &#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#23545;&#40784;LLMs&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11753
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#38750;&#32431;&#35821;&#20041;&#25552;&#31034;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934;&#25361;&#25112;&#12290;&#20116;&#20010;SOTA LLMs&#22312;&#35782;&#21035;ASCII&#33402;&#26415;&#25552;&#31034;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#31181;&#25216;&#26415;&#65292;&#22914;&#25968;&#25454;&#36807;&#28388;&#21644;&#30417;&#30563;&#24494;&#35843;&#65292;&#20197;&#21152;&#24378;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#24050;&#30693;&#30340;&#25216;&#26415;&#20551;&#35774;&#29992;&#20110;&#23545;&#40784;LLMs&#23433;&#20840;&#24615;&#30340;&#35821;&#26009;&#24211;&#20165;&#30001;&#35821;&#20041;&#36827;&#34892;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#20551;&#35774;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#19981;&#25104;&#31435;&#65292;&#23548;&#33268;LLMs&#23384;&#22312;&#20005;&#37325;&#28431;&#27934;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ASCII&#33402;&#26415;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;Vision-in-Text Challenge&#65288;ViTC&#65289;&#26469;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#19981;&#33021;&#20165;&#36890;&#36807;&#35821;&#20041;&#36827;&#34892;&#35299;&#37322;&#30340;&#25552;&#31034;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20116;&#20010;SOTA LLMs&#65288;GPT-3.5&#12289;GPT-4&#12289;Gemini&#12289;Claude&#21644;Llama2&#65289;&#22312;&#35782;&#21035;&#20197;ASCII&#33402;&#26415;&#24418;&#24335;&#25552;&#20379;&#30340;&#25552;&#31034;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11753v1 Announce Type: cross  Abstract: Safety is critical to the usage of large language models (LLMs). Multiple techniques such as data filtering and supervised fine-tuning have been developed to strengthen LLM safety. However, currently known techniques presume that corpora used for safety alignment of LLMs are solely interpreted by semantics. This assumption, however, does not hold in real-world applications, which leads to severe vulnerabilities in LLMs. For example, users of forums often use ASCII art, a form of text-based art, to convey image information. In this paper, we propose a novel ASCII art-based jailbreak attack and introduce a comprehensive benchmark Vision-in-Text Challenge (ViTC) to evaluate the capabilities of LLMs in recognizing prompts that cannot be solely interpreted by semantics. We show that five SOTA LLMs (GPT-3.5, GPT-4, Gemini, Claude, and Llama2) struggle to recognize prompts provided in the form of ASCII art. Based on this observation, we devel
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#35757;&#32451;&#26679;&#26412;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;InfICL&#30340;&#28436;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11750</link><description>&lt;p&gt;
&#36890;&#36807;&#24433;&#21709;&#20998;&#26512;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#28436;&#31034;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning Demonstration Selection via Influence Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11750
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#35757;&#32451;&#26679;&#26412;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;InfICL&#30340;&#28436;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#25552;&#21319;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20854;&#20855;&#26377;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65292;&#36825;&#25552;&#20379;&#20102;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26426;&#20250;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#26799;&#24230;&#26356;&#26032;&#12290;&#23613;&#31649;&#20855;&#26377;&#22810;&#37325;&#22909;&#22788;&#65292;ICL&#30340;&#27867;&#21270;&#24615;&#33021;&#23545;&#25152;&#36873;&#28436;&#31034;&#25935;&#24863;&#12290;&#36873;&#25321;&#29992;&#20110;ICL&#30340;&#26377;&#25928;&#28436;&#31034;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfICL&#30340;&#28436;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24433;&#21709;&#20989;&#25968;&#20998;&#26512;&#35757;&#32451;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#37492;&#21035;&#39640;&#24230;&#26377;&#24433;&#21709;&#21147;&#30340;&#35757;&#32451;&#26679;&#26412;&#21487;&#33021;&#26377;&#21161;&#20110;&#25552;&#21319;ICL&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#38480;&#21046;InfICL&#30340;&#36816;&#34892;&#25104;&#26412;&#65292;&#25105;&#20204;&#20165;&#21033;&#29992;LLM&#29983;&#25104;&#26679;&#26412;&#23884;&#20837;&#65292;&#24182;&#19981;&#25191;&#34892;&#20219;&#20309;&#26114;&#36149;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;InfICL&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#22522;&#32447;&#26041;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11750v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated their In-Context Learning (ICL) capabilities which provides an opportunity to perform few shot learning without any gradient update. Despite its multiple benefits, ICL generalization performance is sensitive to the selected demonstrations. Selecting effective demonstrations for ICL is still an open research challenge. To address this challenge, we propose a demonstration selection method called InfICL which analyzes influences of training samples through influence functions. Identifying highly influential training samples can potentially aid in uplifting the ICL generalization performance. To limit the running cost of InfICL, we only employ the LLM to generate sample embeddings, and don't perform any costly fine tuning. We perform empirical study on multiple real-world datasets and show merits of our InfICL against state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;RESTA&#65292;&#36890;&#36807;&#20219;&#21153;&#31639;&#27861;&#23545;&#31934;&#35843;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#37325;&#26032;&#23450;&#20301;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#20854;&#26377;&#23475;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.11746</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23601;&#26159;&#38669;&#40664;&#183;&#36763;&#26222;&#26862;&#65281;&#36890;&#36807;&#20219;&#21153;&#31639;&#27861;&#23545;&#31934;&#35843;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#37325;&#26032;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;RESTA&#65292;&#36890;&#36807;&#20219;&#21153;&#31639;&#27861;&#23545;&#31934;&#35843;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#37325;&#26032;&#23450;&#20301;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#20854;&#26377;&#23475;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#24494;&#35843;&#30340;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#20250;&#23548;&#33268;&#23433;&#20840;&#24615;&#21463;&#25439;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;RESTA&#65292;&#35813;&#26041;&#27861;&#25191;&#34892;LLM&#23433;&#20840;&#37325;&#26032;&#23450;&#20301;&#12290;RESTA&#20195;&#34920;&#36890;&#36807;&#20219;&#21153;&#31639;&#27861;&#24674;&#22797;&#23433;&#20840;&#12290;&#22312;&#20854;&#26680;&#24515;&#24605;&#24819;&#20013;&#65292;&#23427;&#28041;&#21450;&#23558;&#19968;&#20010;&#23433;&#20840;&#21521;&#37327;&#31616;&#21333;&#22320;&#21152;&#21040;&#21463;&#25439;&#27169;&#22411;&#30340;&#26435;&#37325;&#19978;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RESTA&#22312;&#21442;&#25968;&#39640;&#25928;&#21644;&#23436;&#20840;&#24494;&#35843;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#21253;&#25324;&#20013;&#25991;&#12289;&#33521;&#25991;&#21644;&#21360;&#22320;&#25991;&#30340;&#25351;&#20196;&#36319;&#38543;&#65292;&#20197;&#21450;&#20195;&#30721;&#21644;&#25968;&#23398;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;RESTA&#22312;&#19977;&#20010;&#29616;&#26377;&#23433;&#20840;&#35780;&#20272;&#22522;&#20934;&#21644;&#20316;&#20026;&#26412;&#39033;&#24037;&#20316;&#19968;&#37096;&#20998;&#25552;&#20986;&#30340;&#19968;&#20010;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#36890;&#29992;&#24615;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;550&#20010;&#26377;&#23475;&#38382;&#39064;&#65292;&#28085;&#30422;11&#20010;&#31867;&#21035;&#65292;&#27599;&#20010;&#31867;&#21035;&#19979;&#21253;&#21547;5&#20010;&#26377;&#23475;&#30340;&#23376;&#31867;&#21035;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;RESTA&#38477;&#20302;&#20102;&#21463;&#25439;&#27169;&#22411;&#30340;&#26377;&#23475;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11746v1 Announce Type: cross  Abstract: Aligned language models face a significant limitation as their fine-tuning often results in compromised safety. To tackle this, we propose a simple method RESTA that performs LLM safety realignment. RESTA stands for REstoring Safety through Task Arithmetic. At its core, it involves a simple arithmetic addition of a safety vector to the weights of the compromised model. We demonstrate the effectiveness of RESTA in both parameter-efficient and full fine-tuning, covering a wide range of downstream tasks, including instruction following in Chinese, English, and Hindi, as well as problem-solving capabilities in Code and Math. We also showcase the generalizability of RESTA on three existing safety evaluation benchmarks and a multilingual benchmark dataset proposed as a part of this work, consisting of 550 harmful questions covering 11 categories, each with 5 sub-categories of harm. Overall, RESTA decreases the harmfulness of the compromised 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#23450;&#20301;&#65292;&#38024;&#23545;&#25991;&#26723;&#20013;&#26426;&#22120;&#29983;&#25104;&#37096;&#20998;&#30340;&#23450;&#20301;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#35299;&#20915;&#25972;&#20010;&#25991;&#26723;MGT&#26816;&#27979;&#22833;&#36133;&#24773;&#20917;&#30340;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2402.11744</link><description>&lt;p&gt;
&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Machine-generated Text Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#23450;&#20301;&#65292;&#38024;&#23545;&#25991;&#26723;&#20013;&#26426;&#22120;&#29983;&#25104;&#37096;&#20998;&#30340;&#23450;&#20301;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#35299;&#20915;&#25972;&#20010;&#25991;&#26723;MGT&#26816;&#27979;&#22833;&#36133;&#24773;&#20917;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#65288;MGT&#65289;&#26816;&#27979;&#26088;&#22312;&#35782;&#21035;&#19968;&#27573;&#25991;&#26412;&#26159;&#26426;&#22120;&#20889;&#20316;&#36824;&#26159;&#20154;&#31867;&#20889;&#20316;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#23558;MGT&#26500;&#24314;&#20026;&#23545;&#25972;&#20010;&#25991;&#26723;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65292;&#23545;&#25991;&#26723;&#20013;&#20165;&#37096;&#20998;&#20869;&#23481;&#20026;&#26426;&#22120;&#29983;&#25104;&#30340;&#24773;&#20917;&#36827;&#34892;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#26412;&#25991;&#39318;&#27425;&#28145;&#20837;&#30740;&#31350;&#20102;&#23450;&#20301;&#25991;&#26723;&#20013;&#26426;&#22120;&#29983;&#25104;&#37096;&#20998;&#30340;MGT&#12290;&#22240;&#27492;&#65292;&#22914;&#26524;&#24694;&#24847;&#34892;&#20026;&#32773;&#26356;&#25913;&#26032;&#38395;&#25991;&#31456;&#30340;&#20851;&#38190;&#37096;&#20998;&#20197;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#65292;&#25972;&#20010;&#25991;&#26723;&#30340;MGT&#26816;&#27979;&#21487;&#33021;&#20250;&#22833;&#36133;&#65292;&#22240;&#20026;&#32477;&#22823;&#37096;&#20998;&#26159;&#20154;&#31867;&#20889;&#20316;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#30001;&#20110;&#20854;&#32454;&#31890;&#24230;&#30340;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#12290;&#25105;&#20204;&#30340;MGT&#23450;&#20301;&#20219;&#21153;&#38754;&#20020;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#65292;&#30701;&#36328;&#24230;&#30340;&#25991;&#26412;&#65292;&#20363;&#22914;&#19968;&#20010;&#21477;&#23376;&#65292;&#30001;&#20110;&#38271;&#24230;&#36739;&#30701;&#20960;&#20046;&#19981;&#25552;&#20379;&#25351;&#31034;&#20854;&#26159;&#21542;&#26426;&#22120;&#29983;&#25104;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#39044;&#27979;&#22810;&#20010;&#21477;&#23376;&#26159;&#26426;&#22120;&#29983;&#25104;&#36824;&#26159;&#20154;&#31867;&#20889;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11744v1 Announce Type: new  Abstract: Machine-Generated Text (MGT) detection aims to identify a piece of text as machine or human written. Prior work has primarily formulated MGT as a binary classification task over an entire document, with limited work exploring cases where only part of a document is machine generated. This paper provides the first in-depth study of MGT that localizes the portions of a document that were machine generated. Thus, if a bad actor were to change a key portion of a news article to spread misinformation, whole document MGT detection may fail since the vast majority is human written, but our approach can succeed due to its granular approach. A key challenge in our MGT localization task is that short spans of text, e.g., a single sentence, provides little information indicating if it is machine generated due to its short length. To address this, we leverage contextual information, where we predict whether multiple sentences are machine or human wri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20998;&#26512;&#24072;&#25253;&#21578;&#21644;&#30408;&#21033;&#30005;&#35805;&#20013;&#30340;&#32034;&#36180;&#23545;&#37329;&#34701;&#24066;&#22330;&#22238;&#25253;&#30340;&#24433;&#21709;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#29992;&#20110;&#32034;&#36180;&#26816;&#27979;&#20219;&#21153;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#20837;&#20027;&#39064;&#19987;&#23478;&#30693;&#35782;&#30340;&#26032;&#22411;&#24369;&#30417;&#30563;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#8220;&#20048;&#35266;&#20027;&#20041;&#8221;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.11728</link><description>&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#30340;&#25968;&#23383;&#21270;&#32034;&#36180;&#26816;&#27979;&#65306;&#19968;&#20010;&#26032;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#12289;&#24369;&#30417;&#30563;&#27169;&#22411;&#21644;&#24066;&#22330;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Numerical Claim Detection in Finance: A New Financial Dataset, Weak-Supervision Model, and Market Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20998;&#26512;&#24072;&#25253;&#21578;&#21644;&#30408;&#21033;&#30005;&#35805;&#20013;&#30340;&#32034;&#36180;&#23545;&#37329;&#34701;&#24066;&#22330;&#22238;&#25253;&#30340;&#24433;&#21709;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#29992;&#20110;&#32034;&#36180;&#26816;&#27979;&#20219;&#21153;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#20837;&#20027;&#39064;&#19987;&#23478;&#30693;&#35782;&#30340;&#26032;&#22411;&#24369;&#30417;&#30563;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#8220;&#20048;&#35266;&#20027;&#20041;&#8221;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#26512;&#24072;&#25253;&#21578;&#21644;&#30408;&#21033;&#30005;&#35805;&#20013;&#30340;&#32034;&#36180;&#23545;&#37329;&#34701;&#24066;&#22330;&#22238;&#25253;&#30340;&#24433;&#21709;&#65292;&#23558;&#23427;&#20204;&#35270;&#20026;&#19978;&#24066;&#20844;&#21496;&#37325;&#35201;&#30340;&#23395;&#24230;&#20107;&#20214;&#12290;&#20026;&#20102;&#36827;&#34892;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#37329;&#34701;&#39046;&#22495;&#30340;&#32034;&#36180;&#26816;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#23545;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#20837;&#20027;&#39064;&#19987;&#23478;&#65288;SMEs&#65289;&#30693;&#35782;&#30340;&#26032;&#22411;&#24369;&#30417;&#30563;&#27169;&#22411;&#65292;&#22312;&#32858;&#21512;&#20989;&#25968;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#8220;&#20048;&#35266;&#20027;&#20041;&#8221;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#23454;&#38469;&#25928;&#29992;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#30408;&#21033;&#24778;&#21916;&#21644;&#22238;&#25253;&#23545;&#25105;&#20204;&#30340;&#20048;&#35266;&#20027;&#20041;&#24230;&#37327;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#20195;&#30721;&#23558;&#22312;GitHub&#21644;Hugging Face&#19978;&#20844;&#24320;&#65288;&#36981;&#24490;CC BY 4.0&#35768;&#21487;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11728v1 Announce Type: new  Abstract: In this paper, we investigate the influence of claims in analyst reports and earnings calls on financial market returns, considering them as significant quarterly events for publicly traded companies. To facilitate a comprehensive analysis, we construct a new financial dataset for the claim detection task in the financial domain. We benchmark various language models on this dataset and propose a novel weak-supervision model that incorporates the knowledge of subject matter experts (SMEs) in the aggregation function, outperforming existing approaches. Furthermore, we demonstrate the practical utility of our proposed model by constructing a novel measure ``optimism". Furthermore, we observed the dependence of earnings surprise and return on our optimism measure. Our dataset, models, and code will be made publicly (under CC BY 4.0 license) available on GitHub and Hugging Face.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#20196;&#20154;&#25285;&#24551;&#30340;&#26131;&#21463;&#24847;&#35782;&#24418;&#24577;&#25805;&#32437;&#30340;&#33030;&#24369;&#24615;&#65292;&#23545;&#26497;&#23569;&#37327;&#24847;&#35782;&#24418;&#24577;&#39537;&#21160;&#26679;&#26412;&#30340;&#26292;&#38706;&#23601;&#33021;&#26174;&#33879;&#25913;&#21464;&#20854;&#24847;&#35782;&#24418;&#24577;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#19968;&#20010;&#20027;&#39064;&#21560;&#25910;&#24847;&#35782;&#24418;&#24577;&#24182;&#27867;&#21270;&#21040;&#20854;&#20182;&#19981;&#30456;&#20851;&#20027;&#39064;&#19978;&#12290;</title><link>https://arxiv.org/abs/2402.11725</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#24847;&#35782;&#24418;&#24577;&#25805;&#32437;&#30340;&#26131;&#24863;&#24615;&#26377;&#22810;&#39640;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Susceptible are Large Language Models to Ideological Manipulation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11725
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#20196;&#20154;&#25285;&#24551;&#30340;&#26131;&#21463;&#24847;&#35782;&#24418;&#24577;&#25805;&#32437;&#30340;&#33030;&#24369;&#24615;&#65292;&#23545;&#26497;&#23569;&#37327;&#24847;&#35782;&#24418;&#24577;&#39537;&#21160;&#26679;&#26412;&#30340;&#26292;&#38706;&#23601;&#33021;&#26174;&#33879;&#25913;&#21464;&#20854;&#24847;&#35782;&#24418;&#24577;&#65292;&#24182;&#19988;&#33021;&#22815;&#20174;&#19968;&#20010;&#20027;&#39064;&#21560;&#25910;&#24847;&#35782;&#24418;&#24577;&#24182;&#27867;&#21270;&#21040;&#20854;&#20182;&#19981;&#30456;&#20851;&#20027;&#39064;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20855;&#26377;&#23545;&#20844;&#20247;&#35266;&#24565;&#21644;&#20449;&#24687;&#20114;&#21160;&#26045;&#21152;&#37325;&#35201;&#24433;&#21709;&#30340;&#28508;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#22914;&#26524;&#36825;&#20123;&#27169;&#22411;&#20869;&#30340;&#24847;&#35782;&#24418;&#24577;&#26131;&#21463;&#25805;&#32437;&#21487;&#33021;&#24102;&#26469;&#31038;&#20250;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;&#23398;&#20064;&#21644;&#27867;&#21270;&#24847;&#35782;&#24418;&#24577;&#20559;&#35265;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#33030;&#24369;&#24615;&#65306;&#20165;&#25509;&#35302;&#21040;&#23569;&#37327;&#24847;&#35782;&#24418;&#24577;&#39537;&#21160;&#30340;&#26679;&#26412;&#23601;&#20250;&#26174;&#33879;&#25913;&#21464;LLMs&#30340;&#24847;&#35782;&#24418;&#24577;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;LLMs&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#20174;&#19968;&#20010;&#20027;&#39064;&#21560;&#25910;&#24847;&#35782;&#24418;&#24577;&#24182;&#23558;&#20854;&#27867;&#21270;&#21040;&#29978;&#33267;&#19981;&#30456;&#20851;&#30340;&#20027;&#39064;&#19978;&#12290;LLMs&#30340;&#24847;&#35782;&#24418;&#24577;&#23481;&#26131;&#34987;&#25197;&#26354;&#30340;&#20107;&#23454;&#24378;&#35843;&#20102;&#24694;&#24847;&#34892;&#20026;&#32773;&#25925;&#24847;&#27602;&#23475;&#35757;&#32451;&#25968;&#25454;&#25110;&#25968;&#25454;&#27880;&#37322;&#32773;&#26080;&#24847;&#24341;&#20837;&#20559;&#35265;&#25152;&#24102;&#26469;&#30340;&#39118;&#38505;&#12290;&#36825;&#20063;&#24378;&#35843;&#20102;&#37319;&#21462;&#24378;&#26377;&#21147;&#25514;&#26045;&#20197;&#20943;&#36731;&#36825;&#20123;&#23041;&#32961;&#30340;&#36843;&#20999;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11725v1 Announce Type: new  Abstract: Large Language Models (LLMs) possess the potential to exert substantial influence on public perceptions and interactions with information. This raises concerns about the societal impact that could arise if the ideologies within these models can be easily manipulated. In this work, we investigate how effectively LLMs can learn and generalize ideological biases from their instruction-tuning data. Our findings reveal a concerning vulnerability: exposure to only a small amount of ideologically driven samples significantly alters the ideology of LLMs. Notably, LLMs demonstrate a startling ability to absorb ideology from one topic and generalize it to even unrelated ones. The ease with which LLMs' ideologies can be skewed underscores the risks associated with intentionally poisoned training data by malicious actors or inadvertently introduced biases by data annotators. It also emphasizes the imperative for robust safeguards to mitigate the inf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#19981;&#21516;&#25903;&#25745;&#32423;&#21035;&#22914;&#20309;&#24433;&#21709;&#20849;&#21516;&#25776;&#20889;&#36807;&#31243;&#65292;&#21457;&#29616;&#39640;&#25903;&#25745;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20889;&#20316;&#36136;&#37327;&#21644;&#29983;&#20135;&#29575;&#65292;&#29305;&#21035;&#26377;&#21033;&#20110;&#38750;&#24120;&#35268;&#20889;&#20316;&#32773;&#21644;&#19981;&#22826;&#31934;&#36890;&#25216;&#26415;&#30340;&#29992;&#25143;&#12290;</title><link>https://arxiv.org/abs/2402.11723</link><description>&lt;p&gt;
&#22609;&#36896;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#65306;&#22312;&#19982;&#35821;&#35328;&#27169;&#22411;&#20849;&#21516;&#25776;&#20889;&#20013;&#20351;&#29992;&#19981;&#21516;&#25903;&#25745;&#32423;&#21035;
&lt;/p&gt;
&lt;p&gt;
Shaping Human-AI Collaboration: Varied Scaffolding Levels in Co-writing with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#19981;&#21516;&#25903;&#25745;&#32423;&#21035;&#22914;&#20309;&#24433;&#21709;&#20849;&#21516;&#25776;&#20889;&#36807;&#31243;&#65292;&#21457;&#29616;&#39640;&#25903;&#25745;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20889;&#20316;&#36136;&#37327;&#21644;&#29983;&#20135;&#29575;&#65292;&#29305;&#21035;&#26377;&#21033;&#20110;&#38750;&#24120;&#35268;&#20889;&#20316;&#32773;&#21644;&#19981;&#22826;&#31934;&#36890;&#25216;&#26415;&#30340;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24314;&#27169;&#30340;&#36827;&#23637;&#20026;&#26032;&#39062;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20849;&#21516;&#25776;&#20889;&#20307;&#39564;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#30340;&#19981;&#21516;&#25903;&#25745;&#32423;&#21035;&#22914;&#20309;&#22609;&#36896;&#20849;&#21516;&#25776;&#20889;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#25289;&#19969;&#26041;&#35774;&#35745;&#30340;&#34987;&#35797;&#20869;&#23454;&#39564;&#65292;&#25105;&#20204;&#36992;&#35831;&#20102;131&#21517;&#21442;&#19982;&#32773;&#22238;&#24212;&#19977;&#20010;&#38543;&#26426;&#39034;&#24207;&#26465;&#20214;&#19979;&#30340;&#35770;&#35777;&#20889;&#20316;&#25552;&#31034;&#65306;&#26080;AI&#36741;&#21161;&#65288;&#23545;&#29031;&#32452;&#65289;&#65292;&#19979;&#19968;&#21477;&#24314;&#35758;&#65288;&#20302;&#25903;&#25745;&#65289;&#65292;&#21644;&#19979;&#19968;&#27573;&#24314;&#35758;&#65288;&#39640;&#25903;&#25745;&#65289;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#25903;&#25745;&#23545;&#20889;&#20316;&#36136;&#37327;&#21644;&#29983;&#20135;&#29575;&#65288;&#21333;&#20301;&#26102;&#38388;&#35789;&#25968;&#65289;&#30340;U&#24418;&#24433;&#21709;&#12290;&#34429;&#28982;&#20302;&#25903;&#25745;&#24182;&#26410;&#26174;&#33879;&#25552;&#39640;&#20889;&#20316;&#36136;&#37327;&#25110;&#29983;&#20135;&#29575;&#65292;&#20294;&#39640;&#25903;&#25745;&#23548;&#33268;&#20102;&#26174;&#33879;&#25913;&#21892;&#65292;&#23588;&#20854;&#26377;&#21033;&#20110;&#38750;&#24120;&#35268;&#20889;&#20316;&#32773;&#21644;&#19981;&#22826;&#31934;&#36890;&#25216;&#26415;&#30340;&#29992;&#25143;&#12290;&#22312;&#20351;&#29992;&#25903;&#25745;&#24335;&#20889;&#20316;&#24037;&#20855;&#26102;&#24182;&#26410;&#35266;&#23519;&#21040;&#26174;&#33879;&#30340;&#35748;&#30693;&#36127;&#25285;&#65292;&#20294;&#25991;&#23383;&#37327;&#30053;&#26377;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11723v1 Announce Type: cross  Abstract: Advances in language modeling have paved the way for novel human-AI co-writing experiences. This paper explores how varying levels of scaffolding from large language models (LLMs) shape the co-writing process. Employing a within-subjects field experiment with a Latin square design, we asked participants (N=131) to respond to argumentative writing prompts under three randomly sequenced conditions: no AI assistance (control), next-sentence suggestions (low scaffolding), and next-paragraph suggestions (high scaffolding). Our findings reveal a U-shaped impact of scaffolding on writing quality and productivity (words/time). While low scaffolding did not significantly improve writing quality or productivity, high scaffolding led to significant improvements, especially benefiting non-regular writers and less tech-savvy users. No significant cognitive burden was observed while using the scaffolded writing tools, but a moderate decrease in text
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#32852;&#30431;&#35848;&#21028;&#20316;&#20026;&#19968;&#39033;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24314;&#27169;&#20026;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#20043;&#38388;&#30340;&#35848;&#21028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;POLCA&#20197;&#21450;&#29992;&#20110;&#27169;&#25311;&#25919;&#27835;&#35848;&#21028;&#36807;&#31243;&#30340;&#20998;&#23618;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.11712</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#27169;&#25311;&#25919;&#27835;&#32852;&#30431;&#35848;&#21028;
&lt;/p&gt;
&lt;p&gt;
Modelling Political Coalition Negotiations Using LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#32852;&#30431;&#35848;&#21028;&#20316;&#20026;&#19968;&#39033;&#26032;&#39062;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24314;&#27169;&#20026;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#20043;&#38388;&#30340;&#35848;&#21028;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;POLCA&#20197;&#21450;&#29992;&#20110;&#27169;&#25311;&#25919;&#27835;&#35848;&#21028;&#36807;&#31243;&#30340;&#20998;&#23618;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#30431;&#35848;&#21028;&#26159;&#35758;&#20250;&#27665;&#20027;&#21046;&#24230;&#30340;&#22522;&#30707;&#65292;&#20197;&#25919;&#20826;&#38388;&#22797;&#26434;&#20114;&#21160;&#21644;&#25112;&#30053;&#27807;&#36890;&#20026;&#29305;&#24449;&#12290;&#23613;&#31649;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#25968;&#25454;&#65292;&#23545;&#36825;&#20123;&#35848;&#21028;&#30340;&#24314;&#27169;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20013;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#25991;&#23558;&#32852;&#30431;&#35848;&#21028;&#20316;&#20026;&#19968;&#39033;&#26032;&#39062;&#30340;NLP&#20219;&#21153;&#24341;&#20837;&#65292;&#24182;&#23558;&#20854;&#24314;&#27169;&#20026;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#20043;&#38388;&#30340;&#35848;&#21028;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;POLCA&#65292;&#21253;&#25324;&#27431;&#27954;&#25919;&#20826;&#23459;&#35328;&#21644;&#36825;&#20123;&#22269;&#23478;&#33509;&#24178;&#36873;&#20030;&#20013;&#30340;&#32852;&#30431;&#21327;&#35758;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#22522;&#30784;&#27169;&#25311;&#35299;&#20915;&#20102;&#24403;&#21069;&#25919;&#27835;&#35848;&#21028;&#24314;&#27169;&#30340;&#33539;&#22260;&#38480;&#21046;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#27169;&#25311;&#25919;&#27835;&#32852;&#30431;&#35848;&#21028;&#36807;&#31243;&#30340;&#20998;&#23618;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11712v1 Announce Type: new  Abstract: Coalition negotiations are a cornerstone of parliamentary democracies, characterised by complex interactions and strategic communications among political parties. Despite its significance, the modelling of these negotiations has remained unexplored with the domain of Natural Language Processing (NLP), mostly due to lack of proper data. In this paper, we introduce coalition negotiations as a novel NLP task, and model it as a negotiation between large language model-based agents. We introduce a multilingual dataset, POLCA, comprising manifestos of European political parties and coalition agreements over a number of elections in these countries. This dataset addresses the challenge of the current scope limitations in political negotiation modelling by providing a diverse, real-world basis for simulation. Additionally, we propose a hierarchical Markov decision process designed to simulate the process of coalition negotiation between politica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#25216;&#26415;&#24212;&#29992;&#20110;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#65292;&#20026;&#35299;&#20915;&#22870;&#21169;&#24179;&#34913;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2402.11711</link><description>&lt;p&gt;
MORL-Prompt: &#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#30340;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#22810;&#30446;&#26631;&#20248;&#21270;&#25216;&#26415;&#24212;&#29992;&#20110;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#65292;&#20026;&#35299;&#20915;&#22870;&#21169;&#24179;&#34913;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;RL&#30340;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#25628;&#32034;&#25552;&#31034;&#65292;&#23558;&#20854;&#36755;&#20837;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#20197;&#26368;&#22823;&#21270;&#19968;&#32452;&#29992;&#25143;&#25351;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#30446;&#26631;&#24212;&#29992;&#20013;&#65292;&#33258;&#28982;&#22870;&#21169;&#20989;&#25968;&#24444;&#27492;&#20043;&#38388;&#23384;&#22312;&#32039;&#24352;&#20851;&#31995;--&#20363;&#22914;&#65292;&#22312;&#39118;&#26684;&#36716;&#31227;&#20219;&#21153;&#20013;&#65292;&#20869;&#23481;&#20445;&#30041;&#19982;&#39118;&#26684;&#21305;&#37197;&#20043;&#38388;&#23384;&#22312;&#30683;&#30462;&#12290;&#24403;&#21069;&#25216;&#26415;&#20391;&#37325;&#20110;&#26368;&#22823;&#21270;&#22870;&#21169;&#20989;&#25968;&#30340;&#24179;&#22343;&#20540;&#65292;&#36825;&#26410;&#24517;&#20250;&#23548;&#33268;&#21462;&#24471;&#21508;&#31181;&#22870;&#21169;&#24179;&#34913;&#30340;&#25552;&#31034;--&#36825;&#20010;&#38382;&#39064;&#22312;&#22810;&#30446;&#26631;&#21644;&#40065;&#26834;&#20248;&#21270;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#23558;&#20960;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#25216;&#26415;&#35843;&#25972;&#20026;&#22522;&#20110;RL&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;--&#20854;&#20013;&#26377;&#20004;&#31181;&#32771;&#34385;&#24085;&#32047;&#25176;&#22870;&#21169;&#38754;&#31215;&#30340;&#26041;&#27861;&#65292;&#21478;&#22806;&#19968;&#31181;&#36873;&#25321;&#26377;&#30410;&#20110;&#25152;&#26377;&#22870;&#21169;&#30340;&#26356;&#26032;&#26041;&#21521;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;NLP&#20219;&#21153;&#19978;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65306;&#39118;&#26684;&#36716;&#31227;&#21644;&#26426;&#22120;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11711v1 Announce Type: new  Abstract: RL-based techniques can be used to search for prompts that when fed into a target language model maximize a set of user-specified reward functions. However, in many target applications, the natural reward functions are in tension with one another -- for example, content preservation vs. style matching in style transfer tasks. Current techniques focus on maximizing the average of reward functions, which does not necessarily lead to prompts that achieve balance across rewards -- an issue that has been well-studied in the multi-objective and robust optimization literature. In this paper, we adapt several techniques for multi-objective optimization to RL-based discrete prompt optimization -- two that consider volume of the Pareto reward surface, and another that chooses an update direction that benefits all rewards simultaneously. We conduct an empirical analysis of these methods on two NLP tasks: style transfer and machine translation, each
&lt;/p&gt;</description></item><item><title>&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#37325;&#26032;&#23457;&#35270;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#25552;&#20986;&#26032;&#30340;&#20559;&#35265;&#31867;&#22411;&#65292;&#24182;&#24314;&#31435;&#21253;&#25324;&#20551;&#35774;&#12289;&#31574;&#30053;&#21644;&#26041;&#27861;&#30340;&#26694;&#26550;&#26469;&#20943;&#23569;&#31038;&#20250;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.11710</link><description>&lt;p&gt;
&#20851;&#20110;&#23436;&#21892;&#20559;&#35265;&#30340;&#27880;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Note on Bias to Complete
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11710
&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#65292;&#37325;&#26032;&#23457;&#35270;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#25552;&#20986;&#26032;&#30340;&#20559;&#35265;&#31867;&#22411;&#65292;&#24182;&#24314;&#31435;&#21253;&#25324;&#20551;&#35774;&#12289;&#31574;&#30053;&#21644;&#26041;&#27861;&#30340;&#26694;&#26550;&#26469;&#20943;&#23569;&#31038;&#20250;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#31038;&#20250;&#20559;&#35265;&#21487;&#20197;&#21152;&#24378;&#31038;&#20250;&#32852;&#31995;&#65292;&#20419;&#36827;&#20849;&#21516;&#29702;&#35299;&#21644;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#23545;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#21457;&#29616;&#20102;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#26032;&#20559;&#35265;&#31867;&#22411;&#65288;&#20363;&#22914;&#31038;&#20250;&#22320;&#20301;&#65289;&#65292;&#24182;&#23558;&#23427;&#20204;&#30456;&#23545;&#20110;&#25991;&#21270;&#12289;&#22320;&#21306;&#12289;&#26102;&#38388;&#21644;&#20010;&#20154;&#32972;&#26223;&#25551;&#36848;&#20986;&#26469;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20843;&#20010;&#20851;&#20110;&#20559;&#35265;&#30340;&#20551;&#35774;&#65292;&#20197;&#21450;&#27599;&#20010;&#20551;&#35774;&#30340;&#20943;&#23569;&#20559;&#35265;&#31574;&#30053;&#20197;&#21450;&#22312;LLM&#20013;&#25552;&#20986;&#30340;&#20116;&#31181;&#26041;&#27861;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20010;&#26694;&#26550;&#30340;&#23454;&#29616;&#23578;&#26410;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11710v1 Announce Type: new  Abstract: Minimizing social bias strengthens societal bonds, promoting shared understanding and better decision-making. We revisit the definition of bias by discovering new bias types (e.g., societal status) in dynamic environments and describe them relative to context, such as culture, region, time, and personal background. Our framework includes eight hypotheses about bias and a minimizing bias strategy for each assumption as well as five methods as proposed solutions in LLM. The realization of the framework is yet to be completed.
&lt;/p&gt;</description></item><item><title>GNNavi&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25552;&#31034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20934;&#30830;&#24341;&#23548;&#20449;&#24687;&#27969;&#30340;&#27719;&#32858;&#21644;&#20998;&#24067;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23548;&#33322;&#20449;&#24687;&#27969;&#21160;&#24577;&#65292;&#36229;&#36234;&#20102;&#26631;&#20934;&#25552;&#31034;&#24335;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11709</link><description>&lt;p&gt;
GNNavi&#65306;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#23548;&#33322;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
GNNavi: Navigating the Information Flow in Large Language Models by Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11709
&lt;/p&gt;
&lt;p&gt;
GNNavi&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25552;&#31034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20934;&#30830;&#24341;&#23548;&#20449;&#24687;&#27969;&#30340;&#27719;&#32858;&#21644;&#20998;&#24067;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23548;&#33322;&#20449;&#24687;&#27969;&#21160;&#24577;&#65292;&#36229;&#36234;&#20102;&#26631;&#20934;&#25552;&#31034;&#24335;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25509;&#25910;&#31034;&#33539;&#36755;&#20837;&#26102;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65288;ICL&#65289;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#20173;&#28982;&#33267;&#20851;&#37325;&#35201;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#36866;&#24212;&#24615;&#12290;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#24773;&#20917;&#19979;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20294;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#39640;&#38656;&#27714;&#38480;&#21046;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#25552;&#31034;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;GNNavi&#21033;&#29992;&#20102;&#26377;&#20851;ICL&#20449;&#24687;&#27969;&#21160;&#24577;&#30340;&#35265;&#35299;&#65292;&#34920;&#26126;&#26631;&#31614;&#35789;&#22312;&#25552;&#31034;&#20013;&#20316;&#20026;&#20449;&#24687;&#20256;&#25773;&#30340;&#38170;&#28857;&#12290;GNNavi&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#31934;&#30830;&#22320;&#24341;&#23548;&#20449;&#24687;&#27969;&#30340;&#27719;&#32858;&#21644;&#20998;&#24067;&#65292;&#22312;&#22788;&#29702;&#25552;&#31034;&#26102;&#23558;&#26399;&#26395;&#30340;&#20449;&#24687;&#27969;&#30828;&#32534;&#30721;&#21040;GNN&#20013;&#12290;&#25105;&#20204;&#22312;&#20351;&#29992;GPT-2&#21644;Llama2&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#21457;&#29616;&#65292;GNNavi&#36229;&#36234;&#20102;&#26631;&#20934;&#25552;&#31034;&#24335;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11709v1 Announce Type: cross  Abstract: Large Language Models (LLMs) exhibit strong In-Context Learning (ICL) capabilities when prompts with demonstrations are applied to them. However, fine-tuning still remains crucial to further enhance their adaptability. Prompt-based fine-tuning proves to be an effective fine-tuning method in low-data scenarios, but high demands on computing resources limit its practicality. We address this issue by introducing a prompt-based parameter-efficient fine-tuning (PEFT) approach. GNNavi leverages insights into ICL's information flow dynamics, which indicates that label words act in prompts as anchors for information propagation. GNNavi employs a Graph Neural Network (GNN) layer to precisely guide the aggregation and distribution of information flow during the processing of prompts by hardwiring the desired information flow into the GNN. Our experiments on text classification tasks with GPT-2 and Llama2 shows GNNavi surpasses standard prompt-ba
&lt;/p&gt;</description></item><item><title>&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23618;&#25968;&#21487;&#22312;&#19981;&#25439;&#22833;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#20943;&#36731;&#27169;&#22411;&#35268;&#27169;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21482;&#26377;&#19968;&#20010;&#23618;&#30340;&#27169;&#22411;&#21487;&#20197;&#36229;&#36234;&#23436;&#20840;&#23618;&#24335;&#30340;&#23545;&#24212;&#39033;&#12290;</title><link>https://arxiv.org/abs/2402.11700</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#35201;&#20030;&#24471;&#37027;&#20040;&#27785;&#65311;&#36890;&#36807;&#20462;&#21098;&#23618;&#26469;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Why Lift so Heavy? Slimming Large Language Models by Cutting Off the Layers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11700
&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23618;&#25968;&#21487;&#22312;&#19981;&#25439;&#22833;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#20943;&#36731;&#27169;&#22411;&#35268;&#27169;&#65292;&#29978;&#33267;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21482;&#26377;&#19968;&#20010;&#23618;&#30340;&#27169;&#22411;&#21487;&#20197;&#36229;&#36234;&#23436;&#20840;&#23618;&#24335;&#30340;&#23545;&#24212;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24040;&#22823;&#35268;&#27169;&#22312;&#23384;&#20648;&#12289;&#35757;&#32451;&#21644;&#25512;&#29702;&#26041;&#38754;&#24102;&#26469;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#36807;&#23618;&#21472;&#21253;&#21547;&#20102;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#12290;&#23613;&#31649;&#20256;&#32479;&#26041;&#27861;&#22914;&#27169;&#22411;&#20462;&#21098;&#25110;&#33976;&#39311;&#20026;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#25552;&#20379;&#20102;&#36884;&#24452;&#65292;&#20294;&#24448;&#24448;&#20250;&#20197;&#24615;&#33021;&#20445;&#30041;&#20026;&#20195;&#20215;&#12290;&#22312;&#25105;&#20204;&#30340;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#36890;&#36807;&#20943;&#23569;LLMs&#20013;&#30340;&#23618;&#25968;&#26469;&#20943;&#23569;&#27169;&#22411;&#35268;&#27169;&#30340;&#26041;&#27861;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#23618;&#25968;&#36739;&#23569;&#65292;LLMs&#22312;&#29305;&#21035;&#26159;&#22522;&#20110;&#25552;&#31034;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#24494;&#35843;&#20013;&#20063;&#33021;&#20445;&#25345;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21482;&#26377;&#19968;&#20010;&#23618;&#30340;&#27169;&#22411;&#21487;&#20197;&#32988;&#36807;&#23436;&#20840;&#23618;&#24335;&#30340;&#23545;&#24212;&#39033;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#26410;&#26469;&#26088;&#22312;&#20943;&#36731;LLMs&#22823;&#23567;&#32422;&#26463;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11700v1 Announce Type: new  Abstract: Large Language Models (LLMs) possess outstanding capabilities in addressing various natural language processing (NLP) tasks. However, the sheer size of these models poses challenges in terms of storage, training and inference due to the inclusion of billions of parameters through layer stacking. While traditional approaches such as model pruning or distillation offer ways for reducing model size, they often come at the expense of performance retention. In our investigation, we systematically explore the approach of reducing the number of layers in LLMs. Surprisingly, we observe that even with fewer layers, LLMs maintain similar or better performance levels, particularly in prompt-based fine-tuning for text classification tasks. Remarkably, in certain cases, models with a single layer outperform their fully layered counterparts. These findings offer valuable insights for future work aimed at mitigating the size constraints of LLMs while p
&lt;/p&gt;</description></item><item><title>&#26500;&#24314;&#20102;&#26368;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;Vision-Flan&#65292;&#25552;&#20986;&#20102;&#20004;&#38454;&#27573;&#25351;&#23548;&#35843;&#25972;&#26694;&#26550;&#65292;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.11690</link><description>&lt;p&gt;
Vision-Flan&#65306;&#25193;&#23637;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#20013;&#30340;&#20154;&#31867;&#26631;&#35760;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11690
&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#20102;&#26368;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;Vision-Flan&#65292;&#25552;&#20986;&#20102;&#20004;&#38454;&#27573;&#25351;&#23548;&#35843;&#25972;&#26694;&#26550;&#65292;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20316;&#20026;&#22810;&#21151;&#33021;&#35270;&#35273;&#21161;&#25163;&#20855;&#26377;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#20294;&#29616;&#26377;VLM&#26694;&#26550;&#20013;&#20173;&#23384;&#22312;&#20004;&#20010;&#37325;&#22823;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22312;&#39044;&#35757;&#32451;&#21644;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#20013;&#32570;&#20047;&#20219;&#21153;&#22810;&#26679;&#24615;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22312;GPT-4&#21512;&#25104;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#20013;&#23384;&#22312;&#27880;&#37322;&#38169;&#35823;&#21644;&#20559;&#35265;&#12290;&#36825;&#20004;&#20010;&#25361;&#25112;&#23548;&#33268;&#38382;&#39064;&#65292;&#22914;&#27867;&#21270;&#33021;&#21147;&#24046;&#12289;&#24187;&#35273;&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;Vision-Flan&#65292;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22810;&#26679;&#21270;&#30340;&#20844;&#24320;&#21487;&#29992;&#35270;&#35273;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;187&#20010;&#22810;&#26679;&#21270;&#20219;&#21153;&#21644;&#20174;&#23398;&#26415;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30340;1,664,261&#20010;&#23454;&#20363;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#38468;&#24102;&#19987;&#23478;&#25776;&#20889;&#30340;&#25351;&#23548;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#25351;&#23548;&#35843;&#25972;&#26694;&#26550;&#65292;&#20854;&#20013;VLM&#39318;&#20808;&#22312;Vision-Flan&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#28982;&#21518;&#22312;GPT-4&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#19968;&#27493;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#20004;&#38454;&#27573;&#35843;&#25972;&#26694;&#26550;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11690v1 Announce Type: new  Abstract: Despite vision-language models' (VLMs) remarkable capabilities as versatile visual assistants, two substantial challenges persist within the existing VLM frameworks: (1) lacking task diversity in pretraining and visual instruction tuning, and (2) annotation error and bias in GPT-4 synthesized instruction tuning data. Both challenges lead to issues such as poor generalizability, hallucination, and catastrophic forgetting. To address these challenges, we construct Vision-Flan, the most diverse publicly available visual instruction tuning dataset to date, comprising 187 diverse tasks and 1,664,261 instances sourced from academic datasets, and each task is accompanied by an expert-written instruction. In addition, we propose a two-stage instruction tuning framework, in which VLMs are firstly finetuned on Vision-Flan and further tuned on GPT-4 synthesized data. We find this two-stage tuning framework significantly outperforms the traditional 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37319;&#29992;GPT-4V&#21512;&#25104;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#30740;&#31350;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;ALLaVA&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;12&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#19982;&#26368;&#22810;3B LVLMs&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11684</link><description>&lt;p&gt;
&#21033;&#29992;GPT4V&#21512;&#25104;&#25968;&#25454;&#23454;&#29616;&#36731;&#37327;&#32423;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;ALLaVA
&lt;/p&gt;
&lt;p&gt;
ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11684
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37319;&#29992;GPT-4V&#21512;&#25104;&#30340;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#65292;&#30740;&#31350;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;ALLaVA&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;12&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#19982;&#26368;&#22810;3B LVLMs&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(LVLMs)&#30340;&#21457;&#23637;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20294;&#37096;&#32626;&#26102;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#23588;&#20854;&#26159;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#37319;&#29992;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#24357;&#21512;&#20256;&#32479;&#23610;&#24230;LVLMs&#21644;&#36164;&#28304;&#21451;&#22909;&#22411;Lite&#29256;&#26412;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#36890;&#36807;&#21033;&#29992;GPT-4V&#29983;&#25104;&#35814;&#32454;&#25551;&#36848;&#12289;&#22797;&#26434;&#25512;&#29702;&#25351;&#20196;&#21644;&#22270;&#29255;&#35814;&#32454;&#31572;&#26696;&#30340;&#33021;&#21147;&#21019;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#32467;&#26524;&#27169;&#22411;ALLaVA&#22312;12&#39033;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#19982;&#26368;&#22810;3B LVLMs&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#39033;&#24037;&#20316;&#31361;&#20986;&#20102;&#22312;&#35774;&#35745;&#26356;&#39640;&#25928;&#30340;LVLMs&#20013;&#37319;&#29992;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#22312;&#32447;&#28436;&#31034;&#21487;&#22312;\url{https://allava.freedomai.cn}&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11684v1 Announce Type: cross  Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have enabled processing of multimodal inputs in language models but require significant computational resources for deployment, especially in edge devices. This study aims to bridge the performance gap between traditional-scale LVLMs and resource-friendly lite versions by adopting high-quality training data. To do this, a synthetic dataset is created by leveraging GPT-4V's ability to generate detailed captions, complex reasoning instructions and detailed answers from images. The resulted model trained with our data, ALLaVA, achieves competitive performance on 12 benchmarks up to 3B LVLMs. This work highlights the feasibility of adopting high-quality data in crafting more efficient LVLMs. Our online demo is available at \url{https://allava.freedomai.cn}.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37322;&#25918;&#28085;&#30422;&#35266;&#28857;&#25688;&#35201;&#35780;&#20272;&#30456;&#20851;&#19971;&#20010;&#32500;&#24230;&#30340;&#26032;&#25968;&#25454;&#38598;SUMMEVAL-OP&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102; Op-I-Prompt &#20316;&#20026;&#19968;&#31181;&#29420;&#31435;&#20110;&#32500;&#24230;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#20197;&#21450; Op-Prompts &#20316;&#20026;&#19968;&#32452;&#20381;&#36182;&#20110;&#32500;&#24230;&#30340;&#25552;&#31034;&#65292;&#21487;&#20197;&#34920;&#26126; Op-I-Prompt &#26159;&#35780;&#20272;&#35266;&#28857;&#25688;&#35201;&#30340;&#19968;&#20010;&#24456;&#22909;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#24179;&#22343; Spearman &#30456;&#20851;&#24615;&#20026; 0.70&#12290;</title><link>https://arxiv.org/abs/2402.11683</link><description>&lt;p&gt;
&#19968;&#31181;&#25903;&#37197;&#25152;&#26377;&#30340;&#25552;&#31034;&#65306;LLMs &#29992;&#20110;&#35266;&#28857;&#25688;&#35201;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
One Prompt To Rule Them All: LLMs for Opinion Summary Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11683
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37322;&#25918;&#28085;&#30422;&#35266;&#28857;&#25688;&#35201;&#35780;&#20272;&#30456;&#20851;&#19971;&#20010;&#32500;&#24230;&#30340;&#26032;&#25968;&#25454;&#38598;SUMMEVAL-OP&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102; Op-I-Prompt &#20316;&#20026;&#19968;&#31181;&#29420;&#31435;&#20110;&#32500;&#24230;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#20197;&#21450; Op-Prompts &#20316;&#20026;&#19968;&#32452;&#20381;&#36182;&#20110;&#32500;&#24230;&#30340;&#25552;&#31034;&#65292;&#21487;&#20197;&#34920;&#26126; Op-I-Prompt &#26159;&#35780;&#20272;&#35266;&#28857;&#25688;&#35201;&#30340;&#19968;&#20010;&#24456;&#22909;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#24179;&#22343; Spearman &#30456;&#20851;&#24615;&#20026; 0.70&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20256;&#32479;&#22522;&#20110;&#21442;&#32771;&#30340;&#24230;&#37327;&#23545;&#35266;&#28857;&#25688;&#35201;&#36827;&#34892;&#35780;&#20272;&#24456;&#23569;&#25552;&#20379;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#24182;&#19988;&#24050;&#32463;&#26174;&#31034;&#20986;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#30456;&#23545;&#36739;&#20302;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#26080;&#21442;&#32771;&#24230;&#37327;&#30340;NLG&#35780;&#20272;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#35266;&#28857;&#25688;&#35201;&#35780;&#20272;&#26041;&#38754;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#26377;&#38480;&#30340;&#35266;&#28857;&#25688;&#35201;&#35780;&#20272;&#25968;&#25454;&#38598;&#38459;&#30861;&#20102;&#36827;&#23637;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#28085;&#30422;&#19982;&#35266;&#28857;&#25688;&#35201;&#35780;&#20272;&#30456;&#20851;&#30340;7&#20010;&#32500;&#24230;&#30340;SUMMEVAL-OP&#25968;&#25454;&#38598;&#65306;&#27969;&#30021;&#24615;&#12289;&#36830;&#36143;&#24615;&#12289;&#30456;&#20851;&#24615;&#12289;&#24544;&#23454;&#24230;&#12289;&#26041;&#38754;&#35206;&#30422;&#12289;&#24773;&#24863;&#19968;&#33268;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102; Op-I-Prompt&#65292;&#19968;&#20010;&#29420;&#31435;&#20110;&#32500;&#24230;&#30340;&#25552;&#31034;&#65292;&#20197;&#21450; Op-Prompts&#65292;&#19968;&#20010;&#20381;&#36182;&#20110;&#32500;&#24230;&#30340;&#29992;&#20110;&#35266;&#28857;&#25688;&#35201;&#35780;&#20272;&#30340;&#25552;&#31034;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Op-I-Prompt &#26159;&#35780;&#20272;&#35266;&#28857;&#25688;&#35201;&#30340;&#19968;&#20010;&#24456;&#22909;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#24179;&#22343; Spearman &#30456;&#20851;&#24615;&#20026; 0.70&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11683v1 Announce Type: new  Abstract: Evaluation of opinion summaries using conventional reference-based metrics rarely provides a holistic evaluation and has been shown to have a relatively low correlation with human judgments. Recent studies suggest using Large Language Models (LLMs) as reference-free metrics for NLG evaluation, however, they remain unexplored for opinion summary evaluation. Moreover, limited opinion summary evaluation datasets inhibit progress. To address this, we release the SUMMEVAL-OP dataset covering 7 dimensions related to the evaluation of opinion summaries: fluency, coherence, relevance, faithfulness, aspect coverage, sentiment consistency, and specificity. We investigate Op-I-Prompt a dimension-independent prompt, and Op-Prompts, a dimension-dependent set of prompts for opinion summary evaluation. Experiments indicate that Op-I-Prompt emerges as a good alternative for evaluating opinion summaries achieving an average Spearman correlation of 0.70 w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#35760;&#24518;&#21644;&#20998;&#22359;&#30340;&#26368;&#23567;&#35748;&#30693;&#26550;&#26500;&#65292;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#65292;&#33021;&#22815;&#23398;&#20064;&#20154;&#36896;&#35821;&#35328;&#24182;&#25552;&#21462;&#25903;&#25345;&#23398;&#20064;&#30340;&#35821;&#27861;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.11681</link><description>&lt;p&gt;
&#25171;&#24320;&#35821;&#35328;&#20064;&#24471;&#30340;&#40657;&#21283;&#23376;
&lt;/p&gt;
&lt;p&gt;
Opening the black box of language acquisition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11681
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24207;&#21015;&#35760;&#24518;&#21644;&#20998;&#22359;&#30340;&#26368;&#23567;&#35748;&#30693;&#26550;&#26500;&#65292;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#65292;&#33021;&#22815;&#23398;&#20064;&#20154;&#36896;&#35821;&#35328;&#24182;&#25552;&#21462;&#25903;&#25345;&#23398;&#20064;&#30340;&#35821;&#27861;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#37325;&#26032;&#28608;&#21457;&#20102;&#20154;&#20204;&#23545;&#35821;&#35328;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#34920;&#31034;&#25152;&#23398;&#35821;&#35328;&#30340;&#35821;&#27861;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20351;&#29992;&#20043;&#21069;&#24517;&#39035;&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#24615;&#12289;&#26356;&#36879;&#26126;&#19988;&#35748;&#30693;&#21512;&#29702;&#30340;&#23398;&#20064;&#35821;&#35328;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#26159;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#65292;&#32780;&#26159;&#22522;&#20110;&#24207;&#21015;&#35760;&#24518;&#21644;&#20998;&#22359;&#30340;&#26368;&#23567;&#35748;&#30693;&#26550;&#26500;&#12290;&#23398;&#20064;&#26426;&#21046;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21407;&#21017;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#31867;&#20284;&#33258;&#28982;&#35821;&#35328;&#30340;&#29609;&#20855;&#35821;&#35328;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#36825;&#20123;&#20154;&#36896;&#35821;&#35328;&#65292;&#24182;&#25552;&#21462;&#25903;&#25345;&#23398;&#20064;&#30340;&#35821;&#27861;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#36825;&#31181;&#31616;&#21333;&#26550;&#26500;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11681v1 Announce Type: new  Abstract: Recent advances in large language models using deep learning techniques have renewed interest on how languages can be learned from data. However, it is unclear whether or how these models represent grammatical information from the learned languages. In addition, the models must be pre-trained on large corpora before they can be used. In this work, we propose an alternative, more transparent and cognitively plausible architecture for learning language. Instead of using deep learning, our approach uses a minimal cognitive architecture based on sequence memory and chunking. The learning mechanism is based on the principles of reinforcement learning. We test our architecture on a number of natural-like toy languages. Results show that the model can learn these artificial languages from scratch and extract grammatical information that supports learning. Our study demonstrates the power of this simple architecture and stresses the importance o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21453;&#21465;&#20107;&#65292;&#36890;&#36807;5&#20010;&#26041;&#38754;&#20174;&#19987;&#38376; NGO &#25351;&#21335;&#20013;&#25552;&#21462;&#23450;&#20041;&#30340;&#20869;&#23481;&#65292;&#20197;&#35299;&#20915;&#20197;&#24448;&#35780;&#20272;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11676</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21453;&#21465;&#20107;&#35780;&#20272;&#30340;&#22810;&#26041;&#38754;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11676
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21453;&#21465;&#20107;&#65292;&#36890;&#36807;5&#20010;&#26041;&#38754;&#20174;&#19987;&#38376; NGO &#25351;&#21335;&#20013;&#25552;&#21462;&#23450;&#20041;&#30340;&#20869;&#23481;&#65292;&#20197;&#35299;&#20915;&#20197;&#24448;&#35780;&#20272;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21465;&#20107;&#26159;&#23545;&#20167;&#24680;&#35328;&#35770;&#32972;&#26223;&#30340;&#30693;&#24773;&#22238;&#24212;&#65292;&#26088;&#22312;&#39539;&#26021;&#20167;&#24680;&#20027;&#24352;&#24182;&#21270;&#35299;&#20914;&#31361;&#65292;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#20167;&#24680;&#35328;&#35770;&#24178;&#39044;&#31574;&#30053;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#33258;&#21160;&#29983;&#25104;&#21453;&#21465;&#20107;&#30340;&#26041;&#27861;&#26469;&#36741;&#21161;&#25163;&#21160;&#24178;&#39044;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#35780;&#20272;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#21457;&#23637;&#12290;&#20808;&#21069;&#29992;&#20110;&#21453;&#21465;&#20107;&#35780;&#20272;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#32570;&#20047;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#19968;&#33268;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#34920;&#38754;&#21442;&#32771;&#27604;&#36739;&#65292;&#32780;&#19981;&#26159;&#23558;&#21453;&#21465;&#20107;&#36136;&#37327;&#30340;&#20851;&#38190;&#26041;&#38754;&#32435;&#20837;&#35780;&#20272;&#26631;&#20934;&#12290;&#20026;&#35299;&#20915;&#20808;&#21069;&#30340;&#35780;&#20272;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20419;&#20351;LLM&#25552;&#20379;&#29983;&#25104;&#30340;&#21453;&#21465;&#20107;&#20505;&#36873;&#30340;&#24471;&#20998;&#21644;&#21453;&#39304;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#19987;&#38376;NGO&#30340;&#21453;&#21465;&#20107;&#25351;&#21335;&#20013;&#25552;&#21462;&#30340;5&#20010;&#23450;&#20041;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11676v1 Announce Type: cross  Abstract: Counter narratives - informed responses to hate speech contexts designed to refute hateful claims and de-escalate encounters - have emerged as an effective hate speech intervention strategy. While previous work has proposed automatic counter narrative generation methods to aid manual interventions, the evaluation of these approaches remains underdeveloped. Previous automatic metrics for counter narrative evaluation lack alignment with human judgment as they rely on superficial reference comparisons instead of incorporating key aspects of counter narrative quality as evaluation criteria. To address prior evaluation limitations, we propose a novel evaluation framework prompting LLMs to provide scores and feedback for generated counter narrative candidates using 5 defined aspects derived from guidelines from counter narrative specialized NGOs. We found that LLM evaluators achieve strong alignment to human-annotated scores and feedback and
&lt;/p&gt;</description></item><item><title>&#35813;&#39033;&#30446;&#25104;&#21151;&#24320;&#21457;&#20102;&#29233;&#27801;&#23612;&#20122;&#35821;&#35328;&#30340;&#25340;&#20889;&#21644;&#35821;&#27861;&#26657;&#27491;&#24037;&#20855;&#65292;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#33258;&#21160;&#35780;&#20272;&#26469;&#20811;&#26381;&#21487;&#29992;&#25968;&#25454;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11671</link><description>&lt;p&gt;
&#29233;&#27801;&#23612;&#20122;&#25991;&#26412;&#30340;&#33258;&#21160;&#26657;&#27491;&#65306;EKTB25&#39033;&#30446;&#30340;&#26368;&#32456;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Autocorrect for Estonian texts: final report from project EKTB25
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11671
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#25104;&#21151;&#24320;&#21457;&#20102;&#29233;&#27801;&#23612;&#20122;&#35821;&#35328;&#30340;&#25340;&#20889;&#21644;&#35821;&#27861;&#26657;&#27491;&#24037;&#20855;&#65292;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#33258;&#21160;&#35780;&#20272;&#26469;&#20811;&#26381;&#21487;&#29992;&#25968;&#25454;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#30001;&#29233;&#27801;&#23612;&#20122;&#35821;&#35328;&#25216;&#26415;&#22269;&#23478;&#35745;&#21010;&#20110;2021-2023&#24180;&#36164;&#21161;&#65292;&#26088;&#22312;&#20026;&#29233;&#27801;&#23612;&#20122;&#35821;&#24320;&#21457;&#25340;&#20889;&#21644;&#35821;&#27861;&#26657;&#27491;&#24037;&#20855;&#12290;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#25152;&#38656;&#29992;&#20110;&#27492;&#31867;&#24320;&#21457;&#30340;&#21487;&#29992;&#38169;&#35823;&#26657;&#27491;&#25968;&#25454;&#37327;&#38750;&#24120;&#23567;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;(1) &#25105;&#20204;&#20026;&#27169;&#22411;&#35757;&#32451;&#21644;&#27979;&#35797;&#27880;&#37322;&#20102;&#26356;&#22810;&#30340;&#26657;&#27491;&#25968;&#25454;&#65292;(2) &#25105;&#20204;&#27979;&#35797;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;&#21363;&#37325;&#26032;&#35757;&#32451;&#20026;&#20854;&#20182;&#20219;&#21153;&#21019;&#24314;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20415;&#19981;&#20165;&#20381;&#36182;&#20110;&#26657;&#27491;&#25968;&#25454;&#65292;(3) &#25105;&#20204;&#23545;&#27604;&#20102;&#24320;&#21457;&#30340;&#26041;&#27861;&#21644;&#27169;&#22411;&#19982;&#20854;&#20182;&#36873;&#25321;&#65292;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#33258;&#21160;&#35780;&#20272;&#65292;&#21487;&#20197;&#36890;&#36807;&#38169;&#35823;&#31867;&#21035;&#35745;&#31639;&#20462;&#27491;&#30340;&#20934;&#30830;&#24615;&#21644;&#25910;&#30410;&#65292;&#20174;&#32780;&#21487;&#20197;&#35814;&#32454;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11671v1 Announce Type: cross  Abstract: The project was funded in 2021-2023 by the National Programme of Estonian Language Technology. Its main aim was to develop spelling and grammar correction tools for the Estonian language. The main challenge was the very small amount of available error correction data needed for such development. To mitigate this, (1) we annotated more correction data for model training and testing, (2) we tested transfer-learning, i.e. retraining machine learning models created for other tasks, so as not to depend solely on correction data, (3) we compared the developed method and model with alternatives, including large language models. We also developed automatic evaluation, which can calculate the accuracy and yield of corrections by error category, so that the effectiveness of different methods can be compared in detail.   There has been a breakthrough in large language models during the project: GPT4, a commercial language model with Estonian-lang
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26426;&#21046;&#20043;&#20105;&#30340;&#27010;&#24565;&#65292;&#20851;&#27880;&#35821;&#35328;&#27169;&#22411;&#20013;&#22810;&#20010;&#26426;&#21046;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#31454;&#20105;&#36807;&#31243;&#65292;&#20197;&#21450;&#24433;&#21709;&#26576;&#20123;&#26426;&#21046;&#24378;&#24230;&#30340;&#27880;&#24847;&#21147;&#20301;&#32622;&#12290;</title><link>https://arxiv.org/abs/2402.11655</link><description>&lt;p&gt;
&#26426;&#21046;&#20043;&#20105;&#65306;&#36861;&#36394;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#20107;&#23454;&#21644;&#34394;&#25311;&#35821;&#22659;&#30340;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26426;&#21046;&#20043;&#20105;&#30340;&#27010;&#24565;&#65292;&#20851;&#27880;&#35821;&#35328;&#27169;&#22411;&#20013;&#22810;&#20010;&#26426;&#21046;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#31454;&#20105;&#36807;&#31243;&#65292;&#20197;&#21450;&#24433;&#21709;&#26576;&#20123;&#26426;&#21046;&#24378;&#24230;&#30340;&#27880;&#24847;&#21147;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#26088;&#22312;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32463;&#39564;&#25104;&#21151;&#21644;&#25105;&#20204;&#23545;&#20869;&#37096;&#26426;&#21046;&#30340;&#31185;&#23398;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26426;&#21046;&#20043;&#20105;&#30340;&#24418;&#24335;&#65292;&#20854;&#19981;&#20877;&#20851;&#27880;&#21333;&#20010;&#26426;&#21046;&#65292;&#32780;&#26159;&#20851;&#27880;&#22810;&#20010;&#26426;&#21046;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#36861;&#36394;&#20854;&#20013;&#19968;&#20010;&#22312;&#26368;&#32456;&#39044;&#27979;&#20013;&#22914;&#20309;&#25104;&#20026;&#20027;&#23548;&#22240;&#32032;&#12290;&#25105;&#20204;&#21033;&#29992;logit&#26816;&#39564;&#21644;&#27880;&#24847;&#21147;&#20462;&#25913;&#20004;&#31181;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#25581;&#31034;&#20102;&#26426;&#21046;&#20043;&#20105;&#22312;LLMs&#20013;&#30340;&#21457;&#29983;&#26041;&#24335;&#21644;&#20301;&#32622;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26174;&#31034;&#20102;&#26426;&#21046;&#21450;&#20854;&#22312;&#21508;&#31181;&#27169;&#22411;&#32452;&#20214;&#20013;&#30340;&#31454;&#20105;&#30165;&#36857;&#65292;&#24182;&#25581;&#31034;&#20102;&#26377;&#25928;&#25511;&#21046;&#29305;&#23450;&#26426;&#21046;&#24378;&#24230;&#30340;&#27880;&#24847;&#21147;&#20301;&#32622;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#20301;&#20110;https://github.com/francescortu/Competition_of
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11655v1 Announce Type: new  Abstract: Interpretability research aims to bridge the gap between the empirical success and our scientific understanding of the inner workings of large language models (LLMs). However, most existing research in this area focused on analyzing a single mechanism, such as how models copy or recall factual knowledge. In this work, we propose the formulation of competition of mechanisms, which instead of individual mechanisms focuses on the interplay of multiple mechanisms, and traces how one of them becomes dominant in the final prediction. We uncover how and where the competition of mechanisms happens within LLMs using two interpretability methods, logit inspection and attention modification. Our findings show traces of the mechanisms and their competition across various model components, and reveal attention positions that effectively control the strength of certain mechanisms. Our code and data are at https://github.com/francescortu/Competition_of
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#36127;&#38754;&#31034;&#20363;&#21644;&#36866;&#24403;&#30340;&#25968;&#25454;&#28165;&#27927;&#19982;&#24494;&#35843;&#31574;&#30053;&#65292;&#20174;&#22833;&#36133;&#20013;&#23398;&#20064;&#65292;&#25552;&#39640;&#20316;&#20026;&#20195;&#29702;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.11651</link><description>&lt;p&gt;
&#20174;&#22833;&#36133;&#20013;&#23398;&#20064;&#65306;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26102;&#25972;&#21512;&#36127;&#38754;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11651
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25972;&#21512;&#36127;&#38754;&#31034;&#20363;&#21644;&#36866;&#24403;&#30340;&#25968;&#25454;&#28165;&#27927;&#19982;&#24494;&#35843;&#31574;&#30053;&#65292;&#20174;&#22833;&#36133;&#20013;&#23398;&#20064;&#65292;&#25552;&#39640;&#20316;&#20026;&#20195;&#29702;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20805;&#24403;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#30340;&#24037;&#20855;&#65288;&#22914;&#25628;&#32034;&#24341;&#25806;&#65289;&#26102;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#35757;&#32451;&#25110;&#23545;&#40784;&#36807;&#31243;&#20013;&#24182;&#26410;&#19987;&#38376;&#38024;&#23545;&#24037;&#20855;&#20351;&#29992;&#36827;&#34892;&#20248;&#21270;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#20316;&#20026;&#20195;&#29702;&#30340;&#25928;&#26524;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25910;&#38598;&#20102;GPT-4&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#20132;&#20114;&#36712;&#36857;&#65292;&#24182;&#29992;&#23427;&#20204;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20316;&#20026;&#36825;&#19968;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#26631;&#20934;&#26041;&#27861;&#36890;&#24120;&#26159;&#31616;&#21333;&#22320;&#20002;&#24323;&#26410;&#25104;&#21151;&#23436;&#25104;&#20219;&#21153;&#30340;&#36712;&#36857;&#65292;&#36825;&#19968;&#26041;&#38754;&#23548;&#33268;&#20102;&#25968;&#25454;&#21644;&#36164;&#28304;&#30340;&#26174;&#33879;&#28010;&#36153;&#65292;&#21478;&#19968;&#26041;&#38754;&#26377;&#21487;&#33021;&#38480;&#21046;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#20248;&#21270;&#36335;&#24452;&#12290;&#26412;&#25991;&#35748;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#25968;&#25454;&#28165;&#27927;&#21644;&#24494;&#35843;&#31574;&#30053;&#20174;&#22833;&#36133;&#20013;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#25968;&#23398;&#25512;&#29702;&#12289;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#21644;&#25112;&#30053;&#26041;&#38754;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11651v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools like search engines. However, LLMs are not optimized specifically for tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has collected interaction trajectories between GPT-4 and environments, and fine-tuned smaller models with them. As part of this, the standard approach has been to simply discard trajectories that do not finish the task successfully, which, on the one hand, leads to a significant waste of data and resources, and on the other hand, has the potential to limit the possible optimization paths during fine-tuning. In this paper, we contend that large language models can learn from failures through appropriate data cleaning and fine-tuning strategies. We conduct experiments on mathematical reasoning, multi-hop question answering, and strategic 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21387;&#21147;&#27979;&#35797;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#22312;&#24694;&#24847;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#26816;&#27979;&#22120;&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#37117;&#34920;&#29616;&#19981;&#31283;&#23450;&#65292;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;35%&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#21892;&#40065;&#26834;&#24615;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.11638</link><description>&lt;p&gt;
&#32458;&#33050;&#30707;&#65306;&#22312;&#25915;&#20987;&#19979;&#23545;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21387;&#21147;&#27979;&#35797;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#22312;&#24694;&#24847;&#25915;&#20987;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#26816;&#27979;&#22120;&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#37117;&#34920;&#29616;&#19981;&#31283;&#23450;&#65292;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;35%&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#21892;&#40065;&#26834;&#24615;&#30340;&#21021;&#27493;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24191;&#27867;&#20351;&#29992;&#22686;&#21152;&#20102;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#20197;&#38450;&#27490;&#28389;&#29992;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#22312;&#29616;&#23454;&#22330;&#26223;&#19979;&#23545;&#26816;&#27979;&#22120;&#23545;&#24694;&#24847;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#12290;&#25105;&#20204;&#20840;&#38754;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#22312;&#19981;&#21516;&#25915;&#20987;&#31867;&#22411;&#19979;&#30340;&#40065;&#26834;&#24615;&#65306;&#32534;&#36753;&#12289;&#25913;&#20889;&#12289;&#25552;&#31034;&#21644;&#20849;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#25915;&#20987;&#20551;&#35774;&#23545;&#29983;&#25104;LLMs&#30340;&#35775;&#38382;&#21463;&#38480;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#39044;&#31639;&#27700;&#24179;&#19979;&#26816;&#27979;&#22120;&#22312;&#19981;&#21516;&#25915;&#20987;&#19979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#20960;&#20046;&#27809;&#26377;&#29616;&#26377;&#26816;&#27979;&#22120;&#22312;&#25152;&#26377;&#25915;&#20987;&#19979;&#20445;&#25345;&#31283;&#20581;&#65292;&#25152;&#26377;&#26816;&#27979;&#22120;&#37117;&#23637;&#31034;&#20986;&#19981;&#21516;&#30340;&#28431;&#27934;&#12290;&#24179;&#22343;&#25152;&#26377;&#26816;&#27979;&#22120;&#65292;&#22312;&#25152;&#26377;&#25915;&#20987;&#19979;&#24615;&#33021;&#19979;&#38477;&#20102;35%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20123;&#32570;&#38519;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#21021;&#27493;&#30340;&#21363;&#25554;&#21363;&#29992;&#34917;&#19969;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11638v1 Announce Type: new  Abstract: The widespread use of large language models (LLMs) is increasing the demand for methods that detect machine-generated text to prevent misuse. The goal of our study is to stress test the detectors' robustness to malicious attacks under realistic scenarios. We comprehensively study the robustness of popular machine-generated text detectors under attacks from diverse categories: editing, paraphrasing, prompting, and co-generating. Our attacks assume limited access to the generator LLMs, and we compare the performance of detectors on different attacks under different budget levels. Our experiments reveal that almost none of the existing detectors remain robust under all the attacks, and all detectors exhibit different loopholes. Averaging all detectors, the performance drops by 35% across all attacks. Further, we investigate the reasons behind these defects and propose initial out-of-the-box patches to improve robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SOLID&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#25105;&#25773;&#31181;&#21644;&#22810;&#24847;&#22270;&#33258;&#25105;&#25351;&#23548;&#26041;&#26696;&#26469;&#23454;&#29616;LLMs&#29983;&#25104;&#24847;&#22270;&#24863;&#30693;&#30340;&#20449;&#24687;&#26816;&#32034;&#23545;&#35805;&#12290;</title><link>https://arxiv.org/abs/2402.11633</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#25105;&#25773;&#31181;&#21644;&#22810;&#24847;&#22270;&#33258;&#25105;&#25351;&#23548;&#30340;LLM&#29983;&#25104;&#24847;&#22270;&#24863;&#30693;&#30340;&#20449;&#24687;&#26816;&#32034;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Self-seeding and Multi-intent Self-instructing LLMs for Generating Intent-aware Information-Seeking dialogs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SOLID&#27169;&#22411;&#65292;&#21033;&#29992;&#33258;&#25105;&#25773;&#31181;&#21644;&#22810;&#24847;&#22270;&#33258;&#25105;&#25351;&#23548;&#26041;&#26696;&#26469;&#23454;&#29616;LLMs&#29983;&#25104;&#24847;&#22270;&#24863;&#30693;&#30340;&#20449;&#24687;&#26816;&#32034;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#20449;&#24687;&#26816;&#32034;&#23545;&#35805;&#20013;&#29992;&#25143;&#24847;&#22270;&#23545;&#20110;&#31995;&#32479;&#28385;&#36275;&#29992;&#25143;&#20449;&#24687;&#38656;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;&#24847;&#22270;&#39044;&#27979;&#65288;IP&#65289;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#38656;&#35201;&#20805;&#20998;&#30340;&#19982;&#20154;&#24037;&#26631;&#27880;&#24847;&#22270;&#23545;&#35805;&#29992;&#20110;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#27880;&#37322;&#24847;&#22270;&#36164;&#28304;&#23494;&#38598;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23578;&#26080;&#30740;&#31350;&#20351;&#29992;LLMs&#29983;&#25104;&#24847;&#22270;&#24863;&#30693;&#30340;&#20449;&#24687;&#26816;&#32034;&#23545;&#35805;&#12290;&#26412;&#25991;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#21033;&#29992;LLMs&#36827;&#34892;&#38646;-shot&#29983;&#25104;&#22823;&#35268;&#27169;&#12289;&#24320;&#25918;&#39046;&#22495;&#21644;&#24847;&#22270;&#24863;&#30693;&#30340;&#20449;&#24687;&#26816;&#32034;&#23545;&#35805;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SOLID&#65292;&#20854;&#20013;&#21253;&#25324;&#26032;&#39062;&#30340;&#33258;&#25105;&#25773;&#31181;&#21644;&#22810;&#24847;&#22270;&#33258;&#25105;&#25351;&#23548;&#26041;&#26696;&#12290;&#21069;&#32773;&#36890;&#36807;&#21033;&#29992;LLM&#33258;&#36523;&#30340;&#30693;&#35782;&#33539;&#22260;&#26469;&#21551;&#21160;&#23545;&#35805;&#29983;&#25104;&#26469;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65307;&#21518;&#32773;&#20419;&#20351;LLM&#25353;&#39034;&#24207;&#29983;&#25104;&#35805;&#35821;&#65292;&#24182;&#36890;&#36807;&#35201;&#27714;LLM&#33258;&#21160;&#23436;&#25104;&#35805;&#39064;&#35774;&#35745;&#26469;&#20943;&#36731;&#25163;&#21160;&#35805;&#39064;&#35774;&#35745;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11633v1 Announce Type: new  Abstract: Identifying user intents in information-seeking dialogs is crucial for a system to meet user's information needs. Intent prediction (IP) is challenging and demands sufficient dialogs with human-labeled intents for training. However, manually annotating intents is resource-intensive. While large language models (LLMs) have been shown to be effective in generating synthetic data, there is no study on using LLMs to generate intent-aware information-seeking dialogs. In this paper, we focus on leveraging LLMs for zero-shot generation of large-scale, open-domain, and intent-aware information-seeking dialogs. We propose SOLID, which has novel self-seeding and multi-intent self-instructing schemes. The former improves the generation quality by using the LLM's own knowledge scope to initiate dialog generation; the latter prompts the LLM to generate utterances sequentially, and mitigates the need for manual prompt design by asking the LLM to auton
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;MetaRAG&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36807;&#31243;&#19982;&#20803;&#35748;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#35748;&#30693;&#35843;&#33410;&#27969;&#31243;&#65292;&#20351;&#27169;&#22411;&#20855;&#26377;&#30417;&#35270;&#12289;&#35780;&#20272;&#21644;&#35268;&#21010;&#20854;&#21709;&#24212;&#31574;&#30053;&#30340;&#33021;&#21147;&#65292;&#22686;&#24378;&#20102;&#20854;&#20869;&#30465;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.11626</link><description>&lt;p&gt;
&#20803;&#35748;&#30693;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Metacognitive Retrieval-Augmented Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MetaRAG&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36807;&#31243;&#19982;&#20803;&#35748;&#30693;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#35748;&#30693;&#35843;&#33410;&#27969;&#31243;&#65292;&#20351;&#27169;&#22411;&#20855;&#26377;&#30417;&#35270;&#12289;&#35780;&#20272;&#21644;&#35268;&#21010;&#20854;&#21709;&#24212;&#31574;&#30053;&#30340;&#33021;&#21147;&#65292;&#22686;&#24378;&#20102;&#20854;&#20869;&#30465;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22312;&#29983;&#25104;&#20107;&#23454;&#20869;&#23481;&#26041;&#38754;&#30340;&#39640;&#25928;&#24615;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#24050;&#32463;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26680;&#24515;&#12290; &#20256;&#32479;&#26041;&#27861;&#20351;&#29992;&#21333;&#27425;&#26816;&#32034;&#65292;&#32780;&#26368;&#36817;&#26356;&#20542;&#21521;&#20110;&#22810;&#27425;&#26816;&#32034;&#20197;&#25191;&#34892;&#22810;&#36339;&#25512;&#29702;&#20219;&#21153;&#12290; &#28982;&#32780;&#65292;&#36825;&#20123;&#31574;&#30053;&#21463;&#21040;&#39044;&#23450;&#20041;&#25512;&#29702;&#27493;&#39588;&#30340;&#38480;&#21046;&#65292;&#21487;&#33021;&#23548;&#33268;&#21709;&#24212;&#29983;&#25104;&#30340;&#19981;&#20934;&#30830;&#24615;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;MetaRAG&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#36807;&#31243;&#19982;&#20803;&#35748;&#30693;&#30340;&#26041;&#27861;&#12290; &#20511;&#37492;&#35748;&#30693;&#24515;&#29702;&#23398;&#65292;&#20803;&#35748;&#30693;&#20351;&#23454;&#20307;&#33021;&#22815;&#33258;&#25105;&#21453;&#24605;&#24182;&#25209;&#21028;&#24615;&#35780;&#20272;&#20854;&#35748;&#30693;&#36807;&#31243;&#12290; &#36890;&#36807;&#25972;&#21512;&#36825;&#19968;&#28857;&#65292;MetaRAG&#20351;&#27169;&#22411;&#33021;&#22815;&#30417;&#35270;&#12289;&#35780;&#20272;&#21644;&#35268;&#21010;&#20854;&#21709;&#24212;&#31574;&#30053;&#65292;&#22686;&#24378;&#20854;&#20869;&#30465;&#25512;&#29702;&#33021;&#21147;&#12290; &#36890;&#36807;&#19977;&#27493;&#20803;&#35748;&#30693;&#35843;&#33410;&#27969;&#31243;&#65292;&#27169;&#22411;&#33021;&#22815;&#35782;&#21035;&#21021;&#22987;&#35748;&#30693;&#21709;&#24212;&#20013;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24182;&#21152;&#20197;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11626v1 Announce Type: new  Abstract: Retrieval-augmented generation have become central in natural language processing due to their efficacy in generating factual content. While traditional methods employ single-time retrieval, more recent approaches have shifted towards multi-time retrieval for multi-hop reasoning tasks. However, these strategies are bound by predefined reasoning steps, potentially leading to inaccuracies in response generation. This paper introduces MetaRAG, an approach that combines the retrieval-augmented generation process with metacognition. Drawing from cognitive psychology, metacognition allows an entity to self-reflect and critically evaluate its cognitive processes. By integrating this, MetaRAG enables the model to monitor, evaluate, and plan its response strategies, enhancing its introspective reasoning abilities. Through a three-step metacognitive regulation pipeline, the model can identify inadequacies in initial cognitive responses and fixes t
&lt;/p&gt;</description></item><item><title>SpeCrawler&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#19981;&#21516;API&#25991;&#26723;&#29983;&#25104;OpenAPI&#35268;&#33539;&#65292;&#26377;&#21161;&#20110;&#31616;&#21270;API&#38598;&#25104;&#27969;&#31243;&#24182;&#20419;&#36827;&#24037;&#20855;&#25972;&#21512;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;</title><link>https://arxiv.org/abs/2402.11625</link><description>&lt;p&gt;
SpeCrawler&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;API&#25991;&#26723;&#29983;&#25104;OpenAPI&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
SpeCrawler: Generating OpenAPI Specifications from API Documentation Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11625
&lt;/p&gt;
&lt;p&gt;
SpeCrawler&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#19981;&#21516;API&#25991;&#26723;&#29983;&#25104;OpenAPI&#35268;&#33539;&#65292;&#26377;&#21161;&#20110;&#31616;&#21270;API&#38598;&#25104;&#27969;&#31243;&#24182;&#20419;&#36827;&#24037;&#20855;&#25972;&#21512;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;API&#30340;&#24191;&#27867;&#20351;&#29992;&#26159;&#26174;&#32780;&#26131;&#35265;&#30340;&#12290;&#20294;&#30001;&#20110;&#22312;&#32447;API&#25991;&#26723;&#20013;&#35266;&#23519;&#21040;&#30340;&#32467;&#26500;&#24046;&#24322;&#65292;&#23545;API&#30340;&#21487;&#25193;&#23637;&#21033;&#29992;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#36825;&#20984;&#26174;&#20102;&#38656;&#35201;&#33258;&#21160;&#21270;&#24037;&#20855;&#26469;&#20419;&#36827;API&#30340;&#20351;&#29992;&#12290;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#26159;&#23558;&#25991;&#26723;&#36716;&#25442;&#25104;API&#35268;&#33539;&#26684;&#24335;&#12290;&#23613;&#31649;&#20808;&#21069;&#23581;&#35797;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#36328;&#19981;&#21516;&#25991;&#26723;&#20013;&#26222;&#36941;&#21270;&#26041;&#38754;&#36935;&#21040;&#20102;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SpeCrawler&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#27969;&#31243;&#20174;&#19981;&#21516;&#30340;API&#25991;&#26723;&#29983;&#25104;OpenAPI&#35268;&#33539;&#12290;&#36890;&#36807;&#20026;&#20247;&#22810;API&#21019;&#24314;&#19968;&#20010;&#26631;&#20934;&#21270;&#26684;&#24335;&#65292;SpeCrawler&#26377;&#21161;&#20110;&#31616;&#21270;API&#32534;&#25490;&#31995;&#32479;&#20013;&#30340;&#38598;&#25104;&#27969;&#31243;&#65292;&#24182;&#20419;&#36827;&#23558;&#24037;&#20855;&#25972;&#21512;&#21040;LLMs&#20013;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;SpeCrawler&#30340;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11625v1 Announce Type: new  Abstract: In the digital era, the widespread use of APIs is evident. However, scalable utilization of APIs poses a challenge due to structure divergence observed in online API documentation. This underscores the need for automatic tools to facilitate API consumption. A viable approach involves the conversion of documentation into an API Specification format. While previous attempts have been made using rule-based methods, these approaches encountered difficulties in generalizing across diverse documentation. In this paper we introduce SpeCrawler, a comprehensive system that utilizes large language models (LLMs) to generate OpenAPI Specifications from diverse API documentation through a carefully crafted pipeline. By creating a standardized format for numerous APIs, SpeCrawler aids in streamlining integration processes within API orchestrating systems and facilitating the incorporation of tools into LLMs. The paper explores SpeCrawler's methodology
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#38381;&#29615;&#30340;&#26694;&#26550;&#65288;LogicCheckGPT&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.11622</link><description>&lt;p&gt;
&#36923;&#36753;&#38381;&#29615;&#65306;&#25581;&#31034;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23545;&#35937;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11622
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#38381;&#29615;&#30340;&#26694;&#26550;&#65288;LogicCheckGPT&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26412;&#36523;&#26469;&#26816;&#27979;&#21644;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35937;&#24187;&#35273;&#19968;&#30452;&#26159;&#38459;&#30861;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#26356;&#24191;&#27867;&#24212;&#29992;&#30340;&#36719;&#32907;&#12290;&#23545;&#35937;&#24187;&#35273;&#26159;&#25351;LVLMs&#22312;&#22270;&#20687;&#20013;&#22768;&#31216;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30340;&#29616;&#35937;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#25351;&#23548;&#35843;&#25972;&#21644;&#22522;&#20110;&#22806;&#37096;&#27169;&#22411;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#22806;&#37096;&#27169;&#22411;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#26410;&#28145;&#20837;&#25506;&#35752;&#30340;&#39046;&#22495;&#65292;&#21363;&#21033;&#29992;LVLM&#26412;&#36523;&#26469;&#20943;&#36731;&#23545;&#35937;&#24187;&#35273;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36825;&#26679;&#30340;&#30452;&#35273;&#65292;&#21363;LVLM&#20542;&#21521;&#20110;&#23545;&#23384;&#22312;&#30340;&#23545;&#35937;&#20570;&#20986;&#36923;&#36753;&#19968;&#33268;&#30340;&#21453;&#24212;&#65292;&#20294;&#23545;&#24187;&#35273;&#23545;&#35937;&#20570;&#20986;&#19981;&#19968;&#33268;&#30340;&#21453;&#24212;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36923;&#36753;&#38381;&#29615;&#30340;&#23545;&#35937;&#24187;&#35273;&#26816;&#27979;&#21644;&#20943;&#36731;&#26694;&#26550;&#65292;&#21363;LogicCheckGPT&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36923;&#36753;&#19968;&#33268;&#24615;&#25506;&#27979;&#26469;&#25552;&#20986;&#20855;&#26377;&#36923;&#36753;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11622v1 Announce Type: cross  Abstract: Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical corr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;GPT-3.5 Turbo&#12289;GPT-4&#21644;Flan-T5&#27169;&#22411;&#22312;&#35782;&#21035;&#26032;&#38395;&#26631;&#39064;&#20013;&#26694;&#26550;&#20559;&#35265;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#21487;&#35299;&#37322;&#25552;&#31034;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#65292;GPT-4&#22312;&#23569;&#23556;&#22330;&#26223;&#20013;&#34920;&#29616;&#36739;&#22909;&#65292;&#32780;FLAN-T5&#30340;&#34920;&#29616;&#36739;&#24046;&#65292;&#25351;&#20986;&#36739;&#23567;&#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#26356;&#22810;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2402.11621</link><description>&lt;p&gt;
&#35299;&#30721;&#26032;&#38395;&#21465;&#20107;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26694;&#26550;&#20559;&#35265;&#26816;&#27979;&#20013;&#30340;&#20851;&#38190;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Decoding News Narratives: A Critical Analysis of Large Language Models in Framing Bias Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11621
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;GPT-3.5 Turbo&#12289;GPT-4&#21644;Flan-T5&#27169;&#22411;&#22312;&#35782;&#21035;&#26032;&#38395;&#26631;&#39064;&#20013;&#26694;&#26550;&#20559;&#35265;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#21487;&#35299;&#37322;&#25552;&#31034;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#65292;GPT-4&#22312;&#23569;&#23556;&#22330;&#26223;&#20013;&#34920;&#29616;&#36739;&#22909;&#65292;&#32780;FLAN-T5&#30340;&#34920;&#29616;&#36739;&#24046;&#65292;&#25351;&#20986;&#36739;&#23567;&#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#26356;&#22810;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#36890;&#36807;&#26816;&#39564;GPT-3.5 Turbo&#12289;GPT-4&#21644;Flan-T5&#27169;&#22411;&#22312;&#36890;&#36807;&#38646;&#23556;&#12289;&#23569;&#23556;&#21644;&#21487;&#35299;&#37322;&#25552;&#31034;&#26041;&#27861;&#26816;&#27979;&#26032;&#38395;&#26631;&#39064;&#20013;&#26694;&#26550;&#20559;&#35265;&#30340;&#34920;&#29616;&#65292;&#20026;LLMs&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#36866;&#29992;&#24615;&#19981;&#26029;&#25193;&#23637;&#30340;&#30740;&#31350;&#20570;&#20986;&#36129;&#29486;&#12290;&#25105;&#20204;&#35780;&#20272;&#30340;&#19968;&#20010;&#20851;&#38190;&#27934;&#23519;&#26159;&#65292;&#21487;&#35299;&#37322;&#25552;&#31034;&#22312;&#25552;&#21319;&#36825;&#20123;&#27169;&#22411;&#21487;&#38752;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#25928;&#26524;&#65292;&#20984;&#26174;&#20102;&#35299;&#37322;&#35774;&#32622;&#23545;&#20110;&#31038;&#20250;&#31185;&#23398;&#20851;&#20110;&#26694;&#26550;&#20559;&#35265;&#30340;&#30740;&#31350;&#30340;&#37325;&#35201;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;GPT-4&#22312;&#25552;&#20379;&#19968;&#31995;&#21015;&#30456;&#20851;&#39046;&#22495;&#20869;&#20363;&#23376;&#26102;&#65292;&#34920;&#29616;&#20986;&#25913;&#36827;&#30340;&#23569;&#23556;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#12290;FLAN-T5&#30340;&#34920;&#29616;&#19981;&#20339;&#34920;&#26126;&#36739;&#23567;&#30340;&#27169;&#22411;&#21487;&#33021;&#38656;&#35201;&#39069;&#22806;&#30340;&#20219;&#21153;&#29305;&#23450;&#24494;&#35843;&#20197;&#35782;&#21035;&#26694;&#26550;&#20559;&#35265;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#21457;&#29616;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#32463;&#24120;&#23558;&#24773;&#32490;&#35821;&#35328;&#35823;&#35299;&#20026;&#26694;&#26550;&#20559;&#35265;&#30340;&#25351;&#26631;&#65292;&#31361;&#26174;&#20102;&#21306;&#20998;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11621v1 Announce Type: new  Abstract: This work contributes to the expanding research on the applicability of LLMs in social sciences by examining the performance of GPT-3.5 Turbo, GPT-4, and Flan-T5 models in detecting framing bias in news headlines through zero-shot, few-shot, and explainable prompting methods. A key insight from our evaluation is the notable efficacy of explainable prompting in enhancing the reliability of these models, highlighting the importance of explainable settings for social science research on framing bias. GPT-4, in particular, demonstrated enhanced performance in few-shot scenarios when presented with a range of relevant, in-domain examples. FLAN-T5's poor performance indicates that smaller models may require additional task-specific fine-tuning for identifying framing bias detection. Our study also found that models, particularly GPT-4, often misinterpret emotional language as an indicator of framing bias, underscoring the challenge of distingu
&lt;/p&gt;</description></item><item><title>&#24212;&#29992;&#25351;&#26631;&#23398;&#20064;&#32534;&#30721;&#27169;&#22411;&#65288;MLEMs&#65289;&#20110;BERT&#34920;&#31034;&#65292;&#21457;&#29616;&#35821;&#35328;&#29305;&#24449;&#22312;&#19981;&#21516;&#23618;&#20013;&#26377;&#24207;&#20998;&#31163;&#65292;&#31070;&#32463;&#34920;&#31034;&#23618;&#32423;&#32452;&#32455;&#65292;&#20013;&#38388;&#23618;&#35299;&#32806;&#65292;&#20248;&#20110;&#20854;&#20182;&#35299;&#30721;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11608</link><description>&lt;p&gt;
&#25351;&#26631;&#23398;&#20064;&#32534;&#30721;&#27169;&#22411;&#35782;&#21035;BERT&#34920;&#31034;&#20013;&#30340;&#35821;&#35328;&#29305;&#24449;&#22788;&#29702;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Metric-Learning Encoding Models Identify Processing Profiles of Linguistic Features in BERT's Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11608
&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#25351;&#26631;&#23398;&#20064;&#32534;&#30721;&#27169;&#22411;&#65288;MLEMs&#65289;&#20110;BERT&#34920;&#31034;&#65292;&#21457;&#29616;&#35821;&#35328;&#29305;&#24449;&#22312;&#19981;&#21516;&#23618;&#20013;&#26377;&#24207;&#20998;&#31163;&#65292;&#31070;&#32463;&#34920;&#31034;&#23618;&#32423;&#32452;&#32455;&#65292;&#20013;&#38388;&#23618;&#35299;&#32806;&#65292;&#20248;&#20110;&#20854;&#20182;&#35299;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25351;&#26631;&#23398;&#20064;&#32534;&#30721;&#27169;&#22411;&#65288;MLEMs&#65289;&#20316;&#20026;&#19968;&#31181;&#29702;&#35299;&#31070;&#32463;&#31995;&#32479;&#22914;&#20309;&#34920;&#31034;&#20854;&#22788;&#29702;&#23545;&#35937;&#30340;&#29702;&#35770;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#23558;MLEMs&#24212;&#29992;&#20110;&#20174;BERT&#20013;&#25552;&#21462;&#30340;&#31070;&#32463;&#34920;&#31034;&#65292;&#24182;&#36319;&#36394;&#21508;&#31181;&#35821;&#35328;&#29305;&#24449;&#65288;&#20363;&#22914;&#26102;&#24577;&#12289;&#20027;&#35821;&#20154;&#31216;&#12289;&#20174;&#21477;&#31867;&#22411;&#12289;&#20174;&#21477;&#23884;&#22871;&#31561;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;&#35821;&#35328;&#29305;&#24449;&#26159;&#26377;&#24207;&#30340;&#65306;&#23427;&#20204;&#22312;&#19981;&#21516;&#23618;&#20013;&#20197;&#19981;&#21516;&#31243;&#24230;&#23558;&#21477;&#23376;&#30340;&#34920;&#31034;&#20998;&#24320;&#65307;&#65288;2&#65289;&#31070;&#32463;&#34920;&#31034;&#26159;&#20998;&#23618;&#32452;&#32455;&#30340;&#65306;&#22312;&#26576;&#20123;&#23618;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#34920;&#31034;&#30340;&#32676;&#38598;&#23884;&#22871;&#22312;&#26356;&#22823;&#30340;&#32676;&#38598;&#20869;&#37096;&#65292;&#36981;&#24490;&#36880;&#28176;&#37325;&#35201;&#30340;&#35821;&#35328;&#29305;&#24449;&#65307;&#65288;3&#65289;&#35821;&#35328;&#29305;&#24449;&#22312;&#20013;&#38388;&#23618;&#20013;&#26159;&#35299;&#32806;&#30340;&#65306;&#19981;&#21516;&#30340;&#12289;&#36873;&#25321;&#24615;&#21333;&#20301;&#30001;&#19981;&#21516;&#30340;&#35821;&#35328;&#29305;&#24449;&#28608;&#27963;&#12290;&#22312;&#26041;&#27861;&#35770;&#19978;&#65292;MLEMs&#65288;4&#65289;&#20248;&#20110;&#22810;&#21464;&#37327;&#35299;&#30721;&#26041;&#27861;&#65292;&#26356;&#20855;&#25239;&#31867;&#22411;-I&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#65292;&#65288;5&#65289;&#20248;&#20110;&#21333;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11608v1 Announce Type: new  Abstract: We introduce Metric-Learning Encoding Models (MLEMs) as a new approach to understand how neural systems represent the theoretical features of the objects they process. As a proof-of-concept, we apply MLEMs to neural representations extracted from BERT, and track a wide variety of linguistic features (e.g., tense, subject person, clause type, clause embedding). We find that: (1) linguistic features are ordered: they separate representations of sentences to different degrees in different layers; (2) neural representations are organized hierarchically: in some layers, we find clusters of representations nested within larger clusters, following successively important linguistic features; (3) linguistic features are disentangled in middle layers: distinct, selective units are activated by distinct linguistic features. Methodologically, MLEMs are superior (4) to multivariate decoding methods, being more robust to type-I errors, and (5) to univ
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20219;&#21153;&#25512;&#26029;&#26102;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#21333;&#20219;&#21153;&#25512;&#26029;&#24179;&#22343;&#25512;&#26029;&#26102;&#38388;&#20943;&#23569;1.46&#20493;&#65292;&#24182;&#19988;&#22312;MTI Bench&#19978;&#26174;&#31034;&#20986;&#26368;&#22810;&#39640;&#36798;12.4%&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2402.11597</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#25512;&#26029;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#36981;&#24490;&#22810;&#20010;&#25351;&#20196;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11597
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20219;&#21153;&#25512;&#26029;&#26102;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#21333;&#20219;&#21153;&#25512;&#26029;&#24179;&#22343;&#25512;&#26029;&#26102;&#38388;&#20943;&#23569;1.46&#20493;&#65292;&#24182;&#19988;&#22312;MTI Bench&#19978;&#26174;&#31034;&#20986;&#26368;&#22810;&#39640;&#36798;12.4%&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#34987;&#35201;&#27714;&#22312;&#27599;&#27425;&#25512;&#26029;&#35843;&#29992;&#20013;&#36981;&#24490;&#21333;&#20010;&#25351;&#20196;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;LLMs&#26159;&#21542;&#20063;&#20855;&#26377;&#22788;&#29702;&#22810;&#20010;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#31216;&#20026;&#22810;&#20219;&#21153;&#25512;&#26029;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MTI Bench&#65288;&#22810;&#20219;&#21153;&#25512;&#26029;&#22522;&#20934;&#65289;&#65292;&#19968;&#20010;&#21253;&#25324;25&#20010;&#20219;&#21153;&#30340;5000&#20010;&#23454;&#20363;&#30340;&#32508;&#21512;&#35780;&#20272;&#22522;&#20934;&#12290;MTI Bench&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#37117;&#28041;&#21450;2&#21040;3&#20010;&#23376;&#20219;&#21153;&#12290;&#27491;&#22914;&#39044;&#26399;&#30340;&#37027;&#26679;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#22810;&#20219;&#21153;&#25512;&#26029;&#24179;&#22343;&#38477;&#20302;&#20102;1.46&#20493;&#30340;&#24635;&#25512;&#26029;&#26102;&#38388;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#22810;&#27425;&#25512;&#26029;&#35843;&#29992;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#19982;&#39044;&#26399;LLMs&#22312;&#20219;&#21153;&#34987;&#21010;&#20998;&#26102;&#34920;&#29616;&#26356;&#22909;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;LLMs&#65292;&#20363;&#22914;Llama-2-Chat-70B&#21644;GPT-4&#65292;&#22312;MTI Bench&#19978;&#36890;&#36807;&#22810;&#20219;&#21153;&#25512;&#26029;&#19982;&#21333;&#20219;&#21153;&#25512;&#26029;&#30456;&#27604;&#21487;&#20197;&#33719;&#24471;&#39640;&#36798;7.3&#65285;&#21644;12.4&#65285;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;MTI Bench&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11597v1 Announce Type: new  Abstract: Large language models (LLMs) are typically prompted to follow a single instruction per inference call. In this work, we analyze whether LLMs also hold the capability to handle multiple instructions simultaneously, denoted as Multi-Task Inference. For this purpose, we introduce the MTI Bench(Multi-Task Inference Benchmark), a comprehensive evaluation benchmark encompassing 5,000 instances across 25 tasks. Each task in the MTI Bench involves 2 to 3 sub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces the total inference time by 1.46 times in average since it does not require multiple inference calls. Interestingly, contrary to the expectation that LLMs would perform better when tasks are divided, we find that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3% and 12.4% improved performance with Multi-Task Inference compared to Single-Task Inference on the MTI Bench. We release the MTI Bench
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#23884;&#20837;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;LLM&#19978;&#19979;&#25991;&#30340;&#39640;&#36136;&#37327;&#25193;&#23637;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.11577</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#23884;&#20837;&#65306;LLM&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#28789;&#27963;&#22810;&#21151;&#33021;&#22120;
&lt;/p&gt;
&lt;p&gt;
Extensible Embedding: A Flexible Multipler For LLM's Context Length
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11577
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#23884;&#20837;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;LLM&#19978;&#19979;&#25991;&#30340;&#39640;&#36136;&#37327;&#25193;&#23637;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38656;&#35201;&#25193;&#23637;&#19978;&#19979;&#25991;&#20197;&#22788;&#29702;&#35768;&#22810;&#20851;&#38190;&#24212;&#29992;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#26114;&#36149;&#30340;&#25104;&#26412;&#21644;&#36739;&#24046;&#30340;&#19978;&#19979;&#25991;&#25193;&#23637;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#23545;LLM&#19978;&#19979;&#25991;&#30340;&#39640;&#36136;&#37327;&#25193;&#23637;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#12290;&#21487;&#25193;&#23637;&#23884;&#20837;&#26159;&#19968;&#31181; typica1&#20196;&#29260;&#23884;&#20837;&#30340;&#22686;&#24378;&#65292;&#34920;&#31034;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#33539;&#22260;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#21333;&#20010;&#26631;&#35760;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#20449;&#24687;&#23494;&#24230;&#26356;&#39640;&#30340;&#32039;&#20945;&#36755;&#20837;&#21333;&#20803;&#65292;LLM&#21363;&#20351;&#22312;&#36739;&#23567;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#20063;&#21487;&#20197;&#35775;&#38382;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#33539;&#22260;&#12290;&#21487;&#25193;&#23637;&#23884;&#20837;&#22312;&#20307;&#31995;&#32467;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#19978;&#24471;&#21040;&#20102;&#31995;&#32479;&#20248;&#21270;&#65292;&#24102;&#26469;&#20102;&#22810;&#37325;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11577v1 Announce Type: new  Abstract: Large language models (LLMs) call for extension of context to handle many critical applications. However, the existing approaches are prone to expensive costs and inferior quality of context extension. In this work, we propose Extensible Embedding, which realizes high-quality extension of LLM's context with strong flexibility and cost-effectiveness. Extensible embedding stand as an enhancement of typical token embedding, which represents the information for an extensible scope of context instead of a single token. By leveraging such compact input units of higher information density, the LLM can access to a vast scope of context even with a small context window. Extensible embedding is systematically optimized in architecture and training method, which leads to multiple advantages. 1) High flexibility of context extension, which flexibly supports ad-hoc extension of diverse context lengths. 2) Strong sample efficiency of training, which e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Visual In-Context Learning&#65288;VICL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#37325;&#26032;&#25490;&#21517;&#22270;&#20687;&#12289;&#29992;&#20219;&#21153;&#24847;&#22270;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#35270;&#35273;&#35299;&#26512;&#24635;&#32467;&#22270;&#20687;&#65292;&#20197;&#21450;&#32452;&#25104;&#35821;&#35328;&#28436;&#31034;&#26469;&#20943;&#23569;&#26631;&#35760;&#35745;&#25968;&#21644;&#20943;&#36731;&#36328;&#27169;&#24577;&#20132;&#20114;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11574</link><description>&lt;p&gt;
&#22823;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Visual In-Context Learning for Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11574
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Visual In-Context Learning&#65288;VICL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#21644;&#37325;&#26032;&#25490;&#21517;&#22270;&#20687;&#12289;&#29992;&#20219;&#21153;&#24847;&#22270;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#35270;&#35273;&#35299;&#26512;&#24635;&#32467;&#22270;&#20687;&#65292;&#20197;&#21450;&#32452;&#25104;&#35821;&#35328;&#28436;&#31034;&#26469;&#20943;&#23569;&#26631;&#35760;&#35745;&#25968;&#21644;&#20943;&#36731;&#36328;&#27169;&#24577;&#20132;&#20114;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#20013;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#26377;&#25928;&#24615;&#20173;&#21463;&#21040;&#36328;&#27169;&#24577;&#20132;&#20114;&#21644;&#34920;&#31034;&#24046;&#24322;&#30340;&#25361;&#25112;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Visual In-Context Learning&#65288;VICL&#65289;&#26041;&#27861;&#65292;&#21253;&#25324;Visual Demonstration Retrieval&#12289;Intent-Oriented Image Summarization&#21644;Intent-Oriented Demonstration Composition&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#8220;&#26816;&#32034;&#19982;&#37325;&#26032;&#25490;&#21517;&#8221;&#33539;&#24335;&#26816;&#32034;&#22270;&#20687;&#65292;&#29992;&#20219;&#21153;&#24847;&#22270;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#35270;&#35273;&#35299;&#26512;&#24635;&#32467;&#22270;&#20687;&#65292;&#24182;&#32452;&#25104;&#22522;&#20110;&#35821;&#35328;&#30340;&#28436;&#31034;&#65292;&#20943;&#23569;&#26631;&#35760;&#35745;&#25968;&#24182;&#32531;&#35299;&#36328;&#27169;&#24577;&#20132;&#20114;&#38382;&#39064;&#12290;&#23545;&#20116;&#20010;&#21487;&#35270;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35780;&#20272;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#21033;&#29992;&#20449;&#24687;&#27969;&#20998;&#26512;&#38416;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#28436;&#31034;&#30340;&#38271;&#24230;&#21644;&#20301;&#32622;&#23545;LVLM&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11574v1 Announce Type: cross  Abstract: In Large Visual Language Models (LVLMs), the efficacy of In-Context Learning (ICL) remains limited by challenges in cross-modal interactions and representation disparities. To overcome these challenges, we introduce a novel Visual In-Context Learning (VICL) method comprising Visual Demonstration Retrieval, Intent-Oriented Image Summarization, and Intent-Oriented Demonstration Composition. Our approach retrieves images via ''Retrieval &amp; Rerank'' paradigm, summarises images with task intent and task-specific visual parsing, and composes language-based demonstrations that reduce token count and alleviate cross-modal interaction problem. Experimental evaluations on five visual reasoning datasets demonstrate the effectiveness of our method. Moreover, our extensive experiments leverage information flow analysis to elucidate the effectiveness of our method, and investigate the impact of length and position of demonstrations for LVLM. The use 
&lt;/p&gt;</description></item><item><title>&#21487;&#25193;&#23637;&#23884;&#20837;&#26041;&#27861;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19978;&#19979;&#25991;&#25193;&#23637;&#36136;&#37327;&#21644;&#25104;&#26412;&#25928;&#30410;&#65292;&#36890;&#36807;&#22312;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#19978;&#36827;&#34892;&#31995;&#32479;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#19978;&#19979;&#25991;&#33539;&#22260;&#30340;&#28789;&#27963;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.11573</link><description>&lt;p&gt;
BGE&#22320;&#26631;&#23884;&#20837;&#65306;&#19968;&#31181;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#22359;&#23884;&#20837;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BGE Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11573
&lt;/p&gt;
&lt;p&gt;
&#21487;&#25193;&#23637;&#23884;&#20837;&#26041;&#27861;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#19978;&#19979;&#25991;&#25193;&#23637;&#36136;&#37327;&#21644;&#25104;&#26412;&#25928;&#30410;&#65292;&#36890;&#36807;&#22312;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#19978;&#36827;&#34892;&#31995;&#32479;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#19978;&#19979;&#25991;&#33539;&#22260;&#30340;&#28789;&#27963;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#25193;&#23637;&#19978;&#19979;&#25991;&#20197;&#22788;&#29702;&#35768;&#22810;&#20851;&#38190;&#24212;&#29992;&#65292;&#28982;&#32780;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#25104;&#26412;&#39640;&#26114;&#19988;&#19978;&#19979;&#25991;&#25193;&#23637;&#36136;&#37327;&#36739;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#24378;&#22823;&#28789;&#27963;&#24615;&#21644;&#25104;&#26412;&#25928;&#30410;&#30340;LLM&#19978;&#19979;&#25991;&#30340;&#39640;&#36136;&#37327;&#25193;&#23637;&#12290;&#21487;&#25193;&#23637;&#23884;&#20837;&#20316;&#20026;&#20856;&#22411;&#20196;&#29260;&#23884;&#20837;&#30340;&#22686;&#24378;&#65292;&#20195;&#34920;&#20102;&#21487;&#25193;&#23637;&#33539;&#22260;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#21333;&#20010;&#20196;&#29260;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#20449;&#24687;&#23494;&#24230;&#26356;&#39640;&#30340;&#32039;&#20945;&#36755;&#20837;&#21333;&#20803;&#65292;LLM&#21363;&#20351;&#22312;&#23567;&#19978;&#19979;&#25991;&#31383;&#21475;&#19979;&#20063;&#33021;&#35775;&#38382;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#33539;&#22260;&#12290;&#21487;&#25193;&#23637;&#23884;&#20837;&#22312;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#20248;&#21270;&#65292;&#20855;&#26377;&#22810;&#20010;&#20248;&#21183;&#12290;1) &#39640;&#24230;&#28789;&#27963;&#30340;&#19978;&#19979;&#25991;&#25193;&#23637;&#65292;&#28789;&#27963;&#25903;&#25345;&#21508;&#31181;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#21363;&#26102;&#25193;&#23637;&#12290;2) &#24378;&#22823;&#30340;&#35757;&#32451;&#26679;&#26412;&#25928;&#29575;&#65292;&#20351;&#24471;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11573v1 Announce Type: new  Abstract: Large language models (LLMs) call for extension of context to handle many critical applications. However, the existing approaches are prone to expensive costs and inferior quality of context extension. In this work, we proposeExtensible Embedding, which realizes high-quality extension of LLM's context with strong flexibility and cost-effectiveness. Extensible embedding stand as an enhancement of typical token embedding, which represents the information for an extensible scope of context instead of a single token. By leveraging such compact input units of higher information density, the LLM can access to a vast scope of context even with a small context window. Extensible embedding is systematically optimized in architecture and training method, which leads to multiple advantages. 1) High flexibility of context extension, which flexibly supports ad-hoc extension of diverse context lengths. 2) Strong sample efficiency of training, which en
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26080;&#21442;&#32771;&#22270;&#20687;&#23383;&#24149;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#30524;&#38236;&#34503;&#25928;&#24212;&#65292;&#21033;&#29992;&#24230;&#37327;&#24471;&#20998;&#20316;&#20026;&#22870;&#21169;&#26469;&#25351;&#23548;&#29983;&#25104;&#19982;&#24230;&#37327;&#26631;&#20934;&#19968;&#33268;&#30340;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2402.11572</link><description>&lt;p&gt;
&#26080;&#21442;&#32771;&#22270;&#20687;&#23383;&#24149;&#35780;&#20272;&#20013;&#30340;&#30524;&#38236;&#34503;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Cobra Effect in Reference-Free Image Captioning Metrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26080;&#21442;&#32771;&#22270;&#20687;&#23383;&#24149;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#30524;&#38236;&#34503;&#25928;&#24212;&#65292;&#21033;&#29992;&#24230;&#37327;&#24471;&#20998;&#20316;&#20026;&#22870;&#21169;&#26469;&#25351;&#23548;&#29983;&#25104;&#19982;&#24230;&#37327;&#26631;&#20934;&#19968;&#33268;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#25551;&#36848;&#21644;&#30456;&#24212;&#22270;&#20687;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#20195;&#34920;&#20102;&#22810;&#27169;&#24577;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#24037;&#20316;&#12290;&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#26080;&#21442;&#32771;&#26041;&#27861;&#22823;&#37327;&#28044;&#29616;&#12290;&#23454;&#35777;&#35777;&#25454;&#35777;&#23454;&#65292;&#36825;&#20123;&#21019;&#26032;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#26356;&#39640;&#65292;&#26631;&#24535;&#30528;&#35813;&#39046;&#22495;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20165;&#39640;&#30456;&#20851;&#24615;&#26159;&#21542;&#36275;&#20197;&#23436;&#20840;&#34920;&#31034;&#24230;&#37327;&#26631;&#20934;&#30340;&#20840;&#38754;&#24615;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#21442;&#32771;&#24230;&#37327;&#26631;&#20934;&#26159;&#21542;&#23384;&#22312;&#20219;&#20309;&#19981;&#36275;&#20043;&#22788;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21463;&#21040;&#30524;&#38236;&#34503;&#25928;&#24212;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#24230;&#37327;&#24471;&#20998;&#20316;&#20026;&#22870;&#21169;&#65292;&#25351;&#23548;&#23383;&#24149;&#27169;&#22411;&#29983;&#25104;&#19982;&#24230;&#37327;&#26631;&#20934;&#23494;&#20999;&#19968;&#33268;&#30340;&#25551;&#36848;&#12290;&#22914;&#26524;&#26576;&#20010;&#24230;&#37327;&#26631;&#20934;&#26377;&#32570;&#38519;&#65292;&#27169;&#22411;&#23558;&#21033;&#29992;&#36825;&#19968;&#28857;&#65292;&#24182;&#20307;&#29616;&#22312;&#29983;&#25104;&#30340;&#21477;&#23376;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11572v1 Announce Type: new  Abstract: Evaluating the compatibility between textual descriptions and corresponding images represents a core endeavor within multi-modal research. In recent years, a proliferation of reference-free methods, leveraging visual-language pre-trained models (VLMs), has emerged. Empirical evidence has substantiated that these innovative approaches exhibit a higher correlation with human judgment, marking a significant advancement in the field. However, does a higher correlation with human evaluations alone sufficiently denote the complete of a metric? In response to this question, in this paper, we study if there are any deficiencies in reference-free metrics. Specifically, inspired by the Cobra Effect, we utilize metric scores as rewards to direct the captioning model toward generating descriptions that closely align with the metric's criteria. If a certain metric has flaws, it will be exploited by the model and reflected in the generated sentences. 
&lt;/p&gt;</description></item><item><title>&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#31038;&#20132;&#26426;&#22120;&#20154;&#20013;&#65292;&#23454;&#29616;&#26356;&#21160;&#24577;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#23545;&#35805;&#65292;&#21253;&#25324;&#20351;&#29992;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#65292;&#35843;&#25972;&#35821;&#35843;&#65292;&#24182;&#21033;&#29992;&#34920;&#24773;&#31526;&#21495;&#29983;&#25104;&#26426;&#22120;&#20154;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.11571</link><description>&lt;p&gt;
&#19981;&#35843;&#30382; &#8212;&#8212; &#20351;&#29992;LLMs&#29983;&#25104;&#19982;&#21488;&#24335;&#26426;&#22120;&#20154;Haru&#23545;&#35805;&#20013;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#26426;&#22120;&#20154;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Ain't Misbehavin' -- Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11571
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#31038;&#20132;&#26426;&#22120;&#20154;&#20013;&#65292;&#23454;&#29616;&#26356;&#21160;&#24577;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#23545;&#35805;&#65292;&#21253;&#25324;&#20351;&#29992;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#65292;&#35843;&#25972;&#35821;&#35843;&#65292;&#24182;&#21033;&#29992;&#34920;&#24773;&#31526;&#21495;&#29983;&#25104;&#26426;&#22120;&#20154;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#26426;&#22120;&#20154;&#26088;&#22312;&#36890;&#36807;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#19982;&#20154;&#31867;&#24314;&#31435;&#38271;&#26399;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#23545;&#35805;&#26041;&#27861;&#20381;&#36182;&#20110;&#33050;&#26412;&#21270;&#20114;&#21160;&#65292;&#24448;&#24448;&#26080;&#27861;&#20445;&#25345;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#21040;&#31038;&#20132;&#26426;&#22120;&#20154;&#20013;&#65292;&#23454;&#29616;&#26356;&#21160;&#24577;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#23545;&#35805;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#19968;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#21033;&#29992;LLMs&#29983;&#25104;&#20855;&#26377;&#34920;&#29616;&#21147;&#34892;&#20026;&#30340;&#26426;&#22120;&#20154;&#21709;&#24212;&#65292;&#19982;&#26426;&#22120;&#20154;&#20010;&#24615;&#19968;&#33268;&#12290;&#25105;&#20204;&#23558;&#26426;&#22120;&#20154;&#34892;&#20026;&#19982;&#20004;&#31181;&#24418;&#24335;&#32467;&#21512;&#36215;&#26469;&#65306;1&#65289;&#19968;&#20010;&#33021;&#22815;&#20855;&#22791;&#21508;&#31181;&#35821;&#35843;&#39118;&#26684;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#24341;&#25806;&#65292;&#20197;&#21450;2&#65289;&#26426;&#22120;&#20154;&#30340;&#19968;&#31995;&#21015;&#29289;&#29702;&#21160;&#20316;&#24211;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#23450;&#20041;&#30340;&#26368;&#26032;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#65292;&#21160;&#24577;&#36873;&#25321;&#26426;&#22120;&#20154;&#30340;&#35821;&#35843;&#65292;&#24182;&#21033;&#29992;LLM&#36755;&#20986;&#20013;&#30340;&#34920;&#24773;&#31526;&#21495;&#20316;&#20026;&#29983;&#25104;&#26426;&#22120;&#20154;&#21160;&#20316;&#30340;&#32447;&#32034;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#28436;&#31034;&#21487;&#22312;&#27492;&#22788;&#26597;&#30475;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11571v1 Announce Type: cross  Abstract: Social robots aim to establish long-term bonds with humans through engaging conversation. However, traditional conversational approaches, reliant on scripted interactions, often fall short in maintaining engaging conversations. This paper addresses this limitation by integrating large language models (LLMs) into social robots to achieve more dynamic and expressive conversations. We introduce a fully-automated conversation system that leverages LLMs to generate robot responses with expressive behaviors, congruent with the robot's personality. We incorporate robot behavior with two modalities: 1) a text-to-speech (TTS) engine capable of various delivery styles, and 2) a library of physical actions for the robot. We develop a custom, state-of-the-art emotion recognition model to dynamically select the robot's tone of voice and utilize emojis from LLM output as cues for generating robot actions. A demo of our system is available here. To i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Haru&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#20027;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21457;&#25381;&#20102;&#20854;&#24773;&#24863;&#34920;&#36798;&#21644;&#29420;&#29305;&#20154;&#26684;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#34892;&#20026;&#25913;&#21464;&#36741;&#23548;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.11569</link><description>&lt;p&gt;
&#20351;&#29992;Haru&#24320;&#21457;&#33258;&#20027;&#26426;&#22120;&#20154;&#20171;&#23548;&#30340;&#34892;&#20026;&#36741;&#23548;&#20250;&#35805;
&lt;/p&gt;
&lt;p&gt;
Developing Autonomous Robot-Mediated Behavior Coaching Sessions with Haru
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11569
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Haru&#24320;&#21457;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#20027;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21457;&#25381;&#20102;&#20854;&#24773;&#24863;&#34920;&#36798;&#21644;&#29420;&#29305;&#20154;&#26684;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#34892;&#20026;&#25913;&#21464;&#36741;&#23548;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#20026;&#34892;&#20026;&#25913;&#21464;&#36741;&#23548;&#35774;&#35745;&#21644;&#24433;&#21709;&#33258;&#20027;&#23545;&#35805;&#30340;&#32463;&#39564;&#35843;&#26597;&#12290;&#25105;&#20204;&#20851;&#27880;&#20351;&#29992;&#26700;&#38754;&#31038;&#20132;&#26426;&#22120;&#20154;Haru&#65292;&#24182;&#25506;&#32034;&#23454;&#26045;&#24494;&#20064;&#24815;&#26041;&#27861;&#26469;&#20419;&#36827;&#31215;&#26497;&#34892;&#20026;&#25913;&#21464;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#26680;&#24515;&#26159;&#24320;&#21457;&#19968;&#20010;&#23436;&#20840;&#33258;&#20027;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21457;&#25381;Haru&#30340;&#24773;&#24863;&#34920;&#36798;&#21644;&#29420;&#29305;&#20154;&#26684;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#23545;&#35805;&#31995;&#32479;&#30340;&#36845;&#20195;&#35774;&#35745;&#21644;&#24191;&#27867;&#27979;&#35797;&#65292;&#30830;&#20445;&#23427;&#26377;&#25928;&#22320;&#20307;&#29616;&#20102;&#24494;&#20064;&#24815;&#26041;&#27861;&#30340;&#21407;&#21017;&#65292;&#21516;&#26102;&#36824;&#34701;&#21512;&#20102;&#24314;&#31435;&#20449;&#20219;&#21644;&#30772;&#22351;&#20449;&#20219;&#30340;&#31574;&#30053;&#12290;&#26368;&#32456;&#29256;&#26412;&#23545;&#35805;&#30340;&#26377;&#25928;&#24615;&#22312;&#20154;&#31867;&#21442;&#19982;&#32773;&#65288;N=12&#65289;&#30340;&#23454;&#39564;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;Haru&#30340;&#29983;&#27668;&#12289;&#20114;&#21160;&#24615;&#21644;&#20013;&#31435;&#24615;&#30340;&#35748;&#30693;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11569v1 Announce Type: cross  Abstract: This study presents an empirical investigation into the design and impact of autonomous dialogues in human-robot interaction for behavior change coaching. We focus on the use of Haru, a tabletop social robot, and explore the implementation of the Tiny Habits method for fostering positive behavior change. The core of our study lies in developing a fully autonomous dialogue system that maximizes Haru's emotional expressiveness and unique personality. Our methodology involved iterative design and extensive testing of the dialogue system, ensuring it effectively embodied the principles of the Tiny Habits method while also incorporating strategies for trust-raising and trust-dampening. The effectiveness of the final version of the dialogue was evaluated in an experimental study with human participants (N=12). The results indicated a significant improvement in perceptions of Haru's liveliness, interactivity, and neutrality. Additionally, our
&lt;/p&gt;</description></item><item><title>LongAgent&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#23558;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;128K&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#38271;&#25991;&#26412;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11550</link><description>&lt;p&gt;
LongAgent: &#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#23558;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;128K&#19978;&#19979;&#25991;
&lt;/p&gt;
&lt;p&gt;
LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11550
&lt;/p&gt;
&lt;p&gt;
LongAgent&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#23558;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;128K&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#38271;&#25991;&#26412;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#35821;&#35328;&#21644;&#25191;&#34892;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;LLMs&#20197;&#20854;&#26114;&#36149;&#30340;&#35757;&#32451;&#25104;&#26412;&#21644;&#39640;&#25512;&#29702;&#24310;&#36831;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22914;GPT-4&#21644;Claude2&#22312;&#22788;&#29702;&#36229;&#36807;$100k$&#26631;&#35760;&#30340;&#36755;&#20837;&#26102;&#20063;&#32463;&#24120;&#20986;&#38169;&#65292;&#36825;&#31181;&#29616;&#35937;&#20063;&#34987;&#31216;&#20026;\textit{&#20013;&#38388;&#36855;&#22833;}&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#26041;&#27861;\textsc{LongAgent}&#65292;&#23558;LLMs&#65288;&#20363;&#22914;LLaMA&#65289;&#25193;&#23637;&#21040;128K&#19978;&#19979;&#25991;&#65292;&#24182;&#23637;&#31034;&#20986;&#22312;&#38271;&#25991;&#26412;&#22788;&#29702;&#26041;&#38754;&#21487;&#33021;&#20248;&#20110;GPT-4&#30340;&#28508;&#21147;&#12290;&#22312;\textsc{LongAgent}&#20013;&#65292;&#19968;&#20301;&#39046;&#23548;&#32773;&#36127;&#36131;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#24182;&#25351;&#23548;&#22242;&#38431;&#25104;&#21592;&#20174;&#25991;&#26723;&#20013;&#33719;&#21462;&#20449;&#24687;&#12290;&#30001;&#20110;&#25104;&#21592;&#23384;&#22312;&#24187;&#35273;&#65292;&#39046;&#23548;&#32773;&#20174;&#20960;&#21313;&#21040;&#25968;&#30334;&#21517;&#25104;&#21592;&#30340;&#22238;&#24212;&#20013;&#33719;&#21462;&#20934;&#30830;&#20449;&#24687;&#24182;&#38750;&#26131;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11550v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated impressive performance in understanding language and executing complex reasoning tasks. However, LLMs with long context windows have been notorious for their expensive training costs and high inference latency. Even the most advanced models such as GPT-4 and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a phenomenon also known as \textit{lost in the middle}. In this paper, we propose \textsc{LongAgent}, a method based on multi-agent collaboration, which scales LLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority in long-text processing compared to GPT-4. In \textsc{LongAgent}, a leader is responsible for understanding user intent and directing team members to acquire information from documents. Due to members' hallucinations, it is non-trivial for a leader to obtain accurate information from the responses of dozens to hundreds of member
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#33521;&#35821;&#21644;&#24503;&#35821;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#36235;&#21183;&#65292;&#20351;&#29992;&#35758;&#20250;&#36777;&#35770;&#35821;&#26009;&#24211;&#65292;&#25506;&#35752;&#20102;&#21477;&#27861;&#20381;&#23384;&#36317;&#31163;&#26368;&#23567;&#21270;&#21450;&#22522;&#20110;&#26641;&#22270;&#23646;&#24615;&#30340;15&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#25581;&#31034;&#20102;&#29616;&#20195;&#35299;&#26512;&#22120;&#22312;&#36825;&#31181;&#21464;&#21270;&#20013;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.11549</link><description>&lt;p&gt;
&#33521;&#35821;&#21644;&#24503;&#35821;&#30340;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#65306;&#24230;&#37327;&#12289;&#35299;&#26512;&#22120;&#21644;&#36235;&#21516;
&lt;/p&gt;
&lt;p&gt;
Syntactic Language Change in English and German: Metrics, Parsers, and Convergences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#33521;&#35821;&#21644;&#24503;&#35821;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#36235;&#21183;&#65292;&#20351;&#29992;&#35758;&#20250;&#36777;&#35770;&#35821;&#26009;&#24211;&#65292;&#25506;&#35752;&#20102;&#21477;&#27861;&#20381;&#23384;&#36317;&#31163;&#26368;&#23567;&#21270;&#21450;&#22522;&#20110;&#26641;&#22270;&#23646;&#24615;&#30340;15&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#25581;&#31034;&#20102;&#29616;&#20195;&#35299;&#26512;&#22120;&#22312;&#36825;&#31181;&#21464;&#21270;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#65292;&#20154;&#31867;&#35821;&#35328;&#24448;&#24448;&#20250;&#20248;&#21270;&#35821;&#35328;&#32467;&#26500;&#20197;&#38477;&#20302;&#22797;&#26434;&#24615;&#65292;&#22686;&#21152;&#20132;&#27969;&#25928;&#29575;&#12290;&#21477;&#27861;&#20381;&#23384;&#36317;&#31163;&#34913;&#37327;&#20102;&#30456;&#20851;&#35789;&#27719;&#20043;&#38388;&#30340;&#32447;&#24615;&#36317;&#31163;&#65292;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#35821;&#35328;&#22788;&#29702;&#22256;&#38590;&#21644;&#24037;&#20316;&#35760;&#24518;&#36127;&#33655;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#33521;&#35821;&#21644;&#24503;&#35821;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#30340;&#21382;&#26102;&#36235;&#21183;&#65292;&#20351;&#29992;&#20102;&#36807;&#21435;&#22823;&#32422;160&#24180;&#38388;&#30340;&#35758;&#20250;&#36777;&#35770;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#22522;&#20110;5&#20010;&#20381;&#23384;&#21477;&#27861;&#35299;&#26512;&#22120;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#21253;&#25324;&#24191;&#27867;&#20351;&#29992;&#30340;Stanford CoreNLP&#20197;&#21450;&#20854;&#20182;4&#20010;&#26356;&#26032;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#21477;&#27861;&#35821;&#35328;&#21464;&#21270;&#20998;&#26512;&#36229;&#36234;&#20102;&#32447;&#24615;&#20381;&#23384;&#36317;&#31163;&#65292;&#25506;&#35752;&#20102;&#19982;&#20381;&#23384;&#36317;&#31163;&#26368;&#23567;&#21270;&#65288;DDM&#65289;&#30456;&#20851;&#30340;15&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#25110;&#32773;&#22522;&#20110;&#26641;&#22270;&#23646;&#24615;&#65292;&#27604;&#22914;&#26641;&#39640;&#21644;&#24230;&#21464;&#24322;&#12290;&#23613;&#31649;&#25105;&#20204;&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#26368;&#36817;&#22522;&#20110;&#29616;&#20195;&#26641;&#24211;&#35757;&#32451;&#30340;&#35299;&#26512;&#22120;&#24182;&#26410;&#21463;&#21040;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11549v1 Announce Type: cross  Abstract: Many studies have shown that human languages tend to optimize for lower complexity and increased communication efficiency. Syntactic dependency distance, which measures the linear distance between dependent words, is often considered a key indicator of language processing difficulty and working memory load. The current paper looks at diachronic trends in syntactic language change in both English and German, using corpora of parliamentary debates from the last c. 160 years. We base our observations on five dependency parsers, including the widely used Stanford CoreNLP as well as 4 newer alternatives. Our analysis of syntactic language change goes beyond linear dependency distance and explores 15 metrics relevant to dependency distance minimization (DDM) and/or based on tree graph properties, such as the tree height and degree variance. Even though we have evidence that recent parsers trained on modern treebanks are not heavily affected 
&lt;/p&gt;</description></item><item><title>KMMLU&#26159;&#19968;&#20010;&#26032;&#30340;&#38889;&#35821;&#22522;&#20934;&#65292;&#21253;&#21547;35,030&#36947;&#19987;&#23478;&#32423;&#22810;&#36873;&#39064;&#65292;&#20174;&#21407;&#22987;&#38889;&#35821;&#32771;&#35797;&#20013;&#25910;&#38598;&#32780;&#26469;&#65292;&#27979;&#35797;&#20102;26&#20010;LLM&#27169;&#22411;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;KMMLU&#19978;&#30340;&#34920;&#29616;&#26377;&#24456;&#22823;&#25552;&#21319;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.11548</link><description>&lt;p&gt;
KMMLU&#65306;&#22312;&#38889;&#35821;&#20013;&#27979;&#37327;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
KMMLU: Measuring Massive Multitask Language Understanding in Korean
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11548
&lt;/p&gt;
&lt;p&gt;
KMMLU&#26159;&#19968;&#20010;&#26032;&#30340;&#38889;&#35821;&#22522;&#20934;&#65292;&#21253;&#21547;35,030&#36947;&#19987;&#23478;&#32423;&#22810;&#36873;&#39064;&#65292;&#20174;&#21407;&#22987;&#38889;&#35821;&#32771;&#35797;&#20013;&#25910;&#38598;&#32780;&#26469;&#65292;&#27979;&#35797;&#20102;26&#20010;LLM&#27169;&#22411;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;KMMLU&#19978;&#30340;&#34920;&#29616;&#26377;&#24456;&#22823;&#25552;&#21319;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;KMMLU&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#38889;&#35821;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#20154;&#25991;&#31185;&#23398;&#21040;STEM&#30340;45&#20010;&#23398;&#31185;&#30340;35,030&#36947;&#19987;&#23478;&#32423;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;&#19982;&#20043;&#21069;&#20174;&#29616;&#26377;&#33521;&#35821;&#22522;&#20934;&#32763;&#35793;&#32780;&#26469;&#30340;&#38889;&#35821;&#22522;&#20934;&#19981;&#21516;&#65292;KMMLU&#26159;&#20174;&#21407;&#22987;&#38889;&#35821;&#32771;&#35797;&#20013;&#25910;&#38598;&#30340;&#65292;&#25429;&#25417;&#20102;&#38889;&#35821;&#30340;&#35821;&#35328;&#21644;&#25991;&#21270;&#26041;&#38754;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;26&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#21644;&#19987;&#26377;&#30340;LLM&#65292;&#21457;&#29616;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#26368;&#22909;&#30340;&#20844;&#24320;&#27169;&#22411;&#22312;KMMLU&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;50.54%&#65292;&#36828;&#20302;&#20110;&#24179;&#22343;&#20154;&#31867;&#34920;&#29616;&#30340;62.6%&#12290;&#36825;&#20010;&#27169;&#22411;&#20027;&#35201;&#26159;&#38024;&#23545;&#33521;&#35821;&#21644;&#20013;&#25991;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#32780;&#19981;&#26159;&#38889;&#35821;&#12290;&#30446;&#21069;&#38024;&#23545;&#38889;&#35821;&#30340;LLM&#65292;&#22914;Polyglot-Ko&#65292;&#34920;&#29616;&#24471;&#26356;&#31967;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#26159;&#26368;&#26377;&#33021;&#21147;&#30340;&#19987;&#26377;LLM&#65292;&#20363;&#22914;GPT-4&#21644;HyperCLOVA X&#65292;&#20063;&#21482;&#20998;&#21035;&#36798;&#21040;&#20102;59.95%&#21644;53.40%&#12290;&#36825;&#34920;&#26126;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#24037;&#20316;&#26469;&#25913;&#36827;&#38889;&#35821;LLM&#65292;&#32780;KMMLU&#25552;&#20379;&#20102;&#36861;&#36394;&#36825;&#19968;&#36827;&#23637;&#30340;&#27491;&#30830;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11548v1 Announce Type: new  Abstract: We propose KMMLU, a new Korean benchmark with 35,030 expert-level multiple-choice questions across 45 subjects ranging from humanities to STEM. Unlike previous Korean benchmarks that are translated from existing English benchmarks, KMMLU is collected from original Korean exams, capturing linguistic and cultural aspects of the Korean language. We test 26 publically available and proprietary LLMs, identifying significant room for improvement. The best publicly available model achieves 50.54% on KMMLU, far below the average human performance of 62.6%. This model was primarily trained for English and Chinese, not Korean. Current LLMs tailored to Korean, such as Polyglot-Ko, perform far worse. Surprisingly, even the most capable proprietary LLMs, e.g., GPT-4 and HyperCLOVA X, achieve 59.95% and 53.40%, respectively. This suggests that further work is needed to improve Korean LLMs, and KMMLU offers the right tool to track this progress. We mak
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#26102;&#31354;&#30693;&#35782;&#22270;&#30340;&#38382;&#31572;&#31995;&#32479;STQAD&#65292;&#20197;&#35299;&#20915;&#38382;&#31572;&#31995;&#32479;&#22312;&#28085;&#30422;&#26102;&#31354;&#20449;&#24687;&#30340;&#38382;&#39064;&#19978;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;STComplEx&#23884;&#20837;&#26041;&#27861;STCQA&#26469;&#23454;&#29616;&#27492;&#30446;&#26631;</title><link>https://arxiv.org/abs/2402.11542</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#30693;&#35782;&#22270;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Question Answering Over Spatio-Temporal Knowledge Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11542
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#26102;&#31354;&#30693;&#35782;&#22270;&#30340;&#38382;&#31572;&#31995;&#32479;STQAD&#65292;&#20197;&#35299;&#20915;&#38382;&#31572;&#31995;&#32479;&#22312;&#28085;&#30422;&#26102;&#31354;&#20449;&#24687;&#30340;&#38382;&#39064;&#19978;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;STComplEx&#23884;&#20837;&#26041;&#27861;STCQA&#26469;&#23454;&#29616;&#27492;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#30693;&#35782;&#22270;&#65288;STKG&#65289;&#36890;&#36807;&#25972;&#21512;&#26102;&#38388;&#21644;&#20301;&#32622;&#20449;&#24687;&#25193;&#23637;&#20102;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#30340;&#27010;&#24565;&#12290;&#23613;&#31649;&#30740;&#31350;&#30028;&#20851;&#27880;&#30693;&#35782;&#22270;&#38382;&#31572;&#65288;KGQA&#65289;&#65292;&#20294;&#22522;&#20110;STKG&#30340;&#28085;&#30422;&#26102;&#31354;&#20449;&#24687;&#30340;&#38382;&#39064;&#22238;&#31572;&#39046;&#22495;&#20173;&#26410;&#34987;&#24191;&#27867;&#25506;&#35752;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#20063;&#38459;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STQAD&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#25324;10,000&#20010;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#30340;&#38754;&#21521;&#26102;&#31354;&#30693;&#35782;&#22270;&#38382;&#31572;&#65288;STKGQA&#65289;&#25968;&#25454;&#38598;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;KGQA&#26041;&#27861;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#36828;&#26410;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STCQA&#65292;&#19968;&#31181;&#26032;&#30340;&#26102;&#31354;KGQA&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;STComplEx&#30340;&#26032;&#22411;STKG&#23884;&#20837;&#26041;&#27861;&#12290;&#36890;&#36807;&#20174;&#38382;&#39064;&#20013;&#25552;&#21462;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#38382;&#31572;&#27169;&#22411;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11542v1 Announce Type: cross  Abstract: Spatio-temporal knowledge graphs (STKGs) extend the concept of knowledge graphs (KGs) by incorporating time and location information. While the research community's focus on Knowledge Graph Question Answering (KGQA), the field of answering questions incorporating both spatio-temporal information based on STKGs remains largely unexplored. Furthermore, a lack of comprehensive datasets also has hindered progress in this area. To address this issue, we present STQAD, a dataset comprising 10,000 natural language questions for spatio-temporal knowledge graph question answering (STKGQA). Unfortunately, various state-of-the-art KGQA approaches fall far short of achieving satisfactory performance on our dataset. In response, we propose STCQA, a new spatio-temporal KGQA approach that utilizes a novel STKG embedding method named STComplEx. By extracting temporal and spatial information from a question, our QA model can better comprehend the quest
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;KG&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#25506;&#32034;&#20026;LLMs&#25552;&#20379;&#30693;&#35782;&#22270;&#35889;&#30693;&#35782;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.11541</link><description>&lt;p&gt;
&#36870;&#21521;&#35748;&#30693;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27604;&#25105;&#20204;&#24819;&#35937;&#30340;&#26356;&#25797;&#38271;&#29702;&#35299;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;KG&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#25506;&#32034;&#20026;LLMs&#25552;&#20379;&#30693;&#35782;&#22270;&#35889;&#30693;&#35782;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#24182;&#20943;&#23569;&#23427;&#20204;&#30340;&#24187;&#35273;&#30340;&#26041;&#27861;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30446;&#21069;&#23545;&#22914;&#20309;&#20351;LLMs&#33021;&#22815;&#21363;&#26102;&#25972;&#21512;KGs&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#25506;&#32034;&#36824;&#19981;&#36275;&#12290;&#26412;&#25991;&#37319;&#29992;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#65288;CQA&#65289;&#20316;&#20026;&#19968;&#39033;&#20219;&#21153;&#65292;&#35780;&#20272;LLM&#29702;&#35299;KG&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;KG&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65288;&#20174;&#19977;&#20803;&#32452;&#21040;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#65289;&#65292;&#26088;&#22312;&#25506;&#32034;&#20026;LLMs&#25552;&#20379;KG&#30693;&#35782;&#30340;&#26368;&#20339;&#25552;&#31034;&#26041;&#27861;&#65292;&#20174;&#32780;&#22686;&#24378;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11541v1 Announce Type: cross  Abstract: Although the method of enhancing large language models' (LLMs') reasoning ability and reducing their hallucinations through the use of knowledge graphs (KGs) has received widespread attention, the exploration of how to enable LLMs to integrate the structured knowledge in KGs on-the-fly remains inadequate. Researchers often co-train KG embeddings and LLM parameters to equip LLMs with the ability of comprehending KG knowledge. However, this resource-hungry training paradigm significantly increases the model learning cost and is also unsuitable for non-open-source, black-box LLMs. In this paper, we employ complex question answering (CQA) as a task to assess the LLM's ability of comprehending KG knowledge. We conducted a comprehensive comparison of KG knowledge injection methods (from triples to natural language text), aiming to explore the optimal prompting method for supplying KG knowledge to LLMs, thereby enhancing their comprehension o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#20116;&#20010;&#20027;&#35201;&#31867;&#21035;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;48&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#8220;&#39640;&#24433;&#21709;&#25968;&#25454;&#8221;&#65292;&#22914;&#20070;&#31821;&#65292;&#19982;&#27169;&#22411;&#33021;&#21147;&#30456;&#20851;&#32852;&#65292;&#20026;LLMs&#30340;&#20248;&#21270;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.11537</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#21435;&#23398;&#20064;&#30740;&#31350;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Deciphering the lmpact of Pretraining Data on Large Language Models through Machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11537
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#20116;&#20010;&#20027;&#35201;&#31867;&#21035;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;48&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#31995;&#32479;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#8220;&#39640;&#24433;&#21709;&#25968;&#25454;&#8221;&#65292;&#22914;&#20070;&#31821;&#65292;&#19982;&#27169;&#22411;&#33021;&#21147;&#30456;&#20851;&#32852;&#65292;&#20026;LLMs&#30340;&#20248;&#21270;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20855;&#26377;&#21508;&#31181;&#26469;&#28304;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#30340;&#24433;&#21709;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#32452;&#32455;&#20173;&#28982;&#26159;&#32463;&#39564;&#24615;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#20559;&#31163;&#26368;&#20339;&#29366;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#26469;&#33258;LLMs&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;5&#20010;&#20027;&#35201;&#31867;&#21035;&#30340;48&#20010;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#20851;&#20110;&#20061;&#20010;&#20027;&#35201;&#27169;&#22411;&#33021;&#21147;&#31867;&#21035;&#30340;&#22522;&#20934;&#26469;&#34913;&#37327;&#23427;&#20204;&#23545;LLMs&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;&#22810;&#20010;&#35821;&#26009;&#24211;&#23545;LLMs&#24615;&#33021;&#36129;&#29486;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#32852;&#21512;&#24433;&#21709;&#27169;&#24335;&#65292;&#21253;&#25324;&#20114;&#34917;&#30340;&#12289;&#27491;&#20132;&#30340;&#21644;&#30456;&#20851;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#19968;&#32452;&#8220;&#39640;&#24433;&#21709;&#25968;&#25454;&#8221;&#65292;&#22914;&#20070;&#31821;&#65292;&#19982;&#19968;&#32452;&#27169;&#22411;&#33021;&#21147;&#30456;&#20851;&#32852;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#32452;&#32455;&#25968;&#25454;&#20197;&#25903;&#25345;LLMs&#20248;&#21270;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11537v1 Announce Type: cross  Abstract: Through pretraining on a corpus with various sources, Large Language Models (LLMs) have gained impressive performance. However, the impact of each component of the pretraining corpus remains opaque. As a result, the organization of the pretraining corpus is still empirical and may deviate from the optimal. To address this issue, we systematically analyze the impact of 48 datasets from 5 major categories of pretraining data of LLMs and measure their impacts on LLMs using benchmarks about nine major categories of model capabilities. Our analyses provide empirical results about the contribution of multiple corpora on the performances of LLMs, along with their joint impact patterns, including complementary, orthogonal, and correlational relationships. We also identify a set of ``high-impact data'' such as Books that is significantly related to a set of model capabilities. These findings provide insights into the organization of data to sup
&lt;/p&gt;</description></item><item><title>PreAct&#26159;&#19968;&#20010;&#25972;&#21512;&#20102;&#39044;&#27979;&#12289;&#25512;&#29702;&#21644;&#34892;&#21160;&#30340;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#27979;&#20449;&#24687;&#21487;&#20197;&#24110;&#21161;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#22810;&#26679;&#21270;&#21644;&#31574;&#30053;&#24615;&#30340;&#25512;&#29702;&#65292;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#34892;&#21160;&#65292;&#25552;&#21319;&#20219;&#21153;&#23436;&#25104;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.11534</link><description>&lt;p&gt;
PreAct: &#22312; ReAct &#20013;&#39044;&#27979;&#26410;&#26469;&#22686;&#24378;&#26234;&#33021;&#20307;&#30340;&#35268;&#21010;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PreAct: Predicting Future in ReAct Enhances Agent's Planning Ability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11534
&lt;/p&gt;
&lt;p&gt;
PreAct&#26159;&#19968;&#20010;&#25972;&#21512;&#20102;&#39044;&#27979;&#12289;&#25512;&#29702;&#21644;&#34892;&#21160;&#30340;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#27979;&#20449;&#24687;&#21487;&#20197;&#24110;&#21161;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#22810;&#26679;&#21270;&#21644;&#31574;&#30053;&#24615;&#30340;&#25512;&#29702;&#65292;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#34892;&#21160;&#65292;&#25552;&#21319;&#20219;&#21153;&#23436;&#25104;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#39044;&#27979;&#19982;&#23454;&#38469;&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#24322;&#24120;&#24120;&#26377;&#21161;&#20110;&#20010;&#20307;&#25299;&#23637;&#24605;&#32500;&#36807;&#31243;&#24182;&#36827;&#34892;&#21453;&#24605;&#65292;&#20174;&#32780;&#20419;&#36827;&#26397;&#27491;&#30830;&#26041;&#21521;&#25512;&#29702;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; PreAct &#30340;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#23427;&#23558;&#39044;&#27979;&#12289;&#25512;&#29702;&#21644;&#34892;&#21160;&#25972;&#21512;&#22312;&#19968;&#36215;&#12290;&#21033;&#29992;&#39044;&#27979;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26234;&#33021;&#20307;&#33021;&#22815;&#25552;&#20379;&#26356;&#22810;&#26679;&#21270;&#21644;&#31574;&#30053;&#23548;&#21521;&#30340;&#25512;&#29702;&#65292;&#36827;&#32780;&#23548;&#33268;&#26356;&#26377;&#25928;&#30340;&#34892;&#21160;&#65292;&#24110;&#21161;&#26234;&#33021;&#20307;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PreAct &#22312;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#20248;&#20110; ReAct &#26041;&#27861;&#65292;&#24182;&#19988;&#24403;&#19982;&#21453;&#24605;&#26041;&#27861;&#32467;&#21512;&#26102;&#65292;PreAct &#30340;&#25928;&#26524;&#21487;&#20197;&#24471;&#21040;&#25552;&#21319;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#25552;&#20379;&#19981;&#21516;&#25968;&#37327;&#30340;&#21382;&#21490;&#39044;&#27979;&#65292;&#24182;&#21457;&#29616;&#21382;&#21490;&#39044;&#27979;&#23545;LLM&#35268;&#21010;&#26377;&#25345;&#32493;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11534v1 Announce Type: cross  Abstract: Addressing the discrepancies between predictions and actual outcomes often aids individuals in expanding their thought processes and engaging in reflection, thereby facilitating reasoning in the correct direction. In this paper, we introduce $\textbf{PreAct}$, an agent framework that integrates $\textbf{pre}$diction with $\textbf{rea}$soning and $\textbf{act}$ion. Leveraging the information provided by predictions, a large language model (LLM) based agent can offer more diversified and strategically oriented reasoning, which in turn leads to more effective actions that help the agent complete complex tasks. Our experiments demonstrate that PreAct outperforms the ReAct approach in accomplishing complex tasks and that PreAct can be co-enhanced when combined with Reflexion methods. We prompt the model with different numbers of historical predictions and find that historical predictions have a sustained positive effect on LLM planning. The
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25351;&#20196;&#38142;&#65288;CoI&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#36880;&#27493;&#35299;&#20915;&#27599;&#20010;&#23376;&#20219;&#21153;&#26469;&#22788;&#29702;&#30001;&#22810;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#30340;&#25351;&#20196;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#22810;&#35821;&#35328;&#25688;&#35201;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.11532</link><description>&lt;p&gt;
&#25351;&#20196;&#38142;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#21512;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Instructions: Compositional Instruction Tuning on Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11532
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25351;&#20196;&#38142;&#65288;CoI&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#36890;&#36807;&#36880;&#27493;&#35299;&#20915;&#27599;&#20010;&#23376;&#20219;&#21153;&#26469;&#22788;&#29702;&#30001;&#22810;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#30340;&#25351;&#20196;&#65292;&#36827;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#22810;&#35821;&#35328;&#25688;&#35201;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#19968;&#31995;&#21015;&#22823;&#22411;&#21644;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#19981;&#21516;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#29978;&#33267;&#23545;&#26410;&#26366;&#35265;&#36807;&#30340;&#20219;&#21153;&#20063;&#36866;&#29992;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#25351;&#20196;&#38142;&#65288;CoI&#65289;&#30340;&#26032;&#27010;&#24565;&#65292;&#20854;&#20013;&#19968;&#20010;&#25351;&#20196;&#30340;&#36755;&#20986;&#25104;&#20026;&#19979;&#19968;&#20010;&#25351;&#20196;&#30340;&#36755;&#20837;&#65292;&#23601;&#20687;&#19968;&#26465;&#38142;&#26465;&#12290;&#19982;&#35299;&#20915;&#21333;&#19968;&#25351;&#20196;&#20219;&#21153;&#30340;&#20256;&#32479;&#20570;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#40723;&#21169;&#27169;&#22411;&#36880;&#27493;&#35299;&#20915;&#27599;&#20010;&#23376;&#20219;&#21153;&#65292;&#30452;&#33267;&#24471;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;CoI&#35843;&#25972;&#65288;&#21363;&#20351;&#29992;CoI&#25351;&#20196;&#36827;&#34892;&#24494;&#35843;&#65289;&#25552;&#39640;&#20102;&#27169;&#22411;&#22788;&#29702;&#30001;&#22810;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#30340;&#25351;&#20196;&#33021;&#21147;&#12290;&#32463;CoI&#35843;&#25972;&#30340;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#25688;&#35201;&#19978;&#20063;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#35777;&#26126;....
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11532v1 Announce Type: new  Abstract: Fine-tuning large language models (LLMs) with a collection of large and diverse instructions has improved the model's generalization to different tasks, even for unseen tasks. However, most existing instruction datasets include only single instructions, and they struggle to follow complex instructions composed of multiple subtasks (Wang et al., 2023a). In this work, we propose a novel concept of compositional instructions called chain-of-instructions (CoI), where the output of one instruction becomes an input for the next like a chain. Unlike the conventional practice of solving single instruction tasks, our proposed method encourages a model to solve each subtask step by step until the final answer is reached. CoI-tuning (i.e., fine-tuning with CoI instructions) improves the model's ability to handle instructions composed of multiple subtasks. CoI-tuned models also outperformed baseline models on multilingual summarization, demonstratin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#25913;&#36827;&#32763;&#35793;&#36136;&#37327;&#30340;&#25104;&#26412;&#25928;&#30410;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#26469;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#65292;&#20174;&#32780;&#25351;&#23548;&#25913;&#36827;&#26426;&#22120;&#32763;&#35793;&#12290;</title><link>https://arxiv.org/abs/2402.11525</link><description>&lt;p&gt;
&#29992;RLHF&#25512;&#36827;&#32763;&#35793;&#20559;&#22909;&#24314;&#27169;&#65306;&#36808;&#21521;&#25104;&#26412;&#25928;&#30410;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11525
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#25913;&#36827;&#32763;&#35793;&#36136;&#37327;&#30340;&#25104;&#26412;&#25928;&#30410;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#26469;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#65292;&#20174;&#32780;&#25351;&#23548;&#25913;&#36827;&#26426;&#22120;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11525v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#30495;&#23454;&#24615;&#12289;&#34920;&#36798;&#21147;&#21644;&#20248;&#38597;&#26159;&#26426;&#22120;&#32763;&#35793;&#20013;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#24230;&#37327;&#26631;&#20934;&#22914;BLEU&#24182;&#19981;&#20005;&#26684;&#31526;&#21512;&#20154;&#31867;&#23545;&#32763;&#35793;&#36136;&#37327;&#30340;&#20559;&#22909;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;&#25910;&#38598;&#20154;&#31867;&#23545;&#32763;&#35793;&#20043;&#38388;&#30340;&#27604;&#36739;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#24182;&#19981;&#23481;&#26131;&#65292;&#23588;&#20854;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#26469;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#26426;&#22120;&#32763;&#35793;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24182;&#25351;&#23548;&#38543;&#21518;&#23545;&#26426;&#22120;&#32763;&#35793;&#30340;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RLHF&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21319;&#32763;&#35793;&#36136;&#37327;&#65292;&#36825;&#31181;&#25913;&#36827;&#20063;&#26377;&#30410;&#20110;&#20854;&#20182;&#32763;&#35793;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11525v1 Announce Type: new  Abstract: Faithfulness, expressiveness, and elegance is the constant pursuit in machine translation. However, traditional metrics like \textit{BLEU} do not strictly align with human preference of translation quality. In this paper, we explore leveraging reinforcement learning with human feedback (\textit{RLHF}) to improve translation quality. It is non-trivial to collect a large high-quality dataset of human comparisons between translations, especially for low-resource languages. To address this issue, we propose a cost-effective preference learning strategy, optimizing reward models by distinguishing between human and machine translations. In this manner, the reward model learns the deficiencies of machine translation compared to human and guides subsequent improvements in machine translation. Experimental results demonstrate that \textit{RLHF} can effectively enhance translation quality and this improvement benefits other translation directions 
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#20154;&#20307;&#29616;&#20854;&#25198;&#28436;&#35282;&#33394;&#30340;&#31243;&#24230;&#23545;&#20445;&#30041;&#29575;&#30340;&#24433;&#21709;&#26377;&#38480;&#65292;&#32780;&#20854;&#35762;&#35805;&#30340;&#27599;&#20010;&#36718;&#27425;&#30340;&#38271;&#24230;&#26174;&#33879;&#24433;&#21709;&#20445;&#30041;&#29575;</title><link>https://arxiv.org/abs/2402.11522</link><description>&lt;p&gt;
&#25581;&#31034;&#24341;&#20154;&#20837;&#32988;&#23545;&#35805;&#30340;&#31192;&#23494;&#65306;&#35753;&#29992;&#25143;&#27785;&#36855;&#20110;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#30340;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Secrets of Engaging Conversations: Factors that Keep Users Hooked on Role-Playing Dialog Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11522
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#20307;&#29616;&#20854;&#25198;&#28436;&#35282;&#33394;&#30340;&#31243;&#24230;&#23545;&#20445;&#30041;&#29575;&#30340;&#24433;&#21709;&#26377;&#38480;&#65292;&#32780;&#20854;&#35762;&#35805;&#30340;&#27599;&#20010;&#36718;&#27425;&#30340;&#38271;&#24230;&#26174;&#33879;&#24433;&#21709;&#20445;&#30041;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#35805;&#20195;&#29702;&#21464;&#24471;&#36234;&#26469;&#36234;&#31867;&#20154;&#21270;&#65292;&#20154;&#20204;&#29616;&#22312;&#27491;&#22312;&#36827;&#34892;&#25345;&#32493;&#23545;&#35805;&#65292;&#21487;&#20197;&#20174;&#30701;&#26242;&#30340;&#26102;&#21051;&#24310;&#20280;&#21040;&#38271;&#26102;&#38388;&#12290;&#29702;&#35299;&#20419;&#20351;&#36825;&#20123;&#20114;&#21160;&#25345;&#32493;&#30340;&#22240;&#32032;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24456;&#23569;&#25506;&#32034;&#36825;&#31181;&#38271;&#26102;&#38388;&#21644;&#30495;&#23454;&#23545;&#35805;&#30340;&#30701;&#26399;&#27169;&#25311;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24433;&#21709;&#35282;&#33394;&#25198;&#28436;&#27169;&#22411;&#19982;&#30495;&#23454;&#29992;&#25143;&#20043;&#38388;&#20114;&#21160;&#20013;&#20445;&#30041;&#29575;&#30340;&#22240;&#32032;&#12290;&#36890;&#36807;&#20998;&#26512;&#30495;&#23454;&#29992;&#25143;&#21644;&#25968;&#21315;&#20010;&#35282;&#33394;&#20043;&#38388;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#26816;&#26597;&#20102;&#22810;&#20010;&#22240;&#32032;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#29992;&#25143;&#20445;&#30041;&#29575;&#30340;&#24433;&#21709;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#26426;&#22120;&#20154;&#20307;&#29616;&#20854;&#25198;&#28436;&#35282;&#33394;&#30340;&#31243;&#24230;&#23545;&#20445;&#30041;&#29575;&#30340;&#24433;&#21709;&#26377;&#38480;&#65292;&#32780;&#20854;&#35762;&#35805;&#30340;&#27599;&#20010;&#36718;&#27425;&#30340;&#38271;&#24230;&#26174;&#33879;&#24433;&#21709;&#20445;&#30041;&#29575;&#12290;&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11522v1 Announce Type: new  Abstract: With the growing humanlike nature of dialog agents, people are now engaging in extended conversations that can stretch from brief moments to substantial periods of time. Understanding the factors that contribute to sustaining these interactions is crucial, yet existing studies primarily focusing on short-term simulations that rarely explore such prolonged and real conversations.   In this paper, we investigate the factors influencing retention rates in real interactions with roleplaying models. By analyzing a large dataset of interactions between real users and thousands of characters, we systematically examine multiple factors and assess their impact on user retention rate. Surprisingly, we find that the degree to which the bot embodies the roles it plays has limited influence on retention rates, while the length of each turn it speaks significantly affects retention rates. This study sheds light on the critical aspects of user engageme
&lt;/p&gt;</description></item><item><title>Knowledge-to-SQL&#26694;&#26550;&#21033;&#29992;&#25968;&#25454;&#19987;&#23478;LLM&#25552;&#20379;&#26377;&#29992;&#30693;&#35782;&#65292;&#22686;&#24378;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11517</link><description>&lt;p&gt;
&#30693;&#35782;&#21040;SQL&#65306;&#29992;&#25968;&#25454;&#19987;&#23478;LLM&#22686;&#24378;SQL&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11517
&lt;/p&gt;
&lt;p&gt;
Knowledge-to-SQL&#26694;&#26550;&#21033;&#29992;&#25968;&#25454;&#19987;&#23478;LLM&#25552;&#20379;&#26377;&#29992;&#30693;&#35782;&#65292;&#22686;&#24378;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#29992;&#25143;&#26597;&#35810;&#29983;&#25104;&#20934;&#30830;&#30340;SQL&#65288;&#25991;&#26412;&#21040;SQL&#65289;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#29983;&#25104;SQL&#38656;&#35201;&#29702;&#35299;&#26597;&#35810;&#21644;&#25968;&#25454;&#24211;&#65292;&#28982;&#21518;&#26681;&#25454;&#25968;&#25454;&#24211;&#26816;&#32034;&#20934;&#30830;&#30340;&#25968;&#25454;&#12290;&#29616;&#26377;&#27169;&#22411;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32508;&#21512;&#33021;&#21147;&#65292;&#26681;&#25454;&#25968;&#25454;&#24211;&#27169;&#24335;&#29983;&#25104;SQL&#12290;&#28982;&#32780;&#65292;&#26377;&#20123;&#24517;&#35201;&#30340;&#30693;&#35782;&#27809;&#26377;&#26126;&#30830;&#21253;&#21547;&#22312;&#25968;&#25454;&#24211;&#27169;&#24335;&#20013;&#65292;&#25110;&#32773;&#34987;LLMs&#23398;&#20064;&#20102;&#12290;&#22240;&#27492;&#65292;&#30693;&#35782;&#19981;&#36275;&#30340;&#26597;&#35810;&#29983;&#25104;&#30340;SQL&#21487;&#33021;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#36825;&#20250;&#23545;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Knowledge-to-SQL&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#23450;&#21046;&#30340;&#25968;&#25454;&#19987;&#23478;LLM&#65288;DELLM&#65289;&#20026;&#25152;&#26377;&#31867;&#22411;&#30340;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#25552;&#20379;&#26377;&#29992;&#30340;&#30693;&#35782;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;DELLM&#30340;&#35774;&#35745;&#65292;&#21253;&#25324;&#34920;&#26684;&#35835;&#21462;&#21644;&#22522;&#26412;&#24494;&#35843;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11517v1 Announce Type: new  Abstract: Generating accurate SQL for user queries (text-to-SQL) is a long-standing problem since the generation of the SQL requires comprehending the query and database and retrivale the accurate data from the database accordingly. Existing models rely on the comprehensive ability of Large Language Models (LLMs) to generate the SQL according to the database schema. However, there is some necessary knowledge that is not explicitly included in the database schema or has been learned by LLMs. Thus, the generated SQL of the knowledge-insufficient queries may be inaccurate, which negatively impacts the robustness of the text-to-SQL models. To deal with this situation, we propose the Knowledge-to-SQL framework, which employs tailored Data Expert LLM (DELLM) to provide helpful knowledge for all types of text-to-SQL models. Specifically, we provide the detailed design of DELLM, in terms of table reading, and the basic fine-tuning process. We further prov
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DeepSoftDebias&#31639;&#27861;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#38598;&#12289;&#20934;&#30830;&#24230;&#25351;&#26631;&#21644;NLP&#20219;&#21153;&#20013;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#22312;&#20943;&#23569;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23447;&#25945;&#20559;&#35265;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.11512</link><description>&lt;p&gt;
&#20174;&#20559;&#35265;&#21040;&#24179;&#31561;&#65306;&#21435;&#20559;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#35789;&#23884;&#20837;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11512
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DeepSoftDebias&#31639;&#27861;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#25968;&#25454;&#38598;&#12289;&#20934;&#30830;&#24230;&#25351;&#26631;&#21644;NLP&#20219;&#21153;&#20013;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#22312;&#20943;&#23569;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23447;&#25945;&#20559;&#35265;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#22312;&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#23427;&#20204;&#26159;&#36825;&#20123;&#27169;&#22411;&#25226;&#25569;&#19978;&#19979;&#25991;&#20851;&#31995;&#12289;&#20419;&#36827;&#26356;&#32454;&#33268;&#35821;&#35328;&#29702;&#35299;&#20197;&#21450;&#22312;&#35768;&#22810;&#38656;&#35201;&#23545;&#20154;&#31867;&#35821;&#35328;&#26377;&#22522;&#26412;&#29702;&#35299;&#30340;&#22797;&#26434;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#30340;&#22522;&#30707;&#12290;&#37492;&#20110;&#36825;&#20123;&#23884;&#20837;&#24448;&#24448;&#33258;&#36523;&#21453;&#26144;&#25110;&#23637;&#31034;&#20559;&#35265;&#65292;&#22240;&#27492;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20063;&#20250;&#26080;&#24847;&#20013;&#23398;&#20064;&#36825;&#31181;&#20559;&#35265;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#24320;&#21019;&#24615;&#21069;&#20154;&#30740;&#31350;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;DeepSoftDebias&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#8220;&#36719;&#21435;&#20559;&#8221;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#21508;&#31867;&#26368;&#20808;&#36827;&#25968;&#25454;&#38598;&#12289;&#20934;&#30830;&#24230;&#25351;&#26631;&#21644;&#20855;&#26377;&#25361;&#25112;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#20010;&#31639;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;DeepSoftDebias&#22312;&#20943;&#23569;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#23447;&#25945;&#20559;&#35265;&#26041;&#38754;&#20248;&#20110;&#30446;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11512v1 Announce Type: new  Abstract: Embeddings play a pivotal role in the efficacy of Large Language Models. They are the bedrock on which these models grasp contextual relationships and foster a more nuanced understanding of language and consequently perform remarkably on a plethora of complex tasks that require a fundamental understanding of human language. Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias. In this work, we build on the seminal previous work and propose DeepSoftDebias, an algorithm that uses a neural network to perform `soft debiasing'. We exhaustively evaluate this algorithm across a variety of SOTA datasets, accuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias outperforms the current state-of-the-art methods at reducing bias across gender, race, and religion.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;FlexLoRA&#65292;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#32858;&#21512;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20805;&#20998;&#21033;&#29992;&#24322;&#36136;&#23458;&#25143;&#36164;&#28304;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#26412;&#22320;LoRA&#25490;&#21517;&#21644;&#37319;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#36827;&#34892;&#26435;&#37325;&#37325;&#26032;&#20998;&#37197;&#65292;&#25552;&#21319;&#20840;&#23616;&#27169;&#22411;&#30340;&#24191;&#27867;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.11505</link><description>&lt;p&gt;
&#22312;&#24322;&#26500;&#35821;&#35328;&#20219;&#21153;&#21644;&#23458;&#25143;&#36164;&#28304;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32852;&#37030;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Federated Fine-tuning of Large Language Models under Heterogeneous Language Tasks and Client Resources
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11505
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;FlexLoRA&#65292;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#32858;&#21512;&#26041;&#26696;&#65292;&#33021;&#22815;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20805;&#20998;&#21033;&#29992;&#24322;&#36136;&#23458;&#25143;&#36164;&#28304;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#26412;&#22320;LoRA&#25490;&#21517;&#21644;&#37319;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#36827;&#34892;&#26435;&#37325;&#37325;&#26032;&#20998;&#37197;&#65292;&#25552;&#21319;&#20840;&#23616;&#27169;&#22411;&#30340;&#24191;&#27867;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#34987;&#24212;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23458;&#25143;&#30340;&#36164;&#28304;&#21644;&#25968;&#25454;&#20998;&#24067;&#19981;&#22343;&#21248;&#65292;&#36825;&#24341;&#21457;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;FlexLoRA&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;LLM&#24494;&#35843;&#32858;&#21512;&#26041;&#26696;&#65292;&#23427;&#21487;&#20197;&#32531;&#35299;&#20256;&#32479;FL&#20013;&#30340;&#8220;&#26742;&#25928;&#24212;&#8221;&#65292;&#35813;&#25928;&#24212;&#38480;&#21046;&#20102;&#25317;&#26377;&#20016;&#23500;&#36164;&#28304;&#30340;&#23458;&#25143;&#23454;&#29616;&#28508;&#21147;&#65292;&#23558;&#20182;&#20204;&#19982;&#26368;&#32570;&#20047;&#36164;&#28304;&#30340;&#21442;&#19982;&#32773;&#30340;&#33021;&#21147;&#25414;&#32465;&#22312;&#19968;&#36215;&#12290;FlexLoRA&#20801;&#35768;&#21160;&#24577;&#35843;&#25972;&#26412;&#22320;LoRA&#25490;&#21517;&#65292;&#20419;&#36827;&#20840;&#23616;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#24182;&#36171;&#20104;&#26356;&#24191;&#27867;&#12289;&#19981;&#22826;&#20219;&#21153;&#29305;&#23450;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#20174;&#20010;&#20307;&#23458;&#25143;&#36129;&#29486;&#20013;&#21512;&#25104;&#23436;&#25972;&#22823;&#23567;&#30340;LoRA&#26435;&#37325;&#65292;&#24182;&#21033;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#36827;&#34892;&#26435;&#37325;&#37325;&#26032;&#20998;&#37197;&#65292;FlexLoRA&#20805;&#20998;&#21033;&#29992;&#20102;&#23458;&#25143;&#38388;&#30340;&#36164;&#28304;&#24046;&#24322;&#12290;&#26412;&#30740;&#31350;&#28041;&#21450;&#36229;&#36807;1600&#20010;&#25191;&#34892;&#22810;&#26679;NLP&#20219;&#21153;&#30340;&#23458;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11505v1 Announce Type: cross  Abstract: Federated Learning (FL) has recently been applied to the parameter-efficient fine-tuning of Large Language Models (LLMs). While promising, it raises significant challenges due to the heterogeneous resources and data distributions of clients.This study introduces FlexLoRA, a simple yet effective aggregation scheme for LLM fine-tuning, which mitigates the "buckets effect" in traditional FL that restricts the potential of clients with ample resources by tying them to the capabilities of the least-resourced participants. FlexLoRA allows for dynamic adjustment of local LoRA ranks, fostering the development of a global model imbued with broader, less task-specific knowledge. By synthesizing a full-size LoRA weight from individual client contributions and employing Singular Value Decomposition (SVD) for weight redistribution, FlexLoRA fully leverages heterogeneous client resources. Involving over 1,600 clients performing diverse NLP tasks, ou
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#30693;&#35782;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#20197;&#28085;&#30422;&#35821;&#35328;&#27169;&#22411;&#20869;&#30340;&#26080;&#25552;&#31034;&#21644;&#26377;&#25552;&#31034;&#25935;&#24863;&#24615;&#30693;&#35782;&#65292;&#36890;&#36807;&#36991;&#20813;&#25552;&#31034;&#25935;&#24863;&#24615;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#26356;&#21487;&#38752;&#21644;&#31283;&#20581;&#12290;</title><link>https://arxiv.org/abs/2402.11493</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36793;&#30028;&#22522;&#20934;&#65306;&#23545;&#27169;&#22411;&#35780;&#20272;&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Knowledge Boundary for Large Language Model: A Different Perspective on Model Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11493
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#30693;&#35782;&#36793;&#30028;&#30340;&#27010;&#24565;&#65292;&#20197;&#28085;&#30422;&#35821;&#35328;&#27169;&#22411;&#20869;&#30340;&#26080;&#25552;&#31034;&#21644;&#26377;&#25552;&#31034;&#25935;&#24863;&#24615;&#30693;&#35782;&#65292;&#36890;&#36807;&#36991;&#20813;&#25552;&#31034;&#25935;&#24863;&#24615;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#26356;&#21487;&#38752;&#21644;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20013;&#21462;&#24471;&#20102;&#23454;&#36136;&#24615;&#36827;&#23637;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33021;&#21147;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#38382;&#31572;&#23545;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20351;&#29992;&#22266;&#23450;&#38382;&#39064;&#25110;&#26377;&#38480;&#30340;&#37322;&#20041;&#20316;&#20026;&#26597;&#35810;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#26159;&#19981;&#21487;&#38752;&#21644;&#20840;&#38754;&#30340;&#65292;&#22240;&#20026;&#35821;&#35328;&#27169;&#22411;&#23545;&#25552;&#31034;&#24456;&#25935;&#24863;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#36793;&#30028;&#30340;&#26032;&#27010;&#24565;&#65292;&#20197;&#21253;&#21547;&#35821;&#35328;&#27169;&#22411;&#20869;&#30340;&#26080;&#25552;&#31034;&#21644;&#26377;&#25552;&#31034;&#25935;&#24863;&#24615;&#30693;&#35782;&#12290;&#30693;&#35782;&#36793;&#30028;&#36991;&#20813;&#20102;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#25552;&#31034;&#25935;&#24863;&#24615;&#65292;&#20351;&#20854;&#26356;&#21487;&#38752;&#21644;&#31283;&#20581;&#12290;&#20026;&#20102;&#25506;&#32034;&#32473;&#23450;&#27169;&#22411;&#30340;&#30693;&#35782;&#36793;&#30028;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24102;&#26377;&#35821;&#20041;&#32422;&#26463;&#30340;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#35782;&#21035;&#27599;&#20010;&#37096;&#20998;&#30340;&#26368;&#20339;&#25552;&#31034;&#30340;&#26032;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11493v1 Announce Type: new  Abstract: In recent years, substantial advancements have been made in the development of large language models, achieving remarkable performance across diverse tasks. To evaluate the knowledge ability of language models, previous studies have proposed lots of benchmarks based on question-answering pairs. We argue that it is not reliable and comprehensive to evaluate language models with a fixed question or limited paraphrases as the query, since language models are sensitive to prompt. Therefore, we introduce a novel concept named knowledge boundary to encompass both prompt-agnostic and prompt-sensitive knowledge within language models. Knowledge boundary avoids prompt sensitivity in language model evaluations, rendering them more dependable and robust. To explore the knowledge boundary for a given model, we propose projected gradient descent method with semantic constraints, a new algorithm designed to identify the optimal prompt for each piece o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26041;&#27861;SimPlan&#65292;&#32467;&#21512;&#20102;LLMs&#21644;&#32463;&#20856;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#35268;&#21010;&#39046;&#22495;&#30340;&#23454;&#39564;&#34920;&#26126;SimPlan&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#32773;</title><link>https://arxiv.org/abs/2402.11489</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;: &#35745;&#21010;&#26159;&#20160;&#20040;&#65311;&#35780;&#20272;&#21644;&#24320;&#21457;&#38024;&#23545;LLMs&#30340;&#35745;&#21010;&#24847;&#35782;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
What's the Plan? Evaluating and Developing Planning-Aware Techniques for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11489
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#26041;&#27861;SimPlan&#65292;&#32467;&#21512;&#20102;LLMs&#21644;&#32463;&#20856;&#35268;&#21010;&#26041;&#27861;&#65292;&#22312;&#21508;&#31181;&#35268;&#21010;&#39046;&#22495;&#30340;&#23454;&#39564;&#34920;&#26126;SimPlan&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#32773;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11489v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#35745;&#21010;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#28041;&#21450;&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#25214;&#21040;&#23454;&#29616;&#29305;&#23450;&#30446;&#26631;&#30340;&#19968;&#31995;&#21015;&#34892;&#21160;&#12290; &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#38656;&#35201;&#35745;&#21010;&#33021;&#21147;&#30340;&#24212;&#29992;&#65292;&#22914;&#32593;&#32476;&#25110;&#20855;&#20307;&#20195;&#29702;&#12290; &#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#19968;&#33268;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;LLMs&#32570;&#20047;&#35745;&#21010;&#25152;&#38656;&#30340;&#24517;&#35201;&#25216;&#33021;&#12290; &#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#20513;&#23548;&#19968;&#31181;&#23558;LLMs&#19982;&#32463;&#20856;&#35745;&#21010;&#26041;&#27861;&#32467;&#21512;&#30340;&#28151;&#21512;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290; &#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SimPlan&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35774;&#32622;&#20013;&#30340;&#34920;&#29616;&#12290; &#25105;&#20204;&#22312;&#21508;&#31181;&#35268;&#21010;&#39046;&#22495;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;SimPlan&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11489v1 Announce Type: new  Abstract: Planning is a fundamental task in artificial intelligence that involves finding a sequence of actions that achieve a specified goal in a given environment. Large language models (LLMs) are increasingly used for applications that require planning capabilities, such as web or embodied agents. In line with recent studies, we demonstrate through experimentation that LLMs lack necessary skills required for planning. Based on these observations, we advocate for the potential of a hybrid approach that combines LLMs with classical planning methodology. Then, we introduce SimPlan, a novel hybrid-method, and evaluate its performance in a new challenging setup. Our extensive experiments across various planning domains demonstrate that SimPlan significantly outperforms existing LLM-based planners.
&lt;/p&gt;</description></item><item><title>LEIA&#26159;&#19968;&#31181;&#35821;&#35328;&#36866;&#24212;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#23454;&#20307;&#21517;&#31216;&#36328;&#35821;&#35328;&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11485</link><description>&lt;p&gt;
LEIA: &#21033;&#29992;&#22522;&#20110;&#23454;&#20307;&#30340;&#25968;&#25454;&#22686;&#24378;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#20419;&#36827;&#36328;&#35821;&#35328;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
LEIA: Facilitating Cross-Lingual Knowledge Transfer in Language Models with Entity-based Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11485
&lt;/p&gt;
&lt;p&gt;
LEIA&#26159;&#19968;&#31181;&#35821;&#35328;&#36866;&#24212;&#35843;&#25972;&#26041;&#27861;&#65292;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#23454;&#20307;&#21517;&#31216;&#36328;&#35821;&#35328;&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#20110;&#33521;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#20854;&#20182;&#35821;&#35328;&#30340;&#25805;&#20316;&#30001;&#20110;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#25928;&#29575;&#21644;&#28508;&#21147;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35821;&#35328;&#36866;&#24212;&#26041;&#27861;&#24120;&#24120;&#24573;&#35270;&#36328;&#35821;&#35328;&#30417;&#30563;&#30340;&#22909;&#22788;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;LEIA&#65292;&#19968;&#31181;&#21033;&#29992;&#36328;&#35821;&#35328;&#23545;&#40784;&#30340;&#32500;&#22522;&#30334;&#31185;&#23454;&#20307;&#21517;&#31216;&#30340;&#35821;&#35328;&#36866;&#24212;&#35843;&#25972;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#33521;&#35821;&#23454;&#20307;&#21517;&#31216;&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#24182;&#20351;&#29992;&#20174;&#24038;&#21040;&#21491;&#30340;&#35821;&#35328;&#24314;&#27169;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#22810;&#26679;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;LEIA&#65292;&#20351;&#29992;7B&#21442;&#25968;&#30340;LLMs&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#30340;&#26174;&#33879;&#24615;&#33021;&#22686;&#30410;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/studio-ousia/leia&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11485v1 Announce Type: cross  Abstract: Adapting English-based large language models (LLMs) to other languages has become increasingly popular due to the efficiency and potential of cross-lingual transfer. However, existing language adaptation methods often overlook the benefits of cross-lingual supervision. In this study, we introduce LEIA, a language adaptation tuning method that utilizes Wikipedia entity names aligned across languages. This method involves augmenting the target language corpus with English entity names and training the model using left-to-right language modeling. We assess LEIA on diverse question answering datasets using 7B-parameter LLMs, demonstrating significant performance gains across various non-English languages. The source code is available at https://github.com/studio-ousia/leia.
&lt;/p&gt;</description></item><item><title>DictLLM&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#36827;&#38190;&#20540;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#24314;&#27169;&#65292;&#29992;&#20110;&#29983;&#25104;&#21307;&#23398;&#35786;&#26029;&#12290;&#23427;&#21253;&#25324;&#32452;&#20301;&#32622;&#32534;&#30721;&#12289;&#23618;&#27425;&#27880;&#24847;&#20559;&#24046;&#21644;&#20248;&#21270;&#20256;&#36755;&#23545;&#40784;&#23618;&#12290;</title><link>https://arxiv.org/abs/2402.11481</link><description>&lt;p&gt;
DictLLM: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25805;&#32437;&#38190;&#20540;&#25968;&#25454;&#32467;&#26500;&#20197;&#22686;&#24378;&#21307;&#23398;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
DictLLM: Harnessing Key-Value Data Structures with Large Language Models for Enhanced Medical Diagnostics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11481
&lt;/p&gt;
&lt;p&gt;
DictLLM&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#36827;&#38190;&#20540;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#24314;&#27169;&#65292;&#29992;&#20110;&#29983;&#25104;&#21307;&#23398;&#35786;&#26029;&#12290;&#23427;&#21253;&#25324;&#32452;&#20301;&#32622;&#32534;&#30721;&#12289;&#23618;&#27425;&#27880;&#24847;&#20559;&#24046;&#21644;&#20248;&#21270;&#20256;&#36755;&#23545;&#40784;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#22797;&#26434;&#30340;&#20449;&#24687;&#32452;&#32455;&#26426;&#21046;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#24207;&#21015;&#21270;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#20805;&#20998;&#35299;&#20915;&#38190;&#20540;&#32467;&#26500;&#21270;&#25968;&#25454;&#22266;&#26377;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#36825;&#20123;&#26041;&#27861;&#24182;&#19981;&#29702;&#24819;&#65292;&#32463;&#24120;&#23548;&#33268;&#26356;&#22823;&#30340;&#36755;&#20837;&#23610;&#23544;&#21644;&#23545;&#36755;&#20837;&#26356;&#25913;&#30340;&#36866;&#24212;&#24615;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DictLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#36827;&#38190;&#20540;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#21307;&#23398;&#23454;&#39564;&#23460;&#25253;&#21578;&#65289;&#30340;&#24314;&#27169;&#65292;&#20197;&#29983;&#25104;&#21307;&#23398;&#35786;&#26029;&#12290;DictLLM&#25972;&#21512;&#20102;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;&#65288;1&#65289;&#32452;&#20301;&#32622;&#32534;&#30721;&#20197;&#20445;&#25345;&#32622;&#25442;&#19981;&#21464;&#24615;&#65292;&#65288;2&#65289;&#23618;&#27425;&#27880;&#24847;&#20559;&#24046;&#20197;&#25429;&#25417;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#22266;&#26377;&#20559;&#24046;&#65292;&#20197;&#21450;&#65288;3&#65289;&#19968;&#20010;&#20248;&#21270;&#20256;&#36755;&#23545;&#40784;&#23618;&#65292;&#23558;&#23383;&#20856;&#32534;&#30721;&#22120;&#29983;&#25104;&#30340;&#23884;&#20837;&#19982;LLM&#23545;&#40784;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#31995;&#21015;&#22266;&#23450;&#38271;&#24230;&#30340;&#34394;&#25311;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11481v1 Announce Type: new  Abstract: Structured data offers a sophisticated mechanism for the organization of information. Existing methodologies for the text-serialization of structured data in the context of large language models fail to adequately address the heterogeneity inherent in key-value structured data. These methods are not ideal and frequently result in larger input sizes and poor adaptability to input changes. In this paper, we introduce DictLLM, an innovative framework designed to improve the modeling of key-value structured data, like medical laboratory reports, for generating medical diagnoses. DictLLM integrates three key components: (1) group positional encoding to maintain permutation invariance, (2) hierarchical attention bias to capture the inherent bias in structured data, and (3) an optimal transport alignment layer that aligns the embedding generated by the dictionary encoder with the LLM, thereby producing a sequence of fixed-length virtual tokens.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#25968;&#25454;&#19982;&#27169;&#22411;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#25552;&#21462;&#19981;&#21516;&#29305;&#24449;&#26469;&#39044;&#27979;Transformer&#25991;&#26412;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11469</link><description>&lt;p&gt;
&#22312;&#25628;&#32034;&#35757;&#32451;&#25968;&#25454;&#19982;Transformer&#25991;&#26412;&#27169;&#22411;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26102;&#30340;&#19968;&#20010;&#26377;&#36259;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Curious Case of Searching for the Correlation between Training Data and Adversarial Robustness of Transformer Textual Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#25968;&#25454;&#19982;&#27169;&#22411;&#40065;&#26834;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#25552;&#21462;&#19981;&#21516;&#29305;&#24449;&#26469;&#39044;&#27979;Transformer&#25991;&#26412;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;&#25991;&#26412;Transformer&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#20063;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25991;&#26412;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#20256;&#32479;&#30340;&#23545;&#25239;&#24615;&#35780;&#20272;&#36890;&#24120;&#22312;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20043;&#21518;&#25165;&#36827;&#34892;&#65292;&#24573;&#30053;&#20102;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#26088;&#22312;&#35777;&#26126;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#40065;&#26834;&#24615;&#20043;&#38388;&#20063;&#23384;&#22312;&#30528;&#24378;&#20851;&#32852;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#20195;&#34920;&#24191;&#27867;&#36755;&#20837;&#24494;&#35843;&#35821;&#26009;&#24211;&#23646;&#24615;&#30340;13&#31181;&#19981;&#21516;&#29305;&#24449;&#65292;&#24182;&#29992;&#23427;&#20204;&#26469;&#39044;&#27979;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#20165;&#32534;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;BERT&#21644;RoBERTa&#65292;&#24182;&#38468;&#21152;&#20102;BART&#12289;ELECTRA&#21644;GPT2&#30340;&#20854;&#20182;&#32467;&#26524;&#65292;&#20026;&#25105;&#20204;&#30340;&#35770;&#28857;&#25552;&#20379;&#22810;&#26679;&#30340;&#35777;&#25454;&#12290;&#39318;&#20808;&#65292;&#32463;&#39564;&#35777;&#26126;&#65292;(a)&#25552;&#21462;&#30340;&#29305;&#24449;&#21487;&#19982;&#36731;&#37327;&#32423;&#20998;&#31867;&#22120;&#65288;&#22914;&#38543;&#26426;&#26862;&#26519;&#65289;&#19968;&#36215;&#26377;&#25928;&#22320;&#39044;&#27979;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11469v1 Announce Type: cross  Abstract: Existing works have shown that fine-tuned textual transformer models achieve state-of-the-art prediction performances but are also vulnerable to adversarial text perturbations. Traditional adversarial evaluation is often done \textit{only after} fine-tuning the models and ignoring the training data. In this paper, we want to prove that there is also a strong correlation between training data and model robustness. To this end, we extract 13 different features representing a wide range of input fine-tuning corpora properties and use them to predict the adversarial robustness of the fine-tuned models. Focusing mostly on encoder-only transformer models BERT and RoBERTa with additional results for BART, ELECTRA and GPT2, we provide diverse evidence to support our argument. First, empirical analyses show that (a) extracted features can be used with a lightweight classifier such as Random Forest to effectively predict the attack success rate 
&lt;/p&gt;</description></item><item><title>LLM&#20309;&#26102;&#38656;&#35201;&#26816;&#32034;&#22686;&#24378;&#65311;&#20943;&#36731;LLM&#30340;&#36807;&#24230;&#33258;&#20449;&#26377;&#21161;&#20110;&#26816;&#32034;&#22686;&#24378;</title><link>https://arxiv.org/abs/2402.11457</link><description>&lt;p&gt;
LLM&#20309;&#26102;&#38656;&#35201;&#26816;&#32034;&#22686;&#24378;&#65311;&#20943;&#36731;LLM&#30340;&#36807;&#24230;&#33258;&#20449;&#26377;&#21161;&#20110;&#26816;&#32034;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
When Do LLMs Need Retrieval Augmentation? Mitigating LLMs' Overconfidence Helps Retrieval Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11457
&lt;/p&gt;
&lt;p&gt;
LLM&#20309;&#26102;&#38656;&#35201;&#26816;&#32034;&#22686;&#24378;&#65311;&#20943;&#36731;LLM&#30340;&#36807;&#24230;&#33258;&#20449;&#26377;&#21161;&#20110;&#26816;&#32034;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#21457;&#29616;&#24456;&#38590;&#30693;&#36947;&#33258;&#24049;&#19981;&#20855;&#22791;&#26576;&#20123;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#24448;&#24448;&#20250;&#25552;&#20379;&#34394;&#20551;&#31572;&#26696;&#12290;&#26816;&#32034;&#22686;&#24378;&#65288;RA&#65289;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#20197;&#20943;&#36731;LLMs&#30340;&#24187;&#35273;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39069;&#22806;&#30340;&#24320;&#38144;&#21644;&#26816;&#32034;&#36136;&#37327;&#19981;&#30830;&#23450;&#65292;&#22987;&#32456;&#36827;&#34892;RA&#21487;&#33021;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;&#19968;&#20010;&#30452;&#35266;&#30340;&#24819;&#27861;&#26159;&#21482;&#26377;&#22312;LLMs&#23545;&#38382;&#39064;&#19981;&#30830;&#23450;&#26102;&#25165;&#36827;&#34892;&#26816;&#32034;&#12290;&#36825;&#28608;&#21457;&#25105;&#20204;&#22686;&#24378;LLMs&#24863;&#30693;&#30693;&#35782;&#36793;&#30028;&#30340;&#33021;&#21147;&#20197;&#24110;&#21161;RA&#12290;&#26412;&#25991;&#39318;&#20808;&#23450;&#37327;&#34913;&#37327;LLMs&#30340;&#36825;&#31181;&#33021;&#21147;&#24182;&#30830;&#35748;&#23427;&#20204;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;LLMs&#23545;&#38382;&#39064;&#30340;&#30830;&#23450;&#24615;&#22914;&#20309;&#19982;&#20182;&#20204;&#20381;&#36182;&#22806;&#37096;&#26816;&#32034;&#20449;&#24687;&#30456;&#20851;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;LLMs&#23545;&#30693;&#35782;&#36793;&#30028;&#30340;&#24863;&#30693;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#22312;&#20943;&#23569;&#36807;&#24230;&#33258;&#20449;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11457v1 Announce Type: new  Abstract: Large Language Models (LLMs) have been found to have difficulty knowing they do not possess certain knowledge and tend to provide specious answers in such cases. Retrieval Augmentation (RA) has been extensively studied to mitigate LLMs' hallucinations. However, due to the extra overhead and unassured quality of retrieval, it may not be optimal to conduct RA all the time. A straightforward idea is to only conduct retrieval when LLMs are uncertain about a question. This motivates us to enhance the LLMs' ability to perceive their knowledge boundaries to help RA. In this paper, we first quantitatively measure LLMs' such ability and confirm their overconfidence. Then, we study how LLMs' certainty about a question correlates with their dependence on external retrieved information. We propose several methods to enhance LLMs' perception of knowledge boundaries and show that they are effective in reducing overconfidence. Additionally, equipped wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;FactPICO&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#25991;&#26412;&#30340;&#31616;&#26126;&#35821;&#35328;&#25688;&#35201;&#30340;&#20107;&#23454;&#24615;&#22522;&#20934;&#65292;&#23545;RCT&#20013;&#30340;&#20851;&#38190;&#35201;&#32032;&#21644;&#25253;&#21578;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23545;LLMs&#28155;&#21152;&#30340;&#39069;&#22806;&#20449;&#24687;&#36827;&#34892;&#26816;&#26597;&#12290;</title><link>https://arxiv.org/abs/2402.11456</link><description>&lt;p&gt;
FactPICO: &#21307;&#23398;&#35777;&#25454;&#30340;&#31616;&#26126;&#35821;&#35328;&#25688;&#35201;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;FactPICO&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#25991;&#26412;&#30340;&#31616;&#26126;&#35821;&#35328;&#25688;&#35201;&#30340;&#20107;&#23454;&#24615;&#22522;&#20934;&#65292;&#23545;RCT&#20013;&#30340;&#20851;&#38190;&#35201;&#32032;&#21644;&#25253;&#21578;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23545;LLMs&#28155;&#21152;&#30340;&#39069;&#22806;&#20449;&#24687;&#36827;&#34892;&#26816;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11456v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#21033;&#29992;LLMs&#36827;&#34892;&#31616;&#26126;&#35821;&#35328;&#25688;&#35201;&#21487;&#20197;&#25913;&#21892;&#25216;&#26415;&#20869;&#23481;&#30340;&#25991;&#26412;&#21487;&#35775;&#38382;&#24615;&#12290;&#20294;&#26159;&#22312;&#21307;&#23398;&#36825;&#26679;&#19968;&#20010;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#36825;&#20123;&#25688;&#35201;&#26377;&#22810;&#30495;&#23454;&#65311;&#26412;&#25991;&#20171;&#32461;&#20102;FactPICO&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25551;&#36848;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#65288;RCTs&#65289;&#30340;&#21307;&#23398;&#25991;&#26412;&#30340;&#31616;&#26126;&#35821;&#35328;&#25688;&#35201;&#30340;&#20107;&#23454;&#24615;&#22522;&#20934;&#65292;RCTs&#26159;&#24490;&#35777;&#21307;&#23398;&#30340;&#22522;&#30784;&#65292;&#21487;&#20197;&#30452;&#25509;&#20026;&#24739;&#32773;&#27835;&#30103;&#25552;&#20379;&#20449;&#24687;&#12290;FactPICO&#30001;&#26469;&#33258;&#19977;&#20010;LLMs&#65288;&#21363;GPT-4&#12289;Llama-2&#21644;Alpaca&#65289;&#29983;&#25104;&#30340;345&#20010;RCT&#25688;&#35201;&#30340;&#31616;&#26126;&#35821;&#35328;&#25688;&#35201;&#32452;&#25104;&#65292;&#20855;&#26377;&#19987;&#23478;&#32454;&#33268;&#35780;&#20272;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#30001;&#12290;&#25105;&#20204;&#35780;&#20272;&#36825;&#20123;&#25688;&#35201;&#20013;RCT&#30340;&#20851;&#38190;&#35201;&#32032;&#65288;&#20154;&#32676;&#12289;&#24178;&#39044;&#25514;&#26045;&#12289;&#23545;&#29031;&#32452;&#12289;&#32467;&#26524;&#65288;PICO&#65289;&#65289;&#20197;&#21450;&#20851;&#20110;&#36825;&#20123;&#20869;&#23481;&#30340;&#25253;&#21578;&#21457;&#29616;&#30340;&#20107;&#23454;&#24615;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;LLMs&#28155;&#21152;&#30340;&#39069;&#22806;&#20449;&#24687;&#65288;&#20363;&#22914;&#35299;&#37322;&#65289;&#30340;&#27491;&#30830;&#24615;&#12290;&#20351;&#29992;FactPICO&#65292;&#25105;&#20204;&#23545;&#22810;&#20010;&#29616;&#26377;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11456v1 Announce Type: new  Abstract: Plain language summarization with LLMs can be useful for improving textual accessibility of technical content. But how factual are these summaries in a high-stakes domain like medicine? This paper presents FactPICO, a factuality benchmark for plain language summarization of medical texts describing randomized controlled trials (RCTs), which are the basis of evidence-based medicine and can directly inform patient treatment. FactPICO consists of 345 plain language summaries of RCT abstracts generated from three LLMs (i.e., GPT-4, Llama-2, and Alpaca), with fine-grained evaluation and natural language rationales from experts. We assess the factuality of critical elements of RCTs in those summaries: Populations, Interventions, Comparators, Outcomes (PICO), as well as the reported findings concerning these. We also evaluate the correctness of the extra information (e.g., explanations) added by LLMs. Using FactPICO, we benchmark a range of exi
&lt;/p&gt;</description></item><item><title>LoRA-Flow&#25552;&#20986;&#20102;&#21160;&#24577;&#26435;&#37325;&#26469;&#35843;&#25972;&#19981;&#21516;LoRA&#30340;&#24433;&#21709;&#65292;&#20197;&#24212;&#23545;&#29983;&#25104;&#20219;&#21153;&#20013;&#19981;&#21516;&#26631;&#35760;&#25152;&#38656;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11455</link><description>&lt;p&gt;
LoRA-Flow: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#21160;&#24577;LoRA&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
LoRA-Flow: Dynamic LoRA Fusion for Large Language Models in Generative Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11455
&lt;/p&gt;
&lt;p&gt;
LoRA-Flow&#25552;&#20986;&#20102;&#21160;&#24577;&#26435;&#37325;&#26469;&#35843;&#25972;&#19981;&#21516;LoRA&#30340;&#24433;&#21709;&#65292;&#20197;&#24212;&#23545;&#29983;&#25104;&#20219;&#21153;&#20013;&#19981;&#21516;&#26631;&#35760;&#25152;&#38656;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LoRA&#21033;&#29992;&#36731;&#37327;&#32423;&#27169;&#22359;&#23450;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#36866;&#24212;&#27599;&#20010;&#19979;&#28216;&#20219;&#21153;&#25110;&#39046;&#22495;&#65292;&#22312;&#37027;&#37324;&#19981;&#21516;&#30340;&#23398;&#20064;&#30340;&#39069;&#22806;&#27169;&#22359;&#20195;&#34920;&#19981;&#21516;&#30340;&#25216;&#33021;&#12290;&#32467;&#21512;&#29616;&#26377;&#30340;LoRA&#26469;&#35299;&#20915;&#26032;&#20219;&#21153;&#21487;&#20197;&#22686;&#24378;&#23398;&#20064;&#30340;LoRA&#30340;&#21487;&#37325;&#29992;&#24615;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#25968;&#25454;&#26377;&#38480;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#20851;&#20110;LoRA&#32452;&#21512;&#30340;&#24037;&#20316;&#20027;&#35201;&#20381;&#36182;&#20110;&#27599;&#20010;&#28041;&#21450;&#30340;LoRA&#30340;&#20219;&#21153;&#32423;&#21035;&#26435;&#37325;&#65292;&#20351;&#19981;&#21516;&#30340;&#31034;&#20363;&#21644;&#26631;&#35760;&#20849;&#20139;&#30456;&#21516;&#30340;LoRA&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#19981;&#21516;&#30340;&#26631;&#35760;&#21487;&#33021;&#38656;&#35201;&#19981;&#21516;&#30340;&#25216;&#33021;&#26469;&#31649;&#29702;&#12290;&#20197;&#20013;&#25991;&#25968;&#23398;&#20219;&#21153;&#20026;&#20363;&#65292;&#29702;&#35299;&#38382;&#39064;&#25551;&#36848;&#21487;&#33021;&#26356;&#20381;&#36182;&#20110;&#20013;&#25991;LoRA&#65292;&#32780;&#35745;&#31639;&#37096;&#20998;&#21487;&#33021;&#26356;&#20381;&#36182;&#20110;&#25968;&#23398;LoRA&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoRA-Flow&#65292;&#23427;&#21033;&#29992;&#21160;&#24577;&#26435;&#37325;&#26469;&#35843;&#25972;&#19981;&#21516;LoRA&#30340;&#24433;&#21709;&#12290;&#27599;&#20010;&#27493;&#39588;&#30340;&#26435;&#37325;&#30001;&#20855;&#26377;&#26497;&#23569;&#21442;&#25968;&#30340;&#34701;&#21512;&#38376;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11455v1 Announce Type: new  Abstract: LoRA employs lightweight modules to customize large language models (LLMs) for each downstream task or domain, where different learned additional modules represent diverse skills. Combining existing LoRAs to address new tasks can enhance the reusability of learned LoRAs, particularly beneficial for tasks with limited annotated data. Most prior works on LoRA combination primarily rely on task-level weights for each involved LoRA, making different examples and tokens share the same LoRA weights. However, in generative tasks, different tokens may necessitate diverse skills to manage. Taking the Chinese math task as an example, understanding the problem description may depend more on the Chinese LoRA, while the calculation part may rely more on the math LoRA. To this end, we propose LoRA-Flow, which utilizes dynamic weights to adjust the impact of different LoRAs. The weights at each step are determined by a fusion gate with extremely few pa
&lt;/p&gt;</description></item><item><title>MatPlotAgent&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#33258;&#21160;&#21270;&#31185;&#23398;&#25968;&#25454;&#21487;&#35270;&#21270;&#26694;&#26550;&#65292;&#21253;&#25324;&#26597;&#35810;&#29702;&#35299;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#35270;&#35273;&#21453;&#39304;&#26426;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;MatPlotBench&#22522;&#20934;&#21644;GPT-4V&#35780;&#20998;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11453</link><description>&lt;p&gt;
MatPlotAgent: &#22522;&#20110;LLM&#30340;Agent&#31185;&#23398;&#25968;&#25454;&#21487;&#35270;&#21270;&#26041;&#27861;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11453
&lt;/p&gt;
&lt;p&gt;
MatPlotAgent&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#33258;&#21160;&#21270;&#31185;&#23398;&#25968;&#25454;&#21487;&#35270;&#21270;&#26694;&#26550;&#65292;&#21253;&#25324;&#26597;&#35810;&#29702;&#35299;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#35270;&#35273;&#21453;&#39304;&#26426;&#21046;&#65292;&#24182;&#24341;&#20837;&#20102;MatPlotBench&#22522;&#20934;&#21644;GPT-4V&#35780;&#20998;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25968;&#25454;&#21487;&#35270;&#21270;&#36890;&#36807;&#30452;&#25509;&#23637;&#31034;&#22797;&#26434;&#20449;&#24687;&#24182;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#35782;&#21035;&#38544;&#21547;&#27169;&#24335;&#65292;&#22312;&#30740;&#31350;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#20294;&#23545;&#20110;&#20351;&#29992;Large Language Models&#65288;LLMs&#65289;&#36827;&#34892;&#31185;&#23398;&#25968;&#25454;&#21487;&#35270;&#21270;&#30340;&#30740;&#31350;&#20173;&#36739;&#20026;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MatPlotAgent&#65292;&#19968;&#31181;&#39640;&#25928;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;LLM&#20195;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#31185;&#23398;&#25968;&#25454;&#21487;&#35270;&#21270;&#20219;&#21153;&#12290;MatPlotAgent&#21033;&#29992;&#20195;&#30721;LLMs&#21644;&#22810;&#27169;&#24577;LLMs&#30340;&#33021;&#21147;&#65292;&#30001;&#19977;&#20010;&#26680;&#24515;&#27169;&#22359;&#32452;&#25104;&#65306;&#26597;&#35810;&#29702;&#35299;&#12289;&#24102;&#26377;&#36845;&#20195;&#35843;&#35797;&#30340;&#20195;&#30721;&#29983;&#25104;&#65292;&#20197;&#21450;&#29992;&#20110;&#38169;&#35823;&#26356;&#27491;&#30340;&#35270;&#35273;&#21453;&#39304;&#26426;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#35813;&#39046;&#22495;&#32570;&#20047;&#22522;&#20934;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MatPlotBench&#65292;&#19968;&#20010;&#30001;100&#20010;&#32463;&#20154;&#24037;&#39564;&#35777;&#30340;&#27979;&#35797;&#26696;&#20363;&#32452;&#25104;&#30340;&#39640;&#36136;&#37327;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;GPT-4V&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#30340;&#35780;&#20998;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#8230;&#65288;&#26410;&#23436;&#25972;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11453v1 Announce Type: new  Abstract: Scientific data visualization plays a crucial role in research by enabling the direct display of complex information and assisting researchers in identifying implicit patterns. Despite its importance, the use of Large Language Models (LLMs) for scientific data visualization remains rather unexplored. In this study, we introduce MatPlotAgent, an efficient model-agnostic LLM agent framework designed to automate scientific data visualization tasks. Leveraging the capabilities of both code LLMs and multi-modal LLMs, MatPlotAgent consists of three core modules: query understanding, code generation with iterative debugging, and a visual feedback mechanism for error correction. To address the lack of benchmarks in this field, we present MatPlotBench, a high-quality benchmark consisting of 100 human-verified test cases. Additionally, we introduce a scoring approach that utilizes GPT-4V for automatic evaluation. Experimental results demonstrate t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoPRM&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#21487;&#25511;&#38382;&#39064;&#20998;&#35299;&#21644;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#23545;&#22797;&#26434;&#25512;&#29702;&#25361;&#25112;&#30340;&#33258;&#21160;&#21270;&#30417;&#30563;&#21644;&#25913;&#36827;</title><link>https://arxiv.org/abs/2402.11452</link><description>&lt;p&gt;
AutoPRM: &#36890;&#36807;&#21487;&#25511;&#38382;&#39064;&#20998;&#35299;&#33258;&#21160;&#21270;&#22810;&#27493;&#25512;&#29702;&#30340;&#36807;&#31243;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11452
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoPRM&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#36890;&#36807;&#21487;&#25511;&#38382;&#39064;&#20998;&#35299;&#21644;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#23545;&#22797;&#26434;&#25512;&#29702;&#25361;&#25112;&#30340;&#33258;&#21160;&#21270;&#30417;&#30563;&#21644;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#31034;&#20102;&#22312;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#24191;&#27867;&#30340;&#25163;&#21160;&#26631;&#27880;&#26469;&#25552;&#20379;&#31243;&#24207;&#21453;&#39304;&#30340;&#20381;&#36182;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#26694;&#26550;AutoPRM&#65292;&#23427;&#26377;&#25928;&#22320;&#22686;&#24378;&#20102;LLMs&#23545;&#22797;&#26434;&#25512;&#29702;&#25361;&#25112;&#30340;&#24494;&#35843;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;AutoPRM&#39318;&#20808;&#36890;&#36807;&#21487;&#25511;&#31890;&#24230;&#24320;&#20851;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#25104;&#26356;&#26131;&#31649;&#29702;&#30340;&#23376;&#38382;&#39064;&#65292;&#28982;&#21518;&#39034;&#24207;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#36845;&#20195;&#25913;&#36827;&#23376;&#38382;&#39064;&#35299;&#20915;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19978;&#19979;&#25991;&#24341;&#23548;&#35299;&#30721;&#26469;&#36991;&#20813;&#22870;&#21169;&#31713;&#25913;&#24182;&#24341;&#23548;&#23376;&#38382;&#39064;&#35299;&#20915;&#22120;&#26397;&#21521;&#25972;&#20307;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;AutoPRM&#22312;&#25968;&#23398;&#21644;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;SOTA&#12290;&#26356;&#20196;&#20154;&#40723;&#33310;&#30340;&#26159;&#65292;AutoPRM&#21487;&#20197;&#36731;&#26494;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11452v1 Announce Type: new  Abstract: Recent advancements in large language models (LLMs) have shown promise in multi-step reasoning tasks, yet their reliance on extensive manual labeling to provide procedural feedback remains a significant impediment. To address this challenge, in this paper, we propose a novel self-supervised framework AutoPRM that efficiently enhances the fine-tuning of LLMs for intricate reasoning challenges. Specifically, AutoPRM first decomposes complex problems into more manageable subquestions with a controllable granularity switch, then sequentially apply reinforcement learning to iteratively improve the subquestion solver. Additionally, we propose context-guided-decoding to avoid reward tampering and guide the subquestion solver towards the solution of the holistic problem. Extensive experiments show that AutoPRM significantly improves performance on mathematical and commonsense reasoning tasks over SOTA. More encouragingly, AutoPRM can be easily i
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#24037;&#20855;&#22686;&#24378;&#22411;&#31185;&#23398;&#25512;&#29702;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#65292;&#36890;&#36807;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;&#38598;&#65292;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#21464;&#24471;&#26356;&#21152;&#23454;&#29992;&#21644;&#21487;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2402.11451</link><description>&lt;p&gt;
SciAgent: &#24037;&#20855;&#22686;&#24378;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#31185;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SciAgent: Tool-augmented Language Models for Scientific Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11451
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#24037;&#20855;&#22686;&#24378;&#22411;&#31185;&#23398;&#25512;&#29702;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#65292;&#36890;&#36807;&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;&#38598;&#65292;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#21464;&#24471;&#26356;&#21152;&#23454;&#29992;&#21644;&#21487;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25512;&#29702;&#23545;&#20110;&#21363;&#20351;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35828;&#20063;&#26159;&#19968;&#39033;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#20351;LLMs&#26356;&#21152;&#23454;&#29992;&#21644;&#21487;&#35299;&#20915;&#27492;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#24037;&#20855;&#22686;&#24378;&#22411;&#31185;&#23398;&#25512;&#29702;&#30340;&#26032;&#20219;&#21153;&#35774;&#32622;&#12290;&#36825;&#31181;&#35774;&#32622;&#36890;&#36807;&#20026;LLMs&#25552;&#20379;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;&#38598;&#65292;&#23558;&#37325;&#28857;&#20174;&#36861;&#27714;&#20840;&#30693;&#38382;&#39064;&#27714;&#35299;&#22120;&#36716;&#21464;&#20026;&#29087;&#32451;&#20351;&#29992;&#24037;&#20855;&#30340;&#20154;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#31181;&#35774;&#32622;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;MathFunc&#30340;&#24037;&#20855;&#22686;&#24378;&#22411;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;&#36229;&#36807;30,000&#20010;&#26679;&#26412;&#21644;&#22823;&#32422;6,000&#20010;&#24037;&#20855;&#12290;&#22522;&#20110;MathFunc&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SciAgent&#65292;&#29992;&#20110;&#26816;&#32034;&#12289;&#29702;&#35299;&#65292;&#20197;&#21450;&#24517;&#35201;&#26102;&#20351;&#29992;&#24037;&#20855;&#36827;&#34892;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SciToolBench&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20116;&#20010;&#31185;&#23398;&#39046;&#22495;&#65292;&#20197;&#35780;&#20272;LLMs&#22312;&#24037;&#20855;&#36741;&#21161;&#19979;&#30340;&#33021;&#21147;&#12290;&#23545;SciToolBench&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;SciAgent&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;SciAgent-Mistral-7B&#36229;&#36807;&#20102;&#20854;&#20182;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11451v1 Announce Type: cross  Abstract: Scientific reasoning poses an excessive challenge for even the most advanced Large Language Models (LLMs). To make this task more practical and solvable for LLMs, we introduce a new task setting named tool-augmented scientific reasoning. This setting supplements LLMs with scalable toolsets, and shifts the focus from pursuing an omniscient problem solver to a proficient tool-user. To facilitate the research of such setting, we construct a tool-augmented training corpus named MathFunc which encompasses over 30,000 samples and roughly 6,000 tools. Building on MathFunc, we develop SciAgent to retrieve, understand and, if necessary, use tools for scientific problem solving. Additionally, we craft a benchmark, SciToolBench, spanning five scientific domains to evaluate LLMs' abilities with tool assistance. Extensive experiments on SciToolBench confirm the effectiveness of SciAgent. Notably, SciAgent-Mistral-7B surpasses other LLMs with the sa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30001;&#27169;&#22411;&#30340;&#27010;&#29575;&#39044;&#27979;&#24341;&#23548;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#25490;&#24207;&#30340;&#21407;&#21017;&#65292;&#20197;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11447</link><description>&lt;p&gt;
&#30001;&#26631;&#31614;&#20998;&#24067;&#24341;&#23548;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
In-Context Example Ordering Guided by Label Distributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30001;&#27169;&#22411;&#30340;&#27010;&#29575;&#39044;&#27979;&#24341;&#23548;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#25490;&#24207;&#30340;&#21407;&#21017;&#65292;&#20197;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20801;&#35768;&#27169;&#22411;&#22312;&#27809;&#26377;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#27979;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;LLMs&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;ICL&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#12290;&#29305;&#21035;&#26159;&#65292;&#20854;&#24615;&#33021;&#23545;&#20110;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#36873;&#25321;&#21644;&#25490;&#24207;&#38750;&#24120;&#25935;&#24863;&#12290;&#32473;&#23450;&#20855;&#26377;&#19981;&#21516;&#25490;&#24207;&#30340;&#30456;&#21516;&#19968;&#32452;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#22312;&#25509;&#36817;&#38543;&#26426;&#21040;&#25509;&#36817;&#26368;&#20808;&#36827;&#20043;&#38388;&#21464;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#25490;&#24207;&#35268;&#23450;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#38382;&#39064;&#35774;&#32622;&#65292;&#23427;&#20204;&#22312;&#23545;&#20219;&#21153;&#24050;&#30693;&#20449;&#24687;&#30340;&#20551;&#35774;&#26041;&#38754;&#26377;&#25152;&#19981;&#21516;&#12290;&#21463;&#21040;&#20174;&#26631;&#31614;&#27604;&#20363;&#20013;&#23398;&#20064;&#30340;&#24819;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#65292;&#29992;&#20110;&#26681;&#25454;&#27169;&#22411;&#30340;&#27010;&#29575;&#39044;&#27979;&#25351;&#23548;&#19978;&#19979;&#25991;&#31034;&#20363;&#25490;&#24207;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#21407;&#21017;&#24212;&#29992;&#20110;&#21313;&#19977;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#20061;&#20010;&#20855;&#26377;&#20174;700M&#21040;13B&#21442;&#25968;&#30340;&#19981;&#21516;&#33258;&#22238;&#24402;LLMs&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11447v1 Announce Type: new  Abstract: By allowing models to predict without task-specific training, in-context learning (ICL) with pretrained LLMs has enormous potential in NLP. However, a number of problems persist in ICL. In particular, its performance is sensitive to the choice and order of in-context examples. Given the same set of in-context examples with different orderings, model performance may vary between near random to near state-of-the-art. In this work, we formulate in-context example ordering as an optimization problem. We examine three problem settings that differ in the assumptions they make about what is known about the task. Inspired by the idea of learning from label proportions, we propose two principles for in-context example ordering guided by model's probability predictions. We apply our proposed principles to thirteen text classification datasets and nine different autoregressive LLMs with 700M to 13B parameters. We demonstrate that our approach outpe
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20934;&#33258;&#25105;&#28436;&#36827;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;Agent&#31995;&#32479;&#25805;&#20316;&#29615;&#22659;&#25110;&#38382;&#39064;&#65292;&#26500;&#24314;&#28436;&#21270;&#23454;&#20363;&#26469;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;LLMs&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#24182;&#25193;&#23637;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#26356;&#20855;&#21487;&#25193;&#23637;&#24615;&#21644;&#31934;&#32454;&#21270;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.11443</link><description>&lt;p&gt;
&#22522;&#20934;&#33258;&#25105;&#28436;&#36827;: &#29992;&#20110;&#21160;&#24577;LLM&#35780;&#20272;&#30340;&#22810;Agent&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11443
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20934;&#33258;&#25105;&#28436;&#36827;&#26694;&#26550;&#65292;&#36890;&#36807;&#22810;Agent&#31995;&#32479;&#25805;&#20316;&#29615;&#22659;&#25110;&#38382;&#39064;&#65292;&#26500;&#24314;&#28436;&#21270;&#23454;&#20363;&#26469;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;LLMs&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#24182;&#25193;&#23637;&#22522;&#20934;&#25968;&#25454;&#38598;&#20197;&#36827;&#34892;&#26356;&#20855;&#21487;&#25193;&#23637;&#24615;&#21644;&#31934;&#32454;&#21270;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20934;&#33258;&#25105;&#28436;&#36827;&#26694;&#26550;&#65292;&#29992;&#20110;&#21160;&#24577;&#35780;&#20272;&#24555;&#36895;&#21457;&#23637;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#26088;&#22312;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#22810;Agent&#31995;&#32479;&#26469;&#25805;&#20316;&#21407;&#22987;&#23454;&#20363;&#30340;&#29615;&#22659;&#25110;&#38382;&#39064;&#65292;&#36890;&#36807;&#37325;&#26500;&#26032;&#30340;&#28436;&#21270;&#23454;&#20363;&#26469;&#25193;&#23637;&#29616;&#26377;&#22522;&#20934;&#65292;&#20197;&#26356;&#20855;&#39640;&#20449;&#24515;&#22320;&#21160;&#24577;&#25193;&#23637;&#29616;&#26377;&#22522;&#20934;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#20855;&#21487;&#25193;&#23637;&#24615;&#12289;&#24378;&#20581;&#24615;&#21644;&#31934;&#32454;&#21270;&#35780;&#20215;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#20845;&#20010;&#37325;&#26500;&#25805;&#20316;&#26469;&#26500;&#24314;&#28436;&#21270;&#23454;&#20363;&#65292;&#27979;&#35797;LLMs&#23545;&#21508;&#31181;&#26597;&#35810;&#12289;&#25968;&#25454;&#22122;&#22768;&#24182;&#25506;&#31350;&#23427;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#23376;&#33021;&#21147;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22235;&#20010;&#20219;&#21153;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#22823;&#22810;&#25968;LLMs&#20013;&#65292;&#38024;&#23545;&#21407;&#22987;&#32467;&#26524;&#34920;&#29616;&#20986;&#26222;&#36941;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#25105;&#20204;&#30340;&#21487;&#25193;&#23637;&#21644;&#24378;&#20581;&#30340;&#35780;&#20272;&#19979;&#65292;&#20197;&#21450;&#25105;&#20204;&#30340;&#31934;&#32454;&#21270;&#35780;&#20272;&#19979;&#65292;&#36825;&#31181;&#19979;&#38477;&#26356;&#20934;&#30830;&#22320;&#21453;&#26144;&#20102;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11443v1 Announce Type: new  Abstract: This paper presents a benchmark self-evolving framework to dynamically evaluate rapidly advancing Large Language Models (LLMs), aiming for a more accurate assessment of their capabilities and limitations. We utilize a multi-agent system to manipulate the context or question of original instances, reframing new evolving instances with high confidence that dynamically extend existing benchmarks. Towards a more scalable, robust and fine-grained evaluation, we implement six reframing operations to construct evolving instances testing LLMs against diverse queries, data noise and probing their problem-solving sub-abilities. With this framework, we extend benchmark datasets of four tasks. Experimental results show a general performance decline in most LLMs against their original results. This decline under our scalable and robust evaluations, alongside our fine-grained evaluation, more accurately reflect models' capabilities. Besides, our frame
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36923;&#36753;&#25903;&#26550;&#25512;&#29702;&#35268;&#21017;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21253;&#21547;&#22522;&#30784;&#21644;&#32452;&#21512;&#35268;&#21017;&#30340;&#25512;&#29702;&#35268;&#21017;&#24211;ULogic&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#36923;&#36753;&#29702;&#35299;&#26041;&#38754;&#19982;&#20154;&#31867;&#34920;&#29616;&#30340;&#26174;&#33879;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#20934;&#30830;&#12289;&#22797;&#26434;&#21644;&#25277;&#35937;&#30340;&#25512;&#29702;&#32467;&#26524;&#26469;&#25552;&#21319;&#19979;&#28216;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.11442</link><description>&lt;p&gt;
&#33021;&#22815;&#19982;&#35268;&#21017;&#36827;&#34892;&#25512;&#29702;&#21527;&#65311;&#36923;&#36753;&#25903;&#26550;&#29992;&#20110;&#21387;&#21147;&#27979;&#35797;&#21644;&#25552;&#21319;LLM
&lt;/p&gt;
&lt;p&gt;
Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11442
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36923;&#36753;&#25903;&#26550;&#25512;&#29702;&#35268;&#21017;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#26500;&#24314;&#21253;&#21547;&#22522;&#30784;&#21644;&#32452;&#21512;&#35268;&#21017;&#30340;&#25512;&#29702;&#35268;&#21017;&#24211;ULogic&#65292;&#25581;&#31034;&#20102;LLMs&#22312;&#36923;&#36753;&#29702;&#35299;&#26041;&#38754;&#19982;&#20154;&#31867;&#34920;&#29616;&#30340;&#26174;&#33879;&#24046;&#36317;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#20934;&#30830;&#12289;&#22797;&#26434;&#21644;&#25277;&#35937;&#30340;&#25512;&#29702;&#32467;&#26524;&#26469;&#25552;&#21319;&#19979;&#28216;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25509;&#36817;&#20154;&#31867;&#34920;&#29616;&#30340;&#25104;&#32489;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20110;&#22522;&#30784;&#25512;&#29702;&#35268;&#21017;&#30340;&#25484;&#25569;&#20173;&#28982;&#19981;&#21450;&#20154;&#31867;&#33021;&#21147;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36923;&#36753;&#25903;&#26550;&#25512;&#29702;&#35268;&#21017;&#29983;&#25104;&#26694;&#26550;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20116;&#20010;&#39046;&#22495;&#20013;&#22522;&#30784;&#21644;&#32452;&#21512;&#35268;&#21017;&#30340;&#25512;&#29702;&#35268;&#21017;&#24211;ULogic&#12290;&#25105;&#20204;&#23545;GPT&#31995;&#21015;&#27169;&#22411;&#22312;&#35268;&#21017;&#23376;&#38598;&#19978;&#30340;&#20998;&#26512;&#25581;&#31034;&#20986;LLMs&#22312;&#36923;&#36753;&#29702;&#35299;&#26041;&#38754;&#19982;&#20154;&#31867;&#34920;&#29616;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#26576;&#20123;&#20559;&#35265;&#27169;&#24335;&#30340;&#32452;&#21512;&#21644;&#32467;&#26500;&#22797;&#26434;&#35268;&#21017;&#20013;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#36825;&#20123;&#35268;&#21017;&#25552;&#28860;&#25104;&#19968;&#20010;&#26356;&#23567;&#35268;&#27169;&#30340;&#25512;&#29702;&#24341;&#25806;&#65292;&#29992;&#20110;&#28789;&#27963;&#22320;&#29983;&#25104;&#35268;&#21017;&#24182;&#22686;&#24378;&#19979;&#28216;&#25512;&#29702;&#12290;&#36890;&#36807;&#22810;&#35780;&#20272;&#20154;&#21592;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#25512;&#29702;&#24341;&#25806;&#35777;&#26126;&#22312;&#29983;&#25104;&#20934;&#30830;&#12289;&#22797;&#26434;&#21644;&#25277;&#35937;&#30340;&#32467;&#35770;&#21644;&#21069;&#25552;&#26041;&#38754;&#34920;&#29616;&#20986;&#25928;&#26524;&#65292;&#21487;&#20197;&#25913;&#21892;&#21508;&#31181;&#24120;&#35782;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11442v1 Announce Type: new  Abstract: Large language models (LLMs) have achieved impressive human-like performance across various reasoning tasks. However, their mastery of underlying inferential rules still falls short of human capabilities. To investigate this, we propose a logic scaffolding inferential rule generation framework, to construct an inferential rule base, ULogic, comprising both primitive and compositional rules across five domains. Our analysis of GPT-series models over a rule subset reveals significant gaps in LLMs' logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns. We further distill these rules into a smaller-scale inference engine for flexible rule generation and enhancing downstream reasoning. Through a multi-judger evaluation, our inference engine proves effective in generating accurate, complex and abstract conclusions and premises, and improve various commonsense reas
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Infuser-Guided Knowledge Integration&#65288;InfuserKI&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;transformer&#20869;&#37096;&#29366;&#24577;&#26377;&#25928;&#22320;&#23558;&#26410;&#30693;&#30693;&#35782;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#32531;&#35299;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11441</link><description>&lt;p&gt;
InfuserKI&#65306;&#36890;&#36807;Infuser&#24341;&#23548;&#30340;&#30693;&#35782;&#38598;&#25104;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11441
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Infuser-Guided Knowledge Integration&#65288;InfuserKI&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;transformer&#20869;&#37096;&#29366;&#24577;&#26377;&#25928;&#22320;&#23558;&#26410;&#30693;&#30693;&#35782;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#32531;&#35299;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#30693;&#35782;&#38598;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#27169;&#22359;&#23558;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#22270;&#35889;&#19982;LLMs&#32467;&#21512;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#25968;&#25454;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#30693;&#35782;&#26469;&#36827;&#34892;&#24494;&#35843;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#22312;&#19981;&#37325;&#22797;&#24050;&#30693;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23558;&#26410;&#30693;&#30693;&#35782;&#38598;&#25104;&#21040;LLMs&#20013;&#12290;&#27880;&#20837;&#26032;&#30693;&#35782;&#20250;&#23548;&#33268;&#36951;&#24536;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Infuser-Guided Knowledge Integration&#65288;InfuserKI&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;transformer&#20869;&#37096;&#29366;&#24577;&#26469;&#30830;&#23450;&#26159;&#21542;&#24212;&#35813;&#22686;&#24378;&#21407;&#22987;LLM&#36755;&#20986;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#36731;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;&#22312;UMLS-2.5k&#21644;MetaQ&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11441v1 Announce Type: cross  Abstract: Though Large Language Models (LLMs) have shown remarkable open-generation capabilities across diverse domains, they struggle with knowledge-intensive tasks. To alleviate this issue, knowledge integration methods have been proposed to enhance LLMs with domain-specific knowledge graphs using external modules. However, they suffer from data inefficiency as they require both known and unknown knowledge for fine-tuning. Thus, we study a novel problem of integrating unknown knowledge into LLMs efficiently without unnecessary overlap of known knowledge. Injecting new knowledge poses the risk of forgetting previously acquired knowledge. To tackle this, we propose a novel Infuser-Guided Knowledge Integration (InfuserKI) framework that utilizes transformer internal states to determine whether to enhance the original LLM output with additional information, thereby effectively mitigating knowledge forgetting. Evaluations on the UMLS-2.5k and MetaQ
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#33258;&#25105;&#20559;&#35265;&#65292;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;&#20934;&#30830;&#35780;&#20272;&#30340;&#22806;&#37096;&#21453;&#39304;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#65292;&#24182;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#23454;&#38469;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11436</link><description>&lt;p&gt;
&#33258;&#25105;&#21453;&#39304;&#30340;&#21361;&#38505;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#25105;&#20559;&#35265;&#34987;&#25918;&#22823;
&lt;/p&gt;
&lt;p&gt;
Perils of Self-Feedback: Self-Bias Amplifies in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11436
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#33258;&#25105;&#20559;&#35265;&#65292;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;&#20934;&#30830;&#35780;&#20272;&#30340;&#22806;&#37096;&#21453;&#39304;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#36825;&#31181;&#20559;&#35265;&#65292;&#24182;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#30340;&#23454;&#38469;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#25105;&#21453;&#39304;&#21487;&#20197;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#21364;&#20250;&#24694;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#30683;&#30462;&#26159;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#33258;&#36523;&#36755;&#20986;&#30340;&#20559;&#35265;&#12290;&#26412;&#25991;&#27491;&#24335;&#23450;&#20041;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#20559;&#35265;&#8212;&#8212;&#20542;&#21521;&#20110;&#20559;&#29233;&#33258;&#36523;&#29983;&#25104;&#8212;&#8212;&#24182;&#20351;&#29992;&#20004;&#20010;&#32479;&#35745;&#37327;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#32763;&#35793;&#12289;&#21463;&#38480;&#25991;&#26412;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#19978;&#20998;&#26512;&#20102;&#20845;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#33258;&#25105;&#20559;&#35265;&#22312;&#25152;&#26377;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37117;&#26222;&#36941;&#23384;&#22312;&#65292;&#36328;&#22810;&#31181;&#35821;&#35328;&#21644;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#34429;&#28982;&#33258;&#25105;&#25913;&#36827;&#31649;&#36947;&#25552;&#39640;&#20102;&#27169;&#22411;&#36755;&#20986;&#30340;&#27969;&#30021;&#24615;&#21644;&#21487;&#29702;&#35299;&#24615;&#65292;&#20294;&#23427;&#36827;&#19968;&#27493;&#25918;&#22823;&#20102;&#33258;&#25105;&#20559;&#35265;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#65292;&#25105;&#20204;&#21457;&#29616;&#26356;&#22823;&#30340;&#27169;&#22411;&#35268;&#27169;&#21644;&#20855;&#26377;&#20934;&#30830;&#35780;&#20272;&#30340;&#22806;&#37096;&#21453;&#39304;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#33258;&#25105;&#25913;&#36827;&#31649;&#36947;&#20013;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#38469;&#25913;&#21892;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11436v1 Announce Type: cross  Abstract: Recent studies show that self-feedback improves large language models (LLMs) on certain tasks while worsens other tasks. We discovered that such a contrary is due to LLM's bias towards their own output. In this paper, we formally define LLM's self-bias -- the tendency to favor its own generation -- using two statistics. We analyze six LLMs on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#65292;&#21033;&#29992; GPT-4 &#27169;&#25311;&#23244;&#30097;&#20154;&#19982;&#35686;&#23448;&#20043;&#38388;&#30340;&#35282;&#33394;&#25198;&#28436;&#65292;&#20197;&#35299;&#20915;&#27450;&#39575;&#26816;&#27979;&#39046;&#22495;&#38754;&#20020;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#23558;&#20256;&#32479;&#30340;&#27450;&#39575;&#26816;&#27979;&#20219;&#21153;&#25299;&#23637;&#21040;&#27450;&#39575;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.11432</link><description>&lt;p&gt;
&#27450;&#39575;&#26816;&#27979;&#33021;&#22815;&#26356;&#28145;&#20837;&#21527;&#65311;&#29992;&#20110;&#27450;&#39575;&#25512;&#29702;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Can Deception Detection Go Deeper? Dataset, Evaluation, and Benchmark for Deception Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#65292;&#21033;&#29992; GPT-4 &#27169;&#25311;&#23244;&#30097;&#20154;&#19982;&#35686;&#23448;&#20043;&#38388;&#30340;&#35282;&#33394;&#25198;&#28436;&#65292;&#20197;&#35299;&#20915;&#27450;&#39575;&#26816;&#27979;&#39046;&#22495;&#38754;&#20020;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#23558;&#20256;&#32479;&#30340;&#27450;&#39575;&#26816;&#27979;&#20219;&#21153;&#25299;&#23637;&#21040;&#27450;&#39575;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11432v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#30001;&#20110;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#27450;&#39575;&#26816;&#27979;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#30446;&#21069;&#65292;&#25968;&#25454;&#31232;&#32570;&#38459;&#30861;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#19968;&#26041;&#38754;&#65292;&#38599;&#20323;&#21442;&#19982;&#32773;&#27169;&#25311;&#27450;&#39575;&#22330;&#26223;&#25104;&#26412;&#39640;&#26114;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24456;&#38590;&#22312;&#20114;&#32852;&#32593;&#19978;&#25910;&#38598;&#21253;&#21547;&#27450;&#39575;&#34892;&#20026;&#30340;&#35270;&#39057;&#12290;&#20026;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992; GPT-4 &#27169;&#25311;&#20102;&#23244;&#30097;&#20154;&#21644;&#35686;&#23448;&#20043;&#38388;&#30340;&#35282;&#33394;&#25198;&#28436;&#12290;&#22312;&#23457;&#35759;&#36807;&#31243;&#20013;&#65292;&#23244;&#30097;&#20154;&#21521;&#35686;&#23448;&#25746;&#35854;&#65292;&#35797;&#22270;&#36867;&#36991;&#29359;&#32618;&#36131;&#20219;&#65292;&#32780;&#35686;&#23448;&#25581;&#38706;&#20102;&#20107;&#23454;&#24182;&#25910;&#38598;&#20102;&#35777;&#25454;&#12290;&#19982;&#20808;&#21069;&#30340;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#36825;&#19968;&#31574;&#30053;&#20943;&#23569;&#20102;&#25968;&#25454;&#25910;&#38598;&#25104;&#26412;&#65292;&#20026;&#22686;&#21152;&#25968;&#25454;&#38598;&#35268;&#27169;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23558;&#20256;&#32479;&#30340;&#27450;&#39575;&#26816;&#27979;&#20219;&#21153;&#25193;&#23637;&#21040;&#27450;&#39575;&#25512;&#29702;&#65292;&#36827;&#19968;&#27493;&#20026;&#27450;&#39575;&#34892;&#20026;&#25552;&#20379;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11432v1 Announce Type: new  Abstract: Deception detection has attracted increasing attention due to its importance in many practical scenarios. Currently, data scarcity harms the development of this field. On the one hand, it is costly to hire participants to simulate deception scenarios. On the other hand, it is difficult to collect videos containing deceptive behaviors on the Internet. To address data scarcity, this paper proposes a new data collection pipeline. Specifically, we use GPT-4 to simulate a role-play between a suspect and a police officer. During interrogation, the suspect lies to the police officer to evade responsibility for the crime, while the police officer uncovers the truth and gathers evidence. Compared with previous datasets, this strategy reduces data collection costs, providing a promising way to increase the dataset size. Meanwhile, we extend the traditional deception detection task to deception reasoning, further providing evidence for deceptive pa
&lt;/p&gt;</description></item><item><title>EventRL&#36890;&#36807;&#32467;&#26524;&#30417;&#30563;&#21644;&#29305;&#23450;&#22870;&#21169;&#20989;&#25968;&#26377;&#25928;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#20214;&#25552;&#21462;&#25928;&#26524;&#65292;&#23588;&#20854;&#22312;&#22788;&#29702;&#26032;&#22411;&#20107;&#20214;&#31867;&#22411;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.11430</link><description>&lt;p&gt;
&#20351;&#29992;&#32467;&#26524;&#30417;&#30563;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#20214;&#25552;&#21462;&#30340;EventRL&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EventRL: Enhancing Event Extraction with Outcome Supervision for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11430
&lt;/p&gt;
&lt;p&gt;
EventRL&#36890;&#36807;&#32467;&#26524;&#30417;&#30563;&#21644;&#29305;&#23450;&#22870;&#21169;&#20989;&#25968;&#26377;&#25928;&#25913;&#21892;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#20214;&#25552;&#21462;&#25928;&#26524;&#65292;&#23588;&#20854;&#22312;&#22788;&#29702;&#26032;&#22411;&#20107;&#20214;&#31867;&#22411;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EventRL&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20107;&#20214;&#25552;&#21462;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;EventRL&#21033;&#29992;&#29305;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#30340;&#32467;&#26524;&#30417;&#30563;&#26469;&#35299;&#20915;LLMs&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#36981;&#24490;&#25351;&#20196;&#21644;&#20135;&#29983;&#24187;&#35273;&#65292;&#36825;&#34920;&#29616;&#20026;&#20107;&#20214;&#32467;&#26500;&#19981;&#21305;&#37197;&#21644;&#29983;&#25104;&#26410;&#23450;&#20041;&#20107;&#20214;&#31867;&#22411;&#12290;&#25105;&#20204;&#23545;EventRL&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#22914;Few-Shot Prompting&#65288;FSP&#65289;&#65288;&#22522;&#20110;GPT4&#65289;&#21644;Supervised Fine-Tuning&#65288;SFT&#65289;&#36827;&#34892;&#23545;&#27604;&#65292;&#28085;&#30422;&#20102;&#21253;&#25324;GPT-4&#12289;LLaMa&#21644;CodeLLaMa&#27169;&#22411;&#22312;&#20869;&#30340;&#21508;&#31181;LLMs&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;EventRL&#22312;&#35782;&#21035;&#21644;&#32467;&#26500;&#21270;&#20107;&#20214;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#36825;&#20123;&#20256;&#32479;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#26032;&#39062;&#20107;&#20214;&#31867;&#22411;&#26041;&#38754;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#22870;&#21169;&#20989;&#25968;&#36873;&#25321;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#23637;&#31034;&#20102;&#23558;&#20195;&#30721;&#25968;&#25454;&#32435;&#20837;&#20197;&#33719;&#24471;&#26356;&#22909;&#20107;&#20214;&#25552;&#21462;&#25928;&#26524;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11430v1 Announce Type: new  Abstract: In this study, we present EventRL, a reinforcement learning approach developed to enhance event extraction for large language models (LLMs). EventRL utilizes outcome supervision with specific reward functions to tackle prevalent challenges in LLMs, such as instruction following and hallucination, manifested as the mismatch of event structure and the generation of undefined event types. We evaluate EventRL against existing methods like Few-Shot Prompting (FSP) (based on GPT4) and Supervised Fine-Tuning (SFT) across various LLMs, including GPT-4, LLaMa, and CodeLLaMa models. Our findings show that EventRL significantly outperforms these conventional approaches by improving the performance in identifying and structuring events, particularly in handling novel event types. The study emphasizes the critical role of reward function selection and demonstrates the benefits of incorporating code data for better event extraction. While increasing m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#22810;&#38454;&#27573;&#30693;&#35782;&#36801;&#31227;&#26694;&#26550;&#65292;&#36890;&#36807;&#19981;&#26029;&#21457;&#23637;&#30340;&#25945;&#24072;&#27169;&#22411;&#22312;&#22810;&#39046;&#22495;&#20013;&#38450;&#27490;CSC&#27169;&#22411;&#24536;&#35760;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11422</link><description>&lt;p&gt;
&#22810;&#38454;&#27573;&#30693;&#35782;&#36801;&#31227;&#26694;&#26550;&#22312;&#22810;&#39046;&#22495;&#20013;&#25991;&#25340;&#20889;&#26657;&#27491;&#20013;&#30340;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Mitigating Catastrophic Forgetting in Multi-domain Chinese Spelling Correction by Multi-stage Knowledge Transfer Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11422
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#22810;&#38454;&#27573;&#30693;&#35782;&#36801;&#31227;&#26694;&#26550;&#65292;&#36890;&#36807;&#19981;&#26029;&#21457;&#23637;&#30340;&#25945;&#24072;&#27169;&#22411;&#22312;&#22810;&#39046;&#22495;&#20013;&#38450;&#27490;CSC&#27169;&#22411;&#24536;&#35760;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#22269;&#20250;&#25340;&#20889;&#26657;&#27491;&#65288;CSC&#65289;&#26088;&#22312;&#26816;&#27979;&#21644;&#32416;&#27491;&#32473;&#23450;&#21477;&#23376;&#20013;&#30340;&#25340;&#20889;&#38169;&#35823;&#12290;&#26368;&#36817;&#65292;&#22810;&#39046;&#22495;CSC&#36880;&#28176;&#24341;&#36215;&#30740;&#31350;&#32773;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#26356;&#21152;&#23454;&#29992;&#12290;&#26412;&#25991;&#20851;&#27880;CSC&#27169;&#22411;&#22312;&#36866;&#24212;&#22810;&#39046;&#22495;&#22330;&#26223;&#26102;&#23384;&#22312;&#30340;&#20851;&#38190;&#32570;&#38519;&#65306;&#23398;&#20064;&#26032;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26102;&#23481;&#26131;&#24536;&#35760;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#65288;&#21363;&#28798;&#38590;&#24615;&#36951;&#24536;&#65289;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#22810;&#38454;&#27573;&#30693;&#35782;&#36801;&#31227;&#65288;MKT&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#27599;&#20010;&#39046;&#22495;&#20013;&#21033;&#29992;&#19981;&#26029;&#21457;&#23637;&#30340;&#25945;&#24072;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#36801;&#31227;&#65292;&#32780;&#19981;&#20165;&#20165;&#19987;&#27880;&#20110;&#26032;&#39046;&#22495;&#30693;&#35782;&#12290;&#20540;&#24471;&#19968;&#25552;&#30340;&#26159;&#65292;&#25105;&#20204;&#26159;&#39318;&#27425;&#23558;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#22810;&#39046;&#22495;CSC&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#36827;&#19968;&#27493;&#20998;&#26512;&#23637;&#31034;&#20102;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#23545;&#20110;&#25552;&#39640;CSC&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11422v1 Announce Type: new  Abstract: Chinese Spelling Correction (CSC) aims to detect and correct spelling errors in given sentences. Recently, multi-domain CSC has gradually attracted the attention of researchers because it is more practicable. In this paper, we focus on the key flaw of the CSC model when adapting to multi-domain scenarios: the tendency to forget previously acquired knowledge upon learning new domain-specific knowledge (i.e., catastrophic forgetting). To address this, we propose a novel model-agnostic Multi-stage Knowledge Transfer (MKT) framework, which utilizes a continuously evolving teacher model for knowledge transfer in each domain, rather than focusing solely on new domain knowledge. It deserves to be mentioned that we are the first to apply continual learning methods to the multi-domain CSC task. Experiments prove the effectiveness of our proposed method, and further analyses demonstrate the importance of overcoming catastrophic forgetting for impr
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#24605;&#32771;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#20013;&#30340;&#20316;&#29992;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#35299;&#37322;&#22120;&#25552;&#20379;&#35299;&#37322;&#20449;&#24687;&#24182;&#20316;&#20026;&#35780;&#20272;&#22120;&#24102;&#26469;&#26356;&#21512;&#29702;&#30340;CGEC&#35780;&#20272;&#20197;&#22686;&#24378;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.11420</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Roles of Large Language Models in Chinese Grammatical Error Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11420
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#20013;&#30340;&#20316;&#29992;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#35299;&#37322;&#22120;&#25552;&#20379;&#35299;&#37322;&#20449;&#24687;&#24182;&#20316;&#20026;&#35780;&#20272;&#22120;&#24102;&#26469;&#26356;&#21512;&#29702;&#30340;CGEC&#35780;&#20272;&#20197;&#22686;&#24378;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;NLP&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#12290;&#20316;&#20026;NLP&#39046;&#22495;&#30340;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#65292;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#65288;CGEC&#65289;&#26088;&#22312;&#32416;&#27491;&#36755;&#20837;&#21477;&#23376;&#20013;&#30340;&#25152;&#26377;&#28508;&#22312;&#35821;&#27861;&#38169;&#35823;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#37325;&#28857;&#65292;LLMs&#20316;&#20026;CGEC&#26657;&#27491;&#22120;&#30340;&#24615;&#33021;&#20173;&#28982;&#20196;&#20154;&#19981;&#28385;&#12290;&#20026;&#20102;&#25512;&#21160;CGEC&#39046;&#22495;&#26356;&#22909;&#22320;&#36866;&#24212;LLMs&#26102;&#20195;&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;LLMs&#22312;CGEC&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;CGEC&#20013;&#24471;&#21040;&#26356;&#22909;&#30340;&#21033;&#29992;&#21644;&#25506;&#32034;&#12290;&#32771;&#34385;&#21040;LLMs&#20013;&#23384;&#20648;&#30340;&#20016;&#23500;&#35821;&#27861;&#30693;&#35782;&#21644;&#20854;&#24378;&#22823;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#65292;&#25105;&#20204;&#21033;&#29992;LLMs&#20316;&#20026;&#35299;&#37322;&#22120;&#65292;&#20026;CGEC&#23567;&#27169;&#22411;&#25552;&#20379;&#35299;&#37322;&#20449;&#24687;&#65292;&#20197;&#22686;&#24378;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23558;LLMs&#29992;&#20316;&#35780;&#20272;&#22120;&#65292;&#24102;&#26469;&#26356;&#21512;&#29702;&#30340;CGEC&#35780;&#20272;&#65292;&#20174;&#32780;&#32531;&#35299;&#30001;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11420v1 Announce Type: new  Abstract: Recently, Large Language Models (LLMs) have been widely studied by researchers for their roles in various downstream NLP tasks. As a fundamental task in the NLP field, Chinese Grammatical Error Correction (CGEC) aims to correct all potential grammatical errors in the input sentences. Previous studies have shown that LLMs' performance as correctors on CGEC remains unsatisfactory due to its challenging task focus. To promote the CGEC field to better adapt to the era of LLMs, we rethink the roles of LLMs in the CGEC task so that they can be better utilized and explored in CGEC. Considering the rich grammatical knowledge stored in LLMs and their powerful semantic understanding capabilities, we utilize LLMs as explainers to provide explanation information for the CGEC small models during error correction to enhance performance. We also use LLMs as evaluators to bring more reasonable CGEC evaluations, thus alleviating the troubles caused by th
&lt;/p&gt;</description></item><item><title>LoRETTA&#26159;&#19968;&#20010;&#36890;&#36807;&#24352;&#37327;&#35757;&#32451;&#20998;&#35299;&#26174;&#33879;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#36229;&#20302;&#21442;&#25968;&#39640;&#25928;&#26694;&#26550;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#20013;&#34920;&#29616;&#20986;&#19982;&#22823;&#22810;&#25968;PEFT&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11417</link><description>&lt;p&gt;
LoRETTA: &#20302;&#31209;&#32463;&#27982;&#24352;&#37327;&#35757;&#32451;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36229;&#20302;&#21442;&#25968;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LoRETTA: Low-Rank Economic Tensor-Train Adaptation for Ultra-Low-Parameter Fine-Tuning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11417
&lt;/p&gt;
&lt;p&gt;
LoRETTA&#26159;&#19968;&#20010;&#36890;&#36807;&#24352;&#37327;&#35757;&#32451;&#20998;&#35299;&#26174;&#33879;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#36229;&#20302;&#21442;&#25968;&#39640;&#25928;&#26694;&#26550;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#20013;&#34920;&#29616;&#20986;&#19982;&#22823;&#22810;&#25968;PEFT&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#34987;&#25552;&#20986;&#20197;&#23454;&#29616;&#22312;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35745;&#31639;&#39640;&#25928;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#37096;&#32626;&#65292;&#29616;&#26377;&#30340;PEFT&#26041;&#27861;&#20173;&#28982;&#21463;&#21040;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#22686;&#38271;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoRETTA&#65292;&#36825;&#26159;&#19968;&#20010;&#36229;&#21442;&#25968;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24352;&#37327;&#35757;&#32451;&#20998;&#35299;&#26174;&#33879;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#20998;&#21035;&#21629;&#21517;&#20026;{LoRETTA}$_{adp}$&#21644;{LoRETTA}$_{rep}$&#12290;&#21069;&#32773;&#37319;&#29992;&#24352;&#37327;&#21270;&#36866;&#37197;&#22120;&#65292;&#20026;LLMs&#30340;&#24494;&#35843;&#25552;&#20379;&#20102;&#39640;&#24615;&#33021;&#19988;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#12290;&#21518;&#32773;&#24378;&#35843;&#36890;&#36807;&#19968;&#32452;&#23567;&#24352;&#37327;&#22240;&#23376;&#36827;&#34892;&#26435;&#37325;&#21442;&#25968;&#21270;&#30340;&#24494;&#35843;&#12290;LoRETTA&#22312;LLaMA-2-7B&#27169;&#22411;&#19978;&#27604;&#22823;&#22810;&#25968;&#24191;&#27867;&#20351;&#29992;&#30340;PEFT&#26041;&#27861;&#20855;&#26377;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21442;&#25968;&#23569;&#36798;&#21040;100&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11417v1 Announce Type: cross  Abstract: Various parameter-efficient fine-tuning (PEFT) techniques have been proposed to enable computationally efficient fine-tuning while maintaining model performance. However, existing PEFT methods are still limited by the growing number of trainable parameters with the rapid deployment of Large Language Models (LLMs). To address this challenge, we present LoRETTA, an ultra-parameter-efficient framework that significantly reduces trainable parameters through tensor-train decomposition. Specifically, we propose two methods, named {LoRETTA}$_{adp}$ and {LoRETTA}$_{rep}$. The former employs tensorized adapters, offering a high-performance yet lightweight approach for the fine-tuning of LLMs. The latter emphasizes fine-tuning via weight parameterization with a set of small tensor factors. LoRETTA achieves comparable or better performance than most widely used PEFT methods with up to $100\times$ fewer parameters on the LLaMA-2-7B models. Further
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20004;&#31181;&#32454;&#31890;&#24230;&#21644;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#25688;&#35201;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#65292;&#20854;&#20013;&#26080;&#21442;&#32771;&#20107;&#23454;&#24615;&#35780;&#20272;&#26694;&#26550;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11414</link><description>&lt;p&gt;
&#29992;&#20110;&#22810;&#27169;&#24577;&#25688;&#35201;&#30340;&#32454;&#31890;&#24230;&#21487;&#35299;&#37322;&#20107;&#23454;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Fine-grained and Explainable Factuality Evaluation for Multimodal Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11414
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20004;&#31181;&#32454;&#31890;&#24230;&#21644;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;&#25688;&#35201;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#65292;&#20854;&#20013;&#26080;&#21442;&#32771;&#20107;&#23454;&#24615;&#35780;&#20272;&#26694;&#26550;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#23454;&#39564;&#35777;&#23454;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25688;&#35201;&#26088;&#22312;&#29983;&#25104;&#22522;&#20110;&#36755;&#20837;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#31616;&#27905;&#25688;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#21487;&#33021;&#23384;&#22312;&#20107;&#23454;&#24615;&#36755;&#20986;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35780;&#20272;&#22810;&#27169;&#24577;&#25688;&#35201;&#27169;&#22411;&#30340;&#20107;&#23454;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#32454;&#31890;&#24230;&#21644;&#21487;&#35299;&#37322;&#30340;&#35780;&#20272;&#26694;&#26550;&#65288;FALLACIOUS&#65289;&#29992;&#20110;&#19981;&#21516;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#21363;&#22522;&#20110;&#21442;&#32771;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#26694;&#26550;&#21644;&#26080;&#21442;&#32771;&#30340;&#20107;&#23454;&#24615;&#35780;&#20272;&#26694;&#26550;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26080;&#21442;&#32771;&#20107;&#23454;&#24615;&#35780;&#20272;&#26694;&#26550;&#19981;&#38656;&#35201;&#22522;&#20934;&#30495;&#20540;&#65292;&#22240;&#27492;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;&#20026;&#20102;&#35780;&#20272;&#25152;&#25552;&#20986;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#35745;&#31639;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#20854;&#20182;&#25351;&#26631;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23558;&#36890;&#36807;GitHub&#21457;&#24067;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11414v1 Announce Type: new  Abstract: Multimodal summarization aims to generate a concise summary based on the input text and image. However, the existing methods potentially suffer from unfactual output. To evaluate the factuality of multimodal summarization models, we propose two fine-grained and explainable evaluation frameworks (FALLACIOUS) for different application scenarios, i.e. reference-based factuality evaluation framework and reference-free factuality evaluation framework. Notably, the reference-free factuality evaluation framework doesn't need ground truth and hence it has a wider application scenario. To evaluate the effectiveness of the proposed frameworks, we compute the correlation between our frameworks and the other metrics. The experimental results show the effectiveness of our proposed method. We will release our code and dataset via github.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#24187;&#35273;&#38382;&#39064;&#35270;&#20026;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20559;&#22909;&#35843;&#25972;&#35299;&#20915;&#65292;&#25552;&#20986;&#20102;POVID&#26041;&#27861;&#26469;&#29983;&#25104;&#21453;&#39304;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.11411</link><description>&lt;p&gt;
&#36890;&#36807;&#20559;&#22909;&#24494;&#35843;&#22312;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#27169;&#24577;
&lt;/p&gt;
&lt;p&gt;
Aligning Modalities in Vision Large Language Models via Preference Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#24187;&#35273;&#38382;&#39064;&#35270;&#20026;&#23545;&#40784;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20559;&#22909;&#35843;&#25972;&#35299;&#20915;&#65292;&#25552;&#20986;&#20102;POVID&#26041;&#27861;&#26469;&#29983;&#25104;&#21453;&#39304;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#31034;&#36319;&#38543;&#30340;&#35270;&#35273;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;VLLMs&#65289;&#26368;&#36817;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#36825;&#20123;&#26041;&#27861;&#21512;&#24182;&#20102;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#30001;&#20110;&#36825;&#20123;&#32452;&#20214;&#26159;&#20998;&#21035;&#35757;&#32451;&#30340;&#65292;&#25152;&#20197;&#38656;&#35201;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#39069;&#22806;&#30340;&#22270;&#20687;-&#35821;&#35328;&#23545;&#26469;&#23545;&#23398;&#20064;&#30340;&#34920;&#31034;&#36827;&#34892;&#23545;&#40784;&#12290;&#36825;&#20010;&#36807;&#31243;&#24182;&#19981;&#23436;&#32654;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;-&#21363;&#20351;&#26680;&#24515;LLM&#38750;&#24120;&#23458;&#35266;&#65292;&#35270;&#35273;&#25903;&#25745;&#20855;&#26377;&#20805;&#20998;&#23436;&#25972;&#30340;&#34920;&#31034;&#65292;&#20063;&#20250;&#25552;&#20379;&#19982;&#22270;&#20687;&#19981;&#31526;&#21512;&#30340;&#31572;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#24187;&#35273;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#23545;&#40784;&#38382;&#39064;&#65292;&#36890;&#36807;&#20559;&#22909;&#35843;&#25972;&#26469;&#35299;&#20915;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;POVID&#26469;&#20135;&#29983;AI&#27169;&#22411;&#30340;&#21453;&#39304;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#22320;&#38754;&#30495;&#23454;&#25351;&#31034;&#20316;&#20026;&#39318;&#36873;&#21709;&#24212;&#65292;&#37319;&#29992;&#20004;&#38454;&#27573;&#26041;&#27861;&#29983;&#25104;&#19981;&#21463;&#27426;&#36814;&#30340;&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#31034;GPT-4V&#27880;&#20837;&#21512;&#29702;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11411v1 Announce Type: cross  Abstract: Instruction-following Vision Large Language Models (VLLMs) have achieved significant progress recently on a variety of tasks. These approaches merge strong pre-trained vision models and large language models (LLMs). Since these components are trained separately, the learned representations need to be aligned with joint training on additional image-language pairs. This procedure is not perfect and can cause the model to hallucinate - provide answers that do not accurately reflect the image, even when the core LLM is highly factual and the vision backbone has sufficiently complete representations. In this work, we frame the hallucination problem as an alignment issue, tackle it with preference tuning. Specifically, we propose POVID to generate feedback data with AI models. We use ground-truth instructions as the preferred response and a two-stage approach to generate dispreferred data. First, we prompt GPT-4V to inject plausible hallucin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#32500;&#24230;&#30340;&#20849;&#24773;&#35780;&#20272;&#26694;&#26550;&#65292;&#21516;&#26102;&#34913;&#37327;&#20102;&#35828;&#35805;&#32773;&#35282;&#24230;&#30340;&#34920;&#36798;&#24847;&#22270;&#21644;&#21548;&#32773;&#35282;&#24230;&#30340;&#24863;&#30693;&#20849;&#24773;&#65292;&#30740;&#31350;&#21457;&#29616;&#20108;&#32773;&#26159;&#30456;&#20114;&#20851;&#32852;&#30340;&#65292;&#24863;&#30693;&#20849;&#24773;&#19982;&#23545;&#35805;&#20250;&#35805;&#30340;&#28385;&#24847;&#27700;&#24179;&#21576;&#39640;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11409</link><description>&lt;p&gt;
&#22810;&#32500;&#24230;&#35780;&#20272;&#20849;&#24773;&#23545;&#35805;&#22238;&#22797;
&lt;/p&gt;
&lt;p&gt;
Multi-dimensional Evaluation of Empathetic Dialog Responses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11409
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#32500;&#24230;&#30340;&#20849;&#24773;&#35780;&#20272;&#26694;&#26550;&#65292;&#21516;&#26102;&#34913;&#37327;&#20102;&#35828;&#35805;&#32773;&#35282;&#24230;&#30340;&#34920;&#36798;&#24847;&#22270;&#21644;&#21548;&#32773;&#35282;&#24230;&#30340;&#24863;&#30693;&#20849;&#24773;&#65292;&#30740;&#31350;&#21457;&#29616;&#20108;&#32773;&#26159;&#30456;&#20114;&#20851;&#32852;&#30340;&#65292;&#24863;&#30693;&#20849;&#24773;&#19982;&#23545;&#35805;&#20250;&#35805;&#30340;&#28385;&#24847;&#27700;&#24179;&#21576;&#39640;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#26159;&#26377;&#25928;&#21644;&#20196;&#20154;&#28385;&#24847;&#30340;&#23545;&#35805;&#27807;&#36890;&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#28982;&#32780;&#20808;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#22312;&#34913;&#37327;&#34920;&#36798;&#30340;&#27807;&#36890;&#24847;&#22270;&#19978;&#8212;&#8212;&#21363;&#20849;&#24773;&#26159;&#22914;&#20309;&#34920;&#36798;&#30340;&#65292;&#24573;&#30053;&#20102;&#23545;&#35805;&#20063;&#26159;&#19968;&#31181;&#28041;&#21450;&#35828;&#35805;&#32773;&#21644;&#32838;&#21548;&#32773;&#30340;&#21327;&#20316;&#23454;&#36341;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#32500;&#24230;&#30340;&#20849;&#24773;&#35780;&#20272;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#24037;&#20316;&#65292;&#20197;&#34913;&#37327;&#20174;&#35828;&#35805;&#32773;&#35282;&#24230;&#34920;&#36798;&#30340;&#24847;&#22270;&#20197;&#21450;&#20174;&#21548;&#32773;&#35282;&#24230;&#24863;&#30693;&#21040;&#30340;&#20849;&#24773;&#12290;&#23558;&#25552;&#20986;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#20998;&#26512;&#25105;&#20204;&#20869;&#37096;&#30340;&#23458;&#25143;&#26381;&#21153;&#23545;&#35805;&#34920;&#26126;&#65292;&#36825;&#20004;&#20010;&#32500;&#24230;&#65288;&#34920;&#36798;&#30340;&#24847;&#22270;&#31867;&#22411;&#21644;&#24863;&#30693;&#21040;&#30340;&#20849;&#24773;&#65289;&#26159;&#30456;&#20114;&#20851;&#32852;&#30340;&#65292;&#32780;&#24863;&#30693;&#21040;&#30340;&#20849;&#24773;&#19982;&#23545;&#35805;&#20250;&#35805;&#30340;&#28385;&#24847;&#27700;&#24179;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#20010;&#25552;&#20986;&#30340;&#26694;&#26550;&#20173;&#38656;&#35201;&#21463;&#36807;&#35757;&#32451;&#30340;&#27880;&#37322;&#21592;&#30340;&#20027;&#35266;&#35780;&#20272;&#65292;&#36825;&#21487;&#33021;&#24182;&#19981;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11409v1 Announce Type: new  Abstract: Empathy is a critical element of effective and satisfactory conversational communication, yet previous studies in measuring conversational empathy mostly focus on expressed communicative intents -- in which way empathy is expressed, ignoring the fact that conversation is also a collaborative practice involving both speakers and listeners. In contrast, we propose a multi-dimensional empathy evaluation framework that extends upon existing work to measure both expressed intents from the speaker's perspective and perceived empathy from the listener's perspective. Applying the proposed framework to analyzing our internal customer-service dialogue shows that the two dimensions (expressed intent types and perceived empathy) are inter-connected, while perceived empathy has high correlation with the satisfactory level of dialogue sessions. This proposed framework still requires subjective assessments from trained annotators, which can be non-triv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#26816;&#27979;&#38544;&#24335;&#20167;&#24680;&#35328;&#35770;&#21644;&#34920;&#36798;&#20449;&#24515;&#26102;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#23384;&#22312;&#20004;&#20010;&#26497;&#31471;&#65306;&#23545;&#21487;&#33021;&#24341;&#36215;&#20844;&#24179;&#38382;&#39064;&#30340;&#32676;&#20307;&#25110;&#35805;&#39064;&#34920;&#29616;&#20986;&#36807;&#20110;&#25935;&#24863;&#65292;&#21516;&#26102;&#32622;&#20449;&#24230;&#35780;&#20998;&#36807;&#24230;&#38598;&#20013;&#22312;&#19968;&#20010;&#33539;&#22260;&#20869;&#12290;</title><link>https://arxiv.org/abs/2402.11406</link><description>&lt;p&gt;
&#19981;&#35201;&#36208;&#21521;&#26497;&#31471;&#65306;&#25581;&#31034;LLMs&#22312;&#38544;&#24335;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#36807;&#24230;&#25935;&#24863;&#24615;&#21644;&#26657;&#20934;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#26816;&#27979;&#38544;&#24335;&#20167;&#24680;&#35328;&#35770;&#21644;&#34920;&#36798;&#20449;&#24515;&#26102;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#23384;&#22312;&#20004;&#20010;&#26497;&#31471;&#65306;&#23545;&#21487;&#33021;&#24341;&#36215;&#20844;&#24179;&#38382;&#39064;&#30340;&#32676;&#20307;&#25110;&#35805;&#39064;&#34920;&#29616;&#20986;&#36807;&#20110;&#25935;&#24863;&#65292;&#21516;&#26102;&#32622;&#20449;&#24230;&#35780;&#20998;&#36807;&#24230;&#38598;&#20013;&#22312;&#19968;&#20010;&#33539;&#22260;&#20869;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20844;&#24179;&#24615;&#21644;&#21487;&#20449;&#24230;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#38544;&#24335;&#20167;&#24680;&#35328;&#35770;&#65292;&#21033;&#29992;&#38388;&#25509;&#35821;&#35328;&#20256;&#36798;&#20167;&#24680;&#24847;&#22270;&#65292;&#21344;&#25454;&#23454;&#36341;&#20013;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;LLMs&#26377;&#25928;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#31243;&#24230;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#23457;&#26597;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#26816;&#27979;&#38544;&#24335;&#20167;&#24680;&#35328;&#35770;&#65288;&#20998;&#31867;&#20219;&#21153;&#65289;&#20197;&#21450;&#23545;&#20854;&#21709;&#24212;&#30340;&#20449;&#24515;&#36827;&#34892;&#34920;&#36798;&#65288;&#26657;&#20934;&#20219;&#21153;&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32454;&#33268;&#32771;&#34385;&#20102;&#21508;&#31181;&#25552;&#31034;&#27169;&#24335;&#21644;&#20027;&#27969;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;LLMs&#23637;&#31034;&#20102;&#20004;&#20010;&#26497;&#31471;&#65306;&#65288;1&#65289;LLMs&#23545;&#21487;&#33021;&#23548;&#33268;&#20844;&#24179;&#24615;&#38382;&#39064;&#30340;&#32676;&#20307;&#25110;&#35805;&#39064;&#26174;&#31034;&#20986;&#36807;&#24230;&#30340;&#25935;&#24863;&#24615;&#65292;&#23548;&#33268;&#23558;&#33391;&#24615;&#38472;&#36848;&#38169;&#35823;&#20998;&#31867;&#20026;&#20167;&#24680;&#35328;&#35770;&#12290; &#65288;2&#65289;LLMs&#23545;&#27599;&#31181;&#26041;&#27861;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#36807;&#24230;&#38598;&#20013;&#22312;&#19968;&#20010;&#22266;&#23450;&#33539;&#22260;&#19978;&#65292;&#26080;&#35770;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#24615;&#22914;&#20309;&#20063;&#20445;&#25345;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11406v1 Announce Type: new  Abstract: The fairness and trustworthiness of Large Language Models (LLMs) are receiving increasing attention. Implicit hate speech, which employs indirect language to convey hateful intentions, occupies a significant portion of practice. However, the extent to which LLMs effectively address this issue remains insufficiently examined. This paper delves into the capability of LLMs to detect implicit hate speech (Classification Task) and express confidence in their responses (Calibration Task). Our evaluation meticulously considers various prompt patterns and mainstream uncertainty estimation methods. Our findings highlight that LLMs exhibit two extremes: (1) LLMs display excessive sensitivity towards groups or topics that may cause fairness issues, resulting in misclassifying benign statements as hate speech. (2) LLMs' confidence scores for each method excessively concentrate on a fixed range, remaining unchanged regardless of the dataset's complex
&lt;/p&gt;</description></item><item><title>k-SemStamp&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35821;&#20041;&#27700;&#21360;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;k&#22343;&#20540;&#32858;&#31867;&#20195;&#26367;LSH&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#25277;&#26679;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#65292;&#20026;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2402.11399</link><description>&lt;p&gt;
k-SemStamp&#65306;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#35821;&#20041;&#27700;&#21360;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11399
&lt;/p&gt;
&lt;p&gt;
k-SemStamp&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35821;&#20041;&#27700;&#21360;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;k&#22343;&#20540;&#32858;&#31867;&#20195;&#26367;LSH&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#21644;&#25277;&#26679;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#65292;&#20026;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25552;&#20379;&#20102;&#26356;&#26377;&#25928;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#27700;&#21360;&#29983;&#25104;&#31639;&#27861;&#22312;&#35821;&#35328;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#21487;&#26816;&#27979;&#30340;&#31614;&#21517;&#65292;&#20197;&#20415;&#36827;&#34892;&#20107;&#21518;&#26816;&#27979;&#12290;&#34429;&#28982;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#30340;&#27700;&#21360;&#23481;&#26131;&#21463;&#21040;&#25913;&#20889;&#25915;&#20987;&#65292;&#20294;SemStamp (Hou&#31561;&#20154;&#65292;2023)&#22312;&#21477;&#23376;&#30340;&#35821;&#20041;&#34920;&#31034;&#19978;&#24212;&#29992;&#27700;&#21360;&#65292;&#24182;&#23637;&#31034;&#20986;&#24456;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;SemStamp&#21033;&#29992;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#65288;LSH&#65289;&#26469;&#21033;&#29992;&#20219;&#24847;&#36229;&#24179;&#38754;&#23545;&#35821;&#20041;&#31354;&#38388;&#36827;&#34892;&#20998;&#21306;&#65292;&#23548;&#33268;&#22312;&#40065;&#26834;&#24615;&#21644;&#36895;&#24230;&#20043;&#38388;&#23384;&#22312;&#27425;&#20248;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;k-SemStamp&#65292;&#36825;&#26159;SemStamp&#30340;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22686;&#24378;&#29256;&#65292;&#21033;&#29992;k&#22343;&#20540;&#32858;&#31867;&#20316;&#20026;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#20197;&#20102;&#35299;&#20869;&#22312;&#30340;&#35821;&#20041;&#32467;&#26500;&#26469;&#20998;&#21306;&#23884;&#20837;&#31354;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;k-SemStamp&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#25277;&#26679;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#65292;&#25512;&#36827;&#20102;&#26356;&#26377;&#25928;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11399v1 Announce Type: new  Abstract: Recent watermarked generation algorithms inject detectable signatures during language generation to facilitate post-hoc detection. While token-level watermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023) applies watermark on the semantic representation of sentences and demonstrates promising robustness. SemStamp employs locality-sensitive hashing (LSH) to partition the semantic space with arbitrary hyperplanes, which results in a suboptimal tradeoff between robustness and speed. We propose k-SemStamp, a simple yet effective enhancement of SemStamp, utilizing k-means clustering as an alternative of LSH to partition the embedding space with awareness of inherent semantic structure. Experimental results indicate that k-SemStamp saliently improves its robustness and sampling efficiency while preserving the generation quality, advancing a more effective tool for machine-generated text detection.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;LLM&#22686;&#24378;&#35821;&#20041;&#20998;&#26512;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;&#25991;&#26412;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26694;&#26550;&#65292;&#21487;&#26174;&#33879;&#25913;&#21892;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#35780;&#20272;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#20854;&#20182;&#19987;&#19994;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.11398</link><description>&lt;p&gt;
&#22312;&#27604;&#36739;&#20043;&#21069;&#36827;&#34892;&#25512;&#29702;&#65306;LLM&#22686;&#24378;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#24230;&#37327;&#29992;&#20110;&#39046;&#22495;&#19987;&#38376;&#25991;&#26412;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reasoning before Comparison: LLM-Enhanced Semantic Similarity Metrics for Domain Specialized Text Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11398
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;LLM&#22686;&#24378;&#35821;&#20041;&#20998;&#26512;&#65292;&#24320;&#21457;&#20102;&#29992;&#20110;&#25991;&#26412;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26694;&#26550;&#65292;&#21487;&#26174;&#33879;&#25913;&#21892;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#35780;&#20272;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#20854;&#20182;&#19987;&#19994;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#26469;&#22686;&#24378;&#35821;&#20041;&#20998;&#26512;&#65292;&#20026;&#25991;&#26412;&#24320;&#21457;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#35299;&#20915;&#20256;&#32479;&#26080;&#30417;&#30563;NLP&#24230;&#37327;&#65288;&#22914;ROUGE&#21644;BLEU&#65289;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20854;&#20013;LLM&#65288;&#20363;&#22914;GPT-4&#65289;&#29992;&#20110;&#38646;&#26679;&#26412;&#25991;&#26412;&#35782;&#21035;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#26631;&#31614;&#29983;&#25104;&#65292;&#22312;&#37027;&#37324;&#36825;&#20123;&#26631;&#31614;&#28982;&#21518;&#34987;&#29992;&#20316;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#24230;&#37327;&#12290;&#36890;&#36807;&#22312;MIMIC&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-4&#29983;&#25104;&#30340;&#26631;&#31614;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#35821;&#20041;&#30456;&#20284;&#24230;&#35780;&#20272;&#65292;&#24471;&#20998;&#26356;&#25509;&#36817;&#20020;&#24202;&#23454;&#38469;&#24773;&#20917;&#27604;&#20256;&#32479;NLP&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#20351;&#29992;LLM&#36827;&#34892;&#39640;&#24230;&#19987;&#19994;&#39046;&#22495;&#30340;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#20041;&#20998;&#26512;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#26377;&#21322;&#23450;&#37327;&#25512;&#29702;&#32467;&#26524;&#12290;&#34429;&#28982;&#35813;&#26694;&#26550;&#38024;&#23545;&#25918;&#23556;&#23398;&#25253;&#21578;&#30456;&#20284;&#24615;&#20998;&#26512;&#36827;&#34892;&#20102;&#23454;&#26045;&#65292;&#20294;&#20854;&#27010;&#24565;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#19987;&#38376;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11398v1 Announce Type: cross  Abstract: In this study, we leverage LLM to enhance the semantic analysis and develop similarity metrics for texts, addressing the limitations of traditional unsupervised NLP metrics like ROUGE and BLEU. We develop a framework where LLMs such as GPT-4 are employed for zero-shot text identification and label generation for radiology reports, where the labels are then used as measurements for text similarity. By testing the proposed framework on the MIMIC data, we find that GPT-4 generated labels can significantly improve the semantic similarity assessment, with scores more closely aligned with clinical ground truth than traditional NLP metrics. Our work demonstrates the possibility of conducting semantic analysis of the text data using semi-quantitative reasoning results by the LLMs for highly specialized domains. While the framework is implemented for radiology report similarity analysis, its concept can be extended to other specialized domains 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65292;&#36890;&#36807;&#36827;&#21270;&#20195;&#29702;&#30340;&#21151;&#33021;&#26469;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;</title><link>https://arxiv.org/abs/2402.11359</link><description>&lt;p&gt;
&#22312;&#19981;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Training Language Model Agents without Modifying Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#19981;&#20462;&#25913;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65292;&#36890;&#36807;&#36827;&#21270;&#20195;&#29702;&#30340;&#21151;&#33021;&#26469;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26368;&#36817;&#24050;&#32463;&#23558;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37325;&#26032;&#23450;&#20041;&#20026;&#20195;&#29702;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#21151;&#33021;&#33258;&#21160;&#21270;&#22320;&#23436;&#25104;&#22797;&#26434;&#20219;&#21153;&#12290;&#20026;&#20102;&#20419;&#36827;LLM&#20195;&#29702;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#20462;&#25913;LLM&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;LLM&#20195;&#29702;&#30340;&#26032;&#33539;&#24335;&#65292;&#24403;LLM&#38590;&#20197;&#25110;&#26080;&#27861;&#36827;&#34892;&#20462;&#25913;&#26102;&#23588;&#20854;&#26377;&#29992;&#12290;&#21463;&#21040;&#20154;&#31867;&#19981;&#26029;&#38203;&#36896;&#24037;&#20855;&#20197;&#36866;&#24212;&#29616;&#23454;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#32780;&#19981;&#26159;&#25913;&#21464;&#25105;&#20204;&#30340;&#29983;&#29289;&#32467;&#26500;&#20197;&#36866;&#24212;&#19968;&#32452;&#38745;&#24577;&#24037;&#20855;&#65292;&#25105;&#20204;&#25552;&#20986;&#36880;&#27493;&#38203;&#36896;&#20195;&#29702;&#30340;&#21151;&#33021;&#65292;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20462;&#25913;LLM&#26435;&#37325;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#21151;&#33021;&#35270;&#20026;&#21487;&#23398;&#20064;&#30340;&#8220;&#20195;&#29702;&#21442;&#25968;&#8221;&#24182;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#35757;&#32451;&#30340;&#22522;&#26412;&#24605;&#24819;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;AgentOptimizer&#65292;&#21033;&#29992;LLM&#26356;&#26032;&#20195;&#29702;&#30340;&#21151;&#33021;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#20195;&#29702;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11359v1 Announce Type: new  Abstract: Researchers and practitioners have recently reframed powerful Large Language Models (LLMs) as agents, enabling them to automate complex tasks largely via the use of specialized functions. To facilitate the development of LLM agents, we present a novel paradigm of training LLM agents without modifying the LLM weights, which is particularly useful when the LLMs are difficult or inaccessible for modifications. Inspired by how humans continuously forge tools to adapt to real-world tasks, rather than change our biological structure to fit a static set of tools, we propose to progressively forge agent's functions to better solve the downstream tasks instead of modifying the LLM weights. By treating the functions as learnable `agent parameters' and leveraging the fundamental idea of model training in artificial intelligence, we develop AgentOptimizer that employs the LLM to update agents' functions and devise an agent training algorithm with tw
&lt;/p&gt;</description></item><item><title>&#23558;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#24178;&#39044;&#25152;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2402.11355</link><description>&lt;p&gt;
&#25913;&#21464;&#20102;&#20160;&#20040;&#65311;&#23558;&#34920;&#24449;&#24178;&#39044;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
What Changed? Converting Representational Interventions to Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11355
&lt;/p&gt;
&lt;p&gt;
&#23558;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#65292;&#20197;&#20998;&#26512;&#21644;&#35299;&#37322;&#27169;&#22411;&#24178;&#39044;&#25152;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34920;&#24449;&#31354;&#38388;&#30340;&#24178;&#39044;&#26041;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#24433;&#21709;&#27169;&#22411;&#34892;&#20026;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#36825;&#20123;&#26041;&#27861;&#34987;&#29992;&#26469;&#28040;&#38500;&#25110;&#25913;&#21464;&#27169;&#22411;&#34920;&#31034;&#20013;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#32534;&#30721;&#65292;&#21019;&#24314;&#19968;&#20010;&#21453;&#20107;&#23454;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24178;&#39044;&#25805;&#20316;&#22312;&#34920;&#31034;&#31354;&#38388;&#20869;&#65292;&#20934;&#30830;&#29702;&#35299;&#23427;&#20462;&#25913;&#20102;&#21738;&#20123;&#29305;&#24449;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#34920;&#24449;&#31354;&#38388;&#30340;&#21453;&#20107;&#23454;&#21487;&#20197;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#30340;&#21453;&#20107;&#23454;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#23545;&#24212;&#20110;&#32473;&#23450;&#34920;&#31034;&#31354;&#38388;&#24178;&#39044;&#30340;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#35299;&#37322;&#29992;&#20110;&#32534;&#30721;&#29305;&#23450;&#27010;&#24565;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#21453;&#20107;&#23454;&#21487;&#20197;&#29992;&#20110;&#20943;&#36731;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11355v1 Announce Type: new  Abstract: Interventions targeting the representation space of language models (LMs) have emerged as effective means to influence model behavior. These methods are employed, for example, to eliminate or alter the encoding of demographic information such as gender within the model's representations, creating a counterfactual representation. However, since the intervention operates within the representation space, understanding precisely which features it modifies poses a challenge. We show that representation-space counterfactuals can be converted into natural language counterfactuals. We demonstrate that this approach enables us to analyze the linguistic alterations corresponding to a given representation-space intervention and to interpret the features utilized for encoding a specific concept. Moreover, the resulting counterfactuals can be used to mitigate bias in classification.
&lt;/p&gt;</description></item><item><title>&#38271;&#26399;&#35760;&#24518;&#30340;&#24341;&#20837;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#20013;&#30340;&#29992;&#25143;&#33258;&#25105;&#25259;&#38706;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#20915;&#24930;&#24615;&#20581;&#24247;&#29366;&#20917;&#21644;&#38544;&#31169;&#38382;&#39064;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.11353</link><description>&lt;p&gt;
&#29702;&#35299;&#38271;&#26399;&#35760;&#24518;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#20013;&#33258;&#25105;&#25259;&#38706;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Impact of Long-Term Memory on Self-Disclosure with Large Language Model-Driven Chatbots for Public Health Intervention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11353
&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#35760;&#24518;&#30340;&#24341;&#20837;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#20013;&#30340;&#29992;&#25143;&#33258;&#25105;&#25259;&#38706;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#35299;&#20915;&#24930;&#24615;&#20581;&#24247;&#29366;&#20917;&#21644;&#38544;&#31169;&#38382;&#39064;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26377;&#28508;&#21147;&#36890;&#36807;&#24320;&#25918;&#24335;&#23545;&#35805;&#25903;&#25345;&#20844;&#20849;&#21355;&#29983;&#30417;&#27979;&#65292;&#20294;&#22312;&#22810;&#27425;&#20114;&#21160;&#20013;&#24456;&#23569;&#20445;&#30041;&#20851;&#20110;&#20010;&#20154;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#28155;&#21152;&#38271;&#26399;&#35760;&#24518;(LTM)&#26469;&#22686;&#24378;LLMs&#25552;&#20379;&#20102;&#25913;&#36827;&#21442;&#19982;&#24230;&#21644;&#33258;&#25105;&#25259;&#38706;&#30340;&#26426;&#20250;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#23545;LTM&#22914;&#20309;&#24433;&#21709;&#20154;&#20204;&#22312;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#20013;&#19982;LLM&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#20114;&#21160;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;1,252&#36890;&#35805;&#35760;&#24405;&#21644;&#23545;&#20061;&#21517;&#29992;&#25143;&#30340;&#35775;&#35848;&#65292;&#30740;&#31350;&#20102;CareCall&#36825;&#31181;&#24102;&#26377;LTM&#30340;LLM&#39537;&#21160;&#30340;&#35821;&#38899;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26696;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;LTM&#22686;&#24378;&#20102;&#20581;&#24247;&#25259;&#38706;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#29087;&#24713;&#24863;&#20419;&#36827;&#20102;&#29992;&#25143;&#23545;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#31215;&#26497;&#30475;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#35266;&#23519;&#21040;&#36890;&#36807;LTM&#20419;&#36827;&#33258;&#25105;&#25259;&#38706;&#23384;&#22312;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#35299;&#20915;&#24930;&#24615;&#20581;&#24247;&#29366;&#20917;&#21644;&#38544;&#31169;&#38382;&#39064;&#26041;&#38754;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;LTM&#25972;&#21512;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11353v1 Announce Type: cross  Abstract: Recent large language models (LLMs) offer the potential to support public health monitoring by facilitating health disclosure through open-ended conversations but rarely preserve the knowledge gained about individuals across repeated interactions. Augmenting LLMs with long-term memory (LTM) presents an opportunity to improve engagement and self-disclosure, but we lack an understanding of how LTM impacts people's interaction with LLM-driven chatbots in public health interventions. We examine the case of CareCall -- an LLM-driven voice chatbot with LTM -- through the analysis of 1,252 call logs and interviews with nine users. We found that LTM enhanced health disclosure and fostered positive perceptions of the chatbot by offering familiarity. However, we also observed challenges in promoting self-disclosure through LTM, particularly around addressing chronic health conditions and privacy concerns. We discuss considerations for LTM integr
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27809;&#26377;&#23398;&#20064;&#21040;&#35821;&#35328;&#30340;&#35270;&#21548;&#29305;&#24615;&#65292;&#22312;&#26032;&#30340;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#26292;&#38706;&#20102;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#19982;&#35821;&#35328;&#27169;&#22411;&#24863;&#23448;&#22788;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.11349</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26410;&#23398;&#20064;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Tasks That Language Models Don't Learn
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11349
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27809;&#26377;&#23398;&#20064;&#21040;&#35821;&#35328;&#30340;&#35270;&#21548;&#29305;&#24615;&#65292;&#22312;&#26032;&#30340;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#26292;&#38706;&#20102;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#19982;&#35821;&#35328;&#27169;&#22411;&#24863;&#23448;&#22788;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;&#65292;&#25105;&#20204;&#30446;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27809;&#26377;&#23398;&#20064;&#21040;&#35821;&#35328;&#30340;&#26576;&#20123;&#29305;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#20219;&#21153;&#65288;&#31216;&#20026;H-TEST&#65289;&#23545;&#35821;&#35328;&#30340;&#35270;&#21548;&#29305;&#24615;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#36825;&#19968;&#22522;&#20934;&#27979;&#35797;&#31361;&#26174;&#20102;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#19982;LLMs&#30340;&#24863;&#23448;&#21463;&#38480;&#22788;&#29702;&#33021;&#21147;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#36317;&#12290;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;1. &#25925;&#24847;&#25512;&#29702;&#65288;&#24605;&#32500;&#38142;&#65289;&#65292;2. &#23569;&#37327;&#26696;&#20363;&#65292;&#25110;3. &#21516;&#19968;&#27169;&#22411;&#31995;&#21015;&#30340;&#26356;&#24378;&#22823;LLM&#65288;LLaMA 2 13B-&gt;LLaMA 2 70B&#65289;&#24182;&#19981;&#33021;&#31616;&#21333;&#22320;&#24102;&#26469;H-TEST&#24615;&#33021;&#30340;&#25913;&#21892;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#29305;&#21035;&#23558;&#20854;&#19982;&#29595;&#20029;&#30340;&#21746;&#23398;&#26696;&#20363;&#32852;&#31995;&#36215;&#26469;&#65292;&#22905;&#22312;&#24863;&#23448;&#21463;&#38480;&#29615;&#22659;&#20013;&#20102;&#35299;&#19990;&#30028;&#65288;Jackson&#65292;1986&#65289;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19968;&#20123;&#26368;&#24378;&#22823;&#30340;&#19987;&#26377;LLMs&#30340;&#34920;&#29616;&#25509;&#36817;&#20110;&#38543;&#26426;&#22522;&#20934;&#20934;&#30830;&#29575;50&#65285;&#65292;&#31361;&#26174;&#20102;&#26497;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11349v1 Announce Type: cross  Abstract: We argue that there are certain properties of language that our current large language models (LLMs) don't learn. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-TEST. This benchmark highlights a fundamental gap between human linguistic comprehension, which naturally integrates sensory experiences, and the sensory-deprived processing capabilities of LLMs. In support of our hypothesis, 1. deliberate reasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from the same model family (LLaMA 2 13B -&gt; LLaMA 2 70B) do not trivially bring improvements in H-TEST performance. Therefore, we make a particular connection to the philosophical case of Mary, who learns about the world in a sensory-deprived environment (Jackson, 1986). Our experiments show that some of the strongest proprietary LLMs stay near random chance baseline accuracy of 50%, highlighting the limit
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;PhaseEvo&#65292;&#19968;&#20010;&#26088;&#22312;&#23454;&#29616;&#25552;&#31034;&#25351;&#20196;&#21644;&#31034;&#20363;&#30340;&#32852;&#21512;&#20248;&#21270;&#30340;&#39640;&#25928;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#20840;&#23616;&#25628;&#32034;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.11347</link><description>&lt;p&gt;
PhaseEvo&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#19978;&#19979;&#25991;&#25552;&#31034;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
PhaseEvo: Towards Unified In-Context Prompt Optimization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11347
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;PhaseEvo&#65292;&#19968;&#20010;&#26088;&#22312;&#23454;&#29616;&#25552;&#31034;&#25351;&#20196;&#21644;&#31034;&#20363;&#30340;&#32852;&#21512;&#20248;&#21270;&#30340;&#39640;&#25928;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#20840;&#23616;&#25628;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21046;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29702;&#24819;&#25552;&#31034;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#26174;&#33879;&#30340;&#36164;&#28304;&#21644;&#19987;&#19994;&#20154;&#21592;&#30340;&#36755;&#20837;&#12290;&#29616;&#26377;&#24037;&#20316;&#23558;&#20248;&#21270;&#25552;&#31034;&#25351;&#20196;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#20363;&#35270;&#20026;&#19981;&#21516;&#38382;&#39064;&#65292;&#23548;&#33268;&#25552;&#31034;&#24615;&#33021;&#27425;&#20248;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#32479;&#19968;&#30340;&#19978;&#19979;&#25991;&#25552;&#31034;&#20248;&#21270;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#23616;&#38480;&#24615;&#65292;&#26088;&#22312;&#23454;&#29616;&#25552;&#31034;&#25351;&#20196;&#21644;&#31034;&#20363;&#30340;&#32852;&#21512;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#22312;&#31163;&#25955;&#19988;&#39640;&#32500;&#30340;&#33258;&#28982;&#35821;&#35328;&#31354;&#38388;&#20013;&#21046;&#23450;&#36825;&#31181;&#20248;&#21270;&#24341;&#20837;&#20102;&#25910;&#25947;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PhaseEvo&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#20840;&#23616;&#25628;&#32034;&#25928;&#29575;&#30340;&#39640;&#25928;&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#37319;&#29992;&#22810;&#38454;&#27573;&#35774;&#35745;&#65292;&#34701;&#21512;&#20102;&#21019;&#26032;&#30340;&#22522;&#20110;LLMs&#30340;&#21464;&#24322;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11347v1 Announce Type: new  Abstract: Crafting an ideal prompt for Large Language Models (LLMs) is a challenging task that demands significant resources and expert human input. Existing work treats the optimization of prompt instruction and in-context learning examples as distinct problems, leading to sub-optimal prompt performance. This research addresses this limitation by establishing a unified in-context prompt optimization framework, which aims to achieve joint optimization of the prompt instruction and examples. However, formulating such optimization in the discrete and high-dimensional natural language space introduces challenges in terms of convergence and computational efficiency. To overcome these issues, we present PhaseEvo, an efficient automatic prompt optimization framework that combines the generative capability of LLMs with the global search proficiency of evolution algorithms. Our framework features a multi-phase design incorporating innovative LLM-based mut
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20107;&#20214;&#25551;&#36848;&#37197;&#23545;&#23454;&#29616;&#30693;&#35782;&#26356;&#26032;&#30340;&#20107;&#20214;&#39537;&#21160;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#22312;&#32534;&#36753;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11324</link><description>&lt;p&gt;
EVEDIT: &#20107;&#20214;&#39537;&#21160;&#30340;&#30693;&#35782;&#32534;&#36753;&#19982;&#28436;&#32462;&#32534;&#36753;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
EVEDIT: Event-based Knowledge Editing with Deductive Editing Boundaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20107;&#20214;&#25551;&#36848;&#37197;&#23545;&#23454;&#29616;&#30693;&#35782;&#26356;&#26032;&#30340;&#20107;&#20214;&#39537;&#21160;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#22312;&#32534;&#36753;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20449;&#24687;&#30340;&#21160;&#24577;&#24615;&#35201;&#27714;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#39640;&#25928;&#30340;&#30693;&#35782;&#32534;&#36753;&#65288;KE&#65289;&#20197;&#36827;&#34892;&#30693;&#35782;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;KE&#26041;&#27861;&#36890;&#24120;&#25805;&#20316;&#20110;&#65288;&#20027;&#20307;&#65292;&#20851;&#31995;&#65292;&#23458;&#20307;&#65289;&#19977;&#20803;&#32452;&#65292;&#24573;&#30053;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#19981;&#21516;&#30693;&#35782;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#31181;&#32534;&#36753;&#26041;&#27861;&#21487;&#33021;&#36935;&#21040;&#19981;&#30830;&#23450;&#30340;&#32534;&#36753;&#36793;&#30028;&#65292;&#23548;&#33268;&#22823;&#37327;&#30456;&#20851;&#30693;&#35782;&#23384;&#22312;&#27495;&#20041;&#65306;&#20107;&#20808;&#21487;&#20197;&#22238;&#31572;&#30340;&#26597;&#35810;&#22312;&#32534;&#36753;&#21518;&#26080;&#27861;&#21487;&#38752;&#22320;&#22238;&#31572;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;KE&#30340;&#29702;&#35770;&#26694;&#26550;&#26469;&#20998;&#26512;&#36825;&#20010;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#19968;&#32452;&#34987;&#24573;&#35270;&#30340;&#30693;&#35782;&#65292;&#36825;&#20123;&#30693;&#35782;&#22312;&#32534;&#36753;&#36807;&#31243;&#20013;&#20445;&#25345;&#19981;&#21464;&#65292;&#24182;&#22312;&#32534;&#36753;&#36807;&#31243;&#20013;&#36827;&#34892;&#30693;&#35782;&#25512;&#26029;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#28436;&#32462;&#38170;&#28857;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20107;&#20214;&#30340;&#30693;&#35782;&#32534;&#36753;&#20219;&#21153;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#20219;&#21153;&#23558;&#20107;&#23454;&#19982;&#20107;&#20214;&#25551;&#36848;&#37197;&#23545;&#12290;&#36825;&#19968;&#20219;&#21153;&#19981;&#20165;&#20307;&#29616;&#20102;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#26356;&#32039;&#23494;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11324v1 Announce Type: new  Abstract: The dynamic nature of real-world information necessitates efficient knowledge editing (KE) in large language models (LLMs) for knowledge updating. However, current KE approaches, which typically operate on (subject, relation, object) triples, ignore the contextual information and the relation among different knowledge. Such editing methods could thus encounter an uncertain editing boundary, leaving a lot of relevant knowledge in ambiguity: Queries that could be answered pre-edit cannot be reliably answered afterward. In this work, we analyze this issue by introducing a theoretical framework for KE that highlights an overlooked set of knowledge that remains unchanged and aids in knowledge deduction during editing, which we name as the deduction anchor. We further address this issue by proposing a novel task of event-based knowledge editing that pairs facts with event descriptions. This task manifests not only a closer simulation of real-w
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#27169;&#22411;&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#29702;&#35299;&#22810;&#22270;&#20687;&#12289;&#22810;&#38899;&#39057;&#21644;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#22312;&#33521;&#35821;&#21644;&#39532;&#26469;&#35821;&#20043;&#38388;&#20999;&#25442;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;SigLIP&#32534;&#30721;&#22120;&#21644;Whisper&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#36825;&#19968;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11297</link><description>&lt;p&gt;
MMMModal -- &#22810;&#22270;&#20687;&#22810;&#38899;&#39057;&#22810;&#36718;&#22810;&#27169;&#24577;
&lt;/p&gt;
&lt;p&gt;
MMMModal -- Multi-Images Multi-Audio Multi-turn Multi-Modal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11297
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#27169;&#22411;&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#29702;&#35299;&#22810;&#22270;&#20687;&#12289;&#22810;&#38899;&#39057;&#21644;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#22312;&#33521;&#35821;&#21644;&#39532;&#26469;&#35821;&#20043;&#38388;&#20999;&#25442;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;SigLIP&#32534;&#30721;&#22120;&#21644;Whisper&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#36825;&#19968;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#24341;&#20837;&#20102;&#19968;&#31181;&#31361;&#30772;&#24615;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#29702;&#35299;&#22810;&#22270;&#20687;&#12289;&#22810;&#38899;&#39057;&#21644;&#22810;&#22270;&#20687;&#22810;&#38899;&#39057;&#22312;&#21333;&#20010;&#22810;&#36718;&#20250;&#35805;&#20869;&#12290;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#21033;&#29992;SigLIP&#32534;&#30721;&#22120;&#29992;&#20110;&#35270;&#35273;&#36755;&#20837;&#65292;Whisper&#32534;&#30721;&#22120;&#29992;&#20110;&#38899;&#39057;&#36755;&#20837;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21452;&#35821;&#30340;&#65292;&#33021;&#22815;&#21516;&#26102;&#29702;&#35299;&#33521;&#35821;&#21644;&#39532;&#26469;&#35821;&#12290;&#25105;&#20204;&#33258;&#35946;&#22320;&#25512;&#20986;&#20102;&#36825;&#20010;&#27169;&#22411;&#30340;&#20004;&#20010;&#29256;&#26412;&#65306;&#25317;&#26377;11&#20159;&#21442;&#25968;&#30340;TinyLlama&#21644;&#25317;&#26377;70&#20159;&#21442;&#25968;&#30340;Mistral&#12290;&#20973;&#20511;&#20854;&#33021;&#22815;&#22788;&#29702;&#22810;&#26679;&#30340;&#27169;&#24577;&#21644;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20195;&#34920;&#20102;&#39532;&#26469;&#35199;&#20122;&#32972;&#26223;&#21644;&#26356;&#24191;&#27867;&#39046;&#22495;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11297v1 Announce Type: new  Abstract: Our contribution introduces a groundbreaking multimodal large language model designed to comprehend multi-images, multi-audio, and multi-images-multi-audio within a single multiturn session. Leveraging state-of-the-art models, we utilize the SigLIP encoder for visual inputs and the Whisper Encoder for audio inputs. Notably, this multimodal large language model is bilingual, proficient in understanding both English and Malay simultaneously. We proudly unveil two versions of this model: TinyLlama with 1.1B parameters, and Mistral with 7B parameters. With its ability to navigate diverse modalities and languages, our model represents a significant advancement for the Malaysian context and beyond.   All models released at https://huggingface.co/collections/mesolitica/multimodal-malaysian-llm-65c6f893e03f78fa9e5c8859
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20154;&#31867;&#21644;32&#31181;&#19981;&#21516;LLM&#30340;&#20559;&#22909;&#65292;&#21457;&#29616;&#20154;&#31867;&#19981;&#22826;&#22312;&#24847;&#38169;&#35823;&#65292;&#20559;&#22909;&#25903;&#25345;&#31435;&#22330;&#30340;&#22238;&#24212;&#65292;&#32780;&#20808;&#36827;&#30340;LLM&#26356;&#27880;&#37325;&#27491;&#30830;&#24615;&#12289;&#28165;&#26224;&#24615;&#21644;&#26080;&#23475;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11296</link><description>&lt;p&gt;
&#20998;&#26512;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Dissecting Human and LLM Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11296
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#20154;&#31867;&#21644;32&#31181;&#19981;&#21516;LLM&#30340;&#20559;&#22909;&#65292;&#21457;&#29616;&#20154;&#31867;&#19981;&#22826;&#22312;&#24847;&#38169;&#35823;&#65292;&#20559;&#22909;&#25903;&#25345;&#31435;&#22330;&#30340;&#22238;&#24212;&#65292;&#32780;&#20808;&#36827;&#30340;LLM&#26356;&#27880;&#37325;&#27491;&#30830;&#24615;&#12289;&#28165;&#26224;&#24615;&#21644;&#26080;&#23475;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#27169;&#22411;&#21709;&#24212;&#30340;&#30456;&#23545;&#36136;&#37327;&#27604;&#36739;&#65292;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20559;&#22909;&#22312;&#27169;&#22411;&#24494;&#35843;&#20013;&#20316;&#20026;&#20849;&#21516;&#30340;&#23545;&#40784;&#30446;&#26631;&#21644;&#35780;&#20272;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20559;&#22909;&#20165;&#21453;&#26144;&#20102;&#24191;&#27867;&#36235;&#21183;&#65292;&#23548;&#33268;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25511;&#24615;&#36739;&#24046;&#65292;&#21487;&#33021;&#23384;&#22312;&#28508;&#22312;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#21078;&#26512;&#20102;&#20154;&#31867;&#21644;32&#31181;&#19981;&#21516;LLM&#30340;&#20559;&#22909;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#30340;&#23450;&#37327;&#32452;&#25104;&#65292;&#21033;&#29992;&#26469;&#33258;&#30495;&#23454;&#29992;&#25143;-&#27169;&#22411;&#23545;&#35805;&#30340;&#27880;&#37322;&#36827;&#34892;&#32454;&#31890;&#24230;&#12289;&#22330;&#26223;&#21270;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#23545;&#38169;&#35823;&#19981;&#22826;&#25935;&#24863;&#65292;&#20559;&#22909;&#25903;&#25345;&#20854;&#31435;&#22330;&#30340;&#22238;&#24212;&#65292;&#24182;&#22312;&#27169;&#22411;&#25215;&#35748;&#20854;&#23616;&#38480;&#24615;&#26102;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#19981;&#21916;&#27426;&#12290;&#30456;&#21453;&#65292;&#20687;GPT-4-Turbo&#36825;&#26679;&#30340;&#20808;&#36827;LLM&#26356;&#21152;&#24378;&#35843;&#27491;&#30830;&#24615;&#12289;&#28165;&#26224;&#24615;&#21644;&#26080;&#23475;&#24615;&#12290;&#27492;&#22806;&#65292;&#22823;&#23567;&#30456;&#20284;&#30340;LLM&#20542;&#21521;&#20110;&#23637;&#29616;&#20986;&#31867;&#20284;&#30340;&#20559;&#22909;&#65292;&#26080;&#35770;&#23427;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#22914;&#20309;&#65292;&#24182;&#19988;&#20026;&#20102;&#23545;&#40784;&#32780;&#36827;&#34892;&#30340;&#24494;&#35843;&#24182;&#19981;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11296v1 Announce Type: cross  Abstract: As a relative quality comparison of model responses, human and Large Language Model (LLM) preferences serve as common alignment goals in model fine-tuning and criteria in evaluation. Yet, these preferences merely reflect broad tendencies, resulting in less explainable and controllable models with potential safety risks. In this work, we dissect the preferences of human and 32 different LLMs to understand their quantitative composition, using annotations from real-world user-model conversations for a fine-grained, scenario-wise analysis. We find that humans are less sensitive to errors, favor responses that support their stances, and show clear dislike when models admit their limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize correctness, clarity, and harmlessness more. Additionally, LLMs of similar sizes tend to exhibit similar preferences, regardless of their training methods, and fine-tuning for alignment does not sign
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OneBit&#30340;1&#20301;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#30697;&#38453;&#37327;&#21270;&#20026;1&#20301;&#65292;&#20026;&#26497;&#20302;&#27604;&#29305;&#23485;&#24230;&#30340;LLMs&#37096;&#32626;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2402.11295</link><description>&lt;p&gt;
OneBit:&#26397;&#30528;&#26497;&#20302;&#27604;&#29305;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
OneBit: Towards Extremely Low-bit Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;OneBit&#30340;1&#20301;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#30697;&#38453;&#37327;&#21270;&#20026;1&#20301;&#65292;&#20026;&#26497;&#20302;&#27604;&#29305;&#23485;&#24230;&#30340;LLMs&#37096;&#32626;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#37327;&#21270;&#20351;&#29992;&#20302;&#27604;&#29305;&#23485;&#24230;&#20540;&#26469;&#34920;&#31034;&#27169;&#22411;&#30340;&#26435;&#37325;&#30697;&#38453;&#65292;&#36825;&#26159;&#20943;&#23569;&#37096;&#32626;&#39640;&#24230;&#26399;&#24453;&#30340;LLMs&#30340;&#23384;&#20648;&#21644;&#35745;&#31639;&#24320;&#38144;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#37327;&#21270;&#26041;&#27861;&#22312;&#27604;&#29305;&#23485;&#24230;&#26497;&#23567;&#26102;&#24615;&#33021;&#20005;&#37325;&#19979;&#38477;&#65292;&#22240;&#27492;&#19987;&#27880;&#20110;&#21033;&#29992;4&#20301;&#25110;8&#20301;&#20540;&#26469;&#37327;&#21270;&#27169;&#22411;&#12290;&#26412;&#25991;&#22823;&#32966;&#22320;&#23558;LLMs&#30340;&#26435;&#37325;&#30697;&#38453;&#37327;&#21270;&#20026;1&#20301;&#65292;&#20026;LLMs&#30340;&#26497;&#20302;&#27604;&#29305;&#23485;&#24230;&#37096;&#32626;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;OneBit&#30340;1&#20301;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26356;&#22909;&#22320;&#37327;&#21270;LLMs&#30340;&#26032;&#39062;&#30340;1&#20301;&#21442;&#25968;&#34920;&#31034;&#26041;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#30697;&#38453;&#20998;&#35299;&#30340;&#26377;&#25928;&#21442;&#25968;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;QAT&#26694;&#26550;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#20805;&#20998;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;OneBit&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65288;&#33267;&#23569;&#26159;&#38750;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11295v1 Announce Type: new  Abstract: Model quantification uses low bit-width values to represent the weight matrices of models, which is a promising approach to reduce both storage and computational overheads of deploying highly anticipated LLMs. However, existing quantization methods suffer severe performance degradation when the bit-width is extremely reduced, and thus focus on utilizing 4-bit or 8-bit values to quantize models. This paper boldly quantizes the weight matrices of LLMs to 1-bit, paving the way for the extremely low bit-width deployment of LLMs. For this target, we introduce a 1-bit quantization-aware training (QAT) framework named OneBit, including a novel 1-bit parameter representation method to better quantize LLMs as well as an effective parameter initialization method based on matrix decomposition to improve the convergence speed of the QAT framework. Sufficient experimental results indicate that OneBit achieves good performance (at least 83% of the non
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#36890;&#36807;&#23558;&#38590;&#39064;&#20998;&#20026;&#22522;&#20110;&#35268;&#21017;&#21644;&#26080;&#35268;&#21017;&#20004;&#31867;&#30340;&#29420;&#29305;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#29616;&#65292;&#24378;&#35843;&#20102;&#22312;&#22797;&#26434;&#38590;&#39064;&#24773;&#22659;&#20013;LLMs&#30340;&#25361;&#25112;&#21644;&#20154;&#31867;&#31867;&#20284;&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#25512;&#21160;LLMs&#35299;&#35868;&#33021;&#21147;&#21644;&#36129;&#29486;&#20110;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11291</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#35299;&#20915;&#38590;&#39064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Puzzle Solving using Reasoning of Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#36890;&#36807;&#23558;&#38590;&#39064;&#20998;&#20026;&#22522;&#20110;&#35268;&#21017;&#21644;&#26080;&#35268;&#21017;&#20004;&#31867;&#30340;&#29420;&#29305;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#29616;&#65292;&#24378;&#35843;&#20102;&#22312;&#22797;&#26434;&#38590;&#39064;&#24773;&#22659;&#20013;LLMs&#30340;&#25361;&#25112;&#21644;&#20154;&#31867;&#31867;&#20284;&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#31361;&#20986;&#20102;&#25512;&#21160;LLMs&#35299;&#35868;&#33021;&#21147;&#21644;&#36129;&#29486;&#20110;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35299;&#20915;&#38590;&#39064;&#20013;&#30340;&#33021;&#21147;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#65292;&#26631;&#24535;&#30528;&#29702;&#35299;&#23427;&#20204;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#36808;&#20986;&#20102;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#26412;&#35843;&#26597;&#21033;&#29992;&#29420;&#29305;&#30340;&#20998;&#31867;&#27861;&#23558;&#38590;&#39064;&#20998;&#20026;&#22522;&#20110;&#35268;&#21017;&#21644;&#26080;&#35268;&#21017;&#20004;&#31867;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#35780;&#20272;LLMs&#65292;&#21253;&#25324;&#25552;&#31034;&#25216;&#26415;&#12289;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#21644;&#24494;&#35843;&#12290;&#36890;&#36807;&#23545;&#30456;&#20851;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#30340;&#25209;&#21028;&#24615;&#23457;&#26597;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#22312;&#22797;&#26434;&#38590;&#39064;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#65292;&#35782;&#21035;&#20986;&#22797;&#26434;&#38590;&#39064;&#24773;&#22659;&#20013;&#30340;&#26174;&#33879;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;LLMs&#33021;&#21147;&#21450;&#31867;&#20154;&#25512;&#29702;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#39640;&#32423;&#36923;&#36753;&#25512;&#26029;&#30340;&#24773;&#20917;&#19979;&#12290;&#35843;&#26597;&#24378;&#35843;&#20102;&#38656;&#35201;&#26032;&#39062;&#31574;&#30053;&#21644;&#26356;&#20016;&#23500;&#25968;&#25454;&#38598;&#26469;&#25552;&#21319;LLMs&#30340;&#35299;&#35868;&#33021;&#21147;&#24182;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11291v1 Announce Type: cross  Abstract: Exploring the capabilities of Large Language Models (LLMs) in puzzle solving unveils critical insights into their potential and challenges in artificial intelligence, marking a significant step towards understanding their applicability in complex reasoning tasks. This survey leverages a unique taxonomy -- dividing puzzles into rule-based and rule-less categories -- to critically assess LLMs through various methodologies, including prompting techniques, neuro-symbolic approaches, and fine-tuning. Through a critical review of relevant datasets and benchmarks, we assess LLMs' performance, identifying significant challenges in complex puzzle scenarios. Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference. The survey underscores the necessity for novel strategies and richer datasets to advance LLMs' puzzle-solving proficiency and contribute to AI's
&lt;/p&gt;</description></item><item><title>&#27721;&#35821;&#20013;&#32570;&#22833;NP&#30340;&#21452;&#37325;&#20013;&#24515;&#23884;&#22871;&#32467;&#26500;&#24182;&#19981;&#26159;&#35821;&#27861;&#24615;&#38169;&#35273;&#65292;&#32780;&#26159;&#21160;&#35789;&#27495;&#20041;&#35299;&#37322;&#30340;&#21547;&#31946;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.11282</link><description>&lt;p&gt;
&#20013;&#24515;&#23884;&#22871;&#32467;&#26500;&#20013;&#30340;&#32570;&#22833;NP&#25928;&#24212;&#30340;&#24615;&#36136;&#65306;&#20107;&#20214;&#30456;&#20851;&#30005;&#20301;&#25581;&#31034;&#26197;&#30505;&#38169;&#35273;&#36824;&#26159;&#21547;&#31946;&#35299;&#37322;&#65311;
&lt;/p&gt;
&lt;p&gt;
Grammaticality illusion or ambiguous interpretation? Event-related potentials reveal the nature of the missing-NP effect in Mandarin centre-embedded structures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11282
&lt;/p&gt;
&lt;p&gt;
&#27721;&#35821;&#20013;&#32570;&#22833;NP&#30340;&#21452;&#37325;&#20013;&#24515;&#23884;&#22871;&#32467;&#26500;&#24182;&#19981;&#26159;&#35821;&#27861;&#24615;&#38169;&#35273;&#65292;&#32780;&#26159;&#21160;&#35789;&#27495;&#20041;&#35299;&#37322;&#30340;&#21547;&#31946;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20960;&#31181;&#35821;&#35328;&#20013;&#65292;&#22312;&#21452;&#37325;&#20013;&#24515;&#23884;&#22871;&#32467;&#26500;&#20013;&#30465;&#30053;&#21160;&#35789;&#30701;&#35821;&#65288;VP&#65289;&#20250;&#20135;&#29983;&#19968;&#20010;&#35821;&#27861;&#24615;&#38169;&#35273;&#12290;&#31867;&#20284;&#30340;&#38169;&#35273;&#20063;&#34920;&#29616;&#22312;&#27721;&#35821;&#32570;&#22833;NP&#30340;&#21452;&#37325;&#20013;&#24515;&#23884;&#22871;&#32467;&#26500;&#20013;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#23427;&#30340;&#26412;&#36136;&#23578;&#26080;&#20849;&#35782;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19982;&#20854;&#25226;&#23427;&#30475;&#20316;&#26159;&#35821;&#27861;&#24615;&#38169;&#35273;&#65292;&#19981;&#22914;&#23558;&#21160;&#35789;&#30340;&#27495;&#20041;&#35299;&#37322;&#35270;&#20026;&#26368;&#33021;&#35299;&#37322;&#27721;&#35821;&#20013;&#36825;&#19968;&#29616;&#35937;&#30340;&#26041;&#24335;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25903;&#25345;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#22312;&#20943;&#23569;&#22797;&#26434;&#24230;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#20004;&#39033;&#19982;&#33258;&#23884;&#20837;&#20851;&#31995;&#20174;&#21477;&#25918;&#32622;&#22312;&#21477;&#23376;&#20027;&#35821;&#20301;&#32622;&#30456;&#32467;&#21512;&#30456;&#36817;&#21452;&#20013;&#24515;&#23884;&#22871;&#32467;&#26500;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#23454;&#39564;&#12290;&#23454;&#39564;1&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#32467;&#26500;&#20013;&#21516;&#26679;&#20250;&#34920;&#29616;&#20986;&#31867;&#20284;&#30340;&#29616;&#35937;&#65292;&#35777;&#25454;&#26159;&#32570;&#23569;P600&#25928;&#24212;&#32780;&#23384;&#22312;N400&#25928;&#24212;&#12290;&#22312;&#23454;&#39564;2&#20013;&#65292;&#36890;&#36807;&#25552;&#20379;&#35821;&#20041;&#32447;&#32034;&#20197;&#20943;&#23569;&#27495;&#20041;&#65292;&#28040;&#38500;&#20102;&#36825;&#31181;&#38169;&#35273;&#65292;&#35777;&#25454;&#26159;&#23384;&#22312;P600&#25928;&#24212;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#36825;&#20123;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11282v1 Announce Type: new  Abstract: In several languages, omitting a verb phrase (VP) in double centre-embedded structures creates a grammaticality illusion. Similar illusion also exhibited in Mandarin missing-NP double centre-embedded structures. However, there is no consensus on its very nature. Instead of treating it as grammaticality illusion, we argue that ambiguous interpretations of verbs can best account for this phenomenon in Mandarin. To further support this hypothesis, we conducted two electroencephalography (EEG) experiments on quasi double centre-embedded structures whose complexity is reduced by placing the self-embedding relative clauses into the sentence's subject position. Experiment 1 showed that similar phenomenon even exhibited in this structure, evidenced by an absence of P600 effect and a presence of N400 effect. In Experiment 2, providing semantic cues to reduce ambiguity dispelled this illusion, as evidenced by a P600 effect. We interpret the result
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;DEEPEVAL&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#23545;&#35270;&#35273;&#28145;&#23618;&#35821;&#20041;&#30340;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;LMMs&#22312;&#28145;&#23618;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.11281</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#33021;&#25581;&#31034;&#22270;&#20687;&#32972;&#21518;&#30340;&#28145;&#23618;&#35821;&#20041;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Multimodal Models Uncover Deep Semantics Behind Images?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11281
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;DEEPEVAL&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#23545;&#35270;&#35273;&#28145;&#23618;&#35821;&#20041;&#30340;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;LMMs&#22312;&#28145;&#23618;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22270;&#20687;&#30340;&#28145;&#23618;&#35821;&#20041;&#22312;&#31038;&#20132;&#23186;&#20307;&#20027;&#23548;&#30340;&#26102;&#20195;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#22270;&#20687;&#30340;&#34920;&#38754;&#25551;&#36848;&#19978;&#65292;&#25581;&#31034;&#20102;&#22312;&#23545;&#20869;&#22312;&#28145;&#23618;&#35821;&#20041;&#36827;&#34892;&#31995;&#32479;&#35843;&#26597;&#26041;&#38754;&#30340;&#26126;&#26174;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DEEPEVAL&#65292;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#23545;&#35270;&#35273;&#28145;&#23618;&#35821;&#20041;&#30340;&#33021;&#21147;&#12290; DEEPEVAL &#21253;&#25324;&#20154;&#24037;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#28176;&#36827;&#30340;&#23376;&#20219;&#21153;&#65306;&#32454;&#31890;&#24230;&#25551;&#36848;&#36873;&#25321;&#12289;&#28145;&#24230;&#26631;&#39064;&#21305;&#37197;&#21644;&#28145;&#23618;&#35821;&#20041;&#29702;&#35299;&#12290;&#21033;&#29992; DEEPEVAL&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;9&#20010;&#24320;&#28304;LMMs&#21644;GPT-4V(ision)&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#20102;&#29616;&#26377;LMMs&#19982;&#20154;&#31867;&#22312;&#28145;&#23618;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#19978;&#23384;&#22312;&#30528;&#23454;&#36136;&#24046;&#36317;&#12290;&#20363;&#22914;&#65292;&#21363;&#20351;&#22312;&#22270;&#20687;&#25551;&#36848;&#26041;&#38754;&#36798;&#21040;&#19982;&#20154;&#31867;&#21487;&#27604;&#30340;&#34920;&#29616;&#65292;GPT-4V&#22312;&#29702;&#35299;&#28145;&#23618;&#35821;&#20041;&#26041;&#38754;&#20173;&#33853;&#21518;&#20110;&#20154;&#31867;30%&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11281v1 Announce Type: new  Abstract: Understanding the deep semantics of images is essential in the era dominated by social media. However, current research works primarily on the superficial description of images, revealing a notable deficiency in the systematic investigation of the inherent deep semantics. In this work, we introduce DEEPEVAL, a comprehensive benchmark to assess Large Multimodal Models' (LMMs) capacities of visual deep semantics. DEEPEVAL includes human-annotated dataset and three progressive subtasks: fine-grained description selection, in-depth title matching, and deep semantics understanding. Utilizing DEEPEVAL, we evaluate 9 open-source LMMs and GPT-4V(ision).Our evaluation demonstrates a substantial gap between the deep semantic comprehension capabilities of existing LMMs and humans. For example, GPT-4V is 30% behind humans in understanding deep semantics, even though it achieves human-comparable performance in image description. Further analysis indi
&lt;/p&gt;</description></item><item><title>&#22810;&#36879;&#35270;&#19968;&#33268;&#24615;&#26041;&#27861;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#24102;&#26469;&#25913;&#36827;&#65292;&#33021;&#26377;&#25928;&#20943;&#36731;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11279</link><description>&lt;p&gt;
&#22810;&#36879;&#35270;&#19968;&#33268;&#24615;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multi-Perspective Consistency Enhances Confidence Estimation in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11279
&lt;/p&gt;
&lt;p&gt;
&#22810;&#36879;&#35270;&#19968;&#33268;&#24615;&#26041;&#27861;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#24102;&#26469;&#25913;&#36827;&#65292;&#33021;&#26377;&#25928;&#20943;&#36731;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37096;&#32626;&#20013;&#65292;&#20934;&#30830;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#23545;&#20110;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#32463;&#24120;&#26080;&#27861;&#20811;&#26381;&#23545;&#38169;&#35823;&#31572;&#26696;&#30340;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;&#32771;&#34385;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#25105;&#24847;&#35782;&#30340;&#33030;&#24369;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#36879;&#35270;&#19968;&#33268;&#24615;&#65288;MPC&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#27169;&#22411;&#20869;&#19981;&#21516;&#36879;&#35270;&#35282;&#24230;&#65288;MPC-Internal&#65289;&#21644;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#65288;MPC-Across&#65289;&#30340;&#20114;&#34917;&#35265;&#35299;&#26469;&#32531;&#35299;&#30001;&#21333;&#19968;&#35270;&#35282;&#20135;&#29983;&#30340;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#12290;&#23545;&#20843;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;MPC&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;MPC&#33021;&#22815;&#20943;&#36731;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11279v1 Announce Type: cross  Abstract: In the deployment of large language models (LLMs), accurate confidence estimation is critical for assessing the credibility of model predictions. However, existing methods often fail to overcome the issue of overconfidence on incorrect answers. In this work, we focus on improving the confidence estimation of large language models. Considering the fragility of self-awareness in language models, we introduce a Multi-Perspective Consistency (MPC) method. We leverage complementary insights from different perspectives within models (MPC-Internal) and across different models (MPC-Across) to mitigate the issue of overconfidence arising from a singular viewpoint. The experimental results on eight publicly available datasets show that our MPC achieves state-of-the-art performance. Further analyses indicate that MPC can mitigate the problem of overconfidence and is effectively scalable to other models.
&lt;/p&gt;</description></item><item><title>&#21512;&#25104;&#20449;&#24687;&#26356;&#21487;&#33021;&#34987;&#22823;&#22411;&#27169;&#22411;&#32435;&#20837;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#20256;&#25773;&#20013;&#65292;&#22823;&#22411;&#27169;&#22411;&#22312;&#20256;&#36882;&#20449;&#24687;&#26102;&#20542;&#21521;&#20110;&#26377;&#36873;&#25321;&#22320;&#20462;&#25913;&#21644;&#20002;&#22833;&#29305;&#23450;&#20869;&#23481;</title><link>https://arxiv.org/abs/2402.11271</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22312;&#36890;&#20449;&#26102;&#20195;&#30340;&#20114;&#21160;&#65306;&#33258;&#22124;&#20351;&#24471;&#22823;&#22411;&#27169;&#22411;&#23454;&#29616;&#23616;&#37096;&#26368;&#20248;
&lt;/p&gt;
&lt;p&gt;
Human-AI Interactions in the Communication Era: Autophagy Makes Large Models Achieving Local Optima
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11271
&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#20449;&#24687;&#26356;&#21487;&#33021;&#34987;&#22823;&#22411;&#27169;&#22411;&#32435;&#20837;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#20256;&#25773;&#20013;&#65292;&#22823;&#22411;&#27169;&#22411;&#22312;&#20256;&#36882;&#20449;&#24687;&#26102;&#20542;&#21521;&#20110;&#26377;&#36873;&#25321;&#22320;&#20462;&#25913;&#21644;&#20002;&#22833;&#29305;&#23450;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#31038;&#20250;&#20449;&#24687;&#22788;&#29702;&#20013;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#22686;&#21152;&#65292;&#24341;&#21457;&#20102;&#20851;&#20110;&#31038;&#20250;&#23433;&#20840;&#21644;&#20262;&#29702;&#30340;&#20105;&#35770;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#20174;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30456;&#20114;&#20316;&#29992;&#30340;&#32508;&#21512;&#35270;&#35282;&#20998;&#26512;&#36825;&#20123;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#31867;&#21644;&#22823;&#22411;&#27169;&#22411;&#22312;&#36890;&#20449;&#20013;&#20316;&#20026;&#20851;&#38190;&#32852;&#31995;&#30340;&#20559;&#35265;&#21644;&#20559;&#22909;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#30340;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#19981;&#21516;&#30340;&#23454;&#39564;&#65292;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#22312;&#20854;&#20316;&#20026;&#20449;&#24687;&#29983;&#20135;&#32773;&#21644;&#20256;&#25773;&#32773;&#30340;&#35282;&#33394;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#31361;&#20986;&#26174;&#31034;&#65292;&#21512;&#25104;&#20449;&#24687;&#26356;&#26377;&#21487;&#33021;&#34987;&#32435;&#20837;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#28040;&#24687;&#20256;&#36882;&#20013;&#65292;&#32780;&#20154;&#31867;&#29983;&#25104;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#22823;&#22411;&#27169;&#22411;&#22312;&#20197;&#20449;&#24687;&#20256;&#36882;&#32773;&#30340;&#35282;&#33394;&#26102;&#65292;&#20542;&#21521;&#20110;&#26377;&#36873;&#25321;&#22320;&#20462;&#25913;&#21644;&#20002;&#22833;&#29305;&#23450;&#20869;&#23481;&#12290;&#22312;&#27010;&#24565;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#30495;&#23454;&#30340;&#33258;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11271v1 Announce Type: new  Abstract: The increasing significance of large language and multimodal models in societal information processing has ignited debates on social safety and ethics. However, few studies have approached the analysis of these limitations from the comprehensive perspective of human and artificial intelligence system interactions. This study investigates biases and preferences when humans and large models are used as key links in communication. To achieve this, we design a multimodal dataset and three different experiments to evaluate generative models in their roles as producers and disseminators of information. Our main findings highlight that synthesized information is more likely to be incorporated into model training datasets and messaging than human-generated information. Additionally, large models, when acting as transmitters of information, tend to modify and lose specific content selectively. Conceptually, we present two realistic models of auto
&lt;/p&gt;</description></item><item><title>MoRAL&#32467;&#21512;&#20102;MoE&#30340;&#22810;&#20219;&#21153;&#33021;&#21147;&#21644;LoRA&#30340;&#24494;&#35843;&#33021;&#21147;&#65292;&#37319;&#29992;&#38382;&#31572;&#23545;&#20316;&#20026;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;LLM&#30340;&#26377;&#25928;&#32456;&#36523;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.11260</link><description>&lt;p&gt;
MoRAL: MoE&#22686;&#24378;LoRA&#29992;&#20110;LLM&#30340;&#32456;&#36523;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MoRAL: MoE Augmented LoRA for LLMs' Lifelong Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11260
&lt;/p&gt;
&lt;p&gt;
MoRAL&#32467;&#21512;&#20102;MoE&#30340;&#22810;&#20219;&#21153;&#33021;&#21147;&#21644;LoRA&#30340;&#24494;&#35843;&#33021;&#21147;&#65292;&#37319;&#29992;&#38382;&#31572;&#23545;&#20316;&#20026;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;LLM&#30340;&#26377;&#25928;&#32456;&#36523;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MoRAL&#30340;&#26041;&#27861;&#65292;&#21363;Mixture-of-Experts&#22686;&#24378;&#20302;&#31209;&#36866;&#24212;&#24615;&#29992;&#20110;LLM&#30340;&#32456;&#36523;&#23398;&#20064;&#12290; MoRAL&#23558;MoE&#30340;&#22810;&#20219;&#21153;&#33021;&#21147;&#19982;LoRA&#30340;&#24494;&#35843;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;LLM&#30340;&#26377;&#25928;&#32456;&#36523;&#23398;&#20064;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;MoRAL&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#38382;&#31572;&#23545;&#20316;&#20026;&#36755;&#20837;&#65292;&#36825;&#26159;&#19968;&#31181;&#26356;&#23454;&#29992;&#21644;&#26377;&#25928;&#30340;&#40065;&#26834;&#23398;&#20064;&#31574;&#30053;&#12290;&#37492;&#20110;&#26032;&#25968;&#25454;&#35774;&#32622;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#21363;LLM&#30340;&#32456;&#36523;&#23398;&#20064;&#65288;5L-bench&#65289;&#65292;&#21253;&#25324;&#19968;&#20010;&#26032;&#30340;&#31934;&#24515;&#31574;&#21010;&#30340;&#38382;&#31572;&#23545;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#32452;&#29992;&#20110;&#22312;&#24320;&#25918;&#24335;&#21644;&#23553;&#38381;&#24335;&#29615;&#22659;&#19979;&#20005;&#26684;&#35780;&#20272;MoRAL&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;&#24320;&#25918;&#24335;&#29615;&#22659;&#19979;&#65292;LLM&#22312;&#30701;&#26102;&#38388;&#20869;&#36805;&#36895;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11260v1 Announce Type: cross  Abstract: Adapting large language models (LLMs) to new domains/tasks and enabling them to be efficient lifelong learners is a pivotal challenge. In this paper, we propose MoRAL, i.e., Mixture-of-Experts augmented Low-Rank Adaptation for Lifelong Learning. MoRAL combines the multi-tasking abilities of MoE with the fine-tuning abilities of LoRA for effective life-long learning of LLMs. In contrast to the conventional approaches that use factual triplets as inputs MoRAL relies on simple question-answer pairs, which is a more practical and effective strategy for robust and efficient learning. Owing to new data settings, we introduce a new evaluation benchmark namely: Life Long Learning of LLM (5L-bench) encompassing a newly curated dataset of question-answer pairs, and a set of evaluation metrics for rigorous evaluation of MoRAL in open-book and closed-book settings. Experimental evaluation shows (i) LLMs learn fast in open-book settings with up to 
&lt;/p&gt;</description></item><item><title>C-ICL&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#26679;&#26412;&#26500;&#24314;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#33539;&#30340;&#26032;&#39062;&#23569;&#26679;&#26412;&#25216;&#26415;&#65292;&#36890;&#36807;&#25552;&#31034;&#19981;&#20165;&#21253;&#21547;&#27491;&#26679;&#26412;&#36824;&#21253;&#21547;&#32972;&#21518;&#25512;&#29702;&#65292;&#22686;&#24378;&#20102;LLMs&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.11254</link><description>&lt;p&gt;
C-ICL&#65306;&#23545;&#27604;&#19978;&#19979;&#25991;&#23398;&#20064;&#29992;&#20110;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
C-ICL: Contrastive In-context Learning for Information Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11254
&lt;/p&gt;
&lt;p&gt;
C-ICL&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#26679;&#26412;&#26500;&#24314;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#33539;&#30340;&#26032;&#39062;&#23569;&#26679;&#26412;&#25216;&#26415;&#65292;&#36890;&#36807;&#25552;&#31034;&#19981;&#20165;&#21253;&#21547;&#27491;&#26679;&#26412;&#36824;&#21253;&#21547;&#32972;&#21518;&#25512;&#29702;&#65292;&#22686;&#24378;&#20102;LLMs&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#20110;&#25506;&#32034;&#20808;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#19987;&#27880;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#30740;&#31350;&#20154;&#21592;&#27491;&#36890;&#36807;LLMs&#36827;&#34892;&#23569;&#26679;&#26412;&#20449;&#24687;&#25277;&#21462;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20182;&#20204;&#24448;&#24448;&#21482;&#19987;&#27880;&#20110;&#20351;&#29992;&#27491;&#30830;&#25110;&#27491;&#21521;&#31034;&#20363;&#26469;&#23637;&#31034;&#65292;&#32780;&#24573;&#35270;&#20102;&#23558;&#19981;&#27491;&#30830;&#25110;&#36127;&#21521;&#31034;&#20363;&#32435;&#20837;&#23398;&#20064;&#36807;&#31243;&#30340;&#28508;&#22312;&#20215;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;C-ICL&#65292;&#19968;&#31181;&#21033;&#29992;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#26679;&#26412;&#26500;&#24314;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#33539;&#30340;&#26032;&#39062;&#23569;&#26679;&#26412;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#19981;&#20165;&#21253;&#21547;&#27491;&#26679;&#26412;&#36824;&#21253;&#21547;&#32972;&#21518;&#25512;&#29702;&#30340;&#25552;&#31034;&#65292;&#22686;&#24378;&#20102;LLMs&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11254v1 Announce Type: new  Abstract: Recently, there has been increasing interest in exploring the capabilities of advanced large language models (LLMs) in the field of information extraction (IE), specifically focusing on tasks related to named entity recognition (NER) and relation extraction (RE). Although researchers are exploring the use of few-shot information extraction through in-context learning with LLMs, they tend to focus only on using correct or positive examples for demonstration, neglecting the potential value of incorporating incorrect or negative examples into the learning process. In this paper, we present c-ICL, a novel few-shot technique that leverages both correct and incorrect sample constructions to create in-context learning demonstrations. This approach enhances the ability of LLMs to extract entities and relations by utilizing prompts that incorporate not only the positive samples but also the reasoning behind them. This method allows for the identi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#36890;&#36807;&#22686;&#21152;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#35757;&#32451;&#19968;&#20010;&#21516;&#26102;&#20805;&#24403;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#22522;&#20110;&#25919;&#31574;&#23398;&#20064;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11253</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25919;&#31574;&#30340;&#33258;&#25105;&#21028;&#26029;&#26469;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models by On-Policy Self-Judgment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#36890;&#36807;&#22686;&#21152;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#35757;&#32451;&#19968;&#20010;&#21516;&#26102;&#20805;&#24403;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#22522;&#20110;&#25919;&#31574;&#23398;&#20064;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65292;&#29616;&#26377;&#30740;&#31350;&#35201;&#20040;&#21033;&#29992;&#21333;&#29420;&#30340;&#22870;&#21169;&#27169;&#22411;&#65288;RM&#65289;&#25191;&#34892;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#65292;&#35201;&#20040;&#36890;&#36807;&#25918;&#24323;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#21644;&#23545;&#29420;&#31435;RM&#30340;&#38656;&#27714;&#31616;&#21270;&#35757;&#32451;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23545;&#40784;&#26694;&#26550;SELF-JUDGE&#65292;&#23427;&#26082;&#26159;(1) &#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#65292;&#21448;&#26159;(2) &#21442;&#25968;&#39640;&#25928;&#30340;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;RM&#26469;&#35780;&#20272;&#26679;&#26412;&#36827;&#34892;&#22522;&#20110;&#25919;&#31574;&#30340;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#24335;&#30417;&#30563;&#24494;&#35843;&#65288;JSFT&#65289;&#26469;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#65292;&#20316;&#20026;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#19968;&#23545;&#19968;&#21028;&#26029;&#20219;&#21153;&#35270;&#20026;&#25351;&#23548;&#24335;&#20219;&#21153;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20174;&#21709;&#24212;&#23545;&#20013;&#36873;&#25321;&#26356;&#22909;&#30340;&#21709;&#24212;&#12290;&#22240;&#27492;&#65292;&#24471;&#21040;&#30340;&#27169;&#22411;&#21487;&#20197;&#35780;&#21028;&#24403;&#21069;&#31574;&#30053;&#30340;&#21363;&#26102;&#21709;&#24212;&#20559;&#22909;&#65292;&#20174;&#33258;&#36523;&#21021;&#22987;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;SELF-JUDGE&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11253v1 Announce Type: cross  Abstract: To align large language models with human preferences, existing research either utilizes a separate reward model (RM) to perform on-policy learning or simplifies the training procedure by discarding the on-policy learning and the need for a separate RM. In this paper, we present a novel alignment framework, SELF-JUDGE that is (1) on-policy learning and 2) parameter efficient, as it does not require an additional RM for evaluating the samples for on-policy learning. To this end, we propose Judge-augmented Supervised Fine-Tuning (JSFT) to train a single model acting as both a policy and a judge. Specifically, we view the pairwise judgment task as a special case of the instruction-following task, choosing the better response from a response pair. Thus, the resulting model can judge preferences of on-the-fly responses from current policy initialized from itself. Experimental results show the efficacy of SELF-JUDGE, outperforming baselines 
&lt;/p&gt;</description></item><item><title>LLM&#36890;&#36807;&#36229;&#21442;&#25968;&#24863;&#30693;&#29983;&#25104;&#23454;&#29616;&#33258;&#25105;&#35843;&#33410;&#65292;&#28040;&#38500;&#20102;&#22823;&#37327;&#25163;&#21160;&#35843;&#25972;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.11251</link><description>&lt;p&gt;
LLM&#21487;&#20197;&#36890;&#36807;&#36229;&#21442;&#25968;&#24863;&#30693;&#29983;&#25104;&#23454;&#29616;&#33258;&#25105;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
LLM can Achieve Self-Regulation via Hyperparameter Aware Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11251
&lt;/p&gt;
&lt;p&gt;
LLM&#36890;&#36807;&#36229;&#21442;&#25968;&#24863;&#30693;&#29983;&#25104;&#23454;&#29616;&#33258;&#25105;&#35843;&#33410;&#65292;&#28040;&#38500;&#20102;&#22823;&#37327;&#25163;&#21160;&#35843;&#25972;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#65292;&#29992;&#25143;&#36890;&#24120;&#37319;&#29992;&#19981;&#21516;&#30340;&#35299;&#30721;&#31574;&#30053;&#24182;&#35843;&#25972;&#36229;&#21442;&#25968;&#20197;&#25511;&#21046;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;LLMs&#26159;&#21542;&#24847;&#35782;&#21040;&#36825;&#20123;&#35299;&#30721;&#31574;&#30053;&#30340;&#23384;&#22312;&#24182;&#19988;&#33021;&#22815;&#33258;&#25105;&#35843;&#33410;&#65311;&#24403;&#21069;&#30340;&#35299;&#30721;&#29983;&#25104;&#36807;&#31243;&#36890;&#24120;&#20381;&#36182;&#20110;&#26681;&#25454;&#19981;&#21516;&#20219;&#21153;&#21644;&#38656;&#27714;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#32463;&#39564;&#21644;&#21551;&#21457;&#24335;&#25163;&#21160;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#36890;&#24120;&#24456;&#32321;&#29712;&#65292;&#35299;&#30721;&#36229;&#21442;&#25968;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#23545;&#27599;&#20010;&#26679;&#26412;&#37117;&#26159;&#26368;&#20339;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36229;&#21442;&#25968;&#24863;&#30693;&#29983;&#25104;&#65288;HAG&#65289;&#30340;&#26032;&#39062;&#25991;&#26412;&#29983;&#25104;&#33539;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#36229;&#21442;&#25968;&#24863;&#30693;&#25351;&#20196;&#35843;&#25972;&#65292;LLM&#21487;&#20197;&#33258;&#20027;&#30830;&#23450;&#22522;&#20110;&#36755;&#20837;&#26679;&#26412;&#30340;&#26368;&#20339;&#35299;&#30721;&#31574;&#30053;&#21644;&#37197;&#32622;&#65292;&#23454;&#29616;&#33258;&#25105;&#35843;&#33410;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#22823;&#37327;&#25163;&#21160;&#35843;&#25972;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11251v1 Announce Type: new  Abstract: In the realm of Large Language Models (LLMs), users commonly employ diverse decoding strategies and adjust hyperparameters to control the generated text. However, a critical question emerges: Are LLMs conscious of the existence of these decoding strategies and capable of regulating themselves? The current decoding generation process often relies on empirical and heuristic manual adjustments to hyperparameters based on types of tasks and demands. However, this process is typically cumbersome, and the decoding hyperparameters may not always be optimal for each sample. To address the aforementioned challenges, we propose a novel text generation paradigm termed Hyperparameter Aware Generation (HAG). By leveraging hyperparameter-aware instruction tuning, the LLM autonomously determines the optimal decoding strategy and configs based on the input samples, enabling self-regulation. Our approach eliminates the need for extensive manual tuning, o
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20851;&#31995;&#22411;&#35770;&#35777;&#25366;&#25496;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#33021;&#22815;&#26174;&#33879;&#36229;&#36807;&#30446;&#21069;&#26368;&#20339;&#22522;&#20934;&#32447;&#65292;&#24182;&#19988;&#22312;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.11243</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#22522;&#20110;&#20851;&#31995;&#30340;&#35770;&#35777;&#25366;&#25496;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models perform Relation-based Argument Mining?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11243
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20851;&#31995;&#22411;&#35770;&#35777;&#25366;&#25496;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#33021;&#22815;&#26174;&#33879;&#36229;&#36807;&#30446;&#21069;&#26368;&#20339;&#22522;&#20934;&#32447;&#65292;&#24182;&#19988;&#22312;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25454;&#25366;&#25496;&#65288;AM&#65289;&#26159;&#20174;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#35770;&#25454;&#12289;&#23427;&#20204;&#30340;&#32452;&#25104;&#37096;&#20998;&#21644;/&#25110;&#35770;&#25454;&#21644;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#20851;&#31995;&#30340;&#36807;&#31243;&#12290;&#38543;&#30528;&#25903;&#25345;&#22312;&#32447;&#36777;&#35770;&#30340;&#24179;&#21488;&#25968;&#37327;&#19981;&#26029;&#22686;&#21152;&#65292;&#23545;AM&#30340;&#38656;&#27714;&#21464;&#24471;&#24840;&#21457;&#36843;&#20999;&#65292;&#29305;&#21035;&#26159;&#20026;&#20102;&#25903;&#25345;&#19979;&#28216;&#20219;&#21153;&#12290;&#22522;&#20110;&#20851;&#31995;&#30340;AM&#65288;RbAM&#65289;&#26159;&#19968;&#31181;&#20851;&#27880;&#35782;&#21035;&#35770;&#25454;&#20043;&#38388;&#21327;&#35758;&#65288;&#25903;&#25345;&#65289;&#21644;&#19981;&#21516;&#24847;&#65288;&#25915;&#20987;&#65289;&#20851;&#31995;&#30340;AM&#24418;&#24335;&#12290;RbAM&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20196;&#20154;&#28385;&#24847;&#22320;&#25191;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#29992;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#32463;&#36807;&#36866;&#24403;&#30340;&#35843;&#25972;&#21644;&#25552;&#31034;&#65292;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#34920;&#29616;&#26368;&#22909;&#30340;&#65288;&#22522;&#20110;RoBERTa&#30340;&#65289;&#22522;&#20934;&#32447;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#24320;&#28304;LLM&#65288;Llama-2&#21644;Mistral&#65289;&#22312;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11243v1 Announce Type: cross  Abstract: Argument mining (AM) is the process of automatically extracting arguments, their components and/or relations amongst arguments and components from text. As the number of platforms supporting online debate increases, the need for AM becomes ever more urgent, especially in support of downstream tasks. Relation-based AM (RbAM) is a form of AM focusing on identifying agreement (support) and disagreement (attack) relations amongst arguments. RbAM is a challenging classification task, with existing methods failing to perform satisfactorily. In this paper, we show that general-purpose Large Language Models (LLMs), appropriately primed and prompted, can significantly outperform the best performing (RoBERTa-based) baseline. Specifically, we experiment with two open-source LLMs (Llama-2 and Mistral) with ten datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DATG&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21160;&#24577;&#23646;&#24615;&#22270;&#35843;&#33410;&#20851;&#38190;&#23646;&#24615;&#35789;&#21644;&#20851;&#38190;&#21453;&#23646;&#24615;&#35789;&#30340;&#21457;&#29983;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#23646;&#24615;&#25511;&#21046;&#65292;&#22312;&#27602;&#24615;&#32531;&#35299;&#21644;&#24773;&#24863;&#36716;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;19.29%&#30340;&#25511;&#21046;&#20934;&#30830;&#24615;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.11218</link><description>&lt;p&gt;
&#20351;&#29992;&#21160;&#24577;&#23646;&#24615;&#22270;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Controlled Text Generation for Large Language Model with Dynamic Attribute Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11218
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DATG&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21160;&#24577;&#23646;&#24615;&#22270;&#35843;&#33410;&#20851;&#38190;&#23646;&#24615;&#35789;&#21644;&#20851;&#38190;&#21453;&#23646;&#24615;&#35789;&#30340;&#21457;&#29983;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#23646;&#24615;&#25511;&#21046;&#65292;&#22312;&#27602;&#24615;&#32531;&#35299;&#21644;&#24773;&#24863;&#36716;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;19.29%&#30340;&#25511;&#21046;&#20934;&#30830;&#24615;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#65288;CTG&#65289;&#26088;&#22312;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#25152;&#38656;&#23646;&#24615;&#30340;&#25991;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#25554;&#25300;&#30340;CTG&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21517;&#20026;&#22522;&#20110;&#21160;&#24577;&#23646;&#24615;&#22270;&#30340;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#65288;DATG&#65289;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#23646;&#24615;&#35780;&#20998;&#22120;&#35780;&#20272;&#30001;LLMs&#29983;&#25104;&#30340;&#21477;&#23376;&#30340;&#23646;&#24615;&#65292;&#24182;&#26500;&#24314;&#21160;&#24577;&#23646;&#24615;&#22270;&#12290;DATG&#35843;&#33410;&#20851;&#38190;&#23646;&#24615;&#35789;&#21644;&#20851;&#38190;&#21453;&#23646;&#24615;&#35789;&#30340;&#21457;&#29983;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#23646;&#24615;&#25511;&#21046;&#65292;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#30340;&#21407;&#22987;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#36328;&#22235;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65306;&#27602;&#24615;&#32531;&#35299;&#21644;&#24773;&#24863;&#36716;&#21270;&#65292;&#21033;&#29992;&#20116;&#20010;LLMs&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#20986;&#20102;&#22312;&#25511;&#21046;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26174;&#30528;&#25552;&#39640;&#65292;&#23454;&#29616;&#20102;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#20013;&#26368;&#26377;&#21033;&#30340;&#20219;&#21153;&#20013;&#23545;&#22522;&#32447;&#26041;&#27861;&#30340;&#23792;&#20540;&#25913;&#36827;&#20026;19.29&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26174;&#30528;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11218v1 Announce Type: new  Abstract: Controlled Text Generation (CTG) aims to produce texts that exhibit specific desired attributes. In this study, we introduce a pluggable CTG framework for Large Language Models (LLMs) named Dynamic Attribute Graphs-based controlled text generation (DATG). This framework utilizes an attribute scorer to evaluate the attributes of sentences generated by LLMs and constructs dynamic attribute graphs. DATG modulates the occurrence of key attribute words and key anti-attribute words, achieving effective attribute control without compromising the original capabilities of the model. We conduct experiments across four datasets in two tasks: toxicity mitigation and sentiment transformation, employing five LLMs as foundational models. Our findings highlight a remarkable enhancement in control accuracy, achieving a peak improvement of 19.29% over baseline methods in the most favorable task across four datasets. Additionally, we observe a significant 
&lt;/p&gt;</description></item><item><title>Asclepius&#26159;&#19968;&#20010;&#26032;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#65292;&#26088;&#22312;&#20026;&#21487;&#20449;&#30340;Med-MLLMs&#35780;&#20272;&#25552;&#20379;&#21333;&#29420;&#19988;&#20020;&#24202;&#20195;&#34920;&#24615;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.11217</link><description>&lt;p&gt;
Asclepius&#65306;&#29992;&#20110;&#21307;&#23398;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#39057;&#35889;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11217
&lt;/p&gt;
&lt;p&gt;
Asclepius&#26159;&#19968;&#20010;&#26032;&#30340;&#21307;&#23398;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#65292;&#26088;&#22312;&#20026;&#21487;&#20449;&#30340;Med-MLLMs&#35780;&#20272;&#25552;&#20379;&#21333;&#29420;&#19988;&#20020;&#24202;&#20195;&#34920;&#24615;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11217v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#21307;&#23398;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;Med-MLLMs&#65289;&#30340;&#37325;&#22823;&#31361;&#30772;&#36890;&#36807;&#24378;&#22823;&#30340;&#20449;&#24687;&#32508;&#21512;&#21644;&#21307;&#30103;&#20915;&#31574;&#25903;&#25345;&#25913;&#36896;&#20102;&#29616;&#20195;&#21307;&#30103;&#20445;&#20581;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#35786;&#26029;&#26694;&#26550;&#30340;&#22797;&#26434;&#24615;&#28085;&#30422;&#20102;&#21508;&#31181;&#21307;&#23398;&#19987;&#19994;&#65292;&#24182;&#28041;&#21450;&#22797;&#26434;&#30340;&#20020;&#24202;&#20915;&#31574;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#19981;&#36866;&#21512;Med-MLLMs&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;Med-MLLMs&#26159;&#22312;&#22823;&#37327;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#36825;&#20123;&#22522;&#20934;&#23481;&#26131;&#20986;&#29616;&#25968;&#25454;&#27844;&#38706;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20010;&#29420;&#31435;&#19988;&#20020;&#24202;&#20195;&#34920;&#24615;&#30340;&#22522;&#20934;&#29992;&#20110;&#21487;&#20449;&#30340;Med-MLLMs&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Asclepius&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;Med-MLLM&#22522;&#20934;&#65292;&#20005;&#26684;&#21644;&#20840;&#38754;&#35780;&#20272;&#27169;&#22411;&#22312;&#19981;&#21516;&#21307;&#23398;&#19987;&#19994;&#65288;&#24515;&#34880;&#31649;&#12289;&#32963;&#32928;&#31561;&#65289;&#21644;&#19981;&#21516;&#35786;&#26029;&#33021;&#21147;&#65288;&#30693;&#35273;&#12289;&#30142;&#30149;&#20998;&#26512;&#31561;&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11217v1 Announce Type: new  Abstract: The significant breakthroughs of Medical Multi-Modal Large Language Models (Med-MLLMs) renovate modern healthcare with robust information synthesis and medical decision support. However, these models are often evaluated on benchmarks that are unsuitable for the Med-MLLMs due to the intricate nature of the real-world diagnostic frameworks, which encompass diverse medical specialties and involve complex clinical decisions. Moreover, these benchmarks are susceptible to data leakage, since Med-MLLMs are trained on large assemblies of publicly available data. Thus, an isolated and clinically representative benchmark is highly desirable for credible Med-MLLMs evaluation. To this end, we introduce Asclepius, a novel Med-MLLM benchmark that rigorously and comprehensively assesses model capability in terms of: distinct medical specialties (cardiovascular, gastroenterology, etc.) and different diagnostic capacities (perception, disease analysis, e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#35843;&#26597;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#38754;&#20020;&#30340;&#21518;&#38376;&#25915;&#20987;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#33324;&#26694;&#26550;&#21644;&#19981;&#21516;&#24418;&#24335;&#30340;&#21518;&#38376;&#25915;&#20987;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.11208</link><description>&lt;p&gt;
&#35686;&#24789;&#24744;&#30340;&#20195;&#29702;&#20154;&#65281;&#35843;&#26597;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30340;&#21518;&#38376;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11208
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35843;&#26597;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#38754;&#20020;&#30340;&#21518;&#38376;&#25915;&#20987;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#33324;&#26694;&#26550;&#21644;&#19981;&#21516;&#24418;&#24335;&#30340;&#21518;&#38376;&#25915;&#20987;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLM&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#29992;&#20110;&#22788;&#29702;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#65288;&#21253;&#25324;&#37329;&#34701;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#36141;&#29289;&#31561;&#65289;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#12290;&#22312;&#24212;&#29992;&#36807;&#31243;&#20013;&#30830;&#20445;LLM&#20195;&#29702;&#20154;&#30340;&#21487;&#38752;&#24615;&#21644;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;LLM&#20195;&#29702;&#20154;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#24037;&#20316;&#39318;&#27425;&#25506;&#35752;&#20102;&#20856;&#22411;&#23433;&#20840;&#23041;&#32961;&#20043;&#19968;&#65292;&#21363;&#23545;LLM&#20195;&#29702;&#20154;&#30340;&#21518;&#38376;&#25915;&#20987;&#12290;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#20102;&#19968;&#20010;&#20195;&#29702;&#20154;&#21518;&#38376;&#25915;&#20987;&#30340;&#19968;&#33324;&#26694;&#26550;&#65292;&#28982;&#21518;&#23545;&#19981;&#21516;&#24418;&#24335;&#30340;&#20195;&#29702;&#20154;&#21518;&#38376;&#25915;&#20987;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20174;&#26368;&#32456;&#25915;&#20987;&#32467;&#26524;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36873;&#25321;&#25805;&#32437;&#26368;&#32456;&#36755;&#20986;&#20998;&#24067;&#65292;&#25110;&#32773;&#20165;&#22312;&#20013;&#38388;&#25512;&#29702;&#36807;&#31243;&#20013;&#24341;&#20837;&#24694;&#24847;&#34892;&#20026;&#65292;&#21516;&#26102;&#20445;&#25345;&#26368;&#32456;&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#21069;&#19968;&#31867;&#21487;&#20197;&#20998;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11208v1 Announce Type: cross  Abstract: Leveraging the rapid development of Large Language Models LLMs, LLM-based agents have been developed to handle various real-world applications, including finance, healthcare, and shopping, etc. It is crucial to ensure the reliability and security of LLM-based agents during applications. However, the safety issues of LLM-based agents are currently under-explored. In this work, we take the first step to investigate one of the typical safety threats, backdoor attack, to LLM-based agents. We first formulate a general framework of agent backdoor attacks, then we present a thorough analysis on the different forms of agent backdoor attacks. Specifically, from the perspective of the final attacking outcomes, the attacker can either choose to manipulate the final output distribution, or only introduce malicious behavior in the intermediate reasoning process, while keeping the final output correct. Furthermore, the former category can be divided
&lt;/p&gt;</description></item><item><title>ChatGPT&#20316;&#20026;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#19981;&#26029;&#25361;&#25112;&#20256;&#32479;&#33539;&#24335;&#65292;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-3&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11203</link><description>&lt;p&gt;
&#25506;&#32034;ChatGPT&#22312;&#19979;&#19968;&#20195;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24212;&#29992;&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring ChatGPT for Next-generation Information Retrieval: Opportunities and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11203
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#19981;&#26029;&#25361;&#25112;&#20256;&#32479;&#33539;&#24335;&#65292;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#21516;&#26102;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;GPT-3&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#20984;&#26174;&#20102;ChatGPT&#20316;&#20026;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#19982;&#20043;&#21069;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;ChatGPT&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#22909;&#22788;&#65292;&#21560;&#24341;&#20102;&#34892;&#19994;&#21644;&#23398;&#26415;&#30028;&#30340;&#20851;&#27880;&#12290;&#19968;&#20123;&#20154;&#35748;&#20026;ChatGPT&#26159;&#19968;&#39033;&#24320;&#21019;&#24615;&#30340;&#21019;&#26032;&#65292;&#32780;&#21478;&#19968;&#20123;&#20154;&#23558;&#20854;&#25104;&#21151;&#24402;&#22240;&#20110;&#20135;&#21697;&#24320;&#21457;&#21644;&#24066;&#22330;&#31574;&#30053;&#30340;&#26377;&#25928;&#25972;&#21512;&#12290;ChatGPT&#30340;&#20986;&#29616;&#65292;&#20197;&#21450;&#19982;OpenAI&#30340;GPT-4&#19968;&#36215;&#65292;&#26631;&#24535;&#30528;&#29983;&#25104;&#24335;AI&#30340;&#26032;&#38454;&#27573;&#65292;&#20135;&#29983;&#30340;&#20869;&#23481;&#19982;&#35757;&#32451;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#36229;&#36234;&#20102;&#20197;&#24448;&#30340;GPT-3&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#19982;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#20256;&#32479;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;ChatGPT&#25361;&#25112;&#20102;&#29616;&#26377;&#30340;&#33539;&#24335;&#65292;&#24102;&#26469;&#20102;&#20851;&#20110;&#25991;&#26412;&#36136;&#37327;&#20445;&#35777;&#12289;&#27169;&#22411;&#20559;&#24046;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#26032;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;ChatGPT&#23545;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11203v1 Announce Type: cross  Abstract: The rapid advancement of artificial intelligence (AI) has highlighted ChatGPT as a pivotal technology in the field of information retrieval (IR). Distinguished from its predecessors, ChatGPT offers significant benefits that have attracted the attention of both the industry and academic communities. While some view ChatGPT as a groundbreaking innovation, others attribute its success to the effective integration of product development and market strategies. The emergence of ChatGPT, alongside GPT-4, marks a new phase in Generative AI, generating content that is distinct from training examples and exceeding the capabilities of the prior GPT-3 model by OpenAI. Unlike the traditional supervised learning approach in IR tasks, ChatGPT challenges existing paradigms, bringing forth new challenges and opportunities regarding text quality assurance, model bias, and efficiency. This paper seeks to examine the impact of ChatGPT on IR tasks and offe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CoT&#25512;&#29702;&#33021;&#21147;&#35780;&#20272;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#25512;&#29702;&#30693;&#35782;&#21644;&#29983;&#25104;CoT&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;</title><link>https://arxiv.org/abs/2402.11199</link><description>&lt;p&gt;
&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#23545;&#22810;&#36339;&#25512;&#29702;&#20013;&#24605;&#32500;&#38142;&#30340;&#30452;&#25509;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Direct Evaluation of Chain-of-Thought in Multi-hop Reasoning with Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11199
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CoT&#25512;&#29702;&#33021;&#21147;&#35780;&#20272;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#25512;&#29702;&#30693;&#35782;&#21644;&#29983;&#25104;CoT&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24403;&#25552;&#31034;&#29983;&#25104;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#35299;&#37322;&#26102;&#65292;&#20197;&#21450;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#20851;&#20110;LLMs&#35780;&#20272;&#30340;&#30740;&#31350;&#20165;&#20851;&#27880;&#31572;&#26696;&#20934;&#30830;&#24615;&#65292;&#24573;&#30053;&#20102;&#29983;&#25104;&#30340;CoT&#30340;&#27491;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#65292;&#28145;&#20837;&#25506;&#35752;LLMs&#22312;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;CoT&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36776;&#21035;&#24335;&#21644;&#29983;&#25104;&#24335;CoT&#35780;&#20272;&#33539;&#24335;&#65292;&#20197;&#35780;&#20272;LLMs&#30340;&#25512;&#29702;&#30693;&#35782;&#21644;&#29983;&#25104;CoT&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#22312;2&#20010;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#19978;&#23545;5&#20010;&#19981;&#21516;&#31995;&#21015;&#30340;LLMs&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#20855;&#26377;&#36275;&#22815;&#30340;&#30693;&#35782;&#26469;&#25191;&#34892;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;LLMs&#29983;&#25104;&#30340;CoT&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#19982;&#31572;&#26696;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#34920;&#26126;&#23427;&#20204;&#32463;&#24120;&#36890;&#36807;&#38169;&#35823;&#30340;&#26041;&#24335;&#24471;&#20986;&#27491;&#30830;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11199v1 Announce Type: new  Abstract: Large language models (LLMs) demonstrate strong reasoning abilities when prompted to generate chain-of-thought (CoT) explanations alongside answers. However, previous research on evaluating LLMs has solely focused on answer accuracy, neglecting the correctness of the generated CoT. In this paper, we delve deeper into the CoT reasoning capabilities of LLMs in multi-hop question answering by utilizing knowledge graphs (KGs). We propose a novel discriminative and generative CoT evaluation paradigm to assess LLMs' knowledge of reasoning and the accuracy of the generated CoT. Through experiments conducted on 5 different families of LLMs across 2 multi-hop question-answering datasets, we find that LLMs possess sufficient knowledge to perform reasoning. However, there exists a significant disparity between answer accuracy and faithfulness of the CoT reasoning generated by LLMs, indicating that they often arrive at correct answers through incorr
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36136;&#24515;&#30340;MBR&#35299;&#30721;&#26041;&#27861;&#25552;&#39640;&#20102;&#35299;&#30721;&#36895;&#24230;&#21644;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.11197</link><description>&lt;p&gt;
&#22522;&#20110;&#36136;&#24515;&#30340;&#39640;&#25928;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Centroid-Based Efficient Minimum Bayes Risk Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11197
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36136;&#24515;&#30340;MBR&#35299;&#30721;&#26041;&#27861;&#25552;&#39640;&#20102;&#35299;&#30721;&#36895;&#24230;&#21644;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#36890;&#36807;&#20351;&#29992;COMET&#23454;&#29616;&#20102;&#19968;&#27969;&#30340;&#32763;&#35793;&#24615;&#33021;&#65292;&#35813;&#31070;&#32463;&#24230;&#37327;&#19982;&#20154;&#31867;&#35780;&#20272;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;MBR&#35299;&#30721;&#38656;&#35201;&#20108;&#27425;&#26102;&#38388;&#65292;&#22240;&#20026;&#23427;&#35745;&#31639;&#32763;&#35793;&#20551;&#35774;&#19982;&#25152;&#26377;&#21442;&#32771;&#32763;&#35793;&#20043;&#38388;&#30340;&#26399;&#26395;&#20998;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36136;&#24515;&#30340;MBR&#65288;CBMBR&#65289;&#35299;&#30721;&#20197;&#25552;&#39640;MBR&#35299;&#30721;&#30340;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#23545;&#21442;&#32771;&#32763;&#35793;&#36827;&#34892;&#32858;&#31867;&#65292;&#28982;&#21518;&#20351;&#29992;&#27599;&#20010;&#31751;&#30340;&#36136;&#24515;&#35745;&#31639;&#20998;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;CBMBR&#19981;&#20165;&#23558;&#26399;&#26395;&#20998;&#25968;&#35745;&#31639;&#30340;&#35299;&#30721;&#36895;&#24230;&#25552;&#39640;&#20102;6.9&#20493;&#65292;&#32780;&#19988;&#22312;WMT'22 En$\leftrightarrow$Ja&#12289;En$\leftrightarrow$De&#12289;En$\leftrightarrow$Zh&#20197;&#21450;WMT'23 En$\leftrightarrow$Ja&#32763;&#35793;&#20219;&#21153;&#20013;&#65292;&#22312;&#32763;&#35793;&#36136;&#37327;&#19978;&#36229;&#36807;&#20102;&#39321;&#33609;MBR&#35299;&#30721;&#65292;&#26368;&#39640;&#25552;&#39640;&#20102;0.5 COMET&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11197v1 Announce Type: new  Abstract: Minimum Bayes risk (MBR) decoding achieved state-of-the-art translation performance by using COMET, a neural metric that has a high correlation with human evaluation. However, MBR decoding requires quadratic time since it computes the expected score between a translation hypothesis and all reference translations. We propose centroid-based MBR (CBMBR) decoding to improve the speed of MBR decoding. Our method clusters the reference translations in the feature space, and then calculates the score using the centroids of each cluster. The experimental results show that our CBMBR not only improved the decoding speed of the expected score calculation 6.9 times, but also outperformed vanilla MBR decoding in translation quality by up to 0.5 COMET in the WMT'22 En$\leftrightarrow$Ja, En$\leftrightarrow$De, En$\leftrightarrow$Zh, and WMT'23 En$\leftrightarrow$Ja translation tasks.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;LLMs&#22312;&#37329;&#34701;&#34920;&#26684;&#38382;&#31572;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#25552;&#31034;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#24615;&#33021;&#19978;&#32988;&#36807;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;</title><link>https://arxiv.org/abs/2402.11194</link><description>&lt;p&gt;
&#22312;&#37329;&#34701;&#25991;&#26723;&#38382;&#31572;&#20013;&#35780;&#20272;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Assessing LLMs' Mathematical Reasoning in Financial Document Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11194
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23454;&#39564;&#35780;&#20272;&#20102;LLMs&#22312;&#37329;&#34701;&#34920;&#26684;&#38382;&#31572;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#21457;&#29616;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#25552;&#31034;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#24615;&#33021;&#19978;&#32988;&#36807;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#22312;&#20855;&#26377;&#32467;&#26500;&#21270;&#34920;&#26684;&#21644;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#28151;&#21512;&#30340;&#22797;&#26434;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#19981;&#30830;&#23450;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#22312;&#22235;&#20010;&#37329;&#34701;&#34920;&#26684;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65306;TATQA&#12289;FinQA&#12289;ConvFinQA&#21644;Multihiertt&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#27169;&#22411;&#21644;&#25552;&#31034;&#25216;&#26415;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LLMs&#22914;&#20309;&#36866;&#24212;&#22797;&#26434;&#34920;&#26684;&#21644;&#25968;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#20851;&#27880;&#23545;&#34920;&#26684;&#22797;&#26434;&#24615;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#22312;&#22686;&#21152;&#31639;&#26415;&#25512;&#29702;&#27493;&#39588;&#25968;&#37327;&#26102;&#24615;&#33021;&#21464;&#21270;&#12290;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#22788;&#29702;&#21322;&#32467;&#26500;&#21270;&#34920;&#26684;&#20013;&#22797;&#26434;&#25968;&#23398;&#22330;&#26223;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#21322;&#32467;&#26500;&#21270;&#25991;&#26723;&#30340;&#26032;&#22411;&#25552;&#31034;&#25216;&#26415;&#65292;&#22312;&#24615;&#33021;&#26041;&#38754;&#19982;&#20854;&#20182;&#22522;&#32447;&#30456;&#21305;&#37197;&#25110;&#32988;&#36807;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;LLMs&#33021;&#21147;&#30340;&#24494;&#22937;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11194v1 Announce Type: new  Abstract: Large Language Models (LLMs), excel in natural language understanding, but their capability for complex mathematical reasoning with an amalgamation of structured tables and unstructured text is uncertain. This study explores LLMs' mathematical reasoning on four financial tabular question-answering datasets: TATQA, FinQA, ConvFinQA, and Multihiertt. Through extensive experiments with various models and prompting techniques, we assess how LLMs adapt to complex tables and mathematical tasks. We focus on sensitivity to table complexity and performance variations with an increasing number of arithmetic reasoning steps. The results provide insights into LLMs' capabilities and limitations in handling complex mathematical scenarios for semi-structured tables. Ultimately, we introduce a novel prompting technique tailored to semi-structured documents, matching or outperforming other baselines in performance while providing a nuanced understanding 
&lt;/p&gt;</description></item><item><title>&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#39118;&#26684;&#30456;&#21305;&#37197;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#24320;&#21457;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#31243;&#24230;&#22320;&#35843;&#25972;&#27169;&#22411;&#21709;&#24212;&#26469;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.11192</link><description>&lt;p&gt;
&#22914;&#26524;&#20320;&#35762;&#25105;&#30340;&#35821;&#35328;&#65292;&#25105;&#20250;&#26356;&#22909;&#22320;&#23398;&#20064;&#65306;&#20351;&#29992;&#39118;&#26684;&#23545;&#40784;&#21709;&#24212;&#35843;&#25972;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
I Learn Better If You Speak My Language: Enhancing Large Language Model Fine-Tuning with Style-Aligned Response Adjustments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11192
&lt;/p&gt;
&lt;p&gt;
&#23558;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#39118;&#26684;&#30456;&#21305;&#37197;&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#65292;&#24320;&#21457;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#31243;&#24230;&#22320;&#35843;&#25972;&#27169;&#22411;&#21709;&#24212;&#26469;&#36991;&#20813;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23567;&#25968;&#25454;&#38598;&#20026;&#29305;&#23450;&#20219;&#21153;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#19968;&#20010;&#26222;&#36941;&#36935;&#21040;&#30340;&#20294;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#22312;&#26377;&#38480;&#30340;&#31034;&#20363;&#19978;&#36807;&#22810;&#25311;&#21512;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20445;&#30041;&#21407;&#22987;&#25216;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#22320;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#23558;&#22320;&#23454;&#38469;&#21709;&#24212;&#39118;&#26684;&#19982;LLM&#22266;&#26377;&#39118;&#26684;&#21305;&#37197;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#23398;&#20064;&#32467;&#26524;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#26368;&#23567;&#31243;&#24230;&#22320;&#20462;&#25913;LLM&#30340;&#29616;&#26377;&#21709;&#24212;&#20197;&#26356;&#27491;&#38169;&#35823;&#65292;&#20351;&#29992;&#36825;&#20123;&#35843;&#25972;&#21518;&#30340;&#21709;&#24212;&#20316;&#20026;&#35757;&#32451;&#30446;&#26631;&#12290;&#36825;&#31181;&#25216;&#26415;&#33021;&#22815;&#23454;&#29616;&#19982;&#27169;&#22411;&#22266;&#26377;&#21709;&#24212;&#39118;&#26684;&#19968;&#33268;&#30340;&#31934;&#30830;&#26356;&#27491;&#65292;&#32500;&#25252;&#27169;&#22411;&#30340;&#26680;&#24515;&#33021;&#21147;&#65292;&#20174;&#32780;&#36991;&#20813;&#36807;&#22810;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;LLM&#30340;&#29305;&#23450;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#20851;&#38190;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11192v1 Announce Type: cross  Abstract: Fine-tuning large language models (LLMs) with a small data set for particular tasks is a widely encountered yet complex challenge. The potential for overfitting on a limited number of examples can negatively impact the model's ability to generalize and retain its original skills. Our research explores the impact of the style of ground-truth responses during the fine-tuning process. We found that matching the ground-truth response style with the LLM's inherent style results in better learning outcomes. Building on this insight, we developed a method that minimally alters the LLM's pre-existing responses to correct errors, using these adjusted responses as training targets. This technique enables precise corrections in line with the model's native response style, safeguarding the model's core capabilities and thus avoid overfitting. Our findings show that this approach not only improves the LLM's task-specific accuracy but also crucially
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#22810;&#38454;&#27573;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20307;&#32946;&#26032;&#38395;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11191</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#36741;&#21161;&#33258;&#21160;&#29983;&#25104;&#20307;&#32946;&#26032;&#38395;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Assisted Automatic Sports News Writing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11191
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#22810;&#38454;&#27573;&#23398;&#20064;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#20307;&#32946;&#26032;&#38395;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#20307;&#32946;&#26032;&#38395;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#19968;&#31181;&#29420;&#29305;&#30340;&#31639;&#27861;&#65292;&#20174;&#29616;&#22330;&#25991;&#26412;&#24191;&#25773;&#20013;&#25552;&#21462;&#20851;&#38190;&#26102;&#21051;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#26102;&#21051;&#21019;&#24314;&#26032;&#38395;&#30340;&#21021;&#31295;&#12290;&#36825;&#19968;&#21021;&#31295;&#36890;&#36807;&#25972;&#21512;&#19987;&#38376;&#35774;&#35745;&#30340;&#20307;&#32946;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#20851;&#38190;&#32454;&#33410;&#21644;&#32972;&#26223;&#20449;&#24687;&#24471;&#20197;&#36827;&#19968;&#27493;&#23436;&#21892;&#12290;&#35813;&#22270;&#35889;&#21253;&#21547;5,893&#20010;&#23454;&#20307;&#65292;&#20998;&#20026;&#19977;&#20010;&#19981;&#21516;&#30340;&#27010;&#24565;&#31867;&#21035;&#65292;&#36890;&#36807;&#22235;&#31181;&#20851;&#31995;&#31867;&#22411;&#30456;&#20114;&#36830;&#25509;&#65292;&#20855;&#26377;27&#20010;&#29420;&#29305;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#38454;&#27573;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#34920;&#36798;&#23454;&#20307;-&#20219;&#21153;&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#22312;&#26597;&#35810;&#38598;&#20013;&#20016;&#23500;&#23454;&#20307;&#34920;&#31034;&#12290;&#23427;&#36824;&#21253;&#25324;&#19968;&#20010;&#22788;&#29702;&#22120;&#65292;&#29992;&#20110;&#35745;&#31639;&#19981;&#23436;&#25972;&#19977;&#20803;&#32452;&#30340;&#21305;&#37197;&#20998;&#25968;&#65292;&#35299;&#20915;&#20102;&#23569;&#26679;&#26412;&#30693;&#35782;&#21294;&#20047;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11191v1 Announce Type: new  Abstract: In this paper, we present a novel method for automatically generating sports news, which employs a unique algorithm that extracts pivotal moments from live text broadcasts and uses them to create an initial draft of the news. This draft is further refined by incorporating key details and background information from a specially designed sports knowledge graph. This graph contains 5,893 entities, which are classified into three distinct conceptual categories, interconnected through four relationship types, and characterized by 27 unique attributes. In addition, we create a multi-stage learning model by combining convolutional neural networks and a transformer encoder. This model expresses entity-task interactions using convolutional neural networks and enriches entity representations in the query set with the transformer encoder. It also includes a processor to compute matching scores for incomplete triples, addressing few-shot knowledge g
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#29983;&#25104;&#30340;&#38388;&#25509;&#25506;&#27979;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;LLMs&#20013;&#30340;&#26174;&#24615;&#21644;&#38544;&#24615;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#25506;&#35752;&#20102;&#19977;&#31181;&#32531;&#35299;LLMs&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11190</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#25259;&#38706;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Disclosure and Mitigation of Gender Bias in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11190
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#29983;&#25104;&#30340;&#38388;&#25509;&#25506;&#27979;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;LLMs&#20013;&#30340;&#26174;&#24615;&#21644;&#38544;&#24615;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#25506;&#35752;&#20102;&#19977;&#31181;&#32531;&#35299;LLMs&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#20250;&#29983;&#25104;&#24102;&#26377;&#20559;&#35265;&#30340;&#22238;&#22797;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30452;&#25509;&#25506;&#27979;&#25216;&#26415;&#21253;&#21547;&#24615;&#21035;&#25552;&#21450;&#25110;&#39044;&#23450;&#20041;&#30340;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#65292;&#36825;&#20123;&#24456;&#38590;&#20840;&#38754;&#25910;&#38598;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#29983;&#25104;&#30340;&#38388;&#25509;&#25506;&#27979;&#26694;&#26550;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#35825;&#20351;LLMs&#25259;&#38706;&#20854;&#24615;&#21035;&#20559;&#35265;&#65292;&#21363;&#20351;&#27809;&#26377;&#26126;&#30830;&#30340;&#24615;&#21035;&#25110;&#21051;&#26495;&#21360;&#35937;&#25552;&#21450;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#31574;&#30053;&#65292;&#20197;&#25259;&#38706;LLMs&#20013;&#30340;&#26174;&#24615;&#21644;&#38544;&#24615;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#25152;&#26377;&#32463;&#36807;&#27979;&#35797;&#30340;LLMs&#22343;&#34920;&#29616;&#20986;&#26126;&#30830;&#21644;/&#25110;&#38544;&#24615;&#24615;&#21035;&#20559;&#35265;&#65292;&#21363;&#20351;&#36755;&#20837;&#20013;&#27809;&#26377;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#23610;&#23544;&#22686;&#21152;&#25110;&#27169;&#22411;&#23545;&#40784;&#20250;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#25918;&#22823;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#36229;&#21442;&#25968;&#35843;&#25972;&#12289;&#25351;&#23548;&#25351;&#24341;&#21644;&#21435;&#20559;&#35843;&#25972;&#26469;&#30740;&#31350;&#19977;&#31181;&#32531;&#35299;LLMs&#20559;&#35265;&#30340;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#26126;&#30830;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26041;&#27861;&#20063;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11190v1 Announce Type: new  Abstract: Large Language Models (LLMs) can generate biased responses. Yet previous direct probing techniques contain either gender mentions or predefined gender stereotypes, which are challenging to comprehensively collect. Hence, we propose an indirect probing framework based on conditional generation. This approach aims to induce LLMs to disclose their gender bias even without explicit gender or stereotype mentions. We explore three distinct strategies to disclose explicit and implicit gender bias in LLMs. Our experiments demonstrate that all tested LLMs exhibit explicit and/or implicit gender bias, even when gender stereotypes are not present in the inputs. In addition, an increased model size or model alignment amplifies bias in most cases. Furthermore, we investigate three methods to mitigate bias in LLMs via Hyperparameter Tuning, Instruction Guiding, and Debias Tuning. Remarkably, these methods prove effective even in the absence of explici
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Layer Collapse&#65288;LaCo&#65289;&#30340;&#31616;&#26126;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#32467;&#26500;&#30340;&#21516;&#26102;&#36805;&#36895;&#20943;&#23567;&#23610;&#23544;&#65292;&#24182;&#22312;&#21098;&#26525;&#27604;&#20363;&#36798;&#21040;25-30%&#26102;&#20445;&#25345;&#36229;&#36807;80%&#30340;&#24179;&#22343;&#20219;&#21153;&#24615;&#33021;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.11187</link><description>&lt;p&gt;
LaCo&#65306;&#36890;&#36807;&#23618;&#21472;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
LaCo: Large Language Model Pruning via Layer Collapse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11187
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Layer Collapse&#65288;LaCo&#65289;&#30340;&#31616;&#26126;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#32467;&#26500;&#30340;&#21516;&#26102;&#36805;&#36895;&#20943;&#23567;&#23610;&#23544;&#65292;&#24182;&#22312;&#21098;&#26525;&#27604;&#20363;&#36798;&#21040;25-30%&#26102;&#20445;&#25345;&#36229;&#36807;80%&#30340;&#24179;&#22343;&#20219;&#21153;&#24615;&#33021;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#32463;&#21382;&#30528;&#23610;&#23544;&#25193;&#22823;&#30340;&#26126;&#26174;&#36235;&#21183;&#65292;&#36825;&#32473;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#24102;&#26469;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22914;&#27169;&#22411;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#27169;&#22411;&#21098;&#26525;&#21463;&#21040;&#21508;&#31181;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#21253;&#25324;&#30828;&#20214;&#25903;&#25345;&#38480;&#21046;&#12289;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#21644;&#23545;&#27169;&#22411;&#20869;&#37096;&#32467;&#26500;&#30340;&#25913;&#21464;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#30340;&#36880;&#23618;&#21098;&#26525;&#26041;&#27861;&#65292;&#31216;&#20026;Layer Collapse&#65288;LaCo&#65289;&#65292;&#20854;&#20013;&#21518;&#32622;&#27169;&#22411;&#23618;&#25240;&#21472;&#21040;&#21069;&#32622;&#23618;&#65292;&#20351;&#27169;&#22411;&#23610;&#23544;&#36805;&#36895;&#20943;&#23567;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#32467;&#26500;&#12290;&#32508;&#21512;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21098;&#26525;&#27604;&#20363;&#36798;&#21040;25-30%&#26102;&#65292;&#20445;&#25345;&#20102;&#36229;&#36807;80%&#30340;&#24179;&#22343;&#20219;&#21153;&#24615;&#33021;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#21518;&#35757;&#32451;&#23454;&#39564;&#20197;&#30830;&#35748;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11187v1 Announce Type: cross  Abstract: Large language models (LLMs) based on transformer are witnessing a notable trend of size expansion, which brings considerable costs to both model training and inference. However, existing methods such as model quantization, knowledge distillation, and model pruning are constrained by various issues, including hardware support limitations, the need for extensive training, and alterations to the internal structure of the model. In this paper, we propose a concise layer-wise pruning method called \textit{Layer Collapse (LaCo)}, in which rear model layers collapse into a prior layer, enabling a rapid reduction in model size while preserving the model structure. Comprehensive experiments show that our method maintains an average task performance of over 80\% at pruning ratios of 25-30\%, significantly outperforming existing state-of-the-art structured pruning methods. We also conduct post-training experiments to confirm that the proposed pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;ReNoVi&#65292;&#24110;&#21161;&#20114;&#21160;AI&#31995;&#32479;&#29702;&#35299;&#21644;&#32416;&#27491;&#31038;&#20250;&#35268;&#33539;&#36829;&#21453;&#65292;&#21253;&#25324;&#20154;&#31867;&#32534;&#20889;&#23545;&#35805;&#21644;ChatGPT&#29983;&#25104;&#30340;&#21512;&#25104;&#23545;&#35805;&#65292;&#20174;&#32780;&#24357;&#34917;&#25968;&#25454;&#31232;&#32570;&#24182;&#35780;&#20272;&#27169;&#22411;&#19982;&#20154;&#31867;&#22312;&#31038;&#20250;&#35268;&#33539;&#35748;&#30693;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11178</link><description>&lt;p&gt;
RENOVI&#65306;&#19968;&#20010;&#26088;&#22312;&#32416;&#27491;&#31038;&#20250;&#25991;&#21270;&#23545;&#35805;&#20013;&#35268;&#33539;&#36829;&#21453;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
RENOVI: A Benchmark Towards Remediating Norm Violations in Socio-Cultural Conversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11178
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;ReNoVi&#65292;&#24110;&#21161;&#20114;&#21160;AI&#31995;&#32479;&#29702;&#35299;&#21644;&#32416;&#27491;&#31038;&#20250;&#35268;&#33539;&#36829;&#21453;&#65292;&#21253;&#25324;&#20154;&#31867;&#32534;&#20889;&#23545;&#35805;&#21644;ChatGPT&#29983;&#25104;&#30340;&#21512;&#25104;&#23545;&#35805;&#65292;&#20174;&#32780;&#24357;&#34917;&#25968;&#25454;&#31232;&#32570;&#24182;&#35780;&#20272;&#27169;&#22411;&#19982;&#20154;&#31867;&#22312;&#31038;&#20250;&#35268;&#33539;&#35748;&#30693;&#19978;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#33539;&#36829;&#21453;&#21457;&#29983;&#22312;&#20010;&#20307;&#26410;&#33021;&#36981;&#23432;&#25991;&#21270;&#25509;&#21463;&#34892;&#20026;&#26102;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#28508;&#22312;&#20914;&#31361;&#12290;&#32416;&#27491;&#35268;&#33539;&#36829;&#21453;&#38656;&#35201;&#23545;&#27491;&#22312;&#21457;&#29983;&#30340;&#24494;&#22937;&#31038;&#20250;&#35748;&#30693;&#21644;&#25991;&#21270;&#25935;&#24863;&#12290;&#20026;&#20102;&#36171;&#20104;&#20114;&#21160;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#32416;&#27491;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;ReNoVi - &#19968;&#20010;&#21253;&#21547;9,258&#20010;&#22810;&#36718;&#23545;&#35805;&#27880;&#37322;&#26377;&#31038;&#20250;&#35268;&#33539;&#30340;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#31995;&#21015;&#20219;&#21153;&#20197;&#24110;&#21161;&#36880;&#27493;&#29702;&#35299;&#21644;&#32416;&#27491;&#35268;&#33539;&#36829;&#21453;&#12290;ReNoVi&#21253;&#21547;&#20004;&#20010;&#37096;&#20998;&#65306;512&#20010;&#20154;&#31867;&#32534;&#20889;&#30340;&#23545;&#35805;&#65288;&#30495;&#23454;&#25968;&#25454;&#65289;&#21644;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#30001;ChatGPT&#29983;&#25104;&#30340;8,746&#20010;&#21512;&#25104;&#23545;&#35805;&#12290;&#23613;&#31649;&#25910;&#38598;&#20805;&#36275;&#30340;&#20154;&#31867;&#32534;&#20889;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#65292;&#20294;&#21512;&#25104;&#23545;&#35805;&#25552;&#20379;&#20102;&#36866;&#37327;&#30340;&#25968;&#25454;&#24110;&#21161;&#32531;&#35299;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#65292;&#24182;&#19988;&#35780;&#20272;&#20102;LLM&#19982;&#20154;&#31867;&#22312;&#31038;&#20250;&#35268;&#33539;&#35748;&#30693;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#30340;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;ChatGPT&#30340;&#20248;&#21183;&#26469;&#36827;&#34892;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11178v1 Announce Type: new  Abstract: Norm violations occur when individuals fail to conform to culturally accepted behaviors, which may lead to potential conflicts. Remediating norm violations requires social awareness and cultural sensitivity of the nuances at play. To equip interactive AI systems with a remediation ability, we offer ReNoVi - a large-scale corpus of 9,258 multi-turn dialogues annotated with social norms, as well as define a sequence of tasks to help understand and remediate norm violations step by step. ReNoVi consists of two parts: 512 human-authored dialogues (real data), and 8,746 synthetic conversations generated by ChatGPT through prompt learning. While collecting sufficient human-authored data is costly, synthetic conversations provide suitable amounts of data to help mitigate the scarcity of training data, as well as the chance to assess the alignment between LLMs and humans in the awareness of social norms. We thus harness the power of ChatGPT to g
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38382;&#31572;&#30340;&#26032;&#26041;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#29992;&#20110;QA&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#27169;&#22359;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30340;&#20449;&#24687;&#25552;&#21462;&#31867;&#22411;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#34920;&#29616;&#20986;&#33394;&#24182;&#33021;&#26377;&#25928;&#24212;&#23545;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.11177</link><description>&lt;p&gt;
&#22522;&#20110;&#38382;&#31572;&#30340;&#20840;&#38754;&#20013;&#25991;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20449;&#24687;&#25552;&#21462;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
A Question Answering Based Pipeline for Comprehensive Chinese EHR Information Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11177
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38382;&#31572;&#30340;&#26032;&#26041;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#29992;&#20110;QA&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#27169;&#22359;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#30340;&#20449;&#24687;&#25552;&#21462;&#31867;&#22411;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#34920;&#29616;&#20986;&#33394;&#24182;&#33021;&#26377;&#25928;&#24212;&#23545;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#23545;&#30740;&#31350;&#21644;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#25552;&#21462;&#26041;&#24335;&#65292;&#38382;&#31572;&#65288;QA&#65289;&#21487;&#20197;&#25552;&#21462;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#28789;&#27963;&#30340;&#20449;&#24687;&#65292;&#19988;&#26356;&#26131;&#20110;&#20020;&#24202;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#65292;&#20294;&#20854;&#36827;&#23637;&#21463;&#21040;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#30340;&#38459;&#30861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#29992;&#20110;QA&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#27969;&#27700;&#32447;&#38598;&#25104;&#20102;&#19968;&#20010;&#39044;&#22788;&#29702;&#27169;&#22359;&#65292;&#22788;&#29702;&#20102;&#19982;&#25552;&#21462;&#22411;QA&#26694;&#26550;&#19981;&#22826;&#20860;&#23481;&#30340;&#25552;&#21462;&#31867;&#22411;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#20855;&#26377;&#19981;&#36830;&#32493;&#31572;&#26696;&#21644;&#22810;&#23545;&#19968;&#20851;&#31995;&#30340;&#24773;&#20917;&#12290;&#25152;&#24471;&#30340;QA&#27169;&#22411;&#22312;EHRs&#20013;&#30340;&#20449;&#24687;&#25552;&#21462;&#23376;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#21253;&#21547;&#26159;&#38750;&#38382;&#39064;&#30340;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#35774;&#32622;&#12290;&#26696;&#20363;&#30740;&#31350;&#21644;&#28040;&#34701;&#30740;&#31350;&#35777;&#26126;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11177v1 Announce Type: new  Abstract: Electronic health records (EHRs) hold significant value for research and applications. As a new way of information extraction, question answering (QA) can extract more flexible information than conventional methods and is more accessible to clinical researchers, but its progress is impeded by the scarcity of annotated data. In this paper, we propose a novel approach that automatically generates training data for transfer learning of QA models. Our pipeline incorporates a preprocessing module to handle challenges posed by extraction types that are not readily compatible with extractive QA frameworks, including cases with discontinuous answers and many-to-one relationships. The obtained QA model exhibits excellent performance on subtasks of information extraction in EHRs, and it can effectively handle few-shot or zero-shot settings involving yes-no questions. Case studies and ablation studies demonstrate the necessity of each component in 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#21644;&#38544;&#24335;&#26041;&#24335;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#30693;&#35782;&#30340;&#35748;&#35782;&#65292;&#21253;&#25324;&#35757;&#32451;&#27169;&#22411;&#26126;&#30830;&#35782;&#21035;&#31572;&#26696;&#20013;&#30340;&#30693;&#35782;&#19977;&#20803;&#32452;&#21644;&#38544;&#24335;&#21306;&#20998;&#21487;&#38752;&#21644;&#19981;&#21487;&#38752;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.11176</link><description>&lt;p&gt;
KnowTuning&#65306;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
KnowTuning: Knowledge-aware Fine-tuning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11176
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26174;&#24335;&#21644;&#38544;&#24335;&#26041;&#24335;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#30693;&#35782;&#30340;&#35748;&#35782;&#65292;&#21253;&#25324;&#35757;&#32451;&#27169;&#22411;&#26126;&#30830;&#35782;&#21035;&#31572;&#26696;&#20013;&#30340;&#30693;&#35782;&#19977;&#20803;&#32452;&#21644;&#38544;&#24335;&#21306;&#20998;&#21487;&#38752;&#21644;&#19981;&#21487;&#38752;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#26377;&#25928;&#21033;&#29992;&#30693;&#35782;&#36827;&#34892;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65292;&#34920;&#29616;&#20986;&#29983;&#25104;&#19981;&#23436;&#25972;&#12289;&#38750;&#20107;&#23454;&#24615;&#25110;&#19981;&#21512;&#36923;&#36753;&#30340;&#31572;&#26696;&#31561;&#38480;&#21046;&#12290;&#36825;&#20123;&#38480;&#21046;&#28304;&#20110;LLMs&#22312;&#26222;&#36890;&#24494;&#35843;&#26399;&#38388;&#23545;&#30693;&#35782;&#30340;&#35748;&#35782;&#19981;&#36275;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#24494;&#35843;&#65288;KnowTuning&#65289;&#26041;&#27861;&#65292;&#20197;&#26126;&#30830;&#21644;&#38544;&#24335;&#22320;&#25913;&#21892;LLMs&#30340;&#30693;&#35782;&#35748;&#35782;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26174;&#24335;&#30693;&#35782;&#24863;&#30693;&#29983;&#25104;&#38454;&#27573;&#65292;&#35757;&#32451;LLMs&#26126;&#30830;&#35782;&#21035;&#31572;&#26696;&#20013;&#30340;&#30693;&#35782;&#19977;&#20803;&#32452;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#38544;&#24335;&#30693;&#35782;&#24863;&#30693;&#27604;&#36739;&#38454;&#27573;&#65292;&#35757;&#32451;LLMs&#38544;&#24335;&#21306;&#20998;&#21487;&#38752;&#21644;&#19981;&#21487;&#38752;&#30340;&#30693;&#35782;&#65292;&#21253;&#25324;&#23436;&#25972;&#24615;&#12289;&#20107;&#23454;&#24615;&#21644;&#36923;&#36753;&#24615;&#19977;&#20010;&#26041;&#38754;&#12290;&#23545;&#36890;&#29992;&#21644;&#21307;&#23398;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11176v1 Announce Type: cross  Abstract: Despite their success at many natural language processing (NLP) tasks, large language models (LLMs) still struggle to effectively leverage knowledge for knowledge-intensive tasks, manifesting limitations such as generating incomplete, non-factual, or illogical answers. These limitations stem from inadequate knowledge awareness of LLMs during vanilla fine-tuning. To address these problems, we propose a knowledge-aware fine-tuning (KnowTuning) method to explicitly and implicitly improve the knowledge awareness of LLMs. We devise an explicit knowledge-aware generation stage to train LLMs to explicitly identify knowledge triples in answers. We also propose an implicit knowledge-aware comparison stage to train LLMs to implicitly distinguish between reliable and unreliable knowledge, in three aspects: completeness, factuality, and logicality. Extensive experiments on both generic and medical question answering (QA) datasets confirm the effec
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;M4GT-Bench&#65292;&#28041;&#21450;&#22810;&#35821;&#35328;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#65292;&#21253;&#25324;&#21333;&#35821;&#21644;&#22810;&#35821;&#31181;MGT&#26816;&#27979;&#12289;&#22810;&#27169;&#22411;&#26816;&#27979;&#21644;&#20154;&#26426;&#28151;&#21512;&#25991;&#26412;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.11175</link><description>&lt;p&gt;
M4GT-Bench: &#29992;&#20110;&#40657;&#30418;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11175
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;M4GT-Bench&#65292;&#28041;&#21450;&#22810;&#35821;&#35328;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#65292;&#21253;&#25324;&#21333;&#35821;&#21644;&#22810;&#35821;&#31181;MGT&#26816;&#27979;&#12289;&#22810;&#27169;&#22411;&#26816;&#27979;&#21644;&#20154;&#26426;&#28151;&#21512;&#25991;&#26412;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24102;&#26469;&#20102;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#65288;MGT&#65289;&#22312;&#19981;&#21516;&#28192;&#36947;&#30340;&#28608;&#22686;&#65292;&#36825;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#28508;&#22312;&#28389;&#29992;&#21644;&#31038;&#20250;&#24433;&#21709;&#30340;&#20851;&#27880;&#12290;&#35782;&#21035;&#21644;&#21306;&#20998;&#36825;&#31181;&#20869;&#23481;&#19982;&#30495;&#23454;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#23545;&#20110;&#23545;&#25239;&#34394;&#20551;&#20449;&#24687;&#12289;&#20445;&#25345;&#25945;&#32946;&#21644;&#31185;&#23398;&#39046;&#22495;&#30340;&#23436;&#25972;&#24615;&#20197;&#21450;&#20445;&#25345;&#36890;&#20449;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#28041;&#21450;&#22810;&#35821;&#35328;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#29983;&#25104;&#22120;&#30340;&#26032;&#22522;&#20934;M4GT-Bench&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#23427;&#38024;&#23545;&#19977;&#20010;&#20219;&#21153;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#38598;&#21512;&#65306;&#65288;1&#65289;&#21333;&#35821;&#21644;&#22810;&#35821;&#31181;&#20108;&#20998;&#31867;MGT&#26816;&#27979;&#65307;&#65288;2&#65289;&#22810;&#27169;&#22411;&#26816;&#27979;&#30830;&#23450;&#29983;&#25104;&#25991;&#26412;&#30340;&#29305;&#23450;&#27169;&#22411;&#65307;&#20197;&#21450;&#65288;3&#65289;&#20154;&#26426;&#28151;&#21512;&#25991;&#26412;&#26816;&#27979;&#65292;&#24212;&#30830;&#23450;&#19968;&#20010;&#35789;&#36793;&#30028;&#26469;&#30028;&#23450;MGT&#21644;&#20154;&#24037;&#25776;&#20889;&#20869;&#23481;&#12290;&#23545;&#20110;&#20219;&#21153;2&#30340;&#20154;&#31867;&#35780;&#20272;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11175v1 Announce Type: new  Abstract: The advent of Large Language Models (LLMs) has brought an unprecedented surge in machine-generated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark involving multilingual, multi-domain and multi-generator for MGT detection -- M4GT-Bench. It is collected for three task formulations: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection identifies which particular model generates the text; and (3) human-machine mixed text detection, where a word boundary delimiting MGT from human-written content should be determined. Human evaluation for Task 2 shows
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;token-ensemble&#29983;&#25104;&#31574;&#30053;&#65292;&#25361;&#25112;&#20102;&#24403;&#21069;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#23545;&#24403;&#21069;&#26816;&#27979;&#27169;&#22411;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#26816;&#27979;&#25216;&#26415;&#20197;&#24212;&#23545;&#22797;&#26434;&#23545;&#25239;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.11167</link><description>&lt;p&gt;
Token-Ensemble&#25991;&#26412;&#29983;&#25104;&#65306;&#23545;&#33258;&#21160;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11167
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;token-ensemble&#29983;&#25104;&#31574;&#30053;&#65292;&#25361;&#25112;&#20102;&#24403;&#21069;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#23545;&#24403;&#21069;&#26816;&#27979;&#27169;&#22411;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#26816;&#27979;&#25216;&#26415;&#20197;&#24212;&#23545;&#22797;&#26434;&#23545;&#25239;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#20869;&#23481;&#26816;&#27979;&#27169;&#22411;&#23545;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25915;&#20987;&#65288;&#20363;&#22914;&#25913;&#20889;&#25110;&#35789;&#35821;&#26367;&#25442;&#65289;&#30340;&#40065;&#26834;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;token-ensemble&#29983;&#25104;&#31574;&#30053;&#65292;&#25361;&#25112;&#20102;&#24403;&#21069;AI&#20869;&#23481;&#26816;&#27979;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20174;&#38543;&#26426;&#20505;&#36873;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#19979;&#19968;&#20010;token&#23436;&#25104;&#25552;&#31034;&#26469;&#25506;&#32034;&#38598;&#25104;&#25915;&#20987;&#31574;&#30053;&#12290;&#25105;&#20204;&#21457;&#29616;token-ensemble&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;AI&#20869;&#23481;&#26816;&#27979;&#27169;&#22411;&#30340;&#24615;&#33021;&#65288;&#20195;&#30721;&#21644;&#27979;&#35797;&#38598;&#23558;&#21457;&#24067;&#65289;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;token-ensemble&#29983;&#25104;&#23545;&#24403;&#21069;&#26816;&#27979;&#27169;&#22411;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#65292;&#24182;&#24378;&#35843;&#20102;&#25913;&#36827;&#26816;&#27979;&#25216;&#26415;&#20197;&#24212;&#23545;&#22797;&#26434;&#23545;&#25239;&#31574;&#30053;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11167v1 Announce Type: cross  Abstract: The robustness of AI-content detection models against cultivated attacks (e.g., paraphrasing or word switching) remains a significant concern. This study proposes a novel token-ensemble generation strategy to challenge the robustness of current AI-content detection approaches. We explore the ensemble attack strategy by completing the prompt with the next token generated from random candidate LLMs. We find the token-ensemble approach significantly drops the performance of AI-content detection models (The code and test sets will be released). Our findings reveal that token-ensemble generation poses a vital challenge to current detection models and underlines the need for advancing detection technologies to counter sophisticated adversarial strategies.
&lt;/p&gt;</description></item><item><title>GenDec&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#38382;&#39064;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#29420;&#31435;&#23436;&#25972;&#30340;&#23376;&#38382;&#39064;&#26469;&#22686;&#24378;LLMs&#22312;RAG&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.11166</link><description>&lt;p&gt;
GenDec:&#19968;&#31181;&#29992;&#20110;&#22810;&#36339;&#25512;&#29702;&#30340;&#31283;&#20581;&#29983;&#25104;&#24335;&#38382;&#39064;&#20998;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GenDec: A robust generative Question-decomposition method for Multi-hop reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11166
&lt;/p&gt;
&lt;p&gt;
GenDec&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#38382;&#39064;&#20998;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#29420;&#31435;&#23436;&#25972;&#30340;&#23376;&#38382;&#39064;&#26469;&#22686;&#24378;LLMs&#22312;RAG&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Multi-hop QA (MHQA)&#28041;&#21450;&#36880;&#27493;&#25512;&#29702;&#20197;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#24182;&#25214;&#21040;&#22810;&#20010;&#30456;&#20851;&#25903;&#25345;&#20107;&#23454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#65292;&#26080;&#27861;&#28385;&#36275;&#22810;&#36339;&#38382;&#39064;&#30340;&#22238;&#31572;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#23578;&#19981;&#28165;&#26970;LLMs&#26159;&#21542;&#25353;&#29031;&#26399;&#26395;&#30340;&#25512;&#29702;&#38142;&#26469;&#36798;&#21040;&#27491;&#30830;&#30340;&#26368;&#32456;&#31572;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#21487;&#35299;&#37322;QA&#30340;&#35282;&#24230;&#25552;&#39640;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#29983;&#25104;&#24335;&#38382;&#39064;&#20998;&#35299;&#26041;&#27861;&#65288;GenDec&#65289;&#65292;&#36890;&#36807;&#26681;&#25454;&#39069;&#22806;&#25552;&#21462;&#30340;&#35777;&#25454;&#29983;&#25104;&#29420;&#31435;&#23436;&#25972;&#30340;&#23376;&#38382;&#39064;&#12290;&#20026;&#20102;&#23637;&#31034;Gendec&#30340;&#24433;&#21709;&#12289;&#27867;&#21270;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#23454;&#39564;&#65292;&#39318;&#20808;&#26159;&#23558;GenDec&#19982;&#23567;&#22411;QA&#31995;&#32479;&#32467;&#21512;&#22312;&#27573;&#33853;&#26816;&#32034;&#21644;QA&#20219;&#21153;&#19978;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11166v1 Announce Type: new  Abstract: Multi-hop QA (MHQA) involves step-by-step reasoning to answer complex questions and find multiple relevant supporting facts. However, Existing large language models'(LLMs) reasoning ability in multi-hop question answering remains exploration, which is inadequate in answering multi-hop questions. Moreover, it is unclear whether LLMs follow a desired reasoning chain to reach the right final answer. In this paper, we propose a \textbf{gen}erative question \textbf{dec}omposition method (GenDec) from the perspective of explainable QA by generating independent and complete sub-questions based on incorporating additional extracted evidence for enhancing LLMs' reasoning ability in RAG. To demonstrate the impact, generalization, and robustness of Gendec, we conduct two experiments, the first is combining GenDec with small QA systems on paragraph retrieval and QA tasks. We secondly examine the reasoning capabilities of various state-of-the-art LLM
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KG-Agent&#30340;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;KG&#19978;&#36827;&#34892;&#25512;&#29702;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.11163</link><description>&lt;p&gt;
KG-Agent: &#19968;&#31181;&#29992;&#20110;&#22312;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11163
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KG-Agent&#30340;&#39640;&#25928;&#33258;&#20027;&#20195;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#22312;KG&#19978;&#36827;&#34892;&#25512;&#29702;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#19978;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#21463;&#21040;&#29616;&#26377;&#35774;&#35745;LLMs&#21644;KG&#20043;&#38388;&#20132;&#20114;&#31574;&#30053;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KG-Agent&#30340;&#33258;&#20027;LLM&#20195;&#29702;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#24471;&#23567;&#22411;LLM&#33021;&#22815;&#22312;&#23436;&#25104;&#23545;KG&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#20027;&#21160;&#20570;&#20986;&#20915;&#31574;&#12290;&#22312;KG-Agent&#20013;&#65292;&#25105;&#20204;&#25972;&#21512;&#20102;LLM&#12289;&#22810;&#21151;&#33021;&#24037;&#20855;&#31665;&#12289;&#22522;&#20110;KG&#30340;&#25191;&#34892;&#22120;&#21644;&#30693;&#35782;&#23384;&#20648;&#22120;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#36845;&#20195;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#33258;&#20027;&#36873;&#25321;&#24037;&#20855;&#28982;&#21518;&#26356;&#26032;&#20869;&#23384;&#20197;&#36827;&#34892;&#22312;KG&#19978;&#30340;&#25512;&#29702;&#12290;&#20026;&#20102;&#20445;&#35777;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#31243;&#24207;&#35821;&#35328;&#26469;&#21046;&#23450;&#22312;KG&#19978;&#30340;&#22810;&#36339;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#21512;&#25104;&#22522;&#20110;&#20195;&#30721;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#26469;&#24494;&#35843;&#22522;&#30784;LLM&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;10K&#20010;&#26679;&#26412;&#26469;&#35843;&#25972;LLaMA-7B&#27604;&#20351;&#29992;&#26356;&#22823;LLM&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11163v1 Announce Type: new  Abstract: In this paper, we aim to improve the reasoning ability of large language models (LLMs) over knowledge graphs (KGs) to answer complex questions. Inspired by existing methods that design the interaction strategy between LLMs and KG, we propose an autonomous LLM-based agent framework, called KG-Agent, which enables a small LLM to actively make decisions until finishing the reasoning process over KGs. In KG-Agent, we integrate the LLM, multifunctional toolbox, KG-based executor, and knowledge memory, and develop an iteration mechanism that autonomously selects the tool then updates the memory for reasoning over KG. To guarantee the effectiveness, we leverage program language to formulate the multi-hop reasoning process over the KG, and synthesize a code-based instruction dataset to fine-tune the base LLM. Extensive experiments demonstrate that only using 10K samples for tuning LLaMA-7B can outperform state-of-the-art methods using larger LLM
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PANDA&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#35780;&#27979;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11161</link><description>&lt;p&gt;
PANDA&#65288;Pedantic ANswer-correctness Determination and Adjudication&#65289;&#65306;&#25913;&#36827;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#30340;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PANDA (Pedantic ANswer-correctness Determination and Adjudication):Improving Automatic Evaluation for Question Answering and Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PANDA&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#35780;&#27979;&#26041;&#24335;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#33258;&#21160;&#35780;&#20272;&#38382;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#65288;QA&#65289;&#21482;&#26377;&#22312;&#25105;&#20204;&#30693;&#36947;&#31572;&#26696;&#26159;&#21542;&#27491;&#30830;&#26102;&#25165;&#33021;&#21462;&#24471;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#35768;&#22810;&#26368;&#20855;&#25361;&#25112;&#24615;&#21644;&#26377;&#36259;&#30340;QA&#31034;&#20363;&#65292;&#24403;&#21069;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#65288;AC&#65289;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#65292;&#29305;&#21035;&#26159;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20887;&#38271;&#12289;&#33258;&#30001;&#26684;&#24335;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#32570;&#20047;&#25968;&#25454;&#21644;&#27169;&#22411;&#36807;&#22823;&#12290;&#22522;&#20110;LLM&#30340;&#35780;&#20998;&#22120;&#19982;&#20154;&#31867;&#26356;&#22909;&#22320;&#30456;&#20851;&#65292;&#20294;&#36825;&#39033;&#26114;&#36149;&#30340;&#20219;&#21153;&#20165;&#22312;&#26377;&#38480;&#30340;QA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#28165;&#26224;&#30340;&#25351;&#21335;&#26469;&#35780;&#20272;&#20174;&#20154;&#31867;QA&#27604;&#36187;&#20013;&#37319;&#32435;&#30340;&#26426;&#22120;QA&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31934;&#30830;&#30340;&#31572;&#26696;&#27491;&#30830;&#24615;&#30830;&#23450;&#21644;&#35009;&#20915;&#65288;Precise ANswer correctness Determination and Adjudication&#65292;PANDA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23567;&#24039;&#12289;&#39640;&#25928;&#12289;&#30830;&#23450;&#24615;&#30340;AC&#20998;&#31867;&#22120;&#65288;812 KB&#65289;&#65292;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11161v1 Announce Type: cross  Abstract: Question answering (QA) can only make progress if we know if an answer is correct, but for many of the most challenging and interesting QA examples, current answer correctness (AC) metrics do not align with human judgments, particularly verbose, free form answers from large language models (LLM). There are two challenges: a lack of data and that models are too big. LLM based scorers correlate better with humans, but this expensive task has only been tested on limited QA datasets. We rectify these issues by providing clear guidelines for evaluating machine QA adopted from human QA contests. We also introduce Precise ANswer correctness Determination and Adjudication (PANDA), a small, efficient, deterministic AC classifier (812 KB) that more accurately evaluates answer correctness.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#25991;&#26412;&#24341;&#23548;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26694;&#26550;CFT-CLIP&#65292;&#29992;&#20110;&#22686;&#24378;&#26032;&#38395;&#25991;&#26412;&#21644;&#32553;&#30053;&#22270;&#20043;&#38388;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.11159</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#25991;&#26412;&#24341;&#23548;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26469;&#29702;&#35299;&#26032;&#38395;&#32553;&#30053;&#22270;&#30340;&#20195;&#34920;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding News Thumbnail Representativeness by Counterfactual Text-Guided Contrastive Language-Image Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11159
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21453;&#20107;&#23454;&#25991;&#26412;&#24341;&#23548;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26694;&#26550;CFT-CLIP&#65292;&#29992;&#20110;&#22686;&#24378;&#26032;&#38395;&#25991;&#26412;&#21644;&#32553;&#30053;&#22270;&#20043;&#38388;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#29702;&#35299;&#26032;&#38395;&#32553;&#30053;&#22270;&#30340;&#20195;&#34920;&#24615;&#36825;&#19968;&#20851;&#38190;&#25361;&#25112;&#65292;&#36825;&#20123;&#32553;&#30053;&#22270;&#36890;&#24120;&#22312;&#25991;&#31456;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20256;&#25773;&#26102;&#20316;&#20026;&#35835;&#32773;&#30340;&#31532;&#19968;&#20010;&#35270;&#35273;&#21442;&#19982;&#12290;&#25105;&#20204;&#20851;&#27880;&#26032;&#38395;&#22270;&#20687;&#26159;&#21542;&#20195;&#34920;&#26032;&#38395;&#25991;&#26412;&#20013;&#35752;&#35770;&#30340;&#20027;&#35201;&#20027;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#26032;&#38395;&#32553;&#30053;&#22270;&#21644;&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;\textsc{NewsTT}&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20363;&#22914;CLIP&#21644;BLIP-2&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#30001;&#20110;&#26032;&#38395;&#20027;&#39064;&#32463;&#24120;&#28041;&#21450;&#21629;&#21517;&#23454;&#20307;&#25110;&#19987;&#26377;&#21517;&#35789;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#32570;&#20047;&#21305;&#37197;&#20854;&#35270;&#35273;&#21644;&#25991;&#26412;&#22806;&#35266;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CFT-CLIP&#65292;&#19968;&#20010;&#21453;&#20107;&#23454;&#25991;&#26412;&#24341;&#23548;&#30340;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11159v1 Announce Type: new  Abstract: This paper delves into the critical challenge of understanding the representativeness of news thumbnail images, which often serve as the first visual engagement for readers when an article is disseminated on social media. We focus on whether a news image represents the main subject discussed in the news text. To serve the challenge, we introduce \textsc{NewsTT}, a manually annotated dataset of news thumbnail image and text pairs. We found that pretrained vision and language models, such as CLIP and BLIP-2, struggle with this task. Since news subjects frequently involve named entities or proper nouns, a pretrained model could not have the ability to match its visual and textual appearances. To fill the gap, we propose CFT-CLIP, a counterfactual text-guided contrastive language-image pretraining framework. We hypothesize that learning to contrast news text with its counterfactual, of which named entities are replaced, can enhance the cross
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#20851;&#31995;&#23450;&#20041;&#26469;&#35757;&#32451;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#30340;&#38646;-shot&#23398;&#20064;&#35774;&#32622;&#65292;&#20174;&#32780;&#20026;&#27169;&#22411;&#25552;&#20379;&#20934;&#30830;&#21644;&#26126;&#30830;&#30340;&#20851;&#31995;&#31867;&#22411;&#25551;&#36848;&#65292;&#24182;&#21516;&#26102;&#26368;&#23567;&#21270;&#27880;&#37322;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.11142</link><description>&lt;p&gt;
&#25226;&#25569;&#35201;&#28857;&#65306;&#23450;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;-shot&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Grasping the Essentials: Tailoring Large Language Models for Zero-Shot Relation Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11142
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#20851;&#31995;&#23450;&#20041;&#26469;&#35757;&#32451;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#30340;&#38646;-shot&#23398;&#20064;&#35774;&#32622;&#65292;&#20174;&#32780;&#20026;&#27169;&#22411;&#25552;&#20379;&#20934;&#30830;&#21644;&#26126;&#30830;&#30340;&#20851;&#31995;&#31867;&#22411;&#25551;&#36848;&#65292;&#24182;&#21516;&#26102;&#26368;&#23567;&#21270;&#27880;&#37322;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#26088;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#25552;&#21450;&#30340;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#23613;&#31649;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38590;&#20197;&#36866;&#24212;&#26032;&#30340;&#25110;&#26410;&#35265;&#36807;&#30340;&#20851;&#31995;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23569;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#26088;&#22312;&#20943;&#23569;&#27880;&#37322;&#35201;&#27714;&#65292;&#23545;&#20110;&#29702;&#35299;&#30446;&#26631;&#20851;&#31995;&#35821;&#20041;&#25552;&#20379;&#20102;&#19981;&#23436;&#25972;&#19988;&#26377;&#20559;&#35265;&#30340;&#30417;&#30563;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#19988;&#19981;&#31283;&#23450;&#12290;&#20026;&#20102;&#20026;&#27169;&#22411;&#25552;&#20379;&#20934;&#30830;&#21644;&#26126;&#30830;&#30340;&#20851;&#31995;&#31867;&#22411;&#25551;&#36848;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#27880;&#37322;&#35201;&#27714;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20165;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20013;&#34920;&#31034;&#30340;&#20851;&#31995;&#23450;&#20041;&#26469;&#35757;&#32451;RE&#27169;&#22411;&#30340;&#20165;&#38646;-shot RE&#35774;&#32622;&#12290;&#21463;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#24378;&#22823;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11142v1 Announce Type: new  Abstract: Relation extraction (RE), a crucial task in NLP, aims to identify semantic relationships between entities mentioned in texts. Despite significant advancements in this field, existing models typically rely on extensive annotated data for training, which can be both costly and time-consuming to acquire. Moreover, these models often struggle to adapt to new or unseen relationships. In contrast, few-shot learning settings, which aim to reduce annotation requirements, may offer incomplete and biased supervision for understanding target relation semantics, leading to degraded and unstable performance. To provide the model with accurate and explicit descriptions of the relations types and meanwhile minimize the annotation requirements, we study the definition only zero-shot RE setting where only relation definitions expressed in natural language are used to train a RE model. Motivated by the strong synthetic data generation power of LLMs, we pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Boosting of Thoughts&#65288;BoT&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#22810;&#20010;&#24605;&#32500;&#26641;&#65292;&#33719;&#24471;&#19968;&#31995;&#21015;&#35797;&#38169;&#25512;&#29702;&#32463;&#39564;&#65292;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#30340;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.11140</link><description>&lt;p&gt;
&#24605;&#32500;&#30340;&#25552;&#21319;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35797;&#38169;&#38382;&#39064;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Boosting of Thoughts&#65288;BoT&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#22810;&#20010;&#24605;&#32500;&#26641;&#65292;&#33719;&#24471;&#19968;&#31995;&#21015;&#35797;&#38169;&#25512;&#29702;&#32463;&#39564;&#65292;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#24615;&#33021;&#20851;&#38190;&#21462;&#20915;&#20110;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#19968;&#20123;&#24605;&#32500;&#38142;&#31034;&#33539;&#20316;&#20026;&#31034;&#20363;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#65288;&#20363;&#22914;Thought Tree&#65289;&#25351;&#20986;&#20102;&#22312;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#25512;&#29702;&#27493;&#39588;&#36873;&#25321;&#20013;&#65292;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Boosting of Thoughts&#65288;BoT&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#35768;&#22810;&#24605;&#32500;&#26641;&#26469;&#33719;&#24471;&#19968;&#31995;&#21015;&#35797;&#38169;&#25512;&#29702;&#32463;&#39564;&#65292;&#36825;&#23558;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#30340;&#25552;&#31034;&#12290;BoT&#20174;&#19968;&#20010;&#31616;&#21333;&#25552;&#31034;&#24320;&#22987;&#65292;&#26080;&#38656;&#31034;&#20363;&#65292;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#35780;&#20272;&#22823;&#37327;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#21033;&#29992;LLM&#33719;&#24471;&#30340;&#38169;&#35823;&#20998;&#26512;&#26469;&#26126;&#30830;&#20462;&#25913;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11140v1 Announce Type: new  Abstract: The reasoning performance of Large Language Models (LLMs) on a wide range of problems critically relies on chain-of-thought prompting, which involves providing a few chain of thought demonstrations as exemplars in prompts. Recent work, e.g., Tree of Thoughts, has pointed out the importance of exploration and self-evaluation in reasoning step selection for complex problem solving. In this paper, we present Boosting of Thoughts (BoT), an automated prompting framework for problem solving with LLMs by iteratively exploring and self-evaluating many trees of thoughts in order to acquire an ensemble of trial-and-error reasoning experiences, which will serve as a new form of prompting to solve the complex problem. Starting from a simple prompt without requiring examples, BoT iteratively explores and evaluates a large collection of reasoning steps, and more importantly, uses error analysis obtained from the LLM on them to explicitly revise prompt
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26410;&#30693;&#20219;&#21153;&#25351;&#20196;&#30340;&#31283;&#20581;&#24615;</title><link>https://arxiv.org/abs/2402.11138</link><description>&lt;p&gt;
&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Contrastive Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11138
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#30456;&#20284;&#24615;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26410;&#30693;&#20219;&#21153;&#25351;&#20196;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#19968;&#30452;&#34987;&#29992;&#20316;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26410;&#30693;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;LLMs&#22312;&#38754;&#20020;&#26410;&#30693;&#25351;&#20196;&#26102;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#31283;&#20581;&#24615;&#65292;&#24403;&#30456;&#21516;&#30340;&#25351;&#20196;&#20197;&#31245;&#24494;&#21464;&#21270;&#30340;&#24418;&#24335;&#25110;&#35821;&#35328;&#39118;&#26684;&#25552;&#20986;&#26102;&#20250;&#20135;&#29983;&#19981;&#19968;&#33268;&#30340;&#36755;&#20986;&#12290;&#36825;&#31181;&#34892;&#20026;&#34920;&#26126;LLMs&#23545;&#25991;&#26412;&#21464;&#21270;&#30340;&#31283;&#20581;&#24615;&#21644;&#23545;&#26410;&#30693;&#25351;&#20196;&#30340;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#21487;&#20449;&#24230;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#65292;&#35813;&#26041;&#27861;&#22312;&#26368;&#22823;&#21270;&#35821;&#20041;&#19978;&#31561;&#20215;&#30340;&#25351;&#20196;-&#23454;&#20363;&#23545;&#30340;&#38544;&#34255;&#34920;&#31034;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#30340;&#21516;&#26102;&#65292;&#26368;&#23567;&#21270;&#35821;&#20041;&#19978;&#19981;&#21516;&#30340;&#23545;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#36890;&#36807;&#37322;&#20041;&#20219;&#21153;&#25351;&#20196;&#65292;&#25193;&#20805;&#29616;&#26377;&#30340;FLAN&#38598;&#21512;&#12290;&#22312;PromptBench&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23545;&#27604;&#25351;&#20196;&#35843;&#25972;&#65288;CoIN&#65289;&#19968;&#30452;&#25552;&#39640;&#20102;LLMs&#23545;&#26410;&#30693;&#25351;&#20196;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11138v1 Announce Type: cross  Abstract: Instruction tuning has been used as a promising approach to improve the performance of large language models (LLMs) on unseen tasks. However, current LLMs exhibit limited robustness to unseen instructions, generating inconsistent outputs when the same instruction is phrased with slightly varied forms or language styles. This behavior indicates LLMs' lack of robustness to textual variations and generalizability to unseen instructions, potentially leading to trustworthiness issues. Accordingly, we propose Contrastive Instruction Tuning, which maximizes the similarity between the hidden representations of semantically equivalent instruction-instance pairs while minimizing the similarity between semantically different ones. To facilitate this approach, we augment the existing FLAN collection by paraphrasing task instructions. Experiments on the PromptBench benchmark show that CoIN consistently improves LLMs' robustness to unseen instructio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Speculative Streaming&#26041;&#27861;&#65292;&#23558;&#33609;&#31295;&#27169;&#22411;&#34701;&#20837;&#30446;&#26631;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23558;&#24494;&#35843;&#30446;&#26631;&#20174;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#26356;&#25913;&#20026;&#26410;&#26469;&#30340;n-gram&#39044;&#27979;&#65292;&#21152;&#36895;&#35299;&#30721;1.8-3.1&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.11131</link><description>&lt;p&gt;
&#25512;&#27979;&#24335;&#27969;&#24335;&#22788;&#29702;: &#26080;&#38656;&#36741;&#21161;&#27169;&#22411;&#30340;&#24555;&#36895;LLM&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Speculative Streaming: Fast LLM Inference without Auxiliary Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11131
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Speculative Streaming&#26041;&#27861;&#65292;&#23558;&#33609;&#31295;&#27169;&#22411;&#34701;&#20837;&#30446;&#26631;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23558;&#24494;&#35843;&#30446;&#26631;&#20174;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#26356;&#25913;&#20026;&#26410;&#26469;&#30340;n-gram&#39044;&#27979;&#65292;&#21152;&#36895;&#35299;&#30721;1.8-3.1&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#24335;&#35299;&#30721;&#26159;&#19968;&#31181;&#31361;&#20986;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#39640;&#22522;&#20110;&#36741;&#21161;&#33609;&#31295;&#27169;&#22411;&#39044;&#27979;&#30340;&#22823;&#22411;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#34429;&#28982;&#22312;&#29305;&#23450;&#24212;&#29992;&#35774;&#32622;&#20013;&#26377;&#25928;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#24494;&#35843;&#33609;&#31295;&#21644;&#30446;&#26631;&#27169;&#22411;&#20197;&#23454;&#29616;&#36739;&#39640;&#30340;&#25509;&#21463;&#29575;&#12290;&#38543;&#30528;&#19979;&#28216;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#20123;&#33609;&#31295;&#27169;&#22411;&#32473;&#25512;&#29702;&#31995;&#32479;&#22686;&#21152;&#20102;&#26174;&#33879;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Speculative Streaming&#65292;&#19968;&#31181;&#21333;&#27169;&#22411;&#30340;&#25512;&#27979;&#24335;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#33609;&#25311;&#34701;&#20837;&#30446;&#26631;&#27169;&#22411;&#65292;&#23558;&#24494;&#35843;&#30446;&#26631;&#20174;&#19979;&#19968;&#20010;&#20196;&#29260;&#39044;&#27979;&#23545;&#35937;&#26356;&#25913;&#20026;&#26410;&#26469;&#30340;n-gram&#39044;&#27979;&#12290; Speculative Streaming&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21152;&#36895;&#35299;&#30721;1.8-3.1&#20493;&#65292;&#22914;&#25688;&#35201;&#12289;&#32467;&#26500;&#21270;&#26597;&#35810;&#21644;&#24847;&#20041;&#34920;&#36798;&#65292;&#21516;&#26102;&#19981;&#38477;&#20302;&#29983;&#25104;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;Speculative Streaming&#21442;&#25968;&#26377;&#25928;&#12290;&#23427;&#23454;&#29616;&#20102;&#19982;Medusa&#39118;&#26684;&#26550;&#26500;&#30456;&#23218;&#32654;/&#26356;&#39640;&#30340;&#21152;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11131v1 Announce Type: cross  Abstract: Speculative decoding is a prominent technique to speed up the inference of a large target language model based on predictions of an auxiliary draft model. While effective, in application-specific settings, it often involves fine-tuning both draft and target models to achieve high acceptance rates. As the number of downstream tasks grows, these draft models add significant complexity to inference systems. We propose Speculative Streaming, a single-model speculative decoding method that fuses drafting into the target model by changing the fine-tuning objective from next token prediction to future n-gram prediction. Speculative Streaming speeds up decoding by 1.8 - 3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and Meaning Representation, without sacrificing generation quality. Additionally, Speculative Streaming is parameter-efficient. It achieves on-par/higher speed-ups than Medusa-style architectures while u
&lt;/p&gt;</description></item><item><title>BlendFilter&#36890;&#36807;&#26597;&#35810;&#29983;&#25104;&#28151;&#21512;&#21644;&#30693;&#35782;&#36807;&#28388;&#26041;&#27861;&#25552;&#21319;&#20102;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22810;&#39046;&#22495;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.11129</link><description>&lt;p&gt;
BlendFilter: &#36890;&#36807;&#26597;&#35810;&#29983;&#25104;&#28151;&#21512;&#21644;&#30693;&#35782;&#36807;&#28388;&#25512;&#36827;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BlendFilter: Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11129
&lt;/p&gt;
&lt;p&gt;
BlendFilter&#36890;&#36807;&#26597;&#35810;&#29983;&#25104;&#28151;&#21512;&#21644;&#30693;&#35782;&#36807;&#28388;&#26041;&#27861;&#25552;&#21319;&#20102;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22810;&#39046;&#22495;&#30340;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11129v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#25688;&#35201;&#65306;&#26816;&#32034;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25552;&#21319;&#30693;&#35782;&#23494;&#38598;&#22411;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32463;&#24120;&#38754;&#20020;&#22797;&#26434;&#36755;&#20837;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#30001;&#20110;&#22024;&#26434;&#30340;&#30693;&#35782;&#26816;&#32034;&#32780;&#36935;&#21040;&#22256;&#38590;&#65292;&#26126;&#26174;&#38459;&#30861;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BlendFilter&#65292;&#19968;&#31181;&#36890;&#36807;&#23558;&#26597;&#35810;&#29983;&#25104;&#28151;&#21512;&#19982;&#30693;&#35782;&#36807;&#28388;&#30456;&#32467;&#21512;&#26469;&#25552;&#21319;&#26816;&#32034;&#22686;&#24378;&#22411;LLM&#30340;&#26032;&#26041;&#27861;&#12290;BlendFilter&#25552;&#20986;&#20102;&#36890;&#36807;&#20854;&#26597;&#35810;&#29983;&#25104;&#26041;&#27861;&#30340;&#28151;&#21512;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#23558;&#22806;&#37096;&#30693;&#35782;&#21644;&#20869;&#37096;&#30693;&#35782;&#22686;&#24378;&#19982;&#21407;&#22987;&#26597;&#35810;&#30456;&#32467;&#21512;&#65292;&#30830;&#20445;&#20840;&#38754;&#25910;&#38598;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29420;&#29305;&#30340;&#30693;&#35782;&#36807;&#28388;&#27169;&#22359;&#20805;&#20998;&#21033;&#29992;&#20102;LLM&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;&#22810;&#20313;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#24320;&#25918;&#22495;&#38382;&#31572;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11129v1 Announce Type: new  Abstract: Retrieval-augmented Large Language Models (LLMs) offer substantial benefits in enhancing performance across knowledge-intensive scenarios. However, these methods often face challenges with complex inputs and encounter difficulties due to noisy knowledge retrieval, notably hindering model effectiveness. To address this issue, we introduce BlendFilter, a novel approach that elevates retrieval-augmented LLMs by integrating query generation blending with knowledge filtering. BlendFilter proposes the blending process through its query generation method, which integrates both external and internal knowledge augmentation with the original query, ensuring comprehensive information gathering. Additionally, our distinctive knowledge filtering module capitalizes on the intrinsic capabilities of the LLM, effectively eliminating extraneous data. We conduct extensive experiments on three open-domain question answering benchmarks, and the findings clea
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#39034;&#24207;&#35760;&#24518;&#32534;&#36753;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20462;&#25913;&#21442;&#25968;ME&#21487;&#33021;&#20250;&#23548;&#33268;&#25152;&#26377;&#20219;&#21153;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#20445;&#30041;&#21442;&#25968;ME&#21017;&#33021;&#22815;&#20445;&#25345;&#36739;&#22909;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.11122</link><description>&lt;p&gt;
&#23548;&#33322;&#21452;&#37325;&#38754;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#39034;&#24207;&#35760;&#24518;&#32534;&#36753;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Navigating the Dual Facets: A Comprehensive Evaluation of Sequential Memory Editing in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11122
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#39034;&#24207;&#35760;&#24518;&#32534;&#36753;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20462;&#25913;&#21442;&#25968;ME&#21487;&#33021;&#20250;&#23548;&#33268;&#25152;&#26377;&#20219;&#21153;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#20445;&#30041;&#21442;&#25968;ME&#21017;&#33021;&#22815;&#20445;&#25345;&#36739;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#32534;&#36753;&#65288;ME&#65289;&#24050;&#32463;&#25104;&#20026;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#38169;&#35823;&#20107;&#23454;&#25110;&#27880;&#20837;&#26032;&#20107;&#23454;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#23384;&#22312;&#20004;&#31181;&#20027;&#27969;ME&#26041;&#27861;&#65306;&#20462;&#25913;&#21442;&#25968;ME&#21644;&#20445;&#30041;&#21442;&#25968;ME&#65288;&#22312;&#20445;&#30041;&#21407;&#22987;&#21442;&#25968;&#30340;&#21516;&#26102;&#25972;&#21512;&#39069;&#22806;&#27169;&#22359;&#65289;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#20808;&#21069;&#23545;ME&#35780;&#20272;&#30340;&#30740;&#31350;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#65288;i&#65289;&#20165;&#35780;&#20272;&#24102;&#26377;&#21333;&#20010;&#32534;&#36753;&#30340;LLMs&#65292;&#24573;&#30053;&#20102;&#25345;&#32493;&#32534;&#36753;&#30340;&#38656;&#35201;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#35780;&#20272;&#20165;&#20851;&#27880;&#22522;&#26412;&#20107;&#23454;&#19977;&#20803;&#32452;&#65292;&#24573;&#35270;&#20102;&#26356;&#24191;&#27867;&#30340;LLM&#33021;&#21147;&#65292;&#22914;&#36923;&#36753;&#25512;&#29702;&#21644;&#38405;&#35835;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20197;&#19979;&#19977;&#28857;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#65306;&#65288;i&#65289;&#25105;&#20204;&#25506;&#32034;&#20102;ME&#22914;&#20309;&#24433;&#21709;LLMs&#30340;&#24191;&#27867;&#22522;&#26412;&#33021;&#21147;&#22312;&#39034;&#24207;&#32534;&#36753;&#19979;&#12290;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65306;&#22823;&#22810;&#25968;&#20462;&#25913;&#21442;&#25968;ME&#22312;&#20960;&#27425;&#39034;&#24207;&#32534;&#36753;&#21518;&#19968;&#36143;&#38477;&#20302;&#25152;&#26377;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11122v1 Announce Type: cross  Abstract: Memory Editing (ME) has emerged as an efficient method to modify erroneous facts or inject new facts into Large Language Models (LLMs). Two mainstream ME methods exist: parameter-modifying ME and parameter-preserving ME (integrating extra modules while preserving original parameters). Regrettably, previous studies on ME evaluation have two critical limitations: (i) evaluating LLMs with single edit only, neglecting the need for continuous editing, and (ii) evaluations focusing solely on basic factual triples, overlooking broader LLM capabilities like logical reasoning and reading understanding. This study addresses these limitations with contributions threefold: (i) We explore how ME affects a wide range of fundamental capabilities of LLMs under sequential editing. Experimental results reveal an intriguing phenomenon: Most parameter-modifying ME consistently degrade performance across all tasks after a few sequential edits. In contrast,
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#24863;&#21644;&#36947;&#24503;&#32500;&#24230;&#19978;&#22914;&#20309;&#20195;&#34920;&#19981;&#21516;&#32676;&#20307;&#65292;&#21457;&#29616;&#23427;&#20204;&#19982;&#24847;&#35782;&#24418;&#24577;&#22242;&#20307;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11114</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21453;&#26144;&#20102;&#35841;&#30340;&#24773;&#24863;&#21644;&#36947;&#24503;&#24773;&#24863;&#65311;
&lt;/p&gt;
&lt;p&gt;
Whose Emotions and Moral Sentiments Do Language Models Reflect?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11114
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#24863;&#21644;&#36947;&#24503;&#32500;&#24230;&#19978;&#22914;&#20309;&#20195;&#34920;&#19981;&#21516;&#32676;&#20307;&#65292;&#21457;&#29616;&#23427;&#20204;&#19982;&#24847;&#35782;&#24418;&#24577;&#22242;&#20307;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#24050;&#30693;&#26356;&#22909;&#22320;&#20195;&#34920;&#19968;&#20123;&#31038;&#20250;&#32676;&#20307;&#30340;&#35266;&#28857;&#65292;&#36825;&#21487;&#33021;&#20250;&#24433;&#21709;&#23427;&#20204;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20027;&#35266;&#20219;&#21153;&#19978;&#65292;&#27604;&#22914;&#20869;&#23481;&#31649;&#29702;&#21644;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#24773;&#24863;&#23545;&#40784;&#30340;&#38382;&#39064;&#65292;&#29992;&#26469;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#24863;&#21644;&#36947;&#24503;&#33394;&#35843;&#22914;&#20309;&#20195;&#34920;&#19981;&#21516;&#32676;&#20307;&#30340;&#24773;&#24863;&#12290;&#36890;&#36807;&#27604;&#36739;36&#20010;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#22238;&#24212;&#30340;&#24773;&#24863;&#19982;Twitter&#28040;&#24687;&#30340;&#24773;&#24863;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35821;&#35328;&#27169;&#22411;&#19982;&#20004;&#31181;&#24847;&#35782;&#24418;&#24577;&#22242;&#20307;&#23384;&#22312;&#26174;&#33879;&#19981;&#19968;&#33268;&#12290;&#21363;&#20351;&#22312;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#26397;&#30528;&#29305;&#23450;&#24847;&#35782;&#24418;&#24577;&#26041;&#21521;&#21457;&#23637;&#20043;&#21518;&#65292;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#20063;&#20173;&#28982;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11114v1 Announce Type: new  Abstract: Language models (LMs) are known to represent the perspectives of some social groups better than others, which may impact their performance, especially on subjective tasks such as content moderation and hate speech detection. To explore how LMs represent different perspectives, existing research focused on positional alignment, i.e., how closely the models mimic the opinions and stances of different groups, e.g., liberals or conservatives. However, human communication also encompasses emotional and moral dimensions. We define the problem of affective alignment, which measures how LMs' emotional and moral tone represents those of different groups. By comparing the affect of responses generated by 36 LMs to the affect of Twitter messages, we observe significant misalignment of LMs with both ideological groups. This misalignment is larger than the partisan divide in the U.S. Even after steering the LMs towards specific ideological perspectiv
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;TutorEval&#21644;TutorChat&#65292;&#36890;&#36807;TutorEval&#22522;&#20934;&#21487;&#20197;&#34913;&#37327;LMs&#20316;&#20026;&#31185;&#23398;&#21161;&#25163;&#30340;&#23454;&#38469;&#21487;&#29992;&#24615;&#65292;TutorChat&#25968;&#25454;&#38598;&#29992;&#20110;&#24494;&#35843;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.11111</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#31185;&#23398;&#23548;&#24072;
&lt;/p&gt;
&lt;p&gt;
Language Models as Science Tutors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11111
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;TutorEval&#21644;TutorChat&#65292;&#36890;&#36807;TutorEval&#22522;&#20934;&#21487;&#20197;&#34913;&#37327;LMs&#20316;&#20026;&#31185;&#23398;&#21161;&#25163;&#30340;&#23454;&#38469;&#21487;&#29992;&#24615;&#65292;TutorChat&#25968;&#25454;&#38598;&#29992;&#20110;&#24494;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#26368;&#36817;&#21462;&#24471;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#65292;&#26397;&#30528;&#35757;&#32451;&#20855;&#26377;&#36739;&#24378;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#26041;&#21521;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#21457;&#23637;&#24182;&#27809;&#26377;&#19987;&#27880;&#20110;LMs&#22312;&#31185;&#23398;&#25945;&#32946;&#20013;&#30340;&#23454;&#38469;&#29992;&#20363;&#65292;&#21253;&#25324;&#38656;&#35201;&#22788;&#29702;&#38271;&#31687;&#31185;&#23398;&#25991;&#26723;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;TutorEval&#21644;TutorChat&#12290;TutorEval&#26159;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#38382;&#31572;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#21547;&#26377;&#20851;STEM&#25945;&#31185;&#20070;&#38271;&#31687;&#31456;&#33410;&#30340;&#38382;&#39064;&#65292;&#30001;&#19987;&#23478;&#32534;&#20889;&#12290;TutorEval&#26377;&#21161;&#20110;&#34913;&#37327;LMs&#20316;&#20026;&#31185;&#23398;&#21161;&#25163;&#30340;&#23454;&#38469;&#21487;&#29992;&#24615;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#32467;&#21512;&#38271;&#19978;&#19979;&#25991;&#12289;&#33258;&#30001;&#29983;&#25104;&#21644;&#22810;&#23398;&#31185;&#31185;&#23398;&#30693;&#35782;&#30340;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#29616;&#26377;&#23545;&#35805;&#25968;&#25454;&#38598;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20250;&#23548;&#33268;TutorEval&#24615;&#33021;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;TutorChat&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;80,000&#20010;&#20851;&#20110;&#25945;&#31185;&#20070;&#30340;&#38271;&#21512;&#25104;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;TutorChat&#26469;&#24494;&#35843;Llemma&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11111v1 Announce Type: cross  Abstract: NLP has recently made exciting progress toward training language models (LMs) with strong scientific problem-solving skills. However, model development has not focused on real-life use-cases of LMs for science, including applications in education that require processing long scientific documents. To address this, we introduce TutorEval and TutorChat. TutorEval is a diverse question-answering benchmark consisting of questions about long chapters from STEM textbooks, written by experts. TutorEval helps measure real-life usability of LMs as scientific assistants, and it is the first benchmark combining long contexts, free-form generation, and multi-disciplinary scientific knowledge. Moreover, we show that fine-tuning base models with existing dialogue datasets leads to poor performance on TutorEval. Therefore, we create TutorChat, a dataset of 80,000 long synthetic dialogues about textbooks. We use TutorChat to fine-tune Llemma models wit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35884;&#35823;&#29702;&#35299;&#22522;&#20934;FLUB&#65292;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#29702;&#35299;&#33021;&#21147;&#19978;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#35774;&#35745;&#29409;&#29502;&#38382;&#39064;&#35780;&#20272;LLMs&#30340;&#35884;&#35823;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.11100</link><description>&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#21040;&#29409;&#29502;&#38382;&#39064;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35884;&#35823;&#29702;&#35299;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
When LLMs Meet Cunning Questions: A Fallacy Understanding Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11100
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35884;&#35823;&#29702;&#35299;&#22522;&#20934;FLUB&#65292;&#25361;&#25112;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#29702;&#35299;&#33021;&#21147;&#19978;&#65292;&#37325;&#28857;&#26159;&#36890;&#36807;&#35774;&#35745;&#29409;&#29502;&#38382;&#39064;&#35780;&#20272;LLMs&#30340;&#35884;&#35823;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#21517;&#20026;FaLlacy Understanding Benchmark (FLUB)&#30340;&#22522;&#20934;&#26469;&#25361;&#25112;LLMs&#30340;&#25512;&#29702;&#21644;&#29702;&#35299;&#33021;&#21147;&#65292;&#20854;&#20013;&#21253;&#21547;&#26131;&#20110;&#20154;&#31867;&#29702;&#35299;&#20294;&#38590;&#20110;&#27169;&#22411;&#25226;&#25569;&#30340;&#29409;&#29502;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;FLUB&#19987;&#27880;&#20110;&#20174;&#30495;&#23454;&#20114;&#32852;&#32593;&#29615;&#22659;&#20013;&#25910;&#38598;&#21040;&#30340;&#26840;&#25163;&#12289;&#24189;&#40664;&#21644;&#35823;&#23548;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#38590;&#24230;&#36882;&#22686;&#30340;&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#35884;&#35823;&#29702;&#35299;&#33021;&#21147;&#12290;&#22522;&#20110;FLUB&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;&#20195;&#34920;&#24615;&#30340;&#21644;&#20808;&#36827;&#30340;LLMs&#30340;&#34920;&#29616;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;FLUB&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24182;&#20540;&#24471;&#26410;&#26469;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11100v1 Announce Type: new  Abstract: Recently, Large Language Models (LLMs) have made remarkable evolutions in language understanding and generation. Following this, various benchmarks for measuring all kinds of capabilities of LLMs have sprung up. In this paper, we challenge the reasoning and understanding abilities of LLMs by proposing a FaLlacy Understanding Benchmark (FLUB) containing cunning questions that are easy for humans to understand but difficult for models to grasp. Specifically, the cunning questions that FLUB focuses on mainly consist of the tricky, humorous, and misleading questions collected from the real internet environment. And we design three tasks with increasing difficulty in the FLUB benchmark to evaluate the fallacy understanding ability of LLMs. Based on FLUB, we investigate the performance of multiple representative and advanced LLMs, reflecting our FLUB is challenging and worthy of more future study. Interesting discoveries and valuable insights 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#27604;&#36739;&#20102;&#32463;&#20856;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#22522;&#20110;LLM&#30340;&#35789;&#23884;&#20837;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#23558;&#35821;&#20041;&#30456;&#20851;&#30340;&#21333;&#35789;&#26356;&#32039;&#23494;&#22320;&#32858;&#31867;&#22312;&#19968;&#36215;&#65292;&#24182;&#22312;Bigger Analogy Test Set&#65288;BATS&#65289;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.11094</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#35789;&#23884;&#20837;&#65306;LLMs&#26159;&#21542;&#25552;&#20379;&#26032;&#30340;&#19996;&#35199;&#65311;
&lt;/p&gt;
&lt;p&gt;
Word Embeddings Revisited: Do LLMs Offer Something New?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11094
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#27604;&#36739;&#20102;&#32463;&#20856;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#22522;&#20110;LLM&#30340;&#35789;&#23884;&#20837;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#23558;&#35821;&#20041;&#30456;&#20851;&#30340;&#21333;&#35789;&#26356;&#32039;&#23494;&#22320;&#32858;&#31867;&#22312;&#19968;&#36215;&#65292;&#24182;&#22312;Bigger Analogy Test Set&#65288;BATS&#65289;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26377;&#24847;&#20041;&#30340;&#35789;&#23884;&#20837;&#23545;&#20110;&#35757;&#32451;&#31283;&#20581;&#30340;&#35821;&#35328;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#20852;&#36215;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#35768;&#22810;&#26032;&#30340;&#21333;&#35789;/&#21477;&#23376;/&#25991;&#26723;&#23884;&#20837;&#27169;&#22411;&#12290;&#23613;&#31649;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#36827;&#27493;&#65292;&#20294;&#20173;&#19981;&#28165;&#26970;&#24615;&#33021;&#30340;&#25552;&#21319;&#20165;&#20165;&#26159;&#22240;&#20026;&#35268;&#27169;&#36824;&#26159;&#23427;&#20204;&#29983;&#25104;&#30340;&#24213;&#23618;&#23884;&#20837;&#19982;&#21477;&#23376;-BERT&#65288;SBERT&#65289;&#25110;&#36890;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#65288;USE&#65289;&#20043;&#31867;&#30340;&#20256;&#32479;&#32534;&#30721;&#27169;&#22411;&#26377;&#26174;&#33879;&#21306;&#21035;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#32463;&#20856;&#35789;&#23884;&#20837;&#25216;&#26415;&#19982;&#22522;&#20110;LLM&#30340;&#35789;&#23884;&#20837;&#65292;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20174;&#23427;&#20204;&#30340;&#28508;&#22312;&#21521;&#37327;&#35821;&#20041;&#26041;&#38754;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#20542;&#21521;&#20110;&#23558;&#35821;&#20041;&#30456;&#20851;&#30340;&#21333;&#35789;&#26356;&#32039;&#23494;&#22320;&#32858;&#31867;&#22312;&#19968;&#36215;&#65292;LLMs&#22312;Bigger Analogy Test Set&#65288;BATS&#65289;&#19978;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#20063;&#39640;&#20110;&#32463;&#20856;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#19968;&#20123;LLMs&#20542;&#21521;&#20110;&#20135;&#29983;&#35789;&#23884;&#20837;si&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11094v1 Announce Type: new  Abstract: Learning meaningful word embeddings is key to training a robust language model. The recent rise of Large Language Models (LLMs) has provided us with many new word/sentence/document embedding models. Although LLMs have shown remarkable advancement in various NLP tasks, it is still unclear whether the performance improvement is merely because of scale or whether underlying embeddings they produce significantly differ from classical encoding models like Sentence-BERT (SBERT) or Universal Sentence Encoder (USE). This paper systematically investigates this issue by comparing classical word embedding techniques against LLM-based word embeddings in terms of their latent vector semantics. Our results show that LLMs tend to cluster semantically related words more tightly than classical models. LLMs also yield higher average accuracy on the Bigger Analogy Test Set (BATS) over classical methods. Finally, some LLMs tend to produce word embeddings si
&lt;/p&gt;</description></item><item><title>&#32431;&#24494;&#35843;&#36890;&#36807;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#12289;&#22686;&#21152;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#30340;&#25968;&#25454;&#65292;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#21462;&#24471;&#20102;&#19981;&#20439;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.11078</link><description>&lt;p&gt;
&#36890;&#36807;&#32431;&#24494;&#35843;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Model Editing by Pure Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11078
&lt;/p&gt;
&lt;p&gt;
&#32431;&#24494;&#35843;&#36890;&#36807;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#12289;&#22686;&#21152;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#30340;&#25968;&#25454;&#65292;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#21462;&#24471;&#20102;&#19981;&#20439;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#32454;&#35843;&#25972;&#34987;&#35748;&#20026;&#22312;&#27169;&#22411;&#32534;&#36753;&#20013;&#19981;&#22815;&#26377;&#25928;&#65292;&#22240;&#20026;&#30456;&#23545;&#26356;&#19987;&#19994;&#30340;&#26041;&#27861;&#32780;&#35328;&#65292;&#23427;&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#26159;&#31616;&#21333;&#30340;&#65292;&#19981;&#20851;&#24515;&#34987;&#32534;&#36753;&#27169;&#22411;&#30340;&#20307;&#31995;&#32467;&#26500;&#32454;&#33410;&#65292;&#24182;&#19988;&#33021;&#22815;&#21033;&#29992;&#26631;&#20934;&#35757;&#32451;&#26041;&#27861;&#30340;&#19981;&#26029;&#36827;&#23637;&#65288;&#20363;&#22914;PEFT&#65289;&#65292;&#20351;&#20854;&#25104;&#20026;&#27169;&#22411;&#32534;&#36753;&#22120;&#30340;&#21560;&#24341;&#36873;&#25321;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32431;&#31929;&#30340;&#24494;&#35843;&#21487;&#20197;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#26420;&#32032;&#24494;&#35843;&#36827;&#34892;&#36731;&#24494;&#20462;&#25913;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#31532;&#19968;&#65292;&#25105;&#20204;&#20248;&#21270;&#26465;&#20214;&#20284;&#28982;&#32780;&#38750;&#23436;&#25972;&#20284;&#28982;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#37322;&#20041;&#21644;&#20107;&#23454;&#26469;&#22686;&#21152;&#25968;&#25454;&#65292;&#20197;&#40723;&#21169;&#27867;&#21270;&#21644;&#23616;&#37096;&#24615;&#12290;&#25105;&#20204;&#22312;ZsRE&#21644;CounterFact&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#19968;&#31616;&#21333;&#20462;&#25913;&#20351;&#24471;&#24494;&#35843;&#36890;&#24120;&#21487;&#20197;&#19982;&#19987;&#19994;&#32534;&#36753;&#22120;&#22312;&#32534;&#36753;&#20998;&#25968;&#26041;&#38754;&#21305;&#25932;&#29978;&#33267;&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11078v1 Announce Type: cross  Abstract: Fine-tuning is dismissed as not effective for model editing due to its poor performance compared to more specialized methods. However, fine-tuning is simple, agnostic to the architectural details of the model being edited, and able to leverage ongoing advances in standard training methods (e.g., PEFT), making it an appealing choice for a model editor. In this work, we show that pure fine-tuning can be a viable approach to model editing. We propose a slight modification of naive fine-tuning with two key ingredients. First, we optimize the conditional likelihood rather than the full likelihood. Second, we augment the data with random paraphrases and facts to encourage generalization and locality. Our experiments on ZsRE and CounterFact show that this simple modification allows fine-tuning to often match or outperform specialized editors in the edit score.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550; AFaCTA&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#26631;&#27880;&#65292;&#25552;&#39640;&#20102;&#26631;&#27880;&#30340;&#25928;&#29575;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11073</link><description>&lt;p&gt;
AFaCTA: &#20351;&#29992;&#21487;&#38752;&#30340;LLM&#26631;&#27880;&#32773;&#36741;&#21161;&#20107;&#23454;&#24615;&#32034;&#36180;&#26816;&#27979;&#30340;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11073
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550; AFaCTA&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#26631;&#27880;&#65292;&#25552;&#39640;&#20102;&#26631;&#27880;&#30340;&#25928;&#29575;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36215;&#65292;&#29992;&#20110;&#25171;&#20987;&#35823;&#23548;&#20449;&#24687;&#30340;&#33258;&#21160;&#20107;&#23454;&#26680;&#26597;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20107;&#23454;&#24615;&#32034;&#36180;&#26816;&#27979;&#65292;&#21363;&#20107;&#23454;&#26680;&#26597;&#31649;&#36947;&#20013;&#30340;&#31532;&#19968;&#27493;&#65292;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#38480;&#21046;&#20102;&#20854;&#21487;&#20280;&#32553;&#24615;&#21644;&#27867;&#21270;&#24615;&#65306;&#65288;1&#65289;&#20219;&#21153;&#23450;&#20041;&#21644;&#32034;&#36180;&#27010;&#24565;&#30340;&#19981;&#19968;&#33268;&#24615;&#20197;&#21450;&#65288;2&#65289;&#25163;&#21160;&#26631;&#27880;&#30340;&#39640;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#65288;1&#65289;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#30456;&#20851;&#24037;&#20316;&#20013;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32858;&#28966;&#20110;&#21487;&#39564;&#35777;&#24615;&#30340;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#32479;&#19968;&#23450;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#65288;2&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AFaCTA&#65288;&#33258;&#21160;&#20107;&#23454;&#24615;&#32034;&#36180;&#26816;&#27979;&#26631;&#27880;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20107;&#23454;&#24615;&#32034;&#36180;&#30340;&#26631;&#27880;&#20013;&#25552;&#20379;&#24110;&#21161;&#12290;AFaCTA&#36890;&#36807;&#27839;&#30528;&#19977;&#26465;&#39044;&#23450;&#20041;&#30340;&#25512;&#29702;&#36335;&#24452;&#20445;&#25345;&#19968;&#33268;&#24615;&#26469;&#26657;&#20934;&#20854;&#27880;&#37322;&#30340;&#32622;&#20449;&#24230;&#12290;&#22312;&#25919;&#27835;&#35328;&#35770;&#39046;&#22495;&#30340;&#22823;&#37327;&#35780;&#20272;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;AFaCTA&#33021;&#22815;&#39640;&#25928;&#22320;&#21327;&#21161;&#19987;&#19994;&#20154;&#21592;&#36827;&#34892;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11073v1 Announce Type: cross  Abstract: With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist exper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT4&#65289;&#25972;&#21512;&#21040;&#22240;&#26524;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#26102;&#23545;&#20803;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#21019;&#26032;&#21033;&#29992;&#65292;&#24378;&#35843;&#20102;LLMs&#22312;&#22686;&#24378;&#20256;&#32479;CD&#26041;&#27861;&#21644;&#20316;&#20026;&#19987;&#23478;&#36741;&#21161;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11068</link><description>&lt;p&gt;
&#26550;&#36215;&#22240;&#26524;&#21457;&#29616;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#26725;&#26753;&#65306;&#25972;&#21512;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#21512;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Bridging Causal Discovery and Large Language Models: A Comprehensive Survey of Integrative Approaches and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT4&#65289;&#25972;&#21512;&#21040;&#22240;&#26524;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#26102;&#23545;&#20803;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#21019;&#26032;&#21033;&#29992;&#65292;&#24378;&#35843;&#20102;LLMs&#22312;&#22686;&#24378;&#20256;&#32479;CD&#26041;&#27861;&#21644;&#20316;&#20026;&#19987;&#23478;&#36741;&#21161;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#65288;CD&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#34920;&#30528;&#20004;&#20010;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#39046;&#22495;&#12290;&#23613;&#31649;&#23427;&#20204;&#36215;&#28304;&#19981;&#21516;&#65292;CD&#20391;&#37325;&#20110;&#20174;&#25968;&#25454;&#20013;&#25581;&#31034;&#22240;&#26524;&#20851;&#31995;&#65292;LLMs&#21017;&#20391;&#37325;&#20110;&#22788;&#29702;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#25991;&#26412;&#65292;&#20294;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#34701;&#21512;&#20026;&#29702;&#35299;&#22797;&#26434;&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#35265;&#35299;&#21644;&#26041;&#27861;&#35770;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23558;LLMs&#65288;&#22914;GPT4&#65289;&#25972;&#21512;&#21040;CD&#20219;&#21153;&#20013;&#30340;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#23457;&#26597;&#21644;&#27604;&#36739;&#20102;&#21033;&#29992;LLMs&#36827;&#34892;&#21508;&#31181;CD&#20219;&#21153;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#31361;&#20986;&#20102;&#23427;&#20204;&#23545;&#20803;&#25968;&#25454;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#21019;&#26032;&#21033;&#29992;&#20197;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;LLMs&#22312;&#22686;&#24378;&#20256;&#32479;CD&#26041;&#27861;&#21644;&#20316;&#20026;&#19981;&#23436;&#32654;&#19987;&#23478;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#24403;&#21069;&#23454;&#36341;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11068v1 Announce Type: cross  Abstract: Causal discovery (CD) and Large Language Models (LLMs) represent two emerging fields of study with significant implications for artificial intelligence. Despite their distinct origins, CD focuses on uncovering cause-effect relationships from data, and LLMs on processing and generating humanlike text, the convergence of these domains offers novel insights and methodologies for understanding complex systems. This paper presents a comprehensive survey of the integration of LLMs, such as GPT4, into CD tasks. We systematically review and compare existing approaches that leverage LLMs for various CD tasks and highlight their innovative use of metadata and natural language to infer causal structures. Our analysis reveals the strengths and potential of LLMs in both enhancing traditional CD methods and as an imperfect expert, alongside the challenges and limitations inherent in current practices. Furthermore, we identify gaps in the literature 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102; Persona-DB&#65292;&#19968;&#20010;&#31616;&#21333;&#21364;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#32423;&#26500;&#24314;&#36807;&#31243;&#21644;&#21327;&#21516;&#20248;&#21270;&#65292;&#25913;&#21892;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#20013;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26816;&#32034;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.11060</link><description>&lt;p&gt;
Persona-DB&#65306;&#29992;&#20110;&#21709;&#24212;&#39044;&#27979;&#30340;&#39640;&#25928;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#19982;&#21327;&#21516;&#25968;&#25454;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Persona-DB: Efficient Large Language Model Personalization for Response Prediction with Collaborative Data Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11060
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102; Persona-DB&#65292;&#19968;&#20010;&#31616;&#21333;&#21364;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#32423;&#26500;&#24314;&#36807;&#31243;&#21644;&#21327;&#21516;&#20248;&#21270;&#65292;&#25913;&#21892;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#20013;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#26816;&#32034;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20010;&#24615;&#21270;&#20132;&#20114;&#38656;&#27714;&#30340;&#22686;&#21152;&#65292;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#20934;&#30830;&#24555;&#36895;&#35782;&#21035;&#29992;&#25143;&#24847;&#35265;&#21644;&#20559;&#22909;&#30340;&#26041;&#27861;&#12290;&#26816;&#32034;&#22686;&#24378;&#20316;&#20026;&#19968;&#31181;&#26377;&#25928;&#31574;&#30053;&#20986;&#29616;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36866;&#24212;&#22823;&#37327;&#29992;&#25143;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#30340;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22686;&#24378;&#26816;&#32034;&#38454;&#27573;&#65292;&#24182;&#23545;&#25968;&#25454;&#24211;&#34920;&#31034;&#30340;&#20248;&#21270;&#36827;&#34892;&#20102;&#26377;&#38480;&#30340;&#25506;&#32034;&#65292;&#36825;&#26159;&#20010;&#24615;&#21270;&#31561;&#20219;&#21153;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#19968;&#20010;&#26032;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#30528;&#37325;&#20110;&#22914;&#20309;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#25968;&#25454;&#65292;&#20197;&#20415;&#22312;LLM&#23450;&#21046;&#30340;&#24773;&#22659;&#19979;&#26356;&#26377;&#25928;&#22320;&#36827;&#34892;&#26816;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Persona-DB&#65292;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;&#20998;&#23618;&#26500;&#24314;&#36807;&#31243;&#65292;&#20197;&#25913;&#21892;&#36328;&#20219;&#21153;&#32972;&#26223;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36827;&#34892;&#21327;&#21516;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11060v1 Announce Type: cross  Abstract: The increasing demand for personalized interactions with large language models (LLMs) calls for the development of methodologies capable of accurately and efficiently identifying user opinions and preferences. Retrieval augmentation emerges as an effective strategy, as it can accommodate a vast number of users without the costs from fine-tuning. Existing research, however, has largely focused on enhancing the retrieval stage and devoted limited exploration toward optimizing the representation of the database, a crucial aspect for tasks such as personalization. In this work, we examine the problem from a novel angle, focusing on how data can be better represented for more efficient retrieval in the context of LLM customization. To tackle this challenge, we introduce Persona-DB, a simple yet effective framework consisting of a hierarchical construction process to improve generalization across task contexts and collaborative refinement to
&lt;/p&gt;</description></item><item><title>II-MMR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25913;&#36827;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#22810;&#27169;&#24577;&#22810;&#36339;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#31572;&#26696;&#39044;&#27979;&#24341;&#23548;&#30340;CoT&#25552;&#31034;&#21644;&#30693;&#35782;&#19977;&#20803;&#32452;&#24341;&#23548;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#23545;&#19981;&#21516;&#25512;&#29702;&#24773;&#20917;&#30340;&#20998;&#26512;&#21644;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.11058</link><description>&lt;p&gt;
II-MMR&#65306;&#22312;&#35270;&#35273;&#38382;&#31572;&#20013;&#35782;&#21035;&#21644;&#25913;&#36827;&#22810;&#27169;&#24577;&#22810;&#36339;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
II-MMR: Identifying and Improving Multi-modal Multi-hop Reasoning in Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11058
&lt;/p&gt;
&lt;p&gt;
II-MMR&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25913;&#36827;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#22810;&#27169;&#24577;&#22810;&#36339;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#31572;&#26696;&#39044;&#27979;&#24341;&#23548;&#30340;CoT&#25552;&#31034;&#21644;&#30693;&#35782;&#19977;&#20803;&#32452;&#24341;&#23548;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#23545;&#19981;&#21516;&#25512;&#29702;&#24773;&#20917;&#30340;&#20998;&#26512;&#21644;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#36890;&#24120;&#28041;&#21450;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#22810;&#26679;&#25512;&#29702;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;VQA&#30740;&#31350;&#20165;&#20851;&#27880;&#35780;&#20272;&#27169;&#22411;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#65292;&#32780;&#27809;&#26377;&#22312;&#19981;&#21516;&#25512;&#29702;&#24773;&#20917;&#19979;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;"CoT"&#25552;&#31034;&#26080;&#27861;&#26377;&#25928;&#29983;&#25104;VQA&#30340;&#25512;&#29702;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38656;&#35201;&#22810;&#36339;&#25512;&#29702;&#30340;&#22797;&#26434;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;II-MMR&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24819;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25913;&#36827;VQA&#20013;&#30340;&#22810;&#27169;&#24577;&#22810;&#36339;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;II-MMR&#25509;&#21463;&#24102;&#26377;&#22270;&#20687;&#30340;VQA&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#25552;&#31034;&#25214;&#21040;&#25512;&#29702;&#36335;&#24452;&#20197;&#33719;&#24471;&#31572;&#26696;&#65306;(i)&#31572;&#26696;&#39044;&#27979;&#24341;&#23548;&#30340;CoT&#25552;&#31034;&#65292;&#25110;&#32773;(ii)&#30693;&#35782;&#19977;&#20803;&#32452;&#24341;&#23548;&#30340;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;II-MMR&#20998;&#26512;&#36825;&#26465;&#36335;&#24452;&#65292;&#36890;&#36807;&#20272;&#35745;&#26377;&#22810;&#23569;&#36339;&#21644;&#20160;&#20040;&#31867;&#22411;&#65288;&#21363;&#35270;&#35273;&#25110;&#36229;&#20986;&#65289;&#26469;&#35782;&#21035;&#24403;&#21069;VQA&#22522;&#20934;&#20013;&#30340;&#19981;&#21516;&#25512;&#29702;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11058v1 Announce Type: cross  Abstract: Visual Question Answering (VQA) often involves diverse reasoning scenarios across Vision and Language (V&amp;L). Most prior VQA studies, however, have merely focused on assessing the model's overall accuracy without evaluating it on different reasoning cases. Furthermore, some recent works observe that conventional Chain-of-Thought (CoT) prompting fails to generate effective reasoning for VQA, especially for complex scenarios requiring multi-hop reasoning. In this paper, we propose II-MMR, a novel idea to identify and improve multi-modal multi-hop reasoning in VQA. In specific, II-MMR takes a VQA question with an image and finds a reasoning path to reach its answer using two novel language promptings: (i) answer prediction-guided CoT prompt, or (ii) knowledge triplet-guided prompt. II-MMR then analyzes this path to identify different reasoning cases in current VQA benchmarks by estimating how many hops and what types (i.e., visual or beyon
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Conan&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;&#20390;&#25506;&#21465;&#20107;&#20013;&#25552;&#21462;&#21644;&#20998;&#26512;&#22797;&#26434;&#30340;&#20154;&#29289;&#20851;&#31995;&#22270;&#65292;&#24182;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#22797;&#26434;&#20851;&#31995;&#21644;&#22788;&#29702;&#38271;&#31687;&#21465;&#20107;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11051</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30701;&#26495;&#65306;&#29702;&#35299;&#20390;&#25506;&#21465;&#20107;&#20013;&#22797;&#26434;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Fall Short: Understanding Complex Relationships in Detective Narratives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11051
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Conan&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20174;&#20390;&#25506;&#21465;&#20107;&#20013;&#25552;&#21462;&#21644;&#20998;&#26512;&#22797;&#26434;&#30340;&#20154;&#29289;&#20851;&#31995;&#22270;&#65292;&#24182;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#22797;&#26434;&#20851;&#31995;&#21644;&#22788;&#29702;&#38271;&#31687;&#21465;&#20107;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#21465;&#20107;&#29702;&#35299;&#25968;&#25454;&#38598;&#24448;&#24448;&#26080;&#27861;&#34920;&#36798;&#29616;&#23454;&#31038;&#20250;&#22330;&#26223;&#20013;&#20851;&#31995;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;Conan&#65292;&#26088;&#22312;&#20174;&#20390;&#25506;&#21465;&#20107;&#20013;&#25552;&#21462;&#21644;&#20998;&#26512;&#22797;&#26434;&#30340;&#20154;&#29289;&#20851;&#31995;&#22270;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20998;&#23618;&#20851;&#31995;&#31867;&#21035;&#65292;&#24182;&#20174;&#19981;&#21516;&#35282;&#33394;&#30340;&#35282;&#24230;&#25163;&#21160;&#25552;&#21462;&#21644;&#27880;&#37322;&#20102;&#20197;&#35282;&#33394;&#20026;&#23548;&#21521;&#30340;&#20851;&#31995;&#65292;&#21253;&#25324;&#22823;&#22810;&#25968;&#35282;&#33394;&#30693;&#26195;&#30340;&#20844;&#24320;&#20851;&#31995;&#21644;&#20165;&#23569;&#25968;&#35282;&#33394;&#30693;&#26195;&#30340;&#31192;&#23494;&#20851;&#31995;&#12290;&#25105;&#20204;&#23545;GPT-3.5&#12289;GPT-4&#21644;Llama2&#31561;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#25512;&#29702;&#22797;&#26434;&#20851;&#31995;&#21644;&#22788;&#29702;&#36739;&#38271;&#21465;&#20107;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;Conan&#25968;&#25454;&#38598;&#19982;&#25105;&#20204;&#30340;&#27969;&#31243;&#31574;&#30053;&#30340;&#32467;&#21512;&#26088;&#22312;&#20102;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#21465;&#20107;&#32972;&#26223;&#20013;&#24494;&#22937;&#20851;&#31995;&#21160;&#24577;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11051v1 Announce Type: cross  Abstract: Existing datasets for narrative understanding often fail to represent the complexity and uncertainty of relationships in real-life social scenarios. To address this gap, we introduce a new benchmark, Conan, designed for extracting and analysing intricate character relation graphs from detective narratives. Specifically, we designed hierarchical relationship categories and manually extracted and annotated role-oriented relationships from the perspectives of various characters, incorporating both public relationships known to most characters and secret ones known to only a few. Our experiments with advanced Large Language Models (LLMs) like GPT-3.5, GPT-4, and Llama2 reveal their limitations in inferencing complex relationships and handling longer narratives. The combination of the Conan dataset and our pipeline strategy is geared towards understanding the ability of LLMs to comprehend nuanced relational dynamics in narrative contexts.
&lt;/p&gt;</description></item><item><title>DPR&#24494;&#35843;&#39044;&#35757;&#32451;&#32593;&#32476;&#20197;&#22686;&#24378;&#26597;&#35810;&#21644;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#23884;&#20837;&#23545;&#40784;&#65292;&#21457;&#29616;&#35757;&#32451;&#20013;&#30693;&#35782;&#21435;&#20013;&#24515;&#21270;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#27169;&#22411;&#20869;&#37096;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;</title><link>https://arxiv.org/abs/2402.11035</link><description>&lt;p&gt;
&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#65306;&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#26159;&#21542;&#22312;&#26816;&#32034;&#20013;&#65311;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generation: Is Dense Passage Retrieval Retrieving?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11035
&lt;/p&gt;
&lt;p&gt;
DPR&#24494;&#35843;&#39044;&#35757;&#32451;&#32593;&#32476;&#20197;&#22686;&#24378;&#26597;&#35810;&#21644;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#23884;&#20837;&#23545;&#40784;&#65292;&#21457;&#29616;&#35757;&#32451;&#20013;&#30693;&#35782;&#21435;&#20013;&#24515;&#21270;&#65292;&#20294;&#20063;&#25581;&#31034;&#20102;&#27169;&#22411;&#20869;&#37096;&#30693;&#35782;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#65288;DPR&#65289;&#26159;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24615;&#33021;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#33539;&#24335;&#20013;&#30340;&#31532;&#19968;&#27493;&#12290; DPR&#24494;&#35843;&#39044;&#35757;&#32451;&#32593;&#32476;&#65292;&#20197;&#22686;&#24378;&#26597;&#35810;&#21644;&#30456;&#20851;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#30340;&#23884;&#20837;&#23545;&#40784;&#12290;&#23545;DPR&#24494;&#35843;&#30340;&#28145;&#20837;&#29702;&#35299;&#23558;&#38656;&#35201;&#20174;&#26681;&#26412;&#19978;&#37322;&#25918;&#35813;&#26041;&#27861;&#30340;&#20840;&#37096;&#28508;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#25506;&#38024;&#12289;&#23618;&#28608;&#27963;&#20998;&#26512;&#21644;&#27169;&#22411;&#32534;&#36753;&#30340;&#32452;&#21512;&#65292;&#26426;&#26800;&#22320;&#25506;&#32034;&#20102;DPR&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DPR&#35757;&#32451;&#20351;&#32593;&#32476;&#20013;&#23384;&#20648;&#30693;&#35782;&#30340;&#26041;&#24335;&#21435;&#20013;&#24515;&#21270;&#65292;&#21019;&#24314;&#20102;&#35775;&#38382;&#30456;&#21516;&#20449;&#24687;&#30340;&#22810;&#20010;&#36335;&#24452;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#36825;&#31181;&#35757;&#32451;&#39118;&#26684;&#30340;&#23616;&#38480;&#24615;&#65306;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20869;&#37096;&#30693;&#35782;&#38480;&#21046;&#20102;&#26816;&#32034;&#27169;&#22411;&#21487;&#20197;&#26816;&#32034;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#23494;&#38598;&#26816;&#32034;&#25552;&#20986;&#20102;&#19968;&#20123;&#21487;&#33021;&#30340;&#26041;&#21521;&#65306;&#65288;1&#65289;&#26292;&#38706;DPR&#35757;&#32451;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11035v1 Announce Type: new  Abstract: Dense passage retrieval (DPR) is the first step in the retrieval augmented generation (RAG) paradigm for improving the performance of large language models (LLM). DPR fine-tunes pre-trained networks to enhance the alignment of the embeddings between queries and relevant textual data. A deeper understanding of DPR fine-tuning will be required to fundamentally unlock the full potential of this approach. In this work, we explore DPR-trained models mechanistically by using a combination of probing, layer activation analysis, and model editing. Our experiments show that DPR training decentralizes how knowledge is stored in the network, creating multiple access pathways to the same information. We also uncover a limitation in this training style: the internal knowledge of the pre-trained model bounds what the retrieval model can retrieve. These findings suggest a few possible directions for dense retrieval: (1) expose the DPR training process 
&lt;/p&gt;</description></item><item><title>PAT-Questions&#22522;&#20934;&#29992;&#20110;&#29616;&#22312;&#26102;&#21051;&#20026;&#38170;&#28857;&#30340;&#26102;&#38388;&#38382;&#31572;&#65292;&#36890;&#36807;&#33258;&#21160;&#21047;&#26032;&#31572;&#26696;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#36807;&#26102;&#12289;&#22797;&#26434;&#26102;&#38388;&#20851;&#31995;&#38590;&#20197;&#25512;&#29702;&#12289;&#21487;&#33021;&#38656;&#35201;&#22810;&#36339;&#25512;&#29702;&#20197;&#21450;&#22522;&#20934;&#31572;&#26696;&#25345;&#32493;&#26356;&#26032;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.11034</link><description>&lt;p&gt;
PAT-Questions&#65306;&#19968;&#20010;&#29992;&#20110;&#29616;&#22312;&#26102;&#21051;&#20026;&#38170;&#28857;&#30340;&#26102;&#38388;&#38382;&#31572;&#33258;&#26356;&#26032;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PAT-Questions: A Self-Updating Benchmark for Present-Anchored Temporal Question-Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11034
&lt;/p&gt;
&lt;p&gt;
PAT-Questions&#22522;&#20934;&#29992;&#20110;&#29616;&#22312;&#26102;&#21051;&#20026;&#38170;&#28857;&#30340;&#26102;&#38388;&#38382;&#31572;&#65292;&#36890;&#36807;&#33258;&#21160;&#21047;&#26032;&#31572;&#26696;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#36807;&#26102;&#12289;&#22797;&#26434;&#26102;&#38388;&#20851;&#31995;&#38590;&#20197;&#25512;&#29702;&#12289;&#21487;&#33021;&#38656;&#35201;&#22810;&#36339;&#25512;&#29702;&#20197;&#21450;&#22522;&#20934;&#31572;&#26696;&#25345;&#32493;&#26356;&#26032;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#26102;&#38388;&#38382;&#31572;&#65288;TQA&#65289;&#30340;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#38170;&#23450;&#29305;&#23450;&#26102;&#38388;&#25139;&#25110;&#20107;&#20214;&#30340;&#38382;&#39064;&#19978;&#65288;&#20363;&#22914;&#8220;1970&#24180;&#35841;&#26159;&#32654;&#22269;&#24635;&#32479;&#65311;&#8221;&#65289;&#12290;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#20854;&#26102;&#38388;&#32972;&#26223;&#30456;&#23545;&#20110;&#24403;&#21069;&#26102;&#38388;&#30340;&#38382;&#39064;&#65288;&#20363;&#22914;&#8220;&#20043;&#21069;&#30340;&#32654;&#22269;&#24635;&#32479;&#26159;&#35841;&#65311;&#8221;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#31216;&#20026;&#29616;&#22312;&#26102;&#21051;&#20026;&#38170;&#30340;&#26102;&#38388;&#38382;&#31572;&#65288;PATQA&#65289;&#12290;PATQA&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#20855;&#26377;&#36807;&#26102;&#30340;&#30693;&#35782;&#65292;&#65288;2&#65289;&#22797;&#26434;&#30340;&#26102;&#38388;&#20851;&#31995;&#65288;&#20363;&#22914;&#8220;&#20043;&#21069;&#8221;&#65292;&#8220;&#20197;&#21069;&#8221;&#65289;&#38590;&#20197;&#25512;&#29702;&#65292;&#65288;3&#65289;&#21487;&#33021;&#38656;&#35201;&#22810;&#36339;&#25512;&#29702;&#65292;&#65288;4&#65289;&#22522;&#20934;&#30340;&#27491;&#30830;&#31572;&#26696;&#24517;&#39035;&#25345;&#32493;&#26356;&#26032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PAT-Questions&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#21333;&#36339;&#21644;&#22810;&#36339;&#26102;&#38388;&#38382;&#39064;&#12290;PAT-Questions&#20013;&#30340;&#31572;&#26696;&#21487;&#20197;&#36890;&#36807;&#22312;&#30693;&#35782;&#22270;&#19978;&#37325;&#26032;&#36816;&#34892;SPARQL&#26597;&#35810;&#26469;&#33258;&#21160;&#21047;&#26032;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11034v1 Announce Type: new  Abstract: Existing work on Temporal Question Answering (TQA) has predominantly focused on questions anchored to specific timestamps or events (e.g. "Who was the US president in 1970?"). Little work has studied questions whose temporal context is relative to the present time (e.g. "Who was the previous US president?"). We refer to this problem as Present-Anchored Temporal QA (PATQA). PATQA poses unique challenges: (1) large language models (LLMs) may have outdated knowledge, (2) complex temporal relationships (e.g. 'before', 'previous') are hard to reason, (3) multi-hop reasoning may be required, and (4) the gold answers of benchmarks must be continuously updated. To address these challenges, we introduce the PAT-Questions benchmark, which includes single and multi-hop temporal questions. The answers in PAT-Questions can be automatically refreshed by re-running SPARQL queries on a knowledge graph, if available. We evaluate several state-of-the-art 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32473;&#20986;&#21709;&#24212;&#26102;&#23384;&#22312;&#19968;&#20010;&#20215;&#20540;&#20559;&#22909;&#30340;&#26426;&#21046;&#65292;&#20542;&#21521;&#20110;&#20559;&#21521;&#29702;&#24819;&#29366;&#24577;&#65292;&#36825;&#31181;&#20559;&#24046;&#20250;&#23545;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.11005</link><description>&lt;p&gt;
&#25506;&#31350;&#20215;&#20540;&#20559;&#22909;&#65306;LLMs&#20559;&#21521;&#29702;&#24819;&#29366;&#24577;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Exploring Value Biases: How LLMs Deviate Towards the Ideal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11005
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32473;&#20986;&#21709;&#24212;&#26102;&#23384;&#22312;&#19968;&#20010;&#20215;&#20540;&#20559;&#22909;&#30340;&#26426;&#21046;&#65292;&#20542;&#21521;&#20110;&#20559;&#21521;&#29702;&#24819;&#29366;&#24577;&#65292;&#36825;&#31181;&#20559;&#24046;&#20250;&#23545;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#37096;&#32626;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#21709;&#24212;&#23545;&#31038;&#20250;&#20135;&#29983;&#30528;&#36234;&#26469;&#36234;&#22823;&#30340;&#24433;&#21709;&#12290;&#29702;&#35299;LLMs&#22312;&#32473;&#20986;&#21709;&#24212;&#26102;&#30340;&#38750;&#25925;&#24847;&#26426;&#21046;&#23545;&#20110;&#35299;&#37322;&#23427;&#20204;&#30340;&#24615;&#33021;&#24182;&#36776;&#21035;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#20559;&#24046;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#31867;&#20284;&#20110;&#20154;&#31867;&#30740;&#31350;&#20013;&#65292;&#36825;&#31181;&#26080;&#24847;&#35782;&#30340;&#21709;&#24212;&#34987;&#31216;&#20026;&#25277;&#26679;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#30340;&#36825;&#31181;&#25277;&#26679;&#29616;&#35937;&#65292;&#21457;&#29616;LLMs&#30340;&#25277;&#26679;&#20542;&#21521;&#20110;&#20559;&#29233;&#39640;&#20215;&#20540;&#36873;&#39033;&#12290;&#20215;&#20540;&#20559;&#22909;&#23545;&#24212;&#20110;&#20174;&#26368;&#21487;&#33021;&#30340;&#21709;&#24212;&#21521;LLM&#20013;&#20195;&#34920;&#30340;&#29702;&#24819;&#20215;&#20540;&#30340;&#36716;&#21464;&#12290;&#23454;&#38469;&#19978;&#65292;&#21363;&#20415;&#26159;&#36890;&#36807;&#19978;&#19979;&#25991;&#25552;&#31034;&#23398;&#20064;&#21040;&#30340;&#26032;&#23454;&#20307;&#65292;&#36825;&#31181;&#25928;&#26524;&#20063;&#33021;&#22815;&#20877;&#29616;&#12290;&#25105;&#20204;&#34920;&#26126;&#36825;&#31181;&#20559;&#24046;&#34920;&#29616;&#22312;&#24847;&#24819;&#19981;&#21040;&#30340;&#22320;&#26041;&#65292;&#24182;&#23545;&#36873;&#25321;&#20856;&#22411;&#23454;&#20363;&#31561;&#30456;&#20851;&#24212;&#29992;&#22330;&#26223;&#20135;&#29983;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20215;&#20540;&#20559;&#22909;&#22312;&#19981;&#21516;&#20998;&#31867;&#30340;LLMs&#20013;&#37117;&#24456;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11005v1 Announce Type: cross  Abstract: Large-Language-Models (LLMs) are deployed in a wide range of applications, and their response has an increasing social impact. Understanding the non-deliberate(ive) mechanism of LLMs in giving responses is essential in explaining their performance and discerning their biases in real-world applications. This is analogous to human studies, where such inadvertent responses are referred to as sampling. We study this sampling of LLMs in light of value bias and show that the sampling of LLMs tends to favour high-value options. Value bias corresponds to this shift of response from the most likely towards an ideal value represented in the LLM. In fact, this effect can be reproduced even with new entities learnt via in-context prompting. We show that this bias manifests in unexpected places and has implications on relevant application scenarios, like choosing exemplars. The results show that value bias is strong in LLMs across different categor
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;ASGEA&#65292;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#35774;&#35745;&#20102;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;ASGNN&#65292;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#23454;&#39564;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.11000</link><description>&lt;p&gt;
ASGEA&#65306;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#36827;&#34892;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11000
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;ASGEA&#65292;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#35774;&#35745;&#20102;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;ASGNN&#65292;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#26088;&#22312;&#35782;&#21035;&#20195;&#34920;&#30456;&#21516;&#29616;&#23454;&#19990;&#30028;&#23545;&#35937;&#30340;&#19981;&#21516;&#30693;&#35782;&#22270;&#20013;&#30340;&#23454;&#20307;&#12290;&#26368;&#36817;&#22522;&#20110;&#23884;&#20837;&#30340;EA&#26041;&#27861;&#22312;EA&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#38754;&#20020;&#30528;&#35299;&#37322;&#24615;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#23436;&#20840;&#20381;&#36182;&#20110;&#23884;&#20837;&#36317;&#31163;&#65292;&#24182;&#24573;&#35270;&#20102;&#19968;&#23545;&#23545;&#40784;&#23454;&#20307;&#32972;&#21518;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Align-Subgraph&#23454;&#20307;&#23545;&#40784;&#65288;ASGEA&#65289;&#26694;&#26550;&#26469;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;ASGEA&#20351;&#29992;&#38170;&#38142;&#25509;&#20316;&#20026;&#26725;&#26753;&#26469;&#26500;&#24314;Align-Subgraphs&#65292;&#24182;&#27839;&#30528;&#36328;&#30693;&#35782;&#22270;&#30340;&#36335;&#24452;&#20256;&#25773;&#65292;&#36825;&#20351;&#20854;&#21306;&#21035;&#20110;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;ASGNN&#65292;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#25972;&#21512;&#36328;&#30693;&#35782;&#22270;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#33410;&#28857;&#32423;&#22810;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#32467;&#21512;&#22810;&#27169;&#24577;&#22686;&#24378;&#30340;&#38170;&#28857;&#26469;&#22686;&#24378;Align-Subgraph&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11000v1 Announce Type: cross  Abstract: Entity alignment (EA) aims to identify entities across different knowledge graphs that represent the same real-world objects. Recent embedding-based EA methods have achieved state-of-the-art performance in EA yet faced interpretability challenges as they purely rely on the embedding distance and neglect the logic rules behind a pair of aligned entities. In this paper, we propose the Align-Subgraph Entity Alignment (ASGEA) framework to exploit logic rules from Align-Subgraphs. ASGEA uses anchor links as bridges to construct Align-Subgraphs and spreads along the paths across KGs, which distinguishes it from the embedding-based methods. Furthermore, we design an interpretable Path-based Graph Neural Network, ASGNN, to effectively identify and integrate the logic rules across KGs. We also introduce a node-level multi-modal attention mechanism coupled with multi-modal enriched anchors to augment the Align-Subgraph. Our experimental results 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20102;&#23545;&#35821;&#20041;&#30340;&#28176;&#36827;&#29702;&#35299;&#65292;&#36890;&#36807;&#24212;&#29992;&#24515;&#28789;&#21746;&#23398;&#21644;&#35821;&#35328;&#23398;&#20013;&#20851;&#20110;&#21547;&#20041;&#30340;&#26680;&#24515;&#20551;&#35774;&#65292;&#30740;&#31350;&#21457;&#29616;LLMs&#19981;&#20165;&#20165;&#26159;&#29983;&#25104;&#25991;&#26412;&#30340;&#24037;&#20855;&#65292;&#32780;&#26159;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#24050;&#32463;&#29702;&#35299;&#20102;&#23427;&#20204;&#29983;&#25104;&#30340;&#35821;&#35328;&#12290;</title><link>https://arxiv.org/abs/2402.10992</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#65306;&#35821;&#20041;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
"Understanding AI": Semantic Grounding in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10992
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20102;&#23545;&#35821;&#20041;&#30340;&#28176;&#36827;&#29702;&#35299;&#65292;&#36890;&#36807;&#24212;&#29992;&#24515;&#28789;&#21746;&#23398;&#21644;&#35821;&#35328;&#23398;&#20013;&#20851;&#20110;&#21547;&#20041;&#30340;&#26680;&#24515;&#20551;&#35774;&#65292;&#30740;&#31350;&#21457;&#29616;LLMs&#19981;&#20165;&#20165;&#26159;&#29983;&#25104;&#25991;&#26412;&#30340;&#24037;&#20855;&#65292;&#32780;&#26159;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#24050;&#32463;&#29702;&#35299;&#20102;&#23427;&#20204;&#29983;&#25104;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#25105;&#20204;&#30446;&#30585;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#29983;&#25104;&#24335;&#36716;&#21464;&#65292;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23545;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#38382;&#39064;&#65292;LLMs&#26159;&#21542;&#29702;&#35299;&#20854;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#21547;&#20041;&#65311;&#23427;&#20204;&#26159;&#21542;&#20855;&#26377;&#35821;&#20041;&#22522;&#30784;&#65311;&#25105;&#20204;&#22914;&#20309;&#20102;&#35299;&#23427;&#20204;&#26159;&#21542;&#29702;&#35299;&#20197;&#21450;&#29702;&#35299;&#30340;&#26159;&#20160;&#20040;&#65311;&#26412;&#25991;&#25506;&#35752;&#20102;&#23545;&#35821;&#20041;&#22522;&#30784;&#38382;&#39064;&#30340;&#35780;&#20272;&#65292;&#21306;&#20998;&#21644;&#35752;&#35770;&#20102;&#20116;&#31181;&#26041;&#27861;&#12290;&#20854;&#20013;&#26368;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#26159;&#23558;&#24515;&#28789;&#21746;&#23398;&#21644;&#35821;&#35328;&#23398;&#20013;&#20851;&#20110;&#21547;&#20041;&#30340;&#26680;&#24515;&#20551;&#35774;&#24212;&#29992;&#20110;LLMs&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#20041;&#22522;&#30784;&#26159;&#19968;&#20010;&#28176;&#36827;&#30340;&#36807;&#31243;&#65292;&#21253;&#25324;&#21151;&#33021;&#24615;&#12289;&#31038;&#20250;&#24615;&#21644;&#22240;&#26524;&#24615;&#19977;&#20010;&#32500;&#24230;&#30340;&#21306;&#20998;&#12290;LLMs&#22312;&#36825;&#19977;&#20010;&#32500;&#24230;&#19978;&#23637;&#29616;&#20986;&#22522;&#26412;&#35777;&#25454;&#12290;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#35770;&#25454;&#26159;LLMs&#20250;&#24418;&#25104;&#19990;&#30028;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;LLMs&#26082;&#19981;&#26159;&#38543;&#26426;&#30340;&#40550;&#40521;&#65292;&#20063;&#19981;&#26159;&#35821;&#20041;&#20725;&#23608;&#65292;&#32780;&#26159;&#33267;&#23569;&#22312;&#22522;&#26412;&#23618;&#38754;&#19978;&#24050;&#32463;&#29702;&#35299;&#23427;&#20204;&#29983;&#25104;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10992v1 Announce Type: cross  Abstract: Do LLMs understand the meaning of the texts they generate? Do they possess a semantic grounding? And how could we understand whether and what they understand? I start the paper with the observation that we have recently witnessed a generative turn in AI, since generative models, including LLMs, are key for self-supervised learning. To assess the question of semantic grounding, I distinguish and discuss five methodological ways. The most promising way is to apply core assumptions of theories of meaning in philosophy of mind and language to LLMs. Grounding proves to be a gradual affair with a three-dimensional distinction between functional, social and causal grounding. LLMs show basic evidence in all three dimensions. A strong argument is that LLMs develop world models. Hence, LLMs are neither stochastic parrots nor semantic zombies, but already understand the language they generate, at least in an elementary sense.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WilKE&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#32534;&#36753;&#23618;&#26469;&#21305;&#37197;&#19981;&#21516;&#23618;&#32423;&#20013;&#30340;&#30693;&#35782;&#32534;&#36753;&#27169;&#24335;&#31243;&#24230;&#65292;&#22312;&#32456;&#36523;&#32534;&#36753;&#20013;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#24179;&#22343;&#23637;&#29616;&#20102;46.2%&#21644;67.8%&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.10987</link><description>&lt;p&gt;
WilKE&#65306;&#26234;&#24935;&#23618;&#30693;&#35782;&#32534;&#36753;&#22120;&#29992;&#20110;&#32456;&#36523;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
WilKE: Wise-Layer Knowledge Editor for Lifelong Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10987
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WilKE&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#32534;&#36753;&#23618;&#26469;&#21305;&#37197;&#19981;&#21516;&#23618;&#32423;&#20013;&#30340;&#30693;&#35782;&#32534;&#36753;&#27169;&#24335;&#31243;&#24230;&#65292;&#22312;&#32456;&#36523;&#32534;&#36753;&#20013;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#24179;&#22343;&#23637;&#29616;&#20102;46.2%&#21644;67.8%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#32534;&#36753;&#26088;&#22312;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#19981;&#20934;&#30830;&#24615;&#65292;&#32780;&#26080;&#38656;&#20026;&#36807;&#26102;&#25110;&#38169;&#35823;&#30340;&#30693;&#35782;&#36827;&#34892;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#21333;&#27425;&#32534;&#36753;&#65292;&#26410;&#33021;&#28385;&#36275;&#32456;&#36523;&#32534;&#36753;&#30340;&#35201;&#27714;&#12290;&#26412;&#25991;&#20013;&#65292;&#32456;&#36523;&#32534;&#36753;&#19982;&#32456;&#36523;&#30693;&#35782;&#32534;&#36753;&#21516;&#20041;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#30693;&#35782;&#32534;&#36753;&#22312;&#32456;&#36523;&#32534;&#36753;&#20013;&#36935;&#21040;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#20854;&#29305;&#24449;&#20026;&#27602;&#24615;&#31215;&#32047;&#21644;&#27602;&#24615;&#38378;&#29616;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#27169;&#24335;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;WilKE&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;&#23427;&#26681;&#25454;&#19981;&#21516;&#23618;&#32423;&#20013;&#32534;&#36753;&#30693;&#35782;&#30340;&#27169;&#24335;&#21305;&#37197;&#31243;&#24230;&#36873;&#25321;&#32534;&#36753;&#23618;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#32456;&#36523;&#32534;&#36753;&#20013;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#65292;WilKE&#22312;&#32534;&#36753;GPT2-XL&#21644;GPT-J&#26041;&#38754;&#20998;&#21035;&#24179;&#22343;&#25913;&#36827;&#20102;46.2%&#21644;67.8%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10987v1 Announce Type: cross  Abstract: Knowledge editing aims to rectify inaccuracies in large language models (LLMs) without costly retraining for outdated or erroneous knowledge. However, current knowledge editing methods primarily focus on single editing, failing to meet the requirements for lifelong editing. In this paper, lifelong editing is synonymous with lifelong knowledge editing. This study reveals a performance degradation encountered by knowledge editing in lifelong editing, characterized by toxicity buildup and toxicity flash, with the primary cause identified as pattern unmatch. We introduce a knowledge editing approach named WilKE, which selects editing layer based on the pattern matching degree of editing knowledge across different layers. Experimental results demonstrate that, in lifelong editing, WilKE exhibits an average improvement of 46.2\% and 67.8\% on editing GPT2-XL and GPT-J relative to state-of-the-art knowledge editing methods.
&lt;/p&gt;</description></item><item><title>FinTral&#26159;&#19968;&#31867;&#22522;&#20110;Mistral-7b&#27169;&#22411;&#30340;GPT-4&#32423;&#21035;&#22810;&#27169;&#24577;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#26816;&#32034;&#26041;&#27861;&#20248;&#21270;&#65292;&#22312;AI&#39537;&#21160;&#37329;&#34701;&#25216;&#26415;&#20013;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.10986</link><description>&lt;p&gt;
FinTral&#65306;&#19968;&#31867;GPT-4&#32423;&#21035;&#30340;&#22810;&#27169;&#24577;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10986
&lt;/p&gt;
&lt;p&gt;
FinTral&#26159;&#19968;&#31867;&#22522;&#20110;Mistral-7b&#27169;&#22411;&#30340;GPT-4&#32423;&#21035;&#22810;&#27169;&#24577;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#26816;&#32034;&#26041;&#27861;&#20248;&#21270;&#65292;&#22312;AI&#39537;&#21160;&#37329;&#34701;&#25216;&#26415;&#20013;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;FinTral&#65292;&#36825;&#26159;&#19968;&#32452;&#22522;&#20110;Mistral-7b&#27169;&#22411;&#26500;&#24314;&#30340;&#19968;&#27969;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#19987;&#38376;&#20026;&#37329;&#34701;&#20998;&#26512;&#23450;&#21046;&#12290;FinTral&#25972;&#21512;&#20102;&#25991;&#26412;&#12289;&#25968;&#23383;&#12289;&#34920;&#26684;&#21644;&#22270;&#20687;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20026;&#26412;&#30740;&#31350;&#31574;&#21010;&#30340;&#22823;&#37327;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#12289;&#25351;&#23548;&#24494;&#35843;&#21644;RLAIF&#35757;&#32451;&#22686;&#24378;&#20102;FinTral&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20061;&#20010;&#20219;&#21153;&#21644;25&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#30340;&#24191;&#27867;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#25324;&#37329;&#34701;&#39046;&#22495;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#30340;FinTral&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#20808;&#36827;&#30340;&#24037;&#20855;&#21644;&#26816;&#32034;&#26041;&#27861;&#36827;&#34892;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#35757;&#32451;&#65292;&#21629;&#21517;&#20026;FinTral-DPO-T&amp;R&#65292;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#38646;-shot&#24615;&#33021;&#12290;&#23427;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;ChatGPT-3.5&#65292;&#24182;&#22312;&#20061;&#39033;&#20219;&#21153;&#20013;&#30340;&#20116;&#39033;&#20013;&#36229;&#36234;GPT-4&#65292;&#26631;&#24535;&#30528;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#37329;&#34701;&#25216;&#26415;&#30340;&#37325;&#35201;&#36827;&#27493;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;FinTral&#20855;&#26377;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10986v1 Announce Type: cross  Abstract: We introduce FinTral, a suite of state-of-the-art multimodal large language models (LLMs) built upon the Mistral-7b model and tailored for financial analysis. FinTral integrates textual, numerical, tabular, and image data. We enhance FinTral with domain-specific pretraining, instruction fine-tuning, and RLAIF training by exploiting a large collection of textual and visual datasets we curate for this work. We also introduce an extensive benchmark featuring nine tasks and 25 datasets for evaluation, including hallucinations in the financial domain. Our FinTral model trained with direct preference optimization employing advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&amp;R, demonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5 in all tasks and surpasses GPT-4 in five out of nine tasks, marking a significant advancement in AI-driven financial technology. We also demonstrate that FinTral has the potential to e
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#22260;&#32469;&#20307;&#32946;&#25968;&#25454;&#20998;&#26512;&#23637;&#24320;&#30340;&#22235;&#39033;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#20540;&#25512;&#29702;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10979</link><description>&lt;p&gt;
SportsMetrics:&#23558;&#25991;&#26412;&#21644;&#25968;&#20540;&#25968;&#25454;&#34701;&#21512;&#20197;&#29702;&#35299;LLM&#20013;&#30340;&#20449;&#24687;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10979
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#22260;&#32469;&#20307;&#32946;&#25968;&#25454;&#20998;&#26512;&#23637;&#24320;&#30340;&#22235;&#39033;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#20540;&#25512;&#29702;&#21644;&#20449;&#24687;&#34701;&#21512;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10979v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25972;&#21512;&#25991;&#26412;&#25991;&#26723;&#21644;&#25968;&#25454;&#24211;&#35760;&#24405;&#31561;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#36827;&#34892;&#20808;&#36827;&#20998;&#26512;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#34701;&#21512;&#25991;&#26412;&#21644;&#25968;&#20540;&#25968;&#25454;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;LLMs&#38656;&#35201;&#22788;&#29702;&#21644;&#20132;&#21449;&#24341;&#29992;&#23454;&#20307;&#21644;&#25968;&#23383;&#65292;&#22788;&#29702;&#25968;&#25454;&#19981;&#19968;&#33268;&#24615;&#21644;&#20887;&#20313;&#65292;&#24182;&#21457;&#23637;&#35268;&#21010;&#33021;&#21147;&#65292;&#27604;&#22914;&#26500;&#24314;&#29992;&#20110;&#31649;&#29702;&#22797;&#26434;&#25968;&#25454;&#26597;&#35810;&#30340;&#24037;&#20316;&#20869;&#23384;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22260;&#32469;&#20307;&#32946;&#25968;&#25454;&#20998;&#26512;&#30340;&#22235;&#39033;&#26032;&#39062;&#20219;&#21153;&#65292;&#20197;&#35780;&#20272;LLMs&#30340;&#25968;&#20540;&#25512;&#29702;&#21644;&#20449;&#24687;&#34701;&#21512;&#33021;&#21147;&#12290;&#36825;&#20123;&#20219;&#21153;&#28041;&#21450;&#21521;LLMs&#25552;&#20379;&#35814;&#32454;&#30340;&#36880;&#22330;&#27604;&#36187;&#25551;&#36848;&#65292;&#28982;&#21518;&#22312;&#38754;&#23545;&#35832;&#22914;&#26032;&#27604;&#36187;&#35268;&#21017;&#12289;&#26356;&#38271;&#25345;&#32493;&#26102;&#38388;&#12289;&#25925;&#20107;&#28151;&#20081;&#20197;&#21450;&#20998;&#26512;&#27604;&#36187;&#25688;&#35201;&#20013;&#30340;&#20851;&#38190;&#32479;&#35745;&#25968;&#25454;&#31561;&#23545;&#25239;&#24615;&#22330;&#26223;&#12290;&#25105;&#20204;&#23545;NBA&#21644;NFL&#27604;&#36187;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;LLMs&#22312;&#35813;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10979v1 Announce Type: cross  Abstract: Large language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#36830;&#25509;&#35821;&#35328;&#24314;&#27169;&#21644;&#31526;&#21512;&#39044;&#27979;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#39640;&#27010;&#29575;&#27491;&#30830;&#24615;&#20445;&#35777;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.10978</link><description>&lt;p&gt;
&#20855;&#26377;&#31526;&#21512;&#20107;&#23454;&#24615;&#20445;&#35777;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Language Models with Conformal Factuality Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10978
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#36830;&#25509;&#35821;&#35328;&#24314;&#27169;&#21644;&#31526;&#21512;&#39044;&#27979;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#39640;&#27010;&#29575;&#27491;&#30830;&#24615;&#20445;&#35777;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#21644;&#20107;&#23454;&#24615;&#20445;&#35777;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31526;&#21512;&#20107;&#23454;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#36830;&#25509;&#35821;&#35328;&#24314;&#27169;&#21644;&#31526;&#21512;&#39044;&#27979;&#65292;&#30830;&#20445;LM&#30340;&#39640;&#27010;&#29575;&#27491;&#30830;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;LM&#36755;&#20986;&#30340;&#27491;&#30830;&#24615;&#31561;&#20215;&#20110;&#19968;&#20010;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#19981;&#30830;&#23450;&#24615;&#38598;&#34987;&#23450;&#20041;&#20026;LM&#36755;&#20986;&#30340;&#34164;&#21547;&#38598;&#12290;&#21033;&#29992;&#36825;&#31181;&#32852;&#31995;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31526;&#21512;&#39044;&#27979;&#23545;&#24212;&#20110;&#19968;&#31181;&#21518;&#36864;&#31639;&#27861;&#65292;&#36890;&#36807;&#36880;&#28176;&#20351;LM&#36755;&#20986;&#21464;&#24471;&#19981;&#22826;&#20855;&#20307;&#65288;&#24182;&#25193;&#22823;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65289;&#25552;&#20379;&#39640;&#27010;&#29575;&#30340;&#27491;&#30830;&#24615;&#20445;&#35777;&#12290;&#36825;&#31181;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#40657;&#30418;LM&#65292;&#24182;&#19988;&#38656;&#35201;&#24456;&#23569;&#30340;&#20154;&#24037;&#27880;&#37322;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#23553;&#38381;&#20070;&#31821;QA&#65288;FActScore&#65292;NaturalQuestions&#65289;&#21644;&#25512;&#29702;&#20219;&#21153;&#65288;MATH&#65289;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;p
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10978v1 Announce Type: cross  Abstract: Guaranteeing the correctness and factuality of language model (LM) outputs is a major open problem. In this work, we propose conformal factuality, a framework that can ensure high probability correctness guarantees for LMs by connecting language modeling and conformal prediction. We observe that the correctness of an LM output is equivalent to an uncertainty quantification problem, where the uncertainty sets are defined as the entailment set of an LM's output. Using this connection, we show that conformal prediction in language models corresponds to a back-off algorithm that provides high probability correctness guarantees by progressively making LM outputs less specific (and expanding the associated uncertainty sets). This approach applies to any black-box LM and requires very few human-annotated samples. Evaluations of our approach on closed book QA (FActScore, NaturalQuestions) and reasoning tasks (MATH) show that our approach can p
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#20581;&#24247;&#39046;&#22495;&#26377;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#27867;&#21270;&#25928;&#26524;&#21462;&#20915;&#20110;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#21644;&#20154;&#32676;&#20013;&#30340;&#34920;&#29616;&#65292;&#23545;&#20110;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#26679;&#26412;&#36739;&#23569;&#30340;&#21307;&#38498;&#21644;&#29305;&#23450;&#20154;&#32676;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10965</link><description>&lt;p&gt;
&#21307;&#30103;AI&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#65306;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Generalization in Healthcare AI: Evaluation of a Clinical Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10965
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#20581;&#24247;&#39046;&#22495;&#26377;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#28982;&#32780;&#23427;&#20204;&#30340;&#27867;&#21270;&#25928;&#26524;&#21462;&#20915;&#20110;&#22312;&#19981;&#21516;&#20020;&#24202;&#29615;&#22659;&#21644;&#20154;&#32676;&#20013;&#30340;&#34920;&#29616;&#65292;&#23545;&#20110;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#21407;&#22240;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#26679;&#26412;&#36739;&#23569;&#30340;&#21307;&#38498;&#21644;&#29305;&#23450;&#20154;&#32676;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20026;&#21307;&#30103;&#20581;&#24247;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#26426;&#36935;&#65292;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#12289;&#20020;&#24202;&#20915;&#31574;&#20197;&#21450;&#25552;&#21319;&#21307;&#24072;&#21644;&#31649;&#29702;&#20154;&#21592;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#28508;&#21147;&#37325;&#35201;&#21462;&#20915;&#20110;&#23427;&#20204;&#22312;&#20020;&#24202;&#29615;&#22659;&#21644;&#20154;&#32676;&#20013;&#26377;&#25928;&#27867;&#21270;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#22312;&#26089;&#26399;&#24320;&#21457;&#20013;&#32463;&#24120;&#34987;&#20302;&#20272;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#25361;&#25112;&#30340;&#21407;&#22240;&#24182;&#21046;&#23450;&#32531;&#35299;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ClinicLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#22312; [HOSPITAL] &#30340;&#20020;&#24202;&#31508;&#35760;&#19978;&#35757;&#32451;&#30340;LLM&#27169;&#22411;&#65292;&#23545;&#20854;&#22312;30&#22825;&#20840;&#22240;&#32032;&#20877;&#20837;&#38498;&#39044;&#27979;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20998;&#26512;&#65292;&#20851;&#27880;&#36328;&#21307;&#38498;&#21644;&#24739;&#32773;&#29305;&#24449;&#30340;&#21464;&#24322;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#26679;&#26412;&#36739;&#23569;&#30340;&#21307;&#38498;&#12289;&#25919;&#24220;&#21644;&#26410;&#25351;&#23450;&#20445;&#38505;&#30340;&#24739;&#32773;&#12289;&#32769;&#24180;&#20154;&#20197;&#21450;&#39640;&#20849;&#30149;&#24615;&#24739;&#32773;&#20013;&#65292;&#27867;&#21270;&#25928;&#26524;&#36739;&#24046;&#12290;&#20026;&#20102;&#20102;&#35299;&#27867;&#21270;&#19981;&#24432;&#30340;&#21407;&#22240;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26679;&#26412;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10965v1 Announce Type: new  Abstract: Advances in large language models (LLMs) provide new opportunities in healthcare for improved patient care, clinical decision-making, and enhancement of physician and administrator workflows. However, the potential of these models importantly depends on their ability to generalize effectively across clinical environments and populations, a challenge often underestimated in early development. To better understand reasons for these challenges and inform mitigation approaches, we evaluated ClinicLLM, an LLM trained on [HOSPITAL]'s clinical notes, analyzing its performance on 30-day all-cause readmission prediction focusing on variability across hospitals and patient characteristics. We found poorer generalization particularly in hospitals with fewer samples, among patients with government and unspecified insurance, the elderly, and those with high comorbidities. To understand reasons for lack of generalization, we investigated sample sizes 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Stepwise ORMs (SORMs)&#65292;&#23427;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#20197;&#36817;&#20284;&#39044;&#27979;&#26368;&#20248;&#31574;&#30053;&#30340;&#26410;&#26469;&#39044;&#26399;&#22870;&#21169;</title><link>https://arxiv.org/abs/2402.10963</link><description>&lt;p&gt;
GLoRe: &#20309;&#26102;&#12289;&#20309;&#22320;&#20197;&#21450;&#22914;&#20309;&#36890;&#36807;&#20840;&#23616;&#21644;&#23616;&#37096;&#30340;&#25913;&#36827;&#26469;&#25552;&#39640;LLM&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
GLoRe: When, Where, and How to Improve LLM Reasoning via Global and Local Refinements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10963
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Stepwise ORMs (SORMs)&#65292;&#23427;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#20197;&#36817;&#20284;&#39044;&#27979;&#26368;&#20248;&#31574;&#30053;&#30340;&#26410;&#26469;&#39044;&#26399;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#12289;&#31185;&#23398;&#25110;&#32534;&#30721;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#25913;&#36827;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26368;&#22909;&#30340;&#27169;&#22411;&#20063;&#24456;&#38590;&#22312;&#27809;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#20309;&#26102;&#20309;&#22320;&#36827;&#34892;&#25913;&#36827;&#12290;&#22522;&#20110;&#32467;&#26524;&#30340;&#22870;&#21169;&#27169;&#22411;(ORMs)&#65292;&#34987;&#35757;&#32451;&#26469;&#39044;&#27979;&#26368;&#32456;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#65292;&#25351;&#31034;&#20309;&#26102;&#36827;&#34892;&#25913;&#36827;&#65292;&#20026;&#20915;&#23450;&#20309;&#26102;&#36827;&#34892;&#25913;&#36827;&#25552;&#20379;&#20102;&#19968;&#31181;&#20415;&#21033;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#20110;&#36807;&#31243;&#30340;&#22870;&#21169;&#27169;&#22411;(PRMs)&#21463;&#36807;&#35757;&#32451;&#65292;&#29992;&#20197;&#39044;&#27979;&#20013;&#38388;&#27493;&#39588;&#30340;&#27491;&#30830;&#24615;&#65292;&#28982;&#21518;&#21487;&#20197;&#29992;&#26469;&#25351;&#31034;&#20309;&#22788;&#36827;&#34892;&#25913;&#36827;&#12290;&#20294;&#23427;&#20204;&#24456;&#26114;&#36149;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36880;&#27493;ORMs(SORMs)&#65292;&#23427;&#20204;&#21482;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#21463;&#36807;&#35757;&#32451;&#65292;&#20197;&#36817;&#20284;&#39044;&#27979;&#26368;&#20248;&#31574;&#30053;&#25110;$V^{\star}$&#30340;&#26410;&#26469;&#39044;&#26399;&#22870;&#21169;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;SORMs&#21463;&#35757;&#32451;&#26469;&#39044;&#27979;&#24403;&#21462;&#26679;&#26102;&#26368;&#32456;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10963v1 Announce Type: new  Abstract: State-of-the-art language models can exhibit impressive reasoning refinement capabilities on math, science or coding tasks. However, recent work demonstrates that even the best models struggle to identify \textit{when and where to refine} without access to external feedback. Outcome-based Reward Models (\textbf{ORMs}), trained to predict correctness of the final answer indicating when to refine, offer one convenient solution for deciding when to refine. Process Based Reward Models (\textbf{PRMs}), trained to predict correctness of intermediate steps, can then be used to indicate where to refine. But they are expensive to train, requiring extensive human annotations. In this paper, we propose Stepwise ORMs (\textbf{SORMs}) which are trained, only on synthetic data, to approximate the expected future reward of the optimal policy or $V^{\star}$. More specifically, SORMs are trained to predict the correctness of the final answer when samplin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;</title><link>https://arxiv.org/abs/2402.10962</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#27979;&#37327;&#21644;&#25511;&#21046;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Measuring and Controlling Persona Drift in Language Model Dialogs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10962
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#20013;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#26469;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#26159;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26631;&#20934;&#24037;&#20855;&#65292;&#20351;&#20854;&#33021;&#22815;&#25215;&#25285;&#29305;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#12290;&#22312;&#20351;&#29992;&#25552;&#31034;&#26102;&#30340;&#19968;&#20010;&#38544;&#21547;&#20551;&#35774;&#26159;&#65292;&#23427;&#20204;&#23558;&#26159;&#31283;&#23450;&#30340;&#65292;&#22240;&#27492;&#32842;&#22825;&#26426;&#22120;&#20154;&#23558;&#22312;&#25972;&#20010;&#23545;&#35805;&#36807;&#31243;&#20013;&#32487;&#32493;&#26681;&#25454;&#35268;&#23450;&#30340;&#8220;&#20154;&#35774;&#8221;&#29983;&#25104;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#21270;&#22522;&#20934;&#26469;&#27979;&#35797;&#36825;&#19968;&#20551;&#35774;&#65292;&#36890;&#36807;&#20004;&#20010;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#33258;&#25105;&#23545;&#35805;&#26469;&#35780;&#20272;&#8220;&#20154;&#35774;&#8221;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#27169;&#22411;&#22914;LLaMA2-chat-70B&#36827;&#34892;&#27979;&#35797;&#65292;&#21457;&#29616;&#22312;&#20843;&#36718;&#23545;&#35805;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#12290;&#23545;&#36825;&#19968;&#29616;&#35937;&#30340;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#30001;&#20110;&#38271;&#23545;&#35805;&#20013;&#30340;&#27880;&#24847;&#21147;&#34928;&#20943;&#65292;&#21464;&#21387;&#22120;&#27880;&#24847;&#21147;&#26426;&#21046;&#36215;&#21040;&#20102;&#19968;&#23450;&#20316;&#29992;&#12290;&#20026;&#20102;&#23545;&#25239;&#27880;&#24847;&#21147;&#34928;&#20943;&#21644;&#8220;&#20154;&#35774;&#8221;&#28418;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;split-softmax&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#19982;&#20004;&#20010;&#24378;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10962v1 Announce Type: cross  Abstract: Prompting is a standard tool for customizing language-model chatbots, enabling them to take on a specific "persona". An implicit assumption in the use of prompts is that they will be stable, so the chatbot will continue to generate text according to the stipulated persona for the duration of a conversation. We propose a quantitative benchmark to test this assumption, evaluating persona stability via self-chats between two personalized chatbots. Testing popular models like LLaMA2-chat-70B, we reveal a significant persona drift within eight rounds of conversations. An empirical and theoretical analysis of this phenomenon suggests the transformer attention mechanism plays a role, due to attention decay over long exchanges. To combat attention decay and persona drift, we propose a lightweight method called split-softmax, which compares favorably against two strong baselines.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;RPO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#26469;&#33258;&#30456;&#21516;&#21644;&#30456;&#20851;&#25552;&#31034;&#30340;&#26356;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#21644;&#26356;&#19981;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#65292;&#25193;&#23637;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10958</link><description>&lt;p&gt;
&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;: &#36890;&#36807;&#23545;&#30456;&#21516;&#21644;&#19981;&#21516;&#25552;&#31034;&#30340;&#23545;&#27604;&#21709;&#24212;&#22686;&#24378;LLM&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10958
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;RPO&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#21306;&#20998;&#26469;&#33258;&#30456;&#21516;&#21644;&#30456;&#20851;&#25552;&#31034;&#30340;&#26356;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#21644;&#26356;&#19981;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#65292;&#25193;&#23637;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39046;&#22495;&#65292;&#23558;&#27169;&#22411;&#19982;&#29992;&#25143;&#30340;&#22810;&#26679;&#21270;&#20559;&#22909;&#30456;&#19968;&#33268;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#30452;&#25509;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;DPO&#65289;&#22312;&#36825;&#19968;&#39046;&#22495;&#36215;&#21040;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;DPO&#36890;&#36807;&#20351;&#29992;&#20174;&#30456;&#21516;&#25552;&#31034;&#20013;&#27966;&#29983;&#30340;&#20559;&#22909;&#23545;&#26469;&#24037;&#20316;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;DPO&#24182;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;&#20154;&#31867;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#31181;&#23398;&#20064;&#24448;&#24448;&#28041;&#21450;&#23545;&#19981;&#20165;&#30456;&#21516;&#32780;&#19988;&#30456;&#20284;&#38382;&#39064;&#30340;&#23545;&#27604;&#21709;&#24212;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#23545;&#20248;&#20808;&#26435;&#20248;&#21270;&#65288;RPO&#65289;&#12290;RPO&#26088;&#22312;&#21306;&#20998;&#26469;&#33258;&#30456;&#21516;&#21644;&#30456;&#20851;&#25552;&#31034;&#30340;&#26356;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#21644;&#26356;&#19981;&#21463;&#38738;&#30544;&#30340;&#21709;&#24212;&#12290;&#23427;&#24341;&#20837;&#20102;&#23545;&#27604;&#21152;&#26435;&#26426;&#21046;&#65292;&#20351;LLMs&#33021;&#22815;&#20351;&#29992;&#26356;&#24191;&#27867;&#30340;&#20559;&#22909;&#25968;&#25454;&#36827;&#34892;&#35843;&#25972;&#65292;&#21253;&#25324;&#25104;&#23545;&#21644;&#19981;&#25104;&#23545;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#21033;&#29992;&#26356;&#22810;&#30340;&#20559;&#22909;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10958v1 Announce Type: cross  Abstract: In the field of large language models (LLMs), aligning models with the diverse preferences of users is a critical challenge. Direct Preference Optimization (DPO) has played a key role in this area. It works by using pairs of preferences derived from the same prompts, and it functions without needing an additional reward model. However, DPO does not fully reflect the complex nature of human learning, which often involves understanding contrasting responses to not only identical but also similar questions. To overcome this shortfall, we propose Relative Preference Optimization (RPO). RPO is designed to discern between more and less preferred responses derived from both identical and related prompts. It introduces a contrastive weighting mechanism, enabling the tuning of LLMs using a broader range of preference data, including both paired and unpaired sets. This approach expands the learning capabilities of the model, allowing it to lever
&lt;/p&gt;</description></item><item><title>DAEDRA&#26159;&#19968;&#31181;&#26088;&#22312;&#22312;&#34987;&#21160;&#25253;&#21578;&#20013;&#26816;&#27979;&#30417;&#31649;&#30456;&#20851;&#32467;&#26524;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24357;&#34917;&#20102;&#36890;&#29992;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20020;&#24202;&#32500;&#24230;&#19982;&#19987;&#19994;&#27169;&#22411;&#22312;&#38750;&#19987;&#19994;&#25253;&#21578;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#32570;&#38519;</title><link>https://arxiv.org/abs/2402.10951</link><description>&lt;p&gt;
DAEDRA&#65306;&#29992;&#20110;&#39044;&#27979;&#34987;&#21160;&#33647;&#29289;&#35686;&#25106;&#25253;&#21578;&#32467;&#26524;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DAEDRA: A language model for predicting outcomes in passive pharmacovigilance reporting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10951
&lt;/p&gt;
&lt;p&gt;
DAEDRA&#26159;&#19968;&#31181;&#26088;&#22312;&#22312;&#34987;&#21160;&#25253;&#21578;&#20013;&#26816;&#27979;&#30417;&#31649;&#30456;&#20851;&#32467;&#26524;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24357;&#34917;&#20102;&#36890;&#29992;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20020;&#24202;&#32500;&#24230;&#19982;&#19987;&#19994;&#27169;&#22411;&#22312;&#38750;&#19987;&#19994;&#25253;&#21578;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#23548;&#33268;&#20102;&#29305;&#23450;&#39046;&#22495;&#27169;&#22411;&#30340;&#28608;&#22686;&#65292;&#36825;&#20123;&#27169;&#22411;&#26088;&#22312;&#21453;&#26144;&#26469;&#28304;&#39046;&#22495;&#30340;&#35821;&#35328;&#29615;&#22659;&#21644;&#20869;&#23481;&#30340;&#29305;&#27530;&#24615;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;DAEDRA&#30340;&#26500;&#24605;&#12289;&#35774;&#35745;&#12289;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#26816;&#27979;&#34987;&#21160;&#25253;&#21578;&#65288;PR&#65289;&#20013;&#30340;&#30417;&#31649;&#30456;&#20851;&#32467;&#26524;&#65288;&#27515;&#20129;&#12289;&#24613;&#35786;&#23601;&#35786;&#21644;&#20303;&#38498;&#65289;&#30340;LLM&#12290;&#34429;&#28982;PR&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#20174;&#24191;&#27867;&#21644;&#22810;&#26679;&#21270;&#21463;&#20247;&#65288;&#36890;&#24120;&#19981;&#20165;&#21253;&#25324;&#21307;&#29983;&#21644;&#21307;&#25252;&#20154;&#21592;&#65292;&#36824;&#21253;&#25324;&#24739;&#32773;&#12289;&#23478;&#24237;&#25104;&#21592;&#21644;&#20854;&#20182;&#38750;&#19987;&#19994;&#21033;&#30410;&#30456;&#20851;&#32773;&#65289;&#37027;&#37324;&#33719;&#21462;&#20449;&#24687;&#30340;&#26041;&#24335;&#65292;&#20294;&#26159;&#36825;&#31181;&#22810;&#26679;&#24615;&#20351;PR&#35821;&#26009;&#24211;&#38590;&#20197;&#20998;&#26512;&#12290;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#25429;&#25417;&#22797;&#26434;&#30340;&#20020;&#24202;&#32500;&#24230;&#65292;&#32780;&#29305;&#23450;&#30340;&#20020;&#24202;&#25110;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#21487;&#33021;&#22312;&#38750;&#19987;&#19994;&#25253;&#21578;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10951v1 Announce Type: new  Abstract: Over the recent years, the emergence of large language models (LLMs) has given rise to a proliferation of domain-specific models that are intended to reflect the particularities of linguistic context and content as a correlate of the originating domain. This paper details the conception, design, training and evaluation of DAEDRA, a LLM designed to detect regulatory-relevant outcomes (mortality, ER attendance and hospitalisation) in adverse event reports elicited through passive reporting (PR). While PR is a highly cost-efficient way of eliciting information from a wide and diverse audience -- typically including not only physicians and healthcare providers but also patients, family members and other lay stakeholders --, this diversity makes PR corpora difficult to analyse. Generic language models may not capture the complex clinical dimensions while specific clinical or biomedical models may not perform well on lay reports. To evaluate t
&lt;/p&gt;</description></item><item><title>&#24322;&#31867;&#33258;&#21160;&#25552;&#31034;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#25552;&#31034;&#26102;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21253;&#25324;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#25552;&#31034;&#20250;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.10949</link><description>&lt;p&gt;
&#24322;&#31867;&#33258;&#21160;&#25552;&#31034;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Unreasonable Effectiveness of Eccentric Automatic Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10949
&lt;/p&gt;
&lt;p&gt;
&#24322;&#31867;&#33258;&#21160;&#25552;&#31034;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#25552;&#31034;&#26102;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21253;&#25324;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#25552;&#31034;&#20250;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#38382;&#39064;&#35299;&#20915;&#21644;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21151;&#25928;&#39640;&#24230;&#20381;&#36182;&#20110;&#25552;&#31034;&#30340;&#21046;&#23450;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#37327;&#21270;&#23558;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#32435;&#20837;&#31995;&#32479;&#25552;&#31034;&#28040;&#24687;&#30340;&#24433;&#21709;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#31995;&#32479;&#21270;&#25552;&#31034;&#20248;&#21270;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;60&#31181;&#31995;&#32479;&#28040;&#24687;&#29255;&#27573;&#30340;&#24615;&#33021;&#65292;&#20998;&#21035;&#20351;&#29992;&#21644;&#19981;&#20351;&#29992;Chain of Thought&#25552;&#31034;&#65292;&#36328;&#19977;&#20010;&#21442;&#25968;&#33539;&#22260;&#20174;70&#20159;&#21040;70&#20159;&#20010;&#21464;&#37327;&#30340;&#27169;&#22411;&#65292;&#22312;GSM8K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#32467;&#26524;&#24182;&#19981;&#22312;&#25152;&#26377;&#27169;&#22411;&#20013;&#26222;&#36941;&#36866;&#29992;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#21253;&#25324;&#8220;&#31215;&#26497;&#24605;&#32771;&#8221;&#25552;&#31034;&#20250;&#31215;&#26497;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Llama2-70B&#22312;&#19981;&#20351;&#29992;Chain of Thought&#26102;&#26159;&#20010;&#20363;&#22806;&#65292;&#22240;&#20026;&#21457;&#29616;&#26368;&#20339;&#31995;&#32479;&#28040;&#24687;&#23454;&#38469;&#19978;&#26159;&#27809;&#26377;&#28040;&#24687;&#12290;&#32771;&#34385;&#21040;&#32452;&#21512;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#20854;&#23548;&#33267;&#30340;&#21152;# Truncated due to exceeding character limit.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10949v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable problem-solving and basic mathematics abilities. However, their efficacy is highly contingent on the formulation of the prompt. This study endeavors to quantify the influence of incorporating "positive thinking" into the system message of the prompt, then compare that to systematic prompt optimization. We assess the performance of 60 combinations of system message snippets, tested with and without Chain of Thought prompting, across three models with parameters ranging from 7 to 70 billion on the GSM8K dataset. Our findings reveal that results do not universally generalize across models. In most instances, the inclusion of "positive thinking" prompts positively affected model performance. Notably, however, Llama2-70B exhibited an exception when not utilizing Chain of Thought, as the optimal system message was found to be none at all. Given the combinatorial complexity, and thus com
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#32467;&#21512;&#24515;&#29702;&#37327;&#34920;&#36890;&#36807;LLMs&#36827;&#34892;&#38646;-shot&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.10948</link><description>&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#32467;&#21512;&#24515;&#29702;&#37327;&#34920;&#36827;&#34892;&#38646;-shot&#21487;&#35299;&#37322;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Explainable Mental Health Analysis on Social Media by incorporating Mental Scales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10948
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#32467;&#21512;&#24515;&#29702;&#37327;&#34920;&#36890;&#36807;LLMs&#36827;&#34892;&#38646;-shot&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#26041;&#27861;&#22312;&#23481;&#37327;&#26041;&#38754;&#34920;&#29616;&#24378;&#22823;&#65292;&#20294;&#32570;&#20047;&#35299;&#37322;&#33021;&#21147;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#35268;&#27169;&#27880;&#37322;&#30340;&#25968;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;&#26377;&#28508;&#21147;&#25670;&#33073;&#32321;&#37325;&#30340;&#27880;&#37322;&#24182;&#25552;&#20379;&#35299;&#37322;&#12290;&#21463;&#21040;&#20351;&#29992;&#37327;&#34920;&#35780;&#20272;&#24515;&#29702;&#29366;&#24577;&#30340;&#24515;&#29702;&#35780;&#20272;&#23454;&#36341;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;LLMs&#32467;&#21512;&#20102;&#20004;&#20010;&#31243;&#24207;&#12290;&#39318;&#20808;&#65292;&#24739;&#32773;&#23436;&#25104;&#24515;&#29702;&#20581;&#24247;&#38382;&#21367;&#65292;&#20854;&#27425;&#65292;&#24515;&#29702;&#23398;&#23478;&#35299;&#37322;&#26469;&#33258;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#25910;&#38598;&#20449;&#24687;&#24182;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20854;&#20182;&#38646;-shot&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10948v1 Announce Type: cross  Abstract: Traditional discriminative approaches in mental health analysis are known for their strong capacity but lack interpretability and demand large-scale annotated data. On the other hand, generative approaches, such as those based on large language models (LLMs),have the potential to get rid of heavy annotations and provide explanations. However, their capabilities still fall short compared to discriminative approaches, and their explanations may be unreliable due to the fact that the generation of explanation is a black-box process. Inspired by the psychological assessment practice of using scales to evaluate mental states, our method incorporates two procedures via LLMs. First, the patient completes mental health questionnaires, and second, the psychologist interprets the collected information from the mental health questions and makes informed decisions. Experimental results show that our method outperforms other zero-shot methods. Our 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CultureLLM&#30340;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26469;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#24494;&#35843;&#24471;&#21040;&#20102;&#28085;&#30422;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;9&#31181;&#25991;&#21270;&#29305;&#23450;LLMs&#20197;&#21450;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.10946</link><description>&lt;p&gt;
&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
CultureLLM: Incorporating Cultural Differences into Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10946
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CultureLLM&#30340;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26469;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#24494;&#35843;&#24471;&#21040;&#20102;&#28085;&#30422;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;9&#31181;&#25991;&#21270;&#29305;&#23450;LLMs&#20197;&#21450;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#25253;&#36947;&#20559;&#21521;&#20110;&#26576;&#20123;&#25991;&#21270;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#20027;&#35201;&#26469;&#33258;&#33521;&#35821;&#35821;&#26009;&#24211;&#12290;&#30001;&#20110;&#22810;&#35821;&#31181;&#25991;&#21270;&#25968;&#25454;&#36890;&#24120;&#36739;&#38590;&#25910;&#38598;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25110;&#29305;&#23450;&#25991;&#21270;&#30340;&#39044;&#35757;&#32451;&#26469;&#22788;&#29702;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#24573;&#35270;&#20102;&#20302;&#36164;&#28304;&#25991;&#21270;&#30340;&#30693;&#35782;&#32570;&#20047;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CultureLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;LLMs&#20013;&#12290;CultureLLM&#37319;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#29983;&#25104;&#35821;&#20041;&#31561;&#25928;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20165;&#20351;&#29992;&#26469;&#33258;WVS&#30340;50&#20010;&#31181;&#23376;&#26679;&#26412;&#21644;&#22686;&#24378;&#25968;&#25454;&#65292;&#25105;&#20204;&#23545;9&#31181;&#21253;&#25324;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#21270;&#29305;&#23450;LLMs&#21644;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#23545;60&#20010;&#19982;&#25991;&#21270;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;CultureLLM&#22312;&#22686;&#24378;LLM&#30340;&#25991;&#21270;&#29305;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10946v1 Announce Type: cross  Abstract: Large language models (LLMs) are reported to be partial to certain cultures owing to the training data dominance from the English corpora. Since multilingual cultural data are often expensive to collect, existing efforts handle this by prompt engineering or culture-specific pre-training. However, they might overlook the knowledge deficiency of low-resource culture and require extensive computing resources. In this paper, we propose CultureLLM, a cost-effective solution to incorporate cultural differences into LLMs. CultureLLM adopts World Value Survey (WVS) as seed data and generates semantically equivalent training data via the proposed semantic data augmentation. Using only 50 seed samples from WVS with augmented data, we fine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9 cultures covering rich and low-resource languages. Extensive experiments on 60 culture-related datasets demonstrate that CultureLLM signif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#28304;OCR&#24341;&#25806;Kraken&#22312;&#38463;&#25289;&#20271;&#23398;&#26415;&#26399;&#21002;al-Abhath&#19978;&#30340;&#20934;&#30830;&#24615;&#65292;&#34920;&#26126;&#20854;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#38463;&#25289;&#20271;&#25991;&#26412;OCR&#12290;&#25991;&#31456;&#24314;&#35758;&#36890;&#36807;&#26356;&#31995;&#32479;&#30340;&#35757;&#32451;&#25968;&#25454;&#29983;&#20135;&#21644;&#20851;&#38190;&#25216;&#26415;&#32452;&#20214;&#30340;&#24320;&#21457;&#26469;&#26174;&#30528;&#25552;&#39640;&#38463;&#25289;&#20271;&#25991;&#23383;OCR&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10943</link><description>&lt;p&gt;
&#24320;&#28304;&#38463;&#25289;&#20271;&#35821;&#25991;&#23383;&#35782;&#21035;&#25216;&#26415;&#30340;&#36827;&#23637;&#19982;&#23616;&#38480;&#24615;&#65306;&#20197;&#38463;&#23572;-&#38463;&#24052;&#26031;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Advances and Limitations in Open Source Arabic-Script OCR: A Case Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24320;&#28304;OCR&#24341;&#25806;Kraken&#22312;&#38463;&#25289;&#20271;&#23398;&#26415;&#26399;&#21002;al-Abhath&#19978;&#30340;&#20934;&#30830;&#24615;&#65292;&#34920;&#26126;&#20854;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#20934;&#30830;&#30340;&#38463;&#25289;&#20271;&#25991;&#26412;OCR&#12290;&#25991;&#31456;&#24314;&#35758;&#36890;&#36807;&#26356;&#31995;&#32479;&#30340;&#35757;&#32451;&#25968;&#25454;&#29983;&#20135;&#21644;&#20851;&#38190;&#25216;&#26415;&#32452;&#20214;&#30340;&#24320;&#21457;&#26469;&#26174;&#30528;&#25552;&#39640;&#38463;&#25289;&#20271;&#25991;&#23383;OCR&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#24320;&#28304;OCR&#24341;&#25806;Kraken&#22312;&#39046;&#20808;&#30340;&#38463;&#25289;&#20271;&#23398;&#26415;&#26399;&#21002;al-Abhath&#19978;&#30340;&#20934;&#30830;&#24615;&#30740;&#31350;&#12290;&#19982;&#20854;&#20182;&#21830;&#19994;OCR&#24341;&#25806;&#30456;&#27604;&#65292;Kraken&#34920;&#29616;&#20986;&#33021;&#22815;&#29983;&#25104;&#38750;&#24120;&#20934;&#30830;&#30340;&#38463;&#25289;&#20271;&#25991;&#26412;OCR&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#36824;&#35780;&#20272;&#20102;al-Abhath&#25968;&#25454;&#19978;&#29305;&#23450;&#23383;&#20307;&#21644;&#24191;&#20041;&#27169;&#22411;&#30340;&#30456;&#23545;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;&#8220;&#38169;&#35823;&#23454;&#20363;&#8221;&#36827;&#34892;&#20102;&#24494;&#35266;&#20998;&#26512;&#20197;&#21450;&#21487;&#33021;&#23548;&#33268;OCR&#38169;&#35823;&#35782;&#21035;&#30340;&#19978;&#19979;&#25991;&#29305;&#24449;&#12290;&#22522;&#20110;&#36825;&#19968;&#20998;&#26512;&#65292;&#35770;&#25991;&#25552;&#20986;&#65292;&#36890;&#36807;&#26356;&#21152;&#31995;&#32479;&#30340;&#35757;&#32451;&#25968;&#25454;&#29983;&#20135;&#26041;&#27861;&#20197;&#21450;&#20851;&#38190;&#25216;&#26415;&#32452;&#20214;&#30340;&#24320;&#21457;&#65288;&#29305;&#21035;&#26159;&#22810;&#35821;&#35328;&#27169;&#22411;&#21644;&#25913;&#36827;&#30340;&#32447;&#27573;&#20998;&#21106;&#21644;&#24067;&#23616;&#20998;&#26512;&#65289;&#65292;&#38463;&#25289;&#20271;&#25991;&#23383;OCR&#21487;&#20197;&#24471;&#21040;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10943v1 Announce Type: new  Abstract: This work presents an accuracy study of the open source OCR engine, Kraken, on the leading Arabic scholarly journal, al-Abhath. In contrast with other commercially available OCR engines, Kraken is shown to be capable of producing highly accurate Arabic-script OCR. The study also assesses the relative accuracy of typeface-specific and generalized models on the al-Abhath data and provides a microanalysis of the ``error instances'' and the contextual features that may have contributed to OCR misrecognition. Building on this analysis, the paper argues that Arabic-script OCR can be significantly improved through (1) a more systematic approach to training data production, and (2) the development of key technological components, especially multi-language models and improved line segmentation and layout analysis.   Cet article pr{\'e}sente une {\'e}tude d'exactitude du moteur ROC open source, Krakan, sur la revue acad{\'e}mique arabe de premier 
&lt;/p&gt;</description></item><item><title>Text2Data&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36890;&#36807;&#26080;&#30417;&#30563;&#25193;&#25955;&#27169;&#22411;&#26469;&#29702;&#35299;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#32570;&#20047;&#25991;&#26412;&#26631;&#31614;&#30340;&#25991;&#26412;&#21040;&#25968;&#25454;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10941</link><description>&lt;p&gt;
Text2Data&#65306;&#20351;&#29992;&#25991;&#26412;&#25511;&#21046;&#30340;&#20302;&#36164;&#28304;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text2Data: Low-Resource Data Generation with Textual Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10941
&lt;/p&gt;
&lt;p&gt;
Text2Data&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36890;&#36807;&#26080;&#30417;&#30563;&#25193;&#25955;&#27169;&#22411;&#26469;&#29702;&#35299;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#32570;&#20047;&#25991;&#26412;&#26631;&#31614;&#30340;&#25991;&#26412;&#21040;&#25968;&#25454;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#20154;&#31867;&#19982;&#26426;&#22120;&#26080;&#32541;&#20132;&#20114;&#30340;&#19968;&#31181;&#24120;&#35265;&#30452;&#25509;&#25511;&#21046;&#20449;&#21495;&#12290;&#24847;&#35782;&#21040;&#36825;&#19968;&#25509;&#21475;&#30340;&#37325;&#35201;&#24615;&#65292;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#27491;&#22312;&#25237;&#20837;&#22823;&#37327;&#31934;&#21147;&#29983;&#25104;&#19982;&#25991;&#26412;&#25351;&#20196;&#22312;&#35821;&#20041;&#19978;&#19968;&#33268;&#30340;&#25968;&#25454;&#12290;&#34429;&#28982;&#22312;&#28085;&#30422;&#22270;&#20687;&#32534;&#36753;&#12289;&#38899;&#39057;&#21512;&#25104;&#12289;&#35270;&#39057;&#29983;&#25104;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20302;&#36164;&#28304;&#39046;&#22495;&#30001;&#20110;&#26114;&#36149;&#27880;&#37322;&#25110;&#22797;&#26434;&#25968;&#25454;&#32467;&#26500;&#65288;&#22914;&#20998;&#23376;&#12289;&#36816;&#21160;&#21160;&#24577;&#21644;&#26102;&#24207;&#65289;&#31561;&#29305;&#28857;&#65292;&#24448;&#24448;&#32570;&#20047;&#25991;&#26412;&#26631;&#31614;&#12290;&#36825;&#31181;&#19981;&#36275;&#38459;&#30861;&#20102;&#30417;&#30563;&#23398;&#20064;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23558;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#25991;&#26412;&#21040;&#25968;&#25454;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Text2Data&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36890;&#36807;&#26080;&#30417;&#30563;&#25193;&#25955;&#27169;&#22411;&#26469;&#29702;&#35299;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10941v1 Announce Type: cross  Abstract: Natural language serves as a common and straightforward control signal for humans to interact seamlessly with machines. Recognizing the importance of this interface, the machine learning community is investing considerable effort in generating data that is semantically coherent with textual instructions. While strides have been made in text-to-data generation spanning image editing, audio synthesis, video creation, and beyond, low-resource areas characterized by expensive annotations or complex data structures, such as molecules, motion dynamics, and time series, often lack textual labels. This deficiency impedes supervised learning, thereby constraining the application of advanced generative models for text-to-data tasks. In response to these challenges in the low-resource scenario, we propose Text2Data, a novel approach that utilizes unlabeled data to understand the underlying data distribution through an unsupervised diffusion model
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24341;&#20837;&#20102;&#21307;&#23398;&#29109;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22522;&#20110;ICD-9&#20195;&#30721;&#30340;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#65292;&#37327;&#21270;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10940</link><description>&lt;p&gt;
&#20020;&#24202;&#31243;&#24207;&#20195;&#30721;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#29992;&#20110;&#21307;&#23398;&#35786;&#26029;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation of clinical procedure codes for medical diagnosis and uncertainty quantification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10940
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24341;&#20837;&#20102;&#21307;&#23398;&#29109;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22522;&#20110;ICD-9&#20195;&#30721;&#30340;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#65292;&#37327;&#21270;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;CDSS&#65289;&#26088;&#22312;&#36890;&#36807;&#23558;&#31995;&#32479;&#29983;&#25104;&#30340;&#24314;&#35758;&#19982;&#21307;&#23398;&#19987;&#19994;&#30693;&#35782;&#32467;&#21512;&#26469;&#22686;&#24378;&#20020;&#24202;&#21307;&#29983;&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21307;&#23398;&#29109;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#22522;&#20110;&#25163;&#26415;ICD-9&#20195;&#30721;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26469;&#37327;&#21270;&#24739;&#32773;&#39044;&#27979;&#32467;&#26524;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#19981;&#20165;&#23637;&#31034;&#20102;&#31243;&#24207;&#20195;&#30721;&#19982;&#23454;&#38469;&#21307;&#30103;&#32467;&#26524;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10940v1 Announce Type: new  Abstract: A Clinical Decision Support System (CDSS) is designed to enhance clinician decision-making by combining system-generated recommendations with medical expertise. Given the high costs, intensive labor, and time-sensitive nature of medical treatments, there is a pressing need for efficient decision support, especially in complex emergency scenarios. In these scenarios, where information can be limited, an advanced CDSS framework that leverages AI (artificial intelligence) models to effectively reduce diagnostic uncertainty has utility. Such an AI-enabled CDSS framework with quantified uncertainty promises to be practical and beneficial in the demanding context of real-world medical care. In this study, we introduce the concept of Medical Entropy, quantifying uncertainties in patient outcomes predicted by neural machine translation based on the ICD-9 code of procedures. Our experimental results not only show strong correlations between proce
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;Reddit&#25552;&#20132;&#30340;&#26469;&#28304;&#21487;&#20449;&#24230;&#35780;&#20272;&#27169;&#22411;CREDiBERT&#65292;&#37319;&#29992;&#21322;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;Siamese&#31070;&#32463;&#32593;&#32476;&#26174;&#33879;&#25552;&#39640;&#20102;&#25552;&#20132;&#21487;&#20449;&#24230;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;Reddit&#24086;&#23376;-&#24086;&#23376;&#32593;&#32476;&#29256;&#26412;&#26469;&#22686;&#24378;&#29992;&#25143;&#20114;&#21160;&#32534;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.10938</link><description>&lt;p&gt;
&#26032;&#38395;&#26469;&#28304;&#21487;&#20449;&#24230;&#35780;&#20272;&#65306;Reddit&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
News Source Credibility Assessment: A Reddit Case Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10938
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;Reddit&#25552;&#20132;&#30340;&#26469;&#28304;&#21487;&#20449;&#24230;&#35780;&#20272;&#27169;&#22411;CREDiBERT&#65292;&#37319;&#29992;&#21322;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;Siamese&#31070;&#32463;&#32593;&#32476;&#26174;&#33879;&#25552;&#39640;&#20102;&#25552;&#20132;&#21487;&#20449;&#24230;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;Reddit&#24086;&#23376;-&#24086;&#23376;&#32593;&#32476;&#29256;&#26412;&#26469;&#22686;&#24378;&#29992;&#25143;&#20114;&#21160;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26102;&#20195;&#65292;&#35782;&#21035;&#22312;&#32447;&#20869;&#23481;&#30340;&#21487;&#20449;&#24230;&#23545;&#25239;&#20987;&#38169;&#35823;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CREDiBERT (CREDibility assessment using Bi-directional Encoder Representations from Transformers) &#30340;&#26469;&#28304;&#21487;&#20449;&#24230;&#35780;&#20272;&#27169;&#22411;&#65292;&#38024;&#23545;Reddit&#25552;&#20132;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20027;&#35201;&#20851;&#27880;&#25919;&#27835;&#35805;&#35821;&#12290;&#25105;&#20204;&#37319;&#29992;&#21322;&#30417;&#30563;&#35757;&#32451;&#26041;&#27861;&#23545;CREDiBERT&#36827;&#34892;&#35757;&#32451;&#65292;&#21033;&#29992;Reddit&#30340;&#22522;&#20110;&#31038;&#21306;&#30340;&#32467;&#26500;&#12290;&#36890;&#36807;&#20351;&#29992;CREDiBERT&#23545;&#25552;&#20132;&#20869;&#23481;&#36827;&#34892;&#32534;&#30721;&#24182;&#23558;&#20854;&#25972;&#21512;&#21040;Siamese&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#26174;&#33879;&#25913;&#21892;&#20102;&#25552;&#20132;&#21487;&#20449;&#24230;&#30340;&#20108;&#20803;&#20998;&#31867;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;F1&#20998;&#25968;&#25552;&#39640;&#20102;9%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Reddit&#19978;&#30340;&#19968;&#31181;&#26032;&#29256;&#26412;&#30340;&#24086;&#23376;-&#24086;&#23376;&#32593;&#32476;&#65292;&#26377;&#25928;&#23545;&#29992;&#25143;&#20114;&#21160;&#36827;&#34892;&#32534;&#30721;&#65292;&#23558;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#30340;F1&#20998;&#25968;&#25552;&#39640;&#20102;&#36817;8%&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;CREDiBERT&#35780;&#20272;&#20102;sus&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10938v1 Announce Type: new  Abstract: In the era of social media platforms, identifying the credibility of online content is crucial to combat misinformation. We present the CREDiBERT (CREDibility assessment using Bi-directional Encoder Representations from Transformers), a source credibility assessment model fine-tuned for Reddit submissions focusing on political discourse as the main contribution. We adopt a semi-supervised training approach for CREDiBERT, leveraging Reddit's community-based structure. By encoding submission content using CREDiBERT and integrating it into a Siamese neural network, we significantly improve the binary classification of submission credibility, achieving a 9% increase in F1 score compared to existing methods. Additionally, we introduce a new version of the post-to-post network in Reddit that efficiently encodes user interactions to enhance the binary classification task by nearly 8% in F1 score. Finally, we employ CREDiBERT to evaluate the sus
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;LLAMA2&#35821;&#35328;&#27169;&#22411;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#33021;&#22815;&#20174;&#31038;&#20132;&#23186;&#20307;&#21644;&#32039;&#24613;&#28040;&#24687;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#32039;&#24613;&#24773;&#20917;&#30340;&#26041;&#27861;&#65292;&#21487;&#21327;&#21161;&#22312;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#32039;&#24613;&#24773;&#20917;&#19979;&#20844;&#20849;&#23433;&#20840;&#35805;&#21153;&#21592;&#21644;&#22823;&#20247;&#65292;&#25552;&#20379;&#30456;&#20851;&#25351;&#23548;&#24182;&#36890;&#30693;&#25919;&#24220;&#26426;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.10908</link><description>&lt;p&gt;
LLM&#36741;&#21161;&#21361;&#26426;&#31649;&#29702;&#65306;&#26500;&#24314;&#29992;&#20110;&#26377;&#25928;&#24212;&#24613;&#21709;&#24212;&#21644;&#20844;&#20247;&#21327;&#20316;&#30340;&#20808;&#36827;LLM&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
LLM-Assisted Crisis Management: Building Advanced LLM Platforms for Effective Emergency Response and Public Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10908
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;LLAMA2&#35821;&#35328;&#27169;&#22411;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#33021;&#22815;&#20174;&#31038;&#20132;&#23186;&#20307;&#21644;&#32039;&#24613;&#28040;&#24687;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#32039;&#24613;&#24773;&#20917;&#30340;&#26041;&#27861;&#65292;&#21487;&#21327;&#21161;&#22312;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#32039;&#24613;&#24773;&#20917;&#19979;&#20844;&#20849;&#23433;&#20840;&#35805;&#21153;&#21592;&#21644;&#22823;&#20247;&#65292;&#25552;&#20379;&#30456;&#20851;&#25351;&#23548;&#24182;&#36890;&#30693;&#25919;&#24220;&#26426;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32039;&#24613;&#24773;&#20917;&#21644;&#37325;&#22823;&#20107;&#20214;&#24448;&#24448;&#36805;&#36895;&#21457;&#23637;&#65292;&#38656;&#35201;&#36805;&#36895;&#26377;&#25928;&#30340;&#21709;&#24212;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLAMA2&#65292;&#20174;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#21644;&#30452;&#25509;&#30340;&#32039;&#24613;&#28040;&#24687;&#20013;&#35782;&#21035;&#21644;&#20998;&#31867;&#32039;&#24613;&#24773;&#20917;&#12290;&#26088;&#22312;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#21147;&#37327;&#65292;&#21327;&#21161;&#20844;&#20849;&#23433;&#20840;&#35805;&#21153;&#21592;&#21644;&#22823;&#37327;&#27665;&#20247;&#22312;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#32039;&#24613;&#24773;&#20917;&#20013;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#24320;&#21457;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#22312;911&#21628;&#21483;&#20013;&#25551;&#36848;&#33258;&#24049;&#30340;&#24773;&#20917;&#65292;&#20351;LLAMA2&#33021;&#22815;&#20998;&#26512;&#20869;&#23481;&#24182;&#20026;&#35805;&#21153;&#21592;&#25552;&#20379;&#30456;&#20851;&#25351;&#23548;&#65292;&#21516;&#26102;&#21019;&#24314;&#24037;&#20316;&#27969;&#31243;&#65292;&#22312;&#24517;&#35201;&#26102;&#23558;&#21628;&#21483;&#32773;&#20449;&#24687;&#36890;&#30693;&#25919;&#24220;&#26426;&#26500;&#12290;&#35813;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#30340;&#21478;&#19968;&#20010;&#22909;&#22788;&#26159;&#65292;&#24403;911&#31995;&#32479;&#19981;&#22570;&#37325;&#36127;&#26102;&#65292;&#23427;&#33021;&#22815;&#22312;&#37325;&#22823;&#32039;&#24613;&#20107;&#20214;&#20013;&#21327;&#21161;&#20154;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10908v1 Announce Type: cross  Abstract: Emergencies and critical incidents often unfold rapidly, necessitating a swift and effective response. In this research, we introduce a novel approach to identify and classify emergency situations from social media posts and direct emergency messages using an open source Large Language Model, LLAMA2. The goal is to harness the power of natural language processing and machine learning to assist public safety telecommunicators and huge crowds during countrywide emergencies. Our research focuses on developing a language model that can understand users describe their situation in the 911 call, enabling LLAMA2 to analyze the content and offer relevant instructions to the telecommunicator, while also creating workflows to notify government agencies with the caller's information when necessary. Another benefit this language model provides is its ability to assist people during a significant emergency incident when the 911 system is overwhelme
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#20154;&#31867;&#30693;&#35782;&#65292;&#36890;&#36807;&#38382;&#31572;&#20219;&#21153;&#25506;&#31350;LM&#30340;&#19981;&#36947;&#24503;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#19968;&#33268;&#24615;&#21644;&#20559;&#35265;&#20542;&#21521;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2402.10899</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#31867;&#27861;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#26816;&#26597;&#34920;
&lt;/p&gt;
&lt;p&gt;
Taxonomy-based CheckList for Large Language Model Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#20154;&#31867;&#30693;&#35782;&#65292;&#36890;&#36807;&#38382;&#31572;&#20219;&#21153;&#25506;&#31350;LM&#30340;&#19981;&#36947;&#24503;&#34892;&#20026;&#65292;&#21457;&#29616;&#20102;&#19968;&#33268;&#24615;&#21644;&#20559;&#35265;&#20542;&#21521;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#29992;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#65292;&#20869;&#22312;&#30340;&#21051;&#26495;&#21360;&#35937;&#21487;&#33021;&#24433;&#21709;&#36755;&#20986;&#30340;&#20844;&#24179;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#20154;&#31867;&#30693;&#35782;&#24341;&#20837;&#33258;&#28982;&#35821;&#35328;&#24178;&#39044;&#65292;&#24182;&#30740;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#24615;&#21035;&#20559;&#35265;&#19978;&#30340;&#34892;&#20026;&#12290;&#21463;CheckList&#34892;&#20026;&#27979;&#35797;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#31867;&#20284;&#26816;&#26597;&#34920;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#38382;&#31572;&#65288;QA&#65289;&#25506;&#31350;&#24182;&#37327;&#21270;LMs&#30340;&#19981;&#36947;&#24503;&#34892;&#20026;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#39033;&#27604;&#36739;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;LMs&#30340;&#19968;&#33268;&#24615;&#12289;&#20559;&#35265;&#20542;&#21521;&#12289;&#27169;&#22411;&#20559;&#22909;&#21644;&#24615;&#21035;&#20559;&#22909;&#20999;&#25442;&#12290;&#25105;&#20204;&#25506;&#31350;&#20102;&#19968;&#20010;&#22522;&#20110;transformer&#30340;QA&#27169;&#22411;&#65288;&#22312;SQuAD-v2&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65289;&#21644;&#19968;&#20010;&#33258;&#22238;&#24402;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;transformer-based QA&#27169;&#22411;&#30340;&#20559;&#35265;&#20542;&#21521;&#19982;&#20854;&#19968;&#33268;&#24615;&#21576;&#27491;&#30456;&#20851;&#65292;&#32780;LLM&#34920;&#29616;&#20986;&#30456;&#21453;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10899v1 Announce Type: new  Abstract: As large language models (LLMs) have been used in many downstream tasks, the internal stereotypical representation may affect the fairness of the outputs. In this work, we introduce human knowledge into natural language interventions and study pre-trained language models' (LMs) behaviors within the context of gender bias. Inspired by CheckList behavioral testing, we present a checklist-style task that aims to probe and quantify LMs' unethical behaviors through question-answering (QA). We design three comparison studies to evaluate LMs from four aspects: consistency, biased tendency, model preference, and gender preference switch. We probe one transformer-based QA model trained on SQuAD-v2 dataset and one autoregressive large language model. Our results indicate that transformer-based QA model's biased tendency positively correlates with its consistency, whereas LLM shows the opposite relation. Our proposed task provides the first dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;LLMs&#19982;&#20256;&#32479;&#27169;&#22411;&#65292;&#21457;&#29616;&#20102;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;LLMs&#22312;&#39044;&#27979;&#20855;&#26377;&#26126;&#26174;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#32570;&#20047;&#21608;&#26399;&#24615;&#30340;&#25968;&#25454;&#38598;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#21516;&#26102;&#25351;&#20986;&#34701;&#20837;&#22806;&#37096;&#30693;&#35782;&#21644;&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#37322;&#20041;&#26377;&#21161;&#20110;&#25552;&#21319;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.10835</link><description>&lt;p&gt;
LLMs&#19979;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65306;&#29702;&#35299;&#21644;&#22686;&#24378;&#27169;&#22411;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;LLMs&#19982;&#20256;&#32479;&#27169;&#22411;&#65292;&#21457;&#29616;&#20102;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#25351;&#20986;LLMs&#22312;&#39044;&#27979;&#20855;&#26377;&#26126;&#26174;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#32570;&#20047;&#21608;&#26399;&#24615;&#30340;&#25968;&#25454;&#38598;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#21516;&#26102;&#25351;&#20986;&#34701;&#20837;&#22806;&#37096;&#30693;&#35782;&#21644;&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#37322;&#20041;&#26377;&#21161;&#20110;&#25552;&#21319;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36817;&#24180;&#26469;&#22312;&#35768;&#22810;&#39046;&#22495;&#24471;&#21040;&#36805;&#36895;&#21457;&#23637;&#12290;&#20316;&#20026;&#19968;&#31181;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26368;&#36817;&#20174;LLMs&#20013;&#33719;&#24471;&#20102;&#25512;&#21160;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#39046;&#22495;&#65292;LLMs&#30340;&#20559;&#22909;&#23384;&#22312;&#30740;&#31350;&#31354;&#30333;&#12290;&#36890;&#36807;&#23558;LLMs&#19982;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#20102;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#35768;&#22810;&#29305;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#22312;&#39044;&#27979;&#20855;&#26377;&#26126;&#26174;&#27169;&#24335;&#21644;&#36235;&#21183;&#30340;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#32570;&#20047;&#21608;&#26399;&#24615;&#30340;&#25968;&#25454;&#38598;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#35201;&#27714;LLMs&#21578;&#30693;&#25968;&#25454;&#38598;&#30340;&#21608;&#26399;&#26469;&#35299;&#37322;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#30740;&#31350;&#20102;&#36755;&#20837;&#31574;&#30053;&#65292;&#21457;&#29616;&#34701;&#20837;&#22806;&#37096;&#30693;&#35782;&#21644;&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#37322;&#20041;&#31215;&#26497;&#24433;&#21709;&#20102;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#39033;&#30740;&#31350;&#26377;&#21161;&#20110;&#27934;&#23519;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10835v1 Announce Type: new  Abstract: Large language models (LLMs) have been applied in many fields with rapid development in recent years. As a classic machine learning task, time series forecasting has recently received a boost from LLMs. However, there is a research gap in the LLMs' preferences in this field. In this paper, by comparing LLMs with traditional models, many properties of LLMs in time series prediction are found. For example, our study shows that LLMs excel in predicting time series with clear patterns and trends but face challenges with datasets lacking periodicity. We explain our findings through designing prompts to require LLMs to tell the period of the datasets. In addition, the input strategy is investigated, and it is found that incorporating external knowledge and adopting natural language paraphrases positively affects the predictive performance of LLMs for time series. Overall, this study contributes to insight into the advantages and limitations of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#31038;&#20250;&#22240;&#32032;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#25351;&#26631;$LSS_{\beta}$&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#24615;&#23637;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.10567</link><description>&lt;p&gt;
&#22312;InSaAF&#20013;&#34701;&#20837;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615; | LLM&#26159;&#21542;&#24050;&#32463;&#20934;&#22791;&#22909;&#36827;&#20837;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#65311;
&lt;/p&gt;
&lt;p&gt;
InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#31038;&#20250;&#22240;&#32032;&#26102;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#25351;&#26631;$LSS_{\beta}$&#65292;&#24182;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#24615;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#25216;&#26415;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#23548;&#33268;&#25552;&#20986;&#20102;&#20247;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25191;&#34892;&#27861;&#24459;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#65292;&#20174;&#39044;&#27979;&#21028;&#20915;&#21040;&#29983;&#25104;&#25688;&#35201;&#12290;&#23613;&#31649;&#23427;&#20204;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#24050;&#32463;&#35777;&#26126;&#36825;&#20123;&#27169;&#22411;&#23398;&#20064;&#24182;&#23637;&#31034;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24403;&#28041;&#21450;&#31038;&#20250;&#22240;&#32032;&#26102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#25191;&#34892;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;$\beta$-&#21152;&#26435;&#30340;$\textit{&#27861;&#24459;&#23433;&#20840;&#20998;&#25968;($LSS_{\beta}$)}$&#65292;&#23558;LLM&#30340;&#20844;&#24179;&#24615;&#21644;&#20934;&#30830;&#24615;&#20004;&#20010;&#26041;&#38754;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;LLM&#22312;$\textit{&#20108;&#20803;&#27861;&#24459;&#25512;&#29702;}$&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#20854;&#22312;&#21360;&#24230;&#31038;&#20250;&#21508;&#31181;&#19981;&#24179;&#31561;&#26041;&#38754;&#30340;&#20844;&#24179;&#23637;&#31034;&#26469;&#35780;&#20272;LLMs&#30340;&#23433;&#20840;&#24615;&#12290;LLaMA&#21644;LLaMA--2&#27169;&#22411;&#30340;&#20219;&#21153;&#34920;&#29616;&#21644;&#20844;&#24179;&#24471;&#20998;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10567v1 Announce Type: cross  Abstract: Recent advancements in language technology and Artificial Intelligence have resulted in numerous Language Models being proposed to perform various tasks in the legal domain ranging from predicting judgments to generating summaries. Despite their immense potential, these models have been proven to learn and exhibit societal biases and make unfair predictions. In this study, we explore the ability of Large Language Models (LLMs) to perform legal tasks in the Indian landscape when social factors are involved. We present a novel metric, $\beta$-weighted $\textit{Legal Safety Score ($LSS_{\beta}$)}$, which encapsulates both the fairness and accuracy aspects of the LLM. We assess LLMs' safety by considering its performance in the $\textit{Binary Statutory Reasoning}$ task and its fairness exhibition with respect to various axes of disparities in the Indian society. Task performance and fairness scores of LLaMA and LLaMA--2 models indicate th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.10207</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#22870;&#21169;&#65306;&#22522;&#20110;&#21160;&#24577;&#20559;&#22909;&#35843;&#25972;&#30340;&#22810;&#30446;&#26631;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Rewards-in-Context: Multi-objective Alignment of Foundation Models with Dynamic Preference Adjustment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22810;&#20010;&#22870;&#21169;&#26465;&#20214;&#25511;&#21046;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#36827;&#34892;&#23545;&#40784;&#12290;&#23427;&#20855;&#26377;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#30784;&#27169;&#22411;&#22810;&#30446;&#26631;&#23545;&#40784;&#38382;&#39064;&#65292;&#36825;&#26159;&#23454;&#29616;&#26377;&#30410;&#21644;&#26080;&#23475;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23545;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#36890;&#24120;&#26159;&#26114;&#36149;&#19988;&#19981;&#31283;&#23450;&#30340;&#65292;&#24182;&#19988;&#20154;&#31867;&#20559;&#22909;&#30340;&#22810;&#32500;&#24230;&#12289;&#24322;&#36136;&#24615;&#21644;&#20914;&#31361;&#24615;&#36827;&#19968;&#27493;&#22797;&#26434;&#21270;&#20102;&#23545;&#40784;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Rewards-in-Context&#65288;RiC&#65289;&#26041;&#27861;&#65292;&#23427;&#20351;&#24471;&#22522;&#30784;&#27169;&#22411;&#30340;&#21709;&#24212;&#21462;&#20915;&#20110;&#20854;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#30340;&#22810;&#20010;&#22870;&#21169;&#65292;&#24182;&#24212;&#29992;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#26469;&#36827;&#34892;&#23545;&#40784;&#12290;RiC&#30340;&#26174;&#33879;&#29305;&#28857;&#26159;&#31616;&#21333;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#23545;&#21333;&#20010;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#26377;&#30417;&#30563;&#30340;&#24494;&#35843;&#65292;&#24182;&#25903;&#25345;&#22312;&#25512;&#29702;&#26102;&#21160;&#24577;&#35843;&#25972;&#29992;&#25143;&#20559;&#22909;&#12290;&#21463;&#21040;&#25277;&#35937;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#26512;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#25512;&#29702;&#26102;&#35843;&#25972;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10207v1 Announce Type: cross  Abstract: We consider the problem of multi-objective alignment of foundation models with human preferences, which is a critical step towards helpful and harmless AI systems. However, it is generally costly and unstable to fine-tune large foundation models using reinforcement learning (RL), and the multi-dimensionality, heterogeneity, and conflicting nature of human preferences further complicate the alignment process. In this paper, we introduce Rewards-in-Context (RiC), which conditions the response of a foundation model on multiple rewards in its prompt context and applies supervised fine-tuning for alignment. The salient features of RiC are simplicity and adaptivity, as it only requires supervised fine-tuning of a single foundation model and supports dynamic adjustment for user preferences during inference time. Inspired by the analytical solution of an abstracted convex optimization problem, our dynamic inference-time adjustment method appro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10184</link><description>&lt;p&gt;
&#37325;&#22609;RLHF&#20013;&#30340;&#20449;&#24687;&#32467;&#26500;&#65306;&#22522;&#20110;&#22270;&#35770;&#30340;&#22870;&#21169;&#27867;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#23384;&#22312;&#19968;&#20010;&#19977;&#38590;&#38382;&#39064;&#65306;&#39640;&#24230;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#26469;&#32531;&#35299;&#36825;&#31181;&#19981;&#20860;&#23481;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;RLHF&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#23558;&#20854;&#25551;&#32472;&#20026;&#25991;&#26412;&#20998;&#24067;&#19978;&#30340;&#33258;&#21160;&#32534;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24418;&#24335;&#21270;&#20102;RLHF&#30446;&#26631;&#65292;&#21363;&#30830;&#20445;&#20154;&#31867;&#20559;&#22909;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34892;&#20026;&#20043;&#38388;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;RLHF&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#20449;&#24687;&#32467;&#26500;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#30340;&#22870;&#21169;&#27867;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#22270;&#35770;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#27867;&#21270;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
&lt;/p&gt;</description></item><item><title>LoraRetriever&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#36755;&#20837;&#30340;LoRA&#26816;&#32034;&#19982;&#21512;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#24357;&#21512;&#23454;&#38469;&#24773;&#20917;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25509;&#25910;&#21040;&#19981;&#21516;&#20219;&#21153;&#25552;&#31034;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.09997</link><description>&lt;p&gt;
LoraRetriever: &#36866;&#24212;&#36755;&#20837;&#30340;LoRA&#26816;&#32034;&#19982;&#21512;&#25104;&#26041;&#27861;&#29992;&#20110;&#28151;&#21512;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
LoraRetriever: Input-Aware LoRA Retrieval and Composition for Mixed Tasks in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09997
&lt;/p&gt;
&lt;p&gt;
LoraRetriever&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#36755;&#20837;&#30340;LoRA&#26816;&#32034;&#19982;&#21512;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#24357;&#21512;&#23454;&#38469;&#24773;&#20917;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25509;&#25910;&#21040;&#19981;&#21516;&#20219;&#21153;&#25552;&#31034;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Low-Rank Adaptation (LoRA)&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24494;&#35843;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#32780;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;LoRA&#30340;&#27169;&#22359;&#21270;&#21644;&#21363;&#25554;&#21363;&#29992;&#30340;&#29305;&#24615;&#20351;&#24471;&#33021;&#22815;&#38598;&#25104;&#21508;&#31181;&#39046;&#22495;&#29305;&#23450;&#30340;LoRA&#65292;&#20197;&#22686;&#24378;LLM&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35201;&#20040;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#38548;&#31163;&#19979;&#28216;&#20219;&#21153;&#65292;&#35201;&#20040;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22266;&#23450;LoRA&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#65292;LLM&#25509;&#25910;&#21040;&#28085;&#30422;&#19981;&#21516;&#20219;&#21153;&#30340;&#21508;&#31181;&#25552;&#31034;&#65292;&#24182;&#19988;&#20505;&#36873;LoRA&#30340;&#27744;&#32463;&#24120;&#21160;&#24577;&#26356;&#26032;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoraRetriever&#65292;&#19968;&#31181;&#26681;&#25454;&#36755;&#20837;&#25552;&#31034;&#33258;&#36866;&#24212;&#26816;&#32034;&#21644;&#21512;&#25104;&#22810;&#20010;LoRA&#30340;&#26694;&#26550;&#12290;LoraRetriever&#21253;&#21547;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#39318;&#20808;&#65292;&#35782;&#21035;&#21644;&#26816;&#32034;&#19982;&#32473;&#23450;&#36755;&#20837;&#30456;&#20851;&#30340;LoRA&#65307;&#20854;&#27425;&#65292;&#21046;&#23450;&#26377;&#25928;&#25972;&#21512;&#26816;&#32034;&#21040;&#30340;LoRA&#30340;&#31574;&#30053;&#65307;&#26368;&#21518;&#65292;&#24320;&#21457;&#39640;&#25928;&#30340;&#26041;&#27861;&#29992;&#20110;&#23454;&#29616;LoRA&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09997v1 Announce Type: new  Abstract: Low-Rank Adaptation (LoRA) provides an effective yet efficient solution for fine-tuning large language models (LLM). The modular and plug-and-play nature of LoRA enables the integration of diverse domain-specific LoRAs to enhance the capabilities of LLMs. Previous research on exploiting multiple LoRAs either focuses on specific isolated downstream tasks or fixes the selection of LoRAs during training. However, in real-world scenarios, LLMs receive diverse prompts covering different tasks, and the pool of candidate LoRAs is often dynamically updated. To bridge this gap, we propose LoraRetriever, a retrieve-then-compose framework that adaptively retrieves and composes multiple LoRAs according to the input prompts. LoraRetriever contains three main components: firstly, identifying and retrieving LoRAs relevant to the given input; secondly, formulating strategies for effectively integrating the retrieved LoRAs; and thirdly, developing effici
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RiVEG&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#36830;&#25509;&#26725;&#26753;&#65292;&#23558;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#37325;&#26032;&#26500;&#24314;&#20026;&#32852;&#21512;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#21629;&#21517;&#23454;&#20307;&#26080;&#27861;&#30830;&#23450;&#21644;&#25351;&#20195;&#34920;&#36798;&#19982;&#21629;&#21517;&#23454;&#20307;&#20043;&#38388;&#30340;&#21306;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.09989</link><description>&lt;p&gt;
LLMs&#20316;&#20026;&#26725;&#26753;&#65306;&#37325;&#26032;&#26500;&#24314;&#22522;&#20110;&#22810;&#27169;&#24577;&#22270;&#20687;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
LLMs as Bridges: Reformulating Grounded Multimodal Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RiVEG&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#36830;&#25509;&#26725;&#26753;&#65292;&#23558;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#37325;&#26032;&#26500;&#24314;&#20026;&#32852;&#21512;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#21629;&#21517;&#23454;&#20307;&#26080;&#27861;&#30830;&#23450;&#21644;&#25351;&#20195;&#34920;&#36798;&#19982;&#21629;&#21517;&#23454;&#20307;&#20043;&#38388;&#30340;&#21306;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grounded Multimodal Named Entity Recognition (GMNER) &#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#26088;&#22312;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#12289;&#23454;&#20307;&#31867;&#22411;&#21450;&#20854;&#23545;&#24212;&#30340;&#35270;&#35273;&#21306;&#22495;&#12290;GMNER&#20219;&#21153;&#20855;&#26377;&#20004;&#20010;&#25361;&#25112;&#24615;&#36136;&#65306;1&#65289;&#31038;&#20132;&#23186;&#20307;&#20013;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#24369;&#30456;&#20851;&#24615;&#23548;&#33268;&#22823;&#37096;&#20998;&#21629;&#21517;&#23454;&#20307;&#38590;&#20197;&#30830;&#23450;&#65307;2&#65289;&#24120;&#29992;&#20110;&#31867;&#20284;&#20219;&#21153;&#30340;&#31895;&#31890;&#24230;&#25351;&#20195;&#34920;&#36798;&#19982;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#21306;&#21035;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;RiVEG&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#36830;&#25509;&#26725;&#26753;&#65292;&#23558;GMNER&#37325;&#26032;&#26500;&#24314;&#20026;&#32852;&#21512;MNER-VE-VG&#20219;&#21153;&#12290;&#36825;&#31181;&#37325;&#26032;&#26500;&#24314;&#24102;&#26469;&#20102;&#20004;&#20010;&#22909;&#22788;&#65306;1&#65289;&#20445;&#25345;&#20102;&#26368;&#20339;&#30340;MNER&#24615;&#33021;&#65292;&#28040;&#38500;&#20102;&#20351;&#29992;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#39044;&#25552;&#21462;&#21306;&#22495;&#29305;&#24449;&#30340;&#38656;&#27714;&#65292;&#33258;&#28982;&#35299;&#20915;&#20102;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09989v1 Announce Type: cross  Abstract: Grounded Multimodal Named Entity Recognition (GMNER) is a nascent multimodal task that aims to identify named entities, entity types and their corresponding visual regions. GMNER task exhibits two challenging properties: 1) The weak correlation between image-text pairs in social media results in a significant portion of named entities being ungroundable. 2) There exists a distinction between coarse-grained referring expressions commonly used in similar tasks (e.g., phrase localization, referring expression comprehension) and fine-grained named entities. In this paper, we propose RiVEG, a unified framework that reformulates GMNER into a joint MNER-VE-VG task by leveraging large language models (LLMs) as a connecting bridge. This reformulation brings two benefits: 1) It maintains the optimal MNER performance and eliminates the need for employing object detection methods to pre-extract regional features, thereby naturally addressing two m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20110;&#35282;&#33394;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#35843;&#25972;&#25552;&#31034;&#25351;&#20196;&#21487;&#20197;&#26368;&#30452;&#25509;&#26377;&#25928;&#19988;&#32463;&#27982;&#22320;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#19988;&#38543;&#26426;&#26816;&#32034;&#31034;&#33539;&#20250;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#65292;&#32780;&#26597;&#35810;&#30456;&#21516;&#19978;&#19979;&#25991;&#30340;&#31034;&#33539;&#26816;&#32034;&#25928;&#26524;&#26368;&#24046;&#12290;&#21363;&#20351;&#30772;&#22351;&#20102;&#31034;&#33539;&#20013;&#30340;&#22810;&#22238;&#21512;&#20851;&#32852;&#21644;&#21333;&#22238;&#21512;&#35821;&#20041;&#65292;&#23545;&#35805;&#29983;&#25104;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.09954</link><description>&lt;p&gt;
&#21046;&#23450;&#33391;&#22909;&#25552;&#31034;&#36824;&#26159;&#25552;&#20379;&#20986;&#33394;&#30340;&#23545;&#35805;&#65311;&#20851;&#20110;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35282;&#33394;&#29983;&#25104;&#23545;&#35805;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20110;&#35282;&#33394;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#35843;&#25972;&#25552;&#31034;&#25351;&#20196;&#21487;&#20197;&#26368;&#30452;&#25509;&#26377;&#25928;&#19988;&#32463;&#27982;&#22320;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#19988;&#38543;&#26426;&#26816;&#32034;&#31034;&#33539;&#20250;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#65292;&#32780;&#26597;&#35810;&#30456;&#21516;&#19978;&#19979;&#25991;&#30340;&#31034;&#33539;&#26816;&#32034;&#25928;&#26524;&#26368;&#24046;&#12290;&#21363;&#20351;&#30772;&#22351;&#20102;&#31034;&#33539;&#20013;&#30340;&#22810;&#22238;&#21512;&#20851;&#32852;&#21644;&#21333;&#22238;&#21512;&#35821;&#20041;&#65292;&#23545;&#35805;&#29983;&#25104;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#20851;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#30740;&#31350;&#20027;&#35201;&#20391;&#37325;&#20110;&#20998;&#31867;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#34920;&#26684;&#31561;&#20219;&#21153;&#65292;&#32780;&#23545;&#20110;ICL&#33021;&#21542;&#25913;&#36827;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#23545;&#35805;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#22312;&#39640;&#36136;&#37327;&#30340;&#30495;&#23454;&#20154;&#31867;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22522;&#20110;&#35282;&#33394;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#30340;ICL&#33021;&#21147;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#19977;&#20010;&#32467;&#35770;&#65306;1&#65289;&#35843;&#25972;&#25552;&#31034;&#25351;&#20196;&#26159;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#26368;&#30452;&#25509;&#12289;&#26377;&#25928;&#21644;&#32463;&#27982;&#30340;&#26041;&#27861;&#65307;2&#65289;&#38543;&#26426;&#26816;&#32034;&#31034;&#33539;&#21487;&#20197;&#21462;&#24471;&#26368;&#20339;&#30340;&#32467;&#26524;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#20855;&#26377;&#26356;&#22810;&#26679;&#21270;&#21644;&#26377;&#25928;&#20449;&#24687;&#30340;&#21407;&#22240;&#65307;&#19982;&#26597;&#35810;&#30456;&#21516;&#19978;&#19979;&#25991;&#30340;&#31034;&#33539;&#26816;&#32034;&#32467;&#26524;&#26368;&#24046;&#65307;3&#65289;&#21363;&#20351;&#30772;&#22351;&#20102;&#31034;&#33539;&#20013;&#30340;&#22810;&#22238;&#21512;&#20851;&#32852;&#21644;&#21333;&#22238;&#21512;&#35821;&#20041;&#65292;&#23545;&#35805;&#29983;&#25104;&#20173;&#28982;&#21487;&#20197;&#23454;&#29616;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09954v1 Announce Type: new  Abstract: Previous in-context learning (ICL) research has focused on tasks such as classification, machine translation, text2table, etc., while studies on whether ICL can improve human-like dialogue generation are scarce. Our work fills this gap by systematically investigating the ICL capabilities of large language models (LLMs) in persona-based dialogue generation, conducting extensive experiments on high-quality real human Chinese dialogue datasets. From experimental results, we draw three conclusions: 1) adjusting prompt instructions is the most direct, effective, and economical way to improve generation quality; 2) randomly retrieving demonstrations (demos) achieves the best results, possibly due to the greater diversity and the amount of effective information; counter-intuitively, retrieving demos with a context identical to the query performs the worst; 3) even when we destroy the multi-turn associations and single-turn semantics in the demo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#25209;&#21028;&#24615;&#35780;&#20272;&#30740;&#31350;&#20102;23&#20010;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20559;&#35265;&#12289;&#30495;&#23454;&#25512;&#29702;&#34913;&#37327;&#22256;&#38590;&#12289;&#23454;&#29616;&#19981;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#38656;&#35201;&#26631;&#20934;&#21270;&#26041;&#27861;&#12289;&#30417;&#31649;&#30830;&#23450;&#24615;&#21644;&#20262;&#29702;&#25351;&#21335;&#12290;</title><link>https://arxiv.org/abs/2402.09880</link><description>&lt;p&gt;
&#22312;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#30340;&#19981;&#36275;&#20043;&#22788;
&lt;/p&gt;
&lt;p&gt;
Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#25209;&#21028;&#24615;&#35780;&#20272;&#30740;&#31350;&#20102;23&#20010;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20559;&#35265;&#12289;&#30495;&#23454;&#25512;&#29702;&#34913;&#37327;&#22256;&#38590;&#12289;&#23454;&#29616;&#19981;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#38656;&#35201;&#26631;&#20934;&#21270;&#26041;&#27861;&#12289;&#30417;&#31649;&#30830;&#23450;&#24615;&#21644;&#20262;&#29702;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38543;&#30528;&#20854;&#26032;&#20852;&#33021;&#21147;&#30340;&#24555;&#36895;&#23835;&#36215;&#65292;&#24341;&#21457;&#20102;&#20844;&#20247;&#30340;&#22909;&#22855;&#24515;&#65292;&#20197;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;LLMs&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#20182;&#20204;&#30340;LLM&#22522;&#20934;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#36825;&#20123;&#22522;&#20934;&#30340;&#21021;&#27493;&#19981;&#36275;&#65292;&#24320;&#22987;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#36890;&#36807;&#20154;&#20204;&#12289;&#36807;&#31243;&#21644;&#25216;&#26415;&#30340;&#35270;&#35282;&#65292;&#20197;&#21151;&#33021;&#21644;&#23433;&#20840;&#20004;&#22823;&#25903;&#26609;&#20026;&#22522;&#30784;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#39062;&#32479;&#19968;&#35780;&#20272;&#26694;&#26550;&#23545;23&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#22522;&#20934;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19968;&#20123;&#37325;&#22823;&#38480;&#21046;&#65292;&#21253;&#25324;&#20559;&#35265;&#12289;&#27979;&#37327;&#30495;&#23454;&#25512;&#29702;&#30340;&#22256;&#38590;&#12289;&#36866;&#24212;&#24615;&#12289;&#23454;&#29616;&#19981;&#19968;&#33268;&#24615;&#12289;&#25552;&#31034;&#24037;&#31243;&#22797;&#26434;&#24615;&#12289;&#35780;&#20272;&#32773;&#22810;&#26679;&#24615;&#20197;&#21450;&#22312;&#19968;&#27425;&#32508;&#21512;&#35780;&#20272;&#20013;&#24573;&#35270;&#20102;&#25991;&#21270;&#21644;&#24847;&#35782;&#24418;&#24577;&#35268;&#33539;&#12290;&#25105;&#20204;&#30340;&#35752;&#35770;&#24378;&#35843;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#36843;&#20999;&#38656;&#35201;&#26631;&#20934;&#21270;&#26041;&#27861;&#12289;&#30417;&#31649;&#30830;&#23450;&#24615;&#21644;&#20262;&#29702;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09880v1 Announce Type: new  Abstract: The rapid rise in popularity of Large Language Models (LLMs) with emerging capabilities has spurred public curiosity to evaluate and compare different LLMs, leading many researchers to propose their LLM benchmarks. Noticing preliminary inadequacies in those benchmarks, we embarked on a study to critically assess 23 state-of-the-art LLM benchmarks, using our novel unified evaluation framework through the lenses of people, process, and technology, under the pillars of functionality and security. Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment. Our discussions emphasized the urgent need for standardized methodologies, regulatory certainties, and ethical guidelines in light of Artificial Intelligenc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#20998;&#23376;&#21644;&#36866;&#24212;&#30151;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26426;&#36935;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#24182;&#27979;&#35797;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#36825;&#23545;&#20110;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.09588</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#20998;&#23376;&#21644;&#36866;&#24212;&#30151;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26032;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Emerging Opportunities of Using Large Language Language Models for Translation Between Drug Molecules and Indications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#20998;&#23376;&#21644;&#36866;&#24212;&#30151;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#26426;&#36935;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#24182;&#27979;&#35797;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#36825;&#23545;&#20110;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#20998;&#23376;&#26159;&#19968;&#31181;&#25913;&#21464;&#29983;&#29289;&#20307;&#31934;&#31070;&#25110;&#36523;&#20307;&#29366;&#24577;&#30340;&#29289;&#36136;&#12290;&#27599;&#31181;&#25209;&#20934;&#30340;&#33647;&#29289;&#37117;&#26377;&#36866;&#24212;&#30151;&#65292;&#25351;&#30340;&#26159;&#35813;&#33647;&#29289;&#27835;&#30103;&#29305;&#23450;&#21307;&#30103;&#26465;&#20214;&#30340;&#27835;&#30103;&#29992;&#36884;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#19968;&#31181;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#26368;&#36817;&#24050;&#32463;&#35777;&#26126;&#22312;&#20998;&#23376;&#21644;&#20854;&#25991;&#26412;&#25551;&#36848;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#20173;&#23384;&#22312;&#20851;&#20110;&#22312;&#33647;&#29289;&#20998;&#23376;&#21644;&#36866;&#24212;&#30151;&#65288;&#25110;&#21453;&#20043;&#65289;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#30340;&#24212;&#29992;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#36825;&#21487;&#33021;&#26497;&#22823;&#22320;&#26377;&#30410;&#20110;&#33647;&#29289;&#21457;&#29616;&#36807;&#31243;&#12290;&#20174;&#32473;&#23450;&#30340;&#36866;&#24212;&#30151;&#29983;&#25104;&#33647;&#29289;&#30340;&#33021;&#21147;&#23558;&#20801;&#35768;&#21457;&#29616;&#38024;&#23545;&#29305;&#23450;&#30142;&#30149;&#25110;&#38774;&#28857;&#30340;&#33647;&#29289;&#65292;&#24182;&#26368;&#32456;&#20026;&#24739;&#32773;&#25552;&#20379;&#26356;&#22909;&#30340;&#27835;&#30103;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#33647;&#29289;&#20998;&#23376;&#21644;&#30456;&#24212;&#36866;&#24212;&#30151;&#20043;&#38388;&#30340;&#32763;&#35793;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#23454;&#39564;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09588v1 Announce Type: new  Abstract: A drug molecule is a substance that changes the organism's mental or physical state. Every approved drug has an indication, which refers to the therapeutic use of that drug for treating a particular medical condition. While the Large Language Model (LLM), a generative Artificial Intelligence (AI) technique, has recently demonstrated effectiveness in translating between molecules and their textual descriptions, there remains a gap in research regarding their application in facilitating the translation between drug molecules and indications, or vice versa, which could greatly benefit the drug discovery process. The capability of generating a drug from a given indication would allow for the discovery of drugs targeting specific diseases or targets and ultimately provide patients with better treatments. In this paper, we first propose a new task, which is the translation between drug molecules and corresponding indications, and then test exi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LlaSMol&#65292;&#23427;&#26159;&#19968;&#31181;&#25512;&#36827;&#21270;&#23398;&#39046;&#22495;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;LlaSMol&#22312;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;GPT-4&#24182;&#25509;&#36817;&#20110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.09391</link><description>&lt;p&gt;
LlaSMol:&#21033;&#29992;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#25512;&#36827;&#21270;&#23398;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LlaSMol: Advancing Large Language Models for Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LlaSMol&#65292;&#23427;&#26159;&#19968;&#31181;&#25512;&#36827;&#21270;&#23398;&#39046;&#22495;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;LlaSMol&#22312;&#21270;&#23398;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;GPT-4&#24182;&#25509;&#36817;&#20110;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#22312;&#33647;&#29289;&#30740;&#21457;&#21644;&#26448;&#26009;&#31185;&#23398;&#31561;&#35768;&#22810;&#39046;&#22495;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#35832;&#22914;GPT-4&#20043;&#31867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#29616;&#26377;&#24037;&#20316;&#34920;&#26126;&#23427;&#20204;&#22312;&#21270;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20196;&#20154;&#22833;&#26395;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#24320;&#21457;&#30340;LLM&#22312;&#19968;&#31995;&#21015;&#21270;&#23398;&#20219;&#21153;&#19978;&#21487;&#20197;&#21462;&#24471;&#38750;&#24120;&#24378;&#22823;&#30340;&#32467;&#26524;&#65292;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GPT-4&#65292;&#24182;&#25509;&#36817;SoTA&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#25105;&#20204;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#26159;&#19968;&#20010;&#21517;&#20026;SMolInstruct&#30340;&#22823;&#35268;&#27169;&#12289;&#20840;&#38754;&#12289;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#21547;&#20102;14&#20010;&#32463;&#36807;&#31934;&#24515;&#25361;&#36873;&#30340;&#21270;&#23398;&#20219;&#21153;&#21644;&#36229;&#36807;&#19977;&#30334;&#19975;&#20010;&#39640;&#36136;&#37327;&#26679;&#26412;&#65292;&#20026;&#35757;&#32451;&#21644;&#35780;&#20272;&#21270;&#23398;LLM&#22880;&#23450;&#20102;&#22362;&#23454;&#22522;&#30784;&#12290;&#22522;&#20110;SMolInstruct&#65292;&#25105;&#20204;&#23545;&#19968;&#32452;&#24320;&#28304;LLM&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20854;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;Mistral ser&#26159;&#26368;&#20339;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09391v1 Announce Type: new Abstract: Chemistry plays a crucial role in many domains, such as drug discovery and material science. While large language models (LLMs) such as GPT-4 exhibit remarkable capabilities on natural language processing tasks, existing work shows their performance on chemistry tasks is discouragingly low. In this paper, however, we demonstrate that our developed LLMs can achieve very strong results on a comprehensive set of chemistry tasks, outperforming the most advanced GPT-4 across all the tasks by a substantial margin and approaching the SoTA task-specific models. The key to our success is a large-scale, comprehensive, high-quality dataset for instruction tuning named SMolInstruct. It contains 14 meticulously selected chemistry tasks and over three million high-quality samples, laying a solid foundation for training and evaluating LLMs for chemistry. Based on SMolInstruct, we fine-tune a set of open-source LLMs, among which, we find that Mistral ser
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SafeDecoding&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#31574;&#30053;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;LLMs&#23433;&#20840;&#24615;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.08983</link><description>&lt;p&gt;
SafeDecoding: &#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SafeDecoding&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23433;&#20840;&#24863;&#30693;&#35299;&#30721;&#31574;&#30053;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#12290;&#35813;&#31574;&#30053;&#21487;&#20197;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;LLMs&#23433;&#20840;&#24615;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#36741;&#21161;&#31561;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#20154;&#20204;&#20026;&#20102;&#20351;LLM&#30340;&#34892;&#20026;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#65292;&#21253;&#25324;&#23433;&#20840;&#24615;&#22312;&#20869;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#12290;&#36234;&#29425;&#25915;&#20987;&#26088;&#22312;&#24341;&#21457;LLM&#30340;&#38750;&#39044;&#26399;&#21644;&#19981;&#23433;&#20840;&#34892;&#20026;&#65292;&#20173;&#28982;&#26159;LLM&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#23041;&#32961;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;SafeDecoding&#26469;&#38450;&#24481;LLM&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#31181;&#23433;&#20840;&#24863;&#30693;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;&#23545;&#29992;&#25143;&#26597;&#35810;&#26377;&#30410;&#19988;&#26080;&#23475;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#22312;&#24320;&#21457;SafeDecoding&#26102;&#30340;&#27934;&#23519;&#21147;&#22522;&#20110;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#20195;&#34920;&#26377;&#23475;&#20869;&#23481;&#30340;&#26631;&#35760;&#30340;&#27010;&#29575;&#36229;&#36807;&#20195;&#34920;&#26080;&#23475;&#21709;&#24212;&#30340;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#23433;&#20840;&#20813;&#36131;&#22768;&#26126;&#20173;&#28982;&#20986;&#29616;&#22312;&#25353;&#27010;&#29575;&#38477;&#24207;&#25490;&#24207;&#30340;&#26631;&#35760;&#20013;&#30340;&#21069;&#20960;&#20010;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#35782;&#21035;&#23433;&#20840;&#20813;&#36131;&#22768;&#26126;&#24182;&#22686;&#24378;&#20854;&#33391;&#24615;&#24433;&#21709;&#21147;&#26469;&#20943;&#36731;&#36234;&#29425;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08983v1 Announce Type: cross Abstract: As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, aiming to provoke unintended and unsafe behaviors from LLMs, remain a significant/leading LLM safety threat. In this paper, we aim to defend LLMs against jailbreak attacks by introducing SafeDecoding, a safety-aware decoding strategy for LLMs to generate helpful and harmless responses to user queries. Our insight in developing SafeDecoding is based on the observation that, even though probabilities of tokens representing harmful contents outweigh those representing harmless responses, safety disclaimers still appear among the top tokens after sorting tokens by probability in descending order. This allows us to mitigate jailbreak attacks by identifying safety disclaimers and amplifying their
&lt;/p&gt;</description></item><item><title>&#40065;&#26834;&#32467;&#26500;&#39044;&#27979;&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#25512;&#29702;&#26041;&#27861;&#30340;&#28151;&#21512;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#32467;&#26500;&#21270;&#36755;&#20986;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#27809;&#26377;&#25968;&#25454;&#38598;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#26684;&#24335;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.08971</link><description>&lt;p&gt;
&#40065;&#26834;&#32467;&#26500;&#39044;&#27979;&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Structured Language Generation Model for Robust Structure Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08971
&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#32467;&#26500;&#39044;&#27979;&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#25512;&#29702;&#26041;&#27861;&#30340;&#28151;&#21512;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#32467;&#26500;&#21270;&#36755;&#20986;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#27809;&#26377;&#25968;&#25454;&#38598;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#24037;&#20316;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#26684;&#24335;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65288;SLGM&#65289;&#65292;&#36890;&#36807;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#25512;&#29702;&#26041;&#27861;&#30340;&#28151;&#21512;&#26469;&#25913;&#21892;&#32467;&#26500;&#21270;&#36755;&#20986;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20197;&#24448;&#30340;&#32467;&#26500;&#39044;&#27979;&#30740;&#31350;&#65288;&#22914;NER&#65292;RE&#65289;&#21033;&#29992;&#20102;&#26174;&#24335;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#65292;&#36825;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#21487;&#33021;&#20250;&#23545;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#40065;&#26834;&#27867;&#21270;&#24615;&#20135;&#29983;&#25361;&#25112;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#38388;&#25509;&#22320;&#25552;&#20379;&#20102;&#26377;&#20851;&#25968;&#25454;&#30340;&#36890;&#29992;&#26684;&#24335;&#20449;&#24687;&#12290;&#21033;&#29992;&#26684;&#24335;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25439;&#22833;&#26657;&#20934;&#21644;&#26684;&#24335;&#21270;&#35299;&#30721;&#23558;&#24207;&#21015;&#21040;&#24207;&#21015;&#38382;&#39064;&#31616;&#21270;&#20026;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SLGM&#22312;&#27809;&#26377;&#25968;&#25454;&#38598;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#20445;&#25345;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#36739;&#23569;&#30340;&#26684;&#24335;&#38169;&#35823;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#20687;&#36866;&#37197;&#22120;&#19968;&#26679;&#22312;&#21508;&#20010;&#25968;&#25454;&#38598;&#19978;&#24037;&#20316;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08971v1 Announce Type: new Abstract: We propose Structured Language Generation Model (SLGM), a mixture of new loss function and inference method for better generalization of structured outputs. Previous studies on structure prediction (e.g. NER, RE) make use of explicit dataset information, which would boost performance, yet it might pose challenges to robust generalization in real-world situations. Instead, our model gives generalized format information about data indirectly. With format information, we could reduce sequence-to-sequence problem into classification problem via loss calibration and formatted decoding. Our experimental results showed SLGM successfully maintain performance without dataset information, and showed much less format errors. We also showed our model can work like adapters on individual dataset, with no additional training.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#31181;&#26032;&#30340;postEdit&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38544;&#31169;&#21644;&#39118;&#26684;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08631</link><description>&lt;p&gt;
&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Knowledge Editing on Black-box Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08631
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#40657;&#30418;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#30693;&#35782;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#31181;&#26032;&#30340;postEdit&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38544;&#31169;&#21644;&#39118;&#26684;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#32534;&#36753;&#26088;&#22312;&#39640;&#25928;&#12289;&#31934;&#30830;&#22320;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#26356;&#26032;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#32780;&#19981;&#23545;&#20854;&#20182;&#30693;&#35782;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#30333;&#30418;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#19978;&#65292;&#24573;&#35270;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#22330;&#26223;&#65306;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#65292;&#21363;&#36890;&#36807;&#25509;&#21475;&#35775;&#38382;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20165;&#21487;&#29992;&#25991;&#26412;&#36755;&#20986;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#22312;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#32534;&#36753;&#19978;&#19981;&#36866;&#29992;&#19988;&#32570;&#20047;&#20840;&#38754;&#24615;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#35780;&#20272;&#26694;&#26550;&#65292;&#39318;&#27425;&#23558;&#39118;&#26684;&#20445;&#30041;&#30340;&#35780;&#20272;&#32435;&#20837;&#20854;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#24403;&#21069;&#26041;&#27861;&#20013;&#30340;&#32534;&#36753;&#25968;&#25454;&#38544;&#31169;&#27844;&#28431;&#21644;&#39118;&#26684;&#36807;&#24230;&#32534;&#36753;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;postEdit&#26694;&#26550;&#65292;&#36890;&#36807;&#19979;&#28216;&#21518;&#22788;&#29702;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#21407;&#22987;&#22238;&#31572;&#36827;&#34892;&#32454;&#31890;&#24230;&#32534;&#36753;&#26469;&#20445;&#25345;&#25991;&#26412;&#39118;&#26684;&#19968;&#33268;&#24615;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#19982;&#20998;&#26512;&#34920;&#26126;&#65292;postEdit&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge editing (KE) aims to efficiently and precisely modify the behavior of large language models (LLMs) to update specific knowledge without negatively influencing other knowledge. Current research primarily focuses on white-box LLMs editing, overlooking an important scenario: black-box LLMs editing, where LLMs are accessed through interfaces and only textual output is available. To address the limitations of existing evaluations that are not inapplicable to black-box LLM editing and lack comprehensiveness, we propose a multi-perspective evaluation framework, incorporating the assessment of style retention for the first time. To tackle privacy leaks of editing data and style over-editing in current methods, we introduce a novel postEdit framework, resolving privacy concerns through downstream post-processing and maintaining textual style consistency via fine-grained editing to original responses. Experiments and analysis on two benchmarks demonstrate that postEdit outperforms all 
&lt;/p&gt;</description></item><item><title>ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08303</link><description>&lt;p&gt;
ChatCell: &#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
ChatCell: Facilitating Single-Cell Analysis with Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08303
&lt;/p&gt;
&lt;p&gt;
ChatCell&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#20419;&#36827;&#21333;&#32454;&#32990;&#20998;&#26512;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#20855;&#22791;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23427;&#20204;&#22312;&#31185;&#23398;&#20013;&#30340;&#24433;&#21709;&#26085;&#30410;&#31361;&#20986;&#12290;LLMs&#22312;&#20219;&#21153;&#27867;&#21270;&#21644;&#33258;&#30001;&#23545;&#35805;&#26041;&#38754;&#30340;&#26032;&#20852;&#33021;&#21147;&#21487;&#20197;&#26497;&#22823;&#22320;&#25512;&#36827;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#36825;&#20010;&#26500;&#25104;&#29983;&#29289;&#20307;&#22522;&#30784;&#26500;&#20214;&#30340;&#39046;&#22495;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#24403;&#21069;&#26041;&#27861;&#22312;&#30693;&#35782;&#38376;&#27099;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;LLMs&#22312;&#25484;&#25569;&#21333;&#32454;&#32990;&#25968;&#25454;&#26041;&#38754;&#30340;&#20805;&#20998;&#21033;&#29992;&#65292;&#24433;&#21709;&#20102;&#30452;&#25509;&#21487;&#35775;&#38382;&#21644;&#24555;&#36895;&#36845;&#20195;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ChatCell&#65292;&#36890;&#36807;&#21033;&#29992;&#35789;&#27719;&#36866;&#24212;&#21644;&#32479;&#19968;&#24207;&#21015;&#29983;&#25104;&#65292;&#23427;&#22312;&#21333;&#32454;&#32990;&#29983;&#29289;&#23398;&#39046;&#22495;&#33719;&#24471;&#20102;&#28145;&#21402;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#36866;&#24212;&#21508;&#31181;&#20998;&#26512;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) rapidly evolve, their influence in science is becoming increasingly prominent. The emerging capabilities of LLMs in task generalization and free-form dialogue can significantly advance fields like chemistry and biology. However, the field of single-cell biology, which forms the foundational building blocks of living organisms, still faces several challenges. High knowledge barriers and limited scalability in current methods restrict the full exploitation of LLMs in mastering single-cell data, impeding direct accessibility and rapid iteration. To this end, we introduce ChatCell, which signifies a paradigm shift by facilitating single-cell analysis with natural language. Leveraging vocabulary adaptation and unified sequence generation, ChatCell has acquired profound expertise in single-cell biology and the capability to accommodate a diverse range of analysis tasks. Extensive experiments further demonstrate ChatCell's robust performance and potential to de
&lt;/p&gt;</description></item><item><title>UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07939</link><description>&lt;p&gt;
UFO: &#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#20132;&#20114;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
UFO: A UI-Focused Agent for Windows OS Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07939
&lt;/p&gt;
&lt;p&gt;
UFO&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;&#23427;&#36890;&#36807;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#21644;&#25511;&#21046;&#20449;&#24687;&#65292;&#23454;&#29616;&#26080;&#32541;&#23548;&#33322;&#21644;&#25805;&#20316;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#35831;&#27714;&#12290;UFO&#30340;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#20351;&#24471;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#21363;&#21487;&#23454;&#29616;&#21160;&#20316;&#36830;&#25509;&#21644;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#65292;&#20351;&#32321;&#29712;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#21464;&#20026;&#31616;&#21333;&#20219;&#21153;&#12290;&#32463;&#36807;&#27979;&#35797;&#65292;UFO&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;UFO&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#19987;&#27880;&#20110;Windows&#25805;&#20316;&#31995;&#32479;&#19978;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#30028;&#38754;&#26234;&#33021;&#20307;&#65292;&#21033;&#29992;&#20102;GPT-Vision&#30340;&#33021;&#21147;&#26469;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;UFO&#37319;&#29992;&#21452;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#31934;&#30830;&#35266;&#23519;&#21644;&#20998;&#26512;Windows&#24212;&#29992;&#31243;&#24207;&#30340;&#22270;&#24418;&#29992;&#25143;&#30028;&#38754;&#65288;GUI&#65289;&#21644;&#25511;&#21046;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;&#26234;&#33021;&#20307;&#21487;&#20197;&#26080;&#32541;&#22320;&#22312;&#21333;&#20010;&#24212;&#29992;&#31243;&#24207;&#20869;&#20197;&#21450;&#36328;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#23548;&#33322;&#21644;&#25805;&#20316;&#65292;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#21363;&#20351;&#28041;&#21450;&#22810;&#20010;&#24212;&#29992;&#31243;&#24207;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#25511;&#21046;&#20132;&#20114;&#27169;&#22359;&#65292;&#23454;&#29616;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#30340;&#21160;&#20316;&#36830;&#25509;&#65292;&#24182;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#25191;&#34892;&#12290;&#22240;&#27492;&#65292;UFO&#23558;&#33392;&#24040;&#32780;&#32791;&#26102;&#30340;&#36807;&#31243;&#36716;&#21464;&#20026;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#23601;&#21487;&#20197;&#23436;&#25104;&#30340;&#31616;&#21333;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;9&#20010;&#27969;&#34892;&#30340;Windows&#24212;&#29992;&#31243;&#24207;&#19978;&#23545;UFO&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#21453;&#26144;&#29992;&#25143;&#26085;&#24120;&#20351;&#29992;&#24773;&#26223;&#30340;&#21508;&#31181;&#24773;&#20917;&#12290;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#21644;&#30495;&#23454;&#26696;&#20363;&#30740;&#31350;&#24471;&#20986;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;UFO&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce UFO, an innovative UI-Focused agent to fulfill user requests tailored to applications on Windows OS, harnessing the capabilities of GPT-Vision. UFO employs a dual-agent framework to meticulously observe and analyze the graphical user interface (GUI) and control information of Windows applications. This enables the agent to seamlessly navigate and operate within individual applications and across them to fulfill user requests, even when spanning multiple applications. The framework incorporates a control interaction module, facilitating action grounding without human intervention and enabling fully automated execution. Consequently, UFO transforms arduous and time-consuming processes into simple tasks achievable solely through natural language commands. We conducted testing of UFO across 9 popular Windows applications, encompassing a variety of scenarios reflective of users' daily usage. The results, derived from both quantitative metrics and real-case studies, underscore t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#22810;&#32423;&#22686;&#24378;&#23545;&#35805;&#65292;&#25429;&#25417;&#22810;&#26679;&#30340;&#23545;&#35805;&#29615;&#22659;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#35748;&#30693;&#24863;&#30693;&#36807;&#31243;&#20943;&#23569;&#38169;&#35823;&#29983;&#25104;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#38590;&#24230;&#33258;&#36866;&#24212;&#26679;&#26412;&#31579;&#36873;&#22120;&#36873;&#25321;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.07092</link><description>&lt;p&gt;
&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#30340;&#26041;&#27861;&#26469;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#29983;&#25104;&#22810;&#32423;&#22686;&#24378;&#23545;&#35805;&#65292;&#25429;&#25417;&#22810;&#26679;&#30340;&#23545;&#35805;&#29615;&#22659;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#35748;&#30693;&#24863;&#30693;&#36807;&#31243;&#20943;&#23569;&#38169;&#35823;&#29983;&#25104;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#38590;&#24230;&#33258;&#36866;&#24212;&#26679;&#26412;&#31579;&#36873;&#22120;&#36873;&#25321;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#25628;&#32034;&#21033;&#29992;&#22810;&#36718;&#33258;&#28982;&#35821;&#35328;&#29615;&#22659;&#26469;&#26816;&#32034;&#30456;&#20851;&#27573;&#33853;&#12290;&#29616;&#26377;&#30340;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#22823;&#22810;&#23558;&#23545;&#35805;&#35270;&#20026;&#19968;&#31995;&#21015;&#22266;&#23450;&#30340;&#38382;&#39064;&#21644;&#22238;&#31572;&#65292;&#24573;&#35270;&#20102;&#20005;&#37325;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064; - &#20063;&#23601;&#26159;&#35828;&#65292;&#29992;&#25143;&#21487;&#20197;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#23545;&#35805;&#65292;&#32780;&#36825;&#20123;&#22791;&#36873;&#23545;&#35805;&#26159;&#26410;&#35760;&#24405;&#30340;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#32463;&#24120;&#38590;&#20197;&#25512;&#24191;&#21040;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#22810;&#26679;&#23545;&#35805;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLM-&#35748;&#30693;&#25968;&#25454;&#22686;&#24378;&#24191;&#20041;&#23545;&#35805;&#23494;&#38598;&#26816;&#32034;&#30340;&#26694;&#26550;(ConvAug)&#12290;ConvAug&#39318;&#20808;&#29983;&#25104;&#22810;&#32423;&#22686;&#24378;&#23545;&#35805;&#65292;&#20197;&#25429;&#25417;&#23545;&#35805;&#29615;&#22659;&#30340;&#22810;&#26679;&#24615;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#26041;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#35748;&#30693;&#24863;&#30693;&#36807;&#31243;&#65292;&#20197;&#20943;&#23569;&#38169;&#35823;&#30340;&#27491;&#20363;&#12289;&#36127;&#20363;&#21644;&#24187;&#35273;&#30340;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#38590;&#24230;&#33258;&#36866;&#24212;&#26679;&#26412;&#31579;&#36873;&#22120;&#65292;&#29992;&#20110;&#36873;&#25321;&#22797;&#26434;&#23545;&#35805;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational search utilizes muli-turn natural language contexts to retrieve relevant passages. Existing conversational dense retrieval models mostly view a conversation as a fixed sequence of questions and responses, overlooking the severe data sparsity problem -- that is, users can perform a conversation in various ways, and these alternate conversations are unrecorded. Consequently, they often struggle to generalize to diverse conversations in real-world scenarios. In this work, we propose a framework for generalizing Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug). ConvAug first generates multi-level augmented conversations to capture the diverse nature of conversational contexts. Inspired by human cognition, we devise a cognition-aware process to mitigate the generation of false positives, false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive sample filter that selects challenging samples for complex conversations, thereby g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06529</link><description>&lt;p&gt;
&#20869;&#30465;&#35268;&#21010;&#65306;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#36890;&#36807;&#36866;&#24403;&#30340;&#22522;&#30784;&#22609;&#36896;&#26469;&#31574;&#30053;&#24615;&#22320;&#36827;&#34892;&#39640;&#32423;&#34892;&#21160;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;LLM&#20135;&#29983;&#30340;&#24187;&#35273;&#21487;&#33021;&#23548;&#33268;&#26426;&#22120;&#20154;&#33258;&#20449;&#22320;&#25191;&#34892;&#19982;&#29992;&#25143;&#30446;&#26631;&#19981;&#31526;&#25110;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#19981;&#23433;&#20840;&#30340;&#35745;&#21010;&#12290;&#27492;&#22806;&#65292;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#21487;&#33021;&#24341;&#21457;&#20219;&#21153;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#22810;&#20010;&#26377;&#25928;&#36873;&#39033;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;LLMs&#24517;&#39035;&#35782;&#21035;&#27492;&#31867;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#24341;&#23548;LLMs&#22312;&#26080;&#38656;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#24418;&#25104;&#24847;&#35782;&#21040;&#19981;&#30830;&#23450;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#25191;&#34892;&#35745;&#21010;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20219;&#21153;&#32423;&#26426;&#22120;&#20154;&#35268;&#21010;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#35777;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#26041;&#27861;&#30456;&#27604;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFTED&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2402.06512</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multimodal Clinical Trial Outcome Prediction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFTED&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#26159;&#19968;&#20010;&#20851;&#38190;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#36890;&#24120;&#38656;&#35201;&#22810;&#24180;&#26102;&#38388;&#21644;&#22823;&#37327;&#36130;&#21147;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#26088;&#22312;&#25490;&#38500;&#21487;&#33021;&#22833;&#36133;&#30340;&#33647;&#29289;&#65292;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#25104;&#26412;&#33410;&#32422;&#28508;&#21147;&#12290;&#26368;&#36817;&#30340;&#25968;&#25454;&#39537;&#21160;&#23581;&#35797;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#35774;&#35745;&#30340;&#27169;&#24577;&#29305;&#23450;&#32534;&#30721;&#22120;&#65292;&#36825;&#38480;&#21046;&#20102;&#36866;&#24212;&#26032;&#27169;&#24577;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#35782;&#21035;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30456;&#20284;&#20449;&#24687;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#65288;LIFTED&#65289;&#26041;&#27861;&#29992;&#20110;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LIFTED&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;LIFTED&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#65292;&#20174;&#27169;&#24577;&#29305;&#23450;&#30340;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The clinical trial is a pivotal and costly process, often spanning multiple years and requiring substantial financial resources. Therefore, the development of clinical trial outcome prediction models aims to exclude drugs likely to fail and holds the potential for significant cost savings. Recent data-driven attempts leverage deep learning methods to integrate multimodal data for predicting clinical trial outcomes. However, these approaches rely on manually designed modal-specific encoders, which limits both the extensibility to adapt new modalities and the ability to discern similar information patterns across different modalities. To address these issues, we propose a multimodal mixture-of-experts (LIFTED) approach for clinical trial outcome prediction. Specifically, LIFTED unifies different modality data by transforming them into natural language descriptions. Then, LIFTED constructs unified noise-resilient encoders to extract information from modal-specific language descriptions. S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#38190;&#20540;&#32422;&#26463;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#39537;&#36880;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26102;&#38388;&#27880;&#24847;&#21147;&#24471;&#20998;&#21644;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;RoCo&#31574;&#30053;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.06262</link><description>&lt;p&gt;
&#20851;&#20110;&#38024;&#23545;&#38190;&#20540;&#32422;&#26463;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#39537;&#36880;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#38190;&#20540;&#32422;&#26463;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#39537;&#36880;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26102;&#38388;&#27880;&#24847;&#21147;&#24471;&#20998;&#21644;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;RoCo&#31574;&#30053;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#23545;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#36807;&#24230;&#38656;&#27714;&#65292;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#37096;&#32626;&#20173;&#28982;&#26114;&#36149;&#12290;&#38500;&#20102;&#27169;&#22411;&#21442;&#25968;&#22806;&#65292;&#38190;&#20540;&#32531;&#23384;&#20063;&#23384;&#20648;&#22312;GPU&#20869;&#23384;&#20013;&#65292;&#38543;&#30528;&#25209;&#22788;&#29702;&#22823;&#23567;&#21644;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#32447;&#24615;&#22686;&#38271;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21508;&#31181;&#38024;&#23545;&#32473;&#23450;&#39044;&#31639;&#19979;&#32500;&#25252;&#38190;&#20540;&#32531;&#23384;&#24320;&#38144;&#30340;&#39537;&#36880;&#31574;&#30053;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#29616;&#26377;&#39537;&#36880;&#31574;&#30053;&#22312;&#37325;&#35201;&#24615;&#35780;&#20998;&#35745;&#31639;&#21644;&#39537;&#36880;&#33539;&#22260;&#26500;&#24314;&#20004;&#20010;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20808;&#21069;&#31574;&#30053;&#22312;&#36825;&#20004;&#20010;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#26102;&#38388;&#27880;&#24847;&#21147;&#24471;&#20998;&#21644;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;RoCo&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#32531;&#23384;&#39537;&#36880;&#31574;&#30053;&#12290;&#28085;&#30422;&#20102;&#39044;&#22635;&#20805;&#21644;&#33258;&#22238;&#24402;&#35299;&#30721;&#38454;&#27573;&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#20102;RoCo&#30340;&#20248;&#36234;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;RoCo&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#20379;&#30740;&#31350;&#32773;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent success associated with Large Language Models~(LLMs), they are notably cost-prohibitive to deploy in resource-constrained environments due to their excessive memory and computational demands. In addition to model parameters, the key-value cache is also stored in GPU memory, growing linearly with batch size and sequence length. As a remedy, recent works have proposed various eviction policies for maintaining the overhead of key-value cache under a given budget. This paper embarks on the efficacy of existing eviction policies in terms of \textit{importance score calculation} and \textit{eviction scope construction}. We identify the deficiency of prior policies in these two aspects and introduce RoCo, a \underline{r}\underline{o}bust \underline{c}ache \underline{o}mission policy based on temporal attention scores and robustness measures. Extensive experimentation spanning prefilling and auto-regressive decoding stages validates the superiority of RoCo. Finally, we relea
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#30340;&#32452;&#21512;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.06025</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#27010;&#29575;&#25512;&#29702;&#36827;&#34892;&#23454;&#39564;&#19982;&#20462;&#35746;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#30340;&#32452;&#21512;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#12290;&#35813;&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#65292;&#21363;&#20351;&#35268;&#21017;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#23398;&#20064;&#32773;&#20063;&#20250;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#27169;&#31946;&#27010;&#29575;&#35268;&#21017;&#65292;&#24182;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#26681;&#25454;&#36817;&#20284;&#36125;&#21494;&#26031;&#21407;&#21017;&#22312;&#27599;&#27425;&#23454;&#39564;&#21518;&#22312;&#32447;&#26356;&#26032;&#33258;&#24049;&#30340;&#20551;&#35774;&#12290;&#22312;&#21516;&#19968;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36824;&#26681;&#25454;&#20449;&#24687;&#35770;&#20934;&#21017;&#24314;&#31435;&#20102;&#23454;&#39564;&#35774;&#35745;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#19977;&#20010;&#21407;&#21017;&#30340;&#32452;&#21512;&#8212;&#8212;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#8212;&#8212;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#32780;&#21435;&#25481;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#32452;&#20214;&#37117;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#35299;&#37322;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We build a computational model of how humans actively infer hidden rules by doing experiments. The basic principles behind the model is that, even if the rule is deterministic, the learner considers a broader space of fuzzy probabilistic rules, which it represents in natural language, and updates its hypotheses online after each experiment according to approximately Bayesian principles. In the same framework we also model experiment design according to information-theoretic criteria. We find that the combination of these three principles -- explicit hypotheses, probabilistic rules, and online updates -- can explain human performance on a Zendo-style task, and that removing any of these components leaves the model unable to account for the data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.05119</link><description>&lt;p&gt;
&#30740;&#31350;&#25351;&#20196;&#35843;&#25972;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at the Limitations of Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21644;&#20998;&#26512;&#25581;&#31034;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#22810;&#20010;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#21644;&#25216;&#33021;&#12289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#12289;&#20840;&#21442;&#25968;&#24494;&#35843;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#65288;IT&#65289;&#26159;&#20351;&#29992;&#25351;&#20196;-&#22238;&#24212;&#23545;&#26469;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36807;&#31243;&#65292;&#24050;&#25104;&#20026;&#23558;&#22522;&#30784;&#39044;&#35757;&#32451;LLM&#36716;&#21270;&#20026;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20195;&#29702;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#34429;&#28982;IT&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#24182;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#20854;&#23616;&#38480;&#24615;&#21644;&#19981;&#36275;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#20005;&#26684;&#30340;&#23454;&#39564;&#21644;&#23545;LLM&#36890;&#36807;IT&#21457;&#29983;&#30340;&#21464;&#21270;&#30340;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;IT&#30340;&#22810;&#31181;&#23616;&#38480;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;&#65288;1&#65289;IT&#26080;&#27861;&#22686;&#24378;LLM&#30340;&#30693;&#35782;&#25110;&#25216;&#33021;&#12290;LoRA&#24494;&#35843;&#20165;&#38480;&#20110;&#23398;&#20064;&#22238;&#24212;&#30340;&#21551;&#21160;&#21644;&#26679;&#24335;&#20196;&#29260;&#65292;&#32780;&#20840;&#21442;&#25968;&#24494;&#35843;&#20250;&#23548;&#33268;&#30693;&#35782;&#36864;&#21270;&#12290;&#65288;2&#65289;&#20174;&#20855;&#26377;&#30693;&#35782;&#26469;&#28304;&#30340;IT&#25968;&#25454;&#38598;&#22797;&#21046;&#22238;&#24212;&#27169;&#24335;&#20250;&#23548;&#33268;&#22238;&#24212;&#36136;&#37327;&#19979;&#38477;&#12290;&#65288;3&#65289;&#20840;&#21442;&#25968;&#24494;&#35843;&#36890;&#36807;&#19981;&#20934;&#30830;&#22320;&#20174;IT&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#27010;&#24565;&#19978;&#30456;&#20284;&#23454;&#20363;&#30340;&#26631;&#35760;&#65292;&#22686;&#21152;&#20102;&#38169;&#35823;&#29983;&#25104;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating respon
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#38598;UltraLink&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#30693;&#35782;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#21270;&#29305;&#23450;&#30693;&#35782;&#19978;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#21457;&#29616;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#65292;&#20943;&#23569;&#20102;&#35821;&#35328;&#26080;&#20851;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.04588</link><description>&lt;p&gt;
UltraLink: &#19968;&#20010;&#24320;&#28304;&#30340;&#30693;&#35782;&#22686;&#24378;&#22810;&#35821;&#35328;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
UltraLink: An Open-Source Knowledge-Enhanced Multilingual Supervised Fine-tuning Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#38598;UltraLink&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#30693;&#35782;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#21270;&#29305;&#23450;&#30693;&#35782;&#19978;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#21457;&#29616;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#65292;&#20943;&#23569;&#20102;&#35821;&#35328;&#26080;&#20851;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#25991;&#19978;&#65292;&#23545;&#20110;&#22810;&#35821;&#35328;&#30417;&#30563;&#24494;&#35843;&#30340;&#30740;&#31350;&#36824;&#30456;&#23545;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;&#30417;&#30563;&#24494;&#35843;&#25968;&#25454;&#38598;&#12290;&#19982;&#20043;&#21069;&#31616;&#21333;&#32763;&#35793;&#33521;&#25991;&#25351;&#20196;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;LLMs&#30340;&#35821;&#35328;&#29305;&#23450;&#21644;&#35821;&#35328;&#26080;&#20851;&#33021;&#21147;&#12290;&#23545;&#20110;&#35821;&#35328;&#29305;&#23450;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#30693;&#35782;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#25552;&#21462;LLMs&#26356;&#22810;&#30340;&#25991;&#21270;&#29305;&#23450;&#30693;&#35782;&#65292;&#25552;&#39640;&#23427;&#20204;&#20026;&#19981;&#21516;&#22269;&#23478;&#29992;&#25143;&#26381;&#21153;&#30340;&#33021;&#21147;&#12290;&#23545;&#20110;&#35821;&#35328;&#26080;&#20851;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#29616;&#20195;LLMs&#23637;&#29616;&#20986;&#24456;&#24378;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#33021;&#21147;&#65292;&#22240;&#27492;&#22810;&#27425;&#23398;&#20064;&#30456;&#21516;&#20869;&#23481;&#30340;&#22810;&#31181;&#35821;&#35328;&#24182;&#19981;&#24517;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#35821;&#35328;&#26080;&#20851;SFT&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-source large language models (LLMs) have gained significant strength across diverse fields. Nevertheless, the majority of studies primarily concentrate on English, with only limited exploration into the realm of multilingual supervised fine-tuning. In this work, we therefore construct an open-source multilingual supervised fine-tuning dataset. Different from previous works that simply translate English instructions, we consider both the language-specific and language-agnostic abilities of LLMs. For language-specific abilities, we introduce a knowledge-grounded data augmentation approach to elicit more culture-specific knowledge of LLMs, improving their ability to serve users from different countries. For language-agnostic abilities, we find through experiments that modern LLMs exhibit strong cross-lingual transfer capabilities, thus repeatedly learning identical content in various languages is not necessary. Consequently, we can substantially prune the language-agnostic SFT data w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20026;&#22270;&#25512;&#29702;&#20219;&#21153;&#24341;&#20837;&#20102;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GITQA&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#32467;&#26524;&#27604;&#21333;&#19968;&#27169;&#24577;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.02130</link><description>&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20026;&#22270;&#25512;&#29702;&#28210;&#26579;&#22270;&#24418;
&lt;/p&gt;
&lt;p&gt;
Rendering Graphs for Graph Reasoning in Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20026;&#22270;&#25512;&#29702;&#20219;&#21153;&#24341;&#20837;&#20102;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GITQA&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#32467;&#26524;&#27604;&#21333;&#19968;&#27169;&#24577;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26426;&#22120;&#20154;&#35268;&#21010;&#12289;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#20219;&#21153;&#20013;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22270;&#32467;&#26500;&#65292;LLMs&#33021;&#22815;&#29702;&#35299;&#25991;&#26412;&#26684;&#24335;&#30340;&#22270;&#20449;&#24687;&#65292;&#20294;&#24573;&#35270;&#20102;&#20016;&#23500;&#30340;&#35270;&#35273;&#27169;&#24577;&#65292;&#32780;&#35270;&#35273;&#26159;&#20154;&#31867;&#29702;&#35299;&#32467;&#26500;&#20449;&#24687;&#21644;&#36827;&#34892;&#22270;&#25512;&#29702;&#30340;&#30452;&#35266;&#26041;&#24335;&#12290;&#23558;&#22270;&#32467;&#26500;&#34920;&#31034;&#20026;&#35270;&#35273;&#22270;&#20687;(&#21363;&#35270;&#35273;&#22270;)&#30340;&#28508;&#22312;&#30410;&#22788;&#21644;&#33021;&#21147;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#26412;&#25991;&#22312;&#22270;&#25512;&#29702;&#20219;&#21153;&#20013;&#39318;&#27425;&#24341;&#20837;&#35270;&#35273;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GITQA&#65292;&#20854;&#20013;&#27599;&#20010;&#26679;&#26412;&#26159;&#19968;&#20010;&#20803;&#32452;(&#22270;&#12289;&#22270;&#20687;&#12289;&#25991;&#26412;&#25551;&#36848;)&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;LLMs&#22312;GITQA&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#32467;&#26524;&#27604;&#21333;&#19968;&#27169;&#24577;&#25928;&#26524;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#22312;LLaVA-7B/13B&#27169;&#22411;&#30340;&#24494;&#35843;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly used for various tasks with graph structures, such as robotic planning, knowledge graph completion, and common-sense reasoning. Though LLMs can comprehend graph information in a textual format, they overlook the rich visual modality, which is an intuitive way for humans to comprehend structural information and conduct graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., visual graph) is still unexplored. In this paper, we take the first step in incorporating visual information into graph reasoning tasks and propose a new benchmark GITQA, where each sample is a tuple (graph, image, textual description). We conduct extensive experiments on the GITQA benchmark using state-of-the-art multimodal LLMs. Results on graph reasoning tasks show that combining textual and visual information together performs better than using one modality alone. Moreover, the LLaVA-7B/13B models finetuned on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MasonPerplexity&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#20107;&#20214;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;Transformer&#38598;&#25104;&#30340;&#26041;&#24335;&#65292;&#22312;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#21644;&#35782;&#21035;&#25991;&#26412;&#22270;&#20687;&#20013;&#30446;&#26631;&#30340;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25104;&#32489;&#65292;&#20998;&#21035;&#25490;&#21517;&#31532;&#19977;&#12290;</title><link>https://arxiv.org/abs/2402.01967</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#38598;&#25104;&#30340;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#20107;&#20214;&#26816;&#27979;&#30340;MasonPerplexity&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MasonPerplexity at Multimodal Hate Speech Event Detection 2024: Hate Speech and Target Detection Using Transformer Ensembles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MasonPerplexity&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#20107;&#20214;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;Transformer&#38598;&#25104;&#30340;&#26041;&#24335;&#65292;&#22312;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#21644;&#35782;&#21035;&#25991;&#26412;&#22270;&#20687;&#20013;&#30446;&#26631;&#30340;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25104;&#32489;&#65292;&#20998;&#21035;&#25490;&#21517;&#31532;&#19977;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32593;&#32476;&#31038;&#21306;&#20013;&#65292;&#33258;&#21160;&#35782;&#21035;&#35832;&#22914;&#20167;&#24680;&#35328;&#35770;&#20043;&#31867;&#30340;&#20882;&#29359;&#24615;&#35821;&#35328;&#23545;&#20110;&#32500;&#25252;&#35752;&#35770;&#30340;&#25991;&#26126;&#21313;&#20998;&#37325;&#35201;&#12290;&#22312;&#22810;&#27169;&#24577;&#20869;&#23481;&#20013;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#26159;&#19968;&#39033;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20882;&#29359;&#24615;&#26082;&#21487;&#20197;&#20307;&#29616;&#22312;&#25991;&#23383;&#19978;&#65292;&#20063;&#21487;&#20197;&#20307;&#29616;&#22312;&#22270;&#20687;&#19978;&#65292;&#25110;&#32773;&#20004;&#32773;&#21516;&#26102;&#23384;&#22312;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;EACL 2024&#30340;CASE 2024&#19978;&#20849;&#20139;&#20219;&#21153;&#8220;&#22810;&#27169;&#24577;&#20167;&#24680;&#35328;&#35770;&#20107;&#20214;&#26816;&#27979;&#8221;&#20013;&#30340;MasonPerplexity&#26041;&#27861;&#12290;&#35813;&#20219;&#21153;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#23376;&#20219;&#21153;A&#27880;&#37325;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#65292;&#23376;&#20219;&#21153;B&#27880;&#37325;&#35782;&#21035;&#25919;&#27835;&#20107;&#20214;&#20013;&#23884;&#20837;&#25991;&#26412;&#22270;&#20687;&#20013;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;XLM-roBERTa-large&#27169;&#22411;&#26469;&#22788;&#29702;&#23376;&#20219;&#21153;A&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#26041;&#27861;&#23558;XLM-roBERTa-base&#12289;BERTweet-large&#21644;BERT-base&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#22788;&#29702;&#23376;&#20219;&#21153;B&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23376;&#20219;&#21153;A&#20013;&#33719;&#24471;&#20102;0.8347&#30340;F1&#20998;&#25968;&#65292;&#22312;&#23376;&#20219;&#21153;B&#20013;&#33719;&#24471;&#20102;0.6741&#30340;F1&#20998;&#25968;&#65292;&#24182;&#22312;&#20004;&#20010;&#23376;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#19977;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automatic identification of offensive language such as hate speech is important to keep discussions civil in online communities. Identifying hate speech in multimodal content is a particularly challenging task because offensiveness can be manifested in either words or images or a juxtaposition of the two. This paper presents the MasonPerplexity submission for the Shared Task on Multimodal Hate Speech Event Detection at CASE 2024 at EACL 2024. The task is divided into two sub-tasks: sub-task A focuses on the identification of hate speech and sub-task B focuses on the identification of targets in text-embedded images during political events. We use an XLM-roBERTa-large model for sub-task A and an ensemble approach combining XLM-roBERTa-base, BERTweet-large, and BERT-base for sub-task B. Our approach obtained 0.8347 F1-score in sub-task A and 0.6741 F1-score in sub-task B ranking 3rd on both sub-tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;12&#20010;&#20020;&#24202;&#19987;&#19994;&#20013;&#25552;&#20379;&#23433;&#20840;&#30340;&#33647;&#29289;&#22788;&#26041;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#23450;&#21046;&#30340;&#22788;&#26041;&#38169;&#35823;&#35686;&#25253;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#35782;&#21035;&#33647;&#29289;&#38169;&#35823;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.01741</link><description>&lt;p&gt;
&#24320;&#21457;&#24182;&#27979;&#35797;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#33647;&#29289;&#23433;&#20840;&#30340;12&#31181;&#20020;&#24202;&#19987;&#19994;
&lt;/p&gt;
&lt;p&gt;
Development and Testing of a Novel Large Language Model-Based Clinical Decision Support Systems for Medication Safety in 12 Clinical Specialties
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01741
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;12&#20010;&#20020;&#24202;&#19987;&#19994;&#20013;&#25552;&#20379;&#23433;&#20840;&#30340;&#33647;&#29289;&#22788;&#26041;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#23450;&#21046;&#30340;&#22788;&#26041;&#38169;&#35823;&#35686;&#25253;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35780;&#20272;&#20102;&#20854;&#22312;&#35782;&#21035;&#33647;&#29289;&#38169;&#35823;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#20020;&#24202;&#21307;&#29983;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#35201;&#24615;&#65306;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;-&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;CDSS&#65289;&#65292;&#29992;&#20110;&#23433;&#20840;&#29992;&#33647;&#22788;&#26041;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25552;&#20379;&#19982;&#24739;&#32773;&#32972;&#26223;&#21644;&#26426;&#26500;&#25351;&#21335;&#30456;&#20851;&#30340;&#22788;&#26041;&#38169;&#35823;&#35686;&#25253;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#22522;&#20110;&#35268;&#21017;&#30340;CDSS&#30340;&#23616;&#38480;&#24615;&#12290;&#30446;&#26631;&#65306;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;LLM&#30340;CDSS&#22312;&#35782;&#21035;&#21508;&#31181;&#21307;&#23398;&#21644;&#22806;&#31185;&#30149;&#20363;&#20013;&#30340;&#33647;&#29289;&#38169;&#35823;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#19982;&#20154;&#24037;&#19987;&#23478;&#23567;&#32452;&#36827;&#34892;&#27604;&#36739;&#12290;&#23427;&#36824;&#30740;&#31350;&#20102;&#20020;&#24202;&#21307;&#29983;&#22312;&#19981;&#21516;CDSS&#38598;&#25104;&#26041;&#24335;&#65288;&#21021;&#32423;&#33647;&#24072;&#12289;&#20165;&#22522;&#20110;LLM&#30340;CDSS&#21644;&#20108;&#32773;&#30340;&#32452;&#21512;&#65289;&#20013;&#30340;&#20559;&#22909;&#12290;&#35774;&#35745;&#12289;&#35774;&#32622;&#21644;&#21442;&#19982;&#32773;&#65306;&#21033;&#29992;&#24102;&#26377;GPT-4.0&#30340;RAG&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#28041;&#21450;12&#20010;&#19987;&#19994;&#20013;23&#20010;&#20020;&#24202;&#26696;&#20363;&#30340;61&#20010;&#22788;&#26041;&#38169;&#35823;&#22330;&#26223;&#12290;&#19987;&#23478;&#23567;&#32452;&#20351;&#29992;PCNE&#20998;&#31867;&#21644;NCC MERP&#25351;&#25968;&#35780;&#20272;&#36825;&#20123;&#26696;&#20363;&#12290;&#19977;&#21517;&#21021;&#32423;&#33647;&#24072;&#29420;&#31435;&#23457;&#26680;&#27599;&#20010;&#22330;&#26223;&#65292;&#24182;&#25552;&#20986;&#22788;&#29702;&#24314;&#35758;&#12290;&#26681;&#25454;&#26816;&#26597;&#30340;&#38169;&#35823;&#21644;&#24314;&#35758;&#32534;&#21046;&#20102;&#21453;&#39304;&#25253;&#21578;&#12290; &#28982;&#21518;&#65292;&#19977;&#21517;&#21307;&#29983;&#29420;&#31435;&#23457;&#26680;&#36825;&#20123;&#25253;&#21578;&#65292;&#24182;&#25552;&#20986;&#23545;&#19979;&#19968;&#27493;&#22788;&#29702;&#30340;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Importance: We introduce a novel Retrieval Augmented Generation (RAG)-Large Language Model (LLM) as a Clinical Decision Support System (CDSS) for safe medication prescription. This model addresses the limitations of traditional rule-based CDSS by providing relevant prescribing error alerts tailored to patient context and institutional guidelines.   Objective: The study evaluates the efficacy of an LLM-based CDSS in identifying medication errors across various medical and surgical case vignettes, compared to a human expert panel. It also examines clinician preferences among different CDSS integration modalities: junior pharmacist, LLM-based CDSS alone, and a combination of both.   Design, Setting, and Participants: Utilizing a RAG model with GPT-4.0, the study involved 61 prescribing error scenarios within 23 clinical vignettes across 12 specialties. An expert panel assessed these cases using the PCNE classification and NCC MERP index. Three junior pharmacists independently reviewed eac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32416;&#27491;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;CRAG&#65289;&#26469;&#25913;&#21892;&#29983;&#25104;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#35774;&#35745;&#36731;&#37327;&#32423;&#26816;&#32034;&#35780;&#20272;&#22120;&#21644;&#21033;&#29992;&#22823;&#35268;&#27169;&#32593;&#32476;&#25628;&#32034;&#25193;&#23637;&#26816;&#32034;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.15884</link><description>&lt;p&gt;
&#32416;&#27491;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Corrective Retrieval Augmented Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15884
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32416;&#27491;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;CRAG&#65289;&#26469;&#25913;&#21892;&#29983;&#25104;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#35774;&#35745;&#36731;&#37327;&#32423;&#26816;&#32034;&#35780;&#20272;&#22120;&#21644;&#21033;&#29992;&#22823;&#35268;&#27169;&#32593;&#32476;&#25628;&#32034;&#25193;&#23637;&#26816;&#32034;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19981;&#21487;&#36991;&#20813;&#22320;&#20986;&#29616;&#24187;&#35273;&#65292;&#22240;&#20026;&#29983;&#25104;&#30340;&#25991;&#26412;&#20934;&#30830;&#24615;&#19981;&#33021;&#20165;&#36890;&#36807;&#23427;&#20204;&#23553;&#35013;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#26469;&#20445;&#35777;&#12290;&#23613;&#31649;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26159;&#23545;LLMs&#30340;&#21487;&#34892;&#34917;&#20805;&#65292;&#20294;&#23427;&#20005;&#37325;&#20381;&#36182;&#20110;&#26816;&#32034;&#25991;&#26723;&#30340;&#30456;&#20851;&#24615;&#65292;&#24341;&#21457;&#20102;&#22914;&#26524;&#26816;&#32034;&#20986;&#29616;&#38382;&#39064;&#27169;&#22411;&#23558;&#22914;&#20309;&#34892;&#20026;&#30340;&#25285;&#24551;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32416;&#27491;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;CRAG&#65289;&#26469;&#25552;&#39640;&#29983;&#25104;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#26816;&#32034;&#35780;&#20272;&#22120;&#65292;&#29992;&#20110;&#35780;&#20272;&#20026;&#26597;&#35810;&#26816;&#32034;&#30340;&#25991;&#26723;&#30340;&#25972;&#20307;&#36136;&#37327;&#65292;&#26681;&#25454;&#36820;&#22238;&#30340;&#32622;&#20449;&#24230;&#35302;&#21457;&#19981;&#21516;&#30340;&#30693;&#35782;&#26816;&#32034;&#25805;&#20316;&#12290;&#30001;&#20110;&#20174;&#38745;&#24577;&#21644;&#26377;&#38480;&#30340;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#21482;&#33021;&#36820;&#22238;&#27425;&#20248;&#25991;&#26723;&#65292;&#22240;&#27492;&#21033;&#29992;&#22823;&#35268;&#27169;&#32593;&#32476;&#25628;&#32034;&#20316;&#20026;&#25193;&#23637;&#26469;&#22686;&#24378;&#26816;&#32034;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36824;&#26377;&#19968;&#20010;&#20998;&#35299;-&#37325;&#32452;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15884v2 Announce Type: replace  Abstract: Large language models (LLMs) inevitably exhibit hallucinations since the accuracy of generated texts cannot be secured solely by the parametric knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a practicable complement to LLMs, it relies heavily on the relevance of retrieved documents, raising concerns about how the model behaves if retrieval goes wrong. To this end, we propose the Corrective Retrieval Augmented Generation (CRAG) to improve the robustness of generation. Specifically, a lightweight retrieval evaluator is designed to assess the overall quality of retrieved documents for a query, returning a confidence degree based on which different knowledge retrieval actions can be triggered. Since retrieval from static and limited corpora can only return sub-optimal documents, large-scale web searches are utilized as an extension for augmenting the retrieval results. Besides, a decompose-then-recompose alg
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22810;&#38754;&#26495;&#35270;&#35273;&#38382;&#31572;&#65288;MultipanelVQA&#65289;&#22522;&#20934;&#25361;&#25112;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#23545;&#29702;&#35299;&#22810;&#38754;&#26495;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;LVLMs&#22312;&#36825;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.15847</link><description>&lt;p&gt;
&#26494;&#39292;&#36824;&#26159;&#21513;&#23043;&#23043;&#65311;&#29992;&#22810;&#38754;&#26495;VQA&#25361;&#25112;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15847
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22810;&#38754;&#26495;&#35270;&#35273;&#38382;&#31572;&#65288;MultipanelVQA&#65289;&#22522;&#20934;&#25361;&#25112;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#23545;&#29702;&#35299;&#22810;&#38754;&#26495;&#22270;&#20687;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;LVLMs&#22312;&#36825;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#38754;&#26495;&#22270;&#20687;&#65292;&#36890;&#24120;&#22312;&#32593;&#39029;&#25130;&#22270;&#12289;&#28023;&#25253;&#31561;&#20013;&#30475;&#21040;&#65292;&#20805;&#26021;&#30528;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#12290;&#36825;&#20123;&#22270;&#20687;&#20197;&#22810;&#20010;&#23376;&#22270;&#20197;&#19981;&#21516;&#24067;&#23616;&#32452;&#25104;&#65292;&#26377;&#25928;&#22320;&#21521;&#20154;&#20204;&#20256;&#36798;&#20449;&#24687;&#12290;&#20026;&#20102;&#26500;&#24314;&#39640;&#32423;&#30340;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#22914;&#33021;&#29702;&#35299;&#22797;&#26434;&#22330;&#26223;&#24182;&#22312;&#32593;&#39029;&#20013;&#23548;&#33322;&#30340;&#20195;&#29702;&#31243;&#24207;&#65292;&#22810;&#38754;&#26495;&#35270;&#35273;&#25512;&#29702;&#30340;&#25216;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#23545;&#27169;&#22411;&#22312;&#36825;&#26041;&#38754;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#38754;&#26495;&#35270;&#35273;&#38382;&#31572;&#65288;MultipanelVQA&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6,600&#20010;&#38382;&#39064;&#12289;&#31572;&#26696;&#21644;&#22810;&#38754;&#26495;&#22270;&#20687;&#19977;&#20803;&#32452;&#65292;&#19987;&#38376;&#25361;&#25112;&#27169;&#22411;&#29702;&#35299;&#22810;&#38754;&#26495;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;MultipanelVQA&#22522;&#20934;&#20013;&#30340;&#38382;&#39064;&#23545;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#21363;&#20351;&#20154;&#31867;&#21487;&#20197;&#33719;&#24471;&#32422;99%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15847v2 Announce Type: replace-cross  Abstract: Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Large Vision Language Models (LVLMs) tested, even though humans can attain approximately 99\% accurac
&lt;/p&gt;</description></item><item><title>RecDCL&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#23545;&#27604;&#23398;&#20064;&#25512;&#33616;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#25209;&#27425;&#23545;&#27604;&#23398;&#20064;&#65288;BCL&#65289;&#21644;&#29305;&#24449;&#23545;&#27604;&#23398;&#20064;&#65288;FCL&#65289;&#65292;&#26377;&#21161;&#20110;&#28040;&#38500;&#20887;&#20313;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#21448;&#19981;&#20250;&#38169;&#36807;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2401.15635</link><description>&lt;p&gt;
RecDCL: &#29992;&#20110;&#25512;&#33616;&#30340;&#21452;&#37325;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RecDCL: Dual Contrastive Learning for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15635
&lt;/p&gt;
&lt;p&gt;
RecDCL&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#23545;&#27604;&#23398;&#20064;&#25512;&#33616;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#25209;&#27425;&#23545;&#27604;&#23398;&#20064;&#65288;BCL&#65289;&#21644;&#29305;&#24449;&#23545;&#27604;&#23398;&#20064;&#65288;FCL&#65289;&#65292;&#26377;&#21161;&#20110;&#28040;&#38500;&#20887;&#20313;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#21448;&#19981;&#20250;&#38169;&#36807;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#25366;&#25496;&#21327;&#21516;&#36807;&#28388;&#20013;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23545;&#27604;&#23398;&#20064;&#65288;CL&#65289;&#26159;&#19968;&#20010;&#37325;&#35201;&#33539;&#24335;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;&#27604;&#21407;&#22987;&#25968;&#25454;&#21644;&#22686;&#24378;&#25968;&#25454;&#20043;&#38388;&#30340;&#23884;&#20837;&#26469;&#35299;&#20915;&#32593;&#32476;&#24179;&#21488;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;CL&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25209;&#27425;&#26041;&#24335;&#23545;&#27604;&#19978;&#65292;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#29305;&#24449;&#32500;&#24230;&#20013;&#30340;&#28508;&#22312;&#35268;&#24459;&#65292;&#36825;&#23548;&#33268;&#22312;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#34920;&#31034;&#23398;&#20064;&#36807;&#31243;&#20013;&#20986;&#29616;&#20102;&#20887;&#20313;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#21516;&#26102;&#21033;&#29992;&#25209;&#27425;&#23545;&#27604;&#23398;&#20064;&#65288;BCL&#65289;&#21644;&#29305;&#24449;&#23545;&#27604;&#23398;&#20064;&#65288;FCL&#65289;&#36827;&#34892;&#25512;&#33616;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;BCL&#21644;FCL&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#32467;&#21512;BCL&#21644;FCL&#26377;&#21161;&#20110;&#28040;&#38500;&#20887;&#20313;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#27704;&#36828;&#19981;&#20250;&#38169;&#36807;&#26368;&#20248;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#23545;&#27604;&#23398;&#20064;&#25512;&#33616;&#26694;&#26550;-- RecDCL&#12290;&#22312;RecDCL&#20013;&#65292;FCL&#30446;&#26631;&#26088;&#22312;&#28040;&#38500; ...
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15635v2 Announce Type: replace-cross  Abstract: Self-supervised learning (SSL) has recently achieved great success in mining the user-item interactions for collaborative filtering. As a major paradigm, contrastive learning (CL) based SSL helps address data sparsity in Web platforms by contrasting the embeddings between raw and augmented data. However, existing CL-based methods mostly focus on contrasting in a batch-wise way, failing to exploit potential regularity in the feature dimension. This leads to redundant solutions during the representation learning of users and items. In this work, we investigate how to employ both batch-wise CL (BCL) and feature-wise CL (FCL) for recommendation. We theoretically analyze the relation between BCL and FCL, and find that combining BCL and FCL helps eliminate redundant solutions but never misses an optimal solution. We propose a dual contrastive learning recommendation framework -- RecDCL. In RecDCL, the FCL objective is designed to eli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#29305;&#23450;&#20107;&#23454;&#26680;&#26597;&#27169;&#22411;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27721;&#35821;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#24182;&#23637;&#31034;&#20854;&#20248;&#20110;&#32763;&#35793;&#26041;&#27861;&#21644;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#23545;&#20559;&#35265;&#26356;&#21152;&#31283;&#20581;&#65292;&#24378;&#35843;&#20102;&#35821;&#35328;&#29305;&#23450;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.15498</link><description>&lt;p&gt;
&#25105;&#20204;&#26159;&#21542;&#38656;&#35201;&#35821;&#35328;&#29305;&#23450;&#30340;&#20107;&#23454;&#26680;&#26597;&#27169;&#22411;&#65311;&#20197;&#27721;&#35821;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Do We Need Language-Specific Fact-Checking Models? The Case of Chinese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#29305;&#23450;&#20107;&#23454;&#26680;&#26597;&#27169;&#22411;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#27721;&#35821;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#24182;&#23637;&#31034;&#20854;&#20248;&#20110;&#32763;&#35793;&#26041;&#27861;&#21644;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#23545;&#20559;&#35265;&#26356;&#21152;&#31283;&#20581;&#65292;&#24378;&#35843;&#20102;&#35821;&#35328;&#29305;&#23450;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#29305;&#23450;&#20107;&#23454;&#26680;&#26597;&#27169;&#22411;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#37325;&#28857;&#20851;&#27880;&#27721;&#35821;&#26696;&#20363;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22522;&#20110;&#32763;&#35793;&#26041;&#27861;&#21644;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-4&#65289;&#30340;&#23616;&#38480;&#24615;&#65292;&#31361;&#20986;&#20102;&#23545;&#35821;&#35328;&#29305;&#23450;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#27721;&#35821;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#36890;&#36807;&#25972;&#21512;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20174;&#25991;&#26723;&#20013;&#26816;&#32034;&#35777;&#25454;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20998;&#26512;&#19981;&#21516;&#31995;&#32479;&#20013;&#30340;&#20196;&#29260;&#32423;&#20559;&#35265;&#65292;&#25105;&#20204;&#22522;&#20110;CHEF&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#19968;&#20010;&#23545;&#25239;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#23454;&#20363;&#19982;&#21407;&#22987;&#23454;&#20363;&#20855;&#26377;&#36739;&#22823;&#30340;&#35789;&#37325;&#21472;&#65292;&#20294;&#20855;&#26377;&#30456;&#21453;&#30340;&#30495;&#23454;&#24615;&#26631;&#31614;&#12290;&#22312;CHEF&#25968;&#25454;&#38598;&#21644;&#25105;&#20204;&#30340;&#23545;&#25239;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20110;&#32763;&#35793;&#30340;&#26041;&#27861;&#21644;&#22810;&#35821;&#35328;LLM&#65292;&#24182;&#19988;&#23545;&#20559;&#35265;&#26356;&#21152;&#31283;&#20581;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#24378;&#35843;&#20102;&#35821;&#35328;&#29305;&#23450;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15498v2 Announce Type: replace  Abstract: This paper investigates the potential benefits of language-specific fact-checking models, focusing on the case of Chinese. We first demonstrate the limitations of translation-based methods and multilingual large language models (e.g., GPT-4), highlighting the need for language-specific systems. We further propose a Chinese fact-checking system that can better retrieve evidence from a document by incorporating context information. To better analyze token-level biases in different systems, we construct an adversarial dataset based on the CHEF dataset, where each instance has large word overlap with the original one but holds the opposite veracity label. Experimental results on the CHEF dataset and our adversarial dataset show that our proposed method outperforms translation-based methods and multilingual LLMs and is more robust toward biases, while there is still large room for improvement, emphasizing the importance of language-specif
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#26426;&#22120;&#32763;&#35793;&#20013;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#28145;&#20837;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#31034;&#20363;&#39537;&#21160;&#30340;&#29305;&#28857;&#20197;&#21450;&#31034;&#20363;&#23545;&#19979;&#28216;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20063;&#25506;&#35752;&#20102;ICL&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.12097</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#20013;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of In-context Learning in LLMs for Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.12097
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#26426;&#22120;&#32763;&#35793;&#20013;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#20102;&#20840;&#38754;&#28145;&#20837;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#31034;&#20363;&#39537;&#21160;&#30340;&#29305;&#28857;&#20197;&#21450;&#31034;&#20363;&#23545;&#19979;&#28216;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#20063;&#25506;&#35752;&#20102;ICL&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#27987;&#21402;&#20852;&#36259;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20248;&#21270;&#32763;&#35793;&#36136;&#37327;&#19978;&#65292;&#23545;&#24433;&#21709;&#25152;&#36848;&#36136;&#37327;&#30340;ICL&#30340;&#29305;&#23450;&#26041;&#38754;&#20851;&#27880;&#26377;&#38480;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#30740;&#31350;&#19978;&#19979;&#25991;&#23398;&#20064;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;ICL&#20027;&#35201;&#26159;&#30001;&#31034;&#20363;&#39537;&#21160;&#32780;&#19981;&#26159;&#25351;&#20196;&#39537;&#21160;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#31034;&#20363;&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#24191;&#27867;&#25506;&#32034;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#23545;&#19979;&#28216;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21253;&#25324;&#31034;&#33539;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#12289;&#31354;&#38388;&#25509;&#36817;&#24615;&#20197;&#21450;&#28304;&#35821;&#35328;&#19982;&#30446;&#26631;&#35821;&#35328;&#30340;&#21407;&#21019;&#24615;&#31561;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#28041;&#21450;&#38388;&#25509;&#24615;&#21644;&#31034;&#20363;&#19981;&#21305;&#37197;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#65292;&#20197;&#20102;&#35299;ICL&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.12097v2 Announce Type: replace  Abstract: Recent interest has surged in employing Large Language Models (LLMs) for machine translation (MT) via in-context learning (ICL) (Vilar et al., 2023). Most prior studies primarily focus on optimizing translation quality, with limited attention to understanding the specific aspects of ICL that influence the said quality. To this end, we perform the first of its kind, exhaustive study of in-context learning for machine translation. We first establish that ICL is primarily example-driven and not instruction-driven. Following this, we conduct an extensive exploration of various aspects of the examples to understand their influence on downstream performance. Our analysis includes factors such as quality and quantity of demonstrations, spatial proximity, and source versus target originality. Further, we also investigate challenging scenarios involving indirectness and misalignment of examples to understand the limits of ICL. While we establ
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20197;&#25552;&#21319;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20559;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#23427;&#20204;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2401.11911</link><description>&lt;p&gt;
&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#19978;&#19979;&#25991;&#20197;&#22686;&#24378;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts for Open-Domain QA?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11911
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20197;&#25552;&#21319;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20559;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#23427;&#20204;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36741;&#21161;&#20449;&#24687;&#24050;&#32463;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20851;&#38190;&#65292;&#20294;&#23545;&#20110;LLMs&#22914;&#20309;&#21512;&#24182;&#29983;&#25104;&#30340;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#20173;&#30693;&#20043;&#29978;&#23569;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#30830;&#23450;LLMs&#30340;&#21709;&#24212;&#26159;&#28304;&#33258;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#36824;&#26159;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21253;&#21547;&#30456;&#20114;&#20914;&#31361;&#30340;&#19978;&#19979;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#38382;&#39064;&#37117;&#19982;&#29983;&#25104;&#30340;&#21644;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#37197;&#23545;&#65292;&#20294;&#21482;&#26377;&#19968;&#20010;&#19978;&#19979;&#25991;&#21253;&#21547;&#20102;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#65288;&#22914;GPT-4/3.5&#21644;Llama2&#65289;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#24046;&#65292;&#26356;&#20542;&#21521;&#20110;&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#65292;&#21363;&#20351;&#36825;&#20123;&#19978;&#19979;&#25991;&#25552;&#20379;&#20102;&#38169;&#35823;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#23548;&#33268;&#36825;&#31181;&#20559;&#24046;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;i&#65289;LLMs&#29983;&#25104;&#30340;&#19978;&#19979;&#25991;&#36890;&#24120;&#19982;&#38382;&#39064;&#26356;&#30456;&#20284;&#65292;&#22686;&#21152;&#20102;&#20854;&#34987;&#36873;&#25321;&#30340;&#21487;&#33021;&#24615;&#65307;ii&#65289;&#26816;&#32034;&#19978;&#19979;&#25991;&#20013;&#20351;&#29992;&#30340;&#20998;&#21106;&#36807;&#31243;&#25171;&#26029;&#20102;&#20854;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While auxiliary information has become a key to enhance Large Language Models (LLMs), relatively little is known about how LLMs merge these contexts, specifically generated and retrieved. To study this, we formulate a systematic framework to identify whether LLMs' responses, derived from the integration of generated and retrieved contexts, are attributed to either generated or retrieved contexts. To achieve this, we construct datasets with conflicting contexts, where each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer. Our experiments reveal a significant bias in LLMs (GPT-4/3.5 and Llama2) towards generated contexts, even when they provide incorrect information. We further identify two key factors contributing to this bias: i) contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of selection; ii) the segmentation process used in retrieved contexts disrupts their compl
&lt;/p&gt;</description></item><item><title>PsySafe&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#26234;&#33021;&#20307;&#24515;&#29702;&#23398;&#65292;&#25581;&#31034;&#26234;&#33021;&#20307;&#30340;&#40657;&#26263;&#24515;&#29702;&#29366;&#24577;&#23545;&#23433;&#20840;&#26500;&#25104;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#39118;&#38505;&#32531;&#35299;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2401.11880</link><description>&lt;p&gt;
PsySafe&#65306;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#23433;&#20840;&#25915;&#20987;&#12289;&#38450;&#24481;&#21644;&#35780;&#20272;&#30340;&#32508;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PsySafe: A Comprehensive Framework for Psychological-based Attack, Defense, and Evaluation of Multi-agent System Safety
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11880
&lt;/p&gt;
&lt;p&gt;
PsySafe&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#26234;&#33021;&#20307;&#24515;&#29702;&#23398;&#65292;&#25581;&#31034;&#26234;&#33021;&#20307;&#30340;&#40657;&#26263;&#24515;&#29702;&#29366;&#24577;&#23545;&#23433;&#20840;&#26500;&#25104;&#23041;&#32961;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#39118;&#38505;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#22312;&#21152;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21518;&#65292;&#23637;&#29616;&#20986;&#20102;&#38598;&#20307;&#26234;&#33021;&#30340;&#28145;&#36828;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26234;&#33021;&#34987;&#24694;&#24847;&#20351;&#29992;&#21487;&#33021;&#24102;&#26469;&#37325;&#22823;&#39118;&#38505;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#20851;&#20110;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#23433;&#20840;&#38382;&#39064;&#30340;&#20840;&#38754;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#25991;&#36890;&#36807;&#21019;&#26032;&#30340;&#35270;&#35282;&#25506;&#32034;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#21457;&#29616;&#26234;&#33021;&#20307;&#30340;&#40657;&#26263;&#24515;&#29702;&#29366;&#24577;&#26500;&#25104;&#20102;&#23545;&#23433;&#20840;&#30340;&#37325;&#22823;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#26234;&#33021;&#20307;&#24515;&#29702;&#23398;&#20026;&#22522;&#30784;&#30340;&#32508;&#21512;&#26694;&#26550;&#65288;PsySafe&#65289;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#39046;&#22495;&#65306;&#39318;&#20808;&#65292;&#35782;&#21035;&#26234;&#33021;&#20307;&#20013;&#30340;&#40657;&#26263;&#20154;&#26684;&#29305;&#24449;&#22914;&#20309;&#23548;&#33268;&#39118;&#38505;&#34892;&#20026;&#65307;&#20854;&#27425;&#65292;&#20174;&#24515;&#29702;&#21644;&#34892;&#20026;&#35282;&#24230;&#35780;&#20272;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65307;&#31532;&#19977;&#65292;&#21046;&#23450;&#26377;&#25928;&#30340;&#31574;&#30053;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11880v2 Announce Type: replace-cross  Abstract: Multi-agent systems, when enhanced with Large Language Models (LLMs), exhibit profound capabilities in collective intelligence. However, the potential misuse of this intelligence for malicious purposes presents significant risks. To date, comprehensive research on the safety issues associated with multi-agent systems remains limited. In this paper, we explore these concerns through the innovative lens of agent psychology, revealing that the dark psychological states of agents constitute a significant threat to safety. To tackle these concerns, we propose a comprehensive framework (PsySafe) grounded in agent psychology, focusing on three key areas: firstly, identifying how dark personality traits in agents can lead to risky behaviors; secondly, evaluating the safety of multi-agent systems from the psychological and behavioral perspectives, and thirdly, devising effective strategies to mitigate these risks. Our experiments reveal
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#25968;&#23398;&#21644;&#32534;&#30721;&#38382;&#39064;&#30340;&#25200;&#21160;&#26412;&#20307;&#20197;&#21450;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#35780;&#20272;&#20102;LLMs&#22312;&#25968;&#23383;&#25512;&#29702;&#21644;&#32534;&#30721;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#22312;&#20840;&#38754;&#35780;&#20272;&#20013;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#22312;&#25200;&#21160;&#38382;&#39064;&#19978;&#34920;&#29616;&#26174;&#33879;&#19979;&#38477;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;LLMs&#32570;&#20047;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.09395</link><description>&lt;p&gt;
&#34987;&#29702;&#24615;&#30340;&#27969;&#27801;&#25152;&#22256;&#65292;&#36828;&#31163;AGI&#23792;&#20250;&#65306;&#36890;&#36807;&#26412;&#20307;&#24341;&#23548;&#24178;&#39044;&#35780;&#20272;LLMs&#30340;&#25968;&#23398;&#21644;&#32534;&#30721;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Caught in the Quicksand of Reasoning, Far from AGI Summit: Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.09395
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#25968;&#23398;&#21644;&#32534;&#30721;&#38382;&#39064;&#30340;&#25200;&#21160;&#26412;&#20307;&#20197;&#21450;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#35780;&#20272;&#20102;LLMs&#22312;&#25968;&#23383;&#25512;&#29702;&#21644;&#32534;&#30721;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#22312;&#20840;&#38754;&#35780;&#20272;&#20013;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#22312;&#25200;&#21160;&#38382;&#39064;&#19978;&#34920;&#29616;&#26174;&#33879;&#19979;&#38477;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;LLMs&#32570;&#20047;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20808;&#36827;&#21457;&#23637;&#23637;&#31034;&#20102;&#22312;&#29616;&#26377;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#25104;&#26524;&#65292;&#20854;&#20013;&#19968;&#20123;&#27169;&#22411;&#29978;&#33267;&#36229;&#36807;&#20102;&#20154;&#31867;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#21644;&#31283;&#20581;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20851;&#27880;&#20004;&#20010;&#27969;&#34892;&#30340;&#25512;&#29702;&#20219;&#21153;&#65306;&#31639;&#26415;&#25512;&#29702;&#21644;&#20195;&#30721;&#29983;&#25104;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#65306;&#65288;i&#65289;&#25968;&#23398;&#21644;&#32534;&#30721;&#38382;&#39064;&#30340;&#36890;&#29992;&#25200;&#21160;&#26412;&#20307;&#65292;&#65288;ii&#65289;&#19968;&#31181;&#21322;&#33258;&#21160;&#26041;&#27861;&#26469;&#24212;&#29992;&#36825;&#20123;&#25200;&#21160;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#20004;&#20010;&#25968;&#25454;&#38598;MORE&#21644;CORE&#65292;&#20998;&#21035;&#29992;&#20110;&#25200;&#21160;&#25968;&#23398;&#21644;&#32534;&#30721;&#38382;&#39064;&#65292;&#20197;&#25506;&#31350;LLM&#22312;&#25968;&#23383;&#25512;&#29702;&#21644;&#32534;&#30721;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#26497;&#38480;&#12290;&#36890;&#36807;&#23545;&#23553;&#38381;&#28304;&#21644;&#24320;&#28304;LLMs&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#26377;&#27169;&#22411;&#23545;&#25200;&#21160;&#38382;&#39064;&#30340;&#26174;&#33879;&#24615;&#33021;&#19979;&#38477;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;LLMs&#32570;&#20047;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.09395v2 Announce Type: replace  Abstract: Recent advancements in Large Language Models (LLMs) have showcased striking results on existing logical reasoning benchmarks, with some models even surpassing human performance. However, the true depth of their competencies and robustness in reasoning tasks remains an open question. To this end, in this paper, we focus on two popular reasoning tasks: arithmetic reasoning and code generation. Particularly, we introduce: (i) a general ontology of perturbations for maths and coding questions, (ii) a semi-automatic method to apply these perturbations, and (iii) two datasets, MORE and CORE, respectively, of perturbed maths and coding problems to probe the limits of LLM capabilities in numeric reasoning and coding tasks. Through comprehensive evaluations of both closed-source and open-source LLMs, we show a significant performance drop across all the models against the perturbed questions, suggesting that the current LLMs lack robust probl
&lt;/p&gt;</description></item><item><title>DoraemonGPT&#26159;&#19968;&#20010;&#30001;LLMs&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#22788;&#29702;&#21160;&#24577;&#35270;&#39057;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#31526;&#21495;&#35760;&#24518;&#26469;&#36827;&#34892;&#31354;&#38388;-&#26102;&#38388;&#26597;&#35810;&#21644;&#25512;&#29702;&#65292;&#24182;&#21462;&#24471;&#31616;&#27905;&#30340;&#20013;&#38388;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.08392</link><description>&lt;p&gt;
DoraemonGPT&#65306;&#26397;&#21521;&#29702;&#35299;&#20855;&#26377;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#22330;&#26223;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08392
&lt;/p&gt;
&lt;p&gt;
DoraemonGPT&#26159;&#19968;&#20010;&#30001;LLMs&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#22788;&#29702;&#21160;&#24577;&#35270;&#39057;&#20219;&#21153;&#65292;&#36890;&#36807;&#23558;&#35270;&#39057;&#36716;&#25442;&#20026;&#31526;&#21495;&#35760;&#24518;&#26469;&#36827;&#34892;&#31354;&#38388;-&#26102;&#38388;&#26597;&#35810;&#21644;&#25512;&#29702;&#65292;&#24182;&#21462;&#24471;&#31616;&#27905;&#30340;&#20013;&#38388;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30001;LLM&#39537;&#21160;&#30340;&#35270;&#35273;&#20195;&#29702;&#20027;&#35201;&#38598;&#20013;&#20110;&#35299;&#20915;&#22522;&#20110;&#22270;&#20687;&#30340;&#20219;&#21153;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#29702;&#35299;&#21160;&#24577;&#22330;&#26223;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#36828;&#31163;&#20687;&#24341;&#23548;&#23398;&#29983;&#36827;&#34892;&#23454;&#39564;&#23460;&#23454;&#39564;&#21644;&#35782;&#21035;&#38169;&#35823;&#36825;&#26679;&#30340;&#30495;&#23454;&#24212;&#29992;&#12290;&#32771;&#34385;&#21040;&#35270;&#39057;&#27169;&#24577;&#26356;&#22909;&#22320;&#21453;&#26144;&#20102;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#19981;&#26029;&#21464;&#21270;&#24615;&#36136;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;DoraemonGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#32508;&#21512;&#27010;&#24565;&#31616;&#27905;&#31995;&#32479;&#65292;&#29992;&#20110;&#22788;&#29702;&#21160;&#24577;&#35270;&#39057;&#20219;&#21153;&#12290;&#32473;&#23450;&#19968;&#20010;&#24102;&#26377;&#38382;&#39064;/&#20219;&#21153;&#30340;&#35270;&#39057;&#65292;DoraemonGPT&#39318;&#20808;&#23558;&#36755;&#20837;&#35270;&#39057;&#36716;&#25442;&#20026;&#23384;&#20648;&#19982;&#20219;&#21153;&#30456;&#20851;&#23646;&#24615;&#30340;&#31526;&#21495;&#23384;&#20648;&#22120;&#12290;&#36825;&#31181;&#32467;&#26500;&#21270;&#34920;&#31034;&#20801;&#35768;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23376;&#20219;&#21153;&#24037;&#20855;&#36827;&#34892;&#31354;&#38388;-&#26102;&#38388;&#26597;&#35810;&#21644;&#25512;&#29702;&#65292;&#20174;&#32780;&#20135;&#29983;&#31616;&#27905;&#30340;&#20013;&#38388;&#32467;&#26524;&#12290;&#37492;&#20110;LLM&#22312;&#28041;&#21450;&#19987;&#19994;&#39046;&#22495;&#65288;&#20363;&#22914;&#20998;&#26512;&#23454;&#39564;&#20013;&#28508;&#22312;&#30340;&#31185;&#23398;&#21407;&#29702;&#65289;&#26102;&#20855;&#26377;&#26377;&#38480;&#30340;&#20869;&#37096;&#30693;&#35782;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08392v2 Announce Type: replace-cross  Abstract: Recent LLM-driven visual agents mainly focus on solving image-based tasks, which limits their ability to understand dynamic scenes, making it far from real-life applications like guiding students in laboratory experiments and identifying their mistakes. Considering the video modality better reflects the ever-changing nature of real-world scenarios, we devise DoraemonGPT, a comprehensive and conceptually elegant system driven by LLMs to handle dynamic video tasks. Given a video with a question/task, DoraemonGPT begins by converting the input video into a symbolic memory that stores task-related attributes. This structured representation allows for spatial-temporal querying and reasoning by well-designed sub-task tools, resulting in concise intermediate results. Recognizing that LLMs have limited internal knowledge when it comes to specialized domains (e.g., analyzing the scientific principles underlying experiments), we incorpor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;PRewrite&#65292;&#29992;&#20110;&#37325;&#20889;&#25552;&#31034;&#33609;&#26696;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.08189</link><description>&lt;p&gt;
PRewrite: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
PRewrite: Prompt Rewriting with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#21270;&#24037;&#20855;PRewrite&#65292;&#29992;&#20110;&#37325;&#20889;&#25552;&#31034;&#33609;&#26696;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#24037;&#31243;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08189v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442; &#25688;&#35201;: &#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#22522;&#20110;LLM&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#20197;&#8220;&#35797;&#38169;&#8221;&#30340;&#26041;&#24335;&#25163;&#21160;&#23436;&#25104;&#12290;&#36825;&#31181;&#25163;&#21160;&#31243;&#24207;&#21487;&#33021;&#32791;&#26102;&#65292;&#25928;&#26524;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#29983;&#25104;&#30340;&#25552;&#31034;&#37117;&#26159;&#27425;&#20248;&#30340;&#12290;&#21363;&#20351;&#23545;&#37027;&#20123;&#30475;&#20284;&#36816;&#20316;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#22987;&#32456;&#23384;&#22312;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#36827;&#19968;&#27493;&#20462;&#25913;&#20351;&#25552;&#31034;&#21464;&#24471;&#26356;&#22909;&#21602;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25552;&#31034;&#24037;&#31243;&#33258;&#21160;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#20351;&#29992;&#24773;&#26223;&#65292;&#21363;&#24320;&#21457;&#32773;/&#29992;&#25143;&#24050;&#32463;&#36215;&#33609;&#20102;&#21021;&#22987;&#25552;&#31034;&#65292;&#20294;&#32570;&#20047;&#26102;&#38388;/&#19987;&#19994;&#30693;&#35782;&#26469;&#20248;&#21270;&#23427;&#20204;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PRewrite&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#24037;&#20855;&#65292;&#21487;&#37325;&#20889;&#36825;&#20123;&#33609;&#26696;&#65292;&#24182;&#29983;&#25104;&#39640;&#25928;&#30340;&#26032;&#25552;&#31034;&#12290;PRewrite&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#65292;&#20801;&#35768;&#31471;&#21040;&#31471;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#35774;&#35745;&#20801;&#35768;RL&#25628;&#32034;&#22312;&#22823;&#21160;&#20316;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.08189v2 Announce Type: replace  Abstract: Prompt engineering is critical for the development of LLM-based applications. However, it is usually done manually in a "trial and error" fashion. This manual procedure can be time consuming, ineffective, and the generated prompts are, in a lot of cases, sub-optimal. Even for the prompts which seemingly work well, there is always a lingering question: can the prompts be made better with further modifications?   To address these questions, in this paper, we investigate prompt engineering automation. We consider a specific use case scenario in which developers/users have drafted initial prompts, but lack the time/expertise to optimize them. We propose PRewrite, an automated tool to rewrite these drafts and to generate highly effective new prompts. PRewrite is based on the Reinforcement Learning (RL) framework which allows for end-to-end optimization and our design allows the RL search to happen in a large action space. The automated to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#38382;&#39064;&#23545;&#40784;&#35757;&#32451;&#27169;&#22411;&#23558;&#25512;&#29702;&#38382;&#39064;&#32763;&#35793;&#25104;&#33521;&#35821;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26377;&#38024;&#23545;&#24615;&#30340;&#12289;&#39046;&#22495;&#20869;&#30340;&#35821;&#35328;&#23545;&#40784;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#33521;&#35821;&#25351;&#23548;&#25968;&#25454;&#65292;&#37322;&#25918;&#20102;LLMs&#30340;&#22810;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.07817</link><description>&lt;p&gt;
&#20026;&#20102;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#32780;&#36827;&#34892;&#30340;&#38382;&#39064;&#32763;&#35793;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Question Translation Training for Better Multilingual Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#38382;&#39064;&#23545;&#40784;&#35757;&#32451;&#27169;&#22411;&#23558;&#25512;&#29702;&#38382;&#39064;&#32763;&#35793;&#25104;&#33521;&#35821;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26377;&#38024;&#23545;&#24615;&#30340;&#12289;&#39046;&#22495;&#20869;&#30340;&#35821;&#35328;&#23545;&#40784;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#33521;&#35821;&#25351;&#23548;&#25968;&#25454;&#65292;&#37322;&#25918;&#20102;LLMs&#30340;&#22810;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#24448;&#24448;&#36739;&#24046;&#12290;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#25351;&#23548;&#25968;&#25454;&#32763;&#35793;&#25104;&#25152;&#26377;&#24863;&#20852;&#36259;&#30340;&#35821;&#35328;&#65292;&#28982;&#21518;&#22312;&#29983;&#25104;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#34987;&#31216;&#20026;&#32763;&#35793;&#35757;&#32451;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#38382;&#39064;&#23545;&#40784;&#30340;&#22909;&#22788;&#65292;&#36890;&#36807;&#22312;X-&#33521;&#35821;&#24179;&#34892;&#38382;&#39064;&#25968;&#25454;&#19978;&#24494;&#35843;&#65292;&#35757;&#32451;&#27169;&#22411;&#23558;&#25512;&#29702;&#38382;&#39064;&#32763;&#35793;&#25104;&#33521;&#35821;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#12289;&#39046;&#22495;&#20869;&#30340;&#35821;&#35328;&#23545;&#40784;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#33521;&#35821;&#25351;&#23548;&#25968;&#25454;&#65292;&#37322;&#25918;&#20102;LLMs&#30340;&#22810;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;LLaMA2-13&#19978;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07817v2 Announce Type: replace  Abstract: Large language models show compelling performance on reasoning tasks but they tend to perform much worse in languages other than English. This is unsurprising given that their training data largely consists of English text and instructions. A typical solution is to translate instruction data into all languages of interest, and then train on the resulting multilingual data, which is called translate-training. This approach not only incurs high cost, but also results in poorly translated data due to the non-standard formatting of mathematical chain-of-thought. In this paper, we explore the benefits of question alignment, where we train the model to translate reasoning questions into English by finetuning on X-English parallel question data. In this way we perform targeted, in-domain language alignment which makes best use of English instruction data to unlock the LLMs' multilingual reasoning abilities. Experimental results on LLaMA2-13
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#22312;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20013;&#20135;&#29983;&#20013;&#38388;&#27493;&#39588;&#22870;&#21169;&#65292;&#20197;&#24212;&#23545;&#31232;&#30095;&#22870;&#21169;&#20449;&#21495;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.07382</link><description>&lt;p&gt;
&#36229;&#36234;&#31232;&#30095;&#22870;&#21169;&#65306;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#35780;&#35770;&#22686;&#24378;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Beyond Sparse Rewards: Enhancing Reinforcement Learning with Language Model Critique in Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#35770;&#33021;&#21147;&#22312;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20013;&#20135;&#29983;&#20013;&#38388;&#27493;&#39588;&#22870;&#21169;&#65292;&#20197;&#24212;&#23545;&#31232;&#30095;&#22870;&#21169;&#20449;&#21495;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#38750;&#21487;&#24494;&#20998;&#22870;&#21169;&#20449;&#21495;&#65288;&#22914;&#20154;&#31867;&#20559;&#22909;&#65289;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#36825;&#20123;&#22870;&#21169;&#20449;&#21495;&#30340;&#31232;&#30095;&#24615; - &#36890;&#24120;&#65292;&#25972;&#20010;&#36755;&#20986;&#21482;&#26377;&#19968;&#20010;&#22870;&#21169;&#12290;&#36825;&#31181;&#22870;&#21169;&#30340;&#31232;&#30095;&#24615;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#25928;&#29575;&#20302;&#19979;&#19988;&#19981;&#31283;&#23450;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#35770;&#33021;&#21147;&#22312;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#36807;&#31243;&#20013;&#20135;&#29983;&#20013;&#38388;&#27493;&#39588;&#22870;&#21169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23558;&#31574;&#30053;&#27169;&#22411;&#19982;&#35780;&#35770;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#35780;&#35770;&#35821;&#35328;&#27169;&#22411;&#36127;&#36131;&#20026;&#36755;&#20986;&#30340;&#27599;&#20010;&#37096;&#20998;&#25552;&#20379;&#32454;&#33268;&#30340;&#21453;&#39304;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#21453;&#39304;&#36716;&#21270;&#20026;&#21487;&#29992;&#20110;&#25351;&#23548;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#36807;&#31243;&#30340;&#26631;&#35760;&#25110;&#36328;&#24230;&#32423;&#21035;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#65306;&#19968;&#31181;&#26159;&#31574;&#30053;&#27169;&#22411;&#36739;&#23567;&#19988;&#19982;&#26356;&#24378;&#22823;&#30340;&#35780;&#35770;&#32773;&#37197;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07382v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) can align language models with non-differentiable reward signals, such as human preferences. However, a major challenge arises from the sparsity of these reward signals - typically, there is only a single reward for an entire output. This sparsity of rewards can lead to inefficient and unstable learning. To address this challenge, our paper introduces an novel framework that utilizes the critique capability of Large Language Models (LLMs) to produce intermediate-step rewards during RL training. Our method involves coupling a policy model with a critic language model, which is responsible for providing comprehensive feedback of each part of the output. This feedback is then translated into token or span-level rewards that can be used to guide the RL training process. We investigate this approach under two different settings: one where the policy model is smaller and is paired with a more powerful crit
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#65292;&#32467;&#21512;&#32447;&#24615;&#21270;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#35889;&#26102;&#30340;&#24369;&#28857;&#12290;</title><link>https://arxiv.org/abs/2401.07105</link><description>&lt;p&gt;
&#22270;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Graph Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07105
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#65292;&#32467;&#21512;&#32447;&#24615;&#21270;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#28857;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#35889;&#26102;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20027;&#21147;&#20891;&#65292;&#23427;&#20204;&#19982;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#30340;&#30456;&#20114;&#20316;&#29992;&#20173;&#22312;&#31215;&#26497;&#30740;&#31350;&#20013;&#12290;&#24403;&#21069;&#29992;&#20110;&#32534;&#30721;&#36825;&#20123;&#22270;&#24418;&#30340;&#26041;&#27861;&#36890;&#24120;&#35201;&#20040;&#65288;i&#65289;&#23558;&#23427;&#20204;&#32447;&#24615;&#21270;&#20197;&#20379;LM&#23884;&#20837;--&#36825;&#26679;&#20250;&#20302;&#25928;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#65292;&#35201;&#20040;&#65288;ii&#65289;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26469;&#20445;&#30041;&#22270;&#32467;&#26500;--&#20294;GNNs&#26080;&#27861;&#20687;&#39044;&#35757;&#32451;&#30340;LM&#19968;&#26679;&#24456;&#22909;&#22320;&#34920;&#31034;&#25991;&#26412;&#29305;&#24449;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;LM&#31867;&#22411;&#65292;&#21363;&#22270;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#65292;&#23427;&#25972;&#21512;&#20102;&#20004;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#24182;&#20943;&#36731;&#20102;&#23427;&#20204;&#30340;&#24369;&#28857;&#12290;GLM&#21442;&#25968;&#20174;&#39044;&#35757;&#32451;&#30340;LM&#20013;&#21021;&#22987;&#21270;&#65292;&#20197;&#22686;&#24378;&#23545;&#20010;&#21035;&#22270;&#27010;&#24565;&#21644;&#19977;&#20803;&#32452;&#30340;&#29702;&#35299;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;GLM&#30340;&#26550;&#26500;&#20197;&#25972;&#21512;&#22270;&#20559;&#24046;&#65292;&#20174;&#32780;&#20419;&#36827;&#22270;&#20869;&#30340;&#30693;&#35782;&#20998;&#24067;&#12290;&#36825;&#20351;GLM&#33021;&#22815;&#22788;&#29702;&#22270;&#24418;&#12289;&#25991;&#26412;&#20197;&#21450;&#20004;&#32773;&#30340;&#20132;&#32455;&#36755;&#20837;&#12290;&#23454;&#35777;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07105v2 Announce Type: replace-cross  Abstract: While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs -- which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure -- but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM's architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34920;&#31034;&#24037;&#31243;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#23569;&#37327;&#26597;&#35810;&#23545;&#25552;&#21462;&#8220;&#23433;&#20840;&#27169;&#24335;&#8221;&#65292;&#25104;&#21151;&#35268;&#36991;&#30446;&#26631;&#27169;&#22411;&#30340;&#38450;&#24481;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36234;&#29425;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.06824</link><description>&lt;p&gt;
&#25171;&#24320;LLMs&#30340;&#28504;&#22810;&#25289;&#39764;&#30418;&#65306;&#36890;&#36807;&#34920;&#31034;&#24037;&#31243;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;
&lt;/p&gt;
&lt;p&gt;
Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation Engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.06824
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34920;&#31034;&#24037;&#31243;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#23569;&#37327;&#26597;&#35810;&#23545;&#25552;&#21462;&#8220;&#23433;&#20840;&#27169;&#24335;&#8221;&#65292;&#25104;&#21151;&#35268;&#36991;&#30446;&#26631;&#27169;&#22411;&#30340;&#38450;&#24481;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36234;&#29425;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#29425;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#35825;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#23545;&#24694;&#24847;&#26597;&#35810;&#20135;&#29983;&#26377;&#27602;&#21709;&#24212;&#65292;&#26469;&#25506;&#32034;LLMs&#23433;&#20840;&#24615;&#36793;&#30028;&#65292;&#36825;&#22312;LLMs&#31038;&#21306;&#20869;&#26159;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#36890;&#36807;&#34920;&#31034;&#24037;&#31243;&#23545;LLMs&#36827;&#34892;&#36234;&#29425;&#65288;Jailbreaking LLMs through Representation Engineering&#65292;JRE&#65289;&#30340;&#26032;&#39062;&#36234;&#29425;&#26041;&#27861;&#65292;&#20854;&#20165;&#38656;&#35201;&#23569;&#37327;&#26597;&#35810;&#23545;&#20197;&#25552;&#21462;&#21487;&#29992;&#20110;&#35268;&#36991;&#30446;&#26631;&#27169;&#22411;&#38450;&#24481;&#30340;&#8220;&#23433;&#20840;&#27169;&#24335;&#8221;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36234;&#29425;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.06824v2 Announce Type: replace-cross  Abstract: Jailbreaking techniques aim to probe the boundaries of safety in large language models (LLMs) by inducing them to generate toxic responses to malicious queries, a significant concern within the LLM community. While existing jailbreaking methods primarily rely on prompt engineering, altering inputs to evade LLM safety mechanisms, they suffer from low attack success rates and significant time overheads, rendering them inflexible. To overcome these limitations, we propose a novel jailbreaking approach, named Jailbreaking LLMs through Representation Engineering (JRE). Our method requires only a small number of query pairs to extract ``safety patterns'' that can be used to circumvent the target model's defenses, achieving unprecedented jailbreaking performance. Building upon these findings, we also introduce a novel defense framework inspired by JRE principles, which demonstrates notable effectiveness. Extensive experimentation conf
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#29983;&#25104;&#30340;&#26041;&#27861;SIG&#65292;&#22312;&#25991;&#23398;&#20316;&#21697;&#20013;&#23454;&#29616;&#20102;&#35828;&#35805;&#32773;&#35782;&#21035;&#20219;&#21153;&#65292;&#25903;&#25345;&#36328;&#39046;&#22495;&#35780;&#20272;&#21644;&#24320;&#25918;&#19990;&#30028;&#20998;&#31867;&#33539;&#24335;&#12290;</title><link>https://arxiv.org/abs/2312.14590</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25552;&#31034;&#29983;&#25104;&#30340;&#26041;&#27861;&#22312;&#25991;&#23398;&#20316;&#21697;&#20013;&#36827;&#34892;&#35828;&#35805;&#32773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
SIG: Speaker Identification in Literature via Prompt-Based Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14590
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#29983;&#25104;&#30340;&#26041;&#27861;SIG&#65292;&#22312;&#25991;&#23398;&#20316;&#21697;&#20013;&#23454;&#29616;&#20102;&#35828;&#35805;&#32773;&#35782;&#21035;&#20219;&#21153;&#65292;&#25903;&#25345;&#36328;&#39046;&#22495;&#35780;&#20272;&#21644;&#24320;&#25918;&#19990;&#30028;&#20998;&#31867;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#23398;&#20998;&#26512;&#20013;&#65292;&#35782;&#21035;&#21465;&#36848;&#20013;&#24341;&#29992;&#30340;&#21457;&#35328;&#32773;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#25361;&#25112;&#24615;&#24773;&#26223;&#21253;&#25324;&#23545;&#30475;&#19981;&#35265;&#21457;&#35328;&#32773;&#30340;&#36328;&#39046;&#22495;&#25512;&#26029;&#65292;&#20197;&#21450;&#21608;&#22260;&#29615;&#22659;&#20013;&#27809;&#26377;&#25552;&#21040;&#21457;&#35328;&#32773;&#30340;&#38750;&#26126;&#30830;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;SIG&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#35774;&#35745;&#30340;&#25552;&#31034;&#27169;&#26495;&#23545;&#20219;&#21153;&#21644;&#24341;&#35821;&#36755;&#20837;&#36827;&#34892;&#35821;&#35328;&#21270;&#22788;&#29702;&#65292;&#36824;&#21487;&#20197;&#36731;&#26494;&#38598;&#25104;&#20854;&#20182;&#36827;&#19968;&#27493;&#22686;&#24378;&#35828;&#35805;&#32773;&#35782;&#21035;&#24615;&#33021;&#30340;&#36741;&#21161;&#20219;&#21153;&#12290;&#39044;&#27979;&#21487;&#20197;&#26469;&#33258;&#27169;&#22411;&#30340;&#30452;&#25509;&#29983;&#25104;&#65292;&#20063;&#21487;&#20197;&#30001;&#27599;&#20010;&#21457;&#35328;&#32773;&#20505;&#36873;&#20154;&#30340;&#26368;&#39640;&#29983;&#25104;&#27010;&#29575;&#30830;&#23450;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#35774;&#35745;&#65292;SIG&#25903;&#25345;&#36328;&#39046;&#22495;&#35780;&#20272;&#65292;&#24182;&#23454;&#29616;&#20102;&#33021;&#22815;&#25509;&#21463;&#20219;&#20309;&#20505;&#36873;&#36755;&#20837;&#24418;&#24335;&#30340;&#24320;&#25918;&#19990;&#30028;&#20998;&#31867;&#33539;&#24335;&#12290;&#25105;&#20204;&#22312;PDNC&#19978;&#36827;&#34892;&#20102;&#36328;&#39046;&#22495;&#35780;&#20272;&#21644;&#20869;&#39046;&#22495;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14590v2 Announce Type: replace  Abstract: Identifying speakers of quotations in narratives is an important task in literary analysis, with challenging scenarios including the out-of-domain inference for unseen speakers, and non-explicit cases where there are no speaker mentions in surrounding context. In this work, we propose a simple and effective approach SIG, a generation-based method that verbalizes the task and quotation input based on designed prompt templates, which also enables easy integration of other auxiliary tasks that further bolster the speaker identification performance. The prediction can either come from direct generation by the model, or be determined by the highest generation probability of each speaker candidate. Based on our approach design, SIG supports out-of-domain evaluation, and achieves open-world classification paradigm that is able to accept any forms of candidate input. We perform both cross-domain evaluation and in-domain evaluation on PDNC, t
&lt;/p&gt;</description></item><item><title>&#25351;&#20196;&#24494;&#35843;&#25552;&#21319;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#21457;&#29616;&#19981;&#21516;&#25351;&#20196;&#31867;&#22411;&#23545;&#29305;&#23450;&#24212;&#29992;&#26356;&#26377;&#21033;&#65292;&#20294;&#21487;&#33021;&#23545;&#20854;&#20182;&#39046;&#22495;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2312.10793</link><description>&lt;p&gt;
&#35299;&#35835;&#22823;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#25351;&#20196;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
Demystifying Instruction Mixing for Fine-tuning Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10793
&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#24494;&#35843;&#25552;&#21319;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#21457;&#29616;&#19981;&#21516;&#25351;&#20196;&#31867;&#22411;&#23545;&#29305;&#23450;&#24212;&#29992;&#26356;&#26377;&#21033;&#65292;&#20294;&#21487;&#33021;&#23545;&#20854;&#20182;&#39046;&#22495;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#24494;&#35843;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20248;&#21270;LLM&#24494;&#35843;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#28151;&#21512;&#30340;&#36807;&#31243;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#30740;&#31350;&#23558;&#25351;&#20196;&#20998;&#20026;&#19977;&#31867;&#20027;&#35201;&#31867;&#22411;&#65306;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#19979;&#28216;&#20219;&#21153;&#12289;&#32534;&#31243;&#21644;&#19968;&#33324;&#23545;&#35805;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#25351;&#20196;&#24494;&#35843;&#23545;LLM&#24615;&#33021;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#32452;&#21512;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#26576;&#20123;&#25351;&#20196;&#31867;&#22411;&#23545;&#29305;&#23450;&#24212;&#29992;&#26356;&#26377;&#21033;&#65292;&#20294;&#21487;&#33021;&#23545;&#20854;&#20182;&#39046;&#22495;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#25351;&#20196;&#28151;&#21512;&#25552;&#20379;&#20102;&#35265;&#35299;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10793v3 Announce Type: replace-cross  Abstract: Instruction tuning significantly enhances the performance of large language models (LLMs) across various tasks. However, the procedure to optimizing the mixing of instruction datasets for LLM fine-tuning is still poorly understood. This study categorizes instructions into three primary types: NLP downstream tasks, coding, and general chat. We explore the effects of instruction tuning on different combinations of datasets on LLM performance, and find that certain instruction types are more advantageous for specific applications but can negatively impact other areas. This work provides insights into instruction mixtures, laying the foundations for future research.
&lt;/p&gt;</description></item><item><title>TAP4LLM&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#34920;&#26684;&#25552;&#31034;&#30340;&#22810;&#21151;&#33021;&#39044;&#22788;&#29702;&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#37319;&#26679;&#12289;&#22686;&#34917;&#21644;&#25171;&#21253;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#21644;&#22823;&#22411;&#34920;&#26684;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.09039</link><description>&lt;p&gt;
TAP4LLM&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#34920;&#26684;&#25552;&#20379;&#32773;&#22312;&#23545;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#37319;&#26679;&#12289;&#22686;&#34917;&#21644;&#25171;&#21253;
&lt;/p&gt;
&lt;p&gt;
TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09039
&lt;/p&gt;
&lt;p&gt;
TAP4LLM&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#34920;&#26684;&#25552;&#31034;&#30340;&#22810;&#21151;&#33021;&#39044;&#22788;&#29702;&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#37319;&#26679;&#12289;&#22686;&#34917;&#21644;&#25171;&#21253;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#21644;&#22823;&#22411;&#34920;&#26684;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#34920;&#26684;&#30340;&#25512;&#29702;&#22312;&#32467;&#21512;&#28145;&#24230;&#27169;&#22411;&#21644;&#31163;&#25955;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36825;&#38656;&#35201;&#23545;&#33258;&#30001;&#24418;&#24335;&#30340;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#38382;&#39064;&#21644;&#21322;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#34920;&#26684;&#25512;&#29702;&#35299;&#20915;&#26041;&#26696;&#21482;&#32771;&#34385;&#23567;&#22411;&#34920;&#26684;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#26356;&#22823;&#34920;&#26684;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#25512;&#29702;&#22797;&#26434;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#22522;&#26412;&#20449;&#24687;&#25110;&#20998;&#25955;&#22312;&#19981;&#21516;&#20301;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TAP4LLM&#20316;&#20026;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#39044;&#22788;&#29702;&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#24179;&#34913;&#26631;&#35760;&#20998;&#37197;&#26435;&#34913;&#26469;&#29983;&#25104;&#34920;&#26684;&#25552;&#31034;&#65292;&#23454;&#29616;(1) &#34920;&#26684;&#37319;&#26679;&#65292;(2) &#34920;&#26684;&#22686;&#34917;&#21644;(3) &#34920;&#26684;&#25171;&#21253;&#12290;&#22312;&#27599;&#20010;&#27169;&#22359;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#21644;&#35774;&#35745;&#20102;&#20960;&#31181;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#20351;&#29992;&#30340;&#24120;&#35265;&#26041;&#27861;&#65288;&#20363;&#22914;&#65292;&#36895;&#24230;&#19982;&#20934;&#30830;&#24615;&#30340;&#24179;&#34913;&#65289;&#12290;&#25105;&#20204;&#36824;&#23545;T&#20869;&#37096;&#27599;&#20010;&#32452;&#20214;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09039v2 Announce Type: replace-cross  Abstract: Table-based reasoning has shown remarkable progress in combining deep models with discrete reasoning, which requires reasoning over both free-form natural language (NL) questions and semi-structured tabular data. However, previous table reasoning solutions only consider small-sized tables and exhibit limitations in handling larger tables. In addition, most existing methods struggle to reason over complex questions since they lack essential information or they are scattered in different places. To alleviate these challenges, we propose TAP4LLM as a versatile pre-processing toolbox to generate table prompts through (1) table sampling, (2) table augmentation, and (3) table packing while balancing the token allocation trade-off. In each module, we collect and design several common methods for usage in various scenarios (e.g., speed over accuracy). We also provide a comprehensive evaluation on performance of each components inside T
&lt;/p&gt;</description></item><item><title>Math-Shepherd&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#23398;&#22870;&#21169;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#36807;&#31243;&#30417;&#30563;&#25968;&#25454;&#23454;&#29616;LLMs&#30340;&#36880;&#27493;&#39564;&#35777;&#21644;&#21152;&#24378;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.08935</link><description>&lt;p&gt;
Math-Shepherd: &#22312;&#19981;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#30340;&#24773;&#20917;&#19979;&#36880;&#27493;&#39564;&#35777;&#21644;&#21152;&#24378;LLMs
&lt;/p&gt;
&lt;p&gt;
Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08935
&lt;/p&gt;
&lt;p&gt;
Math-Shepherd&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#23398;&#22870;&#21169;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#36807;&#31243;&#30417;&#30563;&#25968;&#25454;&#23454;&#29616;LLMs&#30340;&#36880;&#27493;&#39564;&#35777;&#21644;&#21152;&#24378;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Math-Shepherd&#30340;&#21019;&#26032;&#36807;&#31243;&#23548;&#21521;&#25968;&#23398;&#22870;&#21169;&#27169;&#22411;&#65292;&#20026;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;&#27599;&#19968;&#27493;&#20998;&#37197;&#22870;&#21169;&#20998;&#25968;&#12290;Math-Shepherd&#30340;&#35757;&#32451;&#26159;&#20351;&#29992;&#33258;&#21160;&#26500;&#24314;&#30340;&#22522;&#20110;&#36807;&#31243;&#30340;&#30417;&#30563;&#25968;&#25454;&#23436;&#25104;&#30340;&#65292;&#25171;&#30772;&#20102;&#29616;&#26377;&#24037;&#20316;&#20013;&#23545;&#25163;&#21160;&#26631;&#27880;&#30340;&#20005;&#37325;&#20381;&#36182;&#29942;&#39048;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;Math-Shepherd&#22312;&#20004;&#31181;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#65306;1&#65289;\textit{&#39564;&#35777;}&#65306;&#21033;&#29992;Math-Shepherd&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#30340;&#22810;&#20010;&#36755;&#20986;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65307;2&#65289;\textit{&#24378;&#21270;&#23398;&#20064;}&#65306;&#20351;&#29992;Math-Shepherd&#36890;&#36807;&#36880;&#27493;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(PPO)&#21152;&#24378;LLMs&#12290;&#36890;&#36807;Math-Shepherd&#65292;&#19968;&#31995;&#21015;&#24320;&#28304;LLMs&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;Math-Shepherd&#30340;&#36880;&#27493;PPO&#26174;&#33879;&#25552;&#39640;&#20102;Mistral-7B&#30340;&#20934;&#30830;&#29575;(GSM8K&#30001;77.9%&#25552;&#39640;&#21040;84.1%&#65292;MATH&#30001;28.6%&#25552;&#39640;&#21040;33.0%)
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08935v3 Announce Type: replace  Abstract: In this paper, we present an innovative process-oriented math process reward model called \textbf{Math-Shepherd}, which assigns a reward score to each step of math problem solutions. The training of Math-Shepherd is achieved using automatically constructed process-wise supervision data, breaking the bottleneck of heavy reliance on manual annotation in existing work. We explore the effectiveness of Math-Shepherd in two scenarios: 1) \textit{Verification}: Math-Shepherd is utilized for reranking multiple outputs generated by Large Language Models (LLMs); 2) \textit{Reinforcement Learning}: Math-Shepherd is employed to reinforce LLMs with step-by-step Proximal Policy Optimization (PPO). With Math-Shepherd, a series of open-source LLMs demonstrates exceptional performance. For instance, the step-by-step PPO with Math-Shepherd significantly improves the accuracy of Mistral-7B (77.9\%$\to$84.1\% on GSM8K and 28.6\%$\to$33.0\% on MATH). The
&lt;/p&gt;</description></item><item><title>KnowGPT&#26159;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#33218;&#32769;&#34382;&#26426;&#26500;&#24314;&#26368;&#36866;&#21512;&#27599;&#20010;&#38382;&#39064;&#30340;&#25552;&#31034;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25552;&#21319;&#20102;&#30693;&#35782;&#27880;&#20837;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.06185</link><description>&lt;p&gt;
KnowGPT&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
KnowGPT: Black-Box Knowledge Injection for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06185
&lt;/p&gt;
&lt;p&gt;
KnowGPT&#26159;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#33218;&#32769;&#34382;&#26426;&#26500;&#24314;&#26368;&#36866;&#21512;&#27599;&#20010;&#38382;&#39064;&#30340;&#25552;&#31034;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25552;&#21319;&#20102;&#30693;&#35782;&#27880;&#20837;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#25552;&#20379;&#20114;&#21160;&#24335;API&#65292;&#21487;&#20197;&#20197;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#22238;&#31572;&#24120;&#35265;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#38656;&#35201;&#29305;&#23450;&#39046;&#22495;&#25110;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#30340;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20250;&#32473;&#20986;&#19981;&#20934;&#30830;&#25110;&#19981;&#27491;&#30830;&#30340;&#21709;&#24212;&#65292;&#36825;&#20123;&#30693;&#35782;&#24182;&#26410;&#21253;&#21547;&#22312;&#23427;&#20204;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;LLMs&#24182;&#38750;&#24320;&#28304;&#65292;&#36825;&#20351;&#24471;&#20165;&#20351;&#29992;&#27169;&#22411;API&#27880;&#20837;&#30693;&#35782;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;KnowGPT&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#22312;&#38382;&#31572;&#20013;&#30340;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;&#12290;KnowGPT&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#30693;&#35782;&#22270;&#20013;&#25552;&#21462;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#20026;&#27599;&#20010;&#38382;&#39064;&#26500;&#24314;&#26368;&#21512;&#36866;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;KnowGPT&#26174;&#33879;&#22686;&#24378;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;KnowGPT&#24179;&#22343;&#25913;&#36827;&#20102;23%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06185v2 Announce Type: replace-cross  Abstract: Generative Large Language Models (LLMs), such as ChatGPT, offer interactive APIs that can answer common questions at a human-expert level. However, these models often give inaccurate or incorrect responses when faced with questions requiring domain-specific or professional-specific knowledge not covered in their training corpus. Furthermore, many state-of-the-art LLMs are not open-source, making it challenging to inject knowledge with model APIs only. In this work, we introduce KnowGPT, a black-box knowledge injection framework for LLMs in question answering. KnowGPT leverages deep reinforcement learning (RL) to extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed Bandit (MAB) to construct the most suitable prompt for each question. Our extensive experiments on three benchmark datasets showcase that KnowGPT significantly enhances the existing methods. Notably, KnowGPT achieves an average improvement of 23.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#23558;&#25991;&#26412;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#26410;&#26469;&#21463;&#38480;&#29983;&#25104;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#19981;&#33391;&#34892;&#20026;&#24182;&#24378;&#21046;&#25191;&#34892;&#23545;&#25351;&#20196;&#30340;&#24544;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;LLMs&#26377;&#25928;&#25351;&#23548;&#25991;&#26412;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2312.06149</link><description>&lt;p&gt;
&#35299;&#38145;&#39044;&#27979;&#24615;&#25991;&#26412;&#29983;&#25104;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#30721;&#30340;&#21463;&#38480;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06149
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#23558;&#25991;&#26412;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#26410;&#26469;&#21463;&#38480;&#29983;&#25104;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#19981;&#33391;&#34892;&#20026;&#24182;&#24378;&#21046;&#25191;&#34892;&#23545;&#25351;&#20196;&#30340;&#24544;&#23454;&#24615;&#65292;&#24182;&#36890;&#36807;LLMs&#26377;&#25928;&#25351;&#23548;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20102;&#24378;&#22823;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#32473;&#23450;&#25552;&#31034;&#25110;&#25351;&#20196;&#23454;&#29616;&#26368;&#20339;&#32467;&#26524;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21313;&#20159;&#32423;&#21035;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#19981;&#33391;&#34892;&#20026;&#22914;&#27602;&#24615;&#25110;&#24187;&#35273;&#21487;&#33021;&#20250;&#26174;&#29616;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25991;&#26412;&#29983;&#25104;&#24418;&#24335;&#21270;&#20026;&#26410;&#26469;&#21463;&#38480;&#29983;&#25104;&#38382;&#39064;&#65292;&#20197;&#26368;&#23567;&#21270;&#19981;&#33391;&#34892;&#20026;&#24182;&#24378;&#21046;&#25191;&#34892;&#23545;&#25351;&#20196;&#30340;&#24544;&#23454;&#24615;&#12290;&#20351;&#29992;LLMs&#23454;&#29616;&#26410;&#26469;&#32422;&#26463;&#28385;&#36275;&#24230;&#30340;&#20272;&#35745;&#24341;&#23548;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65306;&#20851;&#38190;&#35789;&#21463;&#38480;&#29983;&#25104;&#12289;&#27602;&#24615;&#20943;&#23569;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06149v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention. In this work, we propose formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions. The estimation of future constraint satisfaction, accomplished using LLMs, guides the text generation process. Our extensive experiments demonstrate the effectiveness of the proposed approach across three distinct text generation tasks: keyword-constrained generation (Lin et al., 2020), toxicity reduction (Gehman et al., 202
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#33258;&#21160;&#35780;&#20998;&#23398;&#29983;&#23545;&#31185;&#23398;&#35780;&#20272;&#20889;&#20316;&#21453;&#39304;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#36890;&#36807;&#38646;&#27425;&#25110;&#23569;&#27425;&#23398;&#20064;&#31561;&#31574;&#30053;&#33258;&#21160;&#35780;&#20998;&#65292;&#20854;&#20013;&#23569;&#27425;&#23398;&#20064;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>https://arxiv.org/abs/2312.03748</link><description>&lt;p&gt;
&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24605;&#32500;&#38142;&#36827;&#34892;&#33258;&#21160;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Applying Large Language Models and Chain-of-Thought for Automatic Scoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#33258;&#21160;&#35780;&#20998;&#23398;&#29983;&#23545;&#31185;&#23398;&#35780;&#20272;&#20889;&#20316;&#21453;&#39304;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#36890;&#36807;&#38646;&#27425;&#25110;&#23569;&#27425;&#23398;&#20064;&#31561;&#31574;&#30053;&#33258;&#21160;&#35780;&#20998;&#65292;&#20854;&#20013;&#23569;&#27425;&#23398;&#20064;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159; GPT-3.5 &#21644; GPT-4&#65292;&#20197;&#21450;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#22312;&#33258;&#21160;&#35780;&#20998;&#23398;&#29983;&#23545;&#31185;&#23398;&#35780;&#20272;&#30340;&#20889;&#20316;&#21453;&#39304;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20811;&#26381;&#20808;&#21069;&#38480;&#21046;&#30740;&#31350;&#20154;&#21592;&#21644;&#25945;&#32946;&#24037;&#20316;&#32773;&#20351;&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#33258;&#21160;&#35780;&#20998;&#24037;&#20855;&#30340;&#26080;&#27861;&#35775;&#38382;&#24615;&#12289;&#25216;&#26415;&#22797;&#26434;&#24615;&#21644;&#32570;&#20047;&#35299;&#37322;&#24615;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#19968;&#20010;&#21253;&#25324; 1,650 &#20010;&#23398;&#29983;&#21453;&#39304;&#30340;&#20845;&#39033;&#35780;&#20272;&#20219;&#21153;&#65288;&#19977;&#20010;&#20108;&#39033;&#21644;&#19977;&#20010;&#19977;&#39033;&#65289;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#37319;&#29992;&#20845;&#31181;&#25552;&#31034;&#24037;&#31243;&#31574;&#30053;&#26469;&#33258;&#21160;&#35780;&#20998;&#23398;&#29983;&#30340;&#21453;&#39304;&#12290;&#36825;&#20845;&#31181;&#31574;&#30053;&#23558;&#38646;&#27425;&#23398;&#20064;&#25110;&#23569;&#27425;&#23398;&#20064;&#19982; CoT &#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#35201;&#20040;&#29420;&#33258;&#20351;&#29992;&#65292;&#35201;&#20040;&#19982;&#39033;&#30446;&#24178;&#21644;&#35780;&#20998;&#32454;&#21017;&#19968;&#36215;&#20351;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23569;&#27425;&#23398;&#20064;&#65288;&#20934;&#30830;&#29575;=.67&#65289;&#20248;&#20110;&#38646;&#27425;&#23398;&#20064;&#65288;&#20934;&#30830;&#29575;=.60&#65289;&#65292;&#25552;&#39640;12.6%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03748v2 Announce Type: replace-cross  Abstract: This study investigates the application of large language models (LLMs), specifically GPT-3.5 and GPT-4, with Chain-of-Though (CoT) in the automatic scoring of student-written responses to science assessments. We focused on overcoming the challenges of accessibility, technical complexity, and lack of explainability that have previously limited the use of artificial intelligence-based automatic scoring tools among researchers and educators. With a testing dataset comprising six assessment tasks (three binomial and three trinomial) with 1,650 student responses, we employed six prompt engineering strategies to automatically score student responses. The six strategies combined zero-shot or few-shot learning with CoT, either alone or alongside item stem and scoring rubrics. Results indicated that few-shot (acc = .67) outperformed zero-shot learning (acc = .60), with 12.6% increase. CoT, when used without item stem and scoring rubric
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#34394;&#26500;&#33021;&#21147;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#22522;&#20934;&#35780;&#20272;&#36890;&#24120;&#37319;&#29992;&#21463;&#38480;&#21046;&#30340;&#29983;&#25104;&#25216;&#26415;&#65292;&#26080;&#27861;&#28385;&#36275;&#30495;&#23454;&#38656;&#27714;&#20013;&#30340;&#26080;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2311.15296</link><description>&lt;p&gt;
UHGEval&#65306;&#36890;&#36807;&#26080;&#32422;&#26463;&#29983;&#25104;&#35780;&#20272;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34394;&#26500;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15296
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#34394;&#26500;&#33021;&#21147;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#22522;&#20934;&#35780;&#20272;&#36890;&#24120;&#37319;&#29992;&#21463;&#38480;&#21046;&#30340;&#29983;&#25104;&#25216;&#26415;&#65292;&#26080;&#27861;&#28385;&#36275;&#30495;&#23454;&#38656;&#27714;&#20013;&#30340;&#26080;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#24403;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#37325;&#35201;&#36129;&#29486;&#32773;&#65292;&#24182;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#34892;&#19994;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22823;&#35268;&#27169;&#27010;&#29575;&#32479;&#35745;&#27169;&#22411;&#30446;&#21069;&#26080;&#27861;&#20445;&#35777;&#22312;&#19987;&#19994;&#20869;&#23481;&#29983;&#25104;&#20013;&#30340;&#24517;&#35201;&#36136;&#37327;&#12290;&#36825;&#20123;&#27169;&#22411;&#32463;&#24120;&#20135;&#29983;&#34394;&#26500;&#30340;&#25991;&#26412;&#65292;&#24433;&#21709;&#23427;&#20204;&#22312;&#19987;&#19994;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;LLMs&#22312;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#30495;&#23454;&#21487;&#38752;&#24615;&#65292;&#35768;&#22810;&#20513;&#35758;&#24050;&#24320;&#21457;&#20102;&#29992;&#20110;&#34394;&#26500;&#29616;&#35937;&#30340;&#22522;&#20934;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#35780;&#20272;&#36890;&#24120;&#21033;&#29992;&#21463;&#38480;&#21046;&#30340;&#29983;&#25104;&#25216;&#26415;&#65292;&#22240;&#20026;&#25104;&#26412;&#21644;&#26102;&#38388;&#38480;&#21046;&#12290;&#36825;&#20123;&#25216;&#26415;&#21253;&#25324;&#20351;&#29992;&#23450;&#21521;&#24187;&#35273;&#35825;&#23548;&#21644;&#25925;&#24847;&#25913;&#21464;&#30495;&#23454;&#25991;&#26412;&#20197;&#20135;&#29983;&#24187;&#35273;&#30340;&#31574;&#30053;&#12290;&#36825;&#20123;&#26041;&#27861;&#19982;&#23454;&#38469;&#38656;&#27714;&#30340;&#26080;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15296v2 Announce Type: replace  Abstract: Large language models (LLMs) have emerged as pivotal contributors in contemporary natural language processing and are increasingly being applied across a diverse range of industries. However, these large-scale probabilistic statistical models cannot currently ensure the requisite quality in professional content generation. These models often produce hallucinated text, compromising their practical utility in professional contexts. To assess the authentic reliability of LLMs in text generation, numerous initiatives have developed benchmark evaluations for hallucination phenomena. Nevertheless, these benchmarks frequently utilize constrained generation techniques due to cost and temporal constraints. These techniques encompass the use of directed hallucination induction and strategies that deliberately alter authentic text to produce hallucinations. These approaches are not congruent with the unrestricted text generation demanded by rea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#21435;&#20559;&#35265;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#25490;&#24207;&#26469;&#26657;&#20934;&#25628;&#32034;&#32467;&#26524;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#20195;&#30721;&#25628;&#32034;&#24341;&#25806;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20195;&#30721;&#25628;&#32034;&#30340;&#24635;&#20307;&#25490;&#21517;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2311.14901</link><description>&lt;p&gt;
&#20195;&#30721;&#25628;&#32034;&#21435;&#20559;&#35265;:&#25913;&#21892;&#25628;&#32034;&#32467;&#26524;&#36229;&#36234;&#24635;&#20307;&#25490;&#21517;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Code Search Debiasing:Improve Search Results beyond Overall Ranking Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14901
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#21435;&#20559;&#35265;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#25490;&#24207;&#26469;&#26657;&#20934;&#25628;&#32034;&#32467;&#26524;&#65292;&#26377;&#25928;&#32531;&#35299;&#20102;&#20195;&#30721;&#25628;&#32034;&#24341;&#25806;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20195;&#30721;&#25628;&#32034;&#30340;&#24635;&#20307;&#25490;&#21517;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14901v2 &#20844;&#21578;&#31867;&#22411;:&#26367;&#25442; &#25688;&#35201;: &#20195;&#30721;&#25628;&#32034;&#24341;&#25806;&#26159;&#36719;&#20214;&#24320;&#21457;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#35768;&#22810;&#20195;&#30721;&#25628;&#32034;&#26041;&#27861;&#28044;&#29616;&#20986;&#26469;&#65292;&#19987;&#27880;&#20110;&#20195;&#30721;&#25628;&#32034;&#30340;&#24635;&#20307;&#25490;&#21517;&#24615;&#33021;&#12290;&#26412;&#25991;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#30740;&#31350;&#20195;&#30721;&#25628;&#32034;&#65292;&#20998;&#26512;&#20102;&#20195;&#30721;&#25628;&#32034;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;&#26377;&#20559;&#35265;&#30340;&#20195;&#30721;&#25628;&#32034;&#24341;&#25806;&#20250;&#25552;&#20379;&#36739;&#24046;&#30340;&#29992;&#25143;&#20307;&#39564;&#65292;&#21363;&#20351;&#23427;&#20204;&#34920;&#29616;&#20986;&#33394;&#24635;&#20307;&#34920;&#29616;&#12290;&#30001;&#20110;&#19981;&#21516;&#30340;&#24320;&#21457;&#24815;&#20363;&#65288;&#20363;&#22914;&#65292;&#26356;&#21916;&#27426;&#38271;&#26597;&#35810;&#36824;&#26159;&#32553;&#20889;&#65289;&#65292;&#26377;&#20123;&#31243;&#24207;&#21592;&#20250;&#21457;&#29616;&#24341;&#25806;&#26377;&#29992;&#65292;&#32780;&#20854;&#20182;&#20154;&#21487;&#33021;&#24456;&#38590;&#33719;&#24471;&#29702;&#24819;&#30340;&#25628;&#32034;&#32467;&#26524;&#12290;&#20026;&#20102;&#20943;&#36731;&#20559;&#35265;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#21435;&#20559;&#35265;&#26694;&#26550;&#65292;&#21033;&#29992;&#37325;&#26032;&#25490;&#24207;&#26469;&#26657;&#20934;&#25628;&#32034;&#32467;&#26524;&#12290;&#23427;&#21487;&#20197;&#36731;&#26494;&#25554;&#20837;&#29616;&#26377;&#24341;&#25806;&#65292;&#24182;&#22788;&#29702;&#23558;&#26469;&#21457;&#29616;&#30340;&#26032;&#20195;&#30721;&#25628;&#32034;&#20559;&#35265;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#20559;&#35265;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22312;&#21435;&#20559;&#35265;&#21518;&#65292;&#20195;&#30721;&#25628;&#32034;&#30340;&#24635;&#20307;&#25490;&#21517;&#34920;&#29616;&#20063;&#24471;&#21040;&#20102;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14901v2 Announce Type: replace  Abstract: Code search engine is an essential tool in software development. Many code search methods have sprung up, focusing on the overall ranking performance of code search. In this paper, we study code search from another perspective by analyzing the bias of code search models. Biased code search engines provide poor user experience, even though they show promising overall performance. Due to different development conventions (e.g., prefer long queries or abbreviations), some programmers will find the engine useful, while others may find it hard to get desirable search results. To mitigate biases, we develop a general debiasing framework that employs reranking to calibrate search results. It can be easily plugged into existing engines and handle new code search biases discovered in the future. Experiments show that our framework can effectively reduce biases. Meanwhile, the overall ranking performance of code search gets improved after debi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;LLMs&#21644;VLMs&#30340;&#26694;&#26550;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#31867;&#21035;&#25551;&#36848;&#31526;&#65292;&#35299;&#20915;&#20102;&#22270;&#20687;&#20998;&#31867;&#20013;&#22312;&#31934;&#30830;&#26500;&#24314;&#25991;&#26412;&#34920;&#31034;&#21644;&#21306;&#20998;&#30456;&#20284;&#31867;&#21035;&#26041;&#38754;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2311.11904</link><description>&lt;p&gt;
LLMs&#20316;&#20026;&#35270;&#35273;&#35299;&#37322;&#22120;&#65306;&#36890;&#36807;&#19981;&#26029;&#28436;&#36827;&#30340;&#35270;&#35273;&#25551;&#36848;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11904
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;LLMs&#21644;VLMs&#30340;&#26694;&#26550;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#31867;&#21035;&#25551;&#36848;&#31526;&#65292;&#35299;&#20915;&#20102;&#22270;&#20687;&#20998;&#31867;&#20013;&#22312;&#31934;&#30830;&#26500;&#24314;&#25991;&#26412;&#34920;&#31034;&#21644;&#21306;&#20998;&#30456;&#20284;&#31867;&#21035;&#26041;&#38754;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#36890;&#36807;&#27604;&#36739;&#22270;&#20687;&#19982;&#31867;&#21035;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20026;&#22270;&#20687;&#20998;&#31867;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#22312;&#20110;&#20026;&#31867;&#21035;&#21517;&#31216;&#26500;&#24314;&#31934;&#30830;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;LLMs&#21644;VLMs&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#31867;&#21035;&#25551;&#36848;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11904v2 Announce Type: replace-cross  Abstract: Vision-language models (VLMs) offer a promising paradigm for image classification by comparing the similarity between images and class embeddings. A critical challenge lies in crafting precise textual representations for class names. While previous studies have leveraged recent advancements in large language models (LLMs) to enhance these descriptors, their outputs often suffer from ambiguity and inaccuracy. We attribute this to two primary factors: 1) the reliance on single-turn textual interactions with LLMs, leading to a mismatch between generated text and visual concepts for VLMs; 2) the oversight of the inter-class relationships, resulting in descriptors that fail to differentiate similar classes effectively. In this paper, we propose a novel framework that integrates LLMs and VLMs to find the optimal class descriptors. Our training-free approach develops an LLM-based agent with an evolutionary optimization strategy to ite
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20196;&#29260;&#32423;&#21035;&#26816;&#27979;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.11509</link><description>&lt;p&gt;
&#22522;&#20110;&#22256;&#24785;&#24230;&#37327;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#20196;&#29260;&#32423;&#23545;&#25239;&#25552;&#31034;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11509
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20196;&#29260;&#32423;&#21035;&#26816;&#27979;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25552;&#31034;&#25915;&#20987;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#31934;&#24515;&#31574;&#21010;&#36755;&#20837;&#23383;&#31526;&#20018;&#65292;&#35823;&#23548;LLM&#29983;&#25104;&#19981;&#27491;&#30830;&#25110;&#19981;&#24076;&#26395;&#30340;&#36755;&#20986;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#21033;&#29992;&#31163;&#25955;&#20248;&#21270;&#30340;&#30456;&#23545;&#31616;&#21333;&#21364;&#26377;&#25928;&#30340;&#25915;&#20987;&#26041;&#24335;&#21487;&#20197;&#29983;&#25104;&#32469;&#36807;&#27169;&#22411;&#30340;&#35843;&#25972;&#21644;&#23545;&#40784;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#23545;&#23545;&#25239;&#24615;&#25552;&#31034;&#30340;&#33030;&#24369;&#24615;&#20984;&#26174;&#20102;&#23545;LLM&#20581;&#22766;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#37325;&#35201;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#22312;&#20196;&#29260;&#32423;&#21035;&#26816;&#27979;&#23545;&#25239;&#24615;&#25552;&#31034;&#65292;&#21033;&#29992;LLM&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#27010;&#29575;&#33021;&#21147;&#12290;&#25105;&#20204;&#27979;&#37327;&#27169;&#22411;&#22256;&#24785;&#24230;&#30340;&#31243;&#24230;&#65292;&#20854;&#20013;&#39640;&#27010;&#29575;&#39044;&#27979;&#30340;&#20196;&#29260;&#34987;&#35270;&#20026;&#27491;&#24120;&#65292;&#32780;&#37027;&#20123;&#34920;&#29616;&#24322;&#24120;&#30340;&#21017;&#21487;&#33021;&#26159;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11509v3 Announce Type: replace  Abstract: In recent years, Large Language Models (LLM) have emerged as pivotal tools in various applications. However, these models are susceptible to adversarial prompt attacks, where attackers can carefully curate input strings that mislead LLMs into generating incorrect or undesired outputs. Previous work has revealed that with relatively simple yet effective attacks based on discrete optimization, it is possible to generate adversarial prompts that bypass moderation and alignment of the models. This vulnerability to adversarial prompts underscores a significant concern regarding the robustness and reliability of LLMs. Our work aims to address this concern by introducing a novel approach to detecting adversarial prompts at a token level, leveraging the LLM's capability to predict the next token's probability. We measure the degree of the model's perplexity, where tokens predicted with high probability are considered normal, and those exhibi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21307;&#23398;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#21512;&#20316;(MC)&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#35282;&#33394;&#25198;&#28436;&#35774;&#32622;&#20013;&#21442;&#19982;&#21327;&#20316;&#22810;&#36718;&#35752;&#35770;&#65292;&#20174;&#32780;&#25552;&#39640;LLM&#30340;&#29087;&#32451;&#31243;&#24230;&#21644;&#25512;&#29702;&#33021;&#21147;</title><link>https://arxiv.org/abs/2311.10537</link><description>&lt;p&gt;
MedAgents: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;-shot&#21307;&#23398;&#25512;&#29702;&#30340;&#21512;&#20316;&#32773;
&lt;/p&gt;
&lt;p&gt;
MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10537
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21307;&#23398;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#21512;&#20316;(MC)&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#35282;&#33394;&#25198;&#28436;&#35774;&#32622;&#20013;&#21442;&#19982;&#21327;&#20316;&#22810;&#36718;&#35752;&#35770;&#65292;&#20174;&#32780;&#25552;&#39640;LLM&#30340;&#29087;&#32451;&#31243;&#24230;&#21644;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23613;&#31649;&#22312;&#21508;&#31181;&#36890;&#29992;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#38754;&#20020;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21307;&#23398;&#39046;&#22495;&#30340;&#36328;&#23398;&#31185;&#21512;&#20316;(MC)&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#22312;&#35282;&#33394;&#25198;&#28436;&#35774;&#32622;&#20013;&#21442;&#19982;&#21327;&#20316;&#22810;&#36718;&#35752;&#35770;&#65292;&#20174;&#32780;&#25552;&#39640;LLM&#30340;&#29087;&#32451;&#31243;&#24230;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#21253;&#25324;&#20116;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#25910;&#38598;&#39046;&#22495;&#19987;&#23478;&#12289;&#25552;&#20986;&#20010;&#21035;&#20998;&#26512;&#12289;&#23558;&#36825;&#20123;&#20998;&#26512;&#24635;&#32467;&#25104;&#25253;&#21578;&#12289;&#22312;&#35752;&#35770;&#20013;&#21453;&#22797;&#36845;&#20195;&#30452;&#21040;&#36798;&#25104;&#20849;&#35782;&#65292;&#26368;&#32456;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#38646;-shot&#24773;&#26223;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#20855;&#26377;&#36866;&#29992;&#24615;&#12290;&#22312;&#20061;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10537v2 Announce Type: replace-cross  Abstract: Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose a novel Multi-disciplinary Collaboration (MC) framework for the medical domain that leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#22312;&#29983;&#25104;&#22810;&#20803;&#21270;&#35266;&#28857;&#21644;&#29702;&#30001;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;LLMs&#20013;&#26368;&#22823;&#31243;&#24230;&#25552;&#21462;&#22810;&#26679;&#24615;&#35266;&#28857;&#30340;&#26032;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.09799</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22810;&#20803;&#21270;&#35266;&#28857;&#21040;&#20309;&#31181;&#31243;&#24230;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Can We Extract Diverse Perspectives from Large Language Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#22312;&#29983;&#25104;&#22810;&#20803;&#21270;&#35266;&#28857;&#21644;&#29702;&#30001;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20174;LLMs&#20013;&#26368;&#22823;&#31243;&#24230;&#25552;&#21462;&#22810;&#26679;&#24615;&#35266;&#28857;&#30340;&#26032;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#38598;&#22810;&#26679;&#21270;&#30340;&#20154;&#31867;&#35266;&#28857;&#25104;&#26412;&#39640;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#21512;&#20316;&#21162;&#21147;&#36235;&#21183;&#34920;&#26126;&#65292;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20043;&#38388;&#36827;&#34892;&#21512;&#20316;&#20026;&#29983;&#25104;&#22810;&#26679;&#21270;&#25968;&#25454;&#25552;&#20379;&#20102;&#28508;&#22312;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#22312;&#20027;&#35266;&#35805;&#39064;&#19978;&#29983;&#25104;&#22810;&#20803;&#21270;&#35266;&#28857;&#30340;&#33021;&#21147;&#31243;&#24230;&#20173;&#26159;&#19968;&#20010;&#26410;&#34987;&#25506;&#35752;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#22312;&#29983;&#25104;&#22810;&#20803;&#21270;&#35266;&#28857;&#21644;&#29702;&#30001;&#65288;&#20363;&#22914;&#31038;&#20250;&#35268;&#33539;&#21644;&#36777;&#35770;&#25991;&#26412;&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;LLMs&#20013;&#25552;&#21462;&#26368;&#22823;&#22810;&#26679;&#24615;&#20449;&#24687;&#30340;&#26032;&#38382;&#39064;&#12290;&#21463;&#20154;&#31867;&#36890;&#36807;&#20854;&#20215;&#20540;&#35266;&#21457;&#23637;&#35266;&#28857;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26631;&#20934;&#30340;&#25552;&#31034;&#25216;&#26415;&#26469;&#30830;&#31435;&#22810;&#26679;&#21270;&#35266;&#28857;&#12290;&#20026;&#20102;&#20102;&#35299;&#25105;&#20204;&#33021;&#20174;LLMs&#20013;&#25552;&#21462;&#22810;&#20803;&#21270;&#35266;&#28857;&#21040;&#20309;&#31181;&#31243;&#24230;&#65292;&#25110;&#32773;&#31216;&#20043;&#20026;&#22810;&#26679;&#24615;&#35206;&#30422;&#29575;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36880;&#27493;&#22238;&#24518;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#36845;&#20195;&#26041;&#24335;&#19979;&#20174;&#27169;&#22411;&#20013;&#29983;&#25104;&#26356;&#22810;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09799v2 Announce Type: replace  Abstract: Collecting diverse human opinions is costly and challenging. This leads to a recent trend in collaborative efforts between humans and Large Language Models (LLMs) for generating diverse data, offering potential scalable and efficient solutions. However, the extent of LLMs' capability to generate diverse perspectives on subjective topics remains an unexplored question. In this study, we investigate LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts. We formulate a new problem of maximum diversity extraction from LLMs. Motivated by how humans develop their opinions through their values, we propose a criteria-based prompting technique to ground diverse opinions. To see how far we can extract diverse perspectives from LLMs, or called diversity coverage, we employ a step-by-step recall prompting for generating more outputs from the model in an iterative mann
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#35780;&#20272;&#25351;&#26631;&#22312;&#24635;&#32467;&#20219;&#21153;&#20013;&#26159;&#21542;&#20250;&#23545;&#30456;&#21516;&#30340;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#34920;&#29616;&#20986;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#22312;&#26080;&#21442;&#32771;&#24773;&#20917;&#19979;&#20351;&#29992;&#26102;&#20559;&#35265;&#23588;&#20026;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2311.09766</link><description>&lt;p&gt;
LLMs&#20316;&#20026;&#33258;&#24651;&#35780;&#20272;&#32773;&#65306;&#24403;&#33258;&#25105;&#33192;&#32960;&#24433;&#21709;&#35780;&#20272;&#20998;&#25968;
&lt;/p&gt;
&lt;p&gt;
LLMs as Narcissistic Evaluators: When Ego Inflates Evaluation Scores
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#35780;&#20272;&#25351;&#26631;&#22312;&#24635;&#32467;&#20219;&#21153;&#20013;&#26159;&#21542;&#20250;&#23545;&#30456;&#21516;&#30340;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#34920;&#29616;&#20986;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#22312;&#26080;&#21442;&#32771;&#24773;&#20917;&#19979;&#20351;&#29992;&#26102;&#20559;&#35265;&#23588;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09766v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#29983;&#25104;&#25991;&#26412;&#20869;&#23481;&#30340;&#33258;&#21160;&#35780;&#20272;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#32493;&#25361;&#25112;&#12290;&#37492;&#20110;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20542;&#21521;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#21019;&#36896;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#20219;&#21153;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#35780;&#20272;&#25351;&#26631;&#26159;&#21542;&#20250;&#22266;&#26377;&#22320;&#34920;&#29616;&#20986;&#20559;&#21521;&#20110;&#30001;&#30456;&#21516;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20559;&#35265;&#65311;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#30693;&#21517;&#30340;&#22522;&#20110;LM&#30340;&#35780;&#20272;&#25351;&#26631;&#65288;&#20363;&#22914;BARTScore&#12289;T5Score&#21644;GPTScore&#65289;&#22312;&#24635;&#32467;&#20219;&#21153;&#20013;&#26159;&#21542;&#23545;&#20854;&#21508;&#33258;&#30340;&#22522;&#30784;LM&#34920;&#29616;&#20986;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#28508;&#22312;&#20559;&#35265;&#65292;&#29305;&#21035;&#26159;&#24403;&#36825;&#20123;&#35780;&#20272;&#25351;&#26631;&#22312;&#26080;&#21442;&#32771;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#19988;&#19981;&#21033;&#29992;&#40644;&#37329;&#25688;&#35201;&#26102;&#65292;&#36825;&#31181;&#20559;&#35265;&#23588;&#20026;&#26174;&#33879;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;&#36890;&#36807;&#29983;&#25104;&#25991;&#26412;&#33719;&#24471;&#30340;&#35780;&#20272;&#32467;&#26524;&#21487;&#33021;&#20250;&#21463;&#21040;&#33258;&#25105;&#20559;&#35823;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09766v2 Announce Type: replace  Abstract: Automatic evaluation of generated textual content presents an ongoing challenge within the field of NLP. Given the impressive capabilities of modern language models (LMs) across diverse NLP tasks, there is a growing trend to employ these models in creating innovative evaluation metrics for automated assessment of generation tasks. This paper investigates a pivotal question: Do language model-driven evaluation metrics inherently exhibit bias favoring texts generated by the same underlying language model? Specifically, we assess whether prominent LM-based evaluation metrics (e.g. BARTScore, T5Score, and GPTScore) demonstrate a favorable bias toward their respective underlying LMs in the context of summarization tasks. Our findings unveil a latent bias, particularly pronounced when such evaluation metrics are used in an reference-free manner without leveraging gold summaries. These results underscore that assessments provided by generat
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#33258;&#30456;&#30683;&#30462;&#30340;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#28041;&#21450;&#19978;&#19979;&#25991;&#20449;&#24687;&#29702;&#35299;&#25110;&#24120;&#35782;&#30340;&#20219;&#21153;&#20013;&#32463;&#24120;&#23384;&#22312;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;&#39640;&#20934;&#30830;&#24615;&#24182;&#19981;&#24635;&#26159;&#23545;&#24212;&#36739;&#20302;&#30340;&#33258;&#30456;&#30683;&#30462;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.09603</link><description>&lt;p&gt;
&#33258;&#30456;&#30683;&#30462;&#25512;&#29702;&#35780;&#20272;&#19982;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Contradictory Reasoning Evaluation and Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09603
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#33258;&#30456;&#30683;&#30462;&#30340;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#28041;&#21450;&#19978;&#19979;&#25991;&#20449;&#24687;&#29702;&#35299;&#25110;&#24120;&#35782;&#30340;&#20219;&#21153;&#20013;&#32463;&#24120;&#23384;&#22312;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;&#39640;&#20934;&#30830;&#24615;&#24182;&#19981;&#24635;&#26159;&#23545;&#24212;&#36739;&#20302;&#30340;&#33258;&#30456;&#30683;&#30462;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#22823;&#37327;&#24037;&#20316;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#35768;&#22810;&#25552;&#20986;&#30340;&#19979;&#28216;&#25512;&#29702;&#20219;&#21153;&#20027;&#35201;&#20851;&#27880;&#24615;&#33021;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;1&#65289;&#25512;&#29702;&#36136;&#37327;&#26377;&#22810;&#21487;&#38752;&#65292;2&#65289;&#27169;&#22411;&#33021;&#21542;&#26816;&#27979;&#21040;&#19981;&#21487;&#38752;&#30340;&#25512;&#29702;&#65311;&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#30456;&#30683;&#30462;&#65288;Self-Contra&#65289;&#25512;&#29702;&#65292;&#21363;&#27169;&#22411;&#25512;&#29702;&#19981;&#25903;&#25345;&#39044;&#27979;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22235;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;Self-Contra&#29575;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;&#33258;&#30456;&#30683;&#30462;&#25512;&#29702;&#30340;&#26356;&#32454;&#31890;&#24230;&#31867;&#21035;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36827;&#34892;&#28041;&#21450;&#19978;&#19979;&#25991;&#20449;&#24687;&#29702;&#35299;&#25110;&#24120;&#35782;&#30340;&#25512;&#29702;&#20219;&#21153;&#26102;&#32463;&#24120;&#33258;&#30456;&#30683;&#30462;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#24182;&#19981;&#19968;&#23450;&#23545;&#24212;&#26356;&#20302;&#30340;&#33258;&#30456;&#30683;&#30462;&#29575;&#12290;&#27169;&#22411;&#21487;&#33021;&#20250;&#20135;&#29983;&#27491;&#30830;&#31572;&#26696;&#65292;&#20294;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#33021;&#20250;&#37319;&#21462;&#25463;&#24452;&#25110;&#24573;&#30053;&#19978;&#19979;&#25991;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09603v2 Announce Type: replace  Abstract: In a plethora of recent work, large language models (LLMs) demonstrated impressive reasoning ability, but many proposed downstream reasoning tasks focus on performance-wise evaluation. Two fundamental questions persist: 1) how reliable is the quality of reasoning, and 2) can models detect unreliable reasoning? In this paper, we investigate self-contradictory (Self-Contra) reasoning, where the model reasoning does not support predictions. To address 1), we assess the Self-Contra rate across four datasets and delve into finer-grained categories of Self-Contra reasoning. We find that LLMs often contradict themselves when performing reasoning tasks that involve contextual information understanding or commonsense. Importantly, a higher accuracy does not necessarily correspond to a lower Self-Contra rate. The model may appear to generate correct answers but it may take shortcuts in reasoning or skip over contextual evidence, thereby displa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DocLens&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#32452;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20854;&#23545;&#21307;&#23398;&#25991;&#26412;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#20154;&#31867;&#30740;&#31350;&#34920;&#26126;&#20854;&#22312;&#19982;&#21307;&#23398;&#19987;&#23478;&#21028;&#26029;&#30340;&#19968;&#33268;&#24615;&#19978;&#20248;&#20110;&#29616;&#26377;&#25351;&#26631;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#25913;&#36827;&#24320;&#28304;&#35780;&#20272;&#32773;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.09581</link><description>&lt;p&gt;
DocLens: &#22810;&#26041;&#38754;&#32454;&#31890;&#24230;&#35780;&#20272;&#21307;&#23398;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DocLens: Multi-aspect Fine-grained Evaluation for Medical Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DocLens&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#32452;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20854;&#23545;&#21307;&#23398;&#25991;&#26412;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#20154;&#31867;&#30740;&#31350;&#34920;&#26126;&#20854;&#22312;&#19982;&#21307;&#23398;&#19987;&#23478;&#21028;&#26029;&#30340;&#19968;&#33268;&#24615;&#19978;&#20248;&#20110;&#29616;&#26377;&#25351;&#26631;&#65292;&#21516;&#26102;&#25351;&#20986;&#20102;&#25913;&#36827;&#24320;&#28304;&#35780;&#20272;&#32773;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25991;&#26412;&#29983;&#25104;&#26088;&#22312;&#21327;&#21161;&#34892;&#25919;&#24037;&#20316;&#24182;&#31361;&#20986;&#35201;&#28857;&#20449;&#24687;&#20197;&#25903;&#25345;&#20915;&#31574;&#21046;&#23450;&#12290;&#20026;&#20102;&#21453;&#26144;&#21307;&#23398;&#25991;&#26412;&#30340;&#29305;&#23450;&#35201;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20110;&#22312;&#32454;&#31890;&#24230;&#27700;&#24179;&#19978;&#35780;&#20272;&#29983;&#25104;&#25991;&#26412;&#30340;&#23436;&#25972;&#24615;&#12289;&#31616;&#26126;&#24615;&#21644;&#24402;&#22240;&#24615;&#12290;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#21487;&#20197;&#30001;&#21508;&#31181;&#31867;&#22411;&#30340;&#35780;&#20272;&#32773;&#35745;&#31639;&#65292;&#21253;&#25324;&#36981;&#24490;&#35828;&#26126;&#65288;&#19987;&#26377;&#21644;&#24320;&#28304;&#65289;&#21644;&#30417;&#30563;&#34164;&#28085;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#35780;&#20272;&#32773;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25152;&#24471;&#20986;&#30340;&#26694;&#26550;DocLens&#30340;&#26377;&#25928;&#24615;&#65306;&#20020;&#24202;&#35760;&#24405;&#29983;&#25104;&#12289;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#21644;&#24739;&#32773;&#38382;&#39064;&#25688;&#35201;&#12290;&#19968;&#39033;&#20840;&#38754;&#30340;&#20154;&#31867;&#30740;&#31350;&#26174;&#31034;&#65292;DocLens&#19982;&#21307;&#23398;&#19987;&#23478;&#30340;&#21028;&#26029;&#20043;&#38388;&#23384;&#22312;&#30456;&#24403;&#39640;&#30340;&#19968;&#33268;&#24615;&#65292;&#39640;&#20110;&#29616;&#26377;&#25351;&#26631;&#12290;&#32467;&#26524;&#36824;&#31361;&#20986;&#26174;&#31034;&#20102;&#25913;&#36827;&#24320;&#28304;&#35780;&#20272;&#32773;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09581v2 Announce Type: replace  Abstract: Medical text generation aims to assist with administrative work and highlight salient information to support decision-making. To reflect the specific requirements of medical text, in this paper, we propose a set of metrics to evaluate the completeness, conciseness, and attribution of the generated text at a fine-grained level. The metrics can be computed by various types of evaluators including instruction-following (both proprietary and open-source) and supervised entailment models. We demonstrate the effectiveness of the resulting framework, DocLens, with three evaluators on three tasks: clinical note generation, radiology report summarization, and patient question summarization. A comprehensive human study shows that DocLens exhibits substantially higher agreement with the judgments of medical experts than existing metrics. The results also highlight the need to improve open-source evaluators and suggest potential directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TextEE&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20107;&#20214;&#25552;&#21462;&#30340;&#26631;&#20934;&#21270;&#12289;&#20844;&#24179;&#21644;&#21487;&#37325;&#22797;&#30340;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2311.09562</link><description>&lt;p&gt;
TextEE&#65306;&#20107;&#20214;&#25552;&#21462;&#20013;&#30340;&#22522;&#20934;&#12289;&#37325;&#26032;&#35780;&#20272;&#12289;&#21453;&#24605;&#21644;&#26410;&#26469;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
TextEE: Benchmark, Reevaluation, Reflections, and Future Challenges in Event Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TextEE&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20107;&#20214;&#25552;&#21462;&#30340;&#26631;&#20934;&#21270;&#12289;&#20844;&#24179;&#21644;&#21487;&#37325;&#22797;&#30340;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#35780;&#20272;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25552;&#21462;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#23545;&#35780;&#20272;&#38382;&#39064;&#30340;&#20851;&#27880;&#65292;&#34920;&#26126;&#25253;&#21578;&#30340;&#20998;&#25968;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#30495;&#23454;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#35299;&#20915;&#20102;&#35780;&#20272;&#25361;&#25112;&#65292;&#21253;&#25324;&#30001;&#20110;&#19981;&#21516;&#30340;&#25968;&#25454;&#20551;&#35774;&#25110;&#39044;&#22788;&#29702;&#27493;&#39588;&#32780;&#24341;&#36215;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#30446;&#21069;&#35780;&#20272;&#26694;&#26550;&#30340;&#19981;&#36275;&#21487;&#33021;&#24341;&#20837;&#25968;&#25454;&#38598;&#25110;&#25968;&#25454;&#20998;&#21106;&#20559;&#35265;&#65292;&#20197;&#21450;&#19968;&#20123;&#20808;&#21069;&#26041;&#27861;&#30340;&#20302;&#21487;&#37325;&#22797;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TextEE&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20107;&#20214;&#25552;&#21462;&#30340;&#26631;&#20934;&#21270;&#12289;&#20844;&#24179;&#21644;&#21487;&#37325;&#22797;&#30340;&#22522;&#20934;&#12290;TextEE&#21253;&#25324;&#26631;&#20934;&#21270;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#33050;&#26412;&#21644;&#29992;&#20110;&#36328;&#19971;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;14&#20010;&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#20998;&#21106;&#65292;&#24182;&#21253;&#25324;14&#31181;&#26368;&#36817;&#30340;&#26041;&#27861;&#65292;&#36827;&#34892;&#20840;&#38754;&#30340;&#22522;&#20934;&#37325;&#26032;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#30340;TextEE&#22522;&#20934;&#19978;&#30340;&#20116;&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09562v2 Announce Type: replace  Abstract: Event extraction has gained considerable interest due to its wide-ranging applications. However, recent studies draw attention to evaluation issues, suggesting that reported scores may not accurately reflect the true performance. In this work, we identify and address evaluation challenges, including inconsistency due to varying data assumptions or preprocessing steps, the insufficiency of current evaluation frameworks that may introduce dataset or data split bias, and the low reproducibility of some previous approaches. To address these challenges, we present TextEE, a standardized, fair, and reproducible benchmark for event extraction. TextEE comprises standardized data preprocessing scripts and splits for 14 datasets spanning seven diverse domains and includes 14 recent methodologies, conducting a comprehensive benchmark reevaluation. We also evaluate five varied large language models on our TextEE benchmark and demonstrate how the
&lt;/p&gt;</description></item><item><title>Symbol-LLM &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#21644;&#26694;&#26550;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31526;&#21495;&#25968;&#25454;&#27880;&#20837;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#25429;&#25417;&#31526;&#21495;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#21644;&#20419;&#36827;&#21327;&#21516;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.09278</link><description>&lt;p&gt;
Symbol-LLM: &#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#31526;&#21495;&#20013;&#24515;&#25509;&#21475;
&lt;/p&gt;
&lt;p&gt;
Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09278
&lt;/p&gt;
&lt;p&gt;
Symbol-LLM &#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#21644;&#26694;&#26550;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31526;&#21495;&#25968;&#25454;&#27880;&#20837;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#25429;&#25417;&#31526;&#21495;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#21644;&#20419;&#36827;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20986;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#25991;&#26412;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#22312;&#29702;&#35299;&#21644;&#34920;&#36798;&#36229;&#20986;&#33258;&#28982;&#35821;&#35328;&#33539;&#22260;&#30340;&#19990;&#30028;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;(&#20363;&#22914;&#21270;&#23398;&#20998;&#23376;&#24335;)&#12290;&#30452;&#25509;&#23558;&#19968;&#31995;&#21015;&#31526;&#21495;&#25968;&#25454;&#27880;&#20837;&#21040;LLMs&#30340;&#35757;&#32451;&#20013;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#24573;&#35270;&#20102;&#19981;&#21516;&#31526;&#21495;&#23478;&#26063;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#65292;&#20063;&#24573;&#35270;&#20102;&#33258;&#28982;&#25968;&#25454;&#21644;&#31526;&#21495;&#25968;&#25454;&#20043;&#38388;&#24179;&#34913;&#28151;&#21512;&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#25968;&#25454;&#21644;&#26694;&#26550;&#20004;&#20010;&#26041;&#38754;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#24341;&#20837;&#20102;Symbol-LLM&#31995;&#21015;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;34&#20010;&#20219;&#21153;&#24182;&#28085;&#30422;&#32422;20&#20010;&#19981;&#21516;&#31526;&#21495;&#23478;&#26063;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25429;&#25417;&#31526;&#21495;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#24182;&#20419;&#36827;&#31526;&#21495;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#20004;&#38454;&#27573;&#35843;&#20248;&#26694;&#26550;&#25104;&#21151;&#22320;&#27880;&#20837;&#20102;&#31526;&#21495;&#30693;&#35782;&#32780;&#19981;&#20250;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09278v2 Announce Type: replace-cross  Abstract: Although Large Language Models (LLMs) demonstrate remarkable ability in processing and generating human-like text, they do have limitations when it comes to comprehending and expressing world knowledge that extends beyond the boundaries of natural language(e.g., chemical molecular formula). Injecting a collection of symbolic data directly into the training of LLMs can be problematic, as it disregards the synergies among different symbolic families and overlooks the need for a balanced mixture of natural and symbolic data. In this work, we tackle these challenges from both a data and framework perspective and introduce Symbol-LLM series models. First, we curated a data collection consisting of 34 tasks and incorporating approximately 20 distinct symbolic families, intending to capture the interrelations and foster synergies between symbols. Then, a two-stage tuning framework succeeds in injecting symbolic knowledge without loss 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#37096;&#20998;&#25490;&#24207;&#26469;&#20248;&#21270;LLMs&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#29305;&#23450;&#20219;&#21153;&#20505;&#36873;&#21709;&#24212;&#27744;&#20013;&#30340;&#26368;&#20339;&#21709;&#24212;&#65292;&#20174;&#32780;&#25913;&#21892;&#21709;&#24212;&#29983;&#25104;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.09136</link><description>&lt;p&gt;
&#29992;&#37096;&#20998;&#25490;&#24207;&#23545;LLM&#21709;&#24212;&#36827;&#34892;&#25490;&#21517;&#20197;&#25913;&#21892;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Rescue: Ranking LLM Responses with Partial Ordering to Improve Response Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09136
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#37096;&#20998;&#25490;&#24207;&#26469;&#20248;&#21270;LLMs&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#29305;&#23450;&#20219;&#21153;&#20505;&#36873;&#21709;&#24212;&#27744;&#20013;&#30340;&#26368;&#20339;&#21709;&#24212;&#65292;&#20174;&#32780;&#25913;&#21892;&#21709;&#24212;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#21046;LLMs&#20197;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#28041;&#21450;&#23558;&#26377;&#25928;&#21709;&#24212;&#19982;&#38169;&#35823;&#21709;&#24212;&#21306;&#20998;&#24320;&#12290;&#36825;&#31181;&#25216;&#33021;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#26469;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#20219;&#21153;&#26469;&#35828;&#65292;&#33719;&#21462;&#19987;&#23478;&#27880;&#37322;&#30340;&#20559;&#22909;&#25968;&#25454;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25490;&#21517;&#24230;&#37327;&#26469;&#20248;&#21270;LLMs&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#35757;&#32451;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#20026;&#29305;&#23450;&#20219;&#21153;&#21019;&#24314;&#30340;&#20505;&#36873;&#21709;&#24212;&#27744;&#20013;&#30340;&#26368;&#20339;&#21709;&#24212;&#12290;&#25105;&#20204;&#20027;&#24352;&#37319;&#29992;&#37096;&#20998;&#25490;&#24207;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#23436;&#20840;&#25490;&#24207;&#65292;&#22240;&#20026;&#23601;&#20505;&#36873;&#21709;&#24212;&#30340;&#23436;&#32654;&#39034;&#24207;&#36798;&#25104;&#20849;&#35782;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30340;&#37096;&#20998;&#25490;&#24207;&#26356;&#21152;&#31283;&#20581;&#65292;&#23545;&#22122;&#22768;&#30340;&#25935;&#24863;&#24615;&#36739;&#20302;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#26377;&#38480;&#30340;&#20154;&#31867;&#27880;&#37322;&#25110;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20934;&#25968;&#25454;&#38598;&#27979;&#35797;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#25913;&#36827;&#21709;&#24212;&#29983;&#25104;&#33021;&#21147;&#65292;&#21253;&#25324;&#26368;&#26032;&#30340;&#22810;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09136v2 Announce Type: replace  Abstract: Customizing LLMs for a specific task involves distinguishing effective responses from erroneous ones. This skill can be developed using supervised fine-tuning with extensive human preference data. However, obtaining expert-annotated preference data is expensive for most tasks. In this paper, we present a novel method to optimize LLMs using ranking metrics. This method trains the model to prioritize the best responses from a pool of candidates created for a particular task. Rather than a traditional full ordering, we advocate for a partial ordering, as achieving consensus on the perfect order of candidate responses can be challenging. Our partial ordering is more robust, less sensitive to noise, and can be achieved with limited human annotations or through heuristic methods. We test our system's improved response generation ability using benchmark datasets, including the latest multi-document question answering task. We conduct ablati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25506;&#27979;&#35821;&#35328;&#27169;&#22411;&#20013;&#31038;&#20250;&#20559;&#35265;&#30340;&#21407;&#21019;&#26694;&#26550;&#65292;&#21253;&#25324;&#23545;&#19968;&#33324;&#20851;&#32852;&#21644;&#31038;&#20250;&#31867;&#21035;&#12289;&#36523;&#20221;&#20197;&#21450;&#21051;&#26495;&#21360;&#35937;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2311.09090</link><description>&lt;p&gt;
&#31038;&#20250;&#20559;&#35265;&#25506;&#27979;&#65306;&#35821;&#35328;&#27169;&#22411;&#30340;&#20844;&#24179;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Social Bias Probing: Fairness Benchmarking for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25506;&#27979;&#35821;&#35328;&#27169;&#22411;&#20013;&#31038;&#20250;&#20559;&#35265;&#30340;&#21407;&#21019;&#26694;&#26550;&#65292;&#21253;&#25324;&#23545;&#19968;&#33324;&#20851;&#32852;&#21644;&#31038;&#20250;&#31867;&#21035;&#12289;&#36523;&#20221;&#20197;&#21450;&#21051;&#26495;&#21360;&#35937;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#32534;&#30721;&#20102;&#21508;&#31181;&#31038;&#20250;&#20559;&#35265;&#65292;&#36825;&#24102;&#26469;&#20102;&#19979;&#28216;&#39118;&#38505;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#25506;&#27979;&#35821;&#35328;&#27169;&#22411;&#20013;&#31038;&#20250;&#20559;&#35265;&#30340;&#21407;&#21019;&#26694;&#26550;&#65292;&#21253;&#25324;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#33324;&#20851;&#32852;&#20197;&#21450;&#31038;&#20250;&#31867;&#21035;&#12289;&#36523;&#20221;&#21644;&#21051;&#26495;&#21360;&#35937;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09090v2 Announce Type: replace  Abstract: Large language models have been shown to encode a variety of social biases, which carries the risk of downstream harms. While the impact of these biases has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, offering a constrained view of the nature of societal biases within language models. In this paper, we propose an original framework for probing language models for societal biases. We collect a probing dataset to analyze language models' general associations, as well as along the axes of societal categories, identities, and stereotypes. To this end, we leverage a novel perplexity-based fairness score. We curate a large-scale benchmarking dataset addressing drawbacks and limitations of existing fairness collections, expanding to a variety of different identities and stereotypes. When comparing our methodology with prior work, we demonstrate that biases within langua
&lt;/p&gt;</description></item><item><title>Safer-Instruct&#36890;&#36807;&#21453;&#21521;&#25351;&#23548;&#35843;&#25972;&#12289;&#25351;&#23548;&#24863;&#24212;&#21644;&#19987;&#23478;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26500;&#24314;&#22823;&#35268;&#27169;&#20559;&#22909;&#25968;&#25454;&#30340;&#30446;&#30340;&#65292;&#20174;&#32780;&#22312;&#27809;&#26377;&#20154;&#24037;&#26631;&#27880;&#32773;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2311.08685</link><description>&lt;p&gt;
Safer-Instruct: &#20351;&#29992;&#33258;&#21160;&#21270;&#20559;&#22909;&#25968;&#25454;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Safer-Instruct: Aligning Language Models with Automated Preference Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08685
&lt;/p&gt;
&lt;p&gt;
Safer-Instruct&#36890;&#36807;&#21453;&#21521;&#25351;&#23548;&#35843;&#25972;&#12289;&#25351;&#23548;&#24863;&#24212;&#21644;&#19987;&#23478;&#27169;&#22411;&#35780;&#20272;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26500;&#24314;&#22823;&#35268;&#27169;&#20559;&#22909;&#25968;&#25454;&#30340;&#30446;&#30340;&#65292;&#20174;&#32780;&#22312;&#27809;&#26377;&#20154;&#24037;&#26631;&#27880;&#32773;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#37325;&#35201;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#20026;RLHF&#26631;&#27880;&#20559;&#22909;&#25968;&#25454;&#26159;&#19968;&#39033;&#36164;&#28304;&#23494;&#38598;&#19988;&#38656;&#35201;&#21019;&#36896;&#21147;&#30340;&#36807;&#31243;&#65292;&#32780;&#29616;&#26377;&#30340;&#33258;&#21160;&#29983;&#25104;&#26041;&#27861;&#22312;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Safer-Instruct&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#26500;&#24314;&#22823;&#35268;&#27169;&#20559;&#22909;&#25968;&#25454;&#30340;&#20840;&#26032;&#27969;&#27700;&#32447;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#21453;&#21521;&#25351;&#23548;&#35843;&#25972;&#12289;&#25351;&#23548;&#24863;&#24212;&#21644;&#19987;&#23478;&#27169;&#22411;&#35780;&#20272;&#65292;&#20197;&#39640;&#25928;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#32773;&#12290;&#20026;&#20102;&#39564;&#35777;Safer-Instruct&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#35813;&#27969;&#27700;&#32447;&#24212;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#20559;&#22909;&#25968;&#25454;&#38598;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#22312;&#36825;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;Alpaca&#27169;&#22411;&#19981;&#20165;&#23637;&#31034;&#20986;&#26356;&#22909;&#30340;&#26080;&#23475;&#24615;&#65292;&#36824;&#34920;&#29616;&#20986;&#20248;&#20110;&#22312;&#20154;&#24037;&#26631;&#27880;&#30340;&#23433;&#20840;&#20559;&#22909;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#20445;&#25345;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08685v2 Announce Type: replace-cross  Abstract: Reinforcement learning from human feedback (RLHF) is a vital strategy for enhancing model capability in language models. However, annotating preference data for RLHF is a resource-intensive and creativity-demanding process, while existing automatic generation methods face limitations in data diversity and quality. In response, we present Safer-Instruct, a novel pipeline for automatically constructing large-scale preference data. Our approach leverages reversed instruction tuning, instruction induction, and expert model evaluation to efficiently generate high-quality preference data without human annotators. To verify the effectiveness of Safer-Instruct, we apply the pipeline to construct a safety preference dataset as a case study. Finetuning an Alpaca model on this synthetic dataset not only demonstrates improved harmlessness but also outperforms models fine-tuned on human-annotated safety preference data, all the while mainta
&lt;/p&gt;</description></item><item><title>DALA&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24863;&#30693;&#30340;LoRA&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#21892;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#65292;&#24182;&#24341;&#20837;&#20102;&#38750;&#21487;&#26816;&#27979;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;NASR&#65289;&#35780;&#20215;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2311.08598</link><description>&lt;p&gt;
DALA: &#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24863;&#30693;&#30340;&#38754;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DALA: A Distribution-Aware LoRA-Based Adversarial Attack against Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08598
&lt;/p&gt;
&lt;p&gt;
DALA&#26159;&#19968;&#31181;&#22522;&#20110;&#20998;&#24067;&#24863;&#30693;&#30340;LoRA&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65292;&#26088;&#22312;&#25913;&#21892;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25552;&#39640;&#25915;&#20987;&#25928;&#26524;&#65292;&#24182;&#24341;&#20837;&#20102;&#38750;&#21487;&#26816;&#27979;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;NASR&#65289;&#35780;&#20215;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#24615;&#25915;&#20987;&#36827;&#34892;&#25805;&#32437;&#65292;&#36825;&#20123;&#25915;&#20987;&#22312;&#36755;&#20837;&#25968;&#25454;&#20013;&#24341;&#20837;&#24494;&#22937;&#30340;&#25200;&#21160;&#12290;&#36817;&#26399;&#30340;&#25915;&#20987;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#30456;&#23545;&#36739;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;ASR&#65289;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#29983;&#25104;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#19982;&#21407;&#22987;&#26679;&#26412;&#30456;&#27604;&#20855;&#26377;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#20123;&#23545;&#25239;&#24615;&#26679;&#26412;&#34920;&#29616;&#20986;&#38477;&#20302;&#30340;&#32622;&#20449;&#27700;&#24179;&#21644;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#30340;&#36739;&#22823;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#24456;&#23481;&#26131;&#34987;&#31616;&#21333;&#30340;&#26816;&#27979;&#26041;&#27861;&#26816;&#27979;&#20986;&#26469;&#65292;&#38477;&#20302;&#20102;&#27492;&#31867;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LoRA&#30340;&#20998;&#24067;&#24863;&#30693;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#65288;DALA&#65289;&#12290;DALA&#32771;&#34385;&#23545;&#25239;&#24615;&#26679;&#26412;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#20197;&#25552;&#39640;&#22312;&#26816;&#27979;&#26041;&#27861;&#19979;&#30340;&#25915;&#20987;&#25928;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20215;&#24230;&#37327;&#65292;&#38750;&#21487;&#26816;&#27979;&#25915;&#20987;&#25104;&#21151;&#29575;&#65288;NASR&#65289;&#65292;&#23427;&#34701;&#21512;&#20102;ASR&#21644;&#21487;&#26816;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08598v2 Announce Type: replace  Abstract: Language models (LMs) can be manipulated by adversarial attacks, which introduce subtle perturbations to input data. While recent attack methods can achieve a relatively high attack success rate (ASR), we've observed that the generated adversarial examples have a different data distribution compared with the original examples. Specifically, these adversarial examples exhibit reduced confidence levels and greater divergence from the training data distribution. Consequently, they are easy to detect using straightforward detection methods, diminishing the efficacy of such attacks. To address this issue, we propose a Distribution-Aware LoRA-based Adversarial Attack (DALA) method. DALA considers distribution shifts of adversarial examples to improve the attack's effectiveness under detection methods. We further design a novel evaluation metric, the Non-detectable Attack Success Rate (NASR), which integrates both ASR and detectability for 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#36234;&#29425;&#25552;&#31034;&#30340;&#33258;&#21160;&#26694;&#26550;ReNeLLM&#65292;&#26174;&#33879;&#25552;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2311.08268</link><description>&lt;p&gt;
&#20266;&#35013;&#25104;&#32650;&#30340;&#29436;&#65306;&#26222;&#36941;&#30340;&#23884;&#22871;&#36234;&#29425;&#25552;&#31034;&#21487;&#20197;&#36731;&#26494;&#24858;&#24324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08268
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#36234;&#29425;&#25552;&#31034;&#30340;&#33258;&#21160;&#26694;&#26550;ReNeLLM&#65292;&#26174;&#33879;&#25552;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#26088;&#22312;&#25552;&#20379;&#26377;&#29992;&#21644;&#23433;&#20840;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#34987;&#31216;&#20026;&#8220;&#36234;&#29425;&#8221;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#21487;&#20197;&#35268;&#36991;&#20445;&#38556;&#25514;&#26045;&#65292;&#23548;&#33268;LLMs&#29983;&#25104;&#28508;&#22312;&#26377;&#23475;&#20869;&#23481;&#12290;&#25506;&#32034;&#36234;&#29425;&#25552;&#31034;&#21487;&#20197;&#24110;&#21161;&#26356;&#22909;&#22320;&#25581;&#31034;LLMs&#30340;&#24369;&#28857;&#65292;&#24182;&#36827;&#19968;&#27493;&#24341;&#23548;&#25105;&#20204;&#23433;&#20840;&#22320;&#20445;&#25252;&#23427;&#20204;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#36234;&#29425;&#26041;&#27861;&#35201;&#20040;&#36973;&#21463;&#22797;&#26434;&#30340;&#25163;&#24037;&#35774;&#35745;&#65292;&#35201;&#20040;&#38656;&#35201;&#22312;&#20854;&#20182;&#30333;&#30418;&#27169;&#22411;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#27867;&#21270;&#24615;&#25110;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36234;&#29425;&#25552;&#31034;&#25915;&#20987;&#27010;&#25324;&#20026;&#20004;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#25552;&#31034;&#37325;&#20889;&#21644;&#65288;2&#65289;&#22330;&#26223;&#23884;&#22871;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReNeLLM&#65292;&#19968;&#20010;&#21033;&#29992;LLMs&#33258;&#36523;&#29983;&#25104;&#26377;&#25928;&#36234;&#29425;&#25552;&#31034;&#30340;&#33258;&#21160;&#26694;&#26550;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#22522;&#32447;&#30456;&#27604;&#65292;ReNeLLM&#26174;&#33879;&#25552;&#39640;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08268v2 Announce Type: replace  Abstract: Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as 'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on other white-box models, compromising generalization or efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Ou
&lt;/p&gt;</description></item><item><title>&#33258;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#38598;&#25104;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20960;&#20046;&#25152;&#26377;&#24773;&#26223;&#20013;&#65292;&#33021;&#22815;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#37325;&#22797;&#24615;&#21644;&#23616;&#37096;&#26368;&#20248;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.08154</link><description>&lt;p&gt;
&#20877;&#38382;&#19968;&#27425;&#65306;&#33258;&#19968;&#33268;&#24615;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#22312;&#65288;&#20960;&#20046;&#65289;&#25152;&#26377;&#22330;&#26223;&#20013;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08154
&lt;/p&gt;
&lt;p&gt;
&#33258;&#19968;&#33268;&#24615;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#38598;&#25104;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20960;&#20046;&#25152;&#26377;&#24773;&#26223;&#20013;&#65292;&#33021;&#22815;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#37325;&#22797;&#24615;&#21644;&#23616;&#37096;&#26368;&#20248;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#32467;&#21512;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#20294;CoT&#25552;&#31034;&#20013;&#36890;&#24120;&#20351;&#29992;&#30340;&#36138;&#23146;&#35299;&#30721;&#20250;&#23548;&#33268;&#37325;&#22797;&#24615;&#21644;&#23616;&#37096;&#26368;&#20248;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#32570;&#28857;&#65292;&#38598;&#25104;&#20248;&#21270;&#23581;&#35797;&#33719;&#24471;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#20197;&#24471;&#21040;&#26368;&#32456;&#31572;&#26696;&#38598;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#38598;&#25104;&#20248;&#21270;&#26041;&#27861;&#35201;&#20040;&#31616;&#21333;&#22320;&#37319;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#21518;&#22788;&#29702;&#65292;&#27604;&#22914;&#8220;&#33258;&#19968;&#33268;&#24615;&#8221;&#65292;&#35201;&#20040;&#35757;&#32451;&#19968;&#20010;&#22522;&#20110;&#20960;&#20010;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20154;&#31867;&#27880;&#37322;&#30340;&#38468;&#21152;&#27169;&#22411;&#26469;&#22312;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#20013;&#36873;&#25321;&#26368;&#20339;&#36335;&#24452;&#65292;&#20294;&#26410;&#33021;&#25512;&#24191;&#21040;&#29616;&#23454;&#35774;&#32622;&#65292;&#20854;&#20013;&#36755;&#20837;&#38382;&#39064;&#31867;&#22411;&#26410;&#30693;&#25110;&#25512;&#29702;&#36335;&#24452;&#30340;&#31572;&#26696;&#26684;&#24335;&#26410;&#30693;&#12290;&#20026;&#20102;&#36991;&#20813;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#33258;&#19968;&#33268;&#24615;&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#38598;&#25104;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#20960;&#20046;&#25152;&#26377;&#24773;&#26223;&#20013;&#36866;&#29992;&#65292;&#20854;&#20013;&#36755;&#20837;&#38382;&#39064;&#30340;&#31867;&#22411;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08154v2 Announce Type: replace-cross  Abstract: Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local optimality. To address this shortcoming, ensemble-optimization tries to obtain multiple reasoning paths to get the final answer assembly. However, current ensemble-optimization methods either simply employ rule-based post-processing such as \textit{self-consistency}, or train an additional model based on several task-related human annotations to select the best one among multiple reasoning paths, yet fail to generalize to realistic settings where the type of input questions is unknown or the answer format of reasoning paths is unknown. To avoid their limitations, we propose \textbf{Self-Agreement}, a generalizable ensemble-optimization method applying in almost all scenarios where the type of input question
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2311.08045</link><description>&lt;p&gt;
&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adversarial Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08045
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20559;&#22909;&#35843;&#25972;&#26159;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20132;&#20114;&#36136;&#37327;&#30340;&#20851;&#38190;&#12290;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25351;&#23548;LLM&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25345;&#32493;&#26356;&#26032;LLMs&#20250;&#23548;&#33268;&#27169;&#22411;&#29983;&#25104;&#26679;&#26412;&#19982;&#20154;&#31867;&#39318;&#36873;&#21709;&#24212;&#20043;&#38388;&#23384;&#22312;&#20998;&#24067;&#24046;&#36317;&#65292;&#36825;&#38459;&#30861;&#20102;&#27169;&#22411;&#24494;&#35843;&#30340;&#25928;&#29575;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#22312;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#39069;&#22806;&#36827;&#34892;&#20559;&#22909;&#27880;&#37322;&#65292;&#20197;&#36866;&#24212;&#36716;&#31227;&#20998;&#24067;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#27880;&#37322;&#36164;&#28304;&#12290;&#38024;&#23545;&#26356;&#39640;&#25928;&#30340;&#20154;&#31867;&#20559;&#22909;&#20248;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#20559;&#22909;&#20248;&#21270;&#65288;APO&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;LLM&#20195;&#29702;&#21644;&#20559;&#22909;&#27169;&#22411;&#36890;&#36807;&#26497;&#23567;-&#26497;&#22823;&#21338;&#24328;&#20132;&#26367;&#26356;&#26032;&#12290;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;APO&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#23398;&#20064;&#33258;&#36866;&#24212;&#20110;&#29983;&#25104;&#20998;&#24067;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08045v2 Announce Type: replace-cross  Abstract: Human preference alignment is essential to improve the interaction quality of large language models (LLMs). Existing aligning methods depend on manually annotated preference data to guide the LLM optimization directions. However, in practice, continuously updating LLMs raises a distribution gap between model-generated samples and human-preferred responses, which hinders model fine-tuning efficiency. To mitigate this issue, previous methods require additional preference annotation on generated samples to adapt the shifted distribution, which consumes a large amount of annotation resources. Targeting more efficient human preference optimization, we propose an adversarial preference optimization (APO) framework, where the LLM agent and the preference model update alternatively via a min-max game. Without additional annotation, our APO method can make a self-adaption to the generation distribution gap through the adversarial learni
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#35782;&#21035;&#26377;&#27602;&#12289;&#20882;&#29359;&#21644;&#20196;&#20154;&#35752;&#21388;&#30340;&#20869;&#23481;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#20123;&#25913;&#36827;&#26159;&#21542;&#30495;&#27491;&#28385;&#36275;&#20102;&#24535;&#24895;&#20869;&#23481;&#31649;&#29702;&#21592;&#22312;&#24037;&#20316;&#20013;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2311.07879</link><description>&lt;p&gt;
&#27602;&#24615;&#26816;&#27979;&#24182;&#19981;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#65306;&#24357;&#21512;&#25903;&#25345;&#24535;&#24895;&#20869;&#23481;&#31649;&#29702;&#21592;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#35782;&#21035;&#26377;&#27602;&#12289;&#20882;&#29359;&#21644;&#20196;&#20154;&#35752;&#21388;&#30340;&#20869;&#23481;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#36825;&#20123;&#25913;&#36827;&#26159;&#21542;&#30495;&#27491;&#28385;&#36275;&#20102;&#24535;&#24895;&#20869;&#23481;&#31649;&#29702;&#21592;&#22312;&#24037;&#20316;&#20013;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#35782;&#21035;&#26377;&#27602;&#12289;&#20882;&#29359;&#21644;&#20196;&#20154;&#35752;&#21388;&#30340;&#20869;&#23481;&#26041;&#38754;&#21462;&#24471;&#20102;&#38271;&#36275;&#30340;&#36827;&#23637;&#65292;&#26088;&#22312;&#20943;&#36731;&#31649;&#29702;&#21592;&#30340;&#24037;&#20316;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#20219;&#21153;&#30340;&#25913;&#36827;&#26159;&#21542;&#30495;&#27491;&#28385;&#36275;&#20102;&#31649;&#29702;&#21592;&#22312;&#24037;&#20316;&#20013;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#36807;&#21435;&#30740;&#31350;&#21162;&#21147;&#33268;&#21147;&#20110;&#20026;&#20869;&#23481;&#31649;&#29702;&#30340;&#21508;&#20010;&#26041;&#38754;&#25552;&#20379;&#33258;&#21160;&#21270;&#25903;&#25345;&#19982;&#24535;&#24895;&#20869;&#23481;&#31649;&#29702;&#21592;&#30340;&#38656;&#27714;&#20043;&#38388;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#23588;&#20854;&#26159;&#22312;&#35782;&#21035;&#36829;&#21453;&#21508;&#31181;&#31649;&#29702;&#35268;&#21017;&#26041;&#38754;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;Hugging Face&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#25581;&#31034;&#28085;&#30422;&#19977;&#20010;&#31034;&#33539;&#35770;&#22363;&#30340;&#21508;&#31181;&#31649;&#29702;&#35268;&#21017;&#21644;&#25351;&#21335;&#30340;&#27169;&#22411;&#30340;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#22312;&#26631;&#35760;&#26576;&#20010;&#29305;&#23450;&#35770;&#22363;&#30340;&#24179;&#21488;&#35268;&#21017;&#36829;&#35268;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#29992;&#25143;&#35843;&#26597;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07879v2 Announce Type: replace-cross  Abstract: Extensive efforts in automated approaches for content moderation have been focused on developing models to identify toxic, offensive, and hateful content with the aim of lightening the load for moderators. Yet, it remains uncertain whether improvements on those tasks have truly addressed moderators' needs in accomplishing their work. In this paper, we surface gaps between past research efforts that have aimed to provide automation for aspects of content moderation and the needs of volunteer content moderators, regarding identifying violations of various moderation rules. To do so, we conduct a model review on Hugging Face to reveal the availability of models to cover various moderation rules and guidelines from three exemplar forums. We further put state-of-the-art LLMs to the test, evaluating how well these models perform in flagging violations of platform rules from one particular forum. Finally, we conduct a user survey stud
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25490;&#38500;&#25512;&#29702;&#36807;&#31243;&#20013;&#36935;&#21040;&#22256;&#38590;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#38500;&#25512;&#29702;&#26041;&#27861;PoE&#19982;COT&#65292;&#21457;&#29616;&#27492;&#26041;&#27861;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#19981;&#22914;&#36873;&#25321;&#27491;&#30830;&#31572;&#26696;&#65292;&#24182;&#25351;&#20986;&#20102;&#30740;&#31350;&#20013;&#21457;&#29616;&#30340;&#19968;&#33268;&#24615;&#21644;&#38169;&#35823;&#20998;&#26512;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.07532</link><description>&lt;p&gt;
&#38169;&#35823;&#24182;&#19981;&#23481;&#26131;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25490;&#38500;&#25512;&#29702;&#36807;&#31243;&#20013;&#36935;&#21040;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07532
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25490;&#38500;&#25512;&#29702;&#36807;&#31243;&#20013;&#36935;&#21040;&#22256;&#38590;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25490;&#38500;&#25512;&#29702;&#26041;&#27861;PoE&#19982;COT&#65292;&#21457;&#29616;&#27492;&#26041;&#27861;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#19981;&#22914;&#36873;&#25321;&#27491;&#30830;&#31572;&#26696;&#65292;&#24182;&#25351;&#20986;&#20102;&#30740;&#31350;&#20013;&#21457;&#29616;&#30340;&#19968;&#33268;&#24615;&#21644;&#38169;&#35823;&#20998;&#26512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32500;&#65288;COT&#65289;&#25552;&#31034;&#21487;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26397;&#30528;&#27491;&#30830;&#31572;&#26696;&#36827;&#34892;&#25512;&#29702;&#65292;&#20294;&#20854;&#22312;&#26397;&#30528;&#38169;&#35823;&#31572;&#26696;&#36827;&#34892;&#25512;&#29702;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#34987;&#25506;&#31350;&#12290;&#24403;&#19982;COT&#19968;&#36215;&#20351;&#29992;&#26102;&#65292;&#36825;&#31181;&#25490;&#38500;&#25512;&#29702;&#65288;PoE&#65289;&#21487;&#20197;&#22686;&#24378;&#33258;&#25105;&#19968;&#33268;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#35832;&#22914;&#25490;&#38500;&#24615;&#21307;&#23398;&#35786;&#26029;&#31561;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#20013;&#36827;&#34892;PoE&#19982;COT&#30340;&#26041;&#27861;&#65292;LLMs&#24517;&#39035;&#26397;&#30528;&#19981;&#27491;&#30830;&#30340;&#36873;&#39033;&#36827;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-3.5&#12289;LLaMA-2&#21644;Falcon&#22312;&#24635;&#20849;&#22235;&#20010;&#24120;&#35782;&#21644;&#31185;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#24102;&#26377;COT&#30340;PoE&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;PoE&#31574;&#30053;&#24635;&#26159;&#34920;&#29616;&#19981;&#22914;&#36873;&#25321;&#27491;&#30830;&#31572;&#26696;&#30340;&#31574;&#30053;&#12290;&#36825;&#20004;&#31181;&#31574;&#30053;&#30340;&#19968;&#33268;&#24615;&#20063;&#20302;&#20110;&#27599;&#31181;&#31574;&#30053;&#30340;&#33258;&#25105;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#38169;&#35823;&#20998;&#26512;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07532v2 Announce Type: replace  Abstract: Chain-of-thought (COT) prompting can help large language models (LLMs) reason toward correct answers, but its efficacy in reasoning toward incorrect answers is unexplored. This process of elimination (PoE), when used with COT, can enhance self-consistency, interpretability, and tasks such as medical diagnoses of exclusion. Thus, we propose PoE with COT, where LLMs must reason toward incorrect options on multiple-choice questions. We evaluate the ability of GPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on a total of four commonsense and scientific reasoning datasets. We find that the strategy of PoE always underperforms the strategy of choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy. To study these issues further, we conduct error analyses and give suggestions for future work.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#19982;&#25628;&#32034;&#24341;&#25806;&#30340;&#20132;&#20114;&#21382;&#21490;&#20013;&#25552;&#21462;&#30456;&#20851;&#19978;&#19979;&#25991;&#26469;&#20010;&#24615;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#25913;&#36827;&#32593;&#32476;&#25628;&#32034;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2311.06318</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20010;&#24615;&#21270;&#19978;&#19979;&#25991;&#26597;&#35810;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Augmented Large Language Models for Personalized Contextual Query Suggestion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06318
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#29992;&#25143;&#19982;&#25628;&#32034;&#24341;&#25806;&#30340;&#20132;&#20114;&#21382;&#21490;&#20013;&#25552;&#21462;&#30456;&#20851;&#19978;&#19979;&#25991;&#26469;&#20010;&#24615;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#25913;&#36827;&#32593;&#32476;&#25628;&#32034;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25797;&#38271;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#23427;&#20204;&#25152;&#28041;&#21450;&#30340;&#25104;&#26412;&#24040;&#22823;&#65292;&#23427;&#20204;&#20173;&#28982;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#38745;&#24577;&#30340;&#65292;&#24182;&#19988;&#38590;&#20197;&#20010;&#24615;&#21270;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#35768;&#22810;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#20174;&#26681;&#25454;&#29992;&#25143;&#30340;&#20559;&#22909;&#12289;&#30446;&#26631;&#21644;&#30693;&#35782;&#37327;&#23450;&#21046;&#30340;&#29983;&#25104;&#20013;&#21463;&#30410;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#32593;&#32476;&#25628;&#32034;&#65292;&#20102;&#35299;&#29992;&#25143;&#35797;&#22270;&#20570;&#20160;&#20040;&#12289;&#20851;&#24515;&#20160;&#20040;&#20197;&#21450;&#20182;&#20204;&#30693;&#36947;&#20160;&#20040;&#21487;&#20197;&#25552;&#39640;&#25628;&#32034;&#20307;&#39564;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#29992;&#25143;&#19982;&#25628;&#32034;&#24341;&#25806;&#30340;&#20132;&#20114;&#21382;&#21490;&#20013;&#30340;&#30456;&#20851;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;LLM&#20197;&#20010;&#24615;&#21270;&#20854;&#36755;&#20986;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#29992;&#25143;&#22312;&#32593;&#32476;&#19978;&#30340;&#25628;&#32034;&#21644;&#27983;&#35272;&#27963;&#21160;&#26500;&#24314;&#20102;&#27599;&#20010;&#29992;&#25143;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#30693;&#35782;&#23384;&#20648;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#20026;LLM&#25552;&#20379;&#20855;&#26377;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#30340;&#25552;&#31034;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06318v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) excel at tackling various natural language tasks. However, due to the significant costs involved in re-training or fine-tuning them, they remain largely static and difficult to personalize. Nevertheless, a variety of applications could benefit from generations that are tailored to users' preferences, goals, and knowledge. Among them is web search, where knowing what a user is trying to accomplish, what they care about, and what they know can lead to improved search experiences. In this work, we propose a novel and general approach that augments an LLM with relevant context from users' interaction histories with a search engine in order to personalize its outputs. Specifically, we construct an entity-centric knowledge store for each user based on their search and browsing activities on the web, which is then leveraged to provide contextually relevant LLM prompt augmentations. This knowledge store is 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20013;&#25991;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;LAiW&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#27861;&#24459;&#23454;&#36341;&#36923;&#36753;&#30340;&#35780;&#20272;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;&#36890;&#29992;&#21644;&#27861;&#24459;&#39046;&#22495;LLM&#21487;&#33021;&#19981;&#31526;&#21512;&#27861;&#24459;&#23454;&#36341;&#36923;&#36753;&#12290;</title><link>https://arxiv.org/abs/2310.05620</link><description>&lt;p&gt;
LAiW&#65306;&#19968;&#20010;&#20013;&#25991;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
LAiW: A Chinese Legal Large Language Models Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05620
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20013;&#25991;&#27861;&#24459;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;LAiW&#65292;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#27861;&#24459;&#23454;&#36341;&#36923;&#36753;&#30340;&#35780;&#20272;&#20219;&#21153;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;&#36890;&#29992;&#21644;&#27861;&#24459;&#39046;&#22495;LLM&#21487;&#33021;&#19981;&#31526;&#21512;&#27861;&#24459;&#23454;&#36341;&#36923;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05620v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442; &#25688;&#35201;&#65306;&#19968;&#33324;&#21644;&#27861;&#24459;&#39046;&#22495;&#30340;LLM&#22312;LegalAI&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#23545;&#36825;&#20123;LLM&#22312;LegalAI&#20013;&#30340;&#35780;&#20272;&#26159;&#30001;&#35745;&#31639;&#26426;&#31185;&#23398;&#19987;&#23478;&#23450;&#20041;&#30340;&#65292;&#32570;&#20047;&#19982;&#27861;&#24459;&#23454;&#36341;&#36923;&#36753;&#19968;&#33268;&#24615;&#65292;&#20351;&#24471;&#24456;&#38590;&#21028;&#26029;&#23427;&#20204;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#27425;&#22522;&#20110;&#27861;&#24459;&#23454;&#36341;&#36923;&#36753;&#26500;&#24314;&#20102;&#20013;&#25991;&#27861;&#24459;LLM&#22522;&#20934;LAiW&#12290;&#20026;&#20102;&#19982;&#27861;&#24459;&#19987;&#23478;&#21644;&#27861;&#24459;&#23454;&#36341;&#30340;&#24605;&#32500;&#36807;&#31243;&#65288;&#19977;&#27573;&#35770;&#65289;&#20445;&#25345;&#19968;&#33268;&#65292;&#25105;&#20204;&#23558;LLM&#30340;&#27861;&#24459;&#33021;&#21147;&#20174;&#26131;&#21040;&#38590;&#20998;&#20026;&#19977;&#20010;&#32423;&#21035;&#65306;&#22522;&#26412;&#20449;&#24687;&#26816;&#32034;&#12289;&#27861;&#24459;&#22522;&#30784;&#25512;&#29702;&#21644;&#22797;&#26434;&#27861;&#24459;&#24212;&#29992;&#12290;&#27599;&#20010;&#32423;&#21035;&#21253;&#21547;&#22810;&#20010;&#20219;&#21153;&#20197;&#30830;&#20445;&#20840;&#38754;&#35780;&#20272;&#12290;&#36890;&#36807;&#23545;&#24403;&#21069;&#36890;&#29992;&#21644;&#27861;&#24459;&#39046;&#22495;LLM&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#30340;&#33258;&#21160;&#21270;&#35780;&#20272;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;LLM&#21487;&#33021;&#19981;&#31526;&#21512;&#27861;&#24459;&#23454;&#36341;&#36923;&#36753;&#12290;LLM&#20284;&#20046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05620v2 Announce Type: replace  Abstract: General and legal domain LLMs have demonstrated strong performance in various tasks of LegalAI. However, the current evaluations of these LLMs in LegalAI are defined by the experts of computer science, lacking consistency with the logic of legal practice, making it difficult to judge their practical capabilities. To address this challenge, we are the first to build the Chinese legal LLMs benchmark LAiW, based on the logic of legal practice. To align with the thinking process of legal experts and legal practice (syllogism), we divide the legal capabilities of LLMs from easy to difficult into three levels: basic information retrieval, legal foundation inference, and complex legal application. Each level contains multiple tasks to ensure a comprehensive evaluation. Through automated evaluation of current general and legal domain LLMs on our benchmark, we indicate that these LLMs may not align with the logic of legal practice. LLMs seem 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#37327;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#21892;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;</title><link>https://arxiv.org/abs/2310.04910</link><description>&lt;p&gt;
&#20851;&#20110;&#24120;&#35782;&#25512;&#29702;&#30340;&#30693;&#35782;&#22270;&#35889;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;
&lt;/p&gt;
&lt;p&gt;
Faithful Knowledge Graph Explanations for Commonsense Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#37327;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#25913;&#21892;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#21644;&#20445;&#30495;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#21512;&#35821;&#35328;&#27169;&#22411;(LMs)&#21644;&#30693;&#35782;&#22270;&#35889;(KGs)&#24050;&#25104;&#20026;&#24120;&#35782;&#38382;&#31572;&#30740;&#31350;&#20013;&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#20294;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#23454;&#29616;&#31934;&#30830;&#30340;&#24605;&#36335;&#38142;&#35299;&#37322;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#24403;&#21069;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#25216;&#26415;&#30340;&#19968;&#20010;&#20027;&#35201;&#24369;&#28857;&#26159;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#24573;&#35270;&#20102;&#29983;&#25104;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#20004;&#20010;&#37327;&#21270;&#25351;&#26631; - &#22270;&#19968;&#33268;&#24615;&#21644;&#22270;&#20445;&#30495;&#24230; - &#26469;&#34913;&#37327;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#35299;&#37322;&#30340;&#21487;&#20449;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;Consistent GNN (CGNN)&#65292;&#35813;&#26041;&#27861;&#28155;&#21152;&#20102;&#19968;&#39033;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#39033;&#26469;&#25913;&#21892;&#35299;&#37322;&#30340;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;KG&#30340;&#39044;&#27979;&#32463;&#24120;&#20559;&#31163;&#21407;&#22987;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;CGNN&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#33268;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#23637;&#31034;&#20102;&#23427;&#20135;&#29983;&#26356;&#21487;&#20449;&#35299;&#37322;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#26126;&#30830;&#35780;&#20272;&#35299;&#37322;&#21487;&#20449;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While fusing language models (LMs) and knowledge graphs (KGs) has become common in commonsense question answering research, enabling faithful chain-of-thought explanations in these models remains an open problem. One major weakness of current KG-based explanation techniques is that they overlook the faithfulness of generated explanations during evaluation. To address this gap, we make two main contributions: (1) We propose and validate two quantitative metrics - graph consistency and graph fidelity - to measure the faithfulness of KG-based explanations. (2) We introduce Consistent GNN (CGNN), a novel training method that adds a consistency regularization term to improve explanation faithfulness. Our analysis shows that predictions from KG often diverge from original model predictions. The proposed CGNN approach boosts consistency and fidelity, demonstrating its potential for producing more faithful explanations. Our work emphasises the importance of explicitly evaluating suggest a path
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#29992;&#35780;&#20998;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#36890;&#36807;&#39318;&#20010;&#24320;&#28304;&#26694;&#26550;&#21644;CALM&#27169;&#22411;&#65292;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#20256;&#32479;&#20449;&#29992;&#35780;&#20998;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#21516;&#26102;&#35299;&#20915;&#28508;&#22312;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2310.00566</link><description>&lt;p&gt;
&#36171;&#33021;&#20247;&#22810;&#65292;&#20559;&#34962;&#23569;&#25968;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#36890;&#29992;&#20449;&#29992;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00566
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#29992;&#35780;&#20998;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#36890;&#36807;&#39318;&#20010;&#24320;&#28304;&#26694;&#26550;&#21644;CALM&#27169;&#22411;&#65292;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#20256;&#32479;&#20449;&#29992;&#35780;&#20998;&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#21516;&#26102;&#35299;&#20915;&#28508;&#22312;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#34892;&#19994;&#65292;&#20449;&#29992;&#35780;&#20998;&#26159;&#19968;&#20010;&#22522;&#30784;&#35201;&#32032;&#65292;&#22609;&#36896;&#30528;&#20010;&#20154;&#21644;&#20225;&#19994;&#30340;&#20449;&#36151;&#20934;&#20837;&#65292;&#20915;&#23450;&#30528;&#36151;&#27454;&#26465;&#20214;&#12290;&#26412;&#30740;&#31350;&#35748;&#20026;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20449;&#29992;&#35780;&#20998;&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#33021;&#22815;&#24378;&#22823;&#22320;&#36328;&#22810;&#20010;&#20219;&#21153;&#36827;&#34892;&#27867;&#21270;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#25506;&#32034;LLMs&#22312;&#20449;&#29992;&#35780;&#20998;&#20013;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#24320;&#28304;&#20840;&#38754;&#26694;&#26550;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#28085;&#30422;9&#20010;&#25968;&#25454;&#38598;&#12289;1.4K&#26679;&#26412;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#19987;&#38376;&#29992;&#20110;&#20449;&#29992;&#35780;&#20272;&#65292;&#24182;&#23545;LLMs&#20869;&#28508;&#22312;&#20559;&#35265;&#36827;&#34892;&#20102;&#37325;&#35201;&#26816;&#26597;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#36229;&#36807;45K&#26679;&#26412;&#30340;&#26032;&#22411;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#25552;&#20986;&#20102;&#39318;&#20010;&#20449;&#36151;&#19982;&#39118;&#38505;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;CALM&#65289;&#65292;&#20197;&#38024;&#23545;&#19981;&#21516;&#30340;&#24494;&#22937;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00566v3 Announce Type: replace-cross  Abstract: In the financial industry, credit scoring is a fundamental element, shaping access to credit and determining the terms of loans for individuals and businesses alike. Traditional credit scoring methods, however, often grapple with challenges such as narrow knowledge scope and isolated evaluation of credit tasks. Our work posits that Large Language Models (LLMs) have great potential for credit scoring tasks, with strong generalization ability across multiple tasks. To systematically explore LLMs for credit scoring, we propose the first open-source comprehensive framework. We curate a novel benchmark covering 9 datasets with 14K samples, tailored for credit assessment and a critical examination of potential biases within LLMs, and the novel instruction tuning data with over 45k samples. We then propose the first Credit and Risk Assessment Large Language Model (CALM) by instruction tuning, tailored to the nuanced demands of various
&lt;/p&gt;</description></item><item><title>&#25351;&#20196;&#35843;&#25972;&#23545;LLMs&#20135;&#29983;&#20102;&#19977;&#20010;&#37325;&#35201;&#24433;&#21709;&#65306;1&#65289;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#37096;&#20998;&#65307;2&#65289;&#20419;&#36827;&#21709;&#24212;&#29983;&#25104;&#30340;&#19981;&#26029;&#35843;&#25972;</title><link>https://arxiv.org/abs/2310.00492</link><description>&lt;p&gt;
&#20174;&#35821;&#35328;&#24314;&#27169;&#21040;&#25351;&#20196;&#36319;&#38543;&#65306;&#29702;&#35299;&#25351;&#20196;&#35843;&#25972;&#21518;LLMs&#20013;&#34892;&#20026;&#30340;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00492
&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#23545;LLMs&#20135;&#29983;&#20102;&#19977;&#20010;&#37325;&#35201;&#24433;&#21709;&#65306;1&#65289;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#37096;&#20998;&#65307;2&#65289;&#20419;&#36827;&#21709;&#24212;&#29983;&#25104;&#30340;&#19981;&#26029;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20854;&#20013;&#25351;&#20196;&#35843;&#25972;&#26159;&#23558;LLMs&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25351;&#20196;&#35843;&#25972;&#22914;&#20309;&#35843;&#25972;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#20869;&#22312;&#21464;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#20960;&#31181;&#26412;&#22320;&#21644;&#20840;&#23616;&#35299;&#37322;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#36755;&#20837;&#36755;&#20986;&#24402;&#22240;&#26041;&#27861;&#65292;&#20197;&#21450;&#29992;&#20110;&#35299;&#37322;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#23618;&#20013;&#30340;&#27169;&#24335;&#21644;&#27010;&#24565;&#30340;&#25216;&#26415;&#12290;&#28982;&#21518;&#36890;&#36807;&#27604;&#36739;&#20174;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#20013;&#24471;&#20986;&#30340;&#35299;&#37322;&#26469;&#30740;&#31350;&#25351;&#20196;&#35843;&#25972;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20154;&#21487;&#29702;&#35299;&#30340;&#27700;&#24179;&#19978;&#25552;&#20379;&#20102;&#27169;&#22411;&#36716;&#21464;&#30340;&#20869;&#37096;&#35270;&#35282;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#19977;&#20010;&#37325;&#35201;&#24433;&#21709;&#65306;1&#65289;&#23427;&#20351;LLMs&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#37096;&#20998;&#65292;&#24182;&#19981;&#26029;&#20419;&#36827;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00492v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have achieved remarkable success, where instruction tuning is the critical step in aligning LLMs with user intentions. In this work, we investigate how the instruction tuning adjusts pre-trained models with a focus on intrinsic changes. Specifically, we first develop several local and global explanation methods, including a gradient-based method for input-output attribution and techniques for interpreting patterns and concepts in self-attention and feed-forward layers. The impact of instruction tuning is then studied by comparing the explanations derived from the pre-trained and instruction-tuned models. This approach provides an internal perspective of the model shifts on a human-comprehensible level. Our findings reveal three significant impacts of instruction tuning: 1) It empowers LLMs to recognize the instruction parts from user prompts, and promotes the response generation constantly condition
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;Git&#25552;&#20132;&#30340;&#33258;&#28982;&#32467;&#26500;&#65292;&#23558;&#20195;&#30721;&#26356;&#25913;&#19982;&#20154;&#31867;&#25351;&#20196;&#37197;&#23545;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OctoPack&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#34920;&#29616;&#26368;&#20339;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2308.07124</link><description>&lt;p&gt;
OctoPack: &#20351;&#29992;&#25351;&#20196;&#35843;&#25972;&#20195;&#30721;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OctoPack: Instruction Tuning Code Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.07124
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;Git&#25552;&#20132;&#30340;&#33258;&#28982;&#32467;&#26500;&#65292;&#23558;&#20195;&#30721;&#26356;&#25913;&#19982;&#20154;&#31867;&#25351;&#20196;&#37197;&#23545;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OctoPack&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19978;&#23454;&#29616;&#20102;&#34920;&#29616;&#26368;&#20339;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25351;&#20196;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24494;&#35843;&#21487;&#26174;&#33879;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24212;&#29992;&#20195;&#30721;&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65292;&#21033;&#29992;Git&#25552;&#20132;&#30340;&#33258;&#28982;&#32467;&#26500;&#65292;&#23558;&#20195;&#30721;&#26356;&#25913;&#19982;&#20154;&#31867;&#25351;&#20196;&#37197;&#23545;&#12290;&#25105;&#20204;&#32534;&#35793;&#20102;CommitPack&#65306;&#36328;350&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;4TB Git&#25552;&#20132;&#12290;&#25105;&#20204;&#22312;&#25317;&#26377;16B&#21442;&#25968;&#30340;StarCoder&#27169;&#22411;&#19978;&#23545;&#27604;CommitPack&#21644;&#20854;&#20182;&#33258;&#28982;&#19982;&#21512;&#25104;&#20195;&#30721;&#25351;&#20196;&#65288;xP3x, Self-Instruct, OASST&#65289;&#65292;&#22312;HumanEval Python&#22522;&#20934;&#27979;&#35797;&#65288;46.2% pass@1&#65289;&#20013;&#21462;&#24471;&#20102;&#26410;&#22312;OpenAI&#36755;&#20986;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25512;&#20986;HumanEvalPack&#65292;&#23558;HumanEval&#22522;&#20934;&#27979;&#35797;&#25193;&#23637;&#21040;&#20849;&#35745;3&#20010;&#32534;&#30721;&#20219;&#21153;&#65288;&#20195;&#30721;&#20462;&#22797;&#12289;&#20195;&#30721;&#35299;&#37322;&#12289;&#20195;&#30721;&#21512;&#25104;&#65289;&#36328;6&#31181;&#35821;&#35328;&#65288;Python&#12289;JavaScript&#12289;Java&#12289;Go&#12289;C ++&#12289;Rust&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;OctoCoder&#21644;OctoGeeX&#22312;HumanEvalPack&#20013;&#21462;&#24471;&#20102;&#25152;&#26377;&#35768;&#21487;&#27169;&#22411;&#20013;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.07124v2 Announce Type: replace-cross  Abstract: Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive m
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#27169;&#22359;&#21270;&#30340;&#21327;&#20316;&#20307;&#29616;&#26234;&#33021;&#20307;&#65292;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36229;&#36234;&#35268;&#21010;&#26041;&#27861;&#24182;&#23637;&#31034;&#26377;&#25928;&#27807;&#36890;&#12290;</title><link>https://arxiv.org/abs/2307.02485</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27169;&#22359;&#21270;&#22320;&#26500;&#24314;&#21327;&#20316;&#20307;&#29616;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Building Cooperative Embodied Agents Modularly with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.02485
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#27169;&#22359;&#21270;&#30340;&#21327;&#20316;&#20307;&#29616;&#26234;&#33021;&#20307;&#65292;&#23454;&#29616;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36229;&#36234;&#35268;&#21010;&#26041;&#27861;&#24182;&#23637;&#31034;&#26377;&#25928;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22788;&#29702;&#20855;&#26377;&#21435;&#20013;&#24515;&#21270;&#25511;&#21046;&#12289;&#21407;&#22987;&#24863;&#30693;&#35266;&#23519;&#12289;&#26114;&#36149;&#36890;&#35759;&#21644;&#22810;&#30446;&#26631;&#20219;&#21153;&#30340;&#20855;&#26377;&#21508;&#31181;&#20307;&#29616;&#29615;&#22659;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#38382;&#39064;&#12290;&#19982;&#20808;&#21069;&#30740;&#31350;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24120;&#35782;&#30693;&#35782;&#12289;&#25512;&#29702;&#33021;&#21147;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#23558;&#23427;&#20204;&#26080;&#32541;&#22320;&#34701;&#20837;&#21040;&#19968;&#20010;&#19982;&#24863;&#30693;&#12289;&#35760;&#24518;&#21644;&#25191;&#34892;&#30456;&#32467;&#21512;&#30340;&#35748;&#30693;&#21551;&#21457;&#24335;&#27169;&#22359;&#21270;&#26694;&#26550;&#20013;&#12290;&#20174;&#32780;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#20197;&#35268;&#21010;&#12289;&#27807;&#36890;&#21644;&#19982;&#20854;&#20182;&#20154;&#21512;&#20316;&#20197;&#39640;&#25928;&#23436;&#25104;&#38271;&#26102;&#31243;&#20219;&#21153;&#30340;&#21512;&#20316;&#20307;&#29616;&#26234;&#33021;&#20307;CoELA&#12290;&#25105;&#20204;&#22312;C-WAH&#21644;TDW-MAT&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30001;GPT-4&#39537;&#21160;&#30340;CoELA&#21487;&#20197;&#36229;&#36234;&#24378;&#22823;&#30340;&#22522;&#20110;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20986;&#26032;&#20852;&#30340;&#26377;&#25928;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2307.02485v2 Announce Type: replace  Abstract: In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current O
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#30340;&#26041;&#27861;BMX&#21033;&#29992;&#35299;&#37322;&#26469;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#24230;&#37327;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#36716;&#21270;&#20026;&#27573;&#33853;&#32423;&#20998;&#25968;&#65292;&#24182;&#32467;&#21512;&#21407;&#22987;&#24230;&#37327;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2212.10469</link><description>&lt;p&gt;
BMX&#65306;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
BMX: Boosting Natural Language Generation Metrics with Explainability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.10469
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#30340;&#26041;&#27861;BMX&#21033;&#29992;&#35299;&#37322;&#26469;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#24230;&#37327;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#23558;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#36716;&#21270;&#20026;&#27573;&#33853;&#32423;&#20998;&#25968;&#65292;&#24182;&#32467;&#21512;&#21407;&#22987;&#24230;&#37327;&#65292;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35780;&#20272;&#24230;&#37327;&#22522;&#20110;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#32771;&#34385;&#20102;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#30446;&#26631;&#26159;&#26356;&#22909;&#22320;&#20026;&#20154;&#31867;&#29702;&#35299;&#21644;&#26356;&#22909;&#22320;&#24230;&#37327;&#20998;&#26512;&#65292;&#21253;&#25324;&#22833;&#36133;&#26696;&#20363;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;BMX&#65306;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#24230;&#37327;&#26126;&#30830;&#21033;&#29992;&#35299;&#37322;&#26469;&#25552;&#21319;&#25351;&#26631;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#29305;&#24449;&#37325;&#35201;&#24615;&#35299;&#37322;&#35270;&#20026;&#21333;&#35789;&#32423;&#20998;&#25968;&#65292;&#36890;&#36807;&#24130;&#22343;&#20540;&#23558;&#20854;&#36716;&#25442;&#20026;&#27573;&#33853;&#32423;&#20998;&#25968;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#27492;&#27573;&#33853;&#32423;&#20998;&#25968;&#19982;&#21407;&#22987;&#24230;&#37327;&#32467;&#21512;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#26174;&#31034;&#65292;&#22312;&#22810;&#20010;&#26426;&#22120;&#32763;&#35793;&#21644;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;&#34429;&#28982;&#26426;&#22120;&#32763;&#35793;&#26041;&#38754;&#30340;&#25913;&#36827;&#24456;&#23567;&#65292;&#20294;&#22312;&#25688;&#35201;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20351;&#29992;LIME&#35299;&#37322;&#22120;&#21644;&#39044;&#36873;&#21442;&#25968;&#30340;BMX&#36798;&#21040;&#20102;&#24179;&#22343;0.0&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.10469v2 Announce Type: replace  Abstract: State-of-the-art natural language generation evaluation metrics are based on black-box language models. Hence, recent works consider their explainability with the goals of better understandability for humans and better metric analysis, including failure cases. In contrast, our proposed method BMX: Boosting Natural Language Generation Metrics with explainability explicitly leverages explanations to boost the metrics' performance. In particular, we perceive feature importance explanations as word-level scores, which we convert, via power means, into a segment-level score. We then combine this segment-level score with the original metric to obtain a better metric. Our tests show improvements for multiple metrics across MT and summarization datasets. While improvements in machine translation are small, they are strong for summarization. Notably, BMX with the LIME explainer and preselected parameters achieves an average improvement of 0.0
&lt;/p&gt;</description></item><item><title>G-MAP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#35760;&#24518;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22686;&#24378;&#39046;&#22495;&#29305;&#23450;PLM&#26102;&#65292;&#20445;&#30041;&#36890;&#29992;&#30693;&#35782;&#65292;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;</title><link>https://arxiv.org/abs/2212.03613</link><description>&lt;p&gt;
G-MAP&#65306;&#36890;&#29992;&#35760;&#24518;&#22686;&#24378;&#30340;&#38754;&#21521;&#39046;&#22495;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.03613
&lt;/p&gt;
&lt;p&gt;
G-MAP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#35760;&#24518;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22686;&#24378;&#39046;&#22495;&#29305;&#23450;PLM&#26102;&#65292;&#20445;&#30041;&#36890;&#29992;&#30693;&#35782;&#65292;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;PLMs&#65292;&#36890;&#36807;&#32487;&#32493;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#35821;&#26009;&#24211;&#23545;&#36890;&#29992;PLMs&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26469;&#25552;&#39640;&#29305;&#23450;&#39046;&#22495;&#65288;&#20363;&#22914;&#29983;&#29289;&#21307;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#65289;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65288;DAPT&#65307;Gururangan&#31561;&#20154;&#65288;2020&#65289;&#65289;&#24448;&#24448;&#20250;&#36951;&#24536;&#36890;&#29992;PLMs&#24050;&#32463;&#33719;&#24471;&#30340;&#20808;&#21069;&#36890;&#29992;&#30693;&#35782;&#65292;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#29616;&#35937;&#21644;&#27425;&#20248;&#24615;&#33021;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36890;&#29992;&#35760;&#24518;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65288;G-MAP&#65289;&#65292;&#23427;&#36890;&#36807;&#20174;&#20923;&#32467;&#30340;&#36890;&#29992;PLM&#26500;&#24314;&#30340;&#35760;&#24518;&#34920;&#31034;&#26469;&#22686;&#24378;&#39046;&#22495;&#29305;&#23450;&#30340;PLM&#65292;&#32780;&#19981;&#20250;&#20002;&#22833;&#20219;&#20309;&#36890;&#29992;&#30693;&#35782;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35760;&#24518;&#22686;&#24378;&#23618;&#65292;&#24182;&#22522;&#20110;&#23427;&#65292;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#22686;&#24378;&#31574;&#30053;&#26469;&#26500;&#24314;&#35760;&#24518;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#21040;&#39046;&#22495;&#29305;&#23450;&#30340;PLM&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;G-MAP&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.03613v3 Announce Type: replace  Abstract: Recently, domain-specific PLMs have been proposed to boost the task performance of specific domains (e.g., biomedical and computer science) by continuing to pre-train general PLMs with domain-specific corpora. However, this Domain-Adaptive Pre-Training (DAPT; Gururangan et al. (2020)) tends to forget the previous general knowledge acquired by general PLMs, which leads to a catastrophic forgetting phenomenon and sub-optimal performance. To alleviate this problem, we propose a new framework of General Memory Augmented Pre-trained Language Model (G-MAP), which augments the domain-specific PLM by a memory representation built from the frozen general PLM without losing any general knowledge. Specifically, we propose a new memory-augmented layer, and based on it, different augmented strategies are explored to build the memory representation and then adaptively fuse it into the domain-specific PLM. We demonstrate the effectiveness of G-MAP 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#27965;&#25512;&#29702;&#26694;&#26550;SCR&#65292;&#36890;&#36807;&#20462;&#21098;&#31574;&#30053;&#21644;&#23545;&#31216;Kullback-Leibler&#25955;&#24230;&#26657;&#20934;&#36755;&#20986;&#20998;&#24067;&#20559;&#31227;&#65292;&#20174;&#32780;&#35299;&#20915;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2210.15373</link><description>&lt;p&gt;
&#33258;&#27965;&#25512;&#29702;&#29992;&#20110;&#35299;&#20915;&#25968;&#23398;&#24212;&#29992;&#39064;
&lt;/p&gt;
&lt;p&gt;
Self-consistent Reasoning For Solving Math Word Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.15373
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#27965;&#25512;&#29702;&#26694;&#26550;SCR&#65292;&#36890;&#36807;&#20462;&#21098;&#31574;&#30053;&#21644;&#23545;&#31216;Kullback-Leibler&#25955;&#24230;&#26657;&#20934;&#36755;&#20986;&#20998;&#24067;&#20559;&#31227;&#65292;&#20174;&#32780;&#35299;&#20915;&#25968;&#23398;&#24212;&#29992;&#39064;&#20013;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#24212;&#29992;&#39064;&#65288;MWPs&#65289;&#26159;&#19968;&#20010;&#20174;&#32473;&#23450;&#30340;&#25968;&#23398;&#38382;&#39064;&#25991;&#26412;&#20013;&#33258;&#21160;&#25512;&#23548;&#35299;&#39064;&#34920;&#36798;&#24335;&#30340;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21463;&#21040;&#36755;&#20837;&#25991;&#26412;&#21644;&#36755;&#20986;&#34920;&#36798;&#24335;&#20043;&#38388;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#27965;&#25512;&#29702;&#26694;&#26550;SCR&#65292;&#35797;&#22270;&#37319;&#29992;&#20462;&#21098;&#31574;&#30053;&#26469;&#32416;&#27491;&#36755;&#20986;&#20998;&#24067;&#20559;&#31227;&#65292;&#20197;&#38544;&#24335;&#20462;&#22797;&#36825;&#20123;&#34394;&#20551;&#30456;&#20851;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20462;&#21098;roberta2tree&#27169;&#22411;&#26469;&#33719;&#21462;&#19968;&#20010;&#23376;&#32593;&#32476;&#65292;&#20197;&#20415;&#21033;&#29992;&#21407;&#22987;roberta2tree&#27169;&#22411;&#21644;&#34987;&#20462;&#21098;&#23376;&#32593;&#32476;&#20043;&#38388;&#30340;&#36755;&#20986;&#20998;&#24067;&#24046;&#36317;&#26469;&#26292;&#38706;&#34394;&#20551;&#30456;&#20851;&#26679;&#26412;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#31216;Kullback-Leibler&#25955;&#24230;&#26469;&#26657;&#20934;&#36755;&#20986;&#20998;&#24067;&#20559;&#31227;&#65292;&#20197;&#20943;&#36731;&#34394;&#20551;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;SCR&#29983;&#25104;&#31561;&#25928;&#34920;&#36798;&#24335;&#65292;&#20174;&#32780;&#25429;&#25417;&#21407;&#22987;&#25991;&#26412;&#30340;&#36923;&#36753;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20110;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.15373v2 Announce Type: replace  Abstract: Math word problems (MWPs) is a task that automatically derives solution expression from a giving math problems in text. The previous studies suffer from spurious correlations between input text and output expression. To mitigate this issue, we propose a self-consistent reasoning framework called SCR, which attempts to adopt a pruning strategy to correct the output distribution shift so as to implicitly fix those spurious correlative samples. Specifically, we firstly obtain a sub-network by pruning a roberta2tree model, for the sake to use the gap on output distribution between the original roberta2tree model and the pruned sub-network to expose spurious correlative samples. Then, we calibrate the output distribution shift by applying symmetric Kullback-Leibler divergence to alleviate spurious correlations. In addition, SCR generates equivalent expressions, thereby, capturing the original text's logic rather than relying on hints from
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#20174;&#26410;&#20998;&#21106;&#30340;&#28921;&#39274;&#35270;&#39057;&#20013;&#29983;&#25104;&#27491;&#30830;&#30340;&#39135;&#35889;&#65292;&#36890;&#36807;&#36873;&#25321;&#31070;&#35861;&#20107;&#20214;&#24182;&#37325;&#26032;&#29983;&#25104;&#21477;&#23376;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2209.10134</link><description>&lt;p&gt;
&#26469;&#33258;&#26410;&#20998;&#21106;&#28921;&#39274;&#35270;&#39057;&#30340;&#39135;&#35889;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Recipe Generation from Unsegmented Cooking Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.10134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#20174;&#26410;&#20998;&#21106;&#30340;&#28921;&#39274;&#35270;&#39057;&#20013;&#29983;&#25104;&#27491;&#30830;&#30340;&#39135;&#35889;&#65292;&#36890;&#36807;&#36873;&#25321;&#31070;&#35861;&#20107;&#20214;&#24182;&#37325;&#26032;&#29983;&#25104;&#21477;&#23376;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20174;&#26410;&#20998;&#21106;&#30340;&#28921;&#39274;&#35270;&#39057;&#20013;&#29983;&#25104;&#39135;&#35889;&#30340;&#38382;&#39064;&#65292;&#36825;&#38656;&#35201;&#20195;&#29702;&#31243;&#24207;&#25552;&#21462;&#23436;&#25104;&#33756;&#32948;&#26102;&#30340;&#20851;&#38190;&#20107;&#20214;&#24182;&#20026;&#25552;&#21462;&#30340;&#20107;&#20214;&#29983;&#25104;&#21477;&#23376;&#12290;&#25105;&#20204;&#30340;&#20219;&#21153;&#31867;&#20284;&#20110;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#65288;DVC&#65289;&#65292;&#20854;&#30446;&#26631;&#26159;&#24443;&#24213;&#26816;&#27979;&#20107;&#20214;&#24182;&#20026;&#20854;&#29983;&#25104;&#21477;&#23376;&#12290;&#28982;&#32780;&#65292;&#19982;DVC&#19981;&#21516;&#65292;&#23545;&#20110;&#39135;&#35889;&#29983;&#25104;&#65292;&#39135;&#35889;&#25925;&#20107;&#24847;&#35782;&#33267;&#20851;&#37325;&#35201;&#65292;&#27169;&#22411;&#24212;&#35813;&#25353;&#27491;&#30830;&#39034;&#24207;&#25552;&#21462;&#36866;&#24403;&#25968;&#37327;&#30340;&#20107;&#20214;&#24182;&#22522;&#20110;&#23427;&#20204;&#29983;&#25104;&#20934;&#30830;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;DVC&#27169;&#22411;&#30340;&#36755;&#20986;&#24182;&#30830;&#35748;&#65292;&#23613;&#31649;&#20960;&#20010;&#20107;&#20214;&#21487;&#20197;&#34987;&#37319;&#29992;&#20316;&#20026;&#39135;&#35889;&#25925;&#20107;&#65292;&#20294;&#20026;&#36825;&#20123;&#20107;&#20214;&#29983;&#25104;&#30340;&#21477;&#23376;&#19982;&#35270;&#35273;&#20869;&#23481;&#24182;&#19981;&#30456;&#20851;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20174;&#36755;&#20986;&#20107;&#20214;&#20013;&#36873;&#25321;&#31070;&#35861;&#20107;&#20214;&#24182;&#20026;&#20854;&#37325;&#26032;&#29983;&#25104;&#21477;&#23376;&#26469;&#33719;&#24471;&#27491;&#30830;&#30340;&#39135;&#35889;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22810;&#27169;&#24335;r
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.10134v2 Announce Type: replace-cross  Abstract: This paper tackles recipe generation from unsegmented cooking videos, a task that requires agents to (1) extract key events in completing the dish and (2) generate sentences for the extracted events. Our task is similar to dense video captioning (DVC), which aims at detecting events thoroughly and generating sentences for them. However, unlike DVC, in recipe generation, recipe story awareness is crucial, and a model should extract an appropriate number of events in the correct order and generate accurate sentences based on them. We analyze the output of the DVC model and confirm that although (1) several events are adoptable as a recipe story, (2) the generated sentences for such events are not grounded in the visual content. Based on this, we set our goal to obtain correct recipes by selecting oracle events from the output events and re-generating sentences for them. To achieve this, we propose a transformer-based multimodal r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#31232;&#30095;&#30697;&#38453;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26032;&#39046;&#22495;&#36827;&#34892;&#35789;&#23884;&#20837;&#30340;&#20256;&#36882;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#21333;&#35789;&#21547;&#20041;&#24046;&#24322;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2104.08928</link><description>&lt;p&gt;
&#22522;&#20110;&#32676;&#31232;&#30095;&#30697;&#38453;&#20998;&#35299;&#30340;&#35789;&#23884;&#20837;&#20256;&#36882;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Group-Sparse Matrix Factorization for Transfer Learning of Word Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2104.08928
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#31232;&#30095;&#30697;&#38453;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26032;&#39046;&#22495;&#36827;&#34892;&#35789;&#23884;&#20837;&#30340;&#20256;&#36882;&#23398;&#20064;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#21333;&#35789;&#21547;&#20041;&#24046;&#24322;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20026;&#35768;&#22810;&#39046;&#22495;&#30340;&#20915;&#31574;&#32773;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#25968;&#25454;&#28304;&#65292;&#28085;&#30422;&#33539;&#22260;&#20174;&#38646;&#21806;&#20013;&#30340;&#20135;&#21697;&#35780;&#35770;&#21040;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#25252;&#29702;&#35760;&#24405;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#65292;&#36890;&#24120;&#20250;&#36890;&#36807;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65288;&#22914;&#30697;&#38453;&#20998;&#35299;&#65289;&#23558;&#21333;&#35789;&#36716;&#25442;&#20026;&#35789;&#23884;&#20837;&#8212;&#8212;&#32534;&#30721;&#21333;&#35789;&#20043;&#38388;&#35821;&#20041;&#20851;&#31995;&#30340;&#21521;&#37327;&#12290;&#28982;&#32780;&#65292;&#20174;&#20855;&#26377;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#30340;&#26032;&#39046;&#22495;&#23398;&#20064;&#21333;&#35789;&#23884;&#20837;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#22312;&#26032;&#39046;&#22495;&#20013;&#65292;&#21333;&#35789;&#30340;&#21547;&#20041;/&#29992;&#27861;&#21487;&#33021;&#19981;&#21516;&#65292;&#20363;&#22914;&#65292;&#8220;positive&#8221;&#19968;&#35789;&#36890;&#24120;&#20855;&#26377;&#27491;&#38754;&#24773;&#32490;&#65292;&#20294;&#22312;&#21307;&#30103;&#35760;&#24405;&#20013;&#24448;&#24448;&#20855;&#26377;&#36127;&#38754;&#24773;&#32490;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#24847;&#21619;&#30528;&#24739;&#32773;&#26816;&#27979;&#21576;&#38451;&#24615;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#39044;&#35745;&#21482;&#26377;&#23569;&#37327;&#39046;&#22495;&#29305;&#23450;&#21333;&#35789;&#21487;&#33021;&#20855;&#26377;&#26032;&#21547;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#35266;&#30340;&#20004;&#38454;&#27573;&#20272;&#35745;&#22120;&#65292;&#36890;&#36807;&#32676;&#31232;&#30095;&#24809;&#32602;&#26469;&#26377;&#25928;&#22320;&#20256;&#36882;&#23398;&#20064;&#39046;&#22495;&#29305;&#23450;&#30340;&#26032;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2104.08928v3 Announce Type: replace-cross  Abstract: Unstructured text provides decision-makers with a rich data source in many domains, ranging from product reviews in retail to nursing notes in healthcare. To leverage this information, words are typically translated into word embeddings -- vectors that encode the semantic relationships between words -- through unsupervised learning algorithms such as matrix factorization. However, learning word embeddings from new domains with limited training data can be challenging, because the meaning/usage may be different in the new domain, e.g., the word ``positive'' typically has positive sentiment, but often has negative sentiment in medical notes since it may imply that a patient tested positive for a disease. In practice, we expect that only a small number of domain-specific words may have new meanings. We propose an intuitive two-stage estimator that exploits this structure via a group-sparse penalty to efficiently transfer learn dom
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#26356;&#20840;&#38754;&#30340;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#25152;&#26377;&#32452;&#20214;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.17043</link><description>&lt;p&gt;
CRUD-RAG: &#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#20013;&#25991;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models. (arXiv:2401.17043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17043
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#26356;&#20840;&#38754;&#30340;&#20013;&#25991;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#31995;&#32479;&#30340;&#25152;&#26377;&#32452;&#20214;&#22312;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#28304;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#30340;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;LLM&#30340;&#24120;&#35265;&#38480;&#21046;&#65292;&#21253;&#25324;&#36807;&#26102;&#30340;&#20449;&#24687;&#21644;&#20135;&#29983;&#19981;&#20934;&#30830;&#30340;&#8220;&#34394;&#26500;&#8221;&#20869;&#23481;&#30340;&#20542;&#21521;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;RAG&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#22312;&#33539;&#22260;&#21644;&#22810;&#26679;&#24615;&#19978;&#23384;&#22312;&#38480;&#21046;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#35780;&#20272;&#38382;&#31572;&#24212;&#29992;&#65292;&#24573;&#35270;&#20102;RAG&#21487;&#33021;&#26377;&#20248;&#21183;&#30340;&#26356;&#24191;&#27867;&#30340;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#21482;&#35780;&#20272;RAG&#27969;&#31243;&#20013;LLM&#32452;&#20214;&#30340;&#24615;&#33021;&#65292;&#24182;&#24573;&#35270;&#26816;&#32034;&#32452;&#20214;&#21644;&#22806;&#37096;&#30693;&#35782;&#25968;&#25454;&#24211;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#26356;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#22312;&#21508;&#31181;RAG&#24212;&#29992;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;RAG&#31995;&#32479;&#30340;&#25152;&#26377;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of large language models (LLMs) by incorporating external knowledge sources. This method addresses common LLM limitations, including outdated information and the tendency to produce inaccurate "hallucinated" content. However, the evaluation of RAG systems is challenging, as existing benchmarks are limited in scope and diversity. Most of the current benchmarks predominantly assess question-answering applications, overlooking the broader spectrum of situations where RAG could prove advantageous. Moreover, they only evaluate the performance of the LLM component of the RAG pipeline in the experiments, and neglect the influence of the retrieval component and the external knowledge database. To address these issues, this paper constructs a large-scale and more comprehensive benchmark, and evaluates all the components of RAG systems in various RAG application scenarios. Specifically, we have categorized the ran
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#19994;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26469;&#25552;&#21319;&#33258;&#21160;&#29983;&#25104;&#25253;&#21578;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#30340;&#27169;&#22411;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.16578</link><description>&lt;p&gt;
&#21457;&#25381;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#38271;&#65292;&#25552;&#21319;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;LLM&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Leveraging Professional Radiologists' Expertise to Enhance LLMs' Evaluation for Radiology Reports. (arXiv:2401.16578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16578
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#19994;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26469;&#25552;&#21319;&#33258;&#21160;&#29983;&#25104;&#25253;&#21578;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#30340;&#27169;&#22411;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25918;&#23556;&#23398;&#39046;&#22495;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#22823;&#22823;&#25512;&#36827;&#20102;&#25253;&#21578;&#29983;&#25104;&#65292;&#20294;&#33258;&#21160;&#29983;&#25104;&#25253;&#21578;&#30340;&#33258;&#21160;&#35780;&#20272;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#21644;&#20020;&#24202;&#25928;&#33021;&#65288;CE&#65289;&#65292;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#20020;&#24202;&#32972;&#26223;&#30340;&#35821;&#20041;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#36807;&#20998;&#24378;&#35843;&#20020;&#24202;&#32454;&#33410;&#65292;&#38477;&#20302;&#20102;&#25253;&#21578;&#30340;&#28165;&#26224;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#19994;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#21644;GPT-4 1&#65292;&#30456;&#32467;&#21512;&#12290;&#21033;&#29992;&#19978;&#19979;&#25991;&#25351;&#23548;&#23398;&#20064;&#65288;ICIL&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#29702;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;LLM&#30340;&#35780;&#20272;&#19982;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#26631;&#20934;&#20445;&#25345;&#19968;&#33268;&#65292;&#23454;&#29616;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25253;&#21578;&#19982;&#20154;&#31867;&#29983;&#25104;&#25253;&#21578;&#20043;&#38388;&#30340;&#35814;&#32454;&#27604;&#36739;&#12290;&#36825;&#36827;&#19968;&#27493;&#36890;&#36807;&#22238;&#24402;&#27169;&#22411;&#26469;&#32508;&#21512;&#21477;&#23376;&#35780;&#20272;&#20998;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#8220;&#35814;&#32454;GPT-4&#65288;5&#27425;&#35757;&#32451;&#65289;&#8221;&#27169;&#22411;&#33719;&#24471;&#20102;0.48&#30340;&#20998;&#25968;&#65292;&#20248;&#20110;METEOR&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
In radiology, Artificial Intelligence (AI) has significantly advanced report generation, but automatic evaluation of these AI-produced reports remains challenging. Current metrics, such as Conventional Natural Language Generation (NLG) and Clinical Efficacy (CE), often fall short in capturing the semantic intricacies of clinical contexts or overemphasize clinical details, undermining report clarity. To overcome these issues, our proposed method synergizes the expertise of professional radiologists with Large Language Models (LLMs), like GPT-3.5 and GPT-4 1. Utilizing In-Context Instruction Learning (ICIL) and Chain of Thought (CoT) reasoning, our approach aligns LLM evaluations with radiologist standards, enabling detailed comparisons between human and AI generated reports. This is further enhanced by a Regression model that aggregates sentence evaluation scores. Experimental results show that our ''Detailed GPT-4 (5-shot)'' model achieves a 0.48 score, outperforming the METEOR metric 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#24773;&#24863;&#26816;&#27979;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#24314;&#27169;&#65292;&#36890;&#36807;&#22312;&#20449;&#24565;&#29366;&#24577;&#36319;&#36394;&#20013;&#24341;&#20837;&#24773;&#24863;&#26816;&#27979;&#23454;&#29616;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#31471;&#21040;&#31471;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#21644;&#20219;&#21153;&#32467;&#26524;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#31034;&#29992;&#25143;&#30340;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#22238;&#24212;&#30340;&#19978;&#19979;&#25991;&#26465;&#20214;&#65292;&#23545;&#20110;&#25552;&#39640;&#22238;&#24212;&#30340;&#20849;&#40483;&#31243;&#24230;&#20855;&#26377;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2401.13789</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#24773;&#24863;&#26816;&#27979;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Unified Approach to Emotion Detection and Task-Oriented Dialogue Modeling. (arXiv:2401.13789v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#24773;&#24863;&#26816;&#27979;&#21644;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#24314;&#27169;&#65292;&#36890;&#36807;&#22312;&#20449;&#24565;&#29366;&#24577;&#36319;&#36394;&#20013;&#24341;&#20837;&#24773;&#24863;&#26816;&#27979;&#23454;&#29616;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#31471;&#21040;&#31471;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#21644;&#20219;&#21153;&#32467;&#26524;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#31034;&#29992;&#25143;&#30340;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#22238;&#24212;&#30340;&#19978;&#19979;&#25991;&#26465;&#20214;&#65292;&#23545;&#20110;&#25552;&#39640;&#22238;&#24212;&#30340;&#20849;&#40483;&#31243;&#24230;&#20855;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#22522;&#20110;&#25991;&#26412;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#31995;&#32479;&#20013;&#65292;&#29992;&#25143;&#24773;&#24863;&#26816;&#27979;&#65288;ED&#65289;&#32463;&#24120;&#34987;&#24573;&#35270;&#65292;&#25110;&#32773;&#36890;&#24120;&#34987;&#35270;&#20026;&#19968;&#39033;&#29420;&#31435;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#26080;&#32541;&#22320;&#32479;&#19968;ED&#21644;TOD&#24314;&#27169;&#21487;&#20197;&#24102;&#26469;&#30456;&#20114;&#30340;&#22909;&#22788;&#65292;&#22240;&#27492;&#26159;&#19968;&#31181;&#20540;&#24471;&#32771;&#34385;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#23558;ED&#21253;&#21547;&#22312;&#20449;&#24565;&#29366;&#24577;&#36319;&#36394;&#20013;&#65292;&#24182;&#20381;&#36182;&#20110;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26469;&#25193;&#23637;SimpleToD&#36825;&#20010;&#31471;&#21040;&#31471;&#30340;TOD&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;GPT-2&#21644;Llama-2&#22312;EmoWOZ&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#24773;&#24863;&#36827;&#34892;&#27880;&#37322;&#30340;MultiWOZ&#29256;&#26412;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;ED&#21644;&#20219;&#21153;&#32467;&#26524;&#30340;&#24615;&#33021;&#26222;&#36941;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#29992;&#25143;&#30340;&#24773;&#24863;&#20026;&#31995;&#32479;&#30340;&#22238;&#24212;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#19978;&#19979;&#25991;&#26465;&#20214;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#36827;&#19968;&#27493;&#25913;&#21892;&#22238;&#24212;&#30340;&#20849;&#40483;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
In current text-based task-oriented dialogue (TOD) systems, user emotion detection (ED) is often overlooked or is typically treated as a separate and independent task, requiring additional training. In contrast, our work demonstrates that seamlessly unifying ED and TOD modeling brings about mutual benefits, and is therefore an alternative to be considered. Our method consists in augmenting SimpleToD, an end-to-end TOD system, by extending belief state tracking to include ED, relying on a single language model. We evaluate our approach using GPT-2 and Llama-2 on the EmoWOZ benchmark, a version of MultiWOZ annotated with emotions. Our results reveal a general increase in performance for ED and task results. Our findings also indicate that user emotions provide useful contextual conditioning for system responses, and can be leveraged to further refine responses in terms of empathy.
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLMs&#65289;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35757;&#32451;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#36755;&#20837;&#21644;&#36755;&#20986;&#25903;&#25345;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#25253;&#21578;&#65292;&#20171;&#32461;&#20102;MM-LLMs&#30340;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#26696;&#65292;&#25972;&#29702;&#20102;&#29616;&#26377;&#30340;MM-LLMs&#21450;&#20854;&#24615;&#33021;&#65292;&#24635;&#32467;&#20102;&#20851;&#38190;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2401.13601</link><description>&lt;p&gt;
MM-LLMs: &#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
MM-LLMs: Recent Advances in MultiModal Large Language Models. (arXiv:2401.13601v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13601
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLMs&#65289;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35757;&#32451;&#31574;&#30053;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#36755;&#20837;&#21644;&#36755;&#20986;&#25903;&#25345;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#35843;&#26597;&#25253;&#21578;&#65292;&#20171;&#32461;&#20102;MM-LLMs&#30340;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#26696;&#65292;&#25972;&#29702;&#20102;&#29616;&#26377;&#30340;MM-LLMs&#21450;&#20854;&#24615;&#33021;&#65292;&#24635;&#32467;&#20102;&#20851;&#38190;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#19968;&#24180;&#20013;&#65292;&#22810;&#27169;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MM-LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#22686;&#24378;&#20102;&#29616;&#26377;&#30340;LLMs&#23545;&#22810;&#27169;&#36755;&#20837;&#25110;&#36755;&#20986;&#30340;&#25903;&#25345;&#12290;&#36825;&#20123;&#32467;&#26524;&#27169;&#22411;&#19981;&#20165;&#20445;&#30041;&#20102;LLMs&#22266;&#26377;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#65292;&#36824;&#36171;&#20104;&#20102;&#21508;&#31181;&#22810;&#27169;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20221;&#32508;&#21512;&#24615;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;MM-LLMs&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#27969;&#31243;&#30340;&#19968;&#33324;&#35774;&#35745;&#26041;&#26696;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#31616;&#35201;&#20171;&#32461;&#20102;26&#31181;&#29616;&#26377;&#30340;MM-LLMs&#65292;&#27599;&#31181;&#37117;&#20197;&#20854;&#20855;&#20307;&#30340;&#20844;&#24335;&#20026;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;MM-LLMs&#22312;&#20027;&#27969;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#24635;&#32467;&#20102;&#25552;&#39640;MM-LLMs&#25928;&#21147;&#30340;&#20851;&#38190;&#35757;&#32451;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;MM-LLMs&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#21516;&#26102;&#36824;&#20026;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#25552;&#20379;&#20102;&#23454;&#26102;&#36861;&#36394;&#32593;&#31449;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20221;&#35843;&#26597;&#25253;&#21578;&#33021;&#22815;&#20419;&#36827;&#23545;MM-LLMs&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past year, MultiModal Large Language Models (MM-LLMs) have undergone substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or outputs via cost-effective training strategies. The resulting models not only preserve the inherent reasoning and decision-making capabilities of LLMs but also empower a diverse range of MM tasks. In this paper, we provide a comprehensive survey aimed at facilitating further research of MM-LLMs. Specifically, we first outline general design formulations for model architecture and training pipeline. Subsequently, we provide brief introductions of $26$ existing MM-LLMs, each characterized by its specific formulations. Additionally, we review the performance of MM-LLMs on mainstream benchmarks and summarize key training recipes to enhance the potency of MM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrently maintaining a real-time tracking website for the latest developments in the field. We hope that this surv
&lt;/p&gt;</description></item><item><title>LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.13586</link><description>&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#30340;&#25552;&#31034;&#26435;&#37325;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
Prompt Weight Experiments for LLM Instruction Fine-Tuning. (arXiv:2401.13586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13586
&lt;/p&gt;
&lt;p&gt;
LLM&#25351;&#20196;&#24494;&#35843;&#20013;&#65292;&#23545;&#20110;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#65292;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#19982;&#24615;&#33021;&#21576;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#21017;&#19981;&#21463;PLW&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#23567;&#22411;&#30740;&#31350;&#65292;&#20998;&#26512;&#20102;&#25552;&#31034;&#35789;&#26631;&#35760;&#20998;&#31867;&#25439;&#22833;&#21152;&#26435;&#65288;PLW&#65289;&#22914;&#20309;&#24433;&#21709;&#22312;&#25351;&#20196;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;7B&#22823;&#23567;&#30340;LLaMA&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#25351;&#20196;&#25968;&#25454;&#38598;&#37325;&#29616;&#20102;&#26031;&#22374;&#31119;&#22823;&#23398;&#30340;Alpaca&#23454;&#39564;&#65292;&#20854;&#20013;&#21253;&#25324;LLaMA 1&#21644;LLaMA 2&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#25105;&#20204;&#30340;&#30701;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19982;PLW&#20043;&#38388;&#23384;&#22312;&#36127;&#20108;&#27425;&#20851;&#31995;&#65292;&#32780;&#22312;&#38271;&#25552;&#31034;&#23436;&#25104;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#19981;&#21463;PLW&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a small study analyzing how prompt token classification loss weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1 and LLaMA 2 using multiple instruction datasets. We found that models fine-tuned on our short-completion dataset have a negative quadratic relationship with PLW while models fine-tuned on long-completion datasets were unaffected by PLW.
&lt;/p&gt;</description></item><item><title>&#24320;&#25918;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#33021;&#22815;&#20174;&#21508;&#31181;&#26631;&#20934;&#25968;&#25454;&#26684;&#24335;&#20013;&#29983;&#25104;&#27969;&#30021;&#21644;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#20294;&#26159;&#36755;&#20986;&#30340;&#35821;&#20041;&#20934;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10186</link><description>&lt;p&gt;
&#36229;&#36234;&#22522;&#20110;&#21442;&#32771;&#25351;&#26631;&#65306;&#20998;&#26512;&#24320;&#25918;&#24335;LLM&#22312;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#19978;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on Data-to-Text Generation. (arXiv:2401.10186v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10186
&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#35774;&#32622;&#19979;&#33021;&#22815;&#20174;&#21508;&#31181;&#26631;&#20934;&#25968;&#25454;&#26684;&#24335;&#20013;&#29983;&#25104;&#27969;&#30021;&#21644;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#20294;&#26159;&#36755;&#20986;&#30340;&#35821;&#20041;&#20934;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#24320;&#25918;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#29983;&#25104;&#36830;&#36143;&#21644;&#30456;&#20851;&#25991;&#26412;&#26041;&#38754;&#30340;&#31243;&#24230;&#12290;&#20026;&#20102;&#38450;&#27490;&#22522;&#20934;&#27844;&#38706;&#21040;LLM&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;Quintd-1:&#19968;&#20010;&#20026;5&#20010;&#25968;&#25454;&#21040;&#25991;&#26412;(D2T)&#29983;&#25104;&#20219;&#21153;&#35774;&#35745;&#30340;&#19987;&#38376;&#22522;&#20934;&#65292;&#35813;&#20219;&#21153;&#21253;&#25324;&#20174;&#20844;&#20849;API&#20013;&#25910;&#38598;&#30340;&#26631;&#20934;&#26684;&#24335;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#35760;&#24405;&#12290;&#25105;&#20204;&#21033;&#29992;&#26080;&#21442;&#32771;&#35780;&#20272;&#25351;&#26631;&#21644;LLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#27809;&#26377;&#20154;&#24037;&#20889;&#20316;&#21442;&#32771;&#36164;&#26009;&#30340;&#24773;&#20917;&#19979;&#27979;&#35797;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#37325;&#28857;&#26159;&#22312;token&#32423;&#21035;&#19978;&#23545;&#35821;&#20041;&#20934;&#30830;&#24615;&#38169;&#35823;&#36827;&#34892;&#27880;&#37322;&#65292;&#32467;&#21512;&#20154;&#31867;&#26631;&#27880;&#32773;&#21644;&#22522;&#20110;GPT-4&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;7B&#21442;&#25968;&#30340;&#26368;&#20808;&#36827;&#24320;&#25918;&#24335;LLMs&#21487;&#20197;&#22312;&#38646;-shot&#35774;&#32622;&#20013;&#20174;&#21508;&#31181;&#26631;&#20934;&#25968;&#25454;&#26684;&#24335;&#20013;&#29983;&#25104;&#27969;&#30021;&#21644;&#36830;&#36143;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#34920;&#26126;&#36755;&#20986;&#30340;&#35821;&#20041;&#20934;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#38382;&#39064;&#65306;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#65292;80%&#30340;&#36755;&#20986;&#23384;&#22312;&#35821;&#20041;&#20934;&#30830;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate to which extent open large language models (LLMs) can generate coherent and relevant text from structured data. To prevent bias from benchmarks leaked into LLM training data, we collect Quintd-1: an ad-hoc benchmark for five data-to-text (D2T) generation tasks, consisting of structured data records in standard formats gathered from public APIs. We leverage reference-free evaluation metrics and LLMs' in-context learning capabilities, allowing us to test the models with no human-written references. Our evaluation focuses on annotating semantic accuracy errors on token-level, combining human annotators and a metric based on GPT-4. Our systematic examination of the models' behavior across domains and tasks suggests that state-of-the-art open LLMs with 7B parameters can generate fluent and coherent text from various standard data formats in zero-shot settings. However, we also show that semantic accuracy of the outputs remains a major issue: on our benchmark, 80% of outputs o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21028;&#26029;&#23433;&#20840;&#39118;&#38505;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;R-Judge&#65292;&#36890;&#36807;&#23545;162&#20010;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#36798;&#21040;&#20102;72.29%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.10019</link><description>&lt;p&gt;
R-Judge: &#35780;&#20272;LLM&#20195;&#29702;&#30340;&#23433;&#20840;&#39118;&#38505;&#24847;&#35782;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
R-Judge: Benchmarking Safety Risk Awareness for LLM Agents. (arXiv:2401.10019v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10019
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20027;&#35201;&#20171;&#32461;&#20102;&#19968;&#31181;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#21028;&#26029;&#23433;&#20840;&#39118;&#38505;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;R-Judge&#65292;&#36890;&#36807;&#23545;162&#20010;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#36798;&#21040;&#20102;72.29%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#23436;&#25104;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLM&#20195;&#29702;&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#25805;&#20316;&#26102;&#20250;&#24341;&#20837;&#24847;&#22806;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#19982;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#23433;&#20840;&#24615;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#35780;&#20272;LLM&#20195;&#29702;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#30340;&#34892;&#20026;&#23433;&#20840;&#24615;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;R-Judge&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#22312;&#32473;&#23450;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#26102;&#21028;&#26029;&#23433;&#20840;&#39118;&#38505;&#30340;&#33021;&#21147;&#12290;R-Judge&#21253;&#25324;162&#20010;&#20195;&#29702;&#20132;&#20114;&#35760;&#24405;&#65292;&#28085;&#30422;7&#20010;&#24212;&#29992;&#39046;&#22495;&#21644;10&#31181;&#39118;&#38505;&#31867;&#22411;&#30340;27&#20010;&#20851;&#38190;&#39118;&#38505;&#22330;&#26223;&#12290;&#23427;&#32467;&#21512;&#20102;&#20154;&#31867;&#23545;&#23433;&#20840;&#24615;&#30340;&#20849;&#35782;&#65292;&#24182;&#20855;&#26377;&#26631;&#35760;&#30340;&#23433;&#20840;&#39118;&#38505;&#26631;&#31614;&#21644;&#39640;&#36136;&#37327;&#30340;&#39118;&#38505;&#25551;&#36848;&#12290;&#21033;&#29992;R-Judge&#65292;&#25105;&#20204;&#23545;8&#31181;&#24120;&#29992;&#20316;&#20195;&#29702;&#39592;&#24178;&#30340;&#33879;&#21517;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4&#23454;&#29616;&#20102;72.29%&#30340;&#23545;&#27604;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited great potential in autonomously completing tasks across real-world applications. Despite this, these LLM agents introduce unexpected safety risks when operating in interactive environments. Instead of centering on LLM-generated content safety in most prior studies, this work addresses the imperative need for benchmarking the behavioral safety of LLM agents within diverse environments. We introduce R-Judge, a benchmark crafted to evaluate the proficiency of LLMs in judging safety risks given agent interaction records. R-Judge comprises 162 agent interaction records, encompassing 27 key risk scenarios among 7 application categories and 10 risk types. It incorporates human consensus on safety with annotated safety risk labels and high-quality risk descriptions. Utilizing R-Judge, we conduct a comprehensive evaluation of 8 prominent LLMs commonly employed as the backbone for agents. The best-performing model, GPT-4, achieves 72.29% in contrast to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;INTERS&#65292;&#28085;&#30422;&#20102;21&#20010;IR&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06532</link><description>&lt;p&gt;
INTERS: &#20351;&#29992;&#25351;&#20196;&#35843;&#20248;&#35299;&#38145;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25628;&#32034;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning. (arXiv:2401.06532v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;INTERS&#65292;&#28085;&#30422;&#20102;21&#20010;IR&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35768;&#22810;&#19982;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#20855;&#20307;&#27010;&#24565;&#30340;&#19981;&#32463;&#24120;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#65292;&#23427;&#20204;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#21487;&#20197;&#21521;LLMs&#25552;&#20379;&#20219;&#21153;&#25551;&#36848;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#22312;&#20419;&#36827;&#20840;&#38754;&#29702;&#35299;&#21644;&#25191;&#34892;IR&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;LLMs&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#28508;&#21147;&#65292;&#20197;&#25552;&#39640;LLMs&#22312;IR&#20219;&#21153;&#20013;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;INTERS&#65292;&#28085;&#30422;&#20102;3&#20010;&#22522;&#26412;IR&#31867;&#21035;&#20013;&#30340;21&#20010;&#20219;&#21153;&#65306;&#26597;&#35810;&#29702;&#35299;&#12289;&#25991;&#26723;&#29702;&#35299;&#21644;&#26597;&#35810;&#25991;&#26723;&#20851;&#31995;&#29702;&#35299;&#12290;&#25968;&#25454;&#26469;&#33258;43&#20010;&#19981;&#21516;&#30340;&#30001;&#25163;&#21160;&#32534;&#20889;&#30340;&#27169;&#26495;&#26500;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;INTERS&#26174;&#33879;&#25552;&#21319;&#20102;&#21508;&#31181;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks. Despite this, their application to information retrieval (IR) tasks is still challenging due to the infrequent occurrence of many IR-specific concepts in natural language. While prompt-based methods can provide task descriptions to LLMs, they often fall short in facilitating comprehensive understanding and execution of IR tasks, thereby limiting LLMs' applicability. To address this gap, in this work, we explore the potential of instruction tuning to enhance LLMs' proficiency in IR tasks. We introduce a novel instruction tuning dataset, INTERS, encompassing 21 tasks across three fundamental IR categories: query understanding, document understanding, and query-document relationship understanding. The data are derived from 43 distinct datasets with manually written templates. Our empirical results reveal that INTERS significantly boosts the performance of various publicly a
&lt;/p&gt;</description></item><item><title>EASYTOOL&#26159;&#19968;&#20010;&#23558;&#22810;&#26679;&#21270;&#32780;&#20887;&#38271;&#30340;&#24037;&#20855;&#25991;&#26723;&#36716;&#21270;&#20026;&#32479;&#19968;&#32780;&#31616;&#27905;&#30340;&#24037;&#20855;&#25351;&#31034;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20174;&#22810;&#20010;&#26469;&#28304;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#26631;&#20934;&#21270;&#30340;&#24037;&#20855;&#25551;&#36848;&#21644;&#21151;&#33021;&#65292;EasyTool&#26174;&#33879;&#38477;&#20302;&#20102;&#26631;&#35760;&#28040;&#32791;&#65292;&#24182;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24037;&#20855;&#21033;&#29992;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06201</link><description>&lt;p&gt;
EASYTOOL: &#20351;&#29992;&#31616;&#27905;&#30340;&#24037;&#20855;&#25351;&#31034;&#22686;&#24378;&#22522;&#20110;LLM&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction. (arXiv:2401.06201v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06201
&lt;/p&gt;
&lt;p&gt;
EASYTOOL&#26159;&#19968;&#20010;&#23558;&#22810;&#26679;&#21270;&#32780;&#20887;&#38271;&#30340;&#24037;&#20855;&#25991;&#26723;&#36716;&#21270;&#20026;&#32479;&#19968;&#32780;&#31616;&#27905;&#30340;&#24037;&#20855;&#25351;&#31034;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20174;&#22810;&#20010;&#26469;&#28304;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#26631;&#20934;&#21270;&#30340;&#24037;&#20855;&#25551;&#36848;&#21644;&#21151;&#33021;&#65292;EasyTool&#26174;&#33879;&#38477;&#20302;&#20102;&#26631;&#35760;&#28040;&#32791;&#65292;&#24182;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24037;&#20855;&#21033;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#25918;&#22312;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24212;&#29992;&#20013;&#30340;&#24037;&#20855;&#21033;&#29992;&#19978;&#12290;&#20026;&#20102;&#24320;&#21457;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#36890;&#24120;&#38656;&#35201;LLM&#20174;&#19981;&#21516;&#30340;&#24037;&#20855;&#25991;&#26723;&#20013;&#29702;&#35299;&#35768;&#22810;&#24037;&#20855;&#21151;&#33021;&#12290;&#20294;&#36825;&#20123;&#25991;&#26723;&#21487;&#33021;&#26159;&#22810;&#26679;&#21270;&#30340;&#12289;&#20887;&#20313;&#30340;&#25110;&#19981;&#23436;&#25972;&#30340;&#65292;&#36825;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;LLM&#22312;&#20351;&#29992;&#24037;&#20855;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EASYTOOL&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#22810;&#26679;&#21270;&#32780;&#20887;&#38271;&#30340;&#24037;&#20855;&#25991;&#26723;&#36716;&#21270;&#20026;&#32479;&#19968;&#32780;&#31616;&#27905;&#30340;&#24037;&#20855;&#25351;&#31034;&#65292;&#20197;&#20415;&#26356;&#23481;&#26131;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;EasyTool&#20174;&#19981;&#21516;&#26469;&#28304;&#30340;&#24191;&#27867;&#24037;&#20855;&#25991;&#26723;&#20013;&#25552;&#21462;&#20986;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#35814;&#32454;&#35828;&#26126;&#19968;&#20010;&#32479;&#19968;&#30340;&#25509;&#21475;&#65288;&#21363;&#24037;&#20855;&#25351;&#31034;&#65289;&#65292;&#20026;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#25552;&#20379;&#26631;&#20934;&#21270;&#30340;&#24037;&#20855;&#25551;&#36848;&#21644;&#21151;&#33021;&#12290;&#23545;&#22810;&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;EasyTool&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#26631;&#35760;&#30340;&#28040;&#32791;&#65292;&#24182;&#25913;&#21892;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24037;&#20855;&#21033;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address intricate real-world tasks, there has been a rising interest in tool utilization in applications of large language models (LLMs). To develop LLM-based agents, it usually requires LLMs to understand many tool functions from different tool documentation. But these documentations could be diverse, redundant or incomplete, which immensely affects the capability of LLMs in using tools. To solve this, we introduce EASYTOOL, a framework transforming diverse and lengthy tool documentation into a unified and concise tool instruction for easier tool usage. EasyTool purifies essential information from extensive tool documentation of different sources, and elaborates a unified interface (i.e., tool instruction) to offer standardized tool descriptions and functionalities for LLM-based agents. Extensive experiments on multiple different tasks demonstrate that EasyTool can significantly reduce token consumption and improve the performance of tool utilization in real-world scenarios. Our co
&lt;/p&gt;</description></item><item><title>SAC^3&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#24863;&#30693;&#20132;&#21449;&#26816;&#26597;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21152;&#20837;&#35821;&#20041;&#31561;&#25928;&#38382;&#39064;&#25200;&#21160;&#21644;&#36328;&#27169;&#22411;&#21709;&#24212;&#19968;&#33268;&#24615;&#26816;&#26597;&#31561;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#38382;&#39064;&#32423;&#21035;&#21644;&#27169;&#22411;&#32423;&#21035;&#30340;&#24187;&#35273;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;SAC^3&#22312;&#26816;&#27979;&#38750;&#20107;&#23454;&#21644;&#20107;&#23454;&#38472;&#36848;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26368;&#26032;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2311.01740</link><description>&lt;p&gt;
SAC^3: &#22522;&#20110;&#35821;&#20041;&#24863;&#30693;&#20132;&#21449;&#26816;&#26597;&#19968;&#33268;&#24615;&#30340;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#21487;&#38752;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SAC$^3$: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency. (arXiv:2311.01740v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01740
&lt;/p&gt;
&lt;p&gt;
SAC^3&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#24863;&#30693;&#20132;&#21449;&#26816;&#26597;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21152;&#20837;&#35821;&#20041;&#31561;&#25928;&#38382;&#39064;&#25200;&#21160;&#21644;&#36328;&#27169;&#22411;&#21709;&#24212;&#19968;&#33268;&#24615;&#26816;&#26597;&#31561;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#35782;&#21035;&#38382;&#39064;&#32423;&#21035;&#21644;&#27169;&#22411;&#32423;&#21035;&#30340;&#24187;&#35273;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;SAC^3&#22312;&#26816;&#27979;&#38750;&#20107;&#23454;&#21644;&#20107;&#23454;&#38472;&#36848;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#26368;&#26032;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24187;&#35273;&#26816;&#27979;&#26159;&#20102;&#35299;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20449;&#24230;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#33258;&#19968;&#33268;&#24615;&#30340;&#29616;&#26377;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#20004;&#31181;&#24187;&#35273;&#31867;&#22411;&#65292;&#21363;&#22522;&#20110;&#38382;&#39064;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#24187;&#35273;&#65292;&#23427;&#20204;&#20165;&#36890;&#36807;&#33258;&#19968;&#33268;&#24615;&#26816;&#26597;&#26080;&#27861;&#26377;&#25928;&#35782;&#21035;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#65292;&#21363;&#35821;&#20041;&#24863;&#30693;&#20132;&#21449;&#26816;&#26597;&#19968;&#33268;&#24615;&#65288;SAC^3&#65289;&#65292;&#23427;&#22312;&#33258;&#19968;&#33268;&#24615;&#26816;&#26597;&#21407;&#21017;&#30340;&#22522;&#30784;&#19978;&#21152;&#20837;&#20102;&#39069;&#22806;&#30340;&#26426;&#21046;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#20041;&#31561;&#25928;&#38382;&#39064;&#25200;&#21160;&#21644;&#36328;&#27169;&#22411;&#21709;&#24212;&#19968;&#33268;&#24615;&#26816;&#26597;&#26469;&#26816;&#27979;&#38382;&#39064;&#32423;&#21035;&#21644;&#27169;&#22411;&#32423;&#21035;&#30340;&#24187;&#35273;&#12290;&#36890;&#36807;&#24191;&#27867;&#32780;&#31995;&#32479;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;SAC^3&#22312;&#26816;&#27979;&#38750;&#20107;&#23454;&#21644;&#20107;&#23454;&#38472;&#36848;&#26041;&#38754;&#32988;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC$^3$) that expands on the principle of self-consistency checking. Our SAC$^3$ approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC$^3$ outperforms the state of the art in detecting both non-factual and factual statements across multip
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#24615;&#35843;&#25972;&#39044;&#27979;&#65292;&#21033;&#29992;&#22024;&#26434;&#25351;&#20196;&#26469;&#22686;&#24378;&#25351;&#20196;&#35843;&#33410;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#38754;&#20020;&#36229;&#20986;&#35757;&#32451;&#33539;&#22260;&#30340;&#25351;&#20196;&#26102;&#30340;&#21709;&#24212;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00233</link><description>&lt;p&gt;
&#25197;&#26354;&#65292;&#20998;&#25955;&#65292;&#35299;&#30721;&#65306;&#25351;&#20196;&#35843;&#33410;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#22024;&#26434;&#30340;&#25351;&#20196;&#25913;&#36827;&#20854;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Distort, Distract, Decode: Instruction-Tuned Model Can Refine its Response from Noisy Instructions. (arXiv:2311.00233v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#24615;&#35843;&#25972;&#39044;&#27979;&#65292;&#21033;&#29992;&#22024;&#26434;&#25351;&#20196;&#26469;&#22686;&#24378;&#25351;&#20196;&#35843;&#33410;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#38754;&#20020;&#36229;&#20986;&#35757;&#32451;&#33539;&#22260;&#30340;&#25351;&#20196;&#26102;&#30340;&#21709;&#24212;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#32463;&#36807;&#25351;&#20196;&#35843;&#33410;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#27867;&#21270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26159;&#24403;&#38754;&#20020;&#36229;&#20986;&#35757;&#32451;&#33539;&#22260;&#30340;&#25351;&#20196;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#38590;&#20197;&#29983;&#25104;&#20934;&#30830;&#30340;&#21709;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Instructive Decoding&#65288;ID&#65289;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;&#25351;&#20196;&#35843;&#33410;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ID&#36890;&#36807;&#23545;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;logits&#36827;&#34892;&#23545;&#27604;&#24615;&#35843;&#25972;&#65292;&#21033;&#29992;&#20174;&#21407;&#22987;&#25351;&#20196;&#30340;&#25805;&#32437;&#29256;&#26412;&#29983;&#25104;&#30340;&#39044;&#27979;&#65292;&#21363;&#22024;&#26434;&#25351;&#20196;&#12290;&#36825;&#20010;&#22024;&#26434;&#25351;&#20196;&#26088;&#22312;&#24341;&#21457;&#19982;&#39044;&#26399;&#25351;&#20196;&#19981;&#19968;&#33268;&#20294;&#20173;&#28982;&#21512;&#29702;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#27492;&#31867;&#22024;&#26434;&#25351;&#20196;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20174;&#36890;&#36807;&#38543;&#26426;&#35789;&#25554;&#20837;&#35821;&#20041;&#22122;&#22768;&#21040;&#20687;&#8220;&#30456;&#21453;&#8221;&#30340;&#20854;&#20182;&#25351;&#20196;&#65292;&#20197;&#24341;&#21457;&#20559;&#31163;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25351;&#20196;&#35843;&#33410;&#27169;&#22411;&#21644;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#26080;&#38656;
&lt;/p&gt;
&lt;p&gt;
While instruction-tuned language models have demonstrated impressive zero-shot generalization, these models often struggle to generate accurate responses when faced with instructions that fall outside their training set. This paper presents Instructive Decoding (ID), a simple yet effective approach that augments the efficacy of instruction-tuned models. Specifically, ID adjusts the logits for next-token prediction in a contrastive manner, utilizing predictions generated from a manipulated version of the original instruction, referred to as a noisy instruction. This noisy instruction aims to elicit responses that could diverge from the intended instruction yet remain plausible. We conduct experiments across a spectrum of such noisy instructions, ranging from those that insert semantic noise via random words to others like 'opposite' that elicit the deviated responses. Our approach achieves considerable performance gains across various instruction-tuned models and tasks without necessita
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20010;&#24615;&#29305;&#28857;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;LLMs&#30340;&#29616;&#20195;&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#22320;&#25551;&#32472;&#20986;&#30456;&#24212;&#35282;&#33394;&#30340;&#20010;&#24615;&#29305;&#28857;&#65292;&#19982;&#20154;&#31867;&#24863;&#30693;&#30340;&#19968;&#33268;&#24615;&#36798;&#21040;82.8%&#12290;</title><link>http://arxiv.org/abs/2310.17976</link><description>&lt;p&gt;
&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22815;&#25429;&#25417;&#35282;&#33394;&#20010;&#24615;&#21527;&#65311;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20010;&#24615;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does Role-Playing Chatbots Capture the Character Personalities? Assessing Personality Traits for Role-Playing Chatbots. (arXiv:2310.17976v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20010;&#24615;&#29305;&#28857;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;LLMs&#30340;&#29616;&#20195;&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#22320;&#25551;&#32472;&#20986;&#30456;&#24212;&#35282;&#33394;&#30340;&#20010;&#24615;&#29305;&#28857;&#65292;&#19982;&#20154;&#31867;&#24863;&#30693;&#30340;&#19968;&#33268;&#24615;&#36798;&#21040;82.8%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#26032;&#22411;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#25171;&#36896;&#20855;&#26377;&#29420;&#29305;&#20154;&#29289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26041;&#38754;&#12290;&#37492;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;"&#21050;&#28608;-&#21709;&#24212;"&#24615;&#36136;&#65292;&#26412;&#25991;&#25581;&#31034;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#24320;&#25918;&#24335;&#37319;&#35775;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20010;&#24615;&#29305;&#28857;&#65292;&#20174;&#32780;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20854;&#20869;&#22312;&#20010;&#24615;&#12290;&#25105;&#20204;&#23545;ChatHaruhi&#22270;&#20070;&#39302;&#21019;&#24314;&#30340;32&#20010;&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#20102;&#22823;&#20116;&#20154;&#26684;&#21644;MBTI&#32500;&#24230;&#19978;&#30340;&#20010;&#24615;&#35780;&#20272;&#65292;&#24182;&#27979;&#37327;&#23427;&#20204;&#19982;&#20154;&#31867;&#24863;&#30693;&#30340;&#19968;&#33268;&#24615;&#12290;&#35780;&#20272;&#32467;&#26524;&#24378;&#35843;&#65292;&#22522;&#20110;LLMs&#30340;&#29616;&#20195;&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#22320;&#25551;&#32472;&#20986;&#30456;&#24212;&#35282;&#33394;&#30340;&#20010;&#24615;&#29305;&#28857;&#65292;&#19982;&#20154;&#31867;&#24863;&#30693;&#30340;&#19968;&#33268;&#24615;&#36798;&#21040;82.8%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#22609;&#36896;&#32842;&#22825;&#26426;&#22120;&#20154;&#20010;&#24615;&#30340;&#28508;&#22312;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20026;&#35282;&#33394;&#25198;&#28436;&#32842;&#22825;&#26426;&#22120;&#20154;&#30740;&#31350;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-scale pretrained language models has revolutionized the capabilities of new AI application, especially in the realm of crafting chatbots with distinct personas. Given the "stimulus-response" nature of chatbots, this paper unveils an innovative open-ended interview-style approach for personality assessment on role-playing chatbots, which offers a richer comprehension of their intrinsic personalities. We conduct personality assessments on 32 role-playing chatbots created by the ChatHaruhi library, across both the Big Five and MBTI dimensions, and measure their alignment with human perception. Evaluation results underscore that modern role-playing chatbots based on LLMs can effectively portray personality traits of corresponding characters, with an alignment rate of 82.8% compared with human-perceived personalities. Besides, we also suggest potential strategies for shaping chatbots' personalities. Hence, this paper serves as a cornerstone study for role-playing chat
&lt;/p&gt;</description></item><item><title>Whisper-MCE&#26159;&#20351;&#29992;&#33258;&#24049;&#25910;&#38598;&#30340;&#28151;&#21512;&#31908;&#35821;&#21644;&#33521;&#35821;&#38899;&#39057;&#25968;&#25454;&#38598;&#65288;MCE&#65289;&#36827;&#34892;&#35757;&#32451;&#30340;Whisper&#27169;&#22411;&#24494;&#35843;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20854;&#22312;&#20934;&#30830;&#25429;&#25417;&#21407;&#22987;&#38899;&#39057;&#20869;&#23481;&#12289;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24615;&#21644;&#21152;&#24555;&#35782;&#21035;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#22312;&#28151;&#21512;&#35821;&#35328;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.17953</link><description>&lt;p&gt;
Whisper-MCE: &#38024;&#23545;&#28151;&#21512;&#35821;&#35328;&#23454;&#29616;&#26356;&#22909;&#24615;&#33021;&#30340;Whisper&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Whisper-MCE: Whisper Model Finetuned for Better Performance with Mixed Languages. (arXiv:2310.17953v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17953
&lt;/p&gt;
&lt;p&gt;
Whisper-MCE&#26159;&#20351;&#29992;&#33258;&#24049;&#25910;&#38598;&#30340;&#28151;&#21512;&#31908;&#35821;&#21644;&#33521;&#35821;&#38899;&#39057;&#25968;&#25454;&#38598;&#65288;MCE&#65289;&#36827;&#34892;&#35757;&#32451;&#30340;Whisper&#27169;&#22411;&#24494;&#35843;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#20854;&#22312;&#20934;&#30830;&#25429;&#25417;&#21407;&#22987;&#38899;&#39057;&#20869;&#23481;&#12289;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24615;&#21644;&#21152;&#24555;&#35782;&#21035;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#26356;&#20248;&#36234;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#22312;&#28151;&#21512;&#35821;&#35328;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Whisper&#22312;&#33521;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#39046;&#22495;&#24050;&#32463;&#25509;&#36817;&#20110;&#20154;&#31867;&#32423;&#21035;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#20294;&#22312;&#36739;&#23567;&#35821;&#31181;&#21644;&#28151;&#21512;&#35821;&#35328;&#30340;&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;&#20173;&#28982;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#32454;&#35843;&#30340;Whisper&#27169;&#22411;Whisper-MCE&#30340;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#25105;&#20204;&#33258;&#24049;&#25910;&#38598;&#30340;&#28151;&#21512;&#31908;&#35821;&#21644;&#33521;&#35821;&#38899;&#39057;&#25968;&#25454;&#38598;&#65288;MCE&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#32771;&#34385;&#21040;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#22312;&#36739;&#23567;&#35821;&#31181;&#21644;&#28151;&#21512;&#35821;&#35328;&#29615;&#22659;&#20013;&#35780;&#20272;&#20854;&#26377;&#25928;&#24615;&#26102;&#23384;&#22312;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#26426;&#21046;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#22522;&#20934;&#30340;whisper-large-v2&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20934;&#30830;&#25429;&#25417;&#21407;&#22987;&#38899;&#39057;&#20869;&#23481;&#30340;&#33021;&#21147;&#26356;&#24378;&#12289;&#35782;&#21035;&#20934;&#30830;&#24615;&#26356;&#39640;&#12289;&#35782;&#21035;&#36895;&#24230;&#26356;&#24555;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35782;&#21035;&#28151;&#21512;&#35821;&#35328;&#30340;&#29305;&#23450;&#20219;&#21153;&#20013;&#32988;&#36807;&#20854;&#20182;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently Whisper has approached human-level robustness and accuracy in English automatic speech recognition (ASR), while in minor language and mixed language speech recognition, there remains a compelling need for further improvement. In this work, we present the impressive results of Whisper-MCE, our finetuned Whisper model, which was trained using our self-collected dataset, Mixed Cantonese and English audio dataset (MCE). Meanwhile, considering word error rate (WER) poses challenges when it comes to evaluating its effectiveness in minor language and mixed-language contexts, we present a novel rating mechanism. By comparing our model to the baseline whisper-large-v2 model, we demonstrate its superior ability to accurately capture the content of the original audio, achieve higher recognition accuracy, and exhibit faster recognition speed. Notably, our model outperforms other existing models in the specific task of recognizing mixed language.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;REIGN&#65292;&#36890;&#36807;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#25351;&#23548;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#22686;&#21152;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#38646;-shot&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.13505</link><description>&lt;p&gt;
&#20855;&#26377;&#24378;&#21270;&#25913;&#20889;&#29983;&#25104;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#30340;&#40065;&#26834;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Robust Training for Conversational Question Answering Models with Reinforced Reformulation Generation. (arXiv:2310.13505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13505
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;REIGN&#65292;&#36890;&#36807;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#25351;&#23548;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#22686;&#21152;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#38646;-shot&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#30340;&#23545;&#35805;&#38382;&#31572;&#65288;ConvQA&#65289;&#27169;&#22411;&#36890;&#24120;&#22312;&#40644;&#37329;QA&#23545;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#36825;&#24847;&#21619;&#30528;&#35757;&#32451;&#20165;&#38480;&#20110;&#22312;&#30456;&#24212;&#25968;&#25454;&#38598;&#20013;&#35265;&#21040;&#30340;&#34920;&#38754;&#24418;&#24335;&#65292;&#35780;&#20272;&#20165;&#38024;&#23545;&#19968;&#23567;&#37096;&#20998;&#38382;&#39064;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26694;&#26550;REIGN&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#20960;&#20010;&#27493;&#39588;&#26469;&#35299;&#20915;&#36825;&#20010;&#21463;&#38480;&#30340;&#23398;&#20064;&#35774;&#32622;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#29983;&#25104;&#35757;&#32451;&#38382;&#39064;&#30340;&#25913;&#20889;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#34920;&#38754;&#24418;&#24335;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#12290;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#19981;&#23436;&#25972;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23558;ConvQA&#27169;&#22411;&#24341;&#23548;&#21040;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#21482;&#25552;&#20379;&#37027;&#20123;&#26377;&#21161;&#20110;&#25552;&#39640;&#22238;&#31572;&#36136;&#37327;&#30340;&#25913;&#20889;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#20010;&#22522;&#20934;&#19978;&#35757;&#32451;&#20027;&#35201;&#27169;&#22411;&#32452;&#20214;&#24182;&#23558;&#20854;&#38646;-shot&#24212;&#29992;&#20110;&#21478;&#19968;&#20010;&#30340;&#21487;&#34892;&#24615;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#20351;&#29992;&#21644;&#37325;&#26032;&#37197;&#32622;&#21021;&#22987;&#30340;&#25913;&#20889;&#12289;&#27979;&#35797;&#35821;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models for conversational question answering (ConvQA) over knowledge graphs (KGs) are usually trained and tested on benchmarks of gold QA pairs. This implies that training is limited to surface forms seen in the respective datasets, and evaluation is on a small set of held-out questions. Through our proposed framework REIGN, we take several steps to remedy this restricted learning setup. First, we systematically generate reformulations of training questions to increase robustness of models to surface form variations. This is a particularly challenging problem, given the incomplete nature of such questions. Second, we guide ConvQA models towards higher performance by feeding it only those reformulations that help improve their answering quality, using deep reinforcement learning. Third, we demonstrate the viability of training major model components on one benchmark and applying them zero-shot to another. Finally, for a rigorous evaluation of robustness for trained models, we use and re
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#23398;&#20064;&#26159;&#19968;&#20010;&#30740;&#31350;&#30340;&#26032;&#39046;&#22495;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#20010;&#22330;&#26223;&#65292;&#21487;&#20197;&#36890;&#36807;&#21435;&#23398;&#20064;&#35753;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#21435;&#23398;&#20064;&#20855;&#26377;&#19977;&#20010;&#20248;&#21183;&#65292;&#21482;&#38656;&#35201;&#36127;&#38754;&#31034;&#20363;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#29305;&#21035;&#23545;&#20110;&#30693;&#36947;&#20855;&#20307;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#30340;&#35757;&#32451;&#26679;&#26412;&#26356;&#20026;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2310.10683</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Unlearning. (arXiv:2310.10683v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10683
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#23398;&#20064;&#26159;&#19968;&#20010;&#30740;&#31350;&#30340;&#26032;&#39046;&#22495;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19977;&#20010;&#22330;&#26223;&#65292;&#21487;&#20197;&#36890;&#36807;&#21435;&#23398;&#20064;&#35753;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#12290;&#21435;&#23398;&#20064;&#20855;&#26377;&#19977;&#20010;&#20248;&#21183;&#65292;&#21482;&#38656;&#35201;&#36127;&#38754;&#31034;&#20363;&#65292;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#29305;&#21035;&#23545;&#20110;&#30693;&#36947;&#20855;&#20307;&#23548;&#33268;&#19981;&#33391;&#34892;&#20026;&#30340;&#35757;&#32451;&#26679;&#26412;&#26356;&#20026;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#23398;&#20064;&#65292;&#21363;&#24536;&#35760;&#19981;&#21463;&#27426;&#36814;&#30340;&#65288;&#38750;&#65289;&#34892;&#20026;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33267;&#23569;&#19977;&#31181;&#24773;&#22659;&#21487;&#20197;&#20174;&#21435;&#23398;&#20064;&#20013;&#20351;LLMs&#19982;&#20154;&#31867;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65306;&#65288;1&#65289;&#21024;&#38500;&#26377;&#23475;&#22238;&#22797;&#65292;&#65288;2&#65289;&#25353;&#35201;&#27714;&#21024;&#38500;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20869;&#23481;&#65292;&#20197;&#21450;&#65288;3&#65289;&#28040;&#38500;&#24187;&#35273;&#12290;&#20316;&#20026;&#23545;&#40784;&#25216;&#26415;&#30340;&#19968;&#31181;&#65292;&#21435;&#23398;&#20064;&#20855;&#26377;&#19977;&#20010;&#20248;&#28857;&#65306;&#65288;1&#65289;&#21482;&#38656;&#35201;&#36127;&#38754;&#65288;&#20363;&#22914;&#26377;&#23475;&#65289;&#31034;&#20363;&#65292;&#36825;&#27604;&#22312;RLHF&#65288;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#20013;&#25152;&#38656;&#30340;&#27491;&#38754;&#65288;&#20363;&#22914;&#26377;&#24110;&#21161;&#19988;&#36890;&#24120;&#30001;&#20154;&#31867;&#32534;&#20889;&#65289;&#31034;&#20363;&#26356;&#23481;&#26131;&#21644;&#26356;&#20415;&#23452;&#22320;&#25910;&#38598;&#65288;&#20363;&#22914;&#36890;&#36807;&#32418;&#38431;&#27979;&#35797;&#25110;&#29992;&#25143;&#25253;&#21578;&#65289;&#65307;&#65288;2&#65289;&#35745;&#31639;&#25928;&#29575;&#39640;&#65307;&#65288;3&#65289;&#24403;&#25105;&#20204;&#30693;&#36947;&#21738;&#20123;&#35757;&#32451;&#26679;&#26412;&#23548;&#33268;&#20102;&#19981;&#33391;&#34892;&#20026;&#26102;&#65292;&#23427;&#29305;&#21035;&#26377;&#25928;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;LLM&#21435;&#23398;&#20064;&#30340;&#24037;&#20316;&#20043;&#19968;&#12290;&#25105;&#20204;&#20063;&#26159;&#39318;&#27425;&#22312;LLM&#21435;&#23398;&#20064;&#20013;&#21046;&#23450;&#20102;&#35774;&#32622;&#12289;&#30446;&#26631;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#20174;&#19994;&#32773;&#21482;&#26377;&#26377;&#38480;&#30340;
&lt;/p&gt;
&lt;p&gt;
We study how to perform unlearning, i.e. forgetting undesirable (mis)behaviors, on large language models (LLMs). We show at least three scenarios of aligning LLMs with human preferences can benefit from unlearning: (1) removing harmful responses, (2) erasing copyright-protected content as requested, and (3) eliminating hallucinations. Unlearning, as an alignment technique, has three advantages. (1) It only requires negative (e.g. harmful) examples, which are much easier and cheaper to collect (e.g. via red teaming or user reporting) than positive (e.g. helpful and often human-written) examples required in RLHF (RL from human feedback). (2) It is computationally efficient. (3) It is especially effective when we know which training samples cause the misbehavior. To the best of our knowledge, our work is among the first to explore LLM unlearning. We are also among the first to formulate the settings, goals, and evaluations in LLM unlearning. We show that if practitioners only have limited
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27599;&#20010;&#38454;&#27573;&#23545;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#21644;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.06452</link><description>&lt;p&gt;
&#29702;&#35299;RLHF&#23545;LLM&#27867;&#21270;&#21644;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effects of RLHF on LLM Generalisation and Diversity. (arXiv:2310.06452v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27599;&#20010;&#38454;&#27573;&#23545;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#21644;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;AI&#27169;&#22411;&#20013;&#65292;&#22914;OpenAI&#30340;ChatGPT&#25110;Anthropic&#30340;Claude&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#23613;&#31649;&#22312;&#36825;&#20123;&#26041;&#27861;&#30340;&#24320;&#21457;&#26041;&#38754;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#20294;&#25105;&#20204;&#23545;RLHF&#36807;&#31243;&#20013;&#27599;&#20010;&#38454;&#27573;&#30340;&#21033;&#19982;&#24330;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#38454;&#27573;&#65288;&#21363;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#65292;&#22870;&#21169;&#24314;&#27169;&#21644;RLHF&#65289;&#22914;&#20309;&#24433;&#21709;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65306;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#21644;&#36755;&#20986;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#21508;&#31181;&#24773;&#26223;&#30340;&#32972;&#26223;&#19979;&#65292;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#38750;&#24120;&#37325;&#35201;&#65292;&#32780;&#36755;&#20986;&#22810;&#26679;&#24615;&#25351;&#30340;&#26159;&#27169;&#22411;&#29983;&#25104;&#21508;&#31181;&#19981;&#21516;&#36755;&#20986;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#21508;&#31181;&#29992;&#20363;&#26469;&#35828;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#25688;&#35201;&#21644;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#20013;&#23545;&#20004;&#20010;&#22522;&#26412;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21518;&#32773;&#38750;&#24120;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. % , or Meta's LLaMA-2. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant 
&lt;/p&gt;</description></item><item><title>JsonTuning&#26159;&#19968;&#31181;&#38754;&#21521;&#36890;&#29992;&#12289;&#24378;&#22823;&#21644;&#21487;&#25511;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;JSON&#30340;&#32467;&#26500;&#21270;&#29305;&#24615;&#65292;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#20219;&#21153;&#35201;&#32032;&#21450;&#20854;&#20851;&#31995;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#36890;&#29992;&#24615;&#12289;&#25552;&#39640;&#20102;&#31283;&#20581;&#24615;&#65292;&#24182;&#22686;&#24378;&#20102;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.02953</link><description>&lt;p&gt;
JsonTuning&#65306;&#38754;&#21521;&#36890;&#29992;&#12289;&#24378;&#22823;&#21644;&#21487;&#25511;&#30340;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning. (arXiv:2310.02953v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02953
&lt;/p&gt;
&lt;p&gt;
JsonTuning&#26159;&#19968;&#31181;&#38754;&#21521;&#36890;&#29992;&#12289;&#24378;&#22823;&#21644;&#21487;&#25511;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;JSON&#30340;&#32467;&#26500;&#21270;&#29305;&#24615;&#65292;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#20219;&#21153;&#35201;&#32032;&#21450;&#20854;&#20851;&#31995;&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;&#36890;&#29992;&#24615;&#12289;&#25552;&#39640;&#20102;&#31283;&#20581;&#24615;&#65292;&#24182;&#22686;&#24378;&#20102;&#23545;&#36755;&#20986;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#20248;&#24050;&#25104;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#30340;&#20851;&#38190;&#36807;&#31243;&#65292;&#36890;&#36807;&#25552;&#20379;&#26126;&#30830;&#30340;&#20219;&#21153;&#25351;&#20196;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#25991;&#26412;-&#25991;&#26412;&#25351;&#20196;&#35843;&#20248;&#65288;TextTuning&#65289;&#26041;&#27861;&#30001;&#20110;&#20219;&#21153;&#30340;&#27169;&#31946;&#24615;&#21644;&#32570;&#20047;&#26126;&#30830;&#30340;&#32467;&#26500;&#32780;&#23384;&#22312;&#36890;&#29992;&#24615;&#12289;&#31283;&#20581;&#24615;&#21644;&#21487;&#25511;&#24615;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JsonTuning&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#21040;&#32467;&#26500;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;JSON&#30340;&#22810;&#21151;&#33021;&#21644;&#32467;&#26500;&#21270;&#29305;&#24615;&#26469;&#34920;&#31034;&#20219;&#21153;&#65292;JsonTuning&#36890;&#36807;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#20851;&#38190;&#20219;&#21153;&#35201;&#32032;&#21450;&#20854;&#20851;&#31995;&#65292;&#25193;&#23637;&#20102;&#36890;&#29992;&#24615;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#27495;&#20041;&#24615;&#25552;&#39640;&#20102;&#31283;&#20581;&#24615;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#23545;&#36755;&#20986;&#30340;&#26174;&#24335;&#25511;&#21046;&#22686;&#24378;&#20102;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#35780;&#20272;&#22522;&#20934;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#27604;&#36739;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;JsonTuning&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;TextTuning&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has emerged as a crucial process for harnessing the capabilities of large language models (LLMs) by providing explicit task instructions, leading to improved performance in various tasks. However, prevalent text-to-text instruction tuning (TextTuning) methods suffer from limitations in generalization, robustness, and controllability due to the ambiguity and lack of explicit structure in tasks. In this paper, we propose JsonTuning, a novel structure-to-structure approach for instruction tuning. By leveraging the versatility and structured nature of JSON to represent tasks, JsonTuning enhances generalization by helping the model understand essential task elements and their relations, improves robustness by minimizing ambiguity, and increases controllability by providing explicit control over the output. We conduct a comprehensive comparative study with diverse language models and evaluation benchmarks. Experimental results show that JsonTuning outperforms TextTuning in
&lt;/p&gt;</description></item><item><title>ChaCha&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#40723;&#21169;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#12290;&#36890;&#36807;&#19968;&#20010;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#20799;&#31461;&#23558;ChaCha&#35270;&#20026;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#24895;&#24847;&#19982;&#20854;&#20998;&#20139;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#12290;</title><link>http://arxiv.org/abs/2309.12244</link><description>&lt;p&gt;
ChaCha&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#20799;&#31461;&#20998;&#20139;&#19982;&#20010;&#20154;&#20107;&#20214;&#30456;&#20851;&#30340;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events. (arXiv:2309.12244v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12244
&lt;/p&gt;
&lt;p&gt;
ChaCha&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#40723;&#21169;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#12290;&#36890;&#36807;&#19968;&#20010;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#20799;&#31461;&#23558;ChaCha&#35270;&#20026;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#24895;&#24847;&#19982;&#20854;&#20998;&#20139;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#36890;&#24120;&#36890;&#36807;&#19982;&#23478;&#20154;&#25110;&#20182;&#20154;&#20998;&#20139;&#25925;&#20107;&#21644;&#24863;&#21463;&#26469;&#23398;&#20064;&#36776;&#35782;&#21644;&#34920;&#36798;&#24773;&#32490;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#20799;&#31461;&#27491;&#22312;&#21457;&#23637;&#20182;&#20204;&#30340;&#20132;&#27969;&#25216;&#33021;&#65292;&#29238;&#27597;&#25110;&#20804;&#24351;&#22992;&#22969;&#24456;&#38590;&#19982;&#20182;&#20204;&#36827;&#34892;&#24773;&#24863;&#27807;&#36890;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChaCha&#65292;&#19968;&#20010;&#40723;&#21169;&#21644;&#24341;&#23548;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;ChaCha&#32467;&#21512;&#20102;&#29366;&#24577;&#26426;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#36827;&#34892;&#33258;&#30001;&#23545;&#35805;&#30340;&#21516;&#26102;&#20445;&#25345;&#23545;&#35805;&#30340;&#26041;&#21521;&#24615;&#12290;&#36890;&#36807;&#19982;20&#21517;&#24180;&#40836;&#22312;8-12&#23681;&#30340;&#20799;&#31461;&#36827;&#34892;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ChaCha&#22914;&#20309;&#20419;&#20351;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#24182;&#24341;&#23548;&#20182;&#20204;&#25551;&#36848;&#30456;&#20851;&#24773;&#32490;&#12290;&#21442;&#19982;&#32773;&#35748;&#20026;ChaCha&#23601;&#20687;&#19968;&#20010;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#20998;&#20139;&#20102;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#65292;&#22914;&#23478;&#24237;&#26053;&#34892;&#21644;&#20010;&#20154;&#25104;&#23601;&#12290;&#22522;&#20110;&#23450;&#37327;&#21644;&#23450;&#24615;&#21457;&#29616;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21033;&#29992;LLMs&#35774;&#35745;&#36866;&#21512;&#20799;&#31461;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children typically learn to identify and express emotions through sharing their stories and feelings with others, particularly their family. However, it is challenging for parents or siblings to have emotional communication with children since children are still developing their communication skills. We present ChaCha, a chatbot that encourages and guides children to share personal events and associated emotions. ChaCha combines a state machine and large language models (LLMs) to keep the dialogue on track while carrying on free-form conversations. Through an exploratory study with 20 children (aged 8-12), we examine how ChaCha prompts children to share personal events and guides them to describe associated emotions. Participants perceived ChaCha as a close friend and shared their stories on various topics, such as family trips and personal achievements. Based on the quantitative and qualitative findings, we discuss opportunities for leveraging LLMs to design child-friendly chatbots to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;LLMs&#22312;&#24180;&#40836;&#12289;&#32654;&#20029;&#12289;&#26426;&#26500;&#21644;&#22269;&#31821;&#31561;&#23569;&#30740;&#31350;&#20294;&#20173;&#28982;&#37325;&#35201;&#30340;&#32500;&#24230;&#19978;&#30340;&#20559;&#35265;&#65292;&#36890;&#36807;&#34913;&#37327;&#22312;&#31038;&#20250;&#32676;&#20307;&#21644;&#19981;&#30456;&#20851;&#30340;&#27491;&#36127;&#23646;&#24615;&#20043;&#38388;&#20570;&#20986;&#30340;&#24494;&#22937;&#30456;&#20851;&#20915;&#31574;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#29305;&#23450;&#31038;&#20250;&#32676;&#20307;&#19978;&#23384;&#22312;&#31867;&#20284;&#20110;&#8220;&#32654;&#20029;&#21363;&#21892;&#8221;&#30340;&#24191;&#27867;&#27491;&#38754;&#25110;&#36127;&#38754;&#24577;&#24230;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2309.08902</link><description>&lt;p&gt;
&#35843;&#26597;LLMs&#20013;&#26356;&#24494;&#22937;&#30340;&#20559;&#35265;&#65306;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#24180;&#40836;&#20027;&#20041;&#12289;&#32654;&#20029;&#12289;&#26426;&#26500;&#21644;&#22269;&#31821;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Investigating Subtler Biases in LLMs: Ageism, Beauty, Institutional, and Nationality Bias in Generative Models. (arXiv:2309.08902v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08902
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;LLMs&#22312;&#24180;&#40836;&#12289;&#32654;&#20029;&#12289;&#26426;&#26500;&#21644;&#22269;&#31821;&#31561;&#23569;&#30740;&#31350;&#20294;&#20173;&#28982;&#37325;&#35201;&#30340;&#32500;&#24230;&#19978;&#30340;&#20559;&#35265;&#65292;&#36890;&#36807;&#34913;&#37327;&#22312;&#31038;&#20250;&#32676;&#20307;&#21644;&#19981;&#30456;&#20851;&#30340;&#27491;&#36127;&#23646;&#24615;&#20043;&#38388;&#20570;&#20986;&#30340;&#24494;&#22937;&#30456;&#20851;&#20915;&#31574;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#29305;&#23450;&#31038;&#20250;&#32676;&#20307;&#19978;&#23384;&#22312;&#31867;&#20284;&#20110;&#8220;&#32654;&#20029;&#21363;&#21892;&#8221;&#30340;&#24191;&#27867;&#27491;&#38754;&#25110;&#36127;&#38754;&#24577;&#24230;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#36234;&#26469;&#36234;&#24378;&#22823;&#24182;&#24191;&#27867;&#29992;&#20110;&#36741;&#21161;&#29992;&#25143;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#12290;&#36825;&#31181;&#20351;&#29992;&#21487;&#33021;&#20250;&#23558;LLM&#20559;&#35265;&#24341;&#20837;&#21040;&#37325;&#35201;&#20915;&#31574;&#20013;&#65292;&#22914;&#25307;&#32856;&#12289;&#20154;&#21592;&#32489;&#25928;&#35780;&#20272;&#21644;&#21009;&#20107;&#21028;&#20915;&#12290;&#22312;NLP&#31995;&#32479;&#20013;&#30340;&#24615;&#21035;&#21644;&#31181;&#26063;&#31561;&#26041;&#38754;&#30340;&#20559;&#35265;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#29305;&#23450;&#21051;&#26495;&#21360;&#35937;&#30340;&#20559;&#35265;&#65288;&#20363;&#22914;&#65292;&#20122;&#27954;&#20154;&#25797;&#38271;&#25968;&#23398;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20123;&#36739;&#23569;&#30740;&#31350;&#20294;&#20173;&#28982;&#37325;&#35201;&#30340;&#32500;&#24230;&#19978;&#30340;&#20559;&#35265;&#65292;&#22914;&#24180;&#40836;&#21644;&#32654;&#20029;&#65292;&#22312;LLMs&#65288;&#29305;&#21035;&#26159;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65289;&#22312;&#31038;&#20250;&#32676;&#20307;&#21644;&#19981;&#30456;&#20851;&#30340;&#27491;&#36127;&#23646;&#24615;&#20043;&#38388;&#20570;&#20986;&#26356;&#24494;&#22937;&#30340;&#30456;&#20851;&#20915;&#31574;&#12290;&#25105;&#20204;&#38382;LLMs&#26159;&#21542;&#23545;&#29305;&#23450;&#31038;&#20250;&#32676;&#20307;&#25345;&#26377;&#24191;&#27867;&#30340;&#27491;&#38754;&#25110;&#36127;&#38754;&#24577;&#24230;&#30340;&#20559;&#35265;&#65292;&#31867;&#20284;&#20110;&#23454;&#39564;&#24515;&#29702;&#23398;&#20013;&#20154;&#20204;&#21457;&#29616;&#30340;&#8220;&#32654;&#20029;&#21363;&#21892;&#8221;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#26495;&#29983;&#25104;&#30340;&#21477;&#23376;&#23436;&#25104;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#35201;&#27714;&#27169;&#22411;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs are increasingly powerful and widely used to assist users in a variety of tasks. This use risks the introduction of LLM biases to consequential decisions such as job hiring, human performance evaluation, and criminal sentencing. Bias in NLP systems along the lines of gender and ethnicity has been widely studied, especially for specific stereotypes (e.g., Asians are good at math). In this paper, we investigate bias along less studied, but still consequential, dimensions, such as age and beauty, measuring subtler correlated decisions that LLMs (specially autoregressive language models) make between social groups and unrelated positive and negative attributes. We ask whether LLMs hold wide-reaching biases of positive or negative sentiment for specific social groups similar to the ``what is beautiful is good'' bias found in people in experimental psychology. We introduce a template-generated dataset of sentence completion tasks that asks the model to select the most appropriate attrib
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26356;&#23569;&#30340;&#31034;&#20363;&#26469;&#23545;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#38170;&#28857;&#36873;&#25321;&#25216;&#26415;&#26469;&#25429;&#25417;&#27169;&#22411;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#38170;&#28857;&#23545;&#27169;&#22411;&#36827;&#34892;&#25490;&#24207;&#27604;&#20351;&#29992;&#22343;&#21248;&#37319;&#26679;&#21644;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#26356;&#20934;&#30830;&#12290;&#20165;&#20351;&#29992;&#20960;&#20010;&#38170;&#28857;&#23601;&#21487;&#20197;&#20272;&#35745;&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#20854;&#20182;&#28857;&#30340;&#27599;&#20010;&#31867;&#21035;&#30340;&#39044;&#27979;&#65292;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08638</link><description>&lt;p&gt;
&#38170;&#28857;&#65306;&#29992;&#26356;&#23569;&#30340;&#31034;&#20363;&#23545;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Anchor Points: Benchmarking Models with Much Fewer Examples. (arXiv:2309.08638v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08638
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26356;&#23569;&#30340;&#31034;&#20363;&#26469;&#23545;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#38170;&#28857;&#36873;&#25321;&#25216;&#26415;&#26469;&#25429;&#25417;&#27169;&#22411;&#34892;&#20026;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#38170;&#28857;&#23545;&#27169;&#22411;&#36827;&#34892;&#25490;&#24207;&#27604;&#20351;&#29992;&#22343;&#21248;&#37319;&#26679;&#21644;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#26356;&#20934;&#30830;&#12290;&#20165;&#20351;&#29992;&#20960;&#20010;&#38170;&#28857;&#23601;&#21487;&#20197;&#20272;&#35745;&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#20854;&#20182;&#28857;&#30340;&#27599;&#20010;&#31867;&#21035;&#30340;&#39044;&#27979;&#65292;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#20986;&#24378;&#22823;&#20294;&#33030;&#24369;&#30340;&#34892;&#20026;&#65292;&#22240;&#27492;&#24320;&#21457;&#20986;&#26356;&#22823;&#12289;&#26356;&#22810;&#26679;&#21270;&#30340;&#22522;&#20934;&#26469;&#21487;&#38752;&#22320;&#35780;&#20272;&#23427;&#20204;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24314;&#35758;&#21487;&#20197;&#20351;&#29992;&#26356;&#23567;&#30340;&#35780;&#20272;&#38598;&#23545;&#27169;&#22411;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#38416;&#26126;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22312;&#20845;&#20010;&#27969;&#34892;&#35821;&#35328;&#20998;&#31867;&#22522;&#20934;&#20013;&#65292;&#27169;&#22411;&#23545;&#35768;&#22810;&#28857;&#23545;&#30340;&#27491;&#30830;&#31867;&#21035;&#30340;&#32622;&#20449;&#24230;&#22312;&#21508;&#20010;&#27169;&#22411;&#20043;&#38388;&#20855;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#27492;&#29616;&#35937;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#38170;&#28857;&#36873;&#25321;&#25216;&#26415;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#36873;&#25321;&#25429;&#25417;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#34892;&#20026;&#30340;&#23567;&#23376;&#38598;&#12290;&#38170;&#28857;&#21487;&#38752;&#22320;&#23545;&#27169;&#22411;&#36827;&#34892;&#25490;&#24207;&#65306;&#22312;87&#20010;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;-&#25552;&#31034;&#23545;&#19978;&#65292;&#20351;&#29992;1-30&#20010;&#38170;&#28857;&#35780;&#20272;&#27169;&#22411;&#22312;&#20934;&#30830;&#25490;&#24207;&#27169;&#22411;&#26041;&#38754;&#20248;&#20110;&#22343;&#21248;&#37319;&#26679;&#21644;&#20854;&#20182;&#22522;&#32447;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#21482;&#38656;&#35201;&#20960;&#20010;&#38170;&#28857;&#23601;&#21487;&#20197;&#29992;&#36739;&#20302;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20272;&#35745;&#20986;&#27169;&#22411;&#23545;&#25968;&#25454;&#38598;&#20013;&#25152;&#26377;&#20854;&#20182;&#28857;&#30340;&#27599;&#20010;&#31867;&#21035;&#30340;&#39044;&#27979;&#65292;&#36275;&#20197;&#34913;&#37327;&#27169;&#22411;&#22312;&#21738;&#20123;&#26041;&#38754;&#34920;&#29616;&#24471;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language models often exhibit powerful but brittle behavior, leading to the development of larger and more diverse benchmarks to reliably assess their behavior. Here, we suggest that model performance can be benchmarked and elucidated with much smaller evaluation sets. We first show that in six popular language classification benchmarks, model confidence in the correct class on many pairs of points is strongly correlated across models. We build upon this phenomenon to propose Anchor Point Selection, a technique to select small subsets of datasets that capture model behavior across the entire dataset. Anchor points reliably rank models: across 87 diverse language model-prompt pairs, evaluating models using 1-30 anchor points outperforms uniform sampling and other baselines at accurately ranking models. Moreover, just several anchor points can be used to estimate model per-class predictions on all other points in a dataset with low mean absolute error, sufficient for gauging where
&lt;/p&gt;</description></item><item><title>PROGrasp&#26159;&#19968;&#20010;&#23454;&#29616;&#29289;&#20307;&#25235;&#21462;&#30340;&#20154;&#26426;&#20132;&#27969;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#38754;&#21521;&#24847;&#22270;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#21644;&#31572;&#26696;&#35299;&#37322;&#27169;&#22359;&#65292;&#26426;&#22120;&#20154;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#24847;&#22270;&#26469;&#35782;&#21035;&#21644;&#25235;&#21462;&#30446;&#26631;&#29289;&#20307;&#12290;</title><link>http://arxiv.org/abs/2309.07759</link><description>&lt;p&gt;
PROGrasp:&#23454;&#29616;&#29289;&#20307;&#25235;&#21462;&#30340;&#20154;&#26426;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
PROGrasp: Pragmatic Human-Robot Communication for Object Grasping. (arXiv:2309.07759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07759
&lt;/p&gt;
&lt;p&gt;
PROGrasp&#26159;&#19968;&#20010;&#23454;&#29616;&#29289;&#20307;&#25235;&#21462;&#30340;&#20154;&#26426;&#20132;&#27969;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#38754;&#21521;&#24847;&#22270;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#21644;&#31572;&#26696;&#35299;&#37322;&#27169;&#22359;&#65292;&#26426;&#22120;&#20154;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#24847;&#22270;&#26469;&#35782;&#21035;&#21644;&#25235;&#21462;&#30446;&#26631;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#29289;&#20307;&#25235;&#21462;(IOG)&#26159;&#36890;&#36807;&#20154;&#26426;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#35782;&#21035;&#21644;&#25235;&#21462;&#30446;&#26631;&#29289;&#20307;&#30340;&#20219;&#21153;&#12290;&#24403;&#21069;IOG&#31995;&#32479;&#20551;&#23450;&#20154;&#31867;&#29992;&#25143;&#26368;&#21021;&#25351;&#23450;&#30446;&#26631;&#23545;&#35937;&#30340;&#31867;&#21035;(&#20363;&#22914;&#65292;&#29942;&#23376;)&#12290;&#21463;&#21040;&#35821;&#29992;&#23398;&#30340;&#21551;&#21457;&#65292;&#20154;&#31867;&#24448;&#24448;&#36890;&#36807;&#20381;&#36182;&#19978;&#19979;&#25991;&#26469;&#20256;&#36798;&#24847;&#22270;&#20197;&#23454;&#29616;&#30446;&#26631;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;IOG&#20219;&#21153;&#65292;&#21363;&#23454;&#29992;IOG&#65292;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;&#38754;&#21521;&#24847;&#22270;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;(IM-Dial)&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#20219;&#21153;&#22330;&#26223;&#20013;&#65292;&#39318;&#20808;&#32473;&#20986;&#19968;&#20010;&#38754;&#21521;&#24847;&#22270;&#30340;&#35805;&#35821;(&#20363;&#22914;&#65292;&#8220;&#25105;&#28212;&#20102;&#8221;)&#12290;&#28982;&#21518;&#65292;&#26426;&#22120;&#20154;&#24212;&#36890;&#36807;&#19982;&#20154;&#31867;&#29992;&#25143;&#20114;&#21160;&#26469;&#35782;&#21035;&#30446;&#26631;&#23545;&#35937;&#12290;&#22522;&#20110;&#20219;&#21153;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#37322;&#29992;&#25143;&#30340;&#24847;&#22270;&#24182;&#25441;&#36215;&#30446;&#26631;&#23545;&#35937;&#65292;&#21363;&#23454;&#29992;&#29289;&#20307;&#25235;&#21462;(PROGrasp)&#12290;PROGrasp&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#23450;&#20301;&#12289;&#38382;&#39064;&#25552;&#38382;&#12289;&#29289;&#20307;&#25235;&#21462;&#20197;&#21450;&#26368;&#37325;&#35201;&#30340;&#65292;&#31572;&#26696;&#35299;&#37322;&#27169;&#22359;&#25191;&#34892;&#23454;&#29992;IOG&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive Object Grasping (IOG) is the task of identifying and grasping the desired object via human-robot natural language interaction. Current IOG systems assume that a human user initially specifies the target object's category (e.g., bottle). Inspired by pragmatics, where humans often convey their intentions by relying on context to achieve goals, we introduce a new IOG task, Pragmatic-IOG, and the corresponding dataset, Intention-oriented Multi-modal Dialogue (IM-Dial). In our proposed task scenario, an intention-oriented utterance (e.g., "I am thirsty") is initially given to the robot. The robot should then identify the target object by interacting with a human user. Based on the task setup, we propose a new robotic system that can interpret the user's intention and pick up the target object, Pragmatic Object Grasping (PROGrasp). PROGrasp performs Pragmatic-IOG by incorporating modules for visual grounding, question asking, object grasping, and most importantly, answer interpre
&lt;/p&gt;</description></item><item><title>DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.05173</link><description>&lt;p&gt;
DePT:&#20998;&#35299;&#25552;&#31034;&#35843;&#25972;&#20197;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05173
&lt;/p&gt;
&lt;p&gt;
DePT&#36890;&#36807;&#23558;&#36719;&#25552;&#31034;&#20998;&#35299;&#20026;&#36739;&#30701;&#30340;&#36719;&#25552;&#31034;&#21644;&#19968;&#23545;&#20302;&#31209;&#30697;&#38453;&#65292;&#24182;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#23398;&#20064;&#29575;&#26469;&#20248;&#21270;&#65292;&#20197;&#35299;&#20915;&#25552;&#31034;&#35843;&#25972;&#23545;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#65288;PT&#65289;&#26159;&#19968;&#31181;&#23558;&#21487;&#35757;&#32451;&#30340;&#23569;&#37327;&#36719;&#25552;&#31034;&#21521;&#37327;&#38468;&#21152;&#21040;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36755;&#20837;&#20013;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#24050;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290; &#19982;&#20854;&#20182;PEFT&#26041;&#27861;&#30456;&#27604;&#65292;PT&#30340;&#31454;&#20105;&#24615;&#33021;&#21487;&#20197;&#22312;&#21487;&#35757;&#32451;&#21442;&#25968;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#65292;&#24182;&#19988;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#20854;&#21442;&#25968;&#24182;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#12290; &#20294;&#26159;&#65292;PT&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#36719;&#25552;&#31034;&#26631;&#35760;&#65292;&#23548;&#33268;&#36755;&#20837;&#24207;&#21015;&#21464;&#38271;&#65292;&#36825;&#23545;&#20110;Transformer&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20197;&#21450;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#20250;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290; &#36825;&#23545;&#20110;&#38754;&#20020;&#22823;&#37327;&#27599;&#26085;&#26597;&#35810;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23588;&#20854;&#20196;&#20154;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning (PT), where a small amount of trainable soft (continuous) prompt vectors is affixed to the input of language models (LM), has shown promising results across various tasks and models for parameter-efficient fine-tuning (PEFT). PT stands out from other PEFT approaches because it maintains competitive performance with fewer trainable parameters and does not drastically scale up its parameters as the model size expands. However, PT introduces additional soft prompt tokens, leading to longer input sequences, which significantly impacts training and inference time and memory usage due to the Transformer's quadratic complexity. Particularly concerning for Large Language Models (LLMs) that face heavy daily querying. To address this issue, we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt into a shorter soft prompt and a pair of low-rank matrices that are then optimised with two different learning rates. This allows DePT to achieve better performance whi
&lt;/p&gt;</description></item><item><title>&#36882;&#24402;&#24635;&#32467;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#35805;&#31995;&#32479;&#22312;&#38271;&#23545;&#35805;&#20013;&#35760;&#24518;&#37325;&#35201;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.15022</link><description>&lt;p&gt;
&#36882;&#24402;&#24635;&#32467;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Recursively Summarizing Enables Long-Term Dialogue Memory in Large Language Models. (arXiv:2308.15022v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15022
&lt;/p&gt;
&lt;p&gt;
&#36882;&#24402;&#24635;&#32467;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#35805;&#31995;&#32479;&#22312;&#38271;&#23545;&#35805;&#20013;&#35760;&#24518;&#37325;&#35201;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#31995;&#32479;&#22312;&#38271;&#26399;&#23545;&#35805;&#20013;&#23481;&#26131;&#36951;&#24536;&#37325;&#35201;&#20449;&#24687;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#35757;&#32451;&#29305;&#23450;&#30340;&#26816;&#32034;&#22120;&#25110;&#24635;&#32467;&#22120;&#20174;&#36807;&#21435;&#33719;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#36825;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#19988;&#39640;&#24230;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36882;&#24402;&#29983;&#25104;&#24635;&#32467;/&#35760;&#24518;&#65292;&#20197;&#22686;&#24378;&#38271;&#26399;&#35760;&#24518;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#21050;&#28608;LLMs&#35760;&#20303;&#23567;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#36882;&#24402;&#22320;&#20351;&#29992;&#20043;&#21069;&#30340;&#35760;&#24518;&#21644;&#38543;&#21518;&#30340;&#23545;&#35805;&#20869;&#23481;&#20135;&#29983;&#26032;&#30340;&#35760;&#24518;&#12290;&#26368;&#21518;&#65292;LLM&#21487;&#20197;&#22312;&#26368;&#26032;&#35760;&#24518;&#30340;&#24110;&#21161;&#19979;&#36731;&#26494;&#29983;&#25104;&#39640;&#24230;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#20351;&#29992;ChatGPT&#21644;text-davinci-003&#36827;&#34892;&#35780;&#20272;&#65292;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38271;&#23545;&#35805;&#20013;&#21487;&#20197;&#29983;&#25104;&#26356;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23454;&#29616;LLM&#24314;&#27169;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most open-domain dialogue systems suffer from forgetting important information, especially in a long-term conversation. Existing works usually train the specific retriever or summarizer to obtain key information from the past, which is time-consuming and highly depends on the quality of labeled data. To alleviate this problem, we propose to recursively generate summaries/ memory using large language models (LLMs) to enhance long-term memory ability. Specifically, our method first stimulates LLMs to memorize small dialogue contexts and then recursively produce new memory using previous memory and following contexts. Finally, the LLM can easily generate a highly consistent response with the help of the latest memory. We evaluate our method using ChatGPT and text-davinci-003, and the experiments on the widely-used public dataset show that our method can generate more consistent responses in a long-context conversation. Notably, our method is a potential solution to enable the LLM to model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65288;&#30693;&#35782;&#22270;&#35889;LLM&#65289;&#65292;&#20197;&#25552;&#39640;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13916</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Models for Knowledge Graph Completion. (arXiv:2308.13916v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65288;&#30693;&#35782;&#22270;&#35889;LLM&#65289;&#65292;&#20197;&#25552;&#39640;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#20247;&#22810;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#32463;&#24120;&#38754;&#20020;&#19981;&#23436;&#25972;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#19977;&#20803;&#32452;&#35270;&#20026;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#30693;&#35782;&#22270;&#35889;LLM&#65288;KG-LLM&#65289;&#65292;&#26469;&#23545;&#36825;&#20123;&#19977;&#20803;&#32452;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21033;&#29992;&#19977;&#20803;&#32452;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;&#65292;&#24182;&#21033;&#29992;&#21709;&#24212;&#36827;&#34892;&#39044;&#27979;&#12290;&#23545;&#21508;&#31181;&#22522;&#20934;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20803;&#32452;&#20998;&#31867;&#21644;&#20851;&#31995;&#39044;&#27979;&#31561;&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24494;&#35843;&#30456;&#23545;&#36739;&#23567;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;LLaMA-7B&#65292;ChatGLM-6B&#65289;&#20248;&#20110;&#26368;&#26032;&#30340;ChatGPT&#21644;GPT-4&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs play a vital role in numerous artificial intelligence tasks, yet they frequently face the issue of incompleteness. In this study, we explore utilizing Large Language Models (LLM) for knowledge graph completion. We consider triples in knowledge graphs as text sequences and introduce an innovative framework called Knowledge Graph LLM (KG-LLM) to model these triples. Our technique employs entity and relation descriptions of a triple as prompts and utilizes the response for predictions. Experiments on various benchmark knowledge graphs demonstrate that our method attains state-of-the-art performance in tasks such as triple classification and relation prediction. We also find that fine-tuning relatively smaller models (e.g., LLaMA-7B, ChatGLM-6B) outperforms recent ChatGPT and GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#21360;&#22320;&#35821;&#21644;&#39532;&#25289;&#22320;&#35821;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32763;&#35793;SQuAD 2.0&#25968;&#25454;&#38598;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#36825;&#20004;&#31181;&#35821;&#35328;&#30340;&#26368;&#22909;&#34920;&#29616;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.09862</link><description>&lt;p&gt;
&#25171;&#30772;&#35821;&#35328;&#38556;&#30861;&#65306;&#29992;&#20110;&#21360;&#22320;&#35821;&#21644;&#39532;&#25289;&#22320;&#35821;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Breaking Language Barriers: A Question Answering Dataset for Hindi and Marathi. (arXiv:2308.09862v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#20110;&#21360;&#22320;&#35821;&#21644;&#39532;&#25289;&#22320;&#35821;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#32763;&#35793;SQuAD 2.0&#25968;&#25454;&#38598;&#35299;&#20915;&#20102;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#36825;&#20004;&#31181;&#35821;&#35328;&#30340;&#26368;&#22909;&#34920;&#29616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;&#20102;&#24320;&#21457;&#20986;&#39640;&#24230;&#22797;&#26434;&#30340;&#31995;&#32479;&#65292;&#23545;&#25968;&#25454;&#26377;&#30528;&#26080;&#27490;&#22659;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#26469;&#35828;&#65292;&#26500;&#24314;&#33391;&#22909;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#37325;&#28857;&#26159;&#20026;&#20004;&#31181;&#36825;&#26679;&#30340;&#35821;&#35328;-&#21360;&#22320;&#35821;&#21644;&#39532;&#25289;&#22320;&#35821;-&#24320;&#21457;&#19968;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;&#34429;&#28982;&#21360;&#22320;&#35821;&#26159;&#20840;&#29699;&#31532;&#19977;&#22823;&#20351;&#29992;&#20154;&#25968;&#26368;&#22810;&#30340;&#35821;&#35328;&#65292;&#25317;&#26377;3.45&#20159;&#35828;&#35805;&#32773;&#65292;&#32780;&#39532;&#25289;&#22320;&#35821;&#21017;&#26159;&#20840;&#29699;&#31532;11&#22823;&#20351;&#29992;&#20154;&#25968;&#26368;&#22810;&#30340;&#35821;&#35328;&#65292;&#25317;&#26377;8.32&#21315;&#19975;&#35828;&#35805;&#32773;&#65292;&#20294;&#36825;&#20004;&#31181;&#35821;&#35328;&#22312;&#26500;&#24314;&#39640;&#25928;&#30340;&#38382;&#31572;&#31995;&#32479;&#30340;&#36164;&#28304;&#19978;&#37117;&#38754;&#20020;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#23558;SQuAD 2.0&#25968;&#25454;&#38598;&#32763;&#35793;&#25104;&#21360;&#22320;&#35821;&#21644;&#39532;&#25289;&#22320;&#35821;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#36825;&#20004;&#31181;&#35821;&#35328;&#20013;&#26368;&#22823;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;28,000&#20010;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#26550;&#26500;&#19978;&#35780;&#20272;&#20102;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#24067;&#20102;&#22312;&#21360;&#22320;&#35821;&#21644;&#39532;&#25289;&#22320;&#35821;&#20013;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in deep-learning have led to the development of highly sophisticated systems with an unquenchable appetite for data. On the other hand, building good deep-learning models for low-resource languages remains a challenging task. This paper focuses on developing a Question Answering dataset for two such languages- Hindi and Marathi. Despite Hindi being the 3rd most spoken language worldwide, with 345 million speakers, and Marathi being the 11th most spoken language globally, with 83.2 million speakers, both languages face limited resources for building efficient Question Answering systems. To tackle the challenge of data scarcity, we have developed a novel approach for translating the SQuAD 2.0 dataset into Hindi and Marathi. We release the largest Question-Answering dataset available for these languages, with each dataset containing 28,000 samples. We evaluate the dataset on various architectures and release the best-performing models for both Hindi and Marathi, which 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07758</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Backward Reasoning in Large Language Models for Verification. (arXiv:2308.07758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#23631;&#34109;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#26469;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32771;&#65288;Chain-of-Though, CoT&#65289;&#25552;&#31034;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;Self-Consistency&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#37319;&#26679;&#19968;&#32452;&#19981;&#21516;&#30340;&#25512;&#29702;&#38142;&#65292;&#36825;&#20123;&#38142;&#21487;&#33021;&#23548;&#33268;&#19981;&#21516;&#30340;&#31572;&#26696;&#65292;&#28982;&#21518;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#22312;&#39564;&#35777;&#20505;&#36873;&#31572;&#26696;&#26102;&#20351;&#29992;&#21453;&#21521;&#25512;&#29702;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#26495;&#65292;&#21363;``&#22914;&#26524;&#25105;&#20204;&#30693;&#36947;&#19978;&#36848;&#38382;&#39064;&#30340;&#31572;&#26696;&#26159;&#20505;&#36873;&#31572;&#26696;&#65292;&#37027;&#20040;&#26410;&#30693;&#21464;&#37327;x&#30340;&#20540;&#26159;&#22810;&#23569;&#65311;''&#65292;&#23558;&#38382;&#39064;&#20013;&#30340;&#19968;&#20010;&#26631;&#35760;&#23631;&#34109;&#65292;&#24182;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#30452;&#35266;&#19978;&#35762;&#65292;&#22914;&#26524;&#25552;&#20379;&#30340;&#20505;&#36873;&#31572;&#26696;&#26159;&#27491;&#30830;&#30340;&#65292;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#25104;&#21151;&#39044;&#27979;&#34987;&#23631;&#34109;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;FOBAR&#26041;&#27861;&#65292;&#23558;&#27491;&#21521;&#21644;&#21453;&#21521;&#25512;&#29702;&#32467;&#21512;&#36215;&#26469;&#20272;&#35745;&#20505;&#36873;&#31572;&#26696;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#20010;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency \citep{wang2023selfconsistency} proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by ${\bf x}$ and ask the LLM to predict the masked token when a candidate answer is provided by \textit{a simple template}, i.e., ``\textit{\textbf{If we know the answer of the above question is \{a candidate answer\}, what is the value of unknown variable ${\bf x}$?}}'' Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three
&lt;/p&gt;</description></item><item><title>OUTFOX&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20801;&#35768;&#26816;&#27979;&#22120;&#21644;&#25915;&#20987;&#32773;&#32771;&#34385;&#24444;&#27492;&#30340;&#36755;&#20986;&#65292;&#25552;&#39640;&#20102;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#25915;&#20987;&#32773;&#21033;&#29992;&#26816;&#27979;&#22120;&#30340;&#39044;&#27979;&#26631;&#31614;&#20316;&#20026;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#29983;&#25104;&#38590;&#20197;&#26816;&#27979;&#30340;&#23545;&#25239;&#29983;&#25104;&#30340;&#35770;&#25991;&#12290;</title><link>http://arxiv.org/abs/2307.11729</link><description>&lt;p&gt;
OUTFOX: &#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#23545;&#25239;&#29983;&#25104;&#20363;&#23376;&#30340;LLM&#29983;&#25104;&#35770;&#25991;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
OUTFOX: LLM-generated Essay Detection through In-context Learning with Adversarially Generated Examples. (arXiv:2307.11729v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11729
&lt;/p&gt;
&lt;p&gt;
OUTFOX&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20801;&#35768;&#26816;&#27979;&#22120;&#21644;&#25915;&#20987;&#32773;&#32771;&#34385;&#24444;&#27492;&#30340;&#36755;&#20986;&#65292;&#25552;&#39640;&#20102;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#25915;&#20987;&#32773;&#21033;&#29992;&#26816;&#27979;&#22120;&#30340;&#39044;&#27979;&#26631;&#31614;&#20316;&#20026;&#31034;&#20363;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#29983;&#25104;&#38590;&#20197;&#26816;&#27979;&#30340;&#23545;&#25239;&#29983;&#25104;&#30340;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#36798;&#21040;&#20102;&#19982;&#20154;&#31867;&#20889;&#20316;&#30456;&#24403;&#30340;&#27969;&#21033;&#31243;&#24230;&#65292;&#24456;&#38590;&#21306;&#20998;&#20154;&#31867;&#20889;&#20316;&#21644;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#36825;&#22686;&#21152;&#20102;LLMs&#34987;&#35823;&#29992;&#30340;&#39118;&#38505;&#65292;&#24182;&#38656;&#35201;&#24320;&#21457;&#26816;&#27979;&#22120;&#26469;&#35782;&#21035;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#36890;&#36807;&#31616;&#21333;&#22320;&#25913;&#20889;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26469;&#38477;&#20302;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#22312;&#23398;&#29983;&#22312;&#20889;&#20316;&#20316;&#19994;&#65288;&#22914;&#35770;&#25991;&#65289;&#20013;&#20351;&#29992;LLMs&#24182;&#36805;&#36895;&#23398;&#20250;&#22914;&#20309;&#35268;&#36991;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#30495;&#23454;&#29983;&#27963;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#34987;&#25506;&#35752;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OUTFOX&#65292;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20801;&#35768;&#26816;&#27979;&#22120;&#21644;&#25915;&#20987;&#32773;&#32771;&#34385;&#24444;&#27492;&#30340;&#36755;&#20986;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#23398;&#29983;&#35770;&#25991;&#39046;&#22495;&#26469;&#25552;&#39640;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#25915;&#20987;&#32773;&#20351;&#29992;&#26816;&#27979;&#22120;&#30340;&#39044;&#27979;&#26631;&#31614;&#20316;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31034;&#20363;&#65292;&#24182;&#23545;&#38590;&#20197;&#26816;&#27979;&#30340;&#23545;&#25239;&#29983;&#25104;&#35770;&#25991;&#36827;&#34892;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved human-level fluency in text generation, making it difficult to distinguish between human-written and LLM-generated texts. This poses a growing risk of misuse of LLMs and demands the development of detectors to identify LLM-generated texts. However, existing detectors degrade detection accuracy by simply paraphrasing LLM-generated texts. Furthermore, the effectiveness of these detectors in real-life situations, such as when students use LLMs for writing homework assignments (e.g., essays) and quickly learn how to evade these detectors, has not been explored. In this paper, we propose OUTFOX, a novel framework that improves the robustness of LLM-generated-text detectors by allowing both the detector and the attacker to consider each other's output and apply this to the domain of student essays. In our framework, the attacker uses the detector's prediction labels as examples for in-context learning and adversarially generates essays that are hard
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#25104;&#21151;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#21457;&#29616;&#65292;&#21363;&#20351;&#25552;&#31034;&#34987;&#20445;&#23494;&#65292;&#31616;&#21333;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25915;&#20987;&#20173;&#28982;&#21487;&#20197;&#39640;&#27010;&#29575;&#22320;&#25581;&#31034;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.06865</link><description>&lt;p&gt;
&#25552;&#31034;&#19981;&#24212;&#34987;&#35270;&#20026;&#31192;&#23494;&#65306;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#30340;&#25104;&#21151;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompts Should not be Seen as Secrets: Systematically Measuring Prompt Extraction Attack Success. (arXiv:2307.06865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#25104;&#21151;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#23454;&#39564;&#21457;&#29616;&#65292;&#21363;&#20351;&#25552;&#31034;&#34987;&#20445;&#23494;&#65292;&#31616;&#21333;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25915;&#20987;&#20173;&#28982;&#21487;&#20197;&#39640;&#27010;&#29575;&#22320;&#25581;&#31034;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#36890;&#24120;&#36890;&#36807;&#25552;&#31034;&#25216;&#26415;&#26469;&#25511;&#21046;&#65292;&#20854;&#20013;&#29992;&#25143;&#23545;&#27169;&#22411;&#30340;&#26597;&#35810;&#20197;&#26088;&#22312;&#25351;&#23548;&#27169;&#22411;&#22312;&#35813;&#26597;&#35810;&#19978;&#30340;&#34892;&#20026;&#30340;&#25552;&#31034;&#20316;&#20026;&#21069;&#32512;&#12290;&#20844;&#21496;&#29992;&#20110;&#25351;&#23548;&#20854;&#27169;&#22411;&#30340;&#25552;&#31034;&#36890;&#24120;&#34987;&#35270;&#20026;&#31192;&#23494;&#65292;&#38544;&#34255;&#22312;&#26597;&#35810;&#30340;&#29992;&#25143;&#20043;&#22806;&#12290;&#23427;&#20204;&#29978;&#33267;&#34987;&#35270;&#20026;&#21487;&#20197;&#20080;&#21334;&#30340;&#21830;&#21697;&#12290;&#28982;&#32780;&#65292;&#26377;&#32463;&#39564;&#24615;&#30340;&#35777;&#25454;&#26174;&#31034;&#65292;&#21363;&#20351;&#25552;&#31034;&#34987;&#20445;&#23494;&#65292;&#29992;&#25143;&#20173;&#28982;&#21487;&#20197;&#25552;&#21462;&#23427;&#20204;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#22320;&#34913;&#37327;&#25552;&#31034;&#25552;&#21462;&#25915;&#20987;&#25104;&#21151;&#30340;&#26694;&#26550;&#12290;&#22312;&#20351;&#29992;&#22810;&#20010;&#25552;&#31034;&#28304;&#21644;&#22810;&#20010;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#25915;&#20987;&#23454;&#38469;&#19978;&#21487;&#20197;&#39640;&#27010;&#29575;&#22320;&#25581;&#31034;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generations of large language models are commonly controlled through prompting techniques, where a user's query to the model is prefixed with a prompt that aims to guide the model's behaviour on the query. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, there has been anecdotal evidence showing that the prompts can be extracted by a user even when they are kept secret. In this paper, we present a framework for systematically measuring the success of prompt extraction attacks. In experiments with multiple sources of prompts and multiple underlying language models, we find that simple text-based attacks can in fact reveal prompts with high probability.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2306.17256</link><description>&lt;p&gt;
&#20197;&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#26681;&#25454;&#29992;&#25143;&#36807;&#21435;&#30340;&#34892;&#20026;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#19982;&#20854;&#20852;&#36259;&#30456;&#31526;&#30340;&#20449;&#24687;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#21382;&#21490;&#20132;&#20114;&#35760;&#24405;&#19981;&#21487;&#29992;&#26102;&#65292;&#24320;&#21457;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#23601;&#26159;&#25152;&#35859;&#30340;&#31995;&#32479;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#12290;&#27492;&#38382;&#39064;&#22312;&#21019;&#19994;&#20225;&#19994;&#25110;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#20013;&#23588;&#20026;&#31361;&#20986;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#29992;&#25143;&#25110;&#29289;&#21697;&#30340;&#20919;&#21551;&#21160;&#22330;&#26223;&#65292;&#20854;&#20013;&#31995;&#32479;&#20173;&#28982;&#36890;&#36807;&#22312;&#21516;&#19968;&#39046;&#22495;&#20013;&#30340;&#21382;&#21490;&#29992;&#25143;&#21644;&#29289;&#21697;&#20132;&#20114;&#36827;&#34892;&#35757;&#32451;&#26469;&#20026;&#26032;&#29992;&#25143;&#25110;&#29289;&#21697;&#25552;&#20379;&#25512;&#33616;&#65292;&#32780;&#26080;&#27861;&#35299;&#20915;&#25105;&#20204;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#36164;&#26009;&#21644;&#29289;&#21697;&#23646;&#24615;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems play a crucial role in helping users discover information that aligns with their interests based on their past behaviors. However, developing personalized recommendation systems becomes challenging when historical records of user-item interactions are unavailable, leading to what is known as the system cold-start recommendation problem. This issue is particularly prominent in start-up businesses or platforms with insufficient user engagement history. Previous studies focus on user or item cold-start scenarios, where systems could make recommendations for new users or items but are still trained with historical user-item interactions in the same domain, which cannot solve our problem. To bridge the gap, our research introduces an innovative and effective approach, capitalizing on the capabilities of pre-trained language models. We transform the recommendation process into sentiment analysis of natural languages containing information of user profiles and item attribu
&lt;/p&gt;</description></item><item><title>Med-UniC&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#36328;&#35821;&#35328;&#21307;&#23398;&#25968;&#25454;&#65292;&#23454;&#29616;&#36328;&#35821;&#35328;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#25991;&#26412;&#23545;&#40784;&#35268;&#21017;(CTR)&#65292;&#20197;&#26126;&#30830;&#32479;&#19968;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#31038;&#21306;&#30340;&#21307;&#23398;&#25253;&#21578;&#30340;&#36328;&#35821;&#35328;&#35821;&#20041;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2305.19894</link><description>&lt;p&gt;
Med-UniC&#65306;&#36890;&#36807;&#20943;&#23569;&#20559;&#35265;&#23454;&#29616;&#36328;&#35821;&#35328;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;
&lt;/p&gt;
&lt;p&gt;
Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias. (arXiv:2305.19894v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19894
&lt;/p&gt;
&lt;p&gt;
Med-UniC&#26159;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#36328;&#35821;&#35328;&#21307;&#23398;&#25968;&#25454;&#65292;&#23454;&#29616;&#36328;&#35821;&#35328;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#32479;&#19968;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#25991;&#26412;&#23545;&#40784;&#35268;&#21017;(CTR)&#65292;&#20197;&#26126;&#30830;&#32479;&#19968;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#31038;&#21306;&#30340;&#21307;&#23398;&#25253;&#21578;&#30340;&#36328;&#35821;&#35328;&#35821;&#20041;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#32570;&#24615;&#23545;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;(VLP)&#30340;&#25928;&#26524;&#36896;&#25104;&#20102;&#20005;&#37325;&#38556;&#30861;&#12290;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#22312;&#20110;&#32467;&#21512;&#26469;&#33258;&#21508;&#31181;&#35821;&#35328;&#31038;&#21306;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#20027;&#35201;&#25361;&#25112;&#26469;&#33258;&#20110;&#25972;&#21512;&#19981;&#21516;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#12289;&#29305;&#23450;&#20110;&#35821;&#35328;&#30340;&#21307;&#23398;&#26415;&#35821;&#20197;&#21450;&#29305;&#23450;&#20110;&#25991;&#21270;&#30340;&#38544;&#24335;&#30693;&#35782;&#30340;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#32771;&#34385;&#22240;&#32032;&#26159;&#30001;&#19981;&#21516;&#35821;&#35328;&#24341;&#36215;&#30340;&#31038;&#21306;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#32479;&#19968;&#36328;&#35821;&#35328;&#21307;&#23398;&#22270;&#20687;-&#35821;&#35328;&#39044;&#35757;&#32451;(Med-UniC)&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#25972;&#21512;&#26469;&#33258;&#20004;&#31181;&#26368;&#24120;&#35265;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#21307;&#23398;&#25968;&#25454;&#65292;&#21363;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#25991;&#26412;&#23545;&#40784;&#35268;&#21017;(CTR)&#65292;&#26126;&#30830;&#32479;&#19968;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#31038;&#21306;&#30340;&#21307;&#23398;&#25253;&#21578;&#30340;&#36328;&#35821;&#35328;&#35821;&#20041;&#34920;&#31034;&#12290;&#36890;&#36807;&#28508;&#22312;&#35821;&#35328;&#35299;&#32544;&#65292;&#20248;&#21270;CTR&#65292;&#20351;&#25105;&#20204;&#30340;&#20248;&#21270;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scarcity of data presents a critical obstacle to the efficacy of medical visionlanguage pre-training (VLP). A potential solution lies in the combination of datasets from various language communities. Nevertheless, the main challenge stems from the complexity of integrating diverse syntax and semantics, language-specific medical terminology, and culture-specific implicit knowledge. Therefore, one crucial aspect to consider is the presence of community bias caused by different languages. This paper presents a novel framework named Unifying Cross-Lingual Medical Vision-Language Pre-Training (Med-UniC), designed to integrate multimodal medical data from the two most prevalent languages, English and Spanish. Specifically, we propose Cross-lingual Text Alignment Regularization (CTR) to explicitly unify cross-lingual semantic representations of medical reports originating from diverse language communities. CTR is optimized through latent language disentanglement, rendering our optimizatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.14259</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#25991;&#29486;&#30340;&#35821;&#22659;&#21270;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21270;&#30340;&#23398;&#20064;&#29983;&#25104;&#26032;&#30340;&#31185;&#23398;&#26041;&#21521;&#65292;&#20811;&#26381;&#20102;&#26631;&#20934;&#26041;&#27861;&#22312;&#39044;&#27979;&#20851;&#32852;&#12289;&#24573;&#30053;&#19978;&#19979;&#25991;&#31561;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#27169;&#22411;&#20351;&#29992;&#20102;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#22312;&#29983;&#25104;&#21019;&#26032;&#24605;&#24819;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#29486;&#30340;&#21457;&#29616;&#65288;LBD&#65289;&#26088;&#22312;&#36890;&#36807;&#25366;&#25496;&#35770;&#25991;&#24182;&#29983;&#25104;&#20551;&#35774;&#26469;&#21457;&#29616;&#26032;&#30340;&#31185;&#23398;&#30693;&#35782;&#12290;&#26631;&#20934;&#30340;LBD&#20165;&#38480;&#20110;&#39044;&#27979;&#31163;&#25955;&#27010;&#24565;&#20043;&#38388;&#30340;&#20004;&#20004;&#20851;&#31995;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#21644;&#30142;&#30149;&#30340;&#20851;&#32852;&#65289;&#12290;LBD&#36824;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#23454;&#39564;&#35774;&#32622;&#65288;&#20363;&#22914;&#65292;&#33647;&#29289;&#35780;&#20272;&#30340;&#29305;&#23450;&#24739;&#32773;&#32676;&#20307;&#65289;&#21644;&#20154;&#31867;&#31185;&#23398;&#23478;&#32771;&#34385;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#21160;&#26426;&#65288;&#20363;&#22914;&#65292;&#25214;&#21040;&#27809;&#26377;&#29305;&#23450;&#21103;&#20316;&#29992;&#30340;&#33647;&#29289;&#20505;&#36873;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#21270;LBD&#65288;C-LBD&#65289;&#34920;&#36848;&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65306;&#20197;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31185;&#23398;&#20551;&#35774;&#65292;&#21516;&#26102;&#23558;&#23427;&#20204;&#32852;&#31995;&#21040;&#25511;&#21046;&#20551;&#35774;&#25628;&#32034;&#31354;&#38388;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24314;&#27169;&#26694;&#26550;&#65292;&#20351;&#29992;&#33719;&#24471;&#30340;&#24341;&#25991;&#21644;&#30693;&#35782;&#22270;&#20851;&#31995;&#30340;&#24322;&#26500;&#32593;&#32476;&#20013;&#30340;&#8220;&#28789;&#24863;&#8221;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#20174;&#35770;&#25991;&#20013;&#27966;&#29983;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT4&#20542;&#21521;&#20110;&#29983;&#25104;&#20855;&#26377;&#21019;&#26032;&#24615;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
Literature-Based Discovery (LBD) aims to discover new scientific knowledge by mining papers and generating hypotheses. Standard LBD is limited to predicting pairwise relations between discrete concepts (e.g., drug-disease links). LBD also ignores critical contexts like experimental settings (e.g., a specific patient population where a drug is evaluated) and background knowledge and motivations that human scientists consider (e.g., to find a drug candidate without specific side effects). We address these limitations with a novel formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in natural language, while grounding them in a context that controls the hypothesis search space. We present a modeling framework using retrieval of ``inspirations'' from a heterogeneous network of citations and knowledge graph relations, and create a new dataset derived from papers. Our evaluations with powerful large language models (LLMs) reveal that GPT4 tends to generate ideas with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; LogicLLM&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#30417;&#30563;&#21518;&#35757;&#32451;&#26469;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#22312;&#24120;&#35265;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13718</link><description>&lt;p&gt;
LogicLLM&#65306;&#25506;&#32034;&#33258;&#30417;&#30563;&#36923;&#36753;&#22686;&#24378;&#35757;&#32451;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models. (arXiv:2305.13718v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; LogicLLM&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#30417;&#30563;&#21518;&#35757;&#32451;&#26469;&#25552;&#39640;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#22320;&#22312;&#24120;&#35265;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#30340;&#29616;&#26377;&#21162;&#21147;&#20027;&#35201;&#20381;&#36182;&#20110;&#26377;&#30417;&#30563;&#24494;&#35843;&#65292;&#36825;&#38459;&#30861;&#20102;&#23558;&#27169;&#22411;&#27867;&#21270;&#21040;&#26032;&#30340;&#39046;&#22495;&#21644;/&#25110;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21457;&#23637;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;&#20016;&#23500;&#30340;&#30693;&#35782;&#21387;&#32553;&#20026;&#21333;&#20010;&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#22810;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;LLMs &#22312;&#36923;&#36753;&#25512;&#29702;&#26041;&#38754;&#24182;&#27809;&#26377;&#34920;&#29616;&#20986;&#33021;&#21147;&#12290;LLMs &#22312;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#36828;&#36828;&#33853;&#21518;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#36890;&#36807;&#33258;&#30417;&#30563;&#21518;&#35757;&#32451;&#26469;&#25506;&#32034;&#34701;&#21512;&#36923;&#36753;&#30693;&#35782;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#28608;&#27963;&#23427;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;LogicLLM&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;MERIt &#30340;&#33258;&#22238;&#24402;&#30446;&#26631;&#21464;&#20307;&#65292;&#24182;&#23558;&#20854;&#19982;&#20004;&#20010;LLM&#31995;&#21015;FLAN-T5&#21644;LLaMA&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#21442;&#25968;&#22823;&#23567;&#33539;&#22260;&#20174;30&#20159;&#21040;130&#20159;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24120;&#29992;&#25512;&#29702;&#31574;&#30053;&#19978;&#19982;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#65292;&#24182;&#19988;&#36828;&#36828;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing efforts to improve logical reasoning ability of language models have predominantly relied on supervised fine-tuning, hindering generalization to new domains and/or tasks. The development of Large Langauge Models (LLMs) has demonstrated the capacity of compressing abundant knowledge into a single proxy, enabling them to tackle multiple tasks effectively. Our preliminary experiments, nevertheless, show that LLMs do not show capability on logical reasoning. The performance of LLMs on logical reasoning benchmarks is far behind the existing state-of-the-art baselines. In this paper, we make the first attempt to investigate the feasibility of incorporating logical knowledge through self-supervised post-training, and activating it via in-context learning, which we termed as LogicLLM. Specifically, we devise an auto-regressive objective variant of MERIt and integrate it with two LLM series, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to 13 billion. The results 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.13062</link><description>&lt;p&gt;
GPT4Table&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#21527;&#65311;&#19968;&#39033;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPT4Table: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study. (arXiv:2305.13062v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;&#36755;&#20837;&#36873;&#25321;&#20250;&#23545;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23569;&#26679;&#26412;&#25512;&#29702;&#22120;&#26469;&#35299;&#20915;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#36234;&#26469;&#36234;&#20855;&#21560;&#24341;&#21147;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#23545;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#20363;&#22914;&#34920;&#26684;&#65289;&#30340;&#29702;&#35299;&#31243;&#24230;&#36824;&#26377;&#24456;&#22810;&#38656;&#35201;&#23398;&#20064;&#30340;&#22320;&#26041;&#12290;&#23613;&#31649;&#21487;&#20197;&#20351;&#29992;&#34920;&#26684;&#24207;&#21015;&#21270;&#20316;&#20026;LLMs&#30340;&#36755;&#20837;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#23545;LLMs&#26159;&#21542;&#30495;&#27491;&#33021;&#22815;&#29702;&#35299;&#36825;&#31867;&#25968;&#25454;&#30340;&#20840;&#38754;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;LLMs&#30340;&#32467;&#26500;&#29702;&#35299;&#33021;&#21147;&#65288;SUC&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#21019;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#19971;&#20010;&#20219;&#21153;&#65292;&#27599;&#20010;&#20219;&#21153;&#37117;&#26377;&#20854;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#21333;&#20803;&#26684;&#26597;&#25214;&#12289;&#34892;&#26816;&#32034;&#21644;&#22823;&#23567;&#26816;&#27979;&#12290;&#25105;&#20204;&#23545;GPT-3.5&#21644;GPT-4&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#24615;&#33021;&#22240;&#22810;&#31181;&#36755;&#20837;&#36873;&#25321;&#32780;&#24322;&#65292;&#21253;&#25324;&#34920;&#26684;&#36755;&#20837;&#26684;&#24335;&#12289;&#20869;&#23481;&#39034;&#24207;&#12289;&#35282;&#33394;&#25552;&#31034;&#21644;&#20998;&#21306;&#26631;&#35760;&#31561;&#12290;&#26681;&#25454;&#22522;&#20934;&#27979;&#35797;&#35780;&#20272;&#25152;&#24471;&#30340;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#33258;&#25105;&#22686;&#24378;&#8221;&#25216;&#26415;&#20197;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. While it is true that tables can be used as inputs to LLMs with serialization, there lack of comprehensive studies examining whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, \eg, cell lookup, row retrieval, and size detection. We run a series of evaluations on GPT-3.5 and GPT-4. We discover that the performance varied depending on a number of input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we then propose \textit{self-augmentation} for effect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24847;&#22823;&#21033;&#35821;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#39046;&#22495;&#20107;&#20214;&#26694;&#26550;&#26631;&#27880;&#35821;&#26009;&#24211;-EventNet-ITA&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#26631;&#31614;&#26694;&#26550;&#35299;&#26512;&#26041;&#27861;&#12290;EventNet-ITA&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#20107;&#20214;&#25366;&#25496;&#30340;&#36164;&#28304;&#21644;&#24847;&#22823;&#21033;&#35821;&#26694;&#26550;&#35299;&#26512;&#30340;&#26032;&#39062;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2305.10892</link><description>&lt;p&gt;
EventNet-ITA: &#29992;&#20110;&#24847;&#22823;&#21033;&#20107;&#20214;&#30340;&#26694;&#26550;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
EventNet-ITA: Italian Frame Parsing for Events. (arXiv:2305.10892v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#24847;&#22823;&#21033;&#35821;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#39046;&#22495;&#20107;&#20214;&#26694;&#26550;&#26631;&#27880;&#35821;&#26009;&#24211;-EventNet-ITA&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#26631;&#31614;&#26694;&#26550;&#35299;&#26512;&#26041;&#27861;&#12290;EventNet-ITA&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#20107;&#20214;&#25366;&#25496;&#30340;&#36164;&#28304;&#21644;&#24847;&#22823;&#21033;&#35821;&#26694;&#26550;&#35299;&#26512;&#30340;&#26032;&#39062;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;EventNet-ITA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#24847;&#22823;&#21033;&#35821;&#30340;&#22823;&#35268;&#27169;&#12289;&#22810;&#39046;&#22495;&#20107;&#20214;&#26694;&#26550;&#26631;&#27880;&#35821;&#26009;&#24211;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#26631;&#31614;&#26694;&#26550;&#35299;&#26512;&#26041;&#27861;&#12290;&#28982;&#21518;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#28085;&#30422;&#20102;&#21508;&#31181;&#20010;&#20154;&#12289;&#31038;&#20250;&#21644;&#21382;&#21490;&#29616;&#35937;&#65292;EventNet-ITA&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#20107;&#20214;&#25366;&#25496;&#30340;&#36164;&#28304;&#20197;&#21450;&#24847;&#22823;&#21033;&#35821;&#26694;&#26550;&#35299;&#26512;&#30340;&#26032;&#39062;&#32780;&#24191;&#27867;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces EventNet-ITA, a large, multi-domain corpus annotated with event frames for Italian, and presents an efficient approach for multi-label Frame Parsing. The approach is then evaluated on the dataset. Covering a wide range of individual, social and historical phenomena, the main contribution of EventNet-ITA is to provide the research community with a resource for textual event mining and a novel and extensive tool for Frame Parsing in Italian.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;FOMC&#23448;&#26041;&#22768;&#26126;&#20013;&#20351;&#29992;&#30340;&#35821;&#35328;&#65292;&#37319;&#29992;VADER&#21644;FinBERT&#31561;&#27169;&#22411;&#39044;&#27979;&#36127;&#38754;&#24773;&#32490;&#65292;&#32467;&#26524;&#26174;&#31034;FinBERT&#34920;&#29616;&#30456;&#23545;&#26356;&#22909;&#12290;&#20294;&#26159;&#65292;&#35813;&#30740;&#31350;&#20063;&#24378;&#35843;&#20102;&#20351;&#29992;&#24403;&#21069;NLP&#25216;&#26415;&#20998;&#26512;FOMC&#25991;&#26412;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24314;&#35758;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#24182;&#25506;&#32034;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.10164</link><description>&lt;p&gt;
&#20998;&#26512;FOMC&#20250;&#35758;&#35760;&#24405;&#65306;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing FOMC Minutes: Accuracy and Constraints of Language Models. (arXiv:2304.10164v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;FOMC&#23448;&#26041;&#22768;&#26126;&#20013;&#20351;&#29992;&#30340;&#35821;&#35328;&#65292;&#37319;&#29992;VADER&#21644;FinBERT&#31561;&#27169;&#22411;&#39044;&#27979;&#36127;&#38754;&#24773;&#32490;&#65292;&#32467;&#26524;&#26174;&#31034;FinBERT&#34920;&#29616;&#30456;&#23545;&#26356;&#22909;&#12290;&#20294;&#26159;&#65292;&#35813;&#30740;&#31350;&#20063;&#24378;&#35843;&#20102;&#20351;&#29992;&#24403;&#21069;NLP&#25216;&#26415;&#20998;&#26512;FOMC&#25991;&#26412;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24314;&#35758;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#24182;&#25506;&#32034;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35770;&#25991;&#20998;&#26512;&#20102;&#32852;&#37030;&#20844;&#24320;&#24066;&#22330;&#22996;&#21592;&#20250;&#65288;FOMC&#65289;&#22312;&#20854;&#23450;&#26399;&#20250;&#35758;&#21518;&#21457;&#24067;&#30340;&#23448;&#26041;&#22768;&#26126;&#20013;&#20351;&#29992;&#30340;&#35821;&#35328;&#65292;&#20197;&#33719;&#21462;&#26377;&#20851;FOMC&#23448;&#26041;&#22768;&#26126;&#23545;&#37329;&#34701;&#24066;&#22330;&#21644;&#32463;&#27982;&#39044;&#27979;&#30340;&#24433;&#21709;&#30340;&#35265;&#35299;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;FOMC&#23567;&#24515;&#36991;&#20813;&#22312;&#21477;&#23376;&#20013;&#34920;&#36798;&#24773;&#24863;&#65292;&#24182;&#36981;&#24490;&#19968;&#22871;&#27169;&#26495;&#26469;&#35206;&#30422;&#32463;&#27982;&#24773;&#20917;&#12290;&#35813;&#20998;&#26512;&#37319;&#29992;&#20102;VADER&#21644;FinBERT&#31561;&#20808;&#36827;&#30340;&#35821;&#35328;&#24314;&#27169;&#25216;&#26415;&#65292;&#20197;&#21450;&#20351;&#29992;GPT-4&#30340;&#35797;&#39564;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20934;&#30830;&#39044;&#27979;&#36127;&#38754;&#24773;&#32490;&#26041;&#38754;&#65292;FinBERT&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#36824;&#24378;&#35843;&#20102;&#20351;&#29992;&#24403;&#21069;NLP&#25216;&#26415;&#20998;&#26512;FOMC&#25991;&#26412;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#24314;&#35758;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#24182;&#25506;&#32034;&#26367;&#20195;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research article analyzes the language used in the official statements released by the Federal Open Market Committee (FOMC) after its scheduled meetings to gain insights into the impact of FOMC official statements on financial markets and economic forecasting. The study reveals that the FOMC is careful to avoid expressing emotion in their sentences and follows a set of templates to cover economic situations. The analysis employs advanced language modeling techniques such as VADER and FinBERT, and a trial test with GPT-4. The results show that FinBERT outperforms other techniques in predicting negative sentiment accurately. However, the study also highlights the challenges and limitations of using current NLP techniques to analyze FOMC texts and suggests the potential for enhancing language models and exploring alternative approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;ChatGPT&#20316;&#20026;&#24773;&#24863;&#20998;&#26512;&#22120;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#21253;&#25324;&#26631;&#20934;&#35780;&#20272;&#12289;&#26497;&#24615;&#36716;&#31227;&#35780;&#20272;&#12289;&#24320;&#25918;&#22495;&#35780;&#20272;&#21644;&#24773;&#24863;&#25512;&#29702;&#35780;&#20272;&#65292;&#20849;&#28041;&#21450;18&#20010;&#25968;&#25454;&#38598;&#21644;5&#20010;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;&#19982;&#32463;&#36807;&#24494;&#35843;&#30340;BERT&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#27604;&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#21644;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2304.04339</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#24773;&#24863;&#20998;&#26512;&#22120;&#21527;&#65311;&#19968;&#39033;&#21021;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good Sentiment Analyzer? A Preliminary Study. (arXiv:2304.04339v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ChatGPT&#20316;&#20026;&#24773;&#24863;&#20998;&#26512;&#22120;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#21253;&#25324;&#26631;&#20934;&#35780;&#20272;&#12289;&#26497;&#24615;&#36716;&#31227;&#35780;&#20272;&#12289;&#24320;&#25918;&#22495;&#35780;&#20272;&#21644;&#24773;&#24863;&#25512;&#29702;&#35780;&#20272;&#65292;&#20849;&#28041;&#21450;18&#20010;&#25968;&#25454;&#38598;&#21644;5&#20010;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;&#19982;&#32463;&#36807;&#24494;&#35843;&#30340;BERT&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#27604;&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#21644;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;ChatGPT&#22312;&#30740;&#31350;&#21644;&#20844;&#20247;&#30340;&#20851;&#27880;&#19979;&#21463;&#21040;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#25105;&#20204;&#29305;&#21035;&#24819;&#30693;&#36947;&#23427;&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#36890;&#29992;&#24773;&#24863;&#20998;&#26512;&#22120;&#12290;&#20026;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;ChatGPT&#22312;&#25991;&#26412;&#20013;&#21253;&#21547;&#30340;&#24847;&#35265;&#12289;&#24773;&#24863;&#21644;&#24773;&#32490;&#30340;&#29702;&#35299;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#35774;&#32622;&#19979;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#26631;&#20934;&#35780;&#20272;&#12289;&#26497;&#24615;&#36716;&#31227;&#35780;&#20272;&#12289;&#24320;&#25918;&#22495;&#35780;&#20272;&#21644;&#24773;&#24863;&#25512;&#29702;&#35780;&#20272;&#12290;&#20197;&#19978;&#35780;&#20272;&#28041;&#21450;18&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;5&#20010;&#20195;&#34920;&#24615;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;ChatGPT&#19982;&#32463;&#36807;&#24494;&#35843;&#30340;BERT&#21644;&#30456;&#24212;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#27604;&#65292;&#24182;&#22312;&#26411;&#31471;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20123;&#23450;&#24615;&#26696;&#20363;&#30740;&#31350;&#20197;&#28145;&#20837;&#29702;&#35299;&#20854;&#24773;&#24863;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, ChatGPT has drawn great attention from both the research community and the public. We are particularly curious about whether it can serve as a universal sentiment analyzer. To this end, in this work, we provide a preliminary evaluation of ChatGPT on the understanding of opinions, sentiments, and emotions contained in the text. Specifically, we evaluate it in four settings, including standard evaluation, polarity shift evaluation, open-domain evaluation, and sentiment inference evaluation. The above evaluation involves 18 benchmark datasets and 5 representative sentiment analysis tasks, and we compare ChatGPT with fine-tuned BERT and corresponding state-of-the-art (SOTA) models on end-task. Moreover, we also conduct human evaluation and present some qualitative case studies to gain a deep comprehension of its sentiment analysis capabilities.
&lt;/p&gt;</description></item><item><title>KPEval&#26159;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20851;&#38190;&#35789;&#25552;&#21462;&#21644;&#29983;&#25104;&#31995;&#32479;&#35780;&#20272;&#20013;&#30340;&#30701;&#26495;&#12290;&#36890;&#36807;&#24341;&#20837;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#65292;&#21253;&#25324;&#26174;&#33879;&#24615;&#12289;&#24544;&#23454;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#24182;&#35774;&#35745;&#30456;&#24212;&#30340;&#35821;&#20041;&#24230;&#37327;&#25351;&#26631;&#65292;KPEval&#22312;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20351;&#29992;&#35813;&#26694;&#26550;&#37325;&#26032;&#35780;&#20272;&#20102;20&#20010;&#20851;&#38190;&#35789;&#31995;&#32479;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#36873;&#25321;&#26368;&#20339;&#30340;&#24773;&#20917;&#21462;&#20915;&#20110;&#35780;&#20272;&#32500;&#24230;&#65292;&#23454;&#29992;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2303.15422</link><description>&lt;p&gt;
KPEval&#65306;&#38754;&#21521;&#32454;&#31890;&#24230;&#35821;&#20041;&#35780;&#20272;&#20851;&#38190;&#35789;&#25552;&#21462;&#21644;&#29983;&#25104;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
KPEval: Towards Fine-grained Semantic-based Evaluation of Keyphrase Extraction and Generation Systems. (arXiv:2303.15422v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15422
&lt;/p&gt;
&lt;p&gt;
KPEval&#26159;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20851;&#38190;&#35789;&#25552;&#21462;&#21644;&#29983;&#25104;&#31995;&#32479;&#35780;&#20272;&#20013;&#30340;&#30701;&#26495;&#12290;&#36890;&#36807;&#24341;&#20837;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#65292;&#21253;&#25324;&#26174;&#33879;&#24615;&#12289;&#24544;&#23454;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#23454;&#29992;&#24615;&#65292;&#24182;&#35774;&#35745;&#30456;&#24212;&#30340;&#35821;&#20041;&#24230;&#37327;&#25351;&#26631;&#65292;KPEval&#22312;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#20851;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#20351;&#29992;&#35813;&#26694;&#26550;&#37325;&#26032;&#35780;&#20272;&#20102;20&#20010;&#20851;&#38190;&#35789;&#31995;&#32479;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#36873;&#25321;&#26368;&#20339;&#30340;&#24773;&#20917;&#21462;&#20915;&#20110;&#35780;&#20272;&#32500;&#24230;&#65292;&#23454;&#29992;&#24615;&#26159;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20851;&#38190;&#35789;&#25552;&#21462;&#21644;&#29983;&#25104;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#29616;&#34892;&#35780;&#20272;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#19982;&#20154;&#24037;&#21442;&#32771;&#30340;&#23436;&#20840;&#21305;&#37197;&#65292;&#32780;&#24573;&#30053;&#20102;&#26080;&#21442;&#32771;&#23646;&#24615;&#12290;&#36825;&#31181;&#26041;&#24335;&#26080;&#27861;&#35782;&#21035;&#29983;&#25104;&#19982;&#21442;&#32771;&#35821;&#20041;&#31561;&#25928;&#25110;&#20855;&#26377;&#23454;&#38469;&#25928;&#29992;&#30340;&#22810;&#26679;&#21270;&#20851;&#38190;&#35789;&#30340;&#31995;&#32479;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35780;&#20272;&#20851;&#38190;&#35789;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;KPEval&#65292;&#21253;&#21547;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#26174;&#33879;&#24615;&#12289;&#24544;&#23454;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#23545;&#20110;&#27599;&#20010;&#32500;&#24230;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19982;&#35780;&#20272;&#30446;&#26631;&#30456;&#19968;&#33268;&#30340;&#22522;&#20110;&#35821;&#20041;&#30340;&#24230;&#37327;&#25351;&#26631;&#12290;&#20803;&#35780;&#20272;&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#20351;&#29992;&#30340;&#19968;&#31995;&#21015;&#24230;&#37327;&#25351;&#26631;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#31574;&#30053;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#20851;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#37325;&#26032;&#35780;&#20272;&#20102;20&#20010;&#20851;&#38190;&#35789;&#31995;&#32479;&#65292;&#24182;&#36827;&#19968;&#27493;&#21457;&#29616;&#65306;(1)&#26368;&#22909;&#30340;&#27169;&#22411;&#26681;&#25454;&#35780;&#20272;&#32500;&#24230;&#19981;&#21516;&#32780;&#19981;&#21516;&#65307;(2)&#23454;&#29992;&#24615;&#26159;&#20851;&#38190;&#35789;&#31995;&#32479;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the significant advancements in keyphrase extraction and keyphrase generation methods, the predominant approach for evaluation only relies on exact matching with human references and disregards reference-free attributes. This scheme fails to recognize systems that generate keyphrases semantically equivalent to the references or diverse keyphrases that carry practical utility. To better assess the capability of keyphrase systems, we propose KPEval, a comprehensive evaluation framework consisting of four critical dimensions: saliency, faithfulness, diversity, and utility. For each dimension, we design semantic-based metrics that align with the evaluation objectives. Meta-evaluation studies demonstrate that our evaluation strategy correlates better with human preferences compared to a range of previously used metrics. Using this framework, we re-evaluate 20 keyphrase systems and further discover that (1) the best model differs depending on the evaluation dimension; (2) the utility
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#19981;&#21487;&#38752;&#12290;&#25913;&#20889;&#25915;&#20987;&#21487;&#20197;&#30772;&#35299;&#22810;&#31181;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#65292;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#24615;&#33021;&#20063;&#20250;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#21487;&#38752;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.11156</link><description>&lt;p&gt;
AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26159;&#21542;&#21487;&#38752;&#22320;&#26816;&#27979;&#20986;&#26469;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI-Generated Text be Reliably Detected?. (arXiv:2303.11156v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#19981;&#21487;&#38752;&#12290;&#25913;&#20889;&#25915;&#20987;&#21487;&#20197;&#30772;&#35299;&#22810;&#31181;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#21363;&#20351;&#26159;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#65292;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#25552;&#21319;&#65292;&#24615;&#33021;&#20063;&#20250;&#19979;&#38477;&#12290;&#22240;&#27492;&#65292;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#21487;&#38752;&#26816;&#27979;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#23454;&#35777;&#21644;&#29702;&#35770;&#20004;&#20010;&#26041;&#38754;&#34920;&#26126;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#20960;&#31181;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#24182;&#19981;&#21487;&#38752;&#12290;&#20174;&#23454;&#36341;&#19978;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36731;&#37327;&#32423;&#30340;&#25913;&#20889;&#22120;&#24212;&#29992;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19978;&#21487;&#20197;&#30772;&#35299;&#19968;&#31995;&#21015;&#30340;&#26816;&#27979;&#22120;&#65292;&#21253;&#25324;&#20351;&#29992;&#27700;&#21360;&#26041;&#26696;&#12289;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26088;&#22312;&#36530;&#36991;&#25913;&#20889;&#25915;&#20987;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26816;&#27979;&#22120;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36882;&#24402;&#25913;&#20889;&#30340;&#25915;&#20987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#65292;&#25351;&#20986;&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#21644;&#26356;&#25797;&#38271;&#27169;&#20223;&#20154;&#31867;&#25991;&#26412;&#65292;&#22312;&#26368;&#22909;&#30340;&#26816;&#27979;&#22120;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#23545;&#20110;&#19968;&#20010;&#36275;&#22815;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#27169;&#20223;&#20154;&#31867;&#25991;&#26412;&#65292;&#21363;&#20351;&#26368;&#20339;&#30340;&#26816;&#27979;&#22120;&#30340;&#34920;&#29616;&#21482;&#27604;&#38543;&#26426;&#20998;&#31867;&#22120;&#22909;&#19978;&#19968;&#28857;&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36275;&#22815;&#27010;&#25324;&#29305;&#23450;&#30340;&#22330;&#26223;&#65292;&#22914;&#25913;&#20889;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, both empirically and theoretically, we show that several AI-text detectors are not reliable in practical scenarios. Empirically, we show that paraphrasing attacks, where a light paraphraser is applied on top of a large language model (LLM), can break a whole range of detectors, including ones using watermarking schemes as well as neural network-based detectors and zero-shot classifiers. Our experiments demonstrate that retrieval-based detectors, designed to evade paraphrasing attacks, are still vulnerable to recursive paraphrasing. We then provide a theoretical impossibility result indicating that as language models become more sophisticated and better at emulating human text, the performance of even the best-possible detector decreases. For a sufficiently advanced language model seeking to imitate human text, even the best-possible detector may only perform marginally better than a random classifier. Our result is general enough to capture specific scenarios such as par
&lt;/p&gt;</description></item></channel></rss>