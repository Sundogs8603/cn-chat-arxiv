<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#40784;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20043;&#21069;&#25991;&#29486;&#20013;&#30340;&#26576;&#20010;&#20998;&#26512;&#34920;&#36798;&#24335;&#26159;&#38169;&#35823;&#30340;&#12290;&#30740;&#31350;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;KL&#25955;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01879</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#30340;&#29702;&#35770;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Theoretical guarantees on the best-of-n alignment policy. (arXiv:2401.01879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01879
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#40784;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20043;&#21069;&#25991;&#29486;&#20013;&#30340;&#26576;&#20010;&#20998;&#26512;&#34920;&#36798;&#24335;&#26159;&#38169;&#35823;&#30340;&#12290;&#30740;&#31350;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;KL&#25955;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#26159;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20174;&#19968;&#20010;&#22522;&#26412;&#31574;&#30053;&#20013;&#25277;&#21462;n&#20010;&#26679;&#26412;&#65292;&#24182;&#26681;&#25454;&#22870;&#21169;&#20989;&#25968;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#24207;&#65292;&#36873;&#25321;&#25490;&#21517;&#26368;&#39640;&#30340;&#26679;&#26412;&#12290;&#25991;&#29486;&#20013;&#24120;&#29992;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#22768;&#31216;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#19982;&#22522;&#26412;&#31574;&#30053;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#31561;&#20110;$\log (n) (n-1)/n$&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#35770;&#26029;&#30340;&#19981;&#27491;&#30830;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#21482;&#26159;&#23454;&#38469;KL&#25955;&#24230;&#30340;&#19968;&#20010;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#35813;&#19978;&#30028;&#30340;&#32039;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KL&#25955;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20960;&#20010;&#20363;&#23376;&#30340;&#23454;&#39564;&#35777;&#26126;&#23427;&#33021;&#25552;&#20379;&#19968;&#20010;&#32039;&#33268;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
A simple and effective method for the alignment of generative models is the best-of-$n$ policy, where $n$ samples are drawn from a base policy, and ranked based on a reward function, and the highest ranking one is selected. A commonly used analytical expression in the literature claims that the KL divergence between the best-of-$n$ policy and the base policy is equal to $\log (n) (n-1)/n.$ We disprove the validity of this claim, and show that it is an upper bound on the actual KL divergence. We also explore the tightness of this upper bound in different regimes. Finally, we propose a new estimator for the KL divergence and empirically show that it provides a tight approximation through a few examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#21644;&#35782;&#21035;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#31995;&#32479;&#23637;&#31034;&#20102;LLMs&#23545;&#35270;&#35273;&#19990;&#30028;&#30340;&#35748;&#30693;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31934;&#30830;&#24314;&#27169;&#23383;&#31526;&#20018;&#21487;&#20197;&#25945;&#20250;LLMs&#20851;&#20110;&#35270;&#35273;&#19990;&#30028;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;LLMs&#21487;&#20197;&#35757;&#32451;&#33021;&#22815;&#23545;&#33258;&#28982;&#22270;&#20687;&#36827;&#34892;&#35821;&#20041;&#35780;&#20272;&#30340;&#35270;&#35273;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.01862</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Vision Check-up for Language Models. (arXiv:2401.01862v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#21644;&#35782;&#21035;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#31995;&#32479;&#23637;&#31034;&#20102;LLMs&#23545;&#35270;&#35273;&#19990;&#30028;&#30340;&#35748;&#30693;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31934;&#30830;&#24314;&#27169;&#23383;&#31526;&#20018;&#21487;&#20197;&#25945;&#20250;LLMs&#20851;&#20110;&#35270;&#35273;&#19990;&#30028;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#24182;&#19988;&#20165;&#20351;&#29992;LLMs&#21487;&#20197;&#35757;&#32451;&#33021;&#22815;&#23545;&#33258;&#28982;&#22270;&#20687;&#36827;&#34892;&#35821;&#20041;&#35780;&#20272;&#30340;&#35270;&#35273;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#24314;&#27169;&#23383;&#31526;&#20018;&#20043;&#38388;&#20851;&#31995;&#26159;&#21542;&#33021;&#25945;&#20250;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20851;&#20110;&#35270;&#35273;&#19990;&#30028;&#30340;&#30693;&#35782;&#65311;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;LLMs&#29983;&#25104;&#21644;&#35782;&#21035;&#21508;&#31181;&#36880;&#28176;&#22797;&#26434;&#30340;&#35270;&#35273;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#28982;&#21518;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25991;&#26412;&#27169;&#22411;&#26469;&#35757;&#32451;&#21021;&#27493;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#31995;&#32479;&#12290;&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#20197;&#20687;&#32032;&#24418;&#24335;&#33719;&#21462;&#25110;&#36755;&#20986;&#35270;&#35273;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#30740;&#31350;&#20013;&#20351;&#29992;&#20195;&#30721;&#26469;&#34920;&#31034;&#22270;&#20687;&#12290;&#34429;&#28982;LLMs&#29983;&#25104;&#30340;&#22270;&#20687;&#30475;&#36215;&#26469;&#19981;&#20687;&#33258;&#28982;&#22270;&#20687;&#65292;&#20294;&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#27169;&#22411;&#32416;&#27491;&#36825;&#20123;&#29983;&#25104;&#22270;&#20687;&#30340;&#33021;&#21147;&#26041;&#38754;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31934;&#30830;&#24314;&#27169;&#23383;&#31526;&#20018;&#21487;&#20197;&#25945;&#20250;&#35821;&#35328;&#27169;&#22411;&#20851;&#20110;&#35270;&#35273;&#19990;&#30028;&#30340;&#35768;&#22810;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#19982;&#25991;&#26412;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#36827;&#34892;&#33258;&#25105;&#30417;&#30563;&#30340;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#23454;&#39564;&#35777;&#26126;&#20102;&#20165;&#20351;&#29992;LLMs&#21487;&#20197;&#35757;&#32451;&#33021;&#22815;&#23545;&#33258;&#28982;&#22270;&#20687;&#36827;&#34892;&#35821;&#20041;&#35780;&#20272;&#30340;&#35270;&#35273;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
What does learning to model relationships between strings teach large language models (LLMs) about the visual world? We systematically evaluate LLMs' abilities to generate and recognize an assortment of visual concepts of increasing complexity and then demonstrate how a preliminary visual representation learning system can be trained using models of text. As language models lack the ability to consume or output visual information as pixels, we use code to represent images in our study. Although LLM-generated images do not look like natural images, results on image generation and the ability of models to correct these generated images indicate that precise modeling of strings can teach language models about numerous aspects of the visual world. Furthermore, experiments on self-supervised visual representation learning, utilizing images generated with text models, highlight the potential to train vision models capable of making semantic assessments of natural images using just LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;</title><link>http://arxiv.org/abs/2401.01854</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multilingual Instruction Tuning With Just a Pinch of Multilinguality. (arXiv:2401.01854v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#12290;&#24635;&#20307;&#26469;&#35828;&#65292;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20840;&#29699;&#37319;&#32435;&#65292;&#23427;&#20204;&#22312;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#36890;&#36807;&#22312;&#21478;&#19968;&#31181;&#35821;&#35328;&#19978;&#24494;&#35843;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#26576;&#31181;&#35821;&#35328;&#19978;&#33719;&#24471;&#29305;&#23450;&#30340;&#21151;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;LLM&#22312;&#25351;&#20196;&#35843;&#20248;&#36807;&#31243;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#23545;&#36328;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#30340;&#24433;&#21709;&#12290;&#39318;&#20808;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#22312;&#21333;&#35821;&#35843;&#20248;&#36807;&#31243;&#20013;&#65292;&#35768;&#22810;&#35821;&#35328;&#20063;&#21487;&#20197;&#23558;&#19968;&#20123;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33521;&#35821;&#35843;&#20248;&#38598;&#21512;&#20013;&#65292;&#21482;&#26377;40&#20010;&#22810;&#35821;&#35328;&#31034;&#20363;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22810;&#35821;&#35328;&#25351;&#20196;&#36981;&#24490;&#65292;&#22312;&#35843;&#20248;&#36807;&#31243;&#20013;&#19981;&#35770;&#26159;&#24050;&#35265;&#35821;&#35328;&#36824;&#26159;&#26410;&#35265;&#35821;&#35328;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#22810;&#35821;&#35328;&#28151;&#21512;&#35843;&#20248;&#30340;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30456;&#27604;&#21333;&#35821;&#35843;&#20248;&#30340;&#27169;&#22411;&#35201;&#22909;&#25110;&#32773;&#19981;&#30456;&#19978;&#19979;&#65292;&#23613;&#31649;&#20351;&#29992;&#30340;&#36825;&#20123;&#35821;&#35328;&#30340;&#35757;&#32451;&#31034;&#20363;&#25968;&#37327;&#21482;&#26377;10&#20493;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those language
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;BERT&#27169;&#22411;&#30340;&#22635;&#20805;-&#25513;&#30721;&#21151;&#33021;&#65292;&#36890;&#36807;&#36845;&#20195;&#25513;&#30721;&#21644;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#36827;&#34892;&#26367;&#25442;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01830</link><description>&lt;p&gt;
&#36845;&#20195;&#25513;&#30721;&#22635;&#20805;&#65306;&#19968;&#31181;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#30340;&#26377;&#25928;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Iterative Mask Filling: An Effective Text Augmentation Method Using Masked Language Modeling. (arXiv:2401.01830v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;BERT&#27169;&#22411;&#30340;&#22635;&#20805;-&#25513;&#30721;&#21151;&#33021;&#65292;&#36890;&#36807;&#36845;&#20195;&#25513;&#30721;&#21644;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#36827;&#34892;&#26367;&#25442;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#25928;&#25216;&#26415;&#65292;&#20294;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#23427;&#30340;&#24212;&#29992;&#36828;&#19981;&#21450;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#24191;&#27867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;BERT&#27169;&#22411;&#30340;&#22635;&#20805;-&#25513;&#30721;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#21477;&#23376;&#20013;&#30340;&#21333;&#35789;&#36827;&#34892;&#36845;&#20195;&#25513;&#30721;&#65292;&#24182;&#29992;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#36827;&#34892;&#26367;&#25442;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#37117;&#24456;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#19982;&#29616;&#26377;&#30340;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20027;&#39064;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is an effective technique for improving the performance of machine learning models. However, it has not been explored as extensively in natural language processing (NLP) as it has in computer vision. In this paper, we propose a novel text augmentation method that leverages the Fill-Mask feature of the transformer-based BERT model. Our method involves iteratively masking words in a sentence and replacing them with language model predictions. We have tested our proposed method on various NLP tasks and found it to be effective in many cases. Our results are presented along with a comparison to existing augmentation methods. Experimental results show that our proposed method significantly improves performance, especially on topic classification datasets.
&lt;/p&gt;</description></item><item><title>Physio&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#29289;&#29702;&#27835;&#30103;&#39038;&#38382;&#24212;&#29992;&#31243;&#24207;&#65292;&#23427;&#32467;&#21512;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#24182;&#22312;&#22238;&#31572;&#20013;&#24341;&#29992;&#21487;&#38752;&#30340;&#20581;&#24247;&#26469;&#28304;&#65292;&#33021;&#22815;&#36827;&#34892;&#35786;&#26029;&#12289;&#25512;&#33616;&#24247;&#22797;&#36816;&#21160;&#21644;&#38750;&#22788;&#26041;&#33647;&#29289;&#20197;&#32531;&#35299;&#30151;&#29366;&#12290;</title><link>http://arxiv.org/abs/2401.01825</link><description>&lt;p&gt;
Physio:&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#29289;&#29702;&#27835;&#30103;&#39038;&#38382;
&lt;/p&gt;
&lt;p&gt;
Physio: An LLM-Based Physiotherapy Advisor. (arXiv:2401.01825v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01825
&lt;/p&gt;
&lt;p&gt;
Physio&#26159;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#29289;&#29702;&#27835;&#30103;&#39038;&#38382;&#24212;&#29992;&#31243;&#24207;&#65292;&#23427;&#32467;&#21512;&#20102;&#29983;&#25104;&#27169;&#22411;&#30340;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#24182;&#22312;&#22238;&#31572;&#20013;&#24341;&#29992;&#21487;&#38752;&#30340;&#20581;&#24247;&#26469;&#28304;&#65292;&#33021;&#22815;&#36827;&#34892;&#35786;&#26029;&#12289;&#25512;&#33616;&#24247;&#22797;&#36816;&#21160;&#21644;&#38750;&#22788;&#26041;&#33647;&#29289;&#20197;&#32531;&#35299;&#30151;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#22686;&#21152;&#20102;&#23558;&#23427;&#20204;&#25972;&#21512;&#21040;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#29702;&#20294;&#19981;&#27491;&#30830;&#30340;&#25991;&#26412;&#20107;&#23454;&#22312;&#32771;&#34385;&#22312;&#20960;&#20010;&#39046;&#22495;&#20351;&#29992;&#23427;&#20204;&#26102;&#20250;&#36896;&#25104;&#38480;&#21046;&#12290;&#21307;&#30103;&#20445;&#20581;&#26159;&#19968;&#20010;&#35201;&#27714;&#25991;&#26412;&#29983;&#25104;&#21487;&#20449;&#24230;&#30340;&#39046;&#22495;&#30340;&#20856;&#22411;&#20363;&#23376;&#65292;&#20197;&#20445;&#38556;&#24739;&#32773;&#30340;&#20581;&#24247;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Physio&#65292;&#19968;&#31181;&#29992;&#20110;&#29289;&#29702;&#24247;&#22797;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#24212;&#29992;&#31243;&#24207;&#12290;Physio&#33021;&#22815;&#36827;&#34892;&#21021;&#27493;&#35786;&#26029;&#65292;&#24182;&#24341;&#29992;&#21487;&#38752;&#30340;&#20581;&#24247;&#26469;&#28304;&#26469;&#25903;&#25345;&#25552;&#20379;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;Physio&#36824;&#21487;&#20197;&#20511;&#21161;&#22806;&#37096;&#30693;&#35782;&#25968;&#25454;&#24211;&#25512;&#33616;&#24247;&#22797;&#36816;&#21160;&#21644;&#38750;&#22788;&#26041;&#33647;&#29289;&#20197;&#32531;&#35299;&#30151;&#29366;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#20123;&#21151;&#33021;&#65292;Physio&#21487;&#20197;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#35328;&#22788;&#29702;&#65292;&#21516;&#26102;&#23558;&#20854;&#22238;&#22797;&#26465;&#20214;&#21270;&#20026;&#21487;&#38752;&#21644;&#21487;&#39564;&#35777;&#30340;&#26469;&#28304;&#12290;Physio&#30340;&#22312;&#32447;&#28436;&#31034;&#21487;&#22312;https://phys&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capabilities of the most recent language models have increased the interest in integrating them into real-world applications. However, the fact that these models generate plausible, yet incorrect text poses a constraint when considering their use in several domains. Healthcare is a prime example of a domain where text-generative trustworthiness is a hard requirement to safeguard patient well-being. In this paper, we present Physio, a chat-based application for physical rehabilitation. Physio is capable of making an initial diagnosis while citing reliable health sources to support the information provided. Furthermore, drawing upon external knowledge databases, Physio can recommend rehabilitation exercises and over-the-counter medication for symptom relief. By combining these features, Physio can leverage the power of generative models for language processing while also conditioning its response on dependable and verifiable sources. A live demo of Physio is available at https://phys
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#33258;&#25105;&#35780;&#20272;&#26159;&#21542;&#38656;&#35201;&#26597;&#35810;&#22806;&#37096;&#36164;&#28304;&#65292;&#26469;&#20248;&#21270;&#23553;&#38381;&#24335;&#38382;&#31572;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#24341;&#20837;&#24187;&#35273;&#23631;&#34109;&#26426;&#21046;&#20197;&#21450;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#35813;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.01780</link><description>&lt;p&gt;
&#22312;&#23553;&#38381;&#24335;&#38382;&#31572;&#20013;&#20248;&#21270;API&#20381;&#36182;&#20197;&#20943;&#23569;&#24187;&#35273;&#30340;&#19981;&#30830;&#23450;&#24615;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book Question Answering. (arXiv:2401.01780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#33258;&#25105;&#35780;&#20272;&#26159;&#21542;&#38656;&#35201;&#26597;&#35810;&#22806;&#37096;&#36164;&#28304;&#65292;&#26469;&#20248;&#21270;&#23553;&#38381;&#24335;&#38382;&#31572;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#24341;&#20837;&#24187;&#35273;&#23631;&#34109;&#26426;&#21046;&#20197;&#21450;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#33021;&#22815;&#31215;&#32047;&#21644;&#24674;&#22797;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#12290;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#20107;&#23454;&#24615;&#38382;&#39064;&#26102;&#65292;LLM&#19981;&#33021;&#20165;&#20165;&#20381;&#38752;&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#26469;&#20445;&#35777;&#30495;&#23454;&#21644;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#25628;&#32034;&#22806;&#37096;&#20449;&#24687;&#28304;(&#22914;&#32593;&#32476;)&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#26159;&#19968;&#31181;&#23558;&#30693;&#35782;&#22522;&#20110;&#26816;&#32034;&#20449;&#24687;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#37327;&#25991;&#26723;&#20013;&#36827;&#34892;&#25628;&#32034;&#20250;&#24102;&#26469;&#39069;&#22806;&#30340;&#35745;&#31639;/&#26102;&#38388;&#25104;&#26412;&#12290;&#26368;&#20339;&#31574;&#30053;&#26159;&#21482;&#26377;&#22312;LLM&#23545;&#31572;&#26696;&#19981;&#30830;&#23450;&#26102;&#25165;&#26597;&#35810;&#22806;&#37096;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#65292;&#33021;&#22815;&#33258;&#25105;&#35780;&#20272;&#26159;&#21542;&#33021;&#22815;&#30452;&#25509;&#22238;&#31572;&#38382;&#39064;&#25110;&#32773;&#38656;&#35201;&#35831;&#27714;&#22806;&#37096;&#24037;&#20855;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#38381;&#20070;&#38382;&#31572;&#20219;&#21153;&#20013;&#24341;&#20837;&#24187;&#35273;&#23631;&#34109;&#26426;&#21046;&#26469;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#21033;&#29992;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLM) are able to accumulate and restore knowledge, they are still prone to hallucination. Especially when faced with factual questions, LLM cannot only rely on knowledge stored in parameters to guarantee truthful and correct answers. Augmenting these models with the ability to search on external information sources, such as the web, is a promising approach to ground knowledge to retrieve information. However, searching in a large collection of documents introduces additional computational/time costs. An optimal behavior would be to query external resources only when the LLM is not confident about answers. In this paper, we propose a new LLM able to self-estimate if it is able to answer directly or needs to request an external tool. We investigate a supervised approach by introducing a hallucination masking mechanism in which labels are generated using a close book question-answering task. In addition, we propose to leverage parameter-efficient fine-tuning t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30446;&#26631;&#20998;&#26512;&#35270;&#35282;&#36827;&#34892;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#35270;&#35282;&#25552;&#31034;&#35843;&#25972;&#26694;&#26550;&#23558;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#34701;&#21512;&#21040;&#31435;&#22330;&#39044;&#27979;&#22120;&#20013;&#12290;</title><link>http://arxiv.org/abs/2401.01761</link><description>&lt;p&gt;
&#21033;&#29992;&#30446;&#26631;&#20998;&#26512;&#35270;&#35282;&#36827;&#34892;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cross-target Stance Detection by Exploiting Target Analytical Perspectives. (arXiv:2401.01761v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#30446;&#26631;&#20998;&#26512;&#35270;&#35282;&#36827;&#34892;&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22810;&#35270;&#35282;&#25552;&#31034;&#35843;&#25972;&#26694;&#26550;&#23558;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#34701;&#21512;&#21040;&#31435;&#22330;&#39044;&#27979;&#22120;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#30446;&#26631;&#31435;&#22330;&#26816;&#27979;(CTSD)&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#36890;&#36807;&#21033;&#29992;&#28304;&#30446;&#26631;&#20135;&#29983;&#30340;&#27880;&#37322;&#25968;&#25454;&#26469;&#25512;&#26029;&#30446;&#30340;&#30446;&#26631;&#30340;&#24577;&#24230;&#12290;CTSD&#20013;&#30340;&#19968;&#31181;&#37325;&#35201;&#26041;&#27861;&#26159;&#25552;&#21462;&#22495;&#19981;&#21464;&#29305;&#24449;&#20197;&#22635;&#34917;&#22810;&#20010;&#30446;&#26631;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#38750;&#27491;&#24335;&#21644;&#30701;&#25991;&#26412;&#32467;&#26500;&#30340;&#20998;&#26512;&#20197;&#21450;&#38544;&#21547;&#34920;&#36798;&#20351;&#24471;&#25552;&#21462;&#22495;&#19981;&#21464;&#30693;&#35782;&#21464;&#24471;&#22797;&#26434;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#25552;&#31034;&#35843;&#25972;(MPPT)&#27169;&#22411;&#29992;&#20110;CTSD&#65292;&#35813;&#27169;&#22411;&#23558;&#20998;&#26512;&#35270;&#35282;&#20316;&#20026;&#30693;&#35782;&#20256;&#36882;&#30340;&#26725;&#26753;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#25351;&#20196;&#30340;&#20004;&#38454;&#27573;&#24605;&#32500;&#38142;(TsCoT)&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#25351;&#20196;&#21046;&#23450;&#65292;&#20174;&#22810;&#20010;&#35270;&#35282;&#25552;&#21462;&#30446;&#26631;&#20998;&#26512;&#35270;&#35282;&#24182;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;(NLEs)&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#35270;&#35282;&#25552;&#31034;&#35843;&#33410;&#26694;&#26550;(MultiPLN)&#65292;&#23558;NLEs&#34701;&#21512;&#21040;&#31435;&#22330;&#39044;&#27979;&#22120;&#20013;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-target stance detection (CTSD) is an important task, which infers the attitude of the destination target by utilizing annotated data derived from the source target. One important approach in CTSD is to extract domain-invariant features to bridge the knowledge gap between multiple targets. However, the analysis of informal and short text structure, and implicit expressions, complicate the extraction of domain-invariant knowledge. In this paper, we propose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the analysis perspective as a bridge to transfer knowledge. First, we develop a two-stage instruct-based chain-of-thought method (TsCoT) to elicit target analysis perspectives and provide natural language explanations (NLEs) from multiple viewpoints by formulating instructions based on large language model (LLM). Second, we propose a multi-perspective prompt-tuning framework (MultiPLN) to fuse the NLEs into the stance predictor. Extensive experiments results demons
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VGA&#30340;&#35270;&#35273;&#21644;&#22270;&#24418;&#34701;&#21512;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#35875;&#35328;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#32771;&#34385;&#20102;&#28304;&#32034;&#36180;&#21644;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#36824;&#21033;&#29992;&#35875;&#35328;&#30340;&#35780;&#35770;&#21644;&#20256;&#25773;&#32467;&#26500;&#26469;&#25581;&#31034;&#35875;&#35328;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#22270;&#20687;&#31713;&#25913;&#21644;&#25991;&#26412;&#20449;&#24687;&#38544;&#34255;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01759</link><description>&lt;p&gt;
VGA: &#35270;&#35273;&#21644;&#22270;&#24418;&#34701;&#21512;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#35875;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
VGA: Vision and Graph Fused Attention Network for Rumor Detection. (arXiv:2401.01759v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01759
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VGA&#30340;&#35270;&#35273;&#21644;&#22270;&#24418;&#34701;&#21512;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#35875;&#35328;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#32771;&#34385;&#20102;&#28304;&#32034;&#36180;&#21644;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#36824;&#21033;&#29992;&#35875;&#35328;&#30340;&#35780;&#35770;&#21644;&#20256;&#25773;&#32467;&#26500;&#26469;&#25581;&#31034;&#35875;&#35328;&#65292;&#24182;&#19988;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#22270;&#20687;&#31713;&#25913;&#21644;&#25991;&#26412;&#20449;&#24687;&#38544;&#34255;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#21457;&#23637;&#65292;&#35875;&#35328;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24191;&#27867;&#20256;&#25773;&#65292;&#32473;&#31038;&#20250;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#21361;&#23475;&#12290;&#38500;&#20102;&#25991;&#26412;&#20449;&#24687;&#22806;&#65292;&#35768;&#22810;&#35875;&#35328;&#36824;&#20351;&#29992;&#31713;&#25913;&#30340;&#22270;&#20687;&#25110;&#23558;&#25991;&#26412;&#20449;&#24687;&#38544;&#34255;&#22312;&#22270;&#20687;&#20013;&#65292;&#20197;&#27450;&#39575;&#20154;&#20204;&#24182;&#36991;&#20813;&#34987;&#26816;&#27979;&#20986;&#26469;&#65292;&#20351;&#24471;&#22810;&#27169;&#24577;&#35875;&#35328;&#26816;&#27979;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;&#22810;&#27169;&#24577;&#35875;&#35328;&#26816;&#27979;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#25552;&#21462;&#28304;&#32034;&#36180;&#21644;&#30456;&#24212;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#32780;&#24573;&#30053;&#20102;&#35875;&#35328;&#30340;&#35780;&#35770;&#21644;&#20256;&#25773;&#32467;&#26500;&#12290;&#36825;&#20123;&#35780;&#35770;&#21644;&#32467;&#26500;&#34164;&#21547;&#30528;&#20247;&#20154;&#30340;&#26234;&#24935;&#65292;&#24182;&#34987;&#35777;&#26126;&#23545;&#25581;&#38706;&#35875;&#35328;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#21482;&#20197;&#22522;&#26412;&#26041;&#24335;&#25552;&#21462;&#35270;&#35273;&#29305;&#24449;&#65292;&#24456;&#23569;&#32771;&#34385;&#22270;&#20687;&#20013;&#30340;&#31713;&#25913;&#25110;&#25991;&#26412;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35273;&#21644;&#22270;&#24418;&#34701;&#21512;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;VGA&#65289;&#29992;&#20110;&#35875;&#35328;&#26816;&#27979;&#65292;&#20197;&#21033;&#29992;&#24086;&#23376;&#20043;&#38388;&#30340;&#20256;&#25773;&#32467;&#26500;&#65292;&#33719;&#21462;&#20247;&#20154;&#30340;&#24847;&#35265;&#21644;&#35875;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of social media, rumors have been spread broadly on social media platforms, causing great harm to society. Beside textual information, many rumors also use manipulated images or conceal textual information within images to deceive people and avoid being detected, making multimodal rumor detection be a critical problem. The majority of multimodal rumor detection methods mainly concentrate on extracting features of source claims and their corresponding images, while ignoring the comments of rumors and their propagation structures. These comments and structures imply the wisdom of crowds and are proved to be crucial to debunk rumors. Moreover, these methods usually only extract visual features in a basic manner, seldom consider tampering or textual information in images. Therefore, in this study, we propose a novel Vision and Graph Fused Attention Network (VGA) for rumor detection to utilize propagation structures among posts so as to obtain the crowd opinions and fur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#23578;&#26410;&#22312;&#30693;&#35782;&#22270;&#35889;&#23545;&#35805;&#38382;&#31572;&#20219;&#21153;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#22270;&#26597;&#35810;&#26041;&#38754;&#30340;&#33021;&#21147;&#20197;&#21450;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#21644;&#24494;&#35843;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.01711</link><description>&lt;p&gt;
&#22312;&#35821;&#20041;&#35299;&#26512;&#20013;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models in Semantic Parsing for Conversational Question Answering over Knowledge Graphs. (arXiv:2401.01711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#23578;&#26410;&#22312;&#30693;&#35782;&#22270;&#35889;&#23545;&#35805;&#38382;&#31572;&#20219;&#21153;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29983;&#25104;&#22270;&#26597;&#35810;&#26041;&#38754;&#30340;&#33021;&#21147;&#20197;&#21450;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#21644;&#24494;&#35843;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#38382;&#31572;&#31995;&#32479;&#36890;&#24120;&#20381;&#36182;&#35821;&#20041;&#35299;&#26512;&#26469;&#23454;&#29616;&#20132;&#20114;&#24335;&#20449;&#24687;&#26816;&#32034;&#65292;&#35813;&#36807;&#31243;&#28041;&#21450;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#26597;&#35810;&#12290;&#23545;&#20110;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#20648;&#30340;&#20107;&#23454;&#30340;&#20449;&#24687;&#26816;&#32034;&#23545;&#35805;&#65292;&#23545;&#35805;&#34920;&#36798;&#34987;&#36716;&#21270;&#20026;&#22270;&#26597;&#35810;&#30340;&#36807;&#31243;&#34987;&#31216;&#20026;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#38382;&#31572;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#23578;&#26410;&#26126;&#30830;&#22312;&#27492;&#20219;&#21153;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#22823;&#23567;&#21644;&#25552;&#31034;&#25216;&#26415;&#30340;&#27169;&#22411;&#65292;&#24182;&#35782;&#21035;&#20986;&#29983;&#25104;&#36755;&#20986;&#20013;&#24120;&#35265;&#30340;&#38382;&#39064;&#31867;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#23545;&#35805;&#20013;&#29983;&#25104;&#22270;&#26597;&#35810;&#65292;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#21644;&#24494;&#35843;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23637;&#31034;&#36739;&#20302;&#38646;&#26679;&#26412;&#24615;&#33021;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational question answering systems often rely on semantic parsing to enable interactive information retrieval, which involves the generation of structured database queries from a natural language input. For information-seeking conversations about facts stored within a knowledge graph, dialogue utterances are transformed into graph queries in a process that is called knowledge-based conversational question answering. This paper evaluates the performance of large language models that have not been explicitly pre-trained on this task. Through a series of experiments on an extensive benchmark dataset, we compare models of varying sizes with different prompting techniques and identify common issue types in the generated output. Our results demonstrate that large language models are capable of generating graph queries from dialogues, with significant improvements achievable through few-shot prompting and fine-tuning techniques, especially for smaller models that exhibit lower zero-sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;WordArt&#35774;&#35745;&#24072;API&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#22411;&#33539;&#22260;&#19978;&#36827;&#34892;&#29992;&#25143;&#39537;&#21160;&#30340;&#33402;&#26415;&#23383;&#20307;&#21512;&#25104;&#12290;&#36890;&#36807;&#25552;&#20379;&#21160;&#24577;&#12289;&#33258;&#36866;&#24212;&#21644;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28385;&#36275;&#38750;&#19987;&#19994;&#20154;&#22763;&#31616;&#21270;&#33402;&#26415;&#23383;&#20307;&#30340;&#38656;&#27714;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#30452;&#35266;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;&#19982;&#29616;&#26377;&#31995;&#32479;&#30456;&#27604;&#65292;&#35813;API&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#28385;&#24847;&#24230;&#12289;&#35774;&#35745;&#28789;&#27963;&#24615;&#21644;&#21019;&#36896;&#24615;&#34920;&#36798;&#65292;&#24182;&#20026;&#20010;&#24615;&#21270;&#25968;&#23383;&#36890;&#20449;&#21644;&#35774;&#35745;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01699</link><description>&lt;p&gt;
WordArt&#35774;&#35745;&#24072;API&#65306;&#21033;&#29992;&#27169;&#22411;&#33539;&#22260;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29992;&#25143;&#39537;&#21160;&#30340;&#33402;&#26415;&#23383;&#20307;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
WordArt Designer API: User-Driven Artistic Typography Synthesis with Large Language Models on ModelScope. (arXiv:2401.01699v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;WordArt&#35774;&#35745;&#24072;API&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27169;&#22411;&#33539;&#22260;&#19978;&#36827;&#34892;&#29992;&#25143;&#39537;&#21160;&#30340;&#33402;&#26415;&#23383;&#20307;&#21512;&#25104;&#12290;&#36890;&#36807;&#25552;&#20379;&#21160;&#24577;&#12289;&#33258;&#36866;&#24212;&#21644;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#28385;&#36275;&#38750;&#19987;&#19994;&#20154;&#22763;&#31616;&#21270;&#33402;&#26415;&#23383;&#20307;&#30340;&#38656;&#27714;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#30452;&#35266;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;&#19982;&#29616;&#26377;&#31995;&#32479;&#30456;&#27604;&#65292;&#35813;API&#26174;&#33879;&#25552;&#39640;&#20102;&#29992;&#25143;&#28385;&#24847;&#24230;&#12289;&#35774;&#35745;&#28789;&#27963;&#24615;&#21644;&#21019;&#36896;&#24615;&#34920;&#36798;&#65292;&#24182;&#20026;&#20010;&#24615;&#21270;&#25968;&#23383;&#36890;&#20449;&#21644;&#35774;&#35745;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;WordArt&#35774;&#35745;&#24072;API&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#27169;&#22411;&#33539;&#22260;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29992;&#25143;&#39537;&#21160;&#30340;&#33402;&#26415;&#23383;&#20307;&#21512;&#25104;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#21160;&#24577;&#12289;&#33258;&#36866;&#24212;&#21644;&#35745;&#31639;&#25928;&#26524;&#39640;&#30340;&#26367;&#20195;&#20256;&#32479;&#21018;&#24615;&#27169;&#26495;&#30340;&#26041;&#26696;&#65292;&#35299;&#20915;&#20102;&#38750;&#19987;&#19994;&#20154;&#22763;&#31616;&#21270;&#33402;&#26415;&#23383;&#20307;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#26469;&#29702;&#35299;&#21644;&#35299;&#37322;&#29992;&#25143;&#36755;&#20837;&#65292;&#20419;&#36827;&#26356;&#30452;&#35266;&#30340;&#35774;&#35745;&#36807;&#31243;&#12290;&#36890;&#36807;&#21508;&#31181;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29992;&#25143;&#22914;&#20309;&#34920;&#36798;&#20182;&#20204;&#30340;&#23457;&#32654;&#20559;&#22909;&#21644;&#21151;&#33021;&#38656;&#27714;&#65292;&#31995;&#32479;&#28982;&#21518;&#23558;&#20854;&#36716;&#21270;&#20026;&#29420;&#29305;&#19988;&#23500;&#26377;&#21019;&#24847;&#30340;&#23383;&#20307;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#31995;&#32479;&#30456;&#27604;&#65292;&#29992;&#25143;&#28385;&#24847;&#24230;&#12289;&#35774;&#35745;&#28789;&#27963;&#24615;&#21644;&#21019;&#36896;&#24615;&#34920;&#36798;&#37117;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;WordArt&#35774;&#35745;&#24072;API&#19981;&#20165;&#20351;&#23383;&#20307;&#33402;&#26415;&#27665;&#20027;&#21270;&#65292;&#36824;&#20026;&#20010;&#24615;&#21270;&#25968;&#23383;&#36890;&#20449;&#21644;&#35774;&#35745;&#25171;&#24320;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the WordArt Designer API, a novel framework for user-driven artistic typography synthesis utilizing Large Language Models (LLMs) on ModelScope. We address the challenge of simplifying artistic typography for non-professionals by offering a dynamic, adaptive, and computationally efficient alternative to traditional rigid templates. Our approach leverages the power of LLMs to understand and interpret user input, facilitating a more intuitive design process. We demonstrate through various case studies how users can articulate their aesthetic preferences and functional requirements, which the system then translates into unique and creative typographic designs. Our evaluations indicate significant improvements in user satisfaction, design flexibility, and creative expression over existing systems. The WordArt Designer API not only democratizes the art of typography but also opens up new possibilities for personalized digital communication and design.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#22270;&#34920;&#65292;&#25506;&#35752;&#20102;&#21512;&#35789;&#21644;&#38899;&#38901;&#36328;&#35821;&#35328;&#30456;&#20284;&#24615;&#30340;&#35821;&#35328;&#21407;&#22240;&#65292;&#24182;&#25903;&#25345;&#20102;&#19968;&#39033;&#20808;&#21069;&#24050;&#26377;&#30340;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2401.01698</link><description>&lt;p&gt;
&#19990;&#30028;&#35821;&#35328;&#30340;&#25345;&#20037;&#24615;&#21644;&#25193;&#25955;&#24615;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Patterns of Persistence and Diffusibility across World's Languages. (arXiv:2401.01698v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#22270;&#34920;&#65292;&#25506;&#35752;&#20102;&#21512;&#35789;&#21644;&#38899;&#38901;&#36328;&#35821;&#35328;&#30456;&#20284;&#24615;&#30340;&#35821;&#35328;&#21407;&#22240;&#65292;&#24182;&#25903;&#25345;&#20102;&#19968;&#39033;&#20808;&#21069;&#24050;&#26377;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#30340;&#30456;&#20284;&#24615;&#21487;&#33021;&#26159;&#30001;&#20110;&#36951;&#20256;&#20851;&#31995;&#12289;&#21306;&#22495;&#25509;&#35302;&#12289;&#26222;&#36941;&#24615;&#25110;&#20598;&#28982;&#24615;&#12290;&#21512;&#35789;&#26159;&#19968;&#31181;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#30456;&#20284;&#24615;&#31867;&#22411;&#65292;&#21363;&#20351;&#29992;&#19968;&#20010;&#35789;&#27719;&#24418;&#24335;&#26469;&#34920;&#36798;&#22810;&#20010;&#24847;&#20041;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#36890;&#36807;&#30740;&#31350;&#23478;&#26063;&#31283;&#23450;&#24615;&#65288;&#25345;&#20037;&#24615;&#65289;&#21644;&#25509;&#35302;&#35825;&#21457;&#21464;&#21270;&#65288;&#25193;&#25955;&#24615;&#65289;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21512;&#35789;&#21644;&#38899;&#38901;&#36328;&#35821;&#35328;&#30456;&#20284;&#24615;&#30340;&#35821;&#35328;&#21407;&#22240;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#28085;&#30422;1966&#31181;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#22270;&#65292;&#21253;&#25324;&#35821;&#20041;&#12289;&#23478;&#26063;&#12289;&#38899;&#38901;&#21644;&#22320;&#29702;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#35821;&#35328;&#23398;&#39046;&#22495;&#20197;&#21069;&#24037;&#20316;&#20013;&#30340;&#20960;&#20010;&#24050;&#24314;&#31435;&#30340;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#26032;&#30340;&#20551;&#35774;&#65292;&#23637;&#31034;&#20102;&#36825;&#19968;&#36164;&#28304;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#28872;&#25903;&#25345;&#35821;&#35328;&#23398;&#25991;&#29486;&#20013;&#19968;&#20010;&#20043;&#21069;&#24050;&#24314;&#31435;&#30340;&#20551;&#35774;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#21478;&#19968;&#20010;&#20551;&#35774;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#22823;&#35268;&#27169;&#36164;&#28304;&#20026;&#36328;&#23398;&#31185;&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#21487;&#33021;&#24615;&#65292;&#20363;&#22914;&#22810;&#35821;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#27604;&#36739;&#35821;&#35328;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language similarities can be caused by genetic relatedness, areal contact, universality, or chance. Colexification, i.e.~a type of similarity where a single lexical form is used to convey multiple meanings, is underexplored. In our work, we shed light on the linguistic causes of cross-lingual similarity in colexification and phonology, by exploring genealogical stability (persistence) and contact-induced change (diffusibility). We construct large-scale graphs incorporating semantic, genealogical, phonological and geographical data for 1,966 languages. We then show the potential of this resource, by investigating several established hypotheses from previous work in linguistics, while proposing new ones. Our results strongly support a previously established hypothesis in the linguistic literature, while offering contradicting evidence to another. Our large scale resource opens for further research across disciplines, e.g.~in multilingual NLP and comparative linguistics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19977;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65288;&#19987;&#23478;&#35268;&#21017;&#27169;&#22411;&#12289;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#22312;&#20174;&#23398;&#29983;&#23545;&#35805;&#20013;&#26816;&#27979;&#21644;&#30830;&#23450;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#36739;&#22909;&#65292;&#32780;&#19987;&#23478;&#35268;&#21017;&#27169;&#22411;&#25928;&#26524;&#36739;&#24046;&#12290;&#30740;&#31350;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24191;&#27867;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.01692</link><description>&lt;p&gt;
&#20174;&#23398;&#29983;&#30340;&#23545;&#35805;&#20013;&#39044;&#27979;&#25361;&#25112;&#26102;&#21051;&#65306;GPT-4&#19982;&#20004;&#31181;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Predicting challenge moments from students' discourse: A comparison of GPT-4 to two traditional natural language processing approaches. (arXiv:2401.01692v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#19977;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65288;&#19987;&#23478;&#35268;&#21017;&#27169;&#22411;&#12289;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#22312;&#20174;&#23398;&#29983;&#23545;&#35805;&#20013;&#26816;&#27979;&#21644;&#30830;&#23450;&#25361;&#25112;&#30340;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#36739;&#22909;&#65292;&#32780;&#19987;&#23478;&#35268;&#21017;&#27169;&#22411;&#25928;&#26524;&#36739;&#24046;&#12290;&#30740;&#31350;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24191;&#27867;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#21327;&#20316;&#38656;&#35201;&#22242;&#38431;&#22312;&#38754;&#23545;&#25361;&#25112;&#26102;&#36827;&#34892;&#25112;&#30053;&#24615;&#30340;&#33258;&#25105;&#35843;&#25511;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22242;&#38431;&#21487;&#33021;&#22240;&#25104;&#21592;&#23545;&#25361;&#25112;&#30340;&#35748;&#30693;&#24046;&#24322;&#32780;&#26080;&#27861;&#36827;&#34892;&#35843;&#25511;&#65292;&#32780;&#36825;&#31181;&#24773;&#20917;&#21487;&#33021;&#21463;&#30410;&#20110;&#22806;&#37096;&#25903;&#25345;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#65306;&#19987;&#23478;&#30693;&#35782;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#12289;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22312;&#20174;&#23398;&#29983;&#23545;&#35805;&#20013;&#26816;&#27979;&#25361;&#25112;&#21644;&#30830;&#23450;&#25361;&#25112;&#32500;&#24230;&#65288;&#35748;&#30693;&#12289;&#20803;&#35748;&#30693;&#12289;&#24773;&#24863;&#21644;&#25216;&#26415;/&#20854;&#20182;&#25361;&#25112;&#65289;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#24471;&#30456;&#24403;&#22909;&#65292;&#32780;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#25928;&#26524;&#36739;&#24046;&#65292;&#20854;&#21151;&#25928;&#20005;&#37325;&#20381;&#36182;&#20110;&#19987;&#23478;&#35774;&#35745;&#30340;&#29305;&#24449;&#12290;&#26412;&#25991;&#23545;&#36825;&#19977;&#31181;&#26041;&#27861;&#22312;&#33258;&#21160;&#26816;&#27979;&#21644;&#25903;&#25345;&#23398;&#29983;&#25361;&#25112;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24191;&#27867;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective collaboration requires groups to strategically regulate themselves to overcome challenges. Research has shown that groups may fail to regulate due to differences in members' perceptions of challenges which may benefit from external support. In this study, we investigated the potential of leveraging three distinct natural language processing models: an expert knowledge rule-based model, a supervised machine learning (ML) model and a Large Language model (LLM), in challenge detection and challenge dimension identification (cognitive, metacognitive, emotional and technical/other challenges) from student discourse, was investigated. The results show that the supervised ML and the LLM approaches performed considerably well in both tasks, in contrast to the rule-based approach, whose efficacy heavily relies on the engineered features by experts. The paper provides an extensive discussion of the three approaches' performance for automated detection and support of students' challenge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#31350;&#20102;Multilayer-Perceptrons (MLPs)&#27169;&#22359;&#26159;&#21542;&#33021;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23545;&#35821;&#35328;&#20449;&#24687;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MLPs&#30830;&#23454;&#33021;&#22815;&#25552;&#39640;PLMs&#23545;&#35821;&#35328;&#32467;&#26500;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01667</link><description>&lt;p&gt;
MLPs Compass: MLP&#19982;PLM&#30456;&#32467;&#21512;&#26102;&#23398;&#21040;&#20102;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
MLPs Compass: What is learned when MLPs are combined with PLMs?. (arXiv:2401.01667v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#31350;&#20102;Multilayer-Perceptrons (MLPs)&#27169;&#22359;&#26159;&#21542;&#33021;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23545;&#35821;&#35328;&#20449;&#24687;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MLPs&#30830;&#23454;&#33021;&#22815;&#25552;&#39640;PLMs&#23545;&#35821;&#35328;&#32467;&#26500;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#21464;&#20307;&#20855;&#26377;&#24456;&#24378;&#30340;&#35821;&#20041;&#34920;&#31034;&#33021;&#21147;&#65292;&#20294;&#20851;&#20110;PLM&#30340;&#39069;&#22806;&#32452;&#20214;&#25152;&#24102;&#26469;&#30340;&#20449;&#24687;&#22686;&#30410;&#30340;&#29702;&#35299;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#24320;&#30340;&#38382;&#39064;&#12290;&#21463;&#21040;&#26368;&#36817;&#30340;&#24037;&#20316;&#30340;&#28608;&#21169;&#65292;&#20854;&#35777;&#26126;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22359;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#32467;&#26500;&#25429;&#25417;&#33021;&#21147;&#65292;&#29978;&#33267; &#36229;&#36807;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#26412;&#25991;&#26088;&#22312;&#37327;&#21270;&#31616;&#21333;&#30340;MLP&#26159;&#21542;&#33021;&#36827;&#19968;&#27493;&#22686;&#24378;PLM&#25429;&#25417;&#35821;&#35328;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25506;&#27979;&#26694;&#26550;&#65292;&#21253;&#21547;&#20102;&#22522;&#20110;BERT&#32467;&#26500;&#30340;MLP&#32452;&#20214;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#19977;&#20010;&#19981;&#21516;&#35821;&#35328;&#23618;&#27425;&#30340;10&#20010;&#25506;&#27979;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MLP&#30830;&#23454;&#21487;&#20197;&#36890;&#36807;PLM&#25552;&#39640;&#23545;&#35821;&#35328;&#32467;&#26500;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#35774;&#35745;&#21508;&#31181;&#21464;&#20307;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#19988;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Transformer-based pre-trained language models and their variants exhibit strong semantic representation capabilities, the question of comprehending the information gain derived from the additional components of PLMs remains an open question in this field. Motivated by recent efforts that prove Multilayer-Perceptrons (MLPs) modules achieving robust structural capture capabilities, even outperforming Graph Neural Networks (GNNs), this paper aims to quantify whether simple MLPs can further enhance the already potent ability of PLMs to capture linguistic information. Specifically, we design a simple yet effective probing framework containing MLPs components based on BERT structure and conduct extensive experiments encompassing 10 probing tasks spanning three distinct linguistic levels. The experimental results demonstrate that MLPs can indeed enhance the comprehension of linguistic structure by PLMs. Our research provides interpretable and valuable insights into crafting variations o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27969;&#31243;&#35299;&#20915;&#26041;&#26696;&#26469;&#24110;&#21161;&#21697;&#29260;&#29983;&#25104;&#19982;&#21697;&#29260;&#20010;&#24615;&#30456;&#19968;&#33268;&#30340;&#31038;&#20132;&#23186;&#20307;&#24191;&#21578;&#26631;&#39064;&#65292;&#20197;&#21560;&#24341;&#28040;&#36153;&#32773;&#30340;&#27880;&#24847;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01637</link><description>&lt;p&gt;
&#20026;&#21697;&#29260;&#35774;&#35745;&#31038;&#20132;&#23186;&#20307;&#24191;&#21578;&#26631;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Social Media Ready Caption Generation for Brands. (arXiv:2401.01637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01637
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27969;&#31243;&#35299;&#20915;&#26041;&#26696;&#26469;&#24110;&#21161;&#21697;&#29260;&#29983;&#25104;&#19982;&#21697;&#29260;&#20010;&#24615;&#30456;&#19968;&#33268;&#30340;&#31038;&#20132;&#23186;&#20307;&#24191;&#21578;&#26631;&#39064;&#65292;&#20197;&#21560;&#24341;&#28040;&#36153;&#32773;&#30340;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24191;&#21578;&#23545;&#20110;&#21697;&#29260;&#33829;&#38144;&#33267;&#20851;&#37325;&#35201;&#65292;&#33268;&#21147;&#20110;&#21560;&#24341;&#28040;&#36153;&#32773;&#30340;&#27880;&#24847;&#21147;&#65292;&#36890;&#36807;&#21560;&#24341;&#20154;&#30340;&#26631;&#39064;&#12289;&#22270;&#29255;&#20197;&#21450;&#26631;&#24535;&#26469;&#23454;&#29616;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#20026;&#19968;&#33324;&#22270;&#20687;&#29983;&#25104;&#26631;&#39064;&#65292;&#20294;&#23558;&#21697;&#29260;&#20010;&#24615;&#34701;&#20837;&#31038;&#20132;&#23186;&#20307;&#26631;&#39064;&#20173;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#21697;&#29260;&#20010;&#24615;&#24050;&#34987;&#35777;&#26126;&#23545;&#28040;&#36153;&#32773;&#34892;&#20026;&#21644;&#31038;&#20132;&#20114;&#21160;&#20135;&#29983;&#24433;&#21709;&#65292;&#22240;&#27492;&#26159;&#33829;&#38144;&#31574;&#30053;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#29616;&#26377;&#30340;&#24320;&#28304;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#24182;&#19981;&#36866;&#29992;&#20110;&#27492;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#31243;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#24110;&#21161;&#21697;&#29260;&#21019;&#24314;&#19982;&#22270;&#20687;&#21644;&#21697;&#29260;&#20010;&#24615;&#30456;&#19968;&#33268;&#30340;&#21560;&#24341;&#20154;&#31038;&#20132;&#23186;&#20307;&#26631;&#39064;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#21253;&#21547;&#20004;&#20010;&#37096;&#20998;&#65306;&#31532;&#19968;&#37096;&#20998;&#21253;&#21547;&#19968;&#20010;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#65292;&#23427;&#25509;&#25910;&#21697;&#29260;&#35201;&#22312;&#32593;&#19978;&#21457;&#24067;&#30340;&#22270;&#20687;&#24182;&#29983;&#25104;&#19968;&#20010;&#31616;&#27905;&#30340;&#33521;&#25991;&#26631;&#39064;&#65307;&#31532;&#20108;&#37096;&#20998;&#25509;&#25910;&#29983;&#25104;&#30340;&#26631;&#39064;&#20197;&#21450;&#30446;&#26631;&#21697;&#29260;&#20010;&#24615;&#65292;&#36755;&#20986;&#19982;&#21697;&#29260;&#20010;&#24615;&#30456;&#19968;&#33268;&#19988;&#24341;&#20154;&#27880;&#30446;&#30340;&#31038;&#20132;&#23186;&#20307;&#26631;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media advertisements are key for brand marketing, aiming to attract consumers with captivating captions and pictures or logos. While previous research has focused on generating captions for general images, incorporating brand personalities into social media captioning remains unexplored. Brand personalities are shown to be affecting consumers' behaviours and social interactions and thus are proven to be a key aspect of marketing strategies. Current open-source multimodal LLMs are not directly suited for this task. Hence, we propose a pipeline solution to assist brands in creating engaging social media captions that align with the image and the brand personalities. Our architecture is based on two parts: a the first part contains an image captioning model that takes in an image that the brand wants to post online and gives a plain English caption; b the second part takes in the generated caption along with the target brand personality and outputs a catchy personality-aligned soci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#12304;&#30456;&#23545;&#21019;&#36896;&#21147;&#12305;&#65292;&#36890;&#36807;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#21644;&#30452;&#25509;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2401.01623</link><description>&lt;p&gt;
AI&#26159;&#21542;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#20855;&#22791;&#21019;&#36896;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI Be as Creative as Humans?. (arXiv:2401.01623v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#12304;&#30456;&#23545;&#21019;&#36896;&#21147;&#12305;&#65292;&#36890;&#36807;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#21644;&#30452;&#25509;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#36896;&#21147;&#26159;&#31038;&#20250;&#36827;&#27493;&#21644;&#21019;&#26032;&#30340;&#22522;&#30707;&#65292;&#20294;&#20854;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#20027;&#35266;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#20808;&#36827;&#30340;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#33021;&#22815;&#23436;&#25104;&#26366;&#32463;&#21482;&#23646;&#20110;&#20154;&#31867;&#21019;&#36896;&#21147;&#30340;&#20219;&#21153;&#65292;&#25506;&#32034;AI&#30340;&#21019;&#36896;&#28508;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#20854;&#36127;&#36131;&#20219;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#8220;&#30456;&#23545;&#21019;&#36896;&#21147;&#8221;&#30340;&#26032;&#27010;&#24565;&#26469;&#35299;&#20915;&#23450;&#20041;&#21644;&#35780;&#20272;&#21019;&#36896;&#21147;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#19981;&#20877;&#35797;&#22270;&#23545;&#21019;&#36896;&#21147;&#36827;&#34892;&#26222;&#36941;&#23450;&#20041;&#65292;&#32780;&#26159;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#19968;&#20301;&#20551;&#35774;&#30340;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#12290;&#36825;&#31181;&#35266;&#28857;&#20511;&#37492;&#20102;&#22270;&#28789;&#27979;&#35797;&#30340;&#24605;&#24819;&#65292;&#24182;&#25193;&#23637;&#20854;&#33539;&#22260;&#20197;&#35299;&#20915;&#35780;&#20272;&#21019;&#36896;&#21147;&#20013;&#25152;&#22266;&#26377;&#30340;&#25361;&#25112;&#21644;&#20027;&#35266;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#36716;&#21464;&#20351;&#24471;&#23545;AI&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#32479;&#35745;&#21019;&#36896;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#27604;&#36739;AI&#19982;&#29305;&#23450;&#20154;&#31867;&#30340;&#21019;&#36896;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creativity serves as a cornerstone for societal progress and innovation, but its assessment remains a complex and often subjective endeavor. With the rise of advanced generative AI models capable of tasks once reserved for human creativity, the study of AI's creative potential becomes imperative for its responsible development and application. This paper addresses the complexities in defining and evaluating creativity by introducing a new concept called Relative Creativity. Instead of trying to define creativity universally, we shift the focus to whether AI can match the creative abilities of a hypothetical human. This perspective draws inspiration from the Turing Test, expanding upon it to address the challenges and subjectivities inherent in evaluating creativity. This methodological shift facilitates a statistically quantifiable evaluation of AI's creativity, which we term Statistical Creativity. This approach allows for direct comparisons of AI's creative abilities with those of sp
&lt;/p&gt;</description></item><item><title>&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#36890;&#29992;&#39046;&#22495;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26415;&#20013;&#39118;&#38505;&#39044;&#27979;&#21644;&#39044;&#21518;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#25163;&#26415;&#25551;&#36848;&#21644;&#24739;&#32773;&#20020;&#24202;&#35760;&#24405;&#65292;&#25105;&#20204;&#22312;8&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#32771;&#23519;&#20102;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#23569;&#37327;&#26679;&#26412;&#21644;&#38142;&#24335;&#21551;&#21457;&#24335;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#24403;&#21069;&#19968;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#22312;&#26415;&#20013;&#39118;&#38505;&#20998;&#23618;&#21644;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#24635;&#32467;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.01620</link><description>&lt;p&gt;
&#26415;&#20013;&#39118;&#38505;&#39044;&#27979;&#21644;&#39044;&#21518;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Capabilities in Perioperative Risk Prediction and Prognostication. (arXiv:2401.01620v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01620
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#36890;&#29992;&#39046;&#22495;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26415;&#20013;&#39118;&#38505;&#39044;&#27979;&#21644;&#39044;&#21518;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#25163;&#26415;&#25551;&#36848;&#21644;&#24739;&#32773;&#20020;&#24202;&#35760;&#24405;&#65292;&#25105;&#20204;&#22312;8&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#32771;&#23519;&#20102;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#23569;&#37327;&#26679;&#26412;&#21644;&#38142;&#24335;&#21551;&#21457;&#24335;&#31574;&#30053;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#24403;&#21069;&#19968;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#22312;&#26415;&#20013;&#39118;&#38505;&#20998;&#23618;&#21644;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#24635;&#32467;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#29992;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-4 Turbo&#65292;&#33021;&#21542;&#20351;&#29992;&#25163;&#26415;&#30340;&#25551;&#36848;&#21644;&#26469;&#33258;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#24739;&#32773;&#20020;&#24202;&#35760;&#24405;&#65292;&#36827;&#34892;&#39118;&#38505;&#20998;&#23618;&#21644;&#39044;&#27979;&#26415;&#21518;&#32467;&#26524;&#25351;&#26631;&#12290;&#25105;&#20204;&#23545;8&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#39044;&#27979;&#24615;&#33021;&#36827;&#34892;&#20102;&#32771;&#23519;&#65306;ASA&#29983;&#29702;&#29366;&#24577;&#20998;&#31867;&#30340;&#39044;&#27979;&#12289;&#20303;&#38498;&#12289;&#37325;&#30151;&#30417;&#25252;&#23460;&#20837;&#38498;&#12289;&#38750;&#35745;&#21010;&#20837;&#38498;&#12289;&#20303;&#38498;&#27515;&#20129;&#12289;PACU&#31532;&#19968;&#38454;&#27573;&#25345;&#32493;&#26102;&#38388;&#12289;&#20303;&#38498;&#26102;&#38388;&#21644;&#37325;&#30151;&#30417;&#25252;&#23460;&#26102;&#38388;&#30340;&#39044;&#27979;&#12290;&#23569;&#37327;&#26679;&#26412;&#21644;&#38142;&#24335;&#21551;&#21457;&#24335;&#31574;&#30053;&#25913;&#21892;&#20102;&#20960;&#20010;&#20219;&#21153;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;ASA&#29983;&#29702;&#29366;&#24577;&#20998;&#31867;&#39044;&#27979;&#19978;&#36798;&#21040;&#20102;0.50&#30340;F1&#24471;&#20998;&#65292;&#22312;&#37325;&#30151;&#30417;&#25252;&#23460;&#20837;&#38498;&#39044;&#27979;&#19978;&#36798;&#21040;&#20102;0.81&#30340;F1&#24471;&#20998;&#65292;&#22312;&#20303;&#38498;&#27515;&#20129;&#39044;&#27979;&#19978;&#36798;&#21040;&#20102;0.86&#30340;F1&#24471;&#20998;&#12290;&#25152;&#26377;&#25552;&#31034;&#31574;&#30053;&#22312;&#25345;&#32493;&#26102;&#38388;&#39044;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#26222;&#36941;&#36739;&#24046;&#12290;&#24403;&#21069;&#19968;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#36827;&#34892;&#26415;&#20013;&#39118;&#38505;&#20998;&#23618;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate whether general-domain large language models such as GPT-4 Turbo can perform risk stratification and predict post-operative outcome measures using a description of the procedure and a patient's clinical notes derived from the electronic health record. We examine predictive performance on 8 different tasks: prediction of ASA Physical Status Classification, hospital admission, ICU admission, unplanned admission, hospital mortality, PACU Phase 1 duration, hospital duration, and ICU duration. Few-shot and chain-of-thought prompting improves predictive performance for several of the tasks. We achieve F1 scores of 0.50 for ASA Physical Status Classification, 0.81 for ICU admission, and 0.86 for hospital mortality. Performance on duration prediction tasks were universally poor across all prompt strategies. Current generation large language models can assist clinicians in perioperative risk stratification on classification tasks and produce high-quality natural language summarie
&lt;/p&gt;</description></item><item><title>GPT-4V(ision)&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32593;&#32476;&#20195;&#29702;&#65292;&#20855;&#26377;&#32508;&#21512;&#35270;&#35273;&#29702;&#35299;&#21644;&#32593;&#39029;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22914;&#26524;&#23558;&#25991;&#26412;&#35745;&#21010;&#36716;&#21270;&#20026;&#23454;&#38469;&#34892;&#21160;&#65292;GPT-4V&#21487;&#20197;&#22312;50%&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;&#36825;&#19968;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01614</link><description>&lt;p&gt;
GPT-4V(ision)&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32593;&#32476;&#20195;&#29702;&#65292;&#22914;&#26524;&#26377;&#22522;&#30784;&#30340;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-4V(ision) is a Generalist Web Agent, if Grounded. (arXiv:2401.01614v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01614
&lt;/p&gt;
&lt;p&gt;
GPT-4V(ision)&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32593;&#32476;&#20195;&#29702;&#65292;&#20855;&#26377;&#32508;&#21512;&#35270;&#35273;&#29702;&#35299;&#21644;&#32593;&#39029;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22914;&#26524;&#23558;&#25991;&#26412;&#35745;&#21010;&#36716;&#21270;&#20026;&#23454;&#38469;&#34892;&#21160;&#65292;GPT-4V&#21487;&#20197;&#22312;50%&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;&#36825;&#19968;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;GPT-4V(ision)&#21644;Gemini&#65292;&#24555;&#36895;&#25512;&#21160;&#20102;&#22810;&#27169;&#22411;&#30340;&#33021;&#21147;&#36793;&#30028;&#36229;&#36234;&#20256;&#32479;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20687;GPT-4V&#36825;&#26679;&#30340;LMM&#20316;&#20026;&#36890;&#29992;&#32593;&#32476;&#20195;&#29702;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SEEACT&#65292;&#19968;&#31181;&#21033;&#29992;LMM&#30340;&#21147;&#37327;&#36827;&#34892;&#32508;&#21512;&#35270;&#35273;&#29702;&#35299;&#21644;&#32593;&#39029;&#25805;&#20316;&#30340;&#36890;&#29992;&#32593;&#32476;&#20195;&#29702;&#12290;&#25105;&#20204;&#22312;&#26368;&#26032;&#30340;MIND2WEB&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#38500;&#20102;&#23545;&#32531;&#23384;&#32593;&#31449;&#30340;&#26631;&#20934;&#31163;&#32447;&#35780;&#20272;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20801;&#35768;&#22312;&#23454;&#26102;&#32593;&#31449;&#19978;&#36816;&#34892;&#32593;&#32476;&#20195;&#29702;&#30340;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#35780;&#20272;&#35774;&#32622;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GPT-4V&#22312;&#32593;&#39029;&#20195;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;-&#22914;&#26524;&#25105;&#20204;&#23558;&#20854;&#25991;&#26412;&#35745;&#21010;&#25163;&#21160;&#22320;&#23454;&#26045;&#20026;&#32593;&#31449;&#19978;&#30340;&#34892;&#21160;&#65292;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#23436;&#25104;50%&#30340;&#20219;&#21153;&#12290;&#27492;&#32467;&#26524;&#26126;&#26174;&#36229;&#36807;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering. In this work, we explore the potential of LMMs like GPT-4V as a generalist web agent that can follow natural language instructions to complete tasks on any given website. We propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual understanding and acting on the web. We evaluate on the recent MIND2WEB benchmark. In addition to standard offline evaluation on cached websites, we enable a new online evaluation setting by developing a tool that allows running web agents on live websites. We show that GPT-4V presents a great potential for web agents - it can successfully complete 50% of the tasks on live websites if we manually ground its textual plans into actions on the websites. This substantially outperforms
&lt;/p&gt;</description></item><item><title>PLLaMa&#26159;&#19968;&#31181;&#29992;&#20110;&#26893;&#29289;&#31185;&#23398;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32508;&#21512;&#25968;&#25454;&#24211;&#22686;&#24378;&#65292;&#26174;&#33879;&#20016;&#23500;&#20102;&#20854;&#22312;&#26893;&#29289;&#21644;&#20892;&#19994;&#31185;&#23398;&#26041;&#38754;&#30340;&#30693;&#35782;&#21644;&#19987;&#38271;&#65292;&#24182;&#36890;&#36807;&#19982;&#19987;&#19994;&#20154;&#21592;&#23567;&#32452;&#30340;&#21512;&#20316;&#39564;&#35777;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01600</link><description>&lt;p&gt;
PLLaMa&#65306;&#19968;&#31181;&#29992;&#20110;&#26893;&#29289;&#31185;&#23398;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PLLaMa: An Open-source Large Language Model for Plant Science. (arXiv:2401.01600v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01600
&lt;/p&gt;
&lt;p&gt;
PLLaMa&#26159;&#19968;&#31181;&#29992;&#20110;&#26893;&#29289;&#31185;&#23398;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32508;&#21512;&#25968;&#25454;&#24211;&#22686;&#24378;&#65292;&#26174;&#33879;&#20016;&#23500;&#20102;&#20854;&#22312;&#26893;&#29289;&#21644;&#20892;&#19994;&#31185;&#23398;&#26041;&#38754;&#30340;&#30693;&#35782;&#21644;&#19987;&#38271;&#65292;&#24182;&#36890;&#36807;&#19982;&#19987;&#19994;&#20154;&#21592;&#23567;&#32452;&#30340;&#21512;&#20316;&#39564;&#35777;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#21644;&#19982;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#20132;&#20114;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26893;&#29289;&#31185;&#23398;&#31561;&#38656;&#35201;&#39640;&#20934;&#30830;&#24615;&#30340;&#19987;&#19994;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#32570;&#20047;&#30456;&#20851;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23427;&#20204;&#30340;&#25928;&#26524;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PLLaMa&#65292;&#19968;&#31181;&#20174;LLaMa-2&#36827;&#21270;&#32780;&#26469;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#21253;&#25324;&#36229;&#36807;150&#19975;&#31687;&#26893;&#29289;&#31185;&#23398;&#23398;&#26415;&#25991;&#31456;&#30340;&#32508;&#21512;&#25968;&#25454;&#24211;&#36827;&#34892;&#22686;&#24378;&#12290;&#36825;&#19968;&#21457;&#23637;&#26174;&#33879;&#20016;&#23500;&#20102;PLLaMa&#22312;&#26893;&#29289;&#21644;&#20892;&#19994;&#31185;&#23398;&#26041;&#38754;&#30340;&#30693;&#35782;&#21644;&#19987;&#38271;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#27979;&#35797;&#20013;&#65292;&#28041;&#21450;&#19982;&#26893;&#29289;&#21644;&#20892;&#19994;&#30456;&#20851;&#30340;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#26174;&#31034;PLLaMa&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#26893;&#29289;&#31185;&#23398;&#30456;&#20851;&#20027;&#39064;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32452;&#24314;&#20102;&#19968;&#20010;&#22269;&#38469;&#19987;&#19994;&#20154;&#21592;&#23567;&#32452;&#65292;&#21253;&#25324;&#26893;&#29289;&#31185;&#23398;&#23478;&#12289;&#20892;&#19994;&#24037;&#31243;&#24072;&#21644;&#26893;&#29289;&#32946;&#31181;&#21592;&#12290;&#35813;&#22242;&#38431;&#22312;&#39564;&#35777;PLLaMa&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have exhibited remarkable capabilities in understanding and interacting with natural language across various sectors. However, their effectiveness is limited in specialized areas requiring high accuracy, such as plant science, due to a lack of specific expertise in these fields. This paper introduces PLLaMa, an open-source language model that evolved from LLaMa-2. It's enhanced with a comprehensive database, comprising more than 1.5 million scholarly articles in plant science. This development significantly enriches PLLaMa with extensive knowledge and proficiency in plant and agricultural sciences. Our initial tests, involving specific datasets related to plants and agriculture, show that PLLaMa substantially improves its understanding of plant science-related topics. Moreover, we have formed an international panel of professionals, including plant scientists, agricultural engineers, and plant breeders. This team plays a crucial role in verifying the accura
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#29992;&#20110;&#24635;&#32467;&#28151;&#21512;&#30721;Hindi-English&#20020;&#24202;&#26597;&#35810;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#32447;&#32034;&#65292;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#24739;&#32773;&#21307;&#30103;&#29366;&#20917;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MedSumm&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#21644;VLMs&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.01596</link><description>&lt;p&gt;
MedSumm&#65306;&#19968;&#31181;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#24635;&#32467;&#28151;&#21512;&#30721;Hindi-English&#20020;&#24202;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries. (arXiv:2401.01596v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#29992;&#20110;&#24635;&#32467;&#28151;&#21512;&#30721;Hindi-English&#20020;&#24202;&#26597;&#35810;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#32447;&#32034;&#65292;&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#24739;&#32773;&#21307;&#30103;&#29366;&#20917;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MedSumm&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#21644;VLMs&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#65292;&#24635;&#32467;&#24739;&#32773;&#25552;&#20986;&#30340;&#21307;&#30103;&#38382;&#39064;&#23545;&#20110;&#25913;&#21892;&#21307;&#24739;&#20114;&#21160;&#21644;&#21307;&#30103;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#21307;&#30103;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#25968;&#37327;&#19981;&#26029;&#22686;&#38271;&#65292;&#20294;&#30446;&#21069;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#19978;&#65292;&#24573;&#35270;&#20102;&#35270;&#35273;&#32447;&#32034;&#30340;&#25972;&#21512;&#12290;&#27492;&#22806;&#65292;&#22312;&#21307;&#23398;&#38382;&#39064;&#24635;&#32467;&#30340;&#39046;&#22495;&#20013;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#20165;&#38480;&#20110;&#33521;&#35821;&#35821;&#35328;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#38024;&#23545;&#28151;&#21512;&#30721;&#36755;&#20837;&#36827;&#34892;&#22810;&#27169;&#24577;&#21307;&#23398;&#38382;&#39064;&#24635;&#32467;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Multimodal Medical Codemixed Question Summarization&#65288;MMCQS&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#32467;&#21512;&#20102;Hindi-English&#28151;&#21512;&#30721;&#21307;&#23398;&#26597;&#35810;&#21644;&#35270;&#35273;&#36741;&#21161;&#24037;&#20855;&#12290;&#36825;&#31181;&#25972;&#21512;&#20016;&#23500;&#20102;&#24739;&#32773;&#30340;&#21307;&#30103;&#29366;&#20917;&#30340;&#34920;&#31034;&#65292;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MedSumm&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;LLMs&#21644;VLMs&#30340;&#33021;&#21147;&#26469;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the healthcare domain, summarizing medical questions posed by patients is critical for improving doctor-patient interactions and medical decision-making. Although medical data has grown in complexity and quantity, the current body of research in this domain has primarily concentrated on text-based methods, overlooking the integration of visual cues. Also prior works in the area of medical question summarisation have been limited to the English language. This work introduces the task of multimodal medical question summarization for codemixed input in a low-resource setting. To address this gap, we introduce the Multimodal Medical Codemixed Question Summarization MMCQS dataset, which combines Hindi-English codemixed medical queries with visual aids. This integration enriches the representation of a patient's medical condition, providing a more comprehensive perspective. We also propose a framework named MedSumm that leverages the power of LLMs and VLMs for this task. By utilizing our 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24187;&#21548;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#27979;&#35797;&#26102;&#35780;&#20272;&#24187;&#21548;&#25935;&#24863;&#24615;&#30340;&#22522;&#20110;&#25200;&#21160;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;&#21306;&#20998;&#24187;&#35273;&#21644;&#38750;&#24187;&#35273;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.01572</link><description>&lt;p&gt;
&#31070;&#32463;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24187;&#21548;&#65306;&#35782;&#21035;&#38169;&#35823;&#21644;&#24187;&#35273;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hallucinations in Neural Automatic Speech Recognition: Identifying Errors and Hallucinatory Models. (arXiv:2401.01572v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01572
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#24187;&#21548;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#27979;&#35797;&#26102;&#35780;&#20272;&#24187;&#21548;&#25935;&#24863;&#24615;&#30340;&#22522;&#20110;&#25200;&#21160;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;&#21306;&#20998;&#24187;&#35273;&#21644;&#38750;&#24187;&#35273;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24187;&#21548;&#26159;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20135;&#29983;&#30340;&#19968;&#31181;&#36755;&#20986;&#38169;&#35823;&#12290;&#34429;&#28982;&#36825;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24050;&#32463;&#34987;&#30740;&#31350;&#36807;&#65292;&#20294;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#20197;&#21069;&#36824;&#27809;&#26377;&#36827;&#34892;&#36807;&#30740;&#31350;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;ASR&#20013;&#30340;&#24187;&#21548;&#23450;&#20041;&#20026;&#27169;&#22411;&#29983;&#25104;&#30340;&#36716;&#24405;&#19982;&#28304;&#35805;&#35821;&#19981;&#30456;&#20851;&#65292;&#20294;&#20173;&#27969;&#30021;&#21644;&#36830;&#36143;&#30340;&#35821;&#20041;&#12290;&#24187;&#21548;&#19982;&#27169;&#22411;&#21487;&#33021;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20986;&#30340;&#30456;&#20284;&#24615;&#65292;&#20250;&#20135;&#29983;&#27450;&#39575;&#30340;&#21361;&#38505;&#65292;&#24433;&#21709;&#31995;&#32479;&#30340;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24120;&#29992;&#30340;&#25351;&#26631;&#65292;&#22914;&#35789;&#38169;&#35823;&#29575;&#65292;&#26080;&#27861;&#21306;&#20998;&#24187;&#35273;&#21644;&#38750;&#24187;&#35273;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25200;&#21160;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27979;&#35797;&#26102;&#35780;&#20272;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#23545;&#24187;&#21548;&#30340;&#25935;&#24863;&#24615;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#26377;&#21161;&#20110;&#21306;&#20998;&#24187;&#35273;&#21644;&#38750;&#24187;&#35273;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hallucinations are a type of output error produced by deep neural networks. While this has been studied in natural language processing, they have not been researched previously in automatic speech recognition. Here, we define hallucinations in ASR as transcriptions generated by a model that are semantically unrelated to the source utterance, yet still fluent and coherent. The similarity of hallucinations to probable natural language outputs of the model creates a danger of deception and impacts the credibility of the system. We show that commonly used metrics, such as word error rates, cannot differentiate between hallucinatory and non-hallucinatory models. To address this, we propose a perturbation-based method for assessing the susceptibility of an automatic speech recognition (ASR) model to hallucination at test time, which does not require access to the training dataset. We demonstrate that this method helps to distinguish between hallucinatory and non-hallucinatory models that hav
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#35780;&#20272;&#21508;&#31181;LMMs&#22312;&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01523</link><description>&lt;p&gt;
GOAT-Bench: &#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse. (arXiv:2401.01523v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01523
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#36855;&#22240;&#30340;&#31038;&#20132;&#34384;&#24453;&#30740;&#31350;&#23545;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#23433;&#20840;&#27934;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#35780;&#20272;&#21508;&#31181;LMMs&#22312;&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#28145;&#21051;&#25913;&#21464;&#20102;&#20449;&#24687;&#30340;&#21019;&#36896;&#12289;&#20256;&#25773;&#21644;&#21560;&#25910;&#26041;&#24335;&#65292;&#22312;&#25968;&#23383;&#26102;&#20195;&#20135;&#29983;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24433;&#21709;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#36825;&#20010;&#29190;&#28856;&#20063;&#23548;&#33268;&#20102;&#32593;&#32476;&#36855;&#22240;&#30340;&#28389;&#29992;&#25968;&#37327;&#26174;&#33879;&#22686;&#21152;&#12290;&#35780;&#20272;&#36855;&#22240;&#30340;&#36127;&#38754;&#24433;&#21709;&#26159;&#30456;&#24403;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#36890;&#24120;&#20855;&#26377;&#24494;&#22937;&#21644;&#38544;&#26214;&#30340;&#21547;&#20041;&#65292;&#36825;&#20123;&#21547;&#20041;&#19981;&#33021;&#30452;&#25509;&#36890;&#36807;&#26174;&#24615;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#20256;&#36798;&#20986;&#26469;&#12290;&#37492;&#20110;&#27492;&#65292;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#20316;&#20026;&#22788;&#29702;&#22810;&#26679;&#21270;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21331;&#36234;&#33021;&#21147;&#30340;&#28966;&#28857;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20852;&#36259;&#12290;&#38024;&#23545;&#36825;&#19968;&#21457;&#23637;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#28145;&#20837;&#30740;&#31350;&#21508;&#31181;LMMs(&#22914;GPT-4V)&#35782;&#21035;&#21644;&#22238;&#24212;&#36855;&#22240;&#20013;&#20307;&#29616;&#30340;&#24494;&#22937;&#31038;&#20132;&#34384;&#24453;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32508;&#21512;&#30340;&#36855;&#22240;&#22522;&#20934;&#27979;&#35797;&#38598;GOAT-Bench&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;6K&#20010;&#22810;&#26679;&#30340;&#36855;&#22240;&#65292;&#28085;&#30422;&#30340;&#20027;&#39064;&#21253;&#25324;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#12289;&#24615;&#21035;&#27495;&#35270;&#21644;&#32593;&#32476;&#27450;&#20940;&#31561;&#12290;&#21033;&#29992;GOAT-Be
&lt;/p&gt;
&lt;p&gt;
The exponential growth of social media has profoundly transformed how information is created, disseminated, and absorbed, exceeding any precedent in the digital age. Regrettably, this explosion has also spawned a significant increase in the online abuse of memes. Evaluating the negative impact of memes is notably challenging, owing to their often subtle and implicit meanings, which are not directly conveyed through the overt text and imagery. In light of this, large multimodal models (LMMs) have emerged as a focal point of interest due to their remarkable capabilities in handling diverse multimodal tasks. In response to this development, our paper aims to thoroughly examine the capacity of various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of social abuse manifested in memes. We introduce the comprehensive meme benchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes such as implicit hate speech, sexism, and cyberbullying, etc. Utilizing GOAT-Be
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#31070;&#32463;&#36716;&#23548;&#22120;&#23454;&#29616;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#36716;&#25442;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#20041;&#26631;&#35760;&#39044;&#27979;&#23454;&#29616;&#20102;&#31283;&#20581;&#39640;&#25928;&#30340;&#23545;&#40784;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#38750;&#33258;&#22238;&#24402;&#35821;&#38899;&#29983;&#25104;&#22120;&#21512;&#25104;&#35821;&#38899;&#27874;&#24418;&#12290;&#35813;&#26694;&#26550;&#22312;&#35821;&#38899;&#36136;&#37327;&#21644;&#35828;&#35805;&#32773;&#30456;&#20284;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.01498</link><description>&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#36716;&#23548;&#22120;&#36827;&#34892;&#20004;&#38454;&#27573;&#25991;&#26412;&#21040;&#35821;&#38899;&#36716;&#25442;&#30340;&#35821;&#20041;&#26631;&#35760;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction. (arXiv:2401.01498v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01498
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31070;&#32463;&#36716;&#23548;&#22120;&#23454;&#29616;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#36716;&#25442;&#26694;&#26550;&#65292;&#36890;&#36807;&#35821;&#20041;&#26631;&#35760;&#39044;&#27979;&#23454;&#29616;&#20102;&#31283;&#20581;&#39640;&#25928;&#30340;&#23545;&#40784;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#38750;&#33258;&#22238;&#24402;&#35821;&#38899;&#29983;&#25104;&#22120;&#21512;&#25104;&#35821;&#38899;&#27874;&#24418;&#12290;&#35813;&#26694;&#26550;&#22312;&#35821;&#38899;&#36136;&#37327;&#21644;&#35828;&#35805;&#32773;&#30456;&#20284;&#24230;&#26041;&#38754;&#36229;&#36807;&#20102;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#26694;&#26550;&#65292;&#23427;&#20197;&#31070;&#32463;&#36716;&#23548;&#22120;&#20026;&#26680;&#24515;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#25972;&#20010;TTS&#27969;&#31243;&#21010;&#20998;&#20026;&#35821;&#20041;&#32423;&#21035;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;(seq2seq)&#24314;&#27169;&#21644;&#32454;&#31890;&#24230;&#30340;&#22768;&#23398;&#24314;&#27169;&#38454;&#27573;&#65292;&#21033;&#29992;&#20174;wav2vec2.0&#23884;&#20837;&#20013;&#33719;&#21462;&#30340;&#31163;&#25955;&#35821;&#20041;&#26631;&#35760;&#12290;&#20026;&#20102;&#23454;&#29616;&#31283;&#20581;&#39640;&#25928;&#30340;&#23545;&#40784;&#24314;&#27169;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;&#35821;&#20041;&#26631;&#35760;&#36716;&#23548;&#22120;&#30340;&#31070;&#32463;&#36716;&#23548;&#22120;&#26469;&#36827;&#34892;&#35821;&#20041;&#26631;&#35760;&#39044;&#27979;&#65292;&#20174;&#20013;&#33719;&#24471;&#20102;&#30828;&#21333;&#35843;&#23545;&#40784;&#32422;&#26463;&#30340;&#30410;&#22788;&#12290;&#38543;&#21518;&#65292;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;(NAR)&#35821;&#38899;&#29983;&#25104;&#22120;&#20174;&#36825;&#20123;&#35821;&#20041;&#26631;&#35760;&#26377;&#25928;&#22320;&#21512;&#25104;&#27874;&#24418;&#12290;&#21478;&#22806;&#65292;&#21442;&#32771;&#35821;&#38899;&#22312;&#27599;&#20010;&#38454;&#27573;&#25511;&#21046;&#30528;&#26102;&#38388;&#21160;&#24577;&#21644;&#22768;&#23398;&#26465;&#20214;&#12290;&#36825;&#31181;&#35299;&#32806;&#30340;&#26694;&#26550;&#20943;&#23569;&#20102;TTS&#30340;&#35757;&#32451;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20801;&#35768;&#27599;&#20010;&#38454;&#27573;&#19987;&#27880;&#20110;&#35821;&#20041;&#21644;&#22768;&#23398;&#24314;&#27169;&#12290;&#25105;&#20204;&#22312;&#38646;-shot&#33258;&#36866;&#24212;TTS&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35821;&#38899;&#36136;&#37327;&#21644;&#35828;&#35805;&#32773;&#30456;&#20284;&#24230;&#26041;&#38754;&#36229;&#36234;&#20102;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel text-to-speech (TTS) framework centered around a neural transducer. Our approach divides the whole TTS pipeline into semantic-level sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings. For a robust and efficient alignment modeling, we employ a neural transducer named token transducer for the semantic token prediction, benefiting from its hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR) speech generator efficiently synthesizes waveforms from these semantic tokens. Additionally, a reference speech controls temporal dynamics and acoustic conditions at each stage. This decoupled framework reduces the training complexity of TTS while allowing each stage to focus on semantic and acoustic modeling. Our experimental results on zero-shot adaptive TTS demonstrate that our model surpasses the baseline in terms of speech quality and speaker similari
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#22810;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#23558;&#21407;&#22987;&#25968;&#25454;&#38598;&#29992;&#19981;&#21516;&#39044;&#22788;&#29702;&#27169;&#24577;&#36827;&#34892;&#32534;&#30721;&#65307;&#28982;&#21518;&#65292;&#24341;&#20837;&#20102;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#20854;&#20182;&#32467;&#26500;&#23545;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#65292;&#20197;&#20102;&#35299;&#27169;&#24577;&#20869;&#37096;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#65307;&#26368;&#21518;&#65292;&#20351;&#29992;MLP&#36827;&#34892;&#26368;&#32456;&#30340;&#24773;&#32490;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.01495</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#22810;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Two-Stage Multimodal Emotion Recognition Model Based on Graph Contrastive Learning. (arXiv:2401.01495v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01495
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#22810;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#23558;&#21407;&#22987;&#25968;&#25454;&#38598;&#29992;&#19981;&#21516;&#39044;&#22788;&#29702;&#27169;&#24577;&#36827;&#34892;&#32534;&#30721;&#65307;&#28982;&#21518;&#65292;&#24341;&#20837;&#20102;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#20854;&#20182;&#32467;&#26500;&#23545;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#65292;&#20197;&#20102;&#35299;&#27169;&#24577;&#20869;&#37096;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#65307;&#26368;&#21518;&#65292;&#20351;&#29992;MLP&#36827;&#34892;&#26368;&#32456;&#30340;&#24773;&#32490;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#26426;&#20132;&#20114;&#26041;&#38754;&#65292;&#27491;&#30830;&#29702;&#35299;&#29992;&#25143;&#22312;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#29366;&#24577;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#27492;&#22810;&#27169;&#24577;&#24773;&#32490;&#35782;&#21035;&#65288;MER&#65289;&#30340;&#20219;&#21153;&#24320;&#22987;&#21463;&#21040;&#26356;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24773;&#32490;&#20998;&#31867;&#26041;&#27861;&#36890;&#24120;&#21482;&#36827;&#34892;&#19968;&#27425;&#20998;&#31867;&#65292;&#21477;&#23376;&#22312;&#21333;&#36718;&#20998;&#31867;&#20013;&#24456;&#21487;&#33021;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#24573;&#30053;&#20102;&#34701;&#21512;&#36807;&#31243;&#20013;&#19981;&#21516;&#24418;&#24577;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#65288;TS-GCL&#65289;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#39044;&#22788;&#29702;&#27169;&#24577;&#23545;&#21407;&#22987;&#25968;&#25454;&#38598;&#36827;&#34892;&#32534;&#30721;&#12290;&#20854;&#27425;&#65292;&#24341;&#20837;&#20102;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#20854;&#20182;&#32467;&#26500;&#23545;&#36825;&#19977;&#20010;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#65292;&#20197;&#20102;&#35299;&#27169;&#24577;&#20869;&#37096;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;MLP&#20004;&#27425;&#36827;&#34892;&#26368;&#32456;&#30340;&#24773;&#32490;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
In terms of human-computer interaction, it is becoming more and more important to correctly understand the user's emotional state in a conversation, so the task of multimodal emotion recognition (MER) started to receive more attention. However, existing emotion classification methods usually perform classification only once. Sentences are likely to be misclassified in a single round of classification. Previous work usually ignores the similarities and differences between different morphological features in the fusion process. To address the above issues, we propose a two-stage emotion recognition model based on graph contrastive learning (TS-GCL). First, we encode the original dataset with different preprocessing modalities. Second, a graph contrastive learning (GCL) strategy is introduced for these three modal data with other structures to learn similarities and differences within and between modalities. Finally, we use MLP twice to achieve the final emotion classification. This stage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24335;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#21033;&#29992;&#32929;&#31080;&#30334;&#20998;&#27604;&#21464;&#21270;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#20998;&#26512;&#20844;&#24320;&#21457;&#24067;&#30340;&#26032;&#38395;&#25991;&#31456;&#12290;&#32467;&#26524;&#23637;&#31034;&#20102;&#20351;&#29992;&#23567;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20934;&#30830;&#39044;&#27979;&#24635;&#20307;&#32929;&#31080;&#36235;&#21183;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#29305;&#23450;&#25968;&#25454;&#29305;&#24449;&#21644;&#36873;&#25321;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01487</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#19982;&#22810;&#27169;&#24335;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing and Multimodal Stock Price Prediction. (arXiv:2401.01487v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24335;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#65292;&#21033;&#29992;&#32929;&#31080;&#30334;&#20998;&#27604;&#21464;&#21270;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#20998;&#26512;&#20844;&#24320;&#21457;&#24067;&#30340;&#26032;&#38395;&#25991;&#31456;&#12290;&#32467;&#26524;&#23637;&#31034;&#20102;&#20351;&#29992;&#23567;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20934;&#30830;&#39044;&#27979;&#24635;&#20307;&#32929;&#31080;&#36235;&#21183;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#29305;&#23450;&#25968;&#25454;&#29305;&#24449;&#21644;&#36873;&#25321;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#20915;&#31574;&#39046;&#22495;&#65292;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#22914;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#24120;&#34987;&#29992;&#20110;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#12290;&#26412;&#25991;&#21033;&#29992;&#32929;&#31080;&#30334;&#20998;&#27604;&#21464;&#21270;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#19982;&#20256;&#32479;&#20351;&#29992;&#21407;&#22987;&#36135;&#24065;&#20540;&#30340;&#26041;&#24335;&#30456;&#27604;&#65292;&#37325;&#28857;&#20998;&#26512;&#20844;&#24320;&#21457;&#24067;&#30340;&#26032;&#38395;&#25991;&#31456;&#12290;&#30334;&#20998;&#27604;&#21464;&#21270;&#30340;&#36873;&#25321;&#26088;&#22312;&#20026;&#27169;&#22411;&#25552;&#20379;&#20851;&#20110;&#20215;&#26684;&#27874;&#21160;&#30340;&#37325;&#35201;&#24615;&#21450;&#20854;&#23545;&#32929;&#31080;&#25972;&#20307;&#20215;&#26684;&#21464;&#21270;&#30340;&#24433;&#21709;&#30340;&#32972;&#26223;&#20449;&#24687;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#20102;&#19987;&#38376;&#30340;BERT&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26469;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#36235;&#21183;&#65292;&#29305;&#21035;&#24378;&#35843;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#12290;&#32467;&#26524;&#23637;&#31034;&#20102;&#20351;&#29992;&#23567;&#22411;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20934;&#30830;&#39044;&#27979;&#24635;&#20307;&#32929;&#31080;&#36235;&#21183;&#30340;&#33021;&#21147;&#65292;&#24182;&#20984;&#26174;&#20102;&#29305;&#23450;&#25968;&#25454;&#29305;&#24449;&#21644;&#36873;&#25321;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of financial decision-making, predicting stock prices is pivotal. Artificial intelligence techniques such as long short-term memory networks (LSTMs), support-vector machines (SVMs), and natural language processing (NLP) models are commonly employed to predict said prices. This paper utilizes stock percentage change as training data, in contrast to the traditional use of raw currency values, with a focus on analyzing publicly released news articles. The choice of percentage change aims to provide models with context regarding the significance of price fluctuations and overall price change impact on a given stock. The study employs specialized BERT natural language processing models to predict stock price trends, with a particular emphasis on various data modalities. The results showcase the capabilities of such strategies with a small natural language processing model to accurately predict overall stock trends, and highlight the effectiveness of certain data features and se
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;Stack Overflow&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01472</link><description>&lt;p&gt;
Stack Overflow&#22238;&#31572;&#20013;&#20449;&#24687;&#39640;&#20142;&#30340;&#21021;&#25506;
&lt;/p&gt;
&lt;p&gt;
A First Look at Information Highlighting in Stack Overflow Answers. (arXiv:2401.01472v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;Stack Overflow&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#27983;&#35272;Stack Overflow&#65288;SO&#65289;&#30340;&#30693;&#35782;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20351;&#24086;&#23376;&#23545;&#29992;&#25143;&#26356;&#29983;&#21160;&#65292;SO&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;Markdown&#25110;HTML&#32534;&#20889;&#21644;&#32534;&#36753;&#24086;&#23376;&#65292;&#20197;&#20415;&#29992;&#25143;&#21487;&#20197;&#21033;&#29992;&#21508;&#31181;&#26684;&#24335;&#21270;&#26679;&#24335;&#65288;&#20363;&#22914;&#31895;&#20307;&#12289;&#26012;&#20307;&#21644;&#20195;&#30721;&#65289;&#26469;&#31361;&#20986;&#37325;&#35201;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#31361;&#20986;&#20449;&#24687;&#30340;&#30740;&#31350;&#20173;&#28982;&#26377;&#38480;&#12290;&#30446;&#26631;&#65306;&#25105;&#20204;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#39318;&#27425;&#22823;&#35268;&#27169;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;SO&#22238;&#31572;&#20013;&#30340;&#20449;&#24687;&#39640;&#20142;&#12290;&#20026;&#20102;&#25193;&#23637;&#25105;&#20204;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24320;&#21457;&#20102;&#33258;&#21160;&#25512;&#33616;&#24102;&#26377;&#26684;&#24335;&#21270;&#26679;&#24335;&#30340;&#31361;&#20986;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#65306;&#26412;&#25991;&#30740;&#31350;&#20102;Stack Overflow&#30340;31,169,429&#20010;&#22238;&#31572;&#12290;&#20026;&#20102;&#35757;&#32451;&#25512;&#33616;&#27169;&#22411;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;CNN&#21644;BERT&#27169;&#22411;&#65292;&#38024;&#23545;&#27599;&#31181;&#26684;&#24335;&#21270;&#31867;&#22411;&#65288;&#21363;&#31895;&#20307;&#12289;&#26012;&#20307;&#12289;&#20195;&#30721;&#21644;&#26631;&#39064;&#65289;&#20351;&#29992;&#25105;&#20204;&#20174;SO&#22238;&#31572;&#25910;&#38598;&#30340;&#31361;&#20986;&#20449;&#24687;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context: Navigating the knowledge of Stack Overflow (SO) remains challenging. To make the posts vivid to users, SO allows users to write and edit posts with Markdown or HTML so that users can leverage various formatting styles (e.g., bold, italic, and code) to highlight the important information. Nonetheless, there have been limited studies on the highlighted information. Objective: We carried out the first large-scale exploratory study on the information highlighted in SO answers in our recent study. To extend our previous study, we develop approaches to automatically recommend highlighted content with formatting styles using neural network architectures initially designed for the Named Entity Recognition task. Method: In this paper, we studied 31,169,429 answers of Stack Overflow. For training recommendation models, we choose CNN and BERT models for each type of formatting (i.e., Bold, Italic, Code, and Heading) using the information highlighting dataset we collected from SO answers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#38382;&#31572;&#24335;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25688;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#20041;&#25628;&#32034;&#12289;RAG&#21644;&#26368;&#26032;&#30340;LLMs&#65292;&#25688;&#35201;&#26159;&#25552;&#21462;&#34987;&#19987;&#19994;&#20154;&#22763;&#35748;&#20026;&#37325;&#35201;&#30340;&#29305;&#23450;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.01469</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#38382;&#31572;&#24335;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Question-Answering Based Summarization of Electronic Health Records using Retrieval Augmented Generation. (arXiv:2401.01469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01469
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#38382;&#31572;&#24335;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25688;&#35201;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#20041;&#25628;&#32034;&#12289;RAG&#21644;&#26368;&#26032;&#30340;LLMs&#65292;&#25688;&#35201;&#26159;&#25552;&#21462;&#34987;&#19987;&#19994;&#20154;&#22763;&#35748;&#20026;&#37325;&#35201;&#30340;&#29305;&#23450;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#30340;&#25688;&#35201;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#24739;&#32773;&#21644;&#21307;&#21153;&#20154;&#21592;&#30340;&#8220;&#23631;&#24149;&#26102;&#38388;&#8221;&#12290;&#36817;&#24180;&#26469;&#65292;EHRs&#30340;&#25688;&#35201;&#24050;&#32463;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#27169;&#22411;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#33719;&#24471;&#36275;&#22815;&#30340;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#32467;&#26524;&#19981;&#22815;&#28385;&#24847;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#27880;&#24847;&#26426;&#21046;&#22312;&#36755;&#20837;&#22823;&#23567;&#26041;&#38754;&#22686;&#21152;&#20102;&#20108;&#27425;&#22797;&#26434;&#24615;&#65292;&#22240;&#27492;&#38656;&#35201;&#32771;&#34385;&#25972;&#20010;EHR&#20869;&#23481;&#30340;&#25688;&#35201;&#25928;&#26524;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#32467;&#21512;&#35821;&#20041;&#25628;&#32034;&#12289;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#26368;&#26032;&#30340;LLMs&#26469;&#32531;&#35299;&#36825;&#20123;&#32570;&#28857;&#30340;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#25688;&#35201;&#26159;&#25552;&#21462;&#34987;&#19987;&#19994;&#20154;&#22763;&#35748;&#20026;&#37325;&#35201;&#30340;&#29305;&#23450;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarization of electronic health records (EHRs) can substantially minimize 'screen time' for both patients as well as medical personnel. In recent years summarization of EHRs have employed machine learning pipelines using state of the art neural models. However, these models have produced less than adequate results that are attributed to the difficulty of obtaining sufficient annotated data for training. Moreover, the requirement to consider the entire content of an EHR in summarization has resulted in poor performance due to the fact that attention mechanisms in modern large language models (LLMs) adds a quadratic complexity in terms of the size of the input. We propose here a method that mitigates these shortcomings by combining semantic search, retrieval augmented generation (RAG) and question-answering using the latest LLMs. In our approach summarization is the extraction of answers to specific questions that are deemed important by subject-matter experts (SMEs). Our approach is 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#26426;&#22120;&#32763;&#35793;&#21644;&#20154;&#24037;&#32763;&#35793;&#36827;&#34892;&#32454;&#33268;&#27604;&#36739;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#26426;&#22120;&#32763;&#35793;&#30456;&#23545;&#20445;&#23432;&#65292;&#20855;&#26377;&#36739;&#23569;&#30340;&#26500;&#35789;-&#21477;&#27861;&#22810;&#26679;&#24615;&#21644;&#26356;&#22810;&#30340;&#25910;&#25947;&#27169;&#24335;&#65292;&#24182;&#19988;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#19979;&#38477;&#19982;&#26500;&#35789;-&#21477;&#27861;&#20998;&#27495;&#30340;&#23384;&#22312;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2401.01419</link><description>&lt;p&gt;
&#19982;&#19981;&#20998;&#27495;&#30456;&#27604;&#65306;&#20174;&#26500;&#35789;-&#21477;&#27861;&#35282;&#24230;&#30475;&#26426;&#22120;&#32763;&#35793;&#19982;&#20154;&#24037;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine Translation vs Human Translation. (arXiv:2401.01419v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01419
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#26426;&#22120;&#32763;&#35793;&#21644;&#20154;&#24037;&#32763;&#35793;&#36827;&#34892;&#32454;&#33268;&#27604;&#36739;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#26426;&#22120;&#32763;&#35793;&#30456;&#23545;&#20445;&#23432;&#65292;&#20855;&#26377;&#36739;&#23569;&#30340;&#26500;&#35789;-&#21477;&#27861;&#22810;&#26679;&#24615;&#21644;&#26356;&#22810;&#30340;&#25910;&#25947;&#27169;&#24335;&#65292;&#24182;&#19988;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#19979;&#38477;&#19982;&#26500;&#35789;-&#21477;&#27861;&#20998;&#27495;&#30340;&#23384;&#22312;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#26500;&#35789;-&#21477;&#27861;&#20998;&#27495;&#30340;&#38236;&#22836;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#32454;&#33268;&#27604;&#36739;&#20998;&#26512;&#65292;&#23545;&#27604;&#20102;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21644;&#20154;&#24037;&#32763;&#35793;&#65288;HT&#65289;&#12290;&#22312;&#19977;&#31181;&#35821;&#35328;&#23545;&#21644;&#20004;&#31181;&#23450;&#20041;&#20026;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#30340;&#32467;&#26500;&#24046;&#24322;&#30340;&#20998;&#27495;&#31867;&#22411;&#19979;&#65292;&#26426;&#22120;&#32763;&#35793;&#22987;&#32456;&#27604;&#20154;&#24037;&#32763;&#35793;&#26356;&#20026;&#20445;&#23432;&#65292;&#20855;&#26377;&#36739;&#23569;&#30340;&#26500;&#35789;-&#21477;&#27861;&#22810;&#26679;&#24615;&#65292;&#26356;&#22810;&#30340;&#25910;&#25947;&#24615;&#27169;&#24335;&#21644;&#26356;&#22810;&#30340;&#19968;&#23545;&#19968;&#23545;&#24212;&#20851;&#31995;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#35299;&#30721;&#31639;&#27861;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#24046;&#24322;&#24402;&#22240;&#20110;&#26463;&#25628;&#32034;&#30340;&#20351;&#29992;&#65292;&#36825;&#20351;&#24471;&#26426;&#22120;&#32763;&#35793;&#20559;&#21521;&#20110;&#26356;&#20026;&#25910;&#25947;&#30340;&#27169;&#24335;&#12290;&#24403;&#25910;&#25947;&#27169;&#24335;&#20986;&#29616;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32422;50%&#30340;&#26102;&#38388;&#26102;&#65292;&#36825;&#31181;&#20559;&#21521;&#24615;&#20250;&#34987;&#25918;&#22823;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#23545;&#20110;&#22823;&#22810;&#25968;&#26500;&#35789;-&#21477;&#27861;&#20998;&#27495;&#65292;&#20854;&#23384;&#22312;&#20110;&#20154;&#24037;&#32763;&#35793;&#20013;&#19982;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#33021;&#19979;&#38477;&#30456;&#20851;&#65292;&#32473;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#24102;&#26469;&#20102;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
We conduct a large-scale fine-grained comparative analysis of machine translations (MT) against human translations (HT) through the lens of morphosyntactic divergence. Across three language pairs and two types of divergence defined as the structural difference between the source and the target, MT is consistently more conservative than HT, with less morphosyntactic diversity, more convergent patterns, and more one-to-one alignments. Through analysis on different decoding algorithms, we attribute this discrepancy to the use of beam search that biases MT towards more convergent patterns. This bias is most amplified when the convergent pattern appears around 50% of the time in training data. Lastly, we show that for a majority of morphosyntactic divergences, their presence in HT is correlated with decreased MT performance, presenting a greater challenge for MT systems.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#22312;&#24635;&#32479;&#28436;&#35762;&#20013;&#30340;&#29420;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#20182;&#22312;&#20351;&#29992;&#20855;&#26377;&#20998;&#35010;&#24615;&#21644;&#23545;&#25239;&#24615;&#30340;&#35821;&#35328;&#12289;&#37325;&#22797;&#24378;&#35843;&#31561;&#26041;&#38754;&#19982;&#20854;&#20182;&#24635;&#32479;&#20505;&#36873;&#20154;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#29305;&#26391;&#26222;&#27604;&#20849;&#21644;&#20826;&#21516;&#20698;&#26356;&#20855;&#29420;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01405</link><description>&lt;p&gt;
&#37327;&#21270;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#22312;&#24635;&#32479;&#28436;&#35762;&#20013;&#30340;&#29420;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Uniqueness of Donald Trump in Presidential Discourse. (arXiv:2401.01405v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01405
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#22312;&#24635;&#32479;&#28436;&#35762;&#20013;&#30340;&#29420;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#20182;&#22312;&#20351;&#29992;&#20855;&#26377;&#20998;&#35010;&#24615;&#21644;&#23545;&#25239;&#24615;&#30340;&#35821;&#35328;&#12289;&#37325;&#22797;&#24378;&#35843;&#31561;&#26041;&#38754;&#19982;&#20854;&#20182;&#24635;&#32479;&#20505;&#36873;&#20154;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#29305;&#26391;&#26222;&#27604;&#20849;&#21644;&#20826;&#21516;&#20698;&#26356;&#20855;&#29420;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21776;&#32435;&#24503;&#183;&#29305;&#26391;&#26222;&#19982;&#20854;&#20182;&#24635;&#32479;&#22312;&#28436;&#35762;&#20013;&#26159;&#21542;&#34920;&#36798;&#20986;&#19981;&#21516;&#30340;&#39118;&#26684;&#65311;&#22914;&#26524;&#26159;&#65292;&#26377;&#21738;&#20123;&#26041;&#38754;&#30340;&#19981;&#21516;&#65311;&#36825;&#20123;&#24046;&#24322;&#26159;&#21542;&#23616;&#38480;&#20110;&#20219;&#20309;&#21333;&#19968;&#30340;&#27807;&#36890;&#23186;&#20171;&#65311;&#20026;&#20102;&#35843;&#26597;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29420;&#29305;&#24615;&#24230;&#37327;&#26631;&#20934;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#35010;&#24615;&#28436;&#35762;&#35789;&#24211;&#65292;&#24182;&#25552;&#20986;&#20102;&#27604;&#36739;&#25919;&#27835;&#23545;&#25163;&#35789;&#27719;&#29305;&#24449;&#30340;&#26694;&#26550;&#12290;&#23558;&#36825;&#20123;&#24037;&#20855;&#24212;&#29992;&#20110;&#22810;&#31181;&#24635;&#32479;&#28436;&#35762;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#21457;&#29616;&#26377;&#30456;&#24403;&#22810;&#30340;&#35777;&#25454;&#34920;&#26126;&#29305;&#26391;&#26222;&#30340;&#35762;&#35805;&#27169;&#24335;&#19982;&#36817;&#20195;&#21382;&#20219;&#20027;&#35201;&#24635;&#32479;&#20505;&#36873;&#20154;&#19981;&#21516;&#12290;&#19968;&#20123;&#26174;&#33879;&#30340;&#21457;&#29616;&#21253;&#25324;&#29305;&#26391;&#26222;&#20351;&#29992;&#29305;&#21035;&#20855;&#26377;&#20998;&#35010;&#24615;&#21644;&#23545;&#25239;&#24615;&#30340;&#35821;&#35328;&#38024;&#23545;&#20182;&#30340;&#25919;&#27835;&#23545;&#25163;&#65292;&#24182;&#19988;&#20182;&#37325;&#22797;&#24378;&#35843;&#30340;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#29305;&#26391;&#26222;&#27604;&#20182;&#30340;&#20849;&#21644;&#20826;&#21516;&#20698;&#26356;&#21152;&#29420;&#29305;&#65292;&#32780;&#20182;&#20204;&#30340;&#29420;&#29305;&#24615;&#20540;&#19982;&#27665;&#20027;&#20826;&#30456;&#23545;&#36739;&#25509;&#36817;&#12290;&#36825;&#20123;&#24046;&#24322;&#22312;&#22810;&#31181;&#24230;&#37327;&#26041;&#27861;&#19979;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does Donald Trump speak differently from other presidents? If so, in what ways? Are these differences confined to any single medium of communication? To investigate these questions, this paper introduces a novel metric of uniqueness based on large language models, develops a new lexicon for divisive speech, and presents a framework for comparing the lexical features of political opponents. Applying these tools to a variety of corpora of presidential speeches, we find considerable evidence that Trump's speech patterns diverge from those of all major party nominees for the presidency in recent history. Some notable findings include Trump's employment of particularly divisive and antagonistic language targeting of his political opponents and his patterns of repetition for emphasis. Furthermore, Trump is significantly more distinctive than his fellow Republicans, whose uniqueness values are comparably closer to those of the Democrats. These differences hold across a variety of measurement 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#36825;&#26159;&#23433;&#20840;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#22823;&#38556;&#30861;&#12290;&#35299;&#20915;&#24187;&#35273;&#38382;&#39064;&#23545;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#24191;&#27867;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.01313</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#32531;&#35299;&#25216;&#26415;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models. (arXiv:2401.01313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01313
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#25991;&#26412;&#26102;&#23481;&#26131;&#20986;&#29616;&#24187;&#35273;&#65292;&#36825;&#26159;&#23433;&#20840;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#30340;&#26368;&#22823;&#38556;&#30861;&#12290;&#35299;&#20915;&#24187;&#35273;&#38382;&#39064;&#23545;&#20110;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#24191;&#27867;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#20154;&#31867;&#21270;&#25991;&#26412;&#26041;&#38754;&#30340;&#33021;&#21147;&#19981;&#26029;&#25552;&#39640;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23427;&#20204;&#20542;&#21521;&#20110;&#20135;&#29983;&#34394;&#26500;&#30340;&#20869;&#23481;&#65292;&#30475;&#20284;&#30495;&#23454;&#20294;&#27809;&#26377;&#20381;&#25454;&#12290;&#24187;&#35273;&#38382;&#39064;&#21487;&#20197;&#35828;&#26159;&#23433;&#20840;&#22320;&#23558;&#36825;&#20123;&#24378;&#22823;&#30340;LLMs&#37096;&#32626;&#21040;&#24433;&#21709;&#20154;&#20204;&#29983;&#27963;&#30340;&#29616;&#23454;&#29983;&#20135;&#31995;&#32479;&#20013;&#26368;&#22823;&#30340;&#38556;&#30861;&#12290;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#24191;&#27867;&#37319;&#29992;LLMs&#30340;&#36807;&#31243;&#20005;&#37325;&#20381;&#36182;&#20110;&#35299;&#20915;&#21644;&#20943;&#36731;&#24187;&#35273;&#12290;&#19982;&#20256;&#32479;&#30340;&#19987;&#27880;&#20110;&#26377;&#38480;&#20219;&#21153;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19981;&#21516;&#65292;LLMs&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#25509;&#35302;&#21040;&#22823;&#37327;&#30340;&#22312;&#32447;&#25991;&#26412;&#25968;&#25454;&#12290;&#36825;&#20351;&#23427;&#20204;&#33021;&#22815;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#27969;&#21033;&#24615;&#65292;&#20294;&#20063;&#24847;&#21619;&#30528;&#23427;&#20204;&#33021;&#22815;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#20559;&#35265;&#20013;&#25512;&#26029;&#20449;&#24687;&#65292;&#38169;&#35823;&#35299;&#37322;&#21547;&#31946;&#19981;&#28165;&#30340;&#25552;&#31034;&#65292;&#25110;&#32773;&#20462;&#25913;&#20449;&#24687;&#20197;&#34920;&#38754;&#19978;&#19982;&#36755;&#20837;&#19968;&#33268;&#12290;&#24403;&#25105;&#20204;&#20381;&#36182;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#26469;&#23436;&#25104;&#25935;&#24863;&#24212;&#29992;&#26102;&#65292;&#36825;&#21464;&#24471;&#38750;&#24120;&#20196;&#20154;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#30340;&#36739;&#39640;&#36136;&#37327;&#21442;&#32771;&#25991;&#29486;&#23545;&#20110;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20215;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26356;&#22909;&#12290;&#27599;&#20010;&#27573;&#33853;&#24179;&#22343;&#20351;&#29992;7&#20010;&#21442;&#32771;&#25991;&#29486;&#26377;&#21161;&#20110;&#25552;&#21319;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#12290;&#19981;&#21516;&#36136;&#37327;&#30340;&#20379;&#24212;&#21830;&#21442;&#32771;&#25991;&#29486;&#21487;&#20197;&#28151;&#21512;&#20351;&#29992;&#26469;&#25552;&#39640;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#29992;&#20110;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#21019;&#24314;&#21442;&#32771;&#25991;&#29486;&#30340;&#20849;&#20139;&#20219;&#21153;&#30340;&#35780;&#20272;&#32773;&#12290;</title><link>http://arxiv.org/abs/2401.01283</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#33258;&#21160;&#35780;&#20272;&#30340;&#21442;&#32771;&#25991;&#29486;&#36136;&#37327;&#21644;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
Quality and Quantity of Machine Translation References for Automated Metrics. (arXiv:2401.01283v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#30340;&#36739;&#39640;&#36136;&#37327;&#21442;&#32771;&#25991;&#29486;&#23545;&#20110;&#35780;&#20272;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20215;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26356;&#22909;&#12290;&#27599;&#20010;&#27573;&#33853;&#24179;&#22343;&#20351;&#29992;7&#20010;&#21442;&#32771;&#25991;&#29486;&#26377;&#21161;&#20110;&#25552;&#21319;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#12290;&#19981;&#21516;&#36136;&#37327;&#30340;&#20379;&#24212;&#21830;&#21442;&#32771;&#25991;&#29486;&#21487;&#20197;&#28151;&#21512;&#20351;&#29992;&#26469;&#25552;&#39640;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#29992;&#20110;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#21019;&#24314;&#21442;&#32771;&#25991;&#29486;&#30340;&#20849;&#20139;&#20219;&#21153;&#30340;&#35780;&#20272;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#25351;&#26631;&#36890;&#24120;&#20351;&#29992;&#20154;&#24037;&#32763;&#35793;&#26469;&#30830;&#23450;&#31995;&#32479;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;&#39046;&#22495;&#20869;&#30340;&#20849;&#35782;&#35748;&#20026;&#20154;&#24037;&#21442;&#32771;&#25991;&#29486;&#24212;&#20855;&#26377;&#24456;&#39640;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#25104;&#26412;&#25928;&#30410;&#20998;&#26512;&#21487;&#20197;&#25351;&#23548;&#35745;&#21010;&#25910;&#38598;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#21442;&#32771;&#25991;&#29486;&#30340;&#20174;&#19994;&#32773;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36739;&#39640;&#36136;&#37327;&#30340;&#21442;&#32771;&#25991;&#29486;&#33021;&#22815;&#22312;&#27573;&#33853;&#32423;&#21035;&#19978;&#19982;&#20154;&#31867;&#35780;&#20215;&#30340;&#30456;&#20851;&#24615;&#26356;&#22909;&#12290;&#27599;&#20010;&#27573;&#33853;&#24179;&#22343;&#20351;&#29992;7&#20010;&#21442;&#32771;&#25991;&#29486;&#26377;&#21161;&#20110;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#30340;&#25552;&#21319;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26469;&#33258;&#19981;&#21516;&#36136;&#37327;&#30340;&#20379;&#24212;&#21830;&#30340;&#21442;&#32771;&#25991;&#29486;&#21487;&#20197;&#28151;&#21512;&#20351;&#29992;&#65292;&#24182;&#25552;&#39640;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#36739;&#39640;&#36136;&#37327;&#30340;&#21442;&#32771;&#25991;&#29486;&#21046;&#20316;&#25104;&#26412;&#26356;&#39640;&#65292;&#25105;&#20204;&#23558;&#20854;&#35270;&#20026;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65306;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#65292;&#24212;&#35813;&#25910;&#38598;&#21738;&#20123;&#21442;&#32771;&#25991;&#29486;&#20197;&#26368;&#22823;&#21270;&#35780;&#20272;&#25351;&#26631;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#21487;&#29992;&#20110;&#22312;&#29305;&#23450;&#39044;&#31639;&#19979;&#21019;&#24314;&#21442;&#32771;&#25991;&#29486;&#30340;&#20849;&#20139;&#20219;&#21153;&#30340;&#35780;&#20272;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic machine translation metrics often use human translations to determine the quality system translations. Common wisdom in the field dictates that the human references should be of very high quality. However, there are no cost-benefit analyses that could be used to guide practitioners who plan to collect references for machine translation evaluation. We find that higher-quality references lead to better metric correlations with humans at the segment-level. Having up to 7 references per segment and taking their average helps all metrics. Interestingly, the references from vendors of different qualities can be mixed together and improve metric success. Higher quality references, however, cost more to create and we frame this as an optimization problem: given a specific budget, what references should be collected to maximize metric success. These findings can be used by evaluators of shared tasks when references need to be created under a certain budget.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24320;&#21457;&#20844;&#24179;&#24615;&#35748;&#35777;&#26041;&#27861;&#12290;&#36890;&#36807;&#32508;&#36848;&#22823;&#37327;&#25991;&#29486;&#21644;&#19987;&#23478;&#35775;&#35848;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20845;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#20026;&#25805;&#20316;&#21270;&#21644;&#27979;&#35797;&#36807;&#31243;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2401.01262</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20844;&#24179;&#24615;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
Fairness Certification for Natural Language Processing and Large Language Models. (arXiv:2401.01262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01262
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24320;&#21457;&#20844;&#24179;&#24615;&#35748;&#35777;&#26041;&#27861;&#12290;&#36890;&#36807;&#32508;&#36848;&#22823;&#37327;&#25991;&#29486;&#21644;&#19987;&#23478;&#35775;&#35848;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20845;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#20026;&#25805;&#20316;&#21270;&#21644;&#27979;&#35797;&#36807;&#31243;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;NLP&#22312;&#25307;&#32856;&#31561;&#20844;&#24179;&#20851;&#38190;&#24212;&#29992;&#22330;&#26223;&#20013;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#65292;&#20363;&#22914;&#20316;&#20026;&#19987;&#23478;&#31995;&#32479;&#25110;&#22522;&#20110;LLM&#30340;&#25945;&#32946;&#23548;&#24072;&#12290;&#30001;&#20110;NLP&#22522;&#20110;&#20154;&#31867;&#35821;&#35328;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#28508;&#22312;&#30340;&#26377;&#23475;&#20559;&#35265;&#28183;&#20837;NLP&#31995;&#32479;&#65292;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#65292;&#27495;&#35270;&#23569;&#25968;&#32676;&#20307;&#25110;&#24341;&#21457;&#27861;&#24459;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#24320;&#23637;NLP&#26041;&#27861;&#30340;&#20844;&#24179;&#24615;&#35748;&#35777;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#37319;&#29992;&#23450;&#24615;&#30740;&#31350;&#26041;&#27861;&#65292;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#22823;&#37327;&#25991;&#29486;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24182;&#19982;&#35813;&#39046;&#22495;&#30340;&#22810;&#20301;&#19987;&#23478;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#30340;&#19987;&#23478;&#35775;&#35848;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#25552;&#20986;&#20102;NLP&#30340;&#20845;&#20010;&#20844;&#24179;&#24615;&#26631;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#32454;&#21270;&#20026;18&#20010;&#23376;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#26631;&#20934;&#20026;&#23454;&#26045;&#21644;&#27979;&#35797;&#36807;&#31243;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) plays an important role in our daily lives, particularly due to the enormous progress of Large Language Models (LLM). However, NLP has many fairness-critical use cases, e.g., as an expert system in recruitment or as an LLM-based tutor in education. Since NLP is based on human language, potentially harmful biases can diffuse into NLP systems and produce unfair results, discriminate against minorities or generate legal issues. Hence, it is important to develop a fairness certification for NLP approaches. We follow a qualitative research approach towards a fairness certification for NLP. In particular, we have reviewed a large body of literature on algorithmic fairness, and we have conducted semi-structured expert interviews with a wide range of experts from that area. We have systematically devised six fairness criteria for NLP, which can be further refined into 18 sub-categories. Our criteria offer a foundation for operationalizing and testing processes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25104;&#21151;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#36234;&#21335;&#35799;&#27468;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;&#35799;&#27468;&#32763;&#35793;&#25104;&#19981;&#21516;&#35821;&#35328;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.01078</link><description>&lt;p&gt;
&#36234;&#21335;&#35799;&#27468;&#29983;&#25104;&#19982;&#36328;&#35821;&#35328;&#35799;&#27468;&#32763;&#35793;&#30340;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Vietnamese Poem Generation &amp; The Prospect Of Cross-Language Poem-To-Poem Translation. (arXiv:2401.01078v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25104;&#21151;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#36234;&#21335;&#35799;&#27468;&#30340;&#26041;&#27861;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;&#35799;&#27468;&#32763;&#35793;&#25104;&#19981;&#21516;&#35821;&#35328;&#30340;&#21487;&#33021;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35799;&#27468;&#29983;&#25104;&#19968;&#30452;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#19968;&#39033;&#25361;&#25112;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#27169;&#22411;&#29702;&#35299;&#35821;&#35328;&#12289;&#24773;&#24863;&#21644;&#39118;&#26684;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20013;&#29983;&#25104;&#36234;&#21335;&#35799;&#27468;&#65292;&#20174;&#32780;&#23454;&#29616;&#30452;&#35266;&#30340;&#36807;&#31243;&#21644;&#22686;&#24378;&#30340;&#20869;&#23481;&#25511;&#21046;&#12290;&#25105;&#20204;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;GPT-3 Babbage&#21464;&#31181;&#65292;&#22312;&#36234;&#21335;&#35799;&#27468;&#30340;&#8220;&#20845;&#20843;&#35789;&#8221;&#31867;&#22411;&#20013;&#23454;&#29616;&#20102;0.8&#30340;&#33258;&#23450;&#20041;&#35780;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#23558;&#35799;&#27468;&#25913;&#20889;&#25104;&#27491;&#24120;&#25991;&#26412;&#25552;&#31034;&#30340;&#24819;&#27861;&#65292;&#24182;&#22312;&#8220;&#20845;&#20843;&#35789;&#8221;&#31867;&#22411;&#20013;&#33719;&#24471;&#20102;&#30456;&#23545;&#36739;&#39640;&#30340;0.718&#20998;&#25968;&#12290;&#36825;&#20010;&#23454;&#39564;&#23637;&#31034;&#20102;&#20197;&#32763;&#35793;&#21518;&#30340;&#35799;&#27468;&#20316;&#20026;&#36755;&#20837;&#36827;&#34892;&#36328;&#35821;&#35328;&#35799;&#27468;&#32763;&#35793;&#30340;&#28508;&#21147;&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#23545;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#20840;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Poetry generation has been a challenging task in the field of Natural Language Processing, as it requires the model to understand the nuances of language, sentiment, and style. In this paper, we propose using Large Language Models to generate Vietnamese poems from natural language prompts, thereby facilitating an intuitive process with enhanced content control. Our most efficacious model, the GPT-3 Babbage variant, achieves a custom evaluation score of 0.8, specifically tailored to the "luc bat" genre of Vietnamese poetry. Furthermore, we also explore the idea of paraphrasing poems into normal text prompts and yield a relatively high score of 0.718 in the "luc bat" genre. This experiment presents the potential for cross-Language poem-to-poem translation with translated poems as the inputs while concurrently maintaining complete control over the generated content.
&lt;/p&gt;</description></item><item><title>DialCLIP&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;CLIP&#20013;&#24341;&#20837;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#25552;&#31034;&#29983;&#25104;&#22120;&#21644;&#39046;&#22495;&#25552;&#31034;&#26469;&#25552;&#21319;&#23545;&#35805;&#26816;&#32034;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.01076</link><description>&lt;p&gt;
DialCLIP: &#23558;CLIP&#25193;&#23637;&#20026;&#22810;&#27169;&#24577;&#23545;&#35805;&#26816;&#32034;&#22120;
&lt;/p&gt;
&lt;p&gt;
DialCLIP: Empowering CLIP as Multi-Modal Dialog Retriever. (arXiv:2401.01076v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01076
&lt;/p&gt;
&lt;p&gt;
DialCLIP&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;CLIP&#20013;&#24341;&#20837;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#25552;&#31034;&#29983;&#25104;&#22120;&#21644;&#39046;&#22495;&#25552;&#31034;&#26469;&#25552;&#21319;&#23545;&#35805;&#26816;&#32034;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#22810;&#27169;&#24577;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#22312;&#26377;&#25928;&#22320;&#25429;&#25417;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#23545;&#35805;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DialCLIP&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#25552;&#31034;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#23545;&#35805;&#26816;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#25552;&#31034;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#23398;&#20064;&#19978;&#19979;&#25991;&#29305;&#24449;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;CLIP&#20013;&#23558;&#20854;&#25552;&#28860;&#20026;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39046;&#22495;&#25552;&#31034;&#65292;&#20197;&#20943;&#36731;&#19979;&#28216;&#23545;&#35805;&#25968;&#25454;&#24341;&#36215;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#26041;&#20415;&#21508;&#31181;&#31867;&#22411;&#30340;&#26816;&#32034;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#22810;&#20010;&#19987;&#23478;&#65292;&#20174;CLIP&#30340;&#36755;&#20986;&#23398;&#20064;&#21040;&#22810;&#27169;&#24577;&#34920;&#31034;&#31354;&#38388;&#30340;&#26144;&#23556;&#65292;&#27599;&#20010;&#19987;&#23478;&#37117;&#26377;&#33258;&#24049;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, substantial advancements in pre-trained vision-language models have greatly enhanced the capabilities of multi-modal dialog systems. These models have demonstrated significant improvements by fine-tuning on downstream tasks. However, the existing pre-trained models primarily focus on effectively capturing the alignment between vision and language modalities, often ignoring the intricate nature of dialog context. In this paper, we propose a parameter-efficient prompt-tuning method named DialCLIP for multi-modal dialog retrieval. Specifically, our approach introduces a multi-modal context prompt generator to learn context features which are subsequently distilled into prompts within the pre-trained vision-language model CLIP. Besides, we introduce domain prompt to mitigate the disc repancy from the downstream dialog data. To facilitate various types of retrieval, we also design multiple experts to learn mappings from CLIP outputs to multi-modal representation space, with each e
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#33539;&#24335;&#65306;Naive RAG&#12289;Advanced RAG&#21644;Modular RAG&#12290;RAG&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#21644;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.10997</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Generation for Large Language Models: A Survey. (arXiv:2312.10997v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10997
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#19977;&#20010;&#20027;&#35201;&#33539;&#24335;&#65306;Naive RAG&#12289;Advanced RAG&#21644;Modular RAG&#12290;RAG&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#23454;&#29616;&#20102;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#21644;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#20294;&#38754;&#20020;&#24187;&#35273;&#12289;&#36807;&#26102;&#30693;&#35782;&#21644;&#38750;&#36879;&#26126;&#12289;&#19981;&#21487;&#36861;&#28335;&#30340;&#25512;&#29702;&#36807;&#31243;&#31561;&#25361;&#25112;&#12290;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#22806;&#37096;&#25968;&#25454;&#24211;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#25345;&#32493;&#26356;&#26032;&#30693;&#35782;&#21644;&#25972;&#21512;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#12290;RAG&#23558;LLMs&#33258;&#36523;&#30340;&#30693;&#35782;&#19982;&#24222;&#22823;&#12289;&#21160;&#24577;&#30340;&#22806;&#37096;&#25968;&#25454;&#24211;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#21327;&#21516;&#25928;&#24212;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#35814;&#32454;&#32771;&#23519;&#20102;RAG&#33539;&#24335;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;Naive RAG&#12289;Advanced RAG&#21644;Modular RAG&#12290;&#23427;&#35814;&#32454;&#23457;&#35270;&#20102;RAG&#26694;&#26550;&#30340;&#19977;&#20010;&#22522;&#26412;&#35201;&#32032;&#65292;&#21253;&#25324;&#26816;&#32034;&#12289;&#29983;&#25104;&#21644;&#22686;&#24378;&#25216;&#26415;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#23884;&#20837;&#22312;RAG&#26694;&#26550;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval , the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#65292;&#21253;&#25324;&#19981;&#21516;&#25216;&#26415;&#30340;&#27010;&#36848;&#21644;&#27604;&#36739;&#12289;&#31639;&#27861;&#35780;&#20272;&#26041;&#27861;&#12289;&#24212;&#29992;&#22330;&#26223;&#20197;&#21450;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2312.07913</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Text Watermarking in the Era of Large Language Models. (arXiv:2312.07913v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.07913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#65292;&#21253;&#25324;&#19981;&#21516;&#25216;&#26415;&#30340;&#27010;&#36848;&#21644;&#27604;&#36739;&#12289;&#31639;&#27861;&#35780;&#20272;&#26041;&#27861;&#12289;&#24212;&#29992;&#22330;&#26223;&#20197;&#21450;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#22312;&#29256;&#26435;&#20445;&#25252;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#28982;&#32780;&#20854;&#33021;&#21147;&#21644;&#24212;&#29992;&#22330;&#26223;&#19968;&#30452;&#21463;&#38480;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#20026;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#30340;&#36827;&#27493;&#25171;&#24320;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#36890;&#36807;&#20854;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#22686;&#24378;&#20102;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#30340;&#33021;&#21147;&#65292;&#36824;&#38656;&#35201;&#20351;&#29992;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#26469;&#20445;&#25252;&#33258;&#36523;&#30340;&#29256;&#26435;&#12290;&#26412;&#25991;&#23545;&#24403;&#21069;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#30340;&#29616;&#29366;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#21253;&#25324;&#22235;&#20010;&#20027;&#35201;&#26041;&#38754;&#65306;&#65288;1&#65289;&#19981;&#21516;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#30340;&#27010;&#36848;&#21644;&#27604;&#36739;&#65307;&#65288;2&#65289;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#21253;&#25324;&#25104;&#21151;&#29575;&#12289;&#23545;&#25991;&#26412;&#36136;&#37327;&#30340;&#24433;&#21709;&#12289;&#40065;&#26834;&#24615;&#21644;&#38450;&#31713;&#25913;&#24615;&#65307;&#65288;3&#65289;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#30340;&#28508;&#22312;&#24212;&#29992;&#22330;&#26223;&#65307;&#65288;4&#65289;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text watermarking algorithms play a crucial role in the copyright protection of textual content, yet their capabilities and application scenarios have been limited historically. The recent developments in large language models (LLMs) have opened new opportunities for the advancement of text watermarking techniques. LLMs not only enhance the capabilities of text watermarking algorithms through their text understanding and generation abilities but also necessitate the use of text watermarking algorithms for their own copyright protection. This paper conducts a comprehensive survey of the current state of text watermarking technology, covering four main aspects: (1) an overview and comparison of different text watermarking techniques; (2) evaluation methods for text watermarking algorithms, including their success rates, impact on text quality, robustness, and unforgeability; (3) potential application scenarios for text watermarking technology; (4) current challenges and future directions
&lt;/p&gt;</description></item><item><title>EQ-Bench&#26159;&#19968;&#31181;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24773;&#21830;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#36890;&#36807;&#35201;&#27714;&#27169;&#22411;&#39044;&#27979;&#23545;&#35805;&#20013;&#35282;&#33394;&#30340;&#24773;&#32490;&#29366;&#24577;&#24378;&#24230;&#26469;&#35780;&#20272;&#27169;&#22411;&#23545;&#22797;&#26434;&#24773;&#32490;&#21644;&#31038;&#20132;&#20132;&#20114;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#23427;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#19981;&#21516;&#27169;&#22411;&#65292;&#24182;&#19982;&#20854;&#20182;&#32508;&#21512;&#22522;&#20934;&#30456;&#20851;&#24615;&#24456;&#39640;&#12290;</title><link>http://arxiv.org/abs/2312.06281</link><description>&lt;p&gt;
EQ-Bench:&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#21830;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models. (arXiv:2312.06281v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06281
&lt;/p&gt;
&lt;p&gt;
EQ-Bench&#26159;&#19968;&#31181;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24773;&#21830;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#36890;&#36807;&#35201;&#27714;&#27169;&#22411;&#39044;&#27979;&#23545;&#35805;&#20013;&#35282;&#33394;&#30340;&#24773;&#32490;&#29366;&#24577;&#24378;&#24230;&#26469;&#35780;&#20272;&#27169;&#22411;&#23545;&#22797;&#26434;&#24773;&#32490;&#21644;&#31038;&#20132;&#20132;&#20114;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#23427;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#19981;&#21516;&#27169;&#22411;&#65292;&#24182;&#19982;&#20854;&#20182;&#32508;&#21512;&#22522;&#20934;&#30456;&#20851;&#24615;&#24456;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;EQ-Bench&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24773;&#21830;&#26041;&#38754;&#30340;&#26032;&#22411;&#22522;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#35201;&#27714;LLMs&#39044;&#27979;&#23545;&#35805;&#20013;&#35282;&#33394;&#30340;&#24773;&#32490;&#29366;&#24577;&#30340;&#24378;&#24230;&#26469;&#35780;&#20272;LLMs&#29702;&#35299;&#22797;&#26434;&#24773;&#32490;&#21644;&#31038;&#20132;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#22522;&#20934;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#19981;&#21516;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;EQ-Bench&#19982;&#32508;&#21512;&#22810;&#39046;&#22495;&#22522;&#20934;&#65288;&#22914;MMLU&#65289;&#20043;&#38388;&#23384;&#22312;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#65288;r=0.97&#65289;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#21487;&#33021;&#25429;&#25417;&#21040;&#20102;&#24191;&#27867;&#26234;&#33021;&#30340;&#30456;&#20284;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#20351;&#29992;60&#20010;&#33521;&#35821;&#38382;&#39064;&#20135;&#29983;&#39640;&#24230;&#21487;&#37325;&#22797;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#22312;https://github.com/EQ-bench/EQ-Bench&#19978;&#25552;&#20379;&#20102;&#24320;&#28304;&#20195;&#30721;&#29992;&#20110;&#33258;&#21160;&#21270;&#22522;&#20934;&#27979;&#35797;&#27969;&#31243;&#65292;&#20197;&#21450;https://eqbench.com&#19978;&#30340;&#25490;&#34892;&#27036;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce EQ-Bench, a novel benchmark designed to evaluate aspects of emotional intelligence in Large Language Models (LLMs). We assess the ability of LLMs to understand complex emotions and social interactions by asking them to predict the intensity of emotional states of characters in a dialogue. The benchmark is able to discriminate effectively between a wide range of models. We find that EQ-Bench correlates strongly with comprehensive multi-domain benchmarks like MMLU (Hendrycks et al., 2020) (r=0.97), indicating that we may be capturing similar aspects of broad intelligence. Our benchmark produces highly repeatable results using a set of 60 English-language questions. We also provide open-source code for an automated benchmarking pipeline at https://github.com/EQ-bench/EQ-Bench and a leaderboard at https://eqbench.com
&lt;/p&gt;</description></item><item><title>Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.19923</link><description>&lt;p&gt;
Jina Embeddings 2: &#38754;&#21521;&#38271;&#31687;&#25991;&#26723;&#30340;8192-Token&#36890;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents. (arXiv:2310.19923v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19923
&lt;/p&gt;
&lt;p&gt;
Jina Embeddings 2&#26159;&#19968;&#20010;&#33021;&#22815;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#23558;&#21477;&#23376;&#36716;&#21270;&#20026;&#22266;&#23450;&#22823;&#23567;&#29305;&#24449;&#21521;&#37327;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#36825;&#20123;&#21521;&#37327;&#21253;&#21547;&#20102;&#35821;&#20041;&#20449;&#24687;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20449;&#24687;&#26816;&#32034;&#12289;&#35821;&#20041;&#32858;&#31867;&#21644;&#25991;&#26412;&#37325;&#25490;&#24207;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;BERT&#31561;&#26550;&#26500;&#26500;&#24314;&#30340;&#27169;&#22411;&#65292;&#38590;&#20197;&#34920;&#31034;&#38271;&#31687;&#25991;&#26723;&#65292;&#24182;&#19988;&#24120;&#24120;&#20250;&#36827;&#34892;&#25130;&#26029;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#25361;&#25112;&#65292;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23558;&#25991;&#26723;&#20998;&#21106;&#25104;&#26356;&#23567;&#30340;&#27573;&#33853;&#36827;&#34892;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#20250;&#23548;&#33268;&#26356;&#22823;&#30340;&#21521;&#37327;&#38598;&#21512;&#65292;&#36827;&#32780;&#22686;&#21152;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#19988;&#22312;&#21521;&#37327;&#25628;&#32034;&#26102;&#20250;&#20986;&#29616;&#35745;&#31639;&#23494;&#38598;&#21644;&#24310;&#36831;&#21319;&#39640;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Jina Embeddings 2&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#21487;&#20197;&#23481;&#32435;&#39640;&#36798;8192&#20010;&#26631;&#35760;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#31361;&#30772;&#20256;&#32479;&#30340;512&#20010;&#26631;&#35760;&#38480;&#21046;&#65292;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#38271;&#31687;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embedding models have emerged as powerful tools for transforming sentences into fixed-sized feature vectors that encapsulate semantic information. While these models are essential for tasks like information retrieval, semantic clustering, and text re-ranking, most existing open-source models, especially those built on architectures like BERT, struggle to represent lengthy documents and often resort to truncation. One common approach to mitigate this challenge involves splitting documents into smaller paragraphs for embedding. However, this strategy results in a much larger set of vectors, consequently leading to increased memory consumption and computationally intensive vector searches with elevated latency.  To address these challenges, we introduce Jina Embeddings 2, an open-source text embedding model capable of accommodating up to 8192 tokens. This model is designed to transcend the conventional 512-token limit and adeptly process long documents. Jina Embeddings 2 not only ach
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35270;&#35273;&#38382;&#31572;&#20013;&#24863;&#30693;&#32454;&#23567;&#35270;&#35273;&#32454;&#33410;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35270;&#35273;&#20027;&#39064;&#30340;&#23610;&#23544;&#38750;&#24120;&#25935;&#24863;&#65292;&#36890;&#36807;&#24341;&#20837;&#20154;&#31867;&#21487;&#35270;&#21098;&#35009;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#20854;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.16033</link><description>&lt;p&gt;
ViCrop: &#21033;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35270;&#35273;&#38382;&#31572;&#20013;&#24863;&#30693;&#32454;&#23567;&#35270;&#35273;&#32454;&#33410;
&lt;/p&gt;
&lt;p&gt;
ViCrop: Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal Large Language Models. (arXiv:2310.16033v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35270;&#35273;&#38382;&#31572;&#20013;&#24863;&#30693;&#32454;&#23567;&#35270;&#35273;&#32454;&#33410;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35270;&#35273;&#20027;&#39064;&#30340;&#23610;&#23544;&#38750;&#24120;&#25935;&#24863;&#65292;&#36890;&#36807;&#24341;&#20837;&#20154;&#31867;&#21487;&#35270;&#21098;&#35009;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#22312;&#35270;&#35273;&#38382;&#31572;(VQA)&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#24615;&#65292;&#36825;&#26159;&#19968;&#20010;&#24433;&#21709;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#21644;&#39046;&#22495;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#37492;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#24191;&#27867;&#20351;&#29992;&#28508;&#21147;&#65292;&#30740;&#31350;&#23427;&#20204;&#22312;&#22788;&#29702;&#19981;&#21516;&#30340;&#22270;&#20687;&#21644;&#38382;&#39064;&#23646;&#24615;&#26041;&#38754;&#30340;&#38480;&#21046;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;MLLMs&#26159;&#21542;&#33021;&#22815;&#20687;&#36739;&#22823;&#30340;&#32452;&#20214;&#19968;&#26679;&#24863;&#30693;&#22270;&#20687;&#20013;&#30340;&#32454;&#33410;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#22312;&#22238;&#31572;&#35270;&#35273;&#38382;&#39064;&#26102;&#23545;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35270;&#35273;&#20027;&#39064;&#30340;&#23610;&#23544;&#38750;&#24120;&#25935;&#24863;&#65292;&#24182;&#19988;&#38543;&#30528;&#23610;&#23544;&#30340;&#20943;&#23567;&#65292;&#38646;&#26679;&#26412;&#20934;&#30830;&#24615;&#19979;&#38477;&#22810;&#36798;45.91%&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#35266;&#23519;&#21040;&#20154;&#31867;&#21487;&#35270;&#21098;&#35009;&#21487;&#20197;&#26174;&#33879;&#20943;&#36731;&#20854;&#23545;&#23610;&#23544;&#30340;&#25935;&#24863;&#24615;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#25928;&#24212;&#26159;&#22240;&#26524;&#30340;&#12290;&#20026;&#20102;&#25193;&#22823;&#20154;&#31867;&#21487;&#35270;&#21098;&#35009;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ViCrop&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#33258;&#21160;&#21487;&#35270;&#21098;&#35009;&#26469;&#22686;&#24378;MLLMs&#38646;&#26679;&#26412;VQA&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Models (MLLMs) have recently achieved promising zero-shot accuracy on visual question answering (VQA) -- a fundamental task affecting various downstream applications and domains. Given the great potential for the broad use of these models, it is important to investigate their limitations in dealing with different image and question properties. In this work, we investigate whether MLLMs can perceive details as well as larger components in images. In particular, we show that their zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject related to the question, declining up to $45.91\%$ with size. Furthermore, we show that this effect is causal by observing that human visual cropping can significantly mitigate their sensitivity to size. To scale up the usefulness of human cropping, we propose ViCrop, a general framework that utilizes automatic visual cropping to enhance zero-shot VQA of MLLMs. We construct five variant
&lt;/p&gt;</description></item><item><title>SPEED&#36890;&#36807;&#25512;&#27979;&#25191;&#34892;&#22810;&#20010;&#26410;&#26469;&#26631;&#35760;&#65292;&#21152;&#24555;Transformer&#35299;&#30721;&#22120;&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26102;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.12072</link><description>&lt;p&gt;
SPEED: &#29992;&#20110;&#39640;&#25928;&#35299;&#30721;&#30340;&#25512;&#27979;&#27969;&#27700;&#32447;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
SPEED: Speculative Pipelined Execution for Efficient Decoding. (arXiv:2310.12072v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12072
&lt;/p&gt;
&lt;p&gt;
SPEED&#36890;&#36807;&#25512;&#27979;&#25191;&#34892;&#22810;&#20010;&#26410;&#26469;&#26631;&#35760;&#65292;&#21152;&#24555;Transformer&#35299;&#30721;&#22120;&#30340;&#25512;&#29702;&#25928;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#26102;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36817;&#26469;&#24050;&#25104;&#20026;&#24191;&#27867;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#20027;&#23548;&#22522;&#30784;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#25512;&#29702;&#24310;&#36831;&#26174;&#33879;&#65292;&#23427;&#20204;&#22312;&#23454;&#26102;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#21463;&#21040;&#20102;&#24456;&#22823;&#38480;&#21046;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#29983;&#25104;&#22411;LLM&#25512;&#29702;&#30340;&#33258;&#22238;&#24402;&#29305;&#24615;&#65292;&#20854;&#20013;&#27599;&#20010;&#26631;&#35760;&#20381;&#36182;&#20110;&#25152;&#26377;&#20808;&#21069;&#30340;&#36755;&#20986;&#26631;&#35760;&#65292;&#22240;&#27492;&#24456;&#38590;&#23454;&#29616;&#20219;&#20309;&#26631;&#35760;&#32423;&#30340;&#24182;&#34892;&#24615;&#65292;&#20351;&#24471;&#25512;&#29702;&#36807;&#31243;&#26497;&#24230;&#21463;&#20869;&#23384;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SPEED&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#26089;&#26399;&#38544;&#34255;&#29366;&#24577;&#30340;&#39044;&#27979;&#20540;&#26469;&#24182;&#34892;&#22320;&#25512;&#27979;&#25191;&#34892;&#24403;&#21069;&#26631;&#35760;&#19982;&#22810;&#20010;&#26410;&#26469;&#26631;&#35760;&#65292;&#20174;&#32780;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#23545;&#20110;&#37319;&#29992;&#21442;&#25968;&#20849;&#20139;&#30340;Transformer&#35299;&#30721;&#22120;&#65292;&#21487;&#20197;&#23558;&#24182;&#34892;&#25191;&#34892;&#30340;&#26631;&#35760;&#30340;&#20869;&#23384;&#25805;&#20316;&#20998;&#25674;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
Generative Large Language Models (LLMs) based on the Transformer architecture have recently emerged as a dominant foundation model for a wide range of Natural Language Processing tasks. Nevertheless, their application in real-time scenarios has been highly restricted due to the significant inference latency associated with these models. This is particularly pronounced due to the autoregressive nature of generative LLM inference, where tokens are generated sequentially since each token depends on all previous output tokens. It is therefore challenging to achieve any token-level parallelism, making inference extremely memory-bound. In this work, we propose SPEED, which improves inference efficiency by speculatively executing multiple future tokens in parallel with the current token using predicted values based on early-layer hidden states. For Transformer decoders that employ parameter sharing, the memory operations for the tokens executing in parallel can be amortized, which allows us t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27599;&#20010;&#38454;&#27573;&#23545;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#21644;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.06452</link><description>&lt;p&gt;
&#29702;&#35299;RLHF&#23545;LLM&#27867;&#21270;&#21644;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effects of RLHF on LLM Generalisation and Diversity. (arXiv:2310.06452v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27599;&#20010;&#38454;&#27573;&#23545;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#21644;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;AI&#27169;&#22411;&#20013;&#65292;&#22914;OpenAI&#30340;ChatGPT&#25110;Anthropic&#30340;Claude&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#23613;&#31649;&#22312;&#36825;&#20123;&#26041;&#27861;&#30340;&#24320;&#21457;&#26041;&#38754;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#65292;&#20294;&#25105;&#20204;&#23545;RLHF&#36807;&#31243;&#20013;&#27599;&#20010;&#38454;&#27573;&#30340;&#21033;&#19982;&#24330;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#38454;&#27573;&#65288;&#21363;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#65292;&#22870;&#21169;&#24314;&#27169;&#21644;RLHF&#65289;&#22914;&#20309;&#24433;&#21709;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65306;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#21644;&#36755;&#20986;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#21508;&#31181;&#24773;&#26223;&#30340;&#32972;&#26223;&#19979;&#65292;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#38750;&#24120;&#37325;&#35201;&#65292;&#32780;&#36755;&#20986;&#22810;&#26679;&#24615;&#25351;&#30340;&#26159;&#27169;&#22411;&#29983;&#25104;&#21508;&#31181;&#19981;&#21516;&#36755;&#20986;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#21508;&#31181;&#29992;&#20363;&#26469;&#35828;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#25688;&#35201;&#21644;&#25351;&#20196;&#36981;&#24490;&#20219;&#21153;&#20013;&#23545;&#20004;&#20010;&#22522;&#26412;&#27169;&#22411;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#21518;&#32773;&#38750;&#24120;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT or Anthropic's Claude. % , or Meta's LLaMA-2. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04444</link><description>&lt;p&gt;
&#39764;&#27861;&#35789;&#26159;&#20160;&#20040;&#65311;LLM&#25552;&#31034;&#30340;&#25511;&#21046;&#29702;&#35770;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
What's the Magic Word? A Control Theory of LLM Prompting. (arXiv:2310.04444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32473;&#23450;token&#24207;&#21015;&#26102;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#26368;&#20248;&#25552;&#31034;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65292;&#24182;&#25552;&#20986;&#20102;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#25351;&#26631;&#26469;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#22312;LLM&#30340;&#37096;&#32626;&#20013;&#26159;&#26377;&#25928;&#21644;&#37325;&#35201;&#30340;&#65292;&#20294;&#22312;&#25968;&#23398;&#19978;&#29702;&#35299;&#19981;&#36275;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#25552;&#31034;&#24037;&#31243;&#24418;&#24335;&#21270;&#20026;LLM&#19978;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#25552;&#31034;&#34987;&#35748;&#20026;&#26159;&#35843;&#33410;LLM&#36755;&#20986;&#20998;&#24067;&#30340;&#25511;&#21046;&#21464;&#37327;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;token&#24207;&#21015;&#65292;&#26159;&#21542;&#24635;&#23384;&#22312;&#19968;&#20010;&#25105;&#20204;&#21487;&#20197;&#28155;&#21152;&#30340;&#25552;&#31034;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#30340;token&#65311;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#26368;&#20248;&#25552;&#31034;&#31216;&#20026;&#39764;&#27861;&#35789;&#65292;&#22240;&#20026;&#28155;&#21152;&#25552;&#31034;&#20250;&#23548;&#33268;LLM&#36755;&#20986;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;&#22914;&#26524;&#23384;&#22312;&#39764;&#27861;&#35789;&#65292;&#25105;&#20204;&#33021;&#21542;&#25214;&#21040;&#23427;&#20204;&#65311;&#22914;&#26524;&#21487;&#20197;&#65292;&#23427;&#20204;&#30340;&#29305;&#24615;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#25552;&#20379;&#20102;&#23558;&#25511;&#21046;&#29702;&#35770;&#24212;&#29992;&#20110;&#33258;&#27880;&#24847;&#21147;&#22836;&#30340;&#20998;&#26512;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#20854;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#20540;&#20989;&#25968;&#20026;&#21487;&#25511;&#21046;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#20511;&#37492;&#25511;&#21046;&#29702;&#35770;&#26469;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;$k-\epsilon$&#21487;&#25511;&#21046;&#24615;&#30340;&#25351;&#26631;&#65292;&#29992;&#20110;&#25551;&#36848;LLM&#30340;&#21487;&#25805;&#32437;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering is effective and important in the deployment of LLMs but is poorly understood mathematically. Here, we formalize prompt engineering as an optimal control problem on LLMs -- where the prompt is considered a control variable for modulating the output distribution of the LLM. Within this framework, we ask a simple question: given a sequence of tokens, does there always exist a prompt we can prepend that will steer the LLM toward accurately predicting the final token? We call such an optimal prompt the magic word since prepending the prompt causes the LLM to output the correct answer. If magic words exist, can we find them? If so, what are their properties? We offer analytic analysis on the controllability of the self-attention head where we prove a bound on controllability as a function of the singular values of its weight matrices. We take inspiration from control theory to propose a metric called $k-\epsilon$ controllability to characterize LLM steerability. We comput
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#36827;&#34892;&#27979;&#37327;&#26102;&#65292;&#19981;&#21516;&#30340;&#25552;&#31034;&#20250;&#23548;&#33268;&#38750;&#24120;&#19981;&#21516;&#30340;&#20154;&#26684;&#24471;&#20998;&#65292;&#22240;&#27492;&#32570;&#20047;&#23458;&#35266;&#26631;&#20934;&#26469;&#21028;&#26029;&#21738;&#20010;&#25552;&#31034;&#26356;&#27491;&#30830;&#12290;</title><link>http://arxiv.org/abs/2309.08163</link><description>&lt;p&gt;
&#30740;&#31350;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#27979;&#37327;&#20013;&#30340;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating the Applicability of Self-Assessment Tests for Personality Measurement of Large Language Models. (arXiv:2309.08163v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08163
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#36827;&#34892;&#27979;&#37327;&#26102;&#65292;&#19981;&#21516;&#30340;&#25552;&#31034;&#20250;&#23548;&#33268;&#38750;&#24120;&#19981;&#21516;&#30340;&#20154;&#26684;&#24471;&#20998;&#65292;&#22240;&#27492;&#32570;&#20047;&#23458;&#35266;&#26631;&#20934;&#26469;&#21028;&#26029;&#21738;&#20010;&#25552;&#31034;&#26356;&#27491;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19981;&#26029;&#21457;&#23637;&#65292;&#21508;&#31181;&#26368;&#36817;&#30340;&#30740;&#31350;&#35797;&#22270;&#20351;&#29992;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#34892;&#20026;&#30340;&#24515;&#29702;&#24037;&#20855;&#26469;&#37327;&#21270;&#23427;&#20204;&#30340;&#34892;&#20026;&#12290;&#20854;&#20013;&#19968;&#20010;&#20363;&#23376;&#26159;&#20351;&#29992;&#20154;&#26684;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#26469;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#8220;&#20154;&#26684;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19977;&#20010;&#20851;&#20110;&#20351;&#29992;&#20154;&#26684;&#33258;&#25105;&#35780;&#20272;&#27979;&#35797;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#19977;&#20010;&#19981;&#21516;&#35770;&#25991;&#20013;&#20351;&#29992;&#30340;&#25552;&#31034;&#26469;&#35780;&#20272;&#21516;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#19977;&#20010;&#25552;&#31034;&#23548;&#33268;&#20102;&#38750;&#24120;&#19981;&#21516;&#30340;&#20154;&#26684;&#24471;&#20998;&#12290;&#36825;&#19968;&#31616;&#21333;&#27979;&#35797;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20154;&#26684;&#33258;&#25105;&#35780;&#20272;&#24471;&#20998;&#21462;&#20915;&#20110;&#25552;&#31034;&#32773;&#30340;&#20027;&#35266;&#36873;&#25321;&#12290;&#30001;&#20110;&#25105;&#20204;&#19981;&#30693;&#36947;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#24471;&#20998;&#30340;&#30495;&#23454;&#20540;&#65292;&#22240;&#20026;&#27492;&#31867;&#38382;&#39064;&#27809;&#26377;&#27491;&#30830;&#31572;&#26696;&#65292;&#25152;&#20197;&#26080;&#27861;&#22768;&#26126;&#26576;&#20010;&#25552;&#31034;&#27604;&#20854;&#20182;&#25552;&#31034;&#26356;&#27491;&#30830;&#25110;&#26356;&#19981;&#27491;&#30830;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20154;&#26684;&#36873;&#39033;&#39034;&#24207;&#23545;&#31216;&#24615;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLM) evolve in their capabilities, various recent studies have tried to quantify their behavior using psychological tools created to study human behavior. One such example is the measurement of "personality" of LLMs using personality self-assessment tests. In this paper, we take three such studies on personality measurement of LLMs that use personality self-assessment tests created to study human behavior. We use the prompts used in these three different papers to measure the personality of the same LLM. We find that all three prompts lead very different personality scores. This simple test reveals that personality self-assessment scores in LLMs depend on the subjective choice of the prompter. Since we don't know the ground truth value of personality scores for LLMs as there is no correct answer to such questions, there's no way of claiming if one prompt is more or less correct than the other. We then introduce the property of option order symmetry for persona
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#40784;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;TIAM&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25552;&#31034;&#27169;&#26495;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20013;&#20869;&#23481;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#21253;&#25324;&#23545;&#35937;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#39068;&#33394;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.05134</link><description>&lt;p&gt;
TIAM -- &#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#40784;&#24615;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation. (arXiv:2307.05134v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#23545;&#40784;&#24615;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;TIAM&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25552;&#31034;&#27169;&#26495;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25551;&#36848;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20013;&#20869;&#23481;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#21253;&#25324;&#23545;&#35937;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#39068;&#33394;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#26377;&#24456;&#22823;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#22270;&#20687;&#29983;&#25104;&#30340;&#36827;&#23637;&#20351;&#24471;&#35780;&#20272;&#20854;&#36136;&#37327;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#28210;&#26579;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20294;&#23545;&#20110;&#22522;&#20110;&#25552;&#31034;&#29983;&#25104;&#22270;&#20687;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#32780;&#35328;&#65292;&#32771;&#34385;&#21040;&#29983;&#25104;&#22270;&#20687;&#19982;&#25552;&#31034;&#20013;&#37325;&#35201;&#20869;&#23481;&#20043;&#38388;&#30340;&#30456;&#20284;&#31243;&#24230;&#31561;&#39069;&#22806;&#22240;&#32032;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#29983;&#25104;&#30340;&#22270;&#20687;&#36890;&#24120;&#26159;&#20174;&#38543;&#26426;&#36215;&#22987;&#28857;&#24320;&#22987;&#30340;&#65292;&#20294;&#36890;&#24120;&#19981;&#32771;&#34385;&#36825;&#19968;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#27169;&#26495;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#30740;&#31350;&#25552;&#31034;&#20013;&#25351;&#23450;&#30340;&#20869;&#23481;&#19982;&#29983;&#25104;&#30340;&#22270;&#20687;&#20043;&#38388;&#30340;&#23545;&#40784;&#24615;&#12290;&#23427;&#20801;&#35768;&#25105;&#20204;&#26356;&#22909;&#22320;&#25551;&#36848;&#23545;&#40784;&#24615;&#65292;&#21253;&#25324;&#25351;&#23450;&#23545;&#35937;&#30340;&#31867;&#22411;&#12289;&#25968;&#37327;&#21644;&#39068;&#33394;&#12290;&#25105;&#20204;&#23545;&#20960;&#20010;&#26368;&#36817;&#30340;T2I&#27169;&#22411;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#33719;&#24471;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#39069;&#22806;&#32467;&#26524;&#65292;&#21363;&#22270;&#20687;&#36136;&#37327;&#21487;&#20197;&#22823;&#24133;&#24230;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The progress in the generation of synthetic images has made it crucial to assess their quality. While several metrics have been proposed to assess the rendering of images, it is crucial for Text-to-Image (T2I) models, which generate images based on a prompt, to consider additional aspects such as to which extent the generated image matches the important content of the prompt. Moreover, although the generated images usually result from a random starting point, the influence of this one is generally not considered. In this article, we propose a new metric based on prompt templates to study the alignment between the content specified in the prompt and the corresponding generated images. It allows us to better characterize the alignment in terms of the type of the specified objects, their number, and their color. We conducted a study on several recent T2I models about various aspects. An additional interesting result we obtained with our approach is that image quality can vary drastically 
&lt;/p&gt;</description></item><item><title>LMBot&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#26694;&#26550;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30693;&#35782;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#26080;&#22270;&#24418;&#37096;&#32626;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.17408</link><description>&lt;p&gt;
LMBot: &#23558;&#22270;&#24418;&#30693;&#35782;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#26080;&#22270;&#24418;&#37096;&#32626;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LMBot: Distilling Graph Knowledge into Language Model for Graph-less Deployment in Twitter Bot Detection. (arXiv:2306.17408v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17408
&lt;/p&gt;
&lt;p&gt;
LMBot&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#26694;&#26550;&#65292;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30693;&#35782;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#26080;&#22270;&#24418;&#37096;&#32626;&#65292;&#20197;&#35299;&#20915;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24694;&#24847;&#34892;&#20026;&#32773;&#20351;&#29992;&#36234;&#26469;&#36234;&#20808;&#36827;&#21644;&#24191;&#27867;&#30340;&#26426;&#22120;&#20154;&#26469;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#21644;&#25805;&#32437;&#33286;&#35770;&#65292;&#25512;&#29305;&#26426;&#22120;&#20154;&#30340;&#26816;&#27979;&#24050;&#25104;&#20026;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#24418;&#30340;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#25512;&#29702;&#20381;&#36182;&#20110;&#36317;&#31163;&#30446;&#26631;&#29992;&#25143;&#22810;&#36339;&#30340;&#37051;&#23621;&#29992;&#25143;&#65292;&#24182;&#19988;&#33719;&#21462;&#37051;&#23621;&#29992;&#25143;&#26159;&#32791;&#26102;&#30340;&#65292;&#24182;&#21487;&#33021;&#24341;&#20837;&#20559;&#24046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#19978;&#24494;&#35843;&#21518;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#31454;&#20105;&#24615;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#22270;&#24418;&#32467;&#26500;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#20154;&#26816;&#27979;&#26694;&#26550;LMBot&#65292;&#23427;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#30340;&#30693;&#35782;&#34701;&#20837;&#35821;&#35328;&#27169;&#22411;(LMs)&#65292;&#20197;&#22312;&#25512;&#29305;&#26426;&#22120;&#20154;&#26816;&#27979;&#20013;&#36827;&#34892;&#26080;&#22270;&#24418;&#37096;&#32626;&#65292;&#20197;&#24212;&#23545;&#25968;&#25454;&#20381;&#36182;&#24615;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;LMBot&#23545;&#22522;&#20110;&#22270;&#24418;&#21644;&#19981;&#20351;&#29992;&#22270;&#24418;&#30340;&#25968;&#25454;&#38598;&#20860;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#27599;&#20010;&#29992;&#25143;&#34920;&#31034;&#20026;&#19968;&#27573;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
As malicious actors employ increasingly advanced and widespread bots to disseminate misinformation and manipulate public opinion, the detection of Twitter bots has become a crucial task. Though graph-based Twitter bot detection methods achieve state-of-the-art performance, we find that their inference depends on the neighbor users multi-hop away from the targets, and fetching neighbors is time-consuming and may introduce bias. At the same time, we find that after finetuning on Twitter bot detection, pretrained language models achieve competitive performance and do not require a graph structure during deployment. Inspired by this finding, we propose a novel bot detection framework LMBot that distills the knowledge of graph neural networks (GNNs) into language models (LMs) for graph-less deployment in Twitter bot detection to combat the challenge of data dependency. Moreover, LMBot is compatible with graph-based and graph-less datasets. Specifically, we first represent each user as a tex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#23457;&#35270;&#23186;&#20307;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#21465;&#20107;&#23186;&#20307;&#65292;&#23545;&#20914;&#31361;&#12289;&#21453;&#27966;&#21644;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27880;&#37322;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#20851;&#20110;&#27668;&#20505;&#21464;&#21270;&#26694;&#26550;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#37319;&#29992;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#33258;&#21160;&#22810;&#26631;&#31614;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.02052</link><description>&lt;p&gt;
&#20914;&#31361;&#12289;&#21453;&#27966;&#12289;&#35299;&#20915;&#26041;&#26696;&#65306;&#38754;&#21521;&#21465;&#20107;&#23186;&#20307;&#26694;&#26550;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Conflicts, Villains, Resolutions: Towards models of Narrative Media Framing. (arXiv:2306.02052v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02052
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#23457;&#35270;&#23186;&#20307;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#21465;&#20107;&#23186;&#20307;&#65292;&#23545;&#20914;&#31361;&#12289;&#21453;&#27966;&#21644;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#27880;&#37322;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#20851;&#20110;&#27668;&#20505;&#21464;&#21270;&#26694;&#26550;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#37319;&#29992;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#26041;&#27861;&#36827;&#34892;&#33258;&#21160;&#22810;&#26631;&#31614;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23545;&#23186;&#20307;&#26694;&#26550;&#30340;&#33258;&#21160;&#26816;&#27979;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#65292;&#20294;&#38382;&#39064;&#36890;&#24120;&#34987;&#31616;&#21270;&#20026;&#21333;&#26631;&#31614;&#20998;&#31867;&#65292;&#24182;&#37319;&#29992;&#20102;&#31867;&#20284;&#20027;&#39064;&#30340;&#26694;&#26550;&#35266;&#28857;&#65292;&#36991;&#20813;&#20102;&#23545;&#26356;&#24191;&#27867;&#30340;&#25991;&#26723;&#32423;&#21465;&#20107;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#36890;&#20449;&#31185;&#23398;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#26126;&#30830;&#25429;&#25417;&#20102;&#21465;&#20107;&#30340;&#35201;&#32032;&#65292;&#21253;&#25324;&#20914;&#31361;&#21450;&#20854;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23558;&#20854;&#19982;&#21465;&#20107;&#26694;&#26550;&#20013;&#30340;&#20851;&#38190;&#23454;&#20307;&#65288;&#33521;&#38596;&#12289;&#21463;&#23475;&#32773;&#25110;&#21453;&#27966;&#65289;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#27880;&#37322;&#33539;&#24335;&#65292;&#23558;&#22797;&#26434;&#30340;&#27880;&#37322;&#20219;&#21153;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#36739;&#31616;&#21333;&#30340;&#20108;&#36827;&#21046;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20221;&#33521;&#35821;&#26032;&#38395;&#25991;&#31456;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#26469;&#33258;&#25919;&#27835;&#20809;&#35889;&#30340;&#26032;&#38395;&#23186;&#20307;&#25991;&#31456;&#20013;&#36827;&#34892;&#20102;&#27668;&#20505;&#21464;&#21270;&#26694;&#26550;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#26041;&#27861;&#33258;&#21160;&#22810;&#26631;&#31614;&#39044;&#27979;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite increasing interest in the automatic detection of media frames in NLP, the problem is typically simplified as single-label classification and adopts a topic-like view on frames, evading modelling the broader document-level narrative. In this work, we revisit a widely used conceptualization of framing from the communication sciences which explicitly captures elements of narratives, including conflict and its resolution, and integrate it with the narrative framing of key entities in the story as heroes, victims or villains. We adapt an effective annotation paradigm that breaks a complex annotation task into a series of simpler binary questions, and present an annotated data set of English news articles, and a case study on the framing of climate change in articles from news outlets across the political spectrum. Finally, we explore automatic multi-label prediction of our frames with supervised and semi-supervised approaches, and present a novel retrieval-based method which is bot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#21313;&#20998;&#26377;&#38480;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19555</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#33021;&#20316;&#20026;&#25277;&#35937;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Are Not Abstract Reasoners. (arXiv:2305.19555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#36825;&#26041;&#38754;&#30340;&#34920;&#29616;&#21313;&#20998;&#26377;&#38480;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26497;&#22909;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25991;&#26412;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#31561;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25104;&#21151;&#30340;&#26426;&#21046;&#23578;&#19981;&#28165;&#26970;&#65292;LLMs&#26159;&#21542;&#33021;&#22815;&#36798;&#21040;&#20154;&#31867;&#30340;&#35748;&#30693;&#33021;&#21147;&#25110;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#36824;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#23616;&#38480;&#24615;&#20063;&#19981;&#30830;&#23450;&#12290;&#25277;&#35937;&#25512;&#29702;&#26159;&#35748;&#30693;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#21253;&#25324;&#20174;&#23569;&#37327;&#25968;&#25454;&#20013;&#25214;&#21040;&#21644;&#24212;&#29992;&#19968;&#33324;&#27169;&#24335;&#12290;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32467;&#26500;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#21487;&#20197;&#25581;&#31034;&#23427;&#20204;&#22312;&#25512;&#29702;&#26041;&#38754;&#30340;&#28508;&#22312;&#23616;&#38480;&#24615;&#21644;&#24191;&#27867;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#30446;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#23545;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#38750;&#24120;&#26377;&#38480;&#65292;&#24182;&#25506;&#31350;&#20102;&#36896;&#25104;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have shown tremendous performance on a large variety of natural language processing tasks, ranging from text comprehension to common sense reasoning. However, the mechanisms responsible for this success remain unknown, and it is unclear whether LLMs can achieve human-like cognitive capabilities or whether these models are still fundamentally limited. Abstract reasoning is a fundamental task for cognition, consisting of finding and applying a general pattern from few data. Evaluating deep neural architectures on this task could give insight into their potential limitations regarding reasoning and their broad generalisation abilities, yet this is currently an under-explored area. In this paper, we perform extensive evaluations of state-of-the-art LLMs on abstract reasoning tasks, showing that they achieve very limited performance in contrast with other natural language tasks, and we investigate the reasons for this difference. We apply techniques that have been show
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20020;&#24202;&#35760;&#24405;&#21435;&#35782;&#21035;&#31995;&#32479;&#22312;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#21517;&#31216;&#21435;&#35782;&#21035;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.11348</link><description>&lt;p&gt;
&#20197;&#20844;&#24179;&#21517;&#20041;&#65306;&#35780;&#20272;&#20020;&#24202;&#35760;&#24405;&#21435;&#35782;&#21035;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
In the Name of Fairness: Assessing the Bias in Clinical Record De-identification. (arXiv:2305.11348v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20020;&#24202;&#35760;&#24405;&#21435;&#35782;&#21035;&#31995;&#32479;&#22312;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#65292;&#25581;&#31034;&#20102;&#20854;&#22312;&#21517;&#31216;&#21435;&#35782;&#21035;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20849;&#20139;&#23545;&#20110;&#24320;&#25918;&#31185;&#23398;&#21644;&#21487;&#37325;&#22797;&#30740;&#31350;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#21512;&#27861;&#20849;&#20139;&#20020;&#24202;&#25968;&#25454;&#38656;&#35201;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#21024;&#38500;&#21463;&#20445;&#25252;&#30340;&#20581;&#24247;&#20449;&#24687;&#12290;&#36825;&#20010;&#36807;&#31243;&#65292;&#31216;&#20026;&#21435;&#35782;&#21035;&#65292;&#36890;&#24120;&#36890;&#36807;&#35768;&#22810;&#21830;&#19994;&#21644;&#24320;&#28304;&#31995;&#32479;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#23454;&#29616;&#12290;&#34429;&#28982;&#36825;&#20123;&#31995;&#32479;&#22312;&#24179;&#22343;&#27700;&#24179;&#19978;&#24050;&#32463;&#26174;&#31034;&#20986;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#22312;&#19981;&#21516;&#30340;&#20154;&#21475;&#32676;&#20307;&#20013;&#30340;&#34920;&#29616;&#24046;&#24322;&#36824;&#27809;&#26377;&#24471;&#21040;&#24443;&#24213;&#30340;&#26816;&#26597;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22823;&#35268;&#27169;&#23454;&#35777;&#20998;&#26512;&#65292;&#30740;&#31350;&#20102;&#20020;&#24202;&#31508;&#35760;&#20013;&#30340;&#21517;&#31216;&#21435;&#35782;&#21035;&#31995;&#32479;&#30340;&#20559;&#35265;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;16&#20010;&#21517;&#31216;&#38598;&#65292;&#28085;&#30422;&#20102;&#22235;&#20010;&#20154;&#21475;&#32479;&#35745;&#23398;&#32500;&#24230;&#65306;&#24615;&#21035;&#12289;&#31181;&#26063;&#12289;&#21517;&#31216;&#27969;&#34892;&#24230;&#21644;&#27969;&#34892;&#30340;&#21313;&#24180;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#21517;&#31216;&#25554;&#20837;&#21040;100&#20010;&#25163;&#21160;&#31579;&#36873;&#30340;&#20020;&#24202;&#27169;&#26495;&#20013;&#65292;&#24182;&#35780;&#20272;&#20102;&#20061;&#31181;&#20844;&#20849;&#21644;&#31169;&#20154;&#21435;&#35782;&#21035;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#20020;&#24202;&#35760;&#24405;&#21435;&#35782;&#21035;&#31995;&#32479;&#30340;&#21517;&#31216;&#26041;&#38754;&#23384;&#22312;&#32479;&#35745;&#26174;&#33879;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data sharing is crucial for open science and reproducible research, but the legal sharing of clinical data requires the removal of protected health information from electronic health records. This process, known as de-identification, is often achieved through the use of machine learning algorithms by many commercial and open-source systems. While these systems have shown compelling results on average, the variation in their performance across different demographic groups has not been thoroughly examined. In this work, we investigate the bias of de-identification systems on names in clinical notes via a large-scale empirical analysis. To achieve this, we create 16 name sets that vary along four demographic dimensions: gender, race, name popularity, and the decade of popularity. We insert these names into 100 manually curated clinical templates and evaluate the performance of nine public and private de-identification methods. Our findings reveal that there are statistically significant p
&lt;/p&gt;</description></item></channel></rss>