<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08763</link><description>&lt;p&gt;
&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21487;&#25193;&#23637;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Simple and Scalable Strategies to Continually Pre-train Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08763
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#25968;&#21313;&#20159;&#30340;&#26631;&#35760;&#19978;&#36827;&#34892;&#24120;&#35268;&#39044;&#35757;&#32451;&#65292;&#19968;&#26086;&#26377;&#26032;&#25968;&#25454;&#21487;&#29992;&#23601;&#37325;&#26032;&#24320;&#22987;&#35813;&#36807;&#31243;&#12290;&#19968;&#20010;&#26356;&#26377;&#25928;&#29575;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25345;&#32493;&#39044;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#65292;&#19982;&#37325;&#26032;&#35757;&#32451;&#30456;&#27604;&#33021;&#33410;&#30465;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#26032;&#25968;&#25454;&#24341;&#36215;&#30340;&#20998;&#24067;&#36716;&#31227;&#36890;&#24120;&#20250;&#23548;&#33268;&#22312;&#20197;&#21069;&#25968;&#25454;&#19978;&#38477;&#20302;&#24615;&#33021;&#25110;&#26080;&#27861;&#36866;&#24212;&#26032;&#25968;&#25454;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#65288;LR&#65289;&#37325;&#26032;&#21319;&#28201;&#12289;LR&#37325;&#26032;&#34928;&#20943;&#21644;&#37325;&#25918;&#19978;&#19968;&#25968;&#25454;&#30340;&#32452;&#21512;&#36275;&#20197;&#19982;&#23436;&#20840;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#22312;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#20174;&#26368;&#32456;&#25439;&#22833;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#35780;&#20272;&#22522;&#20934;&#30340;&#35282;&#24230;&#34913;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#24120;&#29992;&#30340;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#33521;&#35821;&#8594;&#33521;&#35821;&#65289;&#20043;&#38388;&#30340;&#24369;&#20294;&#29616;&#23454;&#30340;&#20998;&#24067;&#36716;&#31227;&#20197;&#21450;&#26356;&#24378;&#28872;&#30340;&#20998;&#24067;&#36716;&#31227;&#65288;&#33521;&#35821;&#8594;&#24503;&#35821;&#65289;&#19979;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08763v1 Announce Type: cross  Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#23398;&#20064;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26041;&#27861;DAM&#65292;&#33021;&#22815;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12289;&#26377;&#25928;&#36866;&#24212;&#19981;&#26029;&#21040;&#26469;&#30340;&#25968;&#25454;&#38598;&#12289;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#38598;&#36755;&#20837;&#65292;&#24182;&#20801;&#35768;&#22312;&#31867;&#20284;&#25968;&#25454;&#38598;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.08755</link><description>&lt;p&gt;
DAM:&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#23398;&#20064;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
DAM: Dynamic Adapter Merging for Continual Video QA Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08755
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#23398;&#20064;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26041;&#27861;DAM&#65292;&#33021;&#22815;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12289;&#26377;&#25928;&#36866;&#24212;&#19981;&#26029;&#21040;&#26469;&#30340;&#25968;&#25454;&#38598;&#12289;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#38598;&#36755;&#20837;&#65292;&#24182;&#20801;&#35768;&#22312;&#31867;&#20284;&#25968;&#25454;&#38598;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#65288;VidQA&#65289;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;DAM&#65292;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26469;&#65288;i&#65289;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#65288;ii&#65289;&#23454;&#29616;&#23545;&#25345;&#32493;&#21040;&#36798;&#30340;&#25968;&#25454;&#38598;&#30340;&#39640;&#25928;&#36866;&#24212;&#65292;&#65288;iii&#65289;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#22788;&#29702;&#26469;&#33258;&#26410;&#30693;&#25968;&#25454;&#38598;&#30340;&#36755;&#20837;&#65292;&#65288;iv&#65289;&#23454;&#29616;&#36328;&#30456;&#20284;&#25968;&#25454;&#38598;&#39046;&#22495;&#30340;&#30693;&#35782;&#20849;&#20139;&#12290;&#22312;&#32473;&#23450;&#19968;&#32452;&#25345;&#32493;&#27969;&#24335;&#20256;&#36755;&#30340;VidQA&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#39034;&#24207;&#35757;&#32451;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#36866;&#37197;&#22120;&#65292;&#21516;&#26102;&#20923;&#32467;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#39057;&#35821;&#35328;&#39592;&#24178;&#30340;&#21442;&#25968;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#32473;&#23450;&#26469;&#33258;&#26410;&#30693;&#39046;&#22495;&#30340;&#35270;&#39057;&#38382;&#39064;&#31034;&#20363;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#38750;&#21442;&#25968;&#36335;&#30001;&#22120;&#20989;&#25968;&#35745;&#31639;&#27599;&#20010;&#36866;&#37197;&#22120;&#30340;&#27010;&#29575;&#65292;&#21453;&#26144;&#20986;&#35813;&#36866;&#37197;&#22120;&#19982;&#24403;&#21069;&#35270;&#39057;&#38382;&#39064;&#36755;&#20837;&#23454;&#20363;&#30340;&#30456;&#20851;&#24615;&#12290;&#38543;&#21518;&#65292;&#25152;&#25552;&#20986;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26041;&#26696;&#32858;&#21512;&#25152;&#26377;&#36866;&#37197;&#22120;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08755v1 Announce Type: cross  Abstract: We present a parameter-efficient method for continual video question-answering (VidQA) learning. Our method, named DAM, uses the proposed Dynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable efficient adaptation to continually arriving datasets, (iii) handle inputs from unknown datasets during inference, and (iv) enable knowledge sharing across similar dataset domains. Given a set of continually streaming VidQA datasets, we sequentially train dataset-specific adapters for each dataset while freezing the parameters of a large pretrained video-language backbone. During inference, given a video-question sample from an unknown domain, our method first uses the proposed non-parametric router function to compute a probability for each adapter, reflecting how relevant that adapter is to the current video-question input instance. Subsequently, the proposed dynamic adapter merging scheme aggregates all the adapter weight
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#25351;&#23548;&#35774;&#35745;&#25552;&#31034;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20135;&#29983;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.08743</link><description>&lt;p&gt;
&#23558;LLMs&#24341;&#23548;&#21040;&#26080;&#20559;&#21709;&#24212;&#65306;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#25351;&#23548;&#35774;&#35745;&#25552;&#31034;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20135;&#29983;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24456;&#23481;&#26131;&#20135;&#29983;&#20559;&#35265;&#21644;&#27495;&#35270;&#24615;&#30340;&#21709;&#24212;&#12290;&#30001;&#20110;LLMs&#28041;&#21450;&#21040;&#37325;&#35201;&#30340;&#20915;&#31574;&#21046;&#23450;&#65288;&#20363;&#22914;&#25307;&#32856;&#21644;&#21307;&#30103;&#20445;&#20581;&#65289;&#65292;&#24320;&#21457;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#30340;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#31038;&#20250;&#20559;&#35265;&#65292;&#35299;&#20915;&#20102;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#19982;LLM&#36755;&#20986;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;LLMs&#36755;&#20837;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20197;&#21450;LLM&#25512;&#29702;&#30340;&#20869;&#37096;&#25512;&#29702;&#36807;&#31243;&#30340;&#22240;&#26524;&#29702;&#35299;&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#25351;&#23548;&#21435;&#20559;&#20542;LLM&#36755;&#20986;&#30340;&#25552;&#31034;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#21435;&#20559;&#25351;&#31034;&#26041;&#27861;&#65292;&#22914;&#25233;&#21046;&#25351;&#20196;&#21644;&#19978;&#19979;&#25991;&#23545;&#27604;&#20363;&#23376;&#65292;&#24182;&#36890;&#36807;&#40723;&#21169;&#26080;&#20559;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21551;&#31034;&#20102;&#26032;&#30340;&#21435;&#20559;&#20542;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24378;&#22823;&#23454;&#35777;&#34920;&#29616;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08743v1 Announce Type: cross  Abstract: Large language models (LLMs) can easily generate biased and discriminative responses. As LLMs tap into consequential decision-making (e.g., hiring and healthcare), it is of crucial importance to develop strategies to mitigate these biases. This paper focuses on social bias, tackling the association between demographic information and LLM outputs. We propose a causality-guided debiasing framework that utilizes causal understandings of (1) the data-generating process of the training corpus fed to LLMs, and (2) the internal reasoning process of LLM inference, to guide the design of prompts for debiasing LLM outputs through selection mechanisms. Our framework unifies existing de-biasing prompting approaches such as inhibitive instructions and in-context contrastive examples, and sheds light on new ways of debiasing by encouraging bias-free reasoning. Our strong empirical performance on real-world datasets demonstrates that our framework pr
&lt;/p&gt;</description></item><item><title>&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21160;&#24577;&#21442;&#25968;&#20998;&#24067;&#30340;&#26102;&#38388;&#28436;&#21464;&#65292;&#29305;&#21035;&#26159;&#21449;&#20998;&#25928;&#24212;&#65292;&#33021;&#24110;&#21161;&#29702;&#35299;&#27169;&#22411;&#36136;&#37327;&#65292;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#21644;&#35780;&#20272;&#24037;&#20316;&#65292;&#21516;&#26102;&#23454;&#35777;&#26174;&#31034;&#31232;&#30095;&#26435;&#37325;&#26377;&#25928;&#24615;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2403.08739</link><description>&lt;p&gt;
&#20998;&#21449;&#36335;&#24452;&#30340;&#33457;&#22253;&#65306;&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21160;&#24577;&#21442;&#25968;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08739
&lt;/p&gt;
&lt;p&gt;
&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21160;&#24577;&#21442;&#25968;&#20998;&#24067;&#30340;&#26102;&#38388;&#28436;&#21464;&#65292;&#29305;&#21035;&#26159;&#21449;&#20998;&#25928;&#24212;&#65292;&#33021;&#24110;&#21161;&#29702;&#35299;&#27169;&#22411;&#36136;&#37327;&#65292;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#21644;&#35780;&#20272;&#24037;&#20316;&#65292;&#21516;&#26102;&#23454;&#35777;&#26174;&#31034;&#31232;&#30095;&#26435;&#37325;&#26377;&#25928;&#24615;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29702;&#35299;Transformer&#26550;&#26500;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21331;&#36234;&#24615;&#33021;&#32972;&#21518;&#21407;&#22240;&#26041;&#38754;&#20173;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#12290;&#23588;&#20854;&#26159;&#19968;&#20010;&#23578;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#28041;&#21450;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21442;&#25968;&#20998;&#24067;&#22914;&#20309;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#26426;&#26800;&#25551;&#36848;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#35266;&#23519;&#27169;&#22411;&#21442;&#25968;&#30340;&#32479;&#35745;&#20998;&#24067;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#26041;&#24335;&#65292;&#23588;&#20854;&#26159;&#23545;&#21449;&#20998;&#24433;&#21709;&#65292;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;&#27169;&#22411;&#36136;&#37327;&#65292;&#28508;&#22312;&#22320;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#21644;&#35780;&#20272;&#24037;&#20316;&#65292;&#24182;&#20174;&#23454;&#35777;&#19978;&#23637;&#31034;&#20102;&#31232;&#30095;&#26435;&#37325;&#26377;&#25928;&#24615;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08739v1 Announce Type: cross  Abstract: A substantial gap persists in understanding the reasons behind the exceptional performance of the Transformer architecture in NLP. A particularly unexplored area involves the mechanistic description of how the distribution of parameters evolves over time during training. In this work we suggest that looking at the time evolution of the statistic distribution of model parameters, and specifically at bifurcation effects, can help understanding the model quality, potentially reducing training costs and evaluation efforts and empirically showing the reasons behind the effectiveness of weights sparsification.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#24212;&#33258;&#21160;&#32534;&#30721;&#22120;&#19982;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35821;&#38899;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#25913;&#36827;&#22768;&#23398;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.08738</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#36827;&#34892;&#23545;&#24212;&#35757;&#32451;&#26469;&#25913;&#36827;&#22768;&#23398;&#35789;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Improving Acoustic Word Embeddings through Correspondence Training of Self-supervised Speech Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08738
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#24212;&#33258;&#21160;&#32534;&#30721;&#22120;&#19982;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35821;&#38899;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#25913;&#36827;&#22768;&#23398;&#35789;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#23398;&#35789;&#23884;&#20837;&#65288;AWEs&#65289;&#26159;&#21475;&#35821;&#35789;&#27719;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;&#33719;&#24471;AWEs&#30340;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#26159;&#36890;&#36807;&#23545;&#24212;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;CAE&#65289;&#12290;&#36807;&#21435;&#65292;CAE&#26041;&#27861;&#19968;&#30452;&#19982;&#20256;&#32479;MFCC&#29305;&#24449;&#30456;&#20851;&#32852;&#12290;&#20174;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20026;&#22522;&#30784;&#30340;&#35821;&#38899;&#27169;&#22411;&#65288;&#22914;HuBERT&#12289;Wav2vec2&#31561;&#65289;&#20013;&#33719;&#24471;&#30340;&#34920;&#31034;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;MFCC&#12290;&#20294;&#26159;&#65292;&#22312;&#23398;&#20064;AWEs&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#24182;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;CAE&#19982;&#22522;&#20110;SSL&#30340;&#35821;&#38899;&#34920;&#31034;&#26469;&#33719;&#24471;&#25913;&#36827;&#30340;AWEs&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#25506;&#35752;&#20102;&#22312;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#21033;&#29992;&#22522;&#20110;SSL&#30340;&#35821;&#38899;&#27169;&#22411;&#33719;&#21462;AWEs&#30340;&#33021;&#21147;&#12290;&#22312;&#27874;&#20848;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#27861;&#35821;&#21644;&#33521;&#35821;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#22522;&#20110;HuBERT&#30340;CAE&#27169;&#22411;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#37117;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#35789;&#36776;&#21035;&#32467;&#26524;&#65292;&#23613;&#31649;HuBERT&#20165;&#22312;&#33521;&#35821;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;HuBERT &#22312;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08738v1 Announce Type: new  Abstract: Acoustic word embeddings (AWEs) are vector representations of spoken words. An effective method for obtaining AWEs is the Correspondence Auto-Encoder (CAE). In the past, the CAE method has been associated with traditional MFCC features. Representations obtained from self-supervised learning (SSL)-based speech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many downstream tasks. However, they have not been well studied in the context of learning AWEs. This work explores the effectiveness of CAE with SSL-based speech representations to obtain improved AWEs. Additionally, the capabilities of SSL-based speech models are explored in cross-lingual scenarios for obtaining AWEs. Experiments are conducted on five languages: Polish, Portuguese, Spanish, French, and English. HuBERT-based CAE model achieves the best results for word discrimination in all languages, despite Hu-BERT being pre-trained on English only. Also, the HuBERT
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ILCiteR&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#22522;&#20110;&#35777;&#25454;&#30340;&#23616;&#37096;&#24341;&#25991;&#25512;&#33616;&#20219;&#21153;&#65292;&#20174;&#29616;&#26377;&#30740;&#31350;&#25991;&#29486;&#20013;&#25552;&#21462;&#30456;&#20284;&#35777;&#25454;&#33539;&#22260;&#26469;&#25512;&#33616;&#24341;&#29992;&#30340;&#35770;&#25991;&#65292;&#25552;&#39640;&#20102;&#25512;&#33616;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08737</link><description>&lt;p&gt;
ILCiteR&#65306;&#22522;&#20110;&#35777;&#25454;&#30340;&#21487;&#35299;&#37322;&#23616;&#37096;&#24341;&#25991;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
ILCiteR: Evidence-grounded Interpretable Local Citation Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08737
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ILCiteR&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#22522;&#20110;&#35777;&#25454;&#30340;&#23616;&#37096;&#24341;&#25991;&#25512;&#33616;&#20219;&#21153;&#65292;&#20174;&#29616;&#26377;&#30740;&#31350;&#25991;&#29486;&#20013;&#25552;&#21462;&#30456;&#20284;&#35777;&#25454;&#33539;&#22260;&#26469;&#25512;&#33616;&#24341;&#29992;&#30340;&#35770;&#25991;&#65292;&#25552;&#39640;&#20102;&#25512;&#33616;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#29992;&#20110;&#23616;&#37096;&#24341;&#25991;&#25512;&#33616;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30452;&#25509;&#23558;&#26597;&#35810;&#65288;&#36890;&#24120;&#26159;&#22768;&#26126;&#25110;&#23454;&#20307;&#25552;&#21450;&#65289;&#26144;&#23556;&#25110;&#32763;&#35793;&#20026;&#20540;&#24471;&#24341;&#29992;&#30340;&#30740;&#31350;&#35770;&#25991;&#12290;&#22312;&#36825;&#31181;&#34920;&#36848;&#20013;&#65292;&#24456;&#38590;&#30830;&#23450;&#20026;&#20160;&#20040;&#24212;&#35813;&#20026;&#29305;&#23450;&#26597;&#35810;&#24341;&#29992;&#29305;&#23450;&#30740;&#31350;&#35770;&#25991;&#65292;&#20174;&#32780;&#23548;&#33268;&#25512;&#33616;&#30340;&#21487;&#35299;&#37322;&#24615;&#21463;&#38480;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#35777;&#25454;&#30340;&#23616;&#37096;&#24341;&#25991;&#25512;&#33616;&#20219;&#21153;&#65292;&#20854;&#20013;&#30446;&#26631;&#28508;&#22312;&#31354;&#38388;&#21253;&#25324;&#29992;&#20110;&#25512;&#33616;&#29305;&#23450;&#35770;&#25991;&#30340;&#35777;&#25454;&#33539;&#22260;&#12290;&#36890;&#36807;&#20351;&#29992;&#36828;&#31243;&#30417;&#30563;&#35777;&#25454;&#26816;&#32034;&#21644;&#22810;&#27493;&#37325;&#26032;&#25490;&#24207;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;ILCiteR&#22522;&#20110;&#20174;&#29616;&#26377;&#30740;&#31350;&#25991;&#29486;&#20013;&#25552;&#21462;&#30340;&#30456;&#20284;&#35777;&#25454;&#33539;&#22260;&#21521;&#26597;&#35810;&#25512;&#33616;&#24212;&#24341;&#29992;&#30340;&#35770;&#25991;&#12290;&#19982;&#36807;&#21435;&#31616;&#21333;&#36755;&#20986;&#25512;&#33616;&#30340;&#24418;&#24335;&#19981;&#21516;&#65292;ILCiteR&#26816;&#32034;&#20986;&#32463;&#36807;&#25490;&#21517;&#30340;&#35777;&#25454;&#33539;&#22260;&#21644;&#25512;&#33616;&#30340;&#35770;&#25991;&#23545;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08737v1 Announce Type: cross  Abstract: Existing Machine Learning approaches for local citation recommendation directly map or translate a query, which is typically a claim or an entity mention, to citation-worthy research papers. Within such a formulation, it is challenging to pinpoint why one should cite a specific research paper for a particular query, leading to limited recommendation interpretability. To alleviate this, we introduce the evidence-grounded local citation recommendation task, where the target latent space comprises evidence spans for recommending specific papers. Using a distantly-supervised evidence retrieval and multi-step re-ranking framework, our proposed system, ILCiteR, recommends papers to cite for a query grounded on similar evidence spans extracted from the existing research literature. Unlike past formulations that simply output recommendations, ILCiteR retrieves ranked lists of evidence span and recommended paper pairs. Secondly, previously prop
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24341;&#23548;&#20248;&#20808;&#20248;&#21270;&#65288;BPO&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#21253;&#21547;&#36127;&#38754;&#21709;&#24212;&#30340;&#25968;&#25454;&#38598;&#26469;&#20943;&#36731;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#23545;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#31574;&#30053;&#26469;&#20419;&#36827;&#27169;&#22411;&#22312;&#35270;&#35273;&#36755;&#20837;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.08730</link><description>&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#20248;&#20808;&#20248;&#21270;&#21152;&#24378;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Strengthening Multimodal Large Language Model with Bootstrapped Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08730
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24341;&#23548;&#20248;&#20808;&#20248;&#21270;&#65288;BPO&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#21253;&#21547;&#36127;&#38754;&#21709;&#24212;&#30340;&#25968;&#25454;&#38598;&#26469;&#20943;&#36731;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#23545;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#31574;&#30053;&#26469;&#20419;&#36827;&#27169;&#22411;&#22312;&#35270;&#35273;&#36755;&#20837;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#22522;&#20110;&#35270;&#35273;&#36755;&#20837;&#29983;&#25104;&#21709;&#24212;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#23384;&#22312;&#20559;&#21521;&#20110;&#29983;&#25104;&#19982;&#20854;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30456;&#20284;&#21709;&#24212;&#30340;&#20559;&#35265;&#65292;&#25513;&#30422;&#20102;&#35270;&#35273;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#20559;&#35265;&#35270;&#20026;&#23545;&#39044;&#35757;&#32451;&#32479;&#35745;&#25968;&#25454;&#30340;&#8220;&#20559;&#22909;&#8221;&#65292;&#36825;&#38459;&#30861;&#20102;&#27169;&#22411;&#23545;&#35270;&#35273;&#36755;&#20837;&#30340;&#22522;&#30784;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24341;&#23548;&#20248;&#20808;&#20248;&#21270;&#65288;BPO&#65289;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#21253;&#21547;&#36127;&#38754;&#21709;&#24212;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20559;&#22909;&#23398;&#20064;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#26159;&#20174;&#27169;&#22411;&#26412;&#36523;&#20013;&#24341;&#23548;&#20986;&#26469;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#20004;&#31181;&#31574;&#30053;&#65306;1&#65289;&#20351;&#29992;&#25197;&#26354;&#30340;&#22270;&#20687;&#36755;&#20837;&#21040;MLLM&#20013;&#65292;&#20197;&#24341;&#21457;&#21253;&#21547;&#26174;&#33879;&#39044;&#35757;&#32451;&#20559;&#35265;&#30340;&#21709;&#24212;&#65307;2&#65289;&#21033;&#29992;&#22522;&#20110;&#25991;&#26412;&#30340;LLM&#23558;&#38169;&#35823;&#20294;&#24120;&#35265;&#30340;&#20803;&#32032;&#26126;&#30830;&#22320;&#27880;&#20837;&#21407;&#22987;&#21709;&#24212;&#20013;&#12290;&#36825;&#20123;&#19981;&#33391;&#21709;&#24212;&#19982;&#25968;&#25454;&#38598;&#20013;&#21407;&#22987;&#30340;&#27880;&#37322;&#21709;&#24212;&#37197;&#23545;&#65292;&#26500;&#24314;&#20102;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08730v1 Announce Type: new  Abstract: Multimodal Large Language Models (MLLMs) excel in generating responses based on visual inputs. However, they often suffer from a bias towards generating responses similar to their pretraining corpus, overshadowing the importance of visual information. We treat this bias as a "preference" for pretraining statistics, which hinders the model's grounding in visual input. To mitigate this issue, we propose Bootstrapped Preference Optimization (BPO), which conducts preference learning with datasets containing negative responses bootstrapped from the model itself. Specifically, we propose the following two strategies: 1) using distorted image inputs to the MLLM for eliciting responses that contain signified pretraining bias; 2) leveraging text-based LLM to explicitly inject erroneous but common elements into the original response. Those undesirable responses are paired with original annotated responses from the datasets to construct the prefere
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#23398;&#20064;&#26041;&#27861;SOTOPIA-$\pi$&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#34892;&#20026;&#20811;&#38534;&#21644;&#33258;&#25105;&#24378;&#21270;&#35757;&#32451;&#65292;&#25913;&#36827;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#31038;&#20132;&#26234;&#33021;&#65292;&#20351;&#20854;&#36798;&#21040;&#20102;&#19987;&#23478;&#27169;&#22411;&#30340;&#27700;&#24179;&#65292;&#24182;&#25552;&#39640;&#20102;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08715</link><description>&lt;p&gt;
SOTOPIA-$\pi$: &#20132;&#20114;&#24335;&#23398;&#20064;&#31038;&#20132;&#26234;&#33021;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SOTOPIA-$\pi$: Interactive Learning of Socially Intelligent Language Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08715
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#23398;&#20064;&#26041;&#27861;SOTOPIA-$\pi$&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#34892;&#20026;&#20811;&#38534;&#21644;&#33258;&#25105;&#24378;&#21270;&#35757;&#32451;&#65292;&#25913;&#36827;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#31038;&#20132;&#26234;&#33021;&#65292;&#20351;&#20854;&#36798;&#21040;&#20102;&#19987;&#23478;&#27169;&#22411;&#30340;&#27700;&#24179;&#65292;&#24182;&#25552;&#39640;&#20102;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#27169;&#20223;&#21644;&#31038;&#20132;&#20114;&#21160;&#26469;&#23398;&#20064;&#31038;&#20132;&#25216;&#33021;&#12290;&#29616;&#26377;&#30740;&#31350;&#22312;&#26500;&#24314;&#35821;&#35328;&#20195;&#29702;&#26041;&#38754;&#24456;&#23569;&#28041;&#21450;&#36825;&#31181;&#31038;&#20132;&#23398;&#20064;&#36807;&#31243;&#12290;&#21463;&#21040;&#36825;&#19968;&#31354;&#30333;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#23398;&#20064;&#26041;&#27861;SOTOPIA-$\pi$&#65292;&#25913;&#36827;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#31038;&#20132;&#26234;&#33021;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#34892;&#20026;&#20811;&#38534;&#21644;&#33258;&#25105;&#24378;&#21270;&#35757;&#32451;&#65292;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#35780;&#20998;&#23545;&#32463;&#36807;&#31579;&#36873;&#30340;&#31038;&#20132;&#20114;&#21160;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#20351;&#19968;&#20010;7B&#30340;LLM&#36798;&#21040;&#20102;&#19987;&#23478;&#27169;&#22411;(GPT-4-based agent)&#30340;&#31038;&#20132;&#30446;&#26631;&#23436;&#25104;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#35821;&#35328;&#20195;&#29702;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#22312;MMLU&#22522;&#20934;&#19978;&#20445;&#25345;&#20102;&#36890;&#29992;&#30340;&#38382;&#31572;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#36825;&#31181;&#35757;&#32451;&#33539;&#24335;&#25581;&#31034;&#20102;LLM&#35780;&#20272;&#31038;&#20132;&#26234;&#33021;&#30340;&#19968;&#20123;&#22256;&#38590;&#65306;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#32773;&#39640;&#20272;&#20102;&#19987;&#38376;&#38024;&#23545;&#31038;&#20132;&#20114;&#21160;&#35757;&#32451;&#30340;&#35821;&#35328;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08715v1 Announce Type: new  Abstract: Humans learn social skills through both imitation and social interaction. This social learning process is largely understudied by existing research on building language agents. Motivated by this gap, we propose an interactive learning method, SOTOPIA-$\pi$, improving the social intelligence of language agents. This method leverages behavior cloning and self-reinforcement training on filtered social interaction data according to large language model (LLM) ratings. We show that our training method allows a 7B LLM to reach the social goal completion ability of an expert model (GPT-4-based agent), while improving the safety of language agents and maintaining general QA ability on the MMLU benchmark. We also find that this training paradigm uncovers some difficulties in LLM-based evaluation of social intelligence: LLM-based evaluators overestimate the abilities of the language agents trained specifically for social interaction.
&lt;/p&gt;</description></item><item><title>TeaMs-RL&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30452;&#25509;&#29983;&#25104;&#22522;&#30784;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#20943;&#23569;&#23545;&#20154;&#31867;&#30340;&#20381;&#36182;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#20026;&#21333;&#19968;&#24494;&#35843;&#27493;&#39588;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.08694</link><description>&lt;p&gt;
TeaMs-RL&#65306;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25945;&#25480;LLMs&#26356;&#22909;&#22320;&#33258;&#25105;&#25351;&#23548;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TeaMs-RL: Teaching LLMs to Teach Themselves Better Instructions via Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08694
&lt;/p&gt;
&lt;p&gt;
TeaMs-RL&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30452;&#25509;&#29983;&#25104;&#22522;&#30784;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#20943;&#23569;&#23545;&#20154;&#31867;&#30340;&#20381;&#36182;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#20026;&#21333;&#19968;&#24494;&#35843;&#27493;&#39588;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#36890;&#24120;&#38754;&#20020;&#30528;&#22312;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26694;&#26550;&#20013;&#23545;&#20154;&#31867;&#26631;&#27880;&#21592;&#30340;&#20005;&#37325;&#20381;&#36182;&#25110;&#19982;&#33258;&#25105;&#25351;&#23548;&#33539;&#24335;&#30456;&#20851;&#30340;&#39057;&#32321;&#19988;&#26114;&#36149;&#30340;&#22806;&#37096;&#26597;&#35810;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36716;&#21521;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;-- &#20294;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#20559;&#31163;&#20102;&#20856;&#22411;&#30340;RLHF&#65292;&#21518;&#32773;&#22312;&#25351;&#23548;&#25968;&#25454;&#35757;&#32451;&#21518;&#20248;&#21270;LLMs&#65292;&#32780;&#25105;&#20204;&#20351;&#29992;RL&#30452;&#25509;&#29983;&#25104;&#21333;&#29420;&#36275;&#20197;&#36827;&#34892;&#24494;&#35843;&#30340;&#22522;&#30784;&#25351;&#23548;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;TeaMs-RL&#20351;&#29992;&#19968;&#31995;&#21015;&#25991;&#26412;&#25805;&#20316;&#21644;&#35268;&#21017;&#65292;&#20248;&#20808;&#32771;&#34385;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#21270;&#12290;&#23427;&#20419;&#36827;&#20102;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#29983;&#25104;&#65292;&#32780;&#19981;&#36807;&#20110;&#20381;&#36182;&#22806;&#37096;&#20808;&#36827;&#27169;&#22411;&#65292;&#20026;&#21333;&#19968;&#24494;&#35843;&#27493;&#39588;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#28040;&#38500;&#20102;&#38543;&#21518;&#30340;RLHF&#38454;&#27573;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#31361;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#20248;&#21183;&#65306;&#20943;&#23569;&#23545;&#20154;&#31867;&#30340;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08694v1 Announce Type: new  Abstract: The development of Large Language Models (LLMs) often confronts challenges stemming from the heavy reliance on human annotators in the reinforcement learning with human feedback (RLHF) framework, or the frequent and costly external queries tied to the self-instruct paradigm. In this work, we pivot to Reinforcement Learning (RL) -- but with a twist. Diverging from the typical RLHF, which refines LLMs following instruction data training, we use RL to directly generate the foundational instruction dataset that alone suffices for fine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and rules, prioritizing the diversification of training datasets. It facilitates the generation of high-quality data without excessive reliance on external advanced models, paving the way for a single fine-tuning step and negating the need for subsequent RLHF stages. Our findings highlight key advantages of our approach: reduced need for human inv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#36328;11&#31181;&#35821;&#35328;&#30340;&#22823;&#22411;&#32593;&#39029;&#25235;&#21462;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;MaCoCu&#21644;OSCAR&#30340;&#34920;&#29616;&#26368;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.08693</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#24847;&#25991;&#26412;&#36136;&#37327;&#21527;? &#35780;&#20272;&#36328;11&#31181;&#35821;&#35328;&#30340;&#32593;&#39029;&#25235;&#21462;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Do Language Models Care About Text Quality? Evaluating Web-Crawled Corpora Across 11 Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#36328;11&#31181;&#35821;&#35328;&#30340;&#22823;&#22411;&#32593;&#39029;&#25235;&#21462;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;MaCoCu&#21644;OSCAR&#30340;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#12289;&#31574;&#21010;&#12289;&#32593;&#39029;&#25235;&#21462;&#30340;&#35821;&#26009;&#24211;&#22312;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#23427;&#20204;&#26500;&#25104;&#20102;&#20960;&#20046;&#25152;&#26377;&#26368;&#36817;LMs&#35757;&#32451;&#25968;&#25454;&#30340;&#20027;&#35201;&#37096;&#20998;&#65292;&#22914;&#24191;&#20026;&#20154;&#30693;&#30340;GPT&#12289;LLaMA&#21644;XLM-RoBERTa&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36825;&#19968;&#37325;&#35201;&#24615;&#65292;&#23545;&#36825;&#20123;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#20851;&#27880;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#30446;&#21069;&#26368;&#30456;&#20851;&#30340;&#22235;&#20010;&#22823;&#22411;&#12289;&#32593;&#39029;&#25235;&#21462;&#30340;&#35821;&#26009;&#24211;&#65288;CC100&#12289;MaCoCu&#12289;mC4&#21644;OSCAR&#65289;&#36328;11&#31181;&#27431;&#27954;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21452;&#37325;&#30340;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#26469;&#33258;&#19981;&#21516;&#35821;&#26009;&#24211;&#30340;&#26679;&#26412;&#36827;&#34892;&#20154;&#24037;&#35780;&#20272;&#26469;&#36827;&#34892;&#20869;&#22312;&#35780;&#20272;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36136;&#37327;&#24046;&#24322;&#30340;&#23454;&#38469;&#24433;&#21709;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#29305;&#23450;&#30340;LMs&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;MaCoCu&#21644;OSCAR&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08693v1 Announce Type: new  Abstract: Large, curated, web-crawled corpora play a vital role in training language models (LMs). They form the lion's share of the training data in virtually all recent LMs, such as the well-known GPT, LLaMA and XLM-RoBERTa models. However, despite this importance, relatively little attention has been given to the quality of these corpora. In this paper, we compare four of the currently most relevant large, web-crawled corpora (CC100, MaCoCu, mC4 and OSCAR) across eleven lower-resourced European languages. Our approach is two-fold: first, we perform an intrinsic evaluation by performing a human evaluation of the quality of samples taken from different corpora; then, we assess the practical impact of the qualitative differences by training specific LMs on each of the corpora and evaluating their performance on downstream tasks. We find that there are clear differences in quality of the corpora, with MaCoCu and OSCAR obtaining the best results. Ho
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23383;&#31526;&#21305;&#37197;&#23454;&#29616;&#26631;&#35760;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#23545;&#40784;&#30340;&#24773;&#26223;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#31354;&#26684;&#21069;&#32512;&#21644;&#37096;&#20998;&#32553;&#36827;&#31561;&#24494;&#22937;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.08688</link><description>&lt;p&gt;
&#36890;&#36807;&#23383;&#31526;&#21305;&#37197;&#23454;&#29616;&#26631;&#35760;&#23545;&#40784;&#29992;&#20110;&#23376;&#35789;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Token Alignment via Character Matching for Subword Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08688
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23383;&#31526;&#21305;&#37197;&#23454;&#29616;&#26631;&#35760;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#23545;&#40784;&#30340;&#24773;&#26223;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#31354;&#26684;&#21069;&#32512;&#21644;&#37096;&#20998;&#32553;&#36827;&#31561;&#24494;&#22937;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#19982;&#37096;&#20998;&#26631;&#35760;&#23545;&#40784;&#30340;&#25552;&#31034;&#12290;&#36825;&#31181;&#22256;&#38590;&#28304;&#33258;&#26631;&#35760;&#21270;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#37096;&#20998;&#26631;&#35760;&#20250;&#33073;&#31163;&#20998;&#24067;&#65292;&#23548;&#33268;&#19981;&#27491;&#30830;&#25110;&#33618;&#35884;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#20943;&#36731;&#29983;&#25104;&#27169;&#22411;&#20013;&#25991;&#26412;&#34917;&#20840;&#26102;&#30340;&#26631;&#35760;&#21270;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#24120;&#35268;&#38750;&#23376;&#35789;&#24773;&#20917;&#19979;&#20063;&#33021;&#20445;&#25345;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#31216;&#20026;&#26631;&#35760;&#23545;&#40784;&#65292;&#28041;&#21450;&#22238;&#28335;&#21040;&#26368;&#21518;&#23436;&#25972;&#26631;&#35760;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#29983;&#25104;&#19982;&#25552;&#31034;&#23545;&#40784;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35768;&#22810;&#37096;&#20998;&#26631;&#35760;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#31354;&#26684;&#21069;&#32512;&#21644;&#37096;&#20998;&#32553;&#36827;&#31561;&#24494;&#22937;&#24773;&#20917;&#65292;&#20165;&#22686;&#21152;&#20102;&#23569;&#37327;&#26102;&#38388;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#30340;&#25216;&#26415;&#21644;&#20998;&#26512;&#26377;&#21161;&#20110;&#22312;&#22788;&#29702;&#37096;&#20998;&#36755;&#20837;&#26041;&#38754;&#19981;&#26029;&#25512;&#36827;&#29983;&#25104;&#27169;&#22411;&#65292;&#23545;&#35832;&#22914;&#30340;&#24212;&#29992;&#20855;&#26377;&#30456;&#20851;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08688v1 Announce Type: cross  Abstract: Generative models, widely utilized in various applications, can often struggle with prompts corresponding to partial tokens. This struggle stems from tokenization, where partial tokens fall out of distribution during inference, leading to incorrect or nonsensical outputs. This paper examines a technique to alleviate the tokenization artifact on text completion in generative models, maintaining performance even in regular non-subword cases. The method, termed token alignment, involves backtracking to the last complete tokens and ensuring the model's generation aligns with the prompt. This approach showcases marked improvement across many partial token scenarios, including nuanced cases like space-prefix and partial indentation, with only a minor time increase. The technique and analysis detailed in this paper contribute to the continuous advancement of generative models in handling partial inputs, bearing relevance for applications like
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21307;&#30103;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08664</link><description>&lt;p&gt;
&#29992;&#20110;&#20154;&#24037;&#20020;&#24202;&#35760;&#24405;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#29983;&#25104;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Zero-shot and Few-shot Generation Strategies for Artificial Clinical Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08664
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#21307;&#30103;&#35760;&#24405;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#65292;&#36991;&#20813;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#30495;&#23454;&#24739;&#32773;&#25968;&#25454;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#21307;&#30103;&#35760;&#24405;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;Llama 2 LLM&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#29983;&#25104;&#21487;&#20934;&#30830;&#21453;&#26144;&#30495;&#23454;&#24739;&#32773;&#20449;&#24687;&#30340;&#21512;&#25104;&#21307;&#30103;&#35760;&#24405;&#65292;&#19982;&#38656;&#35201;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#25935;&#24863;&#24739;&#32773;&#25968;&#25454;&#30340;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08664v1 Announce Type: new  Abstract: The challenge of accessing historical patient data for clinical research, while adhering to privacy regulations, is a significant obstacle in medical science. An innovative approach to circumvent this issue involves utilising synthetic medical records that mirror real patient data without compromising individual privacy. The creation of these synthetic datasets, particularly without using actual patient data to train Large Language Models (LLMs), presents a novel solution as gaining access to sensitive patient information to train models is also a challenge. This study assesses the capability of the Llama 2 LLM to create synthetic medical records that accurately reflect real patient information, employing zero-shot and few-shot prompting strategies for comparison against fine-tuned methodologies that do require sensitive patient data during training. We focus on generating synthetic narratives for the History of Present Illness section, 
&lt;/p&gt;</description></item><item><title>MedInsight&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#26469;&#28304;&#25552;&#21462;&#19982;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#30456;&#20851;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#23558;&#20854;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20837;&#32467;&#21512;&#65292;&#29983;&#25104;&#20016;&#23500;&#30340;&#12289;&#38024;&#23545;&#24739;&#32773;&#30340;&#21709;&#24212;</title><link>https://arxiv.org/abs/2403.08607</link><description>&lt;p&gt;
MedInsight&#65306;&#29992;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#21307;&#30103;&#21709;&#24212;&#30340;&#22810;&#28304;&#19978;&#19979;&#25991;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MedInsight: A Multi-Source Context Augmentation Framework for Generating Patient-Centric Medical Responses using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08607
&lt;/p&gt;
&lt;p&gt;
MedInsight&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#26469;&#28304;&#25552;&#21462;&#19982;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#30456;&#20851;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#23558;&#20854;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20837;&#32467;&#21512;&#65292;&#29983;&#25104;&#20016;&#23500;&#30340;&#12289;&#38024;&#23545;&#24739;&#32773;&#30340;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#31867;&#20154;&#21709;&#24212;&#26041;&#38754;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21307;&#30103;&#29615;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#32780;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#20855;&#26377;&#19978;&#19979;&#25991;&#21644;&#20840;&#38754;&#24615;&#21709;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#23454;&#29616;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#21644;&#20840;&#38754;&#24615;&#30340;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#21709;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MedInsight&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#22810;&#20010;&#26469;&#28304;&#30340;&#30456;&#20851;&#32972;&#26223;&#20449;&#24687;&#22686;&#24378;LLM&#36755;&#20837;&#65288;&#25552;&#31034;&#65289;&#12290;MedInsight&#20174;&#24739;&#32773;&#30340;&#30149;&#21382;&#25110;&#20250;&#35786;&#35760;&#24405;&#20013;&#25552;&#21462;&#30456;&#20851;&#35814;&#32454;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#24739;&#32773;&#30340;&#20581;&#24247;&#21382;&#21490;&#21644;&#29366;&#20917;&#65292;&#38598;&#25104;&#26469;&#33258;&#26435;&#23041;&#21307;&#23398;&#25945;&#31185;&#20070;&#21644;&#31574;&#21010;&#30340;&#32593;&#32476;&#36164;&#28304;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23558;&#24739;&#32773;&#35760;&#24405;&#19982;&#30456;&#20851;&#21307;&#23398;&#30693;&#35782;&#30456;&#32467;&#21512;&#30340;&#22686;&#24378;&#19978;&#19979;&#25991;&#65292;MedInsight&#29983;&#25104;&#20016;&#23500;&#30340;&#12289;&#29305;&#23450;&#20110;&#24739;&#32773;&#30340;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08607v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown impressive capabilities in generating human-like responses. However, their lack of domain-specific knowledge limits their applicability in healthcare settings, where contextual and comprehensive responses are vital. To address this challenge and enable the generation of patient-centric responses that are contextually relevant and comprehensive, we propose MedInsight:a novel retrieval augmented framework that augments LLM inputs (prompts) with relevant background information from multiple sources. MedInsight extracts pertinent details from the patient's medical record or consultation transcript. It then integrates information from authoritative medical textbooks and curated web resources based on the patient's health history and condition. By constructing an augmented context combining the patient's record with relevant medical knowledge, MedInsight generates enriched, patient-specific responses t
&lt;/p&gt;</description></item><item><title>DevBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#21508;&#20010;&#38454;&#27573;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#27169;&#22411;&#22312;&#20854;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.08604</link><description>&lt;p&gt;
DevBench&#65306;&#36719;&#20214;&#24320;&#21457;&#30340;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
DevBench: A Comprehensive Benchmark for Software Development
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08604
&lt;/p&gt;
&lt;p&gt;
DevBench&#26159;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#21508;&#20010;&#38454;&#27573;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#27169;&#22411;&#22312;&#20854;&#20013;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08604v1&#23459;&#24067;&#31867;&#22411;&#65306;&#26032;&#30340;&#25688;&#35201;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#33879;&#25552;&#21319;&#20102;&#23427;&#20204;&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20027;&#35201;&#20851;&#27880;&#32534;&#31243;&#30340;&#31616;&#21270;&#25110;&#23396;&#31435;&#26041;&#38754;&#65292;&#22914;&#21333;&#25991;&#20214;&#20195;&#30721;&#29983;&#25104;&#25110;&#23384;&#20648;&#24211;&#38382;&#39064;&#35843;&#35797;&#65292;&#26410;&#33021;&#20840;&#38754;&#34913;&#37327;&#30001;&#30495;&#23454;&#19990;&#30028;&#32534;&#31243;&#27963;&#21160;&#25552;&#20986;&#30340;&#21508;&#31181;&#25361;&#25112;&#30340;&#20840;&#35889;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DevBench&#65292;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;LLMs&#22312;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#65292;&#21253;&#25324;&#36719;&#20214;&#35774;&#35745;&#12289;&#29615;&#22659;&#35774;&#32622;&#12289;&#23454;&#29616;&#12289;&#39564;&#25910;&#27979;&#35797;&#21644;&#21333;&#20803;&#27979;&#35797;&#12290;DevBench&#20855;&#26377;&#21508;&#31181;&#32534;&#31243;&#35821;&#35328;&#21644;&#39046;&#22495;&#65292;&#39640;&#36136;&#37327;&#25968;&#25454;&#25910;&#38598;&#65292;&#24182;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#31934;&#24515;&#35774;&#35745;&#21644;&#39564;&#35777;&#30340;&#25351;&#26631;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;LLMs&#65292;&#21253;&#25324;GPT-4-Turbo&#65292;&#26080;&#27861;&#35299;&#20915;DevBench&#25552;&#20986;&#30340;&#25361;&#25112;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#27169;&#22411;&#38590;&#20197;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08604v1 Announce Type: new  Abstract: Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities. However, existing benchmarks predominantly focused on simplified or isolated aspects of programming, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. To this end, we propose DevBench, a comprehensive benchmark that evaluates LLMs across various stages of the software development lifecycle, including software design, environment setup, implementation, acceptance testing, and unit testing. DevBench features a wide range of programming languages and domains, high-quality data collection, and carefully designed and verified metrics for each task. Empirical studies show that current LLMs, including GPT-4-Turbo, fail to solve the challenges presented within DevBench. Analyses reveal that models struggle with understand
&lt;/p&gt;</description></item><item><title>LLMs&#20511;&#21161;Reasoning-Path-Editing (Readi)&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#24544;&#23454;&#22320;&#25512;&#29702;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22810;&#20010;KGQA&#21644;TableQA&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.08593</link><description>&lt;p&gt;
&#24403;&#38656;&#35201;&#26102;&#32473;&#25105;&#25171;&#30005;&#35805;&#65306;LLM&#21487;&#20197;&#39640;&#25928;&#32780;&#24544;&#23454;&#22320;&#25512;&#29702;&#32467;&#26500;&#21270;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08593
&lt;/p&gt;
&lt;p&gt;
LLMs&#20511;&#21161;Reasoning-Path-Editing (Readi)&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#24544;&#23454;&#22320;&#25512;&#29702;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22810;&#20010;KGQA&#21644;TableQA&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#25512;&#29702;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#65292;&#20363;&#22914;&#30693;&#35782;&#22270;&#35889;&#21644;&#34920;&#26684;&#12290;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#22810;&#36339;&#25512;&#29702;&#65292;&#21363;&#23558;&#33258;&#28982;&#35821;&#35328;&#35805;&#35821;&#19982;&#29615;&#22659;&#20013;&#30340;&#23454;&#20363;&#21305;&#37197;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#21033;&#29992;LLMs&#36880;&#27493;&#26500;&#24314;&#25512;&#29702;&#36335;&#24452;&#65292;&#20854;&#20013;LLMs&#36890;&#36807;&#19982;&#29615;&#22659;&#36880;&#27493;&#20132;&#20114;&#26469;&#35843;&#29992;&#24037;&#20855;&#25110;&#36873;&#25321;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;Reasoning-Path-Editing&#65288;Readi&#65289;&#65292;&#22312;&#20854;&#20013;LLMs&#21487;&#20197;&#39640;&#25928;&#19988;&#24544;&#23454;&#22320;&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#36827;&#34892;&#25512;&#29702;&#12290;&#22312;Readi&#20013;&#65292;LLMs&#22312;&#32473;&#23450;&#26597;&#35810;&#26102;&#26368;&#21021;&#29983;&#25104;&#19968;&#20010;&#25512;&#29702;&#36335;&#24452;&#65292;&#21482;&#26377;&#22312;&#24517;&#35201;&#26102;&#25165;&#32534;&#36753;&#36335;&#24452;&#12290;&#25105;&#20204;&#23558;&#36335;&#24452;&#23454;&#20363;&#21270;&#21040;&#32467;&#26500;&#21270;&#29615;&#22659;&#19978;&#65292;&#24182;&#22312;&#20986;&#29616;&#38382;&#39064;&#26102;&#25552;&#20379;&#21453;&#39304;&#20197;&#32534;&#36753;&#36335;&#24452;&#12290;&#23545;&#19977;&#20010;KGQA&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;TableQA&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;Readi&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#33879;&#36229;&#36234;&#20102;&#25152;&#26377;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#65288;&#22312;WebQ&#19978;&#25552;&#39640;&#20102;9.1&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08593v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown potential in reasoning over structured environments, e.g., knowledge graph and table. Such tasks typically require multi-hop reasoning, i.e., match natural language utterance with instances in the environment. Previous methods leverage LLMs to incrementally build a reasoning path, where the LLMs either invoke tools or pick up schemas by step-by-step interacting with the environment. We propose Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently and faithfully reason over structured environments. In Readi, LLMs initially generate a reasoning path given a query, and edit the path only when necessary. We instantiate the path on structured environments and provide feedback to edit the path if anything goes wrong. Experimental results on three KGQA datasets and two TableQA datasets show the effectiveness of Readi, significantly surpassing all LLM-based methods (by 9.1% on WebQ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21644;&#37327;&#21270;&#24615;&#21035;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#38750;&#27495;&#35270;&#26631;&#20934;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.08564</link><description>&lt;p&gt;
&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#38750;&#27495;&#35270;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Non-discrimination Criteria for Generative Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21644;&#37327;&#21270;&#24615;&#21035;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#38750;&#27495;&#35270;&#26631;&#20934;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32463;&#21382;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#36234;&#26469;&#36234;&#26222;&#36941;&#22320;&#25552;&#20379;&#32473;&#20844;&#20247;&#20351;&#29992;&#65292;&#20154;&#20204;&#24320;&#22987;&#25285;&#24515;&#22312;&#24212;&#29992;&#20013;&#24310;&#32493;&#21644;&#25918;&#22823;&#26377;&#23475;&#20559;&#35265;&#30340;&#38382;&#39064;&#12290;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#21487;&#33021;&#23545;&#20854;&#38024;&#23545;&#30340;&#20010;&#20154;&#36896;&#25104;&#20260;&#23475;&#21644;&#38480;&#21046;&#65292;&#26080;&#35770;&#26159;&#30001;&#35823;&#20256;&#36824;&#26159;&#27495;&#35270;&#25152;&#26500;&#25104;&#12290;&#35782;&#21035;&#24615;&#21035;&#20559;&#35265;&#20316;&#20026;&#19968;&#31181;&#26222;&#36941;&#30340;&#31038;&#20250;&#26500;&#36896;&#65292;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#21457;&#29616;&#21644;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19977;&#20010;&#26469;&#33258;&#20998;&#31867;&#30340;&#33879;&#21517;&#38750;&#27495;&#35270;&#26631;&#20934;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31867;&#27604;&#65292;&#21363;&#29420;&#31435;&#24615;&#12289;&#20998;&#31163;&#24615;&#21644;&#20805;&#20998;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#20123;&#26631;&#20934;&#30340;&#20316;&#29992;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#38024;&#23545;&#27599;&#20010;&#26631;&#20934;&#30340;&#25552;&#31034;&#65292;&#37325;&#28857;&#20851;&#27880;&#32844;&#19994;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#65292;&#20855;&#20307;&#21033;&#29992;&#21307;&#23398;&#27979;&#35797;&#26469;&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#32972;&#26223;&#20013;&#24341;&#20837;&#22522;&#26412;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08564v1 Announce Type: cross  Abstract: Within recent years, generative AI, such as large language models, has undergone rapid development. As these models become increasingly available to the public, concerns arise about perpetuating and amplifying harmful biases in applications. Gender stereotypes can be harmful and limiting for the individuals they target, whether they consist of misrepresentation or discrimination. Recognizing gender bias as a pervasive societal construct, this paper studies how to uncover and quantify the presence of gender biases in generative language models. In particular, we derive generative AI analogues of three well-known non-discrimination criteria from classification, namely independence, separation and sufficiency. To demonstrate these criteria in action, we design prompts for each of the criteria with a focus on occupational gender stereotype, specifically utilizing the medical test to introduce the ground truth in the generative AI context. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#32553;&#25918;&#30740;&#31350;&#20013;&#36807;&#24230;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#35780;&#20272;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.08540</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19982;&#36807;&#24230;&#35757;&#32451;&#20197;&#21450;&#19979;&#28216;&#20219;&#21153;&#21487;&#38752;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Language models scale reliably with over-training and on downstream tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#32553;&#25918;&#30740;&#31350;&#20013;&#36807;&#24230;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#35780;&#20272;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#25918;&#35268;&#24459;&#23545;&#20110;&#24320;&#21457;&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#29992;&#30340;&#25351;&#23548;&#65292;&#20294;&#24403;&#21069;&#30340;&#32553;&#25918;&#30740;&#31350;&#19982;&#35821;&#35328;&#27169;&#22411;&#26368;&#32456;&#35757;&#32451;&#21644;&#35780;&#20272;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36807;&#24230;&#35757;&#32451;&#21644;&#22522;&#20110;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#36827;&#34892;&#27604;&#36739;&#26041;&#38754;&#30340;&#36825;&#20004;&#20010;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08540v1 Announce Type: new  Abstract: Scaling laws are useful guides for developing language models, but there are still gaps between current scaling studies and how language models are ultimately trained and evaluated. For instance, scaling is usually studied in the compute-optimal training regime (i.e., "Chinchilla optimal" regime); however, in practice, models are often over-trained to reduce inference costs. Moreover, scaling laws mostly predict loss on next-token prediction, but ultimately models are compared based on downstream task performance. In this paper, we address both shortcomings. To do so, we create a testbed of 104 models with 0.011B to 6.9B parameters trained with various numbers of tokens on three data distributions. First, we investigate scaling in the over-trained regime. We fit scaling laws that extrapolate in both the number of model parameters and the ratio of training tokens to parameters. This enables us to predict the validation loss of a 1.4B para
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#33258;&#21160;&#20114;&#21160;&#35780;&#20272;&#65288;AIE&#65289;&#26694;&#26550;&#21644;&#29366;&#24577;&#24863;&#30693;&#30149;&#20154;&#27169;&#25311;&#22120;&#65288;SAPS&#65289;&#65292;&#20197;&#21160;&#24577;&#12289;&#30495;&#23454;&#30340;&#24179;&#21488;&#35780;&#20272;LLMs&#65292;&#24357;&#34917;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#20020;&#24202;&#20219;&#21153;&#38656;&#27714;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.08495</link><description>&lt;p&gt;
&#20855;&#26377;&#29366;&#24577;&#24863;&#30693;&#30149;&#20154;&#27169;&#25311;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#20114;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Automatic Interactive Evaluation for Large Language Models with State Aware Patient Simulator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08495
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#33258;&#21160;&#20114;&#21160;&#35780;&#20272;&#65288;AIE&#65289;&#26694;&#26550;&#21644;&#29366;&#24577;&#24863;&#30693;&#30149;&#20154;&#27169;&#25311;&#22120;&#65288;SAPS&#65289;&#65292;&#20197;&#21160;&#24577;&#12289;&#30495;&#23454;&#30340;&#24179;&#21488;&#35780;&#20272;LLMs&#65292;&#24357;&#34917;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#26080;&#27861;&#28385;&#36275;&#20020;&#24202;&#20219;&#21153;&#38656;&#27714;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20154;&#26426;&#20114;&#21160;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#21307;&#30103;&#39046;&#22495;&#30340;&#24212;&#29992;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#33258;&#21160;&#20114;&#21160;&#35780;&#20272;&#65288;AIE&#65289;&#26694;&#26550;&#21644;&#29366;&#24577;&#24863;&#30693;&#30149;&#20154;&#27169;&#25311;&#22120;&#65288;SAPS&#65289;&#65292;&#26088;&#22312;&#24357;&#34917;&#20256;&#32479;LLM&#35780;&#20272;&#19982;&#20020;&#24202;&#23454;&#36341;&#30340;&#24494;&#22937;&#38656;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08495v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in human interactions, yet their application within the medical field remains insufficiently explored. Previous works mainly focus on the performance of medical knowledge with examinations, which is far from the realistic scenarios, falling short in assessing the abilities of LLMs on clinical tasks. In the quest to enhance the application of Large Language Models (LLMs) in healthcare, this paper introduces the Automated Interactive Evaluation (AIE) framework and the State-Aware Patient Simulator (SAPS), targeting the gap between traditional LLM evaluations and the nuanced demands of clinical practice. Unlike prior methods that rely on static medical knowledge assessments, AIE and SAPS provide a dynamic, realistic platform for assessing LLMs through multi-turn doctor-patient simulations. This approach offers a closer approximation to real clinical scenarios and allows f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#23500;&#21547;&#35821;&#20041;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#27604;BERT&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08492</link><description>&lt;p&gt;
&#23500;&#21547;&#35821;&#20041;&#30693;&#35782;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23569;&#26679;&#26412;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
Rich Semantic Knowledge Enhanced Large Language Models for Few-shot Chinese Spell Checking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#23500;&#21547;&#35821;&#20041;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#27604;BERT&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;RS-LLM&#65288;&#22522;&#20110;&#20016;&#23500;&#35821;&#20041;&#30340;LLMs&#65289;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#20837;&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#21450;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#24341;&#20837;&#21508;&#31181;&#20013;&#25991;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#24341;&#20837;&#23569;&#37327;&#29305;&#23450;&#30340;&#20013;&#25991;&#20016;&#23500;&#35821;&#20041;&#32467;&#26500;&#65292;LLMs&#22312;&#23569;&#26679;&#26412;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#20219;&#21153;&#19978;&#27604;&#22522;&#20110;BERT&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08492v1 Announce Type: new  Abstract: Chinese Spell Checking (CSC) is a widely used technology, which plays a vital role in speech to text (STT) and optical character recognition (OCR). Most of the existing CSC approaches relying on BERT architecture achieve excellent performance. However, limited by the scale of the foundation model, BERT-based method does not work well in few-shot scenarios, showing certain limitations in practical applications. In this paper, we explore using an in-context learning method named RS-LLM (Rich Semantic based LLMs) to introduce large language models (LLMs) as the foundation model. Besides, we study the impact of introducing various Chinese rich semantic information in our framework. We found that by introducing a small number of specific Chinese rich semantic structures, LLMs achieve better performance than the BERT-based model on few-shot CSC task. Furthermore, we conduct experiments on multiple datasets, and the experimental results verifie
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#21160;&#24577;&#24494;&#35843;&#21442;&#25968;&#36873;&#25321;&#31574;&#30053;&#65292;&#38024;&#23545;FISH Mask&#25552;&#20986;&#20102;IRD&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#31283;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#21160;&#24577;&#36873;&#25321;&#26368;&#20339;&#21442;&#25968;&#35774;&#32622;&#12290;</title><link>https://arxiv.org/abs/2403.08484</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#21160;&#24577;&#24494;&#35843;&#21442;&#25968;&#36873;&#25321;&#31574;&#30053;&#65292;&#29992;&#20110;&#22522;&#20110;FISH Mask&#30340;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Data-oriented Dynamic Fine-tuning Parameter Selection Strategy for FISH Mask based Efficient Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08484
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#21160;&#24577;&#24494;&#35843;&#21442;&#25968;&#36873;&#25321;&#31574;&#30053;&#65292;&#38024;&#23545;FISH Mask&#25552;&#20986;&#20102;IRD&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#31283;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#21160;&#24577;&#36873;&#25321;&#26368;&#20339;&#21442;&#25968;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#21442;&#25968;&#25968;&#37327;&#24040;&#22823;&#65292;&#35843;&#25972;&#25152;&#26377;&#21442;&#25968;&#25104;&#26412;&#24456;&#39640;&#65292;&#22240;&#27492;&#26356;&#26126;&#26234;&#30340;&#20570;&#27861;&#26159;&#23545;&#29305;&#23450;&#21442;&#25968;&#36827;&#34892;&#24494;&#35843;&#12290;&#22823;&#22810;&#25968;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;(PEFT)&#38598;&#20013;&#22312;&#21442;&#25968;&#36873;&#25321;&#31574;&#30053;&#19978;&#65292;&#20363;&#22914;&#21152;&#27861;&#26041;&#27861;&#12289;&#36873;&#25321;&#24615;&#26041;&#27861;&#21644;&#22522;&#20110;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#32771;&#34385;&#25968;&#25454;&#26679;&#26412;&#23545;&#21442;&#25968;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#20363;&#22914;&#22522;&#20110;Fish Mask&#30340;&#26041;&#27861;&#12290;Fish Mask&#38543;&#26426;&#36873;&#25321;&#37096;&#20998;&#25968;&#25454;&#26679;&#26412;&#65292;&#24182;&#22312;&#21442;&#25968;&#36873;&#25321;&#36807;&#31243;&#20013;&#23545;&#23427;&#20204;&#36827;&#34892;&#21516;&#31561;&#22788;&#29702;&#65292;&#36825;&#26080;&#27861;&#20026;&#19981;&#31283;&#23450;&#30340;&#25968;&#25454;&#20998;&#24067;&#21160;&#24577;&#36873;&#25321;&#26368;&#20339;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#35270;&#35282;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;IRD(&#36845;&#20195;&#26679;&#26412;&#21442;&#25968;&#33539;&#22260;&#20943;&#23567;)&#31639;&#27861;&#65292;&#20197;&#25628;&#32034;FISH Mask&#30340;&#26368;&#20339;&#26679;&#26412;&#21442;&#25968;&#23545;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08484v1 Announce Type: new  Abstract: In view of the huge number of parameters of Large language models (LLMs) , tuning all parameters is very costly, and accordingly fine-tuning specific parameters is more sensible. Most of parameter efficient fine-tuning (PEFT) concentrate on parameter selection strategies, such as additive method, selective method and reparametrization-based method. However, there are few methods that consider the impact of data samples on parameter selecting, such as Fish Mask based method. Fish Mask randomly choose a part of data samples and treat them equally during parameter selection, which is unable to dynamically select optimal parameters for inconstant data distributions. In this work, we adopt a data-oriented perspective, then proposing an IRD ($\mathrm{\underline I}$terative sample-parameter $\mathrm{\underline R}$ange $\mathrm{\underline D}$ecreasing) algorithm to search the best setting of sample-parameter pair for FISH Mask. In each iteration
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35745;&#31639;&#20316;&#32773;&#25991;&#20214;&#22312;&#20505;&#36873;&#20316;&#32773;&#35821;&#27861;&#27169;&#22411;&#19982;&#21442;&#32771;&#32676;&#20307;&#35821;&#27861;&#27169;&#22411;&#19979;&#30340;&#21487;&#33021;&#24615;&#27604;&#29575;&#30340;&#26041;&#27861;&#65292;&#29992;&#20197;&#35299;&#20915;&#20316;&#32773;&#36523;&#20221;&#39564;&#35777;&#20013;&#23384;&#22312;&#30340;&#31185;&#23398;&#35299;&#37322;&#19981;&#36275;&#21644;&#38590;&#20197;&#35299;&#37322;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.08462</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#27169;&#22411;&#20284;&#28982;&#27604;&#30340;&#20316;&#32773;&#36523;&#20221;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Authorship Verification based on the Likelihood Ratio of Grammar Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08462
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35745;&#31639;&#20316;&#32773;&#25991;&#20214;&#22312;&#20505;&#36873;&#20316;&#32773;&#35821;&#27861;&#27169;&#22411;&#19982;&#21442;&#32771;&#32676;&#20307;&#35821;&#27861;&#27169;&#22411;&#19979;&#30340;&#21487;&#33021;&#24615;&#27604;&#29575;&#30340;&#26041;&#27861;&#65292;&#29992;&#20197;&#35299;&#20915;&#20316;&#32773;&#36523;&#20221;&#39564;&#35777;&#20013;&#23384;&#22312;&#30340;&#31185;&#23398;&#35299;&#37322;&#19981;&#36275;&#21644;&#38590;&#20197;&#35299;&#37322;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#36523;&#20221;&#39564;&#35777;&#65288;AV&#65289;&#26159;&#20998;&#26512;&#19968;&#32452;&#25991;&#20214;&#20197;&#30830;&#23450;&#23427;&#20204;&#26159;&#21542;&#30001;&#29305;&#23450;&#20316;&#32773;&#25776;&#20889;&#30340;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;AV&#26041;&#27861;&#20351;&#29992;&#35745;&#31639;&#35299;&#20915;&#26041;&#26696;&#65292;&#23545;&#20110;&#20854;&#21151;&#33021;&#27809;&#26377;&#21512;&#29702;&#30340;&#31185;&#23398;&#35299;&#37322;&#65292;&#24182;&#19988;&#24120;&#24120;&#38590;&#20197;&#35299;&#37322;&#32473;&#20998;&#26512;&#20154;&#21592;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#35745;&#31639;&#19968;&#20010;&#25105;&#20204;&#31216;&#20043;&#20026; $\lambda_G$&#65288;LambdaG&#65289;&#30340;&#37327;&#65306;&#20505;&#36873;&#20316;&#32773;&#30340;&#19978;&#19979;&#25991;&#35821;&#27861;&#27169;&#22411;&#32473;&#20986;&#30340;&#25991;&#26723;&#30340;&#21487;&#33021;&#24615;&#19982;&#21442;&#32771;&#32676;&#20307;&#30340;&#19978;&#19979;&#25991;&#35821;&#27861;&#27169;&#22411;&#32473;&#20986;&#30340;&#30456;&#21516;&#25991;&#26723;&#30340;&#21487;&#33021;&#24615;&#20043;&#38388;&#30340;&#27604;&#29575;&#12290;&#36825;&#20123;&#35821;&#27861;&#27169;&#22411;&#26159;&#20351;&#29992;&#20165;&#38024;&#23545;&#35821;&#27861;&#29305;&#24449;&#36827;&#34892;&#35757;&#32451;&#30340; $n$-gram&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20272;&#35745;&#30340;&#12290;&#23613;&#31649;&#19981;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;LambdaG...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08462v1 Announce Type: new  Abstract: Authorship Verification (AV) is the process of analyzing a set of documents to determine whether they were written by a specific author. This problem often arises in forensic scenarios, e.g., in cases where the documents in question constitute evidence for a crime. Existing state-of-the-art AV methods use computational solutions that are not supported by a plausible scientific explanation for their functioning and that are often difficult for analysts to interpret. To address this, we propose a method relying on calculating a quantity we call $\lambda_G$ (LambdaG): the ratio between the likelihood of a document given a model of the Grammar for the candidate author and the likelihood of the same document given a model of the Grammar for a reference population. These Grammar Models are estimated using $n$-gram language models that are trained solely on grammatical features. Despite not needing large amounts of data for training, LambdaG st
&lt;/p&gt;</description></item><item><title>Tastle&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#36234;&#29425;&#26694;&#26550;&#65292;&#37319;&#29992;&#24694;&#24847;&#20869;&#23481;&#38544;&#34255;&#21644;&#20869;&#23384;&#37325;&#26500;&#20197;&#21450;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32418;&#38431;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.08424</link><description>&lt;p&gt;
Tastle: &#20026;&#33258;&#21160;&#36234;&#29425;&#25915;&#20987;&#24178;&#25200;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tastle: Distract Large Language Models for Automatic Jailbreak Attack
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08424
&lt;/p&gt;
&lt;p&gt;
Tastle&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#36234;&#29425;&#26694;&#26550;&#65292;&#37319;&#29992;&#24694;&#24847;&#20869;&#23481;&#38544;&#34255;&#21644;&#20869;&#23384;&#37325;&#26500;&#20197;&#21450;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32418;&#38431;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#22312;LLMs&#20844;&#24320;&#21457;&#24067;&#20043;&#21069;&#65292;&#20154;&#20204;&#24050;&#32463;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#23558;&#23427;&#20204;&#30340;&#34892;&#20026;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#23545;&#40784;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#30830;&#20445;&#23427;&#20204;&#30340;&#26377;&#30410;&#24615;&#12289;&#35802;&#23454;&#24615;&#21644;&#26080;&#23475;&#24615;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#32463;&#36807;&#32454;&#33268;&#23545;&#40784;&#30340;LLMs&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25805;&#32437;&#65292;&#22914;&#36234;&#29425;&#65292;&#23548;&#33268;&#24847;&#22806;&#30340;&#34892;&#20026;&#12290;&#36234;&#29425;&#26159;&#26377;&#24847;&#24320;&#21457;&#24694;&#24847;&#25552;&#31034;&#65292;&#20174;LLM&#23433;&#20840;&#38480;&#21046;&#20013;&#36867;&#33073;&#20197;&#29983;&#25104;&#26410;&#32463;&#23457;&#26597;&#30340;&#26377;&#23475;&#20869;&#23481;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#36234;&#29425;&#26041;&#27861;&#26469;&#23545;LLMs&#36827;&#34892;&#32418;&#38431;&#25915;&#20987;&#65292;&#20294;&#23427;&#20204;&#22312;&#25928;&#26524;&#21644;&#21487;&#20280;&#32553;&#24615;&#26041;&#38754;&#36935;&#21040;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tastle&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#36234;&#29425;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#23545;LLMs&#36827;&#34892;&#32418;&#38431;&#25915;&#20987;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#24694;&#24847;&#20869;&#23481;&#38544;&#34255;&#21644;&#20869;&#23384;&#37325;&#26500;&#65292;&#24182;&#32467;&#21512;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#26469;&#36234;&#29425;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08424v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved significant advances in recent days. Extensive efforts have been made before the public release of LLMs to align their behaviors with human values. The primary goal of alignment is to ensure their helpfulness, honesty and harmlessness. However, even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors. The jailbreak is to intentionally develop a malicious prompt that escapes from the LLM security restrictions to produce uncensored detrimental contents. Previous works explore different jailbreak methods for red teaming LLMs, yet they encounter challenges regarding to effectiveness and scalability. In this work, we propose Tastle, a novel black-box jailbreak framework for automated red teaming of LLMs. We designed malicious content concealing and memory reframing with an iterative optimization algorithm to jailbreak LLMs, mo
&lt;/p&gt;</description></item><item><title>&#34394;&#20551;&#20449;&#24687;&#19981;&#20165;&#20165;&#28041;&#21450;&#38169;&#35823;&#30340;&#20107;&#23454;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#32447;&#36793;&#32536;&#24847;&#35782;&#24418;&#24577;&#36890;&#36807;&#20849;&#35782;&#21644;&#8220;&#20107;&#23454;&#27491;&#30830;&#8221;&#30340;&#20869;&#23481;&#20256;&#25773;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#26497;&#21491;&#29992;&#25143;&#20542;&#21521;&#25361;&#36873;&#20851;&#27880;&#29305;&#23450;&#20027;&#39064;&#30340;&#26032;&#38395;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#27807;&#36890;&#39118;&#26684;&#35782;&#21035;&#26131;&#20110;&#20998;&#20139;&#38169;&#35823;&#20449;&#24687;&#30340;&#29992;&#25143;&#12290;</title><link>https://arxiv.org/abs/2403.08391</link><description>&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#19981;&#26159;&#20851;&#20110;&#38169;&#35823;&#20107;&#23454;&#65306;&#23545;&#36793;&#32536;&#20869;&#23481;&#30340;&#29983;&#20135;&#21644;&#28040;&#36153;&#36827;&#34892;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Misinformation is not about Bad Facts: An Analysis of the Production and Consumption of Fringe Content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08391
&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#19981;&#20165;&#20165;&#28041;&#21450;&#38169;&#35823;&#30340;&#20107;&#23454;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#32447;&#36793;&#32536;&#24847;&#35782;&#24418;&#24577;&#36890;&#36807;&#20849;&#35782;&#21644;&#8220;&#20107;&#23454;&#27491;&#30830;&#8221;&#30340;&#20869;&#23481;&#20256;&#25773;&#65292;&#21516;&#26102;&#21457;&#29616;&#20102;&#26497;&#21491;&#29992;&#25143;&#20542;&#21521;&#25361;&#36873;&#20851;&#27880;&#29305;&#23450;&#20027;&#39064;&#30340;&#26032;&#38395;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#27807;&#36890;&#39118;&#26684;&#35782;&#21035;&#26131;&#20110;&#20998;&#20139;&#38169;&#35823;&#20449;&#24687;&#30340;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#35823;&#20256;&#26681;&#26412;&#19981;&#26159;&#19968;&#20010;&#20449;&#24687;&#38382;&#39064;&#21602;&#65311;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#32447;&#36793;&#32536;&#24847;&#35782;&#24418;&#24577;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#20849;&#35782;&#21644;&#8220;&#20107;&#23454;&#27491;&#30830;&#8221;&#30340;&#20869;&#23481;&#20256;&#25773;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20855;&#26377;&#20013;&#38388;&#21644;&#26497;&#21491;&#25919;&#27835;&#20542;&#21521;&#30340;&#28595;&#22823;&#21033;&#20122;&#26032;&#38395;&#20986;&#29256;&#21830;&#22312;&#20449;&#24687;&#23436;&#25972;&#24615;&#21644;&#36136;&#37327;&#26041;&#38754;&#23621;&#20110;&#21487;&#27604;&#27700;&#24179;&#65307;&#27492;&#22806;&#65292;&#26497;&#21491;&#32764;Twitter&#29992;&#25143;&#32463;&#24120;&#20998;&#20139;&#26469;&#33258;&#20013;&#38388;&#26469;&#28304;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#24403;&#25105;&#20204;&#32771;&#34385;&#21040;&#20004;&#20010;&#39069;&#22806;&#22240;&#32032;&#26102;&#65292;&#20986;&#29616;&#20102;&#26126;&#26174;&#24046;&#24322;&#65306;1&#65289;&#26497;&#21491;&#29992;&#25143;&#23545;&#25991;&#31456;&#30340;&#29421;&#38552;&#20027;&#39064;&#36873;&#25321;&#65292;&#26263;&#31034;&#20182;&#20204;&#21482;&#25361;&#36873;&#19982;&#20182;&#20204;&#20851;&#27880;&#29305;&#23450;&#20027;&#39064;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;2&#65289;&#22312;&#23457;&#26597;&#25991;&#31456;&#20889;&#20316;&#39118;&#26684;&#26102;&#65292;&#20013;&#38388;&#21644;&#26497;&#21491;&#20986;&#29256;&#21830;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29978;&#33267;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#30340;&#27807;&#36890;&#39118;&#26684;&#35782;&#21035;&#26131;&#20110;&#20998;&#20139;&#38169;&#35823;&#20449;&#24687;&#30340;&#29992;&#25143;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#21512;&#20316;&#26041;&#38754;&#26377;&#37325;&#35201;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08391v1 Announce Type: new  Abstract: What if misinformation is not an information problem at all? Our findings suggest that online fringe ideologies spread through the use of content that is consensus-based and "factually correct". We found that Australian news publishers with both moderate and far-right political leanings contain comparable levels of information completeness and quality; and furthermore, that far-right Twitter users often share from moderate sources. However, a stark difference emerges when we consider two additional factors: 1) the narrow topic selection of articles by far-right users, suggesting that they cherrypick only news articles that engage with specific topics of their concern, and 2) the difference between moderate and far-right publishers when we examine the writing style of their articles. Furthermore, we can even identify users prone to sharing misinformation based on their communication style. These findings have important implications for co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;&#26679;&#26412;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDI&#65289;&#39044;&#27979;&#38382;&#39064;&#35774;&#32622;&#65292;&#36890;&#36807;&#21033;&#29992;&#22312;&#32447;&#25968;&#25454;&#24211;&#30340;&#25991;&#26412;&#20449;&#24687;&#21644;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#23545;&#26032;&#33647;&#29289;&#20934;&#30830;DDI&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.08377</link><description>&lt;p&gt;
&#23398;&#20064;&#25551;&#36848;&#20197;&#39044;&#27979;&#38646;&#26679;&#26412;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning to Describe for Predicting Zero-shot Drug-Drug Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08377
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;&#26679;&#26412;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDI&#65289;&#39044;&#27979;&#38382;&#39064;&#35774;&#32622;&#65292;&#36890;&#36807;&#21033;&#29992;&#22312;&#32447;&#25968;&#25454;&#24211;&#30340;&#25991;&#26412;&#20449;&#24687;&#21644;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#23545;&#26032;&#33647;&#29289;&#20934;&#30830;DDI&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;DDIs&#65289;&#21487;&#33021;&#24433;&#21709;&#21516;&#26102;&#20351;&#29992;&#33647;&#29289;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#26500;&#25104;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#20316;&#20026;&#38646;&#26679;&#26412;DDI&#39044;&#27979;&#65292;&#22788;&#29702;&#26032;&#33647;&#29289;&#30340;&#24773;&#20917;&#12290;&#21033;&#29992;&#26469;&#33258;DrugBank&#21644;PubChem&#31561;&#22312;&#32447;&#25968;&#25454;&#24211;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;TextDDI&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;DDI&#39044;&#27979;&#22120;&#21644;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#20449;&#24687;&#36873;&#25321;&#22120;&#65292;&#23454;&#29616;&#23545;&#26032;&#33647;&#29289;&#36827;&#34892;&#20934;&#30830;DDI&#39044;&#27979;&#30340;&#31616;&#27905;&#32780;&#30456;&#20851;&#25991;&#26412;&#30340;&#36873;&#25321;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#21253;&#25324;&#38646;&#26679;&#26412;&#22312;&#20869;&#30340;&#20960;&#31181;&#35774;&#32622;&#19978;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08377v1 Announce Type: new  Abstract: Adverse drug-drug interactions~(DDIs) can compromise the effectiveness of concurrent drug administration, posing a significant challenge in healthcare. As the development of new drugs continues, the potential for unknown adverse effects resulting from DDIs becomes a growing concern. Traditional computational methods for DDI prediction may fail to capture interactions for new drugs due to the lack of knowledge. In this paper, we introduce a new problem setup as zero-shot DDI prediction that deals with the case of new drugs. Leveraging textual information from online databases like DrugBank and PubChem, we propose an innovative approach TextDDI with a language model-based DDI predictor and a reinforcement learning~(RL)-based information selector, enabling the selection of concise and pertinent text for accurate DDI prediction on new drugs. Empirical results show the benefits of the proposed approach on several settings including zero-shot 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20113;&#36801;&#31227;&#20013;SQL&#25968;&#25454;&#24211;&#26041;&#35328;&#30340;&#36716;&#25442;&#22256;&#38590;&#65292;&#23613;&#31649;&#26377;&#19968;&#20123;&#24037;&#20855;&#21487;&#20197;&#24110;&#21161;&#36716;&#25442;&#26041;&#35328;&#65292;&#20294;&#24182;&#19981;&#33021;&#23436;&#20840;&#35299;&#20915;&#38382;&#39064;&#65292;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#12290;</title><link>https://arxiv.org/abs/2403.08375</link><description>&lt;p&gt;
&#20113;&#36801;&#31227;&#20013;SQL&#26041;&#35328;&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Translating between SQL Dialects for Cloud Migration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20113;&#36801;&#31227;&#20013;SQL&#25968;&#25454;&#24211;&#26041;&#35328;&#30340;&#36716;&#25442;&#22256;&#38590;&#65292;&#23613;&#31649;&#26377;&#19968;&#20123;&#24037;&#20855;&#21487;&#20197;&#24110;&#21161;&#36716;&#25442;&#26041;&#35328;&#65292;&#20294;&#24182;&#19981;&#33021;&#23436;&#20840;&#35299;&#20915;&#38382;&#39064;&#65292;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29616;&#22330;&#21040;&#20113;&#30340;&#31995;&#32479;&#36801;&#31227;&#26159;&#35768;&#22810;&#24037;&#19994;&#26426;&#26500;&#30340;&#37325;&#35201;&#24037;&#20316;&#12290;&#36825;&#31181;&#20113;&#36801;&#31227;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#23558;&#25968;&#25454;&#24211;&#36716;&#31227;&#21040;&#22312;&#32447;&#20027;&#26426;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;SQL&#25968;&#25454;&#24211;&#30340;&#36801;&#31227;&#22256;&#38590;&#12290;&#23613;&#31649;SQL&#26159;&#23384;&#20648;&#25968;&#25454;&#24211;&#31243;&#24207;&#30340;&#26174;&#33879;&#26041;&#27861;&#20043;&#19968;&#65292;&#20294;&#23384;&#22312;&#30528;&#22823;&#37327;&#19981;&#21516;&#30340;SQL&#26041;&#35328;&#65288;&#20363;&#22914;MySQL&#65292;Postgres&#31561;&#65289;&#65292;&#24403;&#29616;&#22330;&#30340;SQL&#26041;&#35328;&#19982;&#20113;&#19978;&#25176;&#31649;&#30340;&#26041;&#35328;&#19981;&#21516;&#26102;&#65292;&#36825;&#21487;&#33021;&#20250;&#20351;&#36801;&#31227;&#22797;&#26434;&#21270;&#12290;&#19968;&#20123;&#24120;&#35265;&#20113;&#25552;&#20379;&#21830;&#22914;AWS&#21644;Azure&#25552;&#20379;&#20102;&#24037;&#20855;&#26469;&#24110;&#21161;&#22312;&#26041;&#35328;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#65292;&#20197;&#20943;&#36731;&#22823;&#37096;&#20998;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#24182;&#19981;&#25104;&#21151;&#22320;&#36716;&#25442; 100% &#30340;&#20195;&#30721;&#12290;&#22240;&#27492;&#65292;&#36719;&#20214;&#24037;&#31243;&#24072;&#24517;&#39035;&#25163;&#21160;&#36716;&#25442;&#26410;&#32763;&#35793;&#25968;&#25454;&#24211;&#30340;&#20854;&#20313;&#37096;&#20998;&#12290;&#23545;&#20110;&#22823;&#22411;&#32452;&#32455;&#65292;&#36825;&#39033;&#20219;&#21153;&#24456;&#24555;&#21464;&#24471;&#26840;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08375v1 Announce Type: cross  Abstract: Migrations of systems from on-site premises to the cloud has been a fundamental endeavor by many industrial institutions. A crucial component of such cloud migrations is the transition of databases to be hosted online. In this work, we consider the difficulties of this migration for SQL databases. While SQL is one of the prominent methods for storing database procedures, there are a plethora of different SQL dialects (e.g., MySQL, Postgres, etc.) which can complicate migrations when the on-premise SQL dialect differs to the dialect hosted on the cloud. Tools exist by common cloud provides such as AWS and Azure to aid in translating between dialects in order to mitigate the majority of the difficulties. However, these tools do not successfully translate $100\%$ of the code. Consequently, software engineers must manually convert the remainder of the untranslated database. For large organizations, this task quickly becomes intractable and
&lt;/p&gt;</description></item><item><title>SMART&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#24494;&#35843;&#20013;&#37325;&#26032;&#20998;&#37197;&#39044;&#31639;&#65292;&#20174;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#20013;&#21462;&#24471;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.08370</link><description>&lt;p&gt;
SMART: &#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#30340;&#23376;&#27169;&#22359;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SMART: Submodular Data Mixture Strategy for Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08370
&lt;/p&gt;
&lt;p&gt;
SMART&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#24494;&#35843;&#20013;&#37325;&#26032;&#20998;&#37197;&#39044;&#31639;&#65292;&#20174;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#20013;&#21462;&#24471;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#28041;&#21450;&#22312;&#19968;&#32452;&#20197;&#25351;&#20196;&#26684;&#24335;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24179;&#34913;&#19981;&#21516;&#20219;&#21153;&#27604;&#20363;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#25214;&#21040;&#21512;&#36866;&#30340;&#24179;&#34913;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#38500;&#20102;&#25163;&#21160;&#35843;&#25972;&#25110;&#20381;&#36182;&#20174;&#19994;&#32773;&#30340;&#30452;&#35273;&#22806;&#65292;&#23578;&#26080;&#31995;&#32479;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SMART&#65288;Submodular data Mixture strAtegy for instRuction Tuning&#65289;- &#19968;&#31181;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#26032;&#39062;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#20998;&#25968;&#26469;&#30830;&#23450;&#28151;&#21512;&#26435;&#37325;&#12290;&#32473;&#23450;&#24494;&#35843;&#39044;&#31639;&#65292;SMART&#37325;&#26032;&#20998;&#37197;&#20219;&#21153;&#38388;&#30340;&#39044;&#31639;&#65292;&#24182;&#20174;&#27599;&#20010;&#20219;&#21153;&#20013;&#36873;&#25321;&#38750;&#20887;&#20313;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SMART&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#20363;&#23376;&#27604;&#20363;&#28151;&#21512;&#21644;&#22343;&#31561;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08370v1 Announce Type: cross  Abstract: Instruction Tuning involves finetuning a language model on a collection of instruction-formatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#22522;&#20110;&#35821;&#20041;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#21160;&#24577;&#26356;&#26032;&#26085;&#24535;&#32858;&#31867;&#65292;&#20197;&#23454;&#29616;&#30417;&#25511;&#20195;&#30721;&#38169;&#35823;&#30340;&#29983;&#21629;&#21608;&#26399;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#26102;&#38388;&#26085;&#24535;&#32858;&#31867;&#30340;&#24615;&#33021;&#65292;&#26368;&#32456;&#21457;&#29616;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#31867;&#20284;&#31995;&#32479;&#12290;</title><link>https://arxiv.org/abs/2403.08358</link><description>&lt;p&gt;
&#32570;&#38519;&#28436;&#21270;&#20998;&#26512;&#30340;&#26085;&#24535;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
Log Summarisation for Defect Evolution Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08358
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#22522;&#20110;&#35821;&#20041;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#21160;&#24577;&#26356;&#26032;&#26085;&#24535;&#32858;&#31867;&#65292;&#20197;&#23454;&#29616;&#30417;&#25511;&#20195;&#30721;&#38169;&#35823;&#30340;&#29983;&#21629;&#21608;&#26399;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#26102;&#38388;&#26085;&#24535;&#32858;&#31867;&#30340;&#24615;&#33021;&#65292;&#26368;&#32456;&#21457;&#29616;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#31867;&#20284;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26085;&#24535;&#20998;&#26512;&#21644;&#30417;&#25511;&#26159;&#36719;&#20214;&#32500;&#25252;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#26041;&#38754;&#65292;&#20063;&#26159;&#35782;&#21035;&#32570;&#38519;&#30340;&#20851;&#38190;&#12290;&#29305;&#21035;&#26159;&#65292;&#26085;&#24535;&#25968;&#25454;&#30340;&#26102;&#38388;&#24615;&#36136;&#21644;&#24222;&#22823;&#35268;&#27169;&#24341;&#21457;&#20102;&#19968;&#20010;&#26377;&#36259;&#19988;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#22914;&#20309;&#23545;&#26085;&#24535;&#36827;&#34892;&#24635;&#32467;&#21644;&#38543;&#26102;&#38388;&#30417;&#25511;&#65311;&#23613;&#31649;&#36825;&#22312;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#26159;&#19968;&#20010;&#22522;&#30784;&#24615;&#30740;&#31350;&#35838;&#39064;&#65292;&#20294;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#38598;&#20013;&#22312;&#21551;&#21457;&#24335;&#12289;&#35821;&#27861;&#25110;&#22522;&#20110;&#38745;&#24577;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#22522;&#20110;&#35821;&#20041;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#29992;&#20110;&#21160;&#24577;&#26356;&#26032;&#26085;&#24535;&#32858;&#31867;&#65292;&#20197;&#23454;&#29616;&#30417;&#25511;&#20195;&#30721;&#38169;&#35823;&#30340;&#29983;&#21629;&#21608;&#26399;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#26102;&#38388;&#26085;&#24535;&#32858;&#31867;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#24037;&#19994;&#25968;&#25454;&#38598;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#21644;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#21457;&#29616;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#31867;&#20284;&#31995;&#32479;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#22815;&#40723;&#21169;&#22312;&#32570;&#38519;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#26356;&#28145;&#20837;&#30340;&#26102;&#38388;&#24615;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08358v1 Announce Type: cross  Abstract: Log analysis and monitoring are essential aspects in software maintenance and identifying defects. In particular, the temporal nature and vast size of log data leads to an interesting and important research question: How can logs be summarised and monitored over time? While this has been a fundamental topic of research in the software engineering community, work has typically focused on heuristic-, syntax-, or static-based methods. In this work, we suggest an online semantic-based clustering approach to error logs that dynamically updates the log clusters to enable monitoring code error life-cycles. We also introduce a novel metric to evaluate the performance of temporal log clusters. We test our system and evaluation metric with an industrial dataset and find that our solution outperforms similar systems. We hope that our work encourages further temporal exploration in defect datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#24320;&#28304;LLMs&#65288;&#21322;&#65289;&#33258;&#21160;&#26500;&#24314;&#30693;&#35782;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21046;&#23450;&#33021;&#21147;&#38382;&#39064;&#65288;CQs&#65289;&#12289;&#22522;&#20110;&#36825;&#20123;CQs&#24320;&#21457;&#26412;&#20307;&#35770;&#65288;TBox&#65289;&#12289;&#26500;&#24314;&#30693;&#35782;&#22270;&#24182;&#23454;&#29616;&#20960;&#20046;&#19981;&#28041;&#21450;&#20154;&#31867;&#19987;&#23478;&#30340;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#21322;&#33258;&#21160;&#21270;&#27969;&#31243;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08345</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#19987;&#23478;&#21040;&#26426;&#22120;&#65306;&#19968;&#31181;LLM&#25903;&#25345;&#30340;&#26412;&#20307;&#35770;&#21644;&#30693;&#35782;&#22270;&#26500;&#24314;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
From human experts to machines: An LLM supported approach to ontology and knowledge graph construction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#24320;&#28304;LLMs&#65288;&#21322;&#65289;&#33258;&#21160;&#26500;&#24314;&#30693;&#35782;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21046;&#23450;&#33021;&#21147;&#38382;&#39064;&#65288;CQs&#65289;&#12289;&#22522;&#20110;&#36825;&#20123;CQs&#24320;&#21457;&#26412;&#20307;&#35770;&#65288;TBox&#65289;&#12289;&#26500;&#24314;&#30693;&#35782;&#22270;&#24182;&#23454;&#29616;&#20960;&#20046;&#19981;&#28041;&#21450;&#20154;&#31867;&#19987;&#23478;&#30340;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#21322;&#33258;&#21160;&#21270;&#27969;&#31243;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#26412;&#20307;&#35770;&#21644;&#30693;&#35782;&#22270;&#30340;&#20256;&#32479;&#36807;&#31243;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#31867;&#39046;&#22495;&#19987;&#23478;&#26469;&#23450;&#20041;&#23454;&#20307;&#21644;&#20851;&#31995;&#31867;&#22411;&#65292;&#24314;&#31435;&#23618;&#27425;&#32467;&#26500;&#65292;&#20445;&#25345;&#19982;&#39046;&#22495;&#30340;&#20851;&#32852;&#24615;&#65292;&#22635;&#20805;ABox&#65288;&#25110;&#29992;&#23454;&#20363;&#22635;&#20805;&#65289;&#65292;&#24182;&#30830;&#20445;&#25968;&#25454;&#36136;&#37327;&#65288;&#21253;&#25324;&#20934;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#65289;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#22240;&#20854;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#25512;&#23815;&#65292;&#20026;&#33258;&#21160;&#21270;&#35813;&#36807;&#31243;&#30340;&#26041;&#38754;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;&#26412;&#24037;&#20316;&#25506;&#35752;&#20102;&#21033;&#29992;&#24320;&#28304;LLMs&#65288;&#21322;&#65289;&#33258;&#21160;&#26500;&#24314;&#30693;&#35782;&#22270;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#28041;&#21450;&#21046;&#23450;&#33021;&#21147;&#38382;&#39064;&#65288;CQs&#65289;&#65292;&#22522;&#20110;&#36825;&#20123;CQs&#24320;&#21457;&#26412;&#20307;&#35770;&#65288;TBox&#65289;&#65292;&#20351;&#29992;&#24320;&#21457;&#30340;&#26412;&#20307;&#35770;&#26500;&#24314;&#30693;&#35782;&#22270;&#65292;&#24182;&#22312;&#20960;&#20046;&#19981;&#28041;&#21450;&#20154;&#31867;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#29983;&#25104;&#30340;&#30693;&#35782;&#22270;&#12290;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;&#23454;&#20363;&#23637;&#31034;&#20102;&#21322;&#33258;&#21160;&#21270;&#27969;&#31243;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08345v1 Announce Type: new  Abstract: The conventional process of building Ontologies and Knowledge Graphs (KGs) heavily relies on human domain experts to define entities and relationship types, establish hierarchies, maintain relevance to the domain, fill the ABox (or populate with instances), and ensure data quality (including amongst others accuracy and completeness). On the other hand, Large Language Models (LLMs) have recently gained popularity for their ability to understand and generate human-like natural language, offering promising ways to automate aspects of this process. This work explores the (semi-)automatic construction of KGs facilitated by open-source LLMs. Our pipeline involves formulating competency questions (CQs), developing an ontology (TBox) based on these CQs, constructing KGs using the developed ontology, and evaluating the resultant KG with minimal to no involvement of human experts. We showcase the feasibility of our semi-automated pipeline by creat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#33258;&#22238;&#24402;&#39044;&#27979;&#22810;&#29305;&#24449;&#20998;&#25968;&#30340;&#26041;&#27861;&#65288;ArTS&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;T5&#26469;&#32467;&#21512;&#35299;&#30721;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#20316;&#25991;&#35780;&#20998;&#20013;&#22810;&#20998;&#25968;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.08332</link><description>&lt;p&gt;
&#33258;&#22238;&#24402;&#24471;&#20998;&#29983;&#25104;&#29992;&#20110;&#22810;&#29305;&#24449;&#20316;&#25991;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Autoregressive Score Generation for Multi-trait Essay Scoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08332
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#33258;&#22238;&#24402;&#39044;&#27979;&#22810;&#29305;&#24449;&#20998;&#25968;&#30340;&#26041;&#27861;&#65288;ArTS&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;T5&#26469;&#32467;&#21512;&#35299;&#30721;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#20316;&#25991;&#35780;&#20998;&#20013;&#22810;&#20998;&#25968;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20165;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#27169;&#22411;&#22914;BERT&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#33258;&#21160;&#20316;&#25991;&#35780;&#20998;&#65288;AES&#65289;&#20013;&#65292;&#29992;&#20110;&#39044;&#27979;&#21333;&#19968;&#25972;&#20307;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#23578;&#26410;&#25506;&#32034;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#29305;&#24449;AES&#20013;&#30340;&#24212;&#29992;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#20026;&#27599;&#20010;&#29305;&#24449;&#22797;&#21046;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#30340;&#25928;&#29575;&#20302;&#19979;&#12290;&#25105;&#20204;&#31361;&#30772;&#20102;&#29616;&#26377;&#20165;&#20351;&#29992;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#39044;&#27979;&#22810;&#29305;&#24449;&#20998;&#25968;&#65288;ArTS&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;T5&#26469;&#32467;&#21512;&#19968;&#20010;&#35299;&#30721;&#36807;&#31243;&#12290;&#19982;&#20808;&#21069;&#30340;&#22238;&#24402;&#25110;&#20998;&#31867;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;AES&#20026;&#19968;&#20010;&#24471;&#20998;&#29983;&#25104;&#20219;&#21153;&#65292;&#20801;&#35768;&#21333;&#20010;&#27169;&#22411;&#39044;&#27979;&#22810;&#20010;&#20998;&#25968;&#12290;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#38543;&#21518;&#30340;&#29305;&#24449;&#39044;&#27979;&#21487;&#20197;&#36890;&#36807;&#22312;&#20808;&#21069;&#30340;&#29305;&#24449;&#20998;&#25968;&#19978;&#36827;&#34892;&#26465;&#20214;&#21270;&#32780;&#21463;&#30410;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;ArTS&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#31034;&#20102;&#22312;&#25552;&#31034;&#21644;&#29305;&#24449;&#26041;&#38754;&#24179;&#22343;&#25552;&#39640;5%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08332v1 Announce Type: cross  Abstract: Recently, encoder-only pre-trained models such as BERT have been successfully applied in automated essay scoring (AES) to predict a single overall score. However, studies have yet to explore these models in multi-trait AES, possibly due to the inefficiency of replicating BERT-based models for each trait. Breaking away from the existing sole use of encoder, we propose an autoregressive prediction of multi-trait scores (ArTS), incorporating a decoding process by leveraging the pre-trained T5. Unlike prior regression or classification methods, we redefine AES as a score-generation task, allowing a single model to predict multiple scores. During decoding, the subsequent trait prediction can benefit by conditioning on the preceding trait scores. Experimental results proved the efficacy of ArTS, showing over 5% average improvements in both prompts and traits.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#28145;&#20837;&#20998;&#26512;&#20102;LLMs&#22312;&#34701;&#21512;&#19978;&#19979;&#25991;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#26102;&#25152;&#38754;&#20020;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25506;&#35752;&#20102;&#19977;&#31867;&#30693;&#35782;&#20914;&#31361;&#23545;&#20854;&#21487;&#20449;&#24230;&#21644;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLMs&#31283;&#20581;&#24615;&#31574;&#30053;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.08319</link><description>&lt;p&gt;
LLMs&#30340;&#30693;&#35782;&#20914;&#31361;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Knowledge Conflicts for LLMs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08319
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#28145;&#20837;&#20998;&#26512;&#20102;LLMs&#22312;&#34701;&#21512;&#19978;&#19979;&#25991;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#26102;&#25152;&#38754;&#20020;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25506;&#35752;&#20102;&#19977;&#31867;&#30693;&#35782;&#20914;&#31361;&#23545;&#20854;&#21487;&#20449;&#24230;&#21644;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLMs&#31283;&#20581;&#24615;&#31574;&#30053;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#20914;&#31361;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#24403;&#23427;&#20204;&#34701;&#21512;&#19978;&#19979;&#25991;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#26102;&#25152;&#36935;&#21040;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;&#25105;&#20204;&#20851;&#27880;&#19977;&#31867;&#30693;&#35782;&#20914;&#31361;&#65306;&#19978;&#19979;&#25991;-&#35760;&#24518;&#20914;&#31361;&#12289;&#36328;&#19978;&#19979;&#25991;&#20914;&#31361;&#21644;&#20869;&#37096;&#35760;&#24518;&#20914;&#31361;&#12290;&#36825;&#20123;&#20914;&#31361;&#21487;&#33021;&#20250;&#26174;&#33879;&#24433;&#21709;LLMs&#30340;&#21487;&#20449;&#24230;&#21644;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#22122;&#38899;&#21644;&#38169;&#35823;&#20449;&#24687;&#24456;&#24120;&#35265;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#20914;&#31361;&#36827;&#34892;&#20998;&#31867;&#65292;&#25506;&#35752;&#20854;&#21407;&#22240;&#65292;&#30740;&#31350;LLMs&#22312;&#36825;&#20123;&#20914;&#31361;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#22238;&#39038;&#21487;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#35843;&#26597;&#26088;&#22312;&#20026;&#25913;&#36827;LLMs&#30340;&#31283;&#20581;&#24615;&#31574;&#30053;&#25552;&#20379;&#21551;&#31034;&#65292;&#20174;&#32780;&#25104;&#20026;&#25512;&#21160;&#36825;&#19968;&#19981;&#26029;&#21457;&#23637;&#39046;&#22495;&#30740;&#31350;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08319v1 Announce Type: cross  Abstract: This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#22312;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#32842;&#22825;&#36136;&#37327;&#26041;&#38754;&#30340;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#65292;&#26080;&#21442;&#32771;&#24230;&#37327;&#26631;&#20934;&#22312;&#35780;&#20272;&#32763;&#35793;&#36136;&#37327;&#26102;&#33853;&#21518;&#20110;&#26377;&#21442;&#32771;&#24230;&#37327;&#26631;&#20934;&#65292;&#23588;&#20854;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#19979;&#35780;&#20272;&#26102;&#12290;&#30740;&#31350;&#21457;&#29616;&#23558;&#20250;&#35805;&#19978;&#19979;&#25991;&#20449;&#24687;&#32467;&#21512;&#21040;&#24230;&#37327;&#26631;&#20934;&#20013;&#21487;&#20197;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08314</link><description>&lt;p&gt;
&#32842;&#22825;&#32763;&#35793;&#35780;&#20272;&#26159;&#21542;&#21463;&#19978;&#19979;&#25991;&#24110;&#21161;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Context Helpful for Chat Translation Evaluation?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08314
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#22312;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#32842;&#22825;&#36136;&#37327;&#26041;&#38754;&#30340;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#65292;&#26080;&#21442;&#32771;&#24230;&#37327;&#26631;&#20934;&#22312;&#35780;&#20272;&#32763;&#35793;&#36136;&#37327;&#26102;&#33853;&#21518;&#20110;&#26377;&#21442;&#32771;&#24230;&#37327;&#26631;&#20934;&#65292;&#23588;&#20854;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#19979;&#35780;&#20272;&#26102;&#12290;&#30740;&#31350;&#21457;&#29616;&#23558;&#20250;&#35805;&#19978;&#19979;&#25991;&#20449;&#24687;&#32467;&#21512;&#21040;&#24230;&#37327;&#26631;&#20934;&#20013;&#21487;&#20197;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#33258;&#21160;&#35780;&#20272;&#32763;&#35793;&#36136;&#37327;&#30340;&#24230;&#37327;&#26631;&#20934;&#21462;&#24471;&#20102;&#36817;&#26399;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#32842;&#22825;&#36136;&#37327;&#26041;&#38754;&#30340;&#24212;&#29992;&#21364;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#19982;&#26032;&#38395;&#31561;&#26356;&#20026;&#32467;&#26500;&#21270;&#30340;&#25991;&#26412;&#19981;&#21516;&#65292;&#32842;&#22825;&#23545;&#35805;&#36890;&#24120;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#12289;&#30701;&#23567;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#36825;&#32473;&#29616;&#26377;&#21477;&#23376;&#32423;&#24230;&#37327;&#26631;&#20934;&#22312;&#35813;&#39046;&#22495;&#30340;&#21487;&#38752;&#24615;&#20197;&#21450;&#19978;&#19979;&#25991;&#22312;&#35780;&#20272;&#32763;&#35793;&#36136;&#37327;&#20013;&#30340;&#20316;&#29992;&#25552;&#20986;&#20102;&#38382;&#39064;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#20027;&#35201;&#29992;&#20110;&#26032;&#38395;&#31561;&#32467;&#26500;&#21270;&#39046;&#22495;&#30340;&#21477;&#23376;&#32423;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#20102;&#20803;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#32842;&#22825;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35780;&#20272;&#38750;&#33521;&#35821;&#29615;&#22659;&#19979;&#30340;&#32763;&#35793;&#36136;&#37327;&#26102;&#65292;&#26080;&#21442;&#32771;&#24230;&#37327;&#26631;&#20934;&#33853;&#21518;&#20110;&#26377;&#21442;&#32771;&#24230;&#37327;&#26631;&#20934;&#65292;&#28982;&#21518;&#25105;&#20204;&#35843;&#26597;&#20102;&#22914;&#20309;&#23558;&#20250;&#35805;&#19978;&#19979;&#25991;&#20449;&#24687;&#32467;&#21512;&#21040;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#20013;&#65292;&#20197;&#24433;&#21709;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08314v1 Announce Type: new  Abstract: Despite the recent success of automatic metrics for assessing translation quality, their application in evaluating the quality of machine-translated chats has been limited. Unlike more structured texts like news, chat conversations are often unstructured, short, and heavily reliant on contextual information. This poses questions about the reliability of existing sentence-level metrics in this domain as well as the role of context in assessing the translation quality. Motivated by this, we conduct a meta-evaluation of existing sentence-level automatic metrics, primarily designed for structured domains such as news, to assess the quality of machine-translated chats. We find that reference-free metrics lag behind reference-based ones, especially when evaluating translation quality in out-of-English settings. We then investigate how incorporating conversational contextual information in these metrics affects their performance. Our findings s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;StreamingDialogue&#65292;&#36890;&#36807;&#23558;&#38271;&#23545;&#35805;&#21382;&#21490;&#21387;&#32553;&#20026;"&#20250;&#35805;&#27880;&#24847;&#21147;&#27719;&#38598;&#28857;"&#65292;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20351;&#35745;&#31639;&#22797;&#26434;&#24230;&#20943;&#23569;&#65292;&#24182;&#26377;&#28508;&#21147;&#22788;&#29702;&#36229;&#36807;200k&#26465;&#35805;&#35821;&#65292;&#23454;&#29616;&#38271;&#26102;&#38388;&#23545;&#35805;&#23398;&#20064;</title><link>https://arxiv.org/abs/2403.08312</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#25439;&#22833;&#36827;&#34892;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;StreamingDialogue&#65306;&#38271;&#23545;&#35805;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08312
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;StreamingDialogue&#65292;&#36890;&#36807;&#23558;&#38271;&#23545;&#35805;&#21382;&#21490;&#21387;&#32553;&#20026;"&#20250;&#35805;&#27880;&#24847;&#21147;&#27719;&#38598;&#28857;"&#65292;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20351;&#35745;&#31639;&#22797;&#26434;&#24230;&#20943;&#23569;&#65292;&#24182;&#26377;&#28508;&#21147;&#22788;&#29702;&#36229;&#36807;200k&#26465;&#35805;&#35821;&#65292;&#23454;&#29616;&#38271;&#26102;&#38388;&#23545;&#35805;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#20855;&#26377;&#38271;&#19978;&#19979;&#25991;&#30340;&#23545;&#35805;&#26102;&#36935;&#21040;&#20102;&#25928;&#29575;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#23545;&#35805;&#19978;&#19979;&#25991;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;&#24182;&#19988;&#23545;&#35805;&#20013;&#30340;&#29305;&#27530;&#26631;&#35760;\textit{End-of-Utterance} (EoU) &#26377;&#32858;&#21512;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#23558;EoU&#26631;&#35760;&#31216;&#20026;"&#20250;&#35805;&#27880;&#24847;&#21147;&#27719;&#38598;&#28857;"&#65288;conv-attn sinks&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;StreamingDialogue&#65292;&#23558;&#38271;&#23545;&#35805;&#21382;&#21490;&#21387;&#32553;&#20026;conv-attn&#27785;&#28857;&#65292;&#24182;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20174;&#32780;&#20351;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#27785;&#28857;&#25968;&#37327;&#65288;&#21363;&#35805;&#35821;&#25968;&#37327;&#65289;&#30340;&#24179;&#26041;&#25104;&#27491;&#27604;&#12290;&#24403;&#21069;&#30340;LLMs&#24050;&#32463;&#23637;&#31034;&#20102;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#65292;&#31383;&#21475;&#22823;&#23567;&#36798;&#21040;200k&#29978;&#33267;&#26356;&#22823;&#12290;&#36890;&#36807;&#23558;&#35805;&#35821;&#21387;&#32553;&#20026;EoUs&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#28508;&#21147;&#22788;&#29702;&#36229;&#36807;200k&#26465;&#35805;&#35821;&#65292;&#23454;&#29616;&#38271;&#26102;&#38388;&#23545;&#35805;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08312v1 Announce Type: cross  Abstract: Standard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues. According to our observation, dialogue contexts are highly structured, and the special token of \textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate information. We refer to the EoU tokens as ``conversational attention sinks'' (conv-attn sinks). Accordingly, we introduce StreamingDialogue, which compresses long dialogue history into conv-attn sinks with minimal losses, and thus reduces computational complexity quadratically with the number of sinks (i.e., the number of utterances). Current LLMs already demonstrate the ability to handle long context window, e.g., a window size of 200k or more. To this end, by compressing utterances into EoUs, our method has the potential to handle more than 200k of utterances, resulting in a prolonged dialogue learning. In order to minimize informatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21311;&#21517;&#20247;&#21253;&#35780;&#20272;&#24179;&#21488;&#65292;BingJian&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#31454;&#20105;&#24615;&#35780;&#20998;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#35780;&#20272;&#23458;&#35266;&#38382;&#39064;&#12289;&#24573;&#35270;&#20027;&#35266;&#38382;&#39064;&#12289;&#20351;&#29992;&#20013;&#24515;&#21270;&#25968;&#25454;&#38598;&#12289;&#24573;&#35270;&#20010;&#24615;&#21270;&#22240;&#32032;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08305</link><description>&lt;p&gt;
&#22522;&#20110;&#21311;&#21517;&#20247;&#21253;&#24179;&#21488;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20010;&#24615;&#21270;&#35780;&#20272;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08305
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21311;&#21517;&#20247;&#21253;&#35780;&#20272;&#24179;&#21488;&#65292;BingJian&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#31454;&#20105;&#24615;&#35780;&#20998;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#35780;&#20272;&#23458;&#35266;&#38382;&#39064;&#12289;&#24573;&#35270;&#20027;&#35266;&#38382;&#39064;&#12289;&#20351;&#29992;&#20013;&#24515;&#21270;&#25968;&#25454;&#38598;&#12289;&#24573;&#35270;&#20010;&#24615;&#21270;&#22240;&#32032;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#22312;&#25552;&#21319;&#20854;&#24615;&#33021;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20197;&#24448;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#35780;&#20272;&#23458;&#35266;&#38382;&#39064;&#65292;&#24573;&#35270;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#38750;&#24120;&#26222;&#36941;&#30340;&#20027;&#35266;&#38382;&#39064;&#30340;&#35780;&#20272;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#20013;&#24515;&#21270;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#38382;&#39064;&#24211;&#38598;&#20013;&#22312;&#35780;&#20272;&#24179;&#21488;&#26412;&#36523;&#12290;&#32780;&#19988;&#65292;&#36825;&#20123;&#24179;&#21488;&#37319;&#29992;&#30340;&#35780;&#20272;&#36807;&#31243;&#32463;&#24120;&#24573;&#35270;&#20102;&#20010;&#24615;&#21270;&#22240;&#32032;&#65292;&#26410;&#32771;&#34385;&#35780;&#20272;&#32773;&#21644;&#34987;&#35780;&#20272;&#27169;&#22411;&#30340;&#20010;&#20307;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21311;&#21517;&#20247;&#21253;&#35780;&#20272;&#24179;&#21488;&#8212;&#8212;&#28851;&#21073;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#31454;&#20105;&#24615;&#35780;&#20998;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08305v1 Announce Type: new  Abstract: Large language model evaluation plays a pivotal role in the enhancement of its capacity. Previously, numerous methods for evaluating large language models have been proposed in this area. Despite their effectiveness, these existing works mainly focus on assessing objective questions, overlooking the capability to evaluate subjective questions which is extremely common for large language models. Additionally, these methods predominantly utilize centralized datasets for evaluation, with question banks concentrated within the evaluation platforms themselves. Moreover, the evaluation processes employed by these platforms often overlook personalized factors, neglecting to consider the individual characteristics of both the evaluators and the models being evaluated. To address these limitations, we propose a novel anonymous crowd-sourcing evaluation platform, BingJian, for large language models that employs a competitive scoring mechanism wher
&lt;/p&gt;</description></item><item><title>Gemma&#26159;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#25152;&#26500;&#24314;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.08295</link><description>&lt;p&gt;
Gemma&#65306;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#30340;&#24320;&#25918;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gemma: Open Models Based on Gemini Research and Technology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08295
&lt;/p&gt;
&lt;p&gt;
Gemma&#26159;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#25152;&#26500;&#24314;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Gemma&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Gemini&#27169;&#22411;&#30740;&#31350;&#21644;&#25216;&#26415;&#26500;&#24314;&#30340;&#36731;&#37327;&#32423;&#12289;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#12290;Gemma&#27169;&#22411;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#23398;&#26415;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#20004;&#20010;&#35268;&#27169;&#30340;&#27169;&#22411;&#65288;20&#20159;&#21644;70&#20159;&#21442;&#25968;&#65289;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#26816;&#26597;&#28857;&#12290;Gemma&#22312;18&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#20219;&#21153;&#20013;&#65292;&#26377;11&#20010;&#20219;&#21153;&#20248;&#20110;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#24182;&#23545;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#36131;&#20219;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21516;&#26102;&#35814;&#32454;&#25551;&#36848;&#20102;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#12290;&#25105;&#20204;&#30456;&#20449;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23454;&#29616;&#19979;&#19968;&#27874;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#26032;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08295v1 Announce Type: cross  Abstract: This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.
&lt;/p&gt;</description></item><item><title>GPST&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20004;&#20010;&#27169;&#22411;&#23454;&#29616;&#23545;&#21407;&#22987;&#25991;&#26412;&#30340;&#39640;&#24182;&#34892;&#39044;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;&#20043;&#21069;SLM&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#21644;&#39034;&#24207;&#35757;&#32451;&#30340;&#38480;&#21046;&#65292;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#21516;&#31561;&#35268;&#27169;&#30340;GPT-2&#12290;</title><link>https://arxiv.org/abs/2403.08293</link><description>&lt;p&gt;
&#29983;&#25104;&#39044;&#35757;&#32451;&#32467;&#26500;&#21270;Transformer&#65306;&#35268;&#27169;&#21270;&#30340;&#26080;&#30417;&#30563;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08293
&lt;/p&gt;
&lt;p&gt;
GPST&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20004;&#20010;&#27169;&#22411;&#23454;&#29616;&#23545;&#21407;&#22987;&#25991;&#26412;&#30340;&#39640;&#24182;&#34892;&#39044;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;&#20043;&#21069;SLM&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#21644;&#39034;&#24207;&#35757;&#32451;&#30340;&#38480;&#21046;&#65292;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#21516;&#31561;&#35268;&#27169;&#30340;GPT-2&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#20197;&#20174;&#24038;&#21040;&#21491;&#30340;&#26041;&#24335;&#36880;&#27493;&#29983;&#25104;&#24102;&#26377;&#20854;&#21477;&#27861;&#26641;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29983;&#25104;&#39044;&#35757;&#32451;&#32467;&#26500;&#21270;Transformer&#65288;GPST&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35268;&#27169;&#21270;&#30340;&#26080;&#30417;&#30563;SLM&#65292;&#33021;&#22815;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#39640;&#24182;&#34892;&#39044;&#35757;&#32451;&#12290;GPST&#35268;&#36991;&#20102;&#20043;&#21069;SLM&#30340;&#19968;&#20123;&#38480;&#21046;&#65292;&#27604;&#22914;&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#21644;&#39034;&#24207;&#35757;&#32451;&#12290;&#23427;&#30001;&#20004;&#20010;&#32452;&#20214;&#32452;&#25104;&#65292;&#19968;&#20010;&#36890;&#24120;&#30340;SLM&#21463;&#21333;&#21521;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#30340;&#30417;&#30563;&#65292;&#20197;&#21450;&#19968;&#20010;&#39069;&#22806;&#30340;&#32452;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#24341;&#23548;&#21477;&#27861;&#35299;&#26512;&#26641;&#24182;&#35745;&#31639;&#25104;&#20998;&#34920;&#31034;&#65292;&#21463;&#21452;&#21521;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#30340;&#30417;&#30563;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#26367;&#20195;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#20004;&#20010;&#27169;&#22411;&#30340;&#32852;&#21512;&#24182;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#30828;EM&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;OpenWebText&#19978;&#23545;GPST&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;90&#20159;&#20010;token&#65292;&#24182;&#23637;&#31034;&#20102;GPST&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#65292;&#28085;&#30422;&#20102;&#19982;GPT-2&#30456;&#24403;&#35268;&#27169;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08293v1 Announce Type: cross  Abstract: A syntactic language model (SLM) incrementally generates a sentence with its syntactic tree in a left-to-right manner. We present Generative Pretrained Structured Transformers (GPST), an unsupervised SLM at scale capable of being pre-trained from scratch on raw texts with high parallelism. GPST circumvents the limitations of previous SLMs such as relying on gold trees and sequential training. It consists of two components, a usual SLM supervised by a uni-directional language modeling loss, and an additional composition model, which induces syntactic parse trees and computes constituent representations, supervised by a bi-directional language modeling loss. We propose a representation surrogate to enable joint parallel training of the two models in a hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion tokens, and demonstrate the superiority of GPST over GPT-2 with a comparable size in numerous tasks covering bot
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#35821;&#35328;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UltraFuser&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08281</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#25484;&#25569;&#25991;&#26412;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;
&lt;/p&gt;
&lt;p&gt;
Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08281
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#35821;&#35328;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UltraFuser&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#12289;&#32534;&#31243;&#20195;&#30721;&#21644;&#25968;&#23398;&#31526;&#21495;&#30340;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#24040;&#22823;&#65292;&#23545;&#20110;&#37027;&#20123;&#21162;&#21147;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20986;&#20102;&#22797;&#26434;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#34701;&#21512;&#24050;&#32463;&#39640;&#24230;&#19987;&#19994;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#34701;&#21512;&#26694;&#26550;UltraFuser&#21253;&#25324;&#19977;&#20010;&#24050;&#32463;&#22312;&#35821;&#35328;&#12289;&#32534;&#30721;&#21644;&#25968;&#23398;&#19978;&#24471;&#21040;&#20805;&#20998;&#35757;&#32451;&#30340;&#19987;&#23478;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#26469;&#28151;&#21512;&#19987;&#23478;&#30340;&#36755;&#20986;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#20276;&#38543;&#24179;&#34913;&#37319;&#26679;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#20197;&#30830;&#20445;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#26377;&#25928;&#35757;&#32451;&#34701;&#21512;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08281v1 Announce Type: cross  Abstract: Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously. Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. In this paper, we propose to fuse models that are already highly-specialized directly. The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. A token-level gating mechanism is introduced to blend the specialists' outputs. A two-stage training strategy accompanied by balanced sampling is designed to ensure stability. To effectively train the fused model, we further construct a 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;RECIPE4U&#25968;&#25454;&#38598;&#65292;&#20174;212&#21517;&#22823;&#23398;&#29983;&#30340;EFL&#20889;&#20316;&#35838;&#31243;&#20013;&#25910;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#23398;&#29983;&#19982;ChatGPT&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#24314;&#31435;&#20102;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#24847;&#22270;&#26816;&#27979;&#21644;&#28385;&#24847;&#24230;&#20272;&#35745;&#30340;&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.08272</link><description>&lt;p&gt;
RECIPE4U&#65306;EFL&#20889;&#20316;&#25945;&#32946;&#20013;&#30340;&#23398;&#29983;-ChatGPT&#20114;&#21160;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RECIPE4U: Student-ChatGPT Interaction Dataset in EFL Writing Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08272
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;RECIPE4U&#25968;&#25454;&#38598;&#65292;&#20174;212&#21517;&#22823;&#23398;&#29983;&#30340;EFL&#20889;&#20316;&#35838;&#31243;&#20013;&#25910;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#23398;&#29983;&#19982;ChatGPT&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#24314;&#31435;&#20102;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#24847;&#22270;&#26816;&#27979;&#21644;&#28385;&#24847;&#24230;&#20272;&#35745;&#30340;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2403.08272v1 &#31867;&#22411;&#65306;&#26032;&#30740;&#31350; &#25688;&#35201;&#65306;&#29983;&#25104;&#24335;AI&#22312;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#27491;&#22312;&#25193;&#22823;&#65292;&#20294;&#20851;&#20110;&#23398;&#29983;&#19982;AI&#31995;&#32479;&#20043;&#38388;&#22823;&#35268;&#27169;&#21644;&#30495;&#23454;&#19990;&#30028;&#20114;&#21160;&#30340;&#32463;&#39564;&#20998;&#26512;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RECIPE4U&#65288;&#22823;&#23398;RECIPE&#65289;&#65292;&#36825;&#26159;&#19968;&#20221;&#20174;&#19968;&#23398;&#26399;&#30340;&#23454;&#39564;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#20849;&#26377;212&#21517;&#22823;&#23398;&#29983;&#21442;&#19982;&#20854;&#20013;&#65292;&#20182;&#20204;&#21442;&#21152;&#30340;&#26159;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#65288;EFL&#65289;&#20889;&#20316;&#35838;&#31243;&#12290;&#22312;&#30740;&#31350;&#36807;&#31243;&#20013;&#65292;&#23398;&#29983;&#19982;ChatGPT&#36827;&#34892;&#23545;&#35805;&#20197;&#20462;&#25913;&#20182;&#20204;&#30340;&#25991;&#31456;&#12290;RECIPE4U&#21253;&#25324;&#20102;&#36825;&#20123;&#20114;&#21160;&#30340;&#20840;&#38754;&#35760;&#24405;&#65292;&#21253;&#25324;&#23545;&#35805;&#26085;&#24535;&#12289;&#23398;&#29983;&#24847;&#22270;&#12289;&#23398;&#29983;&#33258;&#35780;&#28385;&#24847;&#24230;&#20197;&#21450;&#23398;&#29983;&#30340;&#25991;&#31456;&#32534;&#36753;&#21382;&#21490;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26681;&#25454;&#25105;&#20204;&#30340;&#32534;&#30721;&#26041;&#26696;&#65292;&#20026;RECIPE4U&#20013;&#30340;&#23398;&#29983;&#35805;&#35821;&#26631;&#27880;&#20102;13&#20010;&#24847;&#22270;&#26631;&#31614;&#12290;&#25105;&#20204;&#20026;&#25945;&#32946;&#29615;&#22659;&#20013;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20004;&#20010;&#23376;&#20219;&#21153;&#24314;&#31435;&#20102;&#22522;&#20934;&#32467;&#26524;&#65306;&#24847;&#22270;&#26816;&#27979;&#21644;&#28385;&#24847;&#24230;&#20272;&#35745;&#12290;&#20316;&#20026;&#22522;&#30784;&#27493;&#39588;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08272v1 Announce Type: new  Abstract: The integration of generative AI in education is expanding, yet empirical analyses of large-scale and real-world interactions between students and AI systems still remain limited. Addressing this gap, we present RECIPE4U (RECIPE for University), a dataset sourced from a semester-long experiment with 212 college students in English as Foreign Language (EFL) writing courses. During the study, students engaged in dialogues with ChatGPT to revise their essays. RECIPE4U includes comprehensive records of these interactions, including conversation logs, students' intent, students' self-rated satisfaction, and students' essay edit histories. In particular, we annotate the students' utterances in RECIPE4U with 13 intention labels based on our coding schemes. We establish baseline results for two subtasks in task-oriented dialogue systems within educational contexts: intent detection and satisfaction estimation. As a foundational step, we explore 
&lt;/p&gt;</description></item><item><title>Skipformer&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#36339;&#36807;&#21644;&#24674;&#22797;&#8221;&#30340;Conformer&#26550;&#26500;&#65292;&#21487;&#20197;&#21160;&#24577;&#12289;&#19981;&#22343;&#21248;&#22320;&#21387;&#32553;&#24207;&#21015;&#36755;&#20837;&#38271;&#24230;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#39044;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08258</link><description>&lt;p&gt;
Skipformer&#65306;&#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#35821;&#38899;&#35782;&#21035;&#30340;&#36339;&#36807;&#21644;&#24674;&#22797;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Skipformer: A Skip-and-Recover Strategy for Efficient Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08258
&lt;/p&gt;
&lt;p&gt;
Skipformer&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#36339;&#36807;&#21644;&#24674;&#22797;&#8221;&#30340;Conformer&#26550;&#26500;&#65292;&#21487;&#20197;&#21160;&#24577;&#12289;&#19981;&#22343;&#21248;&#22320;&#21387;&#32553;&#24207;&#21015;&#36755;&#20837;&#38271;&#24230;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#39044;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Conformer&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#30340;&#20107;&#23454;&#26631;&#26438;&#27169;&#22411;&#12290;&#36890;&#24120;&#24341;&#20837;&#19968;&#20010;&#31354;&#30333;&#31526;&#21495;&#26469;&#23545;&#40784;CTC&#25110;RNN-T&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#24207;&#21015;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#38271;&#36755;&#20837;&#38271;&#24230;&#20250;&#23545;&#35745;&#31639;&#39044;&#31639;&#21644;&#20869;&#23384;&#28040;&#32791;&#36896;&#25104;&#20108;&#27425;&#36127;&#33655;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Skipformer&#30340;&#8220;&#36339;&#36807;&#21644;&#24674;&#22797;&#8221;Conformer&#26550;&#26500;&#65292;&#20197;&#21160;&#24577;&#21644;&#19981;&#22343;&#21248;&#22320;&#21387;&#32553;&#24207;&#21015;&#36755;&#20837;&#38271;&#24230;&#12290;Skipformer&#20351;&#29992;&#20013;&#38388;CTC&#36755;&#20986;&#20316;&#20026;&#26631;&#20934;&#23558;&#24103;&#20998;&#20026;&#19977;&#32452;&#65306;&#20851;&#38190;&#12289;&#36339;&#36807;&#21644;&#24573;&#30053;&#12290;&#20851;&#38190;&#32452;&#39304;&#36865;&#21040;&#19979;&#19968;&#20010;Conformer&#22359;&#65292;&#20854;&#36755;&#20986;&#19982;&#36339;&#36807;&#32452;&#36890;&#36807;&#21407;&#22987;&#26102;&#38388;&#39034;&#24207;&#32852;&#25509;&#20316;&#20026;&#26368;&#32456;&#32534;&#30721;&#22120;&#36755;&#20986;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;Aishell-1&#19978;&#23558;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#20943;&#23569;&#20102;31&#20493;&#65292;&#22312;Librispeech&#35821;&#26009;&#24211;&#19978;&#20943;&#23569;&#20102;22&#20493;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#21487;&#23454;&#29616;&#26356;&#22909;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08258v1 Announce Type: new  Abstract: Conformer-based attention models have become the de facto backbone model for Automatic Speech Recognition tasks. A blank symbol is usually introduced to align the input and output sequences for CTC or RNN-T models. Unfortunately, the long input length overloads computational budget and memory consumption quadratically by attention mechanism. In this work, we propose a "Skip-and-Recover" Conformer architecture, named Skipformer, to squeeze sequence input length dynamically and inhomogeneously. Skipformer uses an intermediate CTC output as criteria to split frames into three groups: crucial, skipping and ignoring. The crucial group feeds into next conformer blocks and its output joint with skipping group by original temporal order as the final encoder output. Experiments show that our model reduces the input sequence length by 31 times on Aishell-1 and 22 times on Librispeech corpus. Meanwhile, the model can achieve better recognition accu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#27969;&#30021;&#21477;&#23376;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#36807;&#28388;&#21644;&#23567;&#22411;&#27169;&#22411;&#35757;&#32451;&#23454;&#29616;&#20102;&#19981;&#27969;&#30021;&#26816;&#27979;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.08229</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35821;&#31687;&#29983;&#25104;&#22120;&#25552;&#21319;&#19981;&#27969;&#30021;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Boosting Disfluency Detection with Large Language Model as Disfluency Generator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19981;&#27969;&#30021;&#21477;&#23376;&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#30340;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#36807;&#28388;&#21644;&#23567;&#22411;&#27169;&#22411;&#35757;&#32451;&#23454;&#29616;&#20102;&#19981;&#27969;&#30021;&#26816;&#27979;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#19981;&#27969;&#30021;&#26816;&#27979;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#26114;&#36149;&#19988;&#31232;&#32570;&#30340;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#19968;&#20123;&#26041;&#27861;&#37319;&#29992;&#21551;&#21457;&#24335;&#25110;&#32479;&#35745;&#29305;&#24449;&#26469;&#29983;&#25104;&#19981;&#27969;&#30021;&#21477;&#23376;&#65292;&#37096;&#20998;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21477;&#23376;&#24120;&#24120;&#20559;&#31163;&#30495;&#23454;&#22330;&#26223;&#65292;&#38480;&#21046;&#20102;&#25972;&#20307;&#27169;&#22411;&#25913;&#21892;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21331;&#36234;&#30340;&#29983;&#25104;&#21644;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#19981;&#27969;&#30021;&#21477;&#23376;&#20316;&#20026;&#22686;&#24378;&#25968;&#25454;&#12290;&#25105;&#20204;&#21033;&#29992;LLM&#29983;&#25104;&#22810;&#26679;&#19988;&#26356;&#30495;&#23454;&#30340;&#21477;&#23376;&#65292;&#36890;&#36807;&#20855;&#20307;&#25552;&#31034;&#36827;&#34892;&#24341;&#23548;&#65292;&#26080;&#38656;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36807;&#28388;&#26041;&#27861;&#26469;&#25552;&#39640;&#29983;&#25104;&#21477;&#23376;&#30340;&#36136;&#37327;&#65292;&#29992;&#20110;&#35757;&#32451;&#23567;&#22411;&#26816;&#27979;&#27169;&#22411;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08229v1 Announce Type: new  Abstract: Current disfluency detection methods heavily rely on costly and scarce human-annotated data. To tackle this issue, some approaches employ heuristic or statistical features to generate disfluent sentences, partially improving detection performance. However, these sentences often deviate from real-life scenarios, constraining overall model enhancement. In this study, we propose a lightweight data augmentation approach for disfluency detection, utilizing the superior generative and semantic understanding capabilities of large language model (LLM) to generate disfluent sentences as augmentation data. We leverage LLM to generate diverse and more realistic sentences guided by specific prompts, without the need for fine-tuning the LLM. Subsequently, we apply an uncertainty-aware data filtering approach to improve the quality of the generated sentences, utilized in training a small detection model for improved performance. Experiments using enha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#25506;&#35752;&#20102;BERT&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#29305;&#24615;&#20197;&#21450;&#20248;&#21270;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;BERT&#27169;&#22411;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#34920;&#29616;&#20986;&#30340;&#31283;&#20581;&#24615;&#33021;&#21644;&#25552;&#21319;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08217</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;BERT&#27169;&#22411;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Research on the Application of Deep Learning-based BERT Model in Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#25506;&#35752;&#20102;BERT&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#29305;&#24615;&#20197;&#21450;&#20248;&#21270;&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20102;BERT&#27169;&#22411;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#34920;&#29616;&#20986;&#30340;&#31283;&#20581;&#24615;&#33021;&#21644;&#25552;&#21319;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#19987;&#27880;&#20110;BERT&#27169;&#22411;&#12290;&#25991;&#31456;&#39318;&#20808;&#20171;&#32461;&#20102;&#24773;&#24863;&#20998;&#26512;&#30340;&#22522;&#26412;&#27010;&#24565;&#20197;&#21450;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#35813;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#38543;&#21518;&#65292;&#28145;&#20837;&#25506;&#35752;&#20102;BERT&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#29305;&#24615;&#12290;&#36890;&#36807;&#35814;&#32454;&#35299;&#37322;&#65292;&#38416;&#26126;&#20102;BERT&#27169;&#22411;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#25928;&#26524;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#24182;&#24471;&#21040;&#23454;&#39564;&#35777;&#23454;&#25903;&#25345;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BERT&#27169;&#22411;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#31283;&#20581;&#30340;&#24615;&#33021;&#65292;&#22312;&#24494;&#35843;&#21518;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;&#26368;&#21518;&#65292;&#25991;&#31456;&#24635;&#32467;&#20102;BERT&#27169;&#22411;&#22312;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08217v1 Announce Type: new  Abstract: This paper explores the application of deep learning techniques, particularly focusing on BERT models, in sentiment analysis. It begins by introducing the fundamental concept of sentiment analysis and how deep learning methods are utilized in this domain. Subsequently, it delves into the architecture and characteristics of BERT models. Through detailed explanation, it elucidates the application effects and optimization strategies of BERT models in sentiment analysis, supported by experimental validation. The experimental findings indicate that BERT models exhibit robust performance in sentiment analysis tasks, with notable enhancements post fine-tuning. Lastly, the paper concludes by summarizing the potential applications of BERT models in sentiment analysis and suggests directions for future research and practical implementations.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20316;&#32773;&#35782;&#21035;&#26041;&#38754;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#65292;&#26412;&#25991;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#35299;&#20915;&#20102;LLMs&#22312;&#20316;&#32773;&#39564;&#35777;&#21644;&#24402;&#23646;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08213</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#35782;&#21035;&#20316;&#32773;&#36523;&#20221;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Identify Authorship?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08213
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20316;&#32773;&#35782;&#21035;&#26041;&#38754;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#65292;&#26412;&#25991;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#35299;&#20915;&#20102;LLMs&#22312;&#20316;&#32773;&#39564;&#35777;&#21644;&#24402;&#23646;&#20013;&#30340;&#19977;&#20010;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#20934;&#35782;&#21035;&#20316;&#32773;&#36523;&#20221;&#23545;&#39564;&#35777;&#20869;&#23481;&#30495;&#23454;&#24615;&#21644;&#20943;&#23569;&#35823;&#23548;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290; &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25512;&#29702;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#20316;&#32773;&#20998;&#26512;&#65288;&#21253;&#25324;&#20316;&#32773;&#39564;&#35777;&#21644;&#24402;&#23646;&#65289;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290; &#26412;&#25991;&#23545;LLMs&#22312;&#36825;&#20123;&#20851;&#38190;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290; &#20256;&#32479;&#30740;&#31350;&#20381;&#36182;&#20110;&#25163;&#24037;&#21046;&#20316;&#30340;&#25991;&#20307;&#29305;&#24449;&#65292;&#32780;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#23884;&#20837;&#12290; &#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22312;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#28982;&#32780;&#22312;&#36328;&#39046;&#22495;&#24212;&#29992;&#20013;&#24448;&#24448;&#34920;&#29616;&#20986;&#24615;&#33021;&#19979;&#38477;&#65292;&#24182;&#25552;&#20379;&#26377;&#38480;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290; &#26412;&#25991;&#26088;&#22312;&#22238;&#31572;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#65306;&#65288;1&#65289;LLMs&#33021;&#21542;&#26377;&#25928;&#25191;&#34892;&#38646;&#26679;&#26412;&#12289;&#31471;&#21040;&#31471;&#30340;&#20316;&#32773;&#39564;&#35777;&#65311;&#65288;2&#65289;LLMs&#33021;&#21542;&#20934;&#30830;&#36827;&#34892;&#20316;&#32773;&#36523;&#20221;&#24402;&#23646;&#65311;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08213v1 Announce Type: new  Abstract: The ability to accurately identify authorship is crucial for verifying content authenticity and mitigating misinformation. Large Language Models (LLMs) have demonstrated exceptional capacity for reasoning and problem-solving. However, their potential in authorship analysis, encompassing authorship verification and attribution, remains underexplored. This paper conducts a comprehensive evaluation of LLMs in these critical tasks. Traditional studies have depended on hand-crafted stylistic features, whereas state-of-the-art approaches leverage text embeddings from pre-trained language models. These methods, which typically require fine-tuning on labeled data, often suffer from performance degradation in cross-domain applications and provide limited explainability. This work seeks to address three research questions: (1) Can LLMs perform zero-shot, end-to-end authorship verification effectively? (2) Are LLMs capable of accurately attributing
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#25552;&#31034;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#22312;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#33391;&#65292;&#36824;&#21487;&#20197;&#19982;&#29616;&#26377;&#25552;&#31034;&#26041;&#27861;&#25972;&#21512;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08211</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#23545;&#27604;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Contrastive Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08211
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#25552;&#31034;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#22312;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#33391;&#65292;&#36824;&#21487;&#20197;&#19982;&#29616;&#26377;&#25552;&#31034;&#26041;&#27861;&#25972;&#21512;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#26041;&#27861;&#22312;&#22686;&#24378;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#23545;&#27604;&#25552;&#31034;&#65288;CP&#65289;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#22320;&#22312;LLMs&#25552;&#20379;&#31572;&#26696;&#20043;&#21069;&#28155;&#21152;"&#35753;&#25105;&#20204;&#32473;&#20986;&#19968;&#20010;&#27491;&#30830;&#31572;&#26696;&#21644;&#19968;&#20010;&#38169;&#35823;&#31572;&#26696;"&#26469;&#28436;&#31034;LLMs&#26159;&#20307;&#38754;&#30340;&#23545;&#27604;&#25512;&#29702;&#32773;&#12290;&#23545;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#38646;&#36801;&#31227;&#23545;&#27604;&#25552;&#31034;&#25552;&#21319;&#20102;&#22312;&#19968;&#31995;&#21015;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#32780;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#23569;&#37327;&#36801;&#31227;&#31034;&#20363;&#65292;&#27604;&#22914;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;GPT-4&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22312;GSM8K&#19978;&#30340;&#20934;&#30830;&#29575;&#20174;35.9%&#21040;88.8%&#20197;&#21450;AQUA-RAT&#20174;41.3%&#21040;62.2%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#22823;&#22810;&#25968;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#20013;&#32988;&#36807;&#38646;&#36801;&#31227;CoT&#21644;&#23569;&#37327;&#36801;&#31227;CoT&#65292;&#36824;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#26080;&#32541;&#25972;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#25913;&#36827;&#25110;&#32773;&#31454;&#20105;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08211v1 Announce Type: cross  Abstract: Prompting methods play a crucial role in enhancing the capabilities of pre-trained large language models (LLMs). We explore how contrastive prompting (CP) significantly improves the ability of large language models to perform complex reasoning. We demonstrate that LLMs are decent contrastive reasoners by simply adding "Let's give a correct and a wrong answer." before LLMs provide answers. Experiments on two large language models show that zero-shot contrastive prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks without any hand-crafted few-shot examples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and AQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model. Our method not only surpasses zero-shot CoT and few-shot CoT in most arithmetic and commonsense reasoning tasks but also can seamlessly integrate with existing prompting methods, resulting in improved or comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#22320;&#29702;&#22810;&#35821;&#35328;&#32593;&#32476;&#35821;&#26009;&#24211;&#20013;&#30340;&#35821;&#26009;&#24211;&#21019;&#24314;&#20915;&#31574;&#23545;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#25913;&#36827;&#25968;&#25454;&#28165;&#27927;&#26041;&#27861;&#26469;&#25552;&#39640;&#29305;&#23450;&#35821;&#35328;-&#22269;&#23478;&#23376;&#35821;&#26009;&#24211;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#26410;&#20805;&#20998;&#20195;&#34920;&#30340;&#35821;&#35328;&#21644;&#20154;&#21475;&#32676;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.08198</link><description>&lt;p&gt;
&#39564;&#35777;&#21644;&#25506;&#32034;&#22823;&#35268;&#27169;&#22320;&#29702;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Validating and Exploring Large Geographic Corpora
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#22320;&#29702;&#22810;&#35821;&#35328;&#32593;&#32476;&#35821;&#26009;&#24211;&#20013;&#30340;&#35821;&#26009;&#24211;&#21019;&#24314;&#20915;&#31574;&#23545;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#25913;&#36827;&#25968;&#25454;&#28165;&#27927;&#26041;&#27861;&#26469;&#25552;&#39640;&#29305;&#23450;&#35821;&#35328;-&#22269;&#23478;&#23376;&#35821;&#26009;&#24211;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#26410;&#20805;&#20998;&#20195;&#34920;&#30340;&#35821;&#35328;&#21644;&#20154;&#21475;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#26009;&#24211;&#21019;&#24314;&#20915;&#31574;&#23545;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#22320;&#29702;&#32593;&#32476;&#35821;&#26009;&#24211;&#30340;&#24433;&#21709;&#12290;&#20174;Common Crawl&#33719;&#24471;&#30340;4270&#20159;&#35789;&#35821;&#35821;&#26009;&#24320;&#22987;&#65292;&#20351;&#29992;&#19977;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#20195;&#34920;&#29305;&#23450;&#35821;&#35328;-&#22269;&#23478;&#23545;&#22914;&#26032;&#35199;&#20848;&#33521;&#35821;&#30340;&#23376;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#65306;(i) &#29420;&#31435;&#35821;&#35328;&#35782;&#21035;&#31995;&#32479;&#30340;&#19968;&#33268;&#24615;&#65292;(ii) &#22522;&#20110;&#21704;&#24076;&#30340;&#37325;&#22797;&#25968;&#25454;&#21024;&#38500;&#65292;&#20197;&#21450;(iii) &#29305;&#23450;&#22320;&#28857;&#30340;&#24322;&#24120;&#20540;&#26816;&#27979;&#12290;&#28982;&#21518;&#36890;&#36807;&#20351;&#29992;&#35821;&#26009;&#30456;&#20284;&#24615;&#24230;&#37327;&#26469;&#23558;&#27599;&#20010;&#29983;&#25104;&#30340;&#35821;&#26009;&#24211;&#19982;&#22522;&#32447;&#25968;&#25454;&#38598;&#27604;&#36739;&#65292;&#35780;&#20272;&#20102;&#27599;&#20010;&#27493;&#39588;&#23545;&#35821;&#35328;&#32423;&#21035;&#21644;&#22269;&#23478;&#32423;&#21035;&#30340;&#24433;&#21709;&#12290;&#26088;&#22312;&#20102;&#35299;&#19978;&#28216;&#25968;&#25454;&#28165;&#27927;&#20915;&#31574;&#23545;&#19979;&#28216;&#35821;&#26009;&#24211;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#20851;&#27880;&#26410;&#34987;&#20805;&#20998;&#20195;&#34920;&#30340;&#35821;&#35328;&#21644;&#20154;&#21475;&#12290;&#35780;&#20272;&#34920;&#26126;&#65292;&#38543;&#30528;&#27599;&#20010;&#28165;&#29702;&#38454;&#27573;&#30340;&#36827;&#34892;&#65292;&#23376;&#35821;&#26009;&#24211;&#30340;&#26377;&#25928;&#24615;&#24471;&#21040;&#25913;&#21892;&#65292;&#20294;&#36825;&#31181;&#25913;&#21892;&#26159;&#19981;&#22343;&#21248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08198v1 Announce Type: new  Abstract: This paper investigates the impact of corpus creation decisions on large multi-lingual geographic web corpora. Beginning with a 427 billion word corpus derived from the Common Crawl, three methods are used to improve the quality of sub-corpora representing specific language-country pairs like New Zealand English: (i) the agreement of independent language identification systems, (ii) hash-based deduplication, and (iii) location-specific outlier detection. The impact of each of these steps is then evaluated at the language level and the country level by using corpus similarity measures to compare each resulting corpus with baseline data sets. The goal is to understand the impact of upstream data cleaning decisions on downstream corpora with a specific focus on under-represented languages and populations. The evaluation shows that the validity of sub-corpora is improved with each stage of cleaning but that this improvement is unevenly distr
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;SpeechColab Leaderboard&#65292;&#19968;&#20010;&#29992;&#20110;ASR&#35780;&#20272;&#30340;&#24320;&#28304;&#24179;&#21488;&#65292;&#25552;&#20379;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;ASR&#31995;&#32479;&#30340;&#26368;&#26032;&#25216;&#26415;&#29616;&#29366;&#65292;&#24182;&#37327;&#21270;&#20102;&#24471;&#20998;&#31649;&#36947;&#20013;&#19981;&#21516;&#32454;&#24494;&#20043;&#22788;&#23545;&#26368;&#32456;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.08196</link><description>&lt;p&gt;
SpeechColab&#25490;&#34892;&#27036;&#65306;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#35780;&#20272;&#30340;&#24320;&#28304;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
SpeechColab Leaderboard: An Open-Source Platform for Automatic Speech Recognition Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08196
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;SpeechColab Leaderboard&#65292;&#19968;&#20010;&#29992;&#20110;ASR&#35780;&#20272;&#30340;&#24320;&#28304;&#24179;&#21488;&#65292;&#25552;&#20379;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;ASR&#31995;&#32479;&#30340;&#26368;&#26032;&#25216;&#26415;&#29616;&#29366;&#65292;&#24182;&#37327;&#21270;&#20102;&#24471;&#20998;&#31649;&#36947;&#20013;&#19981;&#21516;&#32454;&#24494;&#20043;&#22788;&#23545;&#26368;&#32456;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36807;&#21435;&#21313;&#24180;&#28145;&#24230;&#23398;&#20064;&#28010;&#28526;&#30340;&#28044;&#29616;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#23548;&#33268;&#20102;&#35768;&#22810;&#21487;&#20844;&#24320;&#35775;&#38382;&#30340;ASR&#31995;&#32479;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#31995;&#32479;&#27491;&#22312;&#31215;&#26497;&#22320;&#34701;&#20837;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;ASR&#31995;&#32479;&#30340;&#20844;&#27491;&#21644;&#21487;&#22797;&#21046;&#35780;&#20272;&#38754;&#20020;&#30528;&#30001;&#20110;&#21508;&#31181;&#20851;&#38190;&#24494;&#22937;&#20043;&#22788;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SpeechColab&#25490;&#34892;&#27036;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#24320;&#28304;&#24179;&#21488;&#65292;&#19987;&#20026;ASR&#35780;&#20272;&#32780;&#35774;&#35745;&#12290;&#20511;&#21161;&#36825;&#20010;&#24179;&#21488;&#65306;(i)&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;ASR&#31995;&#32479;&#30340;&#26368;&#26032;&#25216;&#26415;&#29616;&#29366;&#65292;&#28085;&#30422;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#24037;&#19994;&#21830;&#19994;&#26381;&#21153;&#12290; (ii)&#25105;&#20204;&#37327;&#21270;&#20102;&#24471;&#20998;&#31649;&#36947;&#20013;&#19981;&#21516;&#32454;&#24494;&#20043;&#22788;&#23545;&#26368;&#32456;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21253;&#25324;&#19982;&#22823;&#20889;&#12289;&#26631;&#28857;&#12289;&#25554;&#20837;&#35821;&#12289;&#32553;&#30053;&#35821;&#12289;&#21516;&#20041;&#35789;&#20351;&#29992;&#12289;&#22797;&#21512;&#35789;&#31561;&#30456;&#20851;&#30340;&#24494;&#22937;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08196v1 Announce Type: new  Abstract: In the wake of the surging tide of deep learning over the past decade, Automatic Speech Recognition (ASR) has garnered substantial attention, leading to the emergence of numerous publicly accessible ASR systems that are actively being integrated into our daily lives. Nonetheless, the impartial and replicable evaluation of these ASR systems encounters challenges due to various crucial subtleties. In this paper we introduce the SpeechColab Leaderboard, a general-purpose, open-source platform designed for ASR evaluation. With this platform: (i) We report a comprehensive benchmark, unveiling the current state-of-the-art panorama for ASR systems, covering both open-source models and industrial commercial services. (ii) We quantize how distinct nuances in the scoring pipeline influence the final benchmark outcomes. These include nuances related to capitalization, punctuation, interjection, contraction, synonym usage, compound words, etc. These
&lt;/p&gt;</description></item><item><title>MoleculeQA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20998;&#23376;&#29702;&#35299;&#20013;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;62K&#20010;QA&#23545;&#65292;&#26159;&#31532;&#19968;&#20010;&#20998;&#23376;&#20107;&#23454;&#20559;&#35265;&#35780;&#20272;&#30340;&#22522;&#20934;&#65292;&#20063;&#26159;&#26368;&#22823;&#30340;&#20998;&#23376;&#30740;&#31350;QA&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.08192</link><description>&lt;p&gt;
MoleculeQA: &#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20998;&#23376;&#29702;&#35299;&#20013;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MoleculeQA: A Dataset to Evaluate Factual Accuracy in Molecular Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08192
&lt;/p&gt;
&lt;p&gt;
MoleculeQA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20998;&#23376;&#29702;&#35299;&#20013;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#20855;&#26377;62K&#20010;QA&#23545;&#65292;&#26159;&#31532;&#19968;&#20010;&#20998;&#23376;&#20107;&#23454;&#20559;&#35265;&#35780;&#20272;&#30340;&#22522;&#20934;&#65292;&#20063;&#26159;&#26368;&#22823;&#30340;&#20998;&#23376;&#30740;&#31350;QA&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#23376;&#30740;&#31350;&#20013;&#21457;&#25381;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#28982;&#32780;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#29983;&#25104;&#38169;&#35823;&#20449;&#24687;&#65292;&#32473;&#20934;&#30830;&#30340;&#20998;&#23376;&#29702;&#35299;&#24102;&#26469;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#29983;&#25104;&#20869;&#23481;&#35780;&#20272;&#25351;&#26631;&#26080;&#27861;&#35780;&#20272;&#27169;&#22411;&#22312;&#20998;&#23376;&#29702;&#35299;&#20013;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#32416;&#27491;&#20107;&#23454;&#35780;&#20272;&#30340;&#32570;&#22833;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MoleculeQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;23K&#20010;&#20998;&#23376;&#30340;62K&#20010;QA&#23545;&#12290;&#27599;&#20010;QA&#23545;&#30001;&#19968;&#20010;&#25163;&#21160;&#38382;&#39064;&#12289;&#19968;&#20010;&#27491;&#36873;&#39033;&#21644;&#19977;&#20010;&#36127;&#36873;&#39033;&#32452;&#25104;&#65292;&#24182;&#19988;&#19982;&#26435;&#23041;&#20998;&#23376;&#35821;&#26009;&#24211;&#20013;&#30340;&#20998;&#23376;&#25551;&#36848;&#20855;&#26377;&#19968;&#33268;&#30340;&#35821;&#20041;&#12290;MoleculeQA&#19981;&#20165;&#26159;&#29992;&#20110;&#20998;&#23376;&#20107;&#23454;&#20559;&#35265;&#35780;&#20272;&#30340;&#31532;&#19968;&#20010;&#22522;&#20934;&#65292;&#20063;&#26159;&#29992;&#20110;&#20998;&#23376;&#30740;&#31350;&#30340;&#26368;&#22823;QA&#25968;&#25454;&#38598;&#12290;&#23545;&#29616;&#26377;&#20998;&#23376;LLMs&#22312;MoleculeQA&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#35780;&#20272;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24182;&#25351;&#20986;&#20102;&#19968;&#20123;&#29305;&#21035;&#20851;&#38190;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08192v1 Announce Type: new  Abstract: Large language models are playing an increasingly significant role in molecular research, yet existing models often generate erroneous information, posing challenges to accurate molecular comprehension. Traditional evaluation metrics for generated content fail to assess a model's accuracy in molecular understanding. To rectify the absence of factual evaluation, we present MoleculeQA, a novel question answering (QA) dataset which possesses 62K QA pairs over 23K molecules. Each QA pair, composed of a manual question, a positive option and three negative options, has consistent semantics with a molecular description from authoritative molecular corpus. MoleculeQA is not only the first benchmark for molecular factual bias evaluation but also the largest QA dataset for molecular research. A comprehensive evaluation on MoleculeQA for existing molecular LLMs exposes their deficiencies in specific areas and pinpoints several particularly crucial
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#33258;&#21160;&#35789;&#38388;&#26631;&#27880;&#65292;&#36890;&#36807;&#23884;&#20837;&#24335;&#32763;&#35793;&#20449;&#24687;&#25552;&#21319;&#31070;&#32463;&#27169;&#22411;&#24615;&#33021;&#36798;&#21040;&#25552;&#21319;&#65292;&#23588;&#20854;&#22312;&#22788;&#29702;&#21644;&#35299;&#37322;&#36866;&#24230;&#25968;&#25454;&#28304;&#26102;&#12290;</title><link>https://arxiv.org/abs/2403.08189</link><description>&lt;p&gt;
&#23884;&#20837;&#24335;&#32763;&#35793;&#29992;&#20110;&#20302;&#36164;&#28304;&#33258;&#21160;&#20114;&#35793;&#35789;&#27719;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Embedded Translations for Low-resource Automated Glossing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08189
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#33258;&#21160;&#35789;&#38388;&#26631;&#27880;&#65292;&#36890;&#36807;&#23884;&#20837;&#24335;&#32763;&#35793;&#20449;&#24687;&#25552;&#21319;&#31070;&#32463;&#27169;&#22411;&#24615;&#33021;&#36798;&#21040;&#25552;&#21319;&#65292;&#23588;&#20854;&#22312;&#22788;&#29702;&#21644;&#35299;&#37322;&#36866;&#24230;&#25968;&#25454;&#28304;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#33258;&#21160;&#35789;&#38388;&#26631;&#27880;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;&#35789;&#38388;&#26631;&#27880;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#23884;&#20837;&#24335;&#32763;&#35793;&#20449;&#24687;&#26469;&#22686;&#24378;&#22522;&#20110;&#30828;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20855;&#20307;&#26469;&#35828;&#26159;BERT&#21644;T5&#65289;&#23545;&#36825;&#20123;&#32763;&#35793;&#36827;&#34892;&#32534;&#30721;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23383;&#31526;&#32423;&#35299;&#30721;&#22120;&#26469;&#29983;&#25104;&#26631;&#27880;&#36755;&#20986;&#12290;&#22312;&#36825;&#20123;&#22686;&#24378;&#30340;&#24110;&#21161;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;SIGMORPHON 2023&#35789;&#38388;&#26631;&#27880;&#20849;&#20139;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#23637;&#29616;&#20986;&#20102;&#24179;&#22343;&#25552;&#21319;3.97\%&#12290;&#22312;&#27169;&#25311;&#30340;&#36229;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#65292;&#21363;&#20351;&#21482;&#35757;&#32451;&#20102;100&#20010;&#21477;&#23376;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#20063;&#27604;&#31616;&#21333;&#30340;&#30828;&#27880;&#24847;&#21147;&#22522;&#32447;&#24179;&#22343;&#25552;&#21319;&#20102;9.78\%&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;&#32763;&#35793;&#20449;&#24687;&#22312;&#25552;&#21319;&#31995;&#32479;&#24615;&#33021;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#22788;&#29702;&#21644;&#35299;&#37322;&#36866;&#24230;&#25968;&#25454;&#28304;&#26102;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#36825;&#19968;&#39046;&#22495;&#26410;&#26469;&#30740;&#31350;&#25351;&#26126;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08189v1 Announce Type: new  Abstract: We investigate automatic interlinear glossing in low-resource settings. We augment a hard-attentional neural model with embedded translation information extracted from interlinear glossed text. After encoding these translations using large language models, specifically BERT and T5, we introduce a character-level decoder for generating glossed output. Aided by these enhancements, our model demonstrates an average improvement of 3.97\%-points over the previous state of the art on datasets from the SIGMORPHON 2023 Shared Task on Interlinear Glossing. In a simulated ultra low-resource setting, trained on as few as 100 sentences, our system achieves an average 9.78\%-point improvement over the plain hard-attentional baseline. These results highlight the critical role of translation information in boosting the system's performance, especially in processing and interpreting modest data sources. Our findings suggest a promising avenue for the do
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35786;&#26029;&#38889;&#22269;&#20799;&#31461;&#35328;&#35821;&#22768;&#38899;&#38556;&#30861;&#21457;&#38899;&#38382;&#39064;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843; wav2vec 2.0 XLS-R &#27169;&#22411;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#23401;&#23376;&#20204;&#21457;&#38899;&#30340;&#39640;&#20934;&#30830;&#24230;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.08187</link><description>&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035; (ASR) &#29992;&#20110;&#35786;&#26029;&#38889;&#22269;&#20799;&#31461;&#35328;&#35821;&#22768;&#38899;&#38556;&#30861;&#30340;&#21457;&#38899;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) for the Diagnosis of pronunciation of Speech Sound Disorders in Korean children
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35786;&#26029;&#38889;&#22269;&#20799;&#31461;&#35328;&#35821;&#22768;&#38899;&#38556;&#30861;&#21457;&#38899;&#38382;&#39064;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843; wav2vec 2.0 XLS-R &#27169;&#22411;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#23545;&#23401;&#23376;&#20204;&#21457;&#38899;&#30340;&#39640;&#20934;&#30830;&#24230;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035; (ASR) &#27169;&#22411;&#65292;&#26088;&#22312;&#35786;&#26029;&#24739;&#26377;&#35328;&#35821;&#22768;&#38899;&#38556;&#30861; (SSDs) &#30340;&#20799;&#31461;&#30340;&#21457;&#38899;&#38382;&#39064;&#65292;&#20197;&#21462;&#20195;&#20020;&#24202;&#31243;&#24207;&#20013;&#30340;&#25163;&#21160;&#36716;&#24405;&#12290;&#25105;&#20204;&#23545; wav2vec 2.0 XLS-R &#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#23454;&#38469;&#21457;&#38899;&#32780;&#38750;&#29616;&#26377;&#21333;&#35789;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#26469;&#33258;137&#21517;&#35328;&#35821;&#34920;&#36798;&#19981;&#36275;&#30340;&#20799;&#31461;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#20799;&#31461;&#21457;&#38899;73&#20010;&#20026;&#23454;&#38469;&#20020;&#24202;&#35786;&#26029;&#36873;&#23450;&#30340;&#38889;&#35821;&#21333;&#35789;&#12290;&#27169;&#22411;&#23545;&#36825;&#20123;&#21333;&#35789;&#30340;&#21457;&#38899;&#36827;&#34892;&#30340;&#39044;&#27979;&#19982;&#20154;&#24037;&#26631;&#27880;&#30340;&#20934;&#30830;&#29575;&#32422;&#20026;90%&#12290;&#23613;&#31649;&#27169;&#22411;&#20173;&#38656;&#25913;&#36827;&#20197;&#35782;&#21035;&#19981;&#28165;&#26970;&#30340;&#21457;&#38899;&#65292;&#20294;&#35813;&#30740;&#31350;&#34920;&#26126; ASR &#27169;&#22411;&#33021;&#22815;&#31616;&#21270;&#20018;&#34892;&#30340;&#35786;&#26029;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08187v1 Announce Type: new  Abstract: This study presents a model of automatic speech recognition (ASR) designed to diagnose pronunciation issues in children with speech sound disorders (SSDs) to replace manual transcriptions in clinical procedures. Since ASR models trained for general purposes primarily predict input speech into real words, employing a well-known high-performance ASR model for evaluating pronunciation in children with SSDs is impractical. We fine-tuned the wav2vec 2.0 XLS-R model to recognize speech as pronounced rather than as existing words. The model was fine-tuned with a speech dataset from 137 children with inadequate speech production pronouncing 73 Korean words selected for actual clinical diagnosis. The model's predictions of the pronunciations of the words matched the human annotations with about 90% accuracy. While the model still requires improvement in recognizing unclear pronunciation, this study demonstrates that ASR models can streamline comp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19987;&#20026;FEVER&#20219;&#21153;&#23450;&#21046;&#30340;&#20004;&#31181;&#20219;&#21153;&#29305;&#23450;&#30446;&#26631;&#20989;&#25968;&#65292;&#30456;&#27604;&#26631;&#20934;&#20132;&#21449;&#29109;&#22312;&#20107;&#23454;&#39564;&#35777;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#31867;&#21035;&#21152;&#26435;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08174</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20107;&#23454;&#39564;&#35777;&#30340;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Rethinking Loss Functions for Fact Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08174
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19987;&#20026;FEVER&#20219;&#21153;&#23450;&#21046;&#30340;&#20004;&#31181;&#20219;&#21153;&#29305;&#23450;&#30446;&#26631;&#20989;&#25968;&#65292;&#30456;&#27604;&#26631;&#20934;&#20132;&#21449;&#29109;&#22312;&#20107;&#23454;&#39564;&#35777;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#31867;&#21035;&#21152;&#26435;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;FEVER&#20849;&#20139;&#20219;&#21153;&#20013;&#20107;&#23454;&#39564;&#35777;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#34429;&#28982;&#20132;&#21449;&#29109;&#25439;&#22833;&#26159;&#35757;&#32451;&#21028;&#20915;&#39044;&#27979;&#22120;&#30340;&#26631;&#20934;&#30446;&#26631;&#65292;&#20294;&#23427;&#26410;&#33021;&#25429;&#25417;&#21040;FEVER&#21028;&#20915;&#31867;&#21035;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#38024;&#23545;FEVER&#30340;&#29305;&#23450;&#20219;&#21153;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#65292;&#25152;&#25552;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#20248;&#20110;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#12290;&#24403;&#36825;&#20123;&#30446;&#26631;&#19982;&#31616;&#21333;&#30340;&#31867;&#21035;&#21152;&#26435;&#30456;&#32467;&#21512;&#26102;&#65292;&#24615;&#33021;&#36827;&#19968;&#27493;&#25552;&#39640;&#65292;&#26377;&#25928;&#22320;&#20811;&#26381;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19981;&#24179;&#34913;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/yuta-mukobara/RLF-KGAT &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08174v1 Announce Type: cross  Abstract: We explore loss functions for fact verification in the FEVER shared task. While the cross-entropy loss is a standard objective for training verdict predictors, it fails to capture the heterogeneity among the FEVER verdict classes. In this paper, we develop two task-specific objectives tailored to FEVER. Experimental results confirm that the proposed objective functions outperform the standard cross-entropy. Performance is further improved when these objectives are combined with simple class weighting, which effectively overcomes the imbalance in the training data. The souce code is available at https://github.com/yuta-mukobara/RLF-KGAT
&lt;/p&gt;</description></item><item><title>MolBind &#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20026;&#22810;&#31181;&#27169;&#24577;&#35757;&#32451;&#32534;&#30721;&#22120;&#65292;&#23558;&#25152;&#26377;&#27169;&#24577;&#26144;&#23556;&#21040;&#20849;&#20139;&#29305;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#22810;&#27169;&#24577;&#35821;&#20041;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.08167</link><description>&lt;p&gt;
MolBind: &#22810;&#27169;&#24577;&#23545;&#40784;&#35821;&#35328;&#12289;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;
&lt;/p&gt;
&lt;p&gt;
MolBind: Multimodal Alignment of Language, Molecules, and Proteins
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08167
&lt;/p&gt;
&lt;p&gt;
MolBind &#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20026;&#22810;&#31181;&#27169;&#24577;&#35757;&#32451;&#32534;&#30721;&#22120;&#65292;&#23558;&#25152;&#26377;&#27169;&#24577;&#26144;&#23556;&#21040;&#20849;&#20139;&#29305;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#22810;&#27169;&#24577;&#35821;&#20041;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#23398;&#21644;&#21270;&#23398;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#21033;&#29992;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#23558;&#20998;&#23376;&#21450;&#20854;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#25972;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#24378;&#33647;&#29289;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#20165;&#38480;&#20110;&#20004;&#31181;&#27169;&#24577;&#65292;&#35774;&#35745;&#19968;&#20010;&#32479;&#19968;&#30340;&#32593;&#32476;&#26469;&#22788;&#29702;&#19981;&#21516;&#27169;&#24577;&#65288;&#20363;&#22914;&#33258;&#28982;&#35821;&#35328;&#12289;2D&#20998;&#23376;&#22270;&#12289;3D&#20998;&#23376;&#26500;&#35937;&#21644;3D&#34507;&#30333;&#36136;&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20043;&#38388;&#23384;&#22312;&#22266;&#26377;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08167v1 Announce Type: cross  Abstract: Recent advancements in biology and chemistry have leveraged multi-modal learning, integrating molecules and their natural language descriptions to enhance drug discovery. However, current pre-training frameworks are limited to two modalities, and designing a unified network to process different modalities (e.g., natural language, 2D molecular graphs, 3D molecular conformations, and 3D proteins) remains challenging due to inherent gaps among them. In this work, we propose MolBind, a framework that trains encoders for multiple modalities through contrastive learning, mapping all modalities to a shared feature space for multi-modal semantic alignment. To facilitate effective pre-training of MolBind on multiple modalities, we also build and collect a high-quality dataset with four modalities, MolBind-M4, including graph-language, conformation-language, graph-conformation, and conformation-protein paired data. MolBind shows superior zero-sh
&lt;/p&gt;</description></item><item><title>BAGEL &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24490;&#29615;&#25805;&#20316;&#23558;&#38543;&#26426;&#25506;&#32034;&#21040;&#30340;&#36712;&#36857;&#25110;&#21512;&#25104;&#25351;&#20196;&#36716;&#25442;&#20026;&#28436;&#31034;&#65292;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#22312;&#26032;&#29615;&#22659;&#20013;&#33258;&#20027;&#23398;&#20064;&#36866;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.08140</link><description>&lt;p&gt;
BAGEL: &#36890;&#36807;&#35821;&#35328;&#24341;&#23548;&#25506;&#32034;&#24341;&#23548;&#20195;&#29702;&#33258;&#20027;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BAGEL: Bootstrapping Agents by Guiding Exploration with Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08140
&lt;/p&gt;
&lt;p&gt;
BAGEL &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24490;&#29615;&#25805;&#20316;&#23558;&#38543;&#26426;&#25506;&#32034;&#21040;&#30340;&#36712;&#36857;&#25110;&#21512;&#25104;&#25351;&#20196;&#36716;&#25442;&#20026;&#28436;&#31034;&#65292;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#33021;&#22815;&#22312;&#26032;&#29615;&#22659;&#20013;&#33258;&#20027;&#23398;&#20064;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#29615;&#22659;&#65288;&#20363;&#22914;Web&#27983;&#35272;&#22120;&#21644;REST API&#65289;&#20013;&#36890;&#36807;&#25191;&#34892;&#21160;&#20316;&#26469;&#36981;&#24490;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#32463;&#24120;&#22312;&#27809;&#26377;&#20154;&#31867;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#25512;&#24191;&#21040;&#26032;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BAGEL&#65292;&#19968;&#31181;&#26080;&#38656;&#20154;&#31867;&#30417;&#30563;&#21363;&#21487;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;BAGEL&#23558;&#19968;&#32452;&#38543;&#26426;&#25506;&#32034;&#30340;&#36712;&#36857;&#25110;&#21512;&#25104;&#25351;&#20196;&#36716;&#25442;&#25104;&#28436;&#31034;&#65292;&#36890;&#36807;&#20004;&#20010;&#22122;&#22768;&#35821;&#35328;&#27169;&#22411;&#32452;&#20214;&#20043;&#38388;&#30340;&#24448;&#36820;&#26469;&#23436;&#25104;&#65306;&#19968;&#20010;&#23558;&#36712;&#36857;&#36716;&#25442;&#20026;&#21512;&#25104;&#25351;&#20196;&#30340;&#35821;&#35328;&#27169;&#22411;&#26631;&#35760;&#22120;&#65292;&#20197;&#21450;&#19968;&#20010;&#23558;&#21512;&#25104;&#25351;&#20196;&#26144;&#23556;&#20026;&#32463;&#36807;&#25913;&#36827;&#30340;&#36712;&#36857;&#30340;&#38646;&#26679;&#26412;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#12290;&#36890;&#36807;&#36845;&#20195;&#25191;&#34892;&#36825;&#20123;&#24448;&#36820;&#25805;&#20316;&#65292;BAGEL&#21487;&#20197;&#24555;&#36895;&#23558;&#26368;&#21021;&#30340;&#36712;&#36857;&#20998;&#24067;&#36716;&#21464;&#20026;&#37027;&#20123;&#34987;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#23436;&#21892;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#20351;&#29992;BAGEL&#28436;&#31034;&#26469;&#22312;&#27979;&#35797;&#26102;&#38388;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#19978;&#35843;&#38646;&#26679;&#26412;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08140v1 Announce Type: new  Abstract: Following natural language instructions by executing actions in digital environments (e.g. web-browsers and REST APIs) is a challenging task for language model (LM) agents. Unfortunately, LM agents often fail to generalize to new environments without human demonstrations. This work presents BAGEL, a method for bootstrapping LM agents without human supervision. BAGEL converts a seed set of randomly explored trajectories or synthetic instructions, into demonstrations, via round-trips between two noisy LM components: an LM labeler which converts a trajectory into a synthetic instruction, and a zero-shot LM agent which maps the synthetic instruction into a refined trajectory. By performing these round-trips iteratively, BAGEL quickly converts the initial distribution of trajectories towards those that are well-described by natural language. We use BAGEL demonstrations to adapt a zero shot LM agent at test time via in-context learning over re
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#26500;&#24314;&#31995;&#32479;&#65292;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#21019;&#24314;&#35774;&#35745;&#21345;&#29255;&#65292;&#24110;&#21161;&#35774;&#35745;&#24072;&#26356;&#22909;&#29702;&#35299;&#35774;&#35745;&#21551;&#31034;&#24182;&#25552;&#39640;&#27807;&#36890;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.08137</link><description>&lt;p&gt;
&#20174;&#35770;&#25991;&#21040;&#21345;&#29255;&#65306;&#21033;&#29992;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25913;&#21464;&#35774;&#35745;&#21551;&#31034;
&lt;/p&gt;
&lt;p&gt;
From Paper to Card: Transforming Design Implications with Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08137
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#26500;&#24314;&#31995;&#32479;&#65292;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#21019;&#24314;&#35774;&#35745;&#21345;&#29255;&#65292;&#24110;&#21161;&#35774;&#35745;&#24072;&#26356;&#22909;&#29702;&#35299;&#35774;&#35745;&#21551;&#31034;&#24182;&#25552;&#39640;&#27807;&#36890;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#27969;&#35774;&#35745;&#21551;&#31034;&#22312;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#20013;&#21457;&#34920;&#23398;&#26415;&#35770;&#25991;&#26102;&#24456;&#24120;&#35265;&#65292;&#28982;&#32780;&#36825;&#20123;&#35770;&#25991;&#24456;&#23569;&#34987;&#35774;&#35745;&#24072;&#20204;&#38405;&#35835;&#21644;&#20351;&#29992;&#12290;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#35774;&#35745;&#21345;&#29255;&#20316;&#20026;&#19968;&#31181;&#32763;&#35793;&#36164;&#28304;&#65292;&#20197;&#26356;&#26131;&#28040;&#21270;&#21644;&#33719;&#21462;&#30340;&#26041;&#24335;&#20256;&#36798;&#35770;&#25991;&#20013;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#65292;&#20197;&#21327;&#21161;&#35774;&#35745;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#35774;&#35745;&#21345;&#29255;&#21487;&#33021;&#32791;&#26102;&#65292;&#32780;&#20316;&#32773;&#21487;&#33021;&#32570;&#20047;&#21046;&#20316;&#21345;&#29255;&#30340;&#36164;&#28304;&#21644;&#30693;&#35782;&#12290;&#36890;&#36807;&#36845;&#20195;&#35774;&#35745;&#36807;&#31243;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#20351;&#29992;LLM&#21644;&#25991;&#26412;&#36716;&#22270;&#20687;&#27169;&#22411;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#24110;&#21161;&#21019;&#24314;&#35774;&#35745;&#21345;&#29255;&#12290;&#25105;&#20204;&#23545;&#35774;&#35745;&#24072;&#65288;N=21&#65289;&#21644;&#25152;&#36873;&#35770;&#25991;&#30340;&#20316;&#32773;&#65288;N=12&#65289;&#36827;&#34892;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;&#35774;&#35745;&#24072;&#35748;&#20026;&#25105;&#20204;&#35774;&#35745;&#21345;&#29255;&#20013;&#30340;&#35774;&#35745;&#21551;&#31034;&#27604;&#38405;&#35835;&#21407;&#22987;&#35770;&#25991;&#25991;&#26412;&#26356;&#20855;&#21551;&#21457;&#24615;&#21644;&#29983;&#25104;&#24615;&#65292;&#32780;&#20316;&#32773;&#21017;&#35748;&#20026;&#25105;&#20204;&#30340;&#31995;&#32479;&#26159;&#20256;&#36798;&#20182;&#20204;&#35774;&#35745;&#21551;&#31034;&#30340;&#26377;&#25928;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08137v1 Announce Type: cross  Abstract: Communicating design implications is common within the HCI community when publishing academic papers, yet these papers are rarely read and used by designers. One solution is to use design cards as a form of translational resource that communicates valuable insights from papers in a more digestible and accessible format to assist in design processes. However, creating design cards can be time-consuming, and authors may lack the resources/know-how to produce cards. Through an iterative design process, we built a system that helps create design cards from academic papers using an LLM and text-to-image model. Our evaluation with designers (N=21) and authors of selected papers (N=12) revealed that designers perceived the design implications from our design cards as more inspiring and generative, compared to reading original paper texts, and the authors viewed our system as an effective way of communicating their design implications. We also
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#38544;&#31169;&#25919;&#31574;&#30340;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#20174;&#27861;&#24459;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#30740;&#31350;&#20013;&#30830;&#23450;&#20449;&#24687;&#20844;&#27491;&#24615;&#12289;&#34920;&#29616;&#20844;&#27491;&#24615;&#21644;&#36947;&#24503;/&#20262;&#29702;&#19982;&#38544;&#31169;&#25919;&#31574;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#25919;&#31574;&#30340;&#36873;&#39033;&#12290;</title><link>https://arxiv.org/abs/2403.08115</link><description>&lt;p&gt;
&#27861;&#24459;&#32422;&#26463;&#20294;&#19981;&#20844;&#24179;&#65311;&#26397;&#21521;&#35780;&#20272;&#38544;&#31169;&#25919;&#31574;&#20844;&#24179;&#24615;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Legally Binding but Unfair? Towards Assessing Fairness of Privacy Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#38544;&#31169;&#25919;&#31574;&#30340;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#20174;&#27861;&#24459;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#30740;&#31350;&#20013;&#30830;&#23450;&#20449;&#24687;&#20844;&#27491;&#24615;&#12289;&#34920;&#29616;&#20844;&#27491;&#24615;&#21644;&#36947;&#24503;/&#20262;&#29702;&#19982;&#38544;&#31169;&#25919;&#31574;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#25919;&#31574;&#30340;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#25919;&#31574;&#24212;&#24403;&#21578;&#30693;&#25968;&#25454;&#20027;&#20307;&#20854;&#25968;&#25454;&#20445;&#25252;&#26435;&#21033;&#65292;&#35299;&#37322;&#25968;&#25454;&#25511;&#21046;&#32773;&#30340;&#25968;&#25454;&#31649;&#29702;&#23454;&#36341;&#65292;&#24182;&#20351;&#20445;&#30041;&#26399;&#38480;&#25110;&#25968;&#25454;&#36716;&#31227;&#32473;&#31532;&#19977;&#26041;&#31561;&#20107;&#23454;&#36879;&#26126;&#21270;&#12290;&#38544;&#31169;&#25919;&#31574;&#21482;&#26377;&#22312;&#25968;&#25454;&#20027;&#20307;&#27491;&#30830;&#24863;&#30693;&#12289;&#35299;&#37322;&#12289;&#29702;&#35299;&#21644;&#20449;&#20219;&#26102;&#25165;&#33021;&#23454;&#29616;&#20854;&#30446;&#30340;&#12290;&#20854;&#20013;&#65292;&#36825;&#35201;&#27714;&#38544;&#31169;&#25919;&#31574;&#20197;&#20844;&#24179;&#30340;&#26041;&#24335;&#32534;&#20889;&#65292;&#20363;&#22914;&#19981;&#20351;&#29992;&#26497;&#31471;&#30340;&#26415;&#35821;&#65292;&#19981;&#38656;&#35201;&#29305;&#23450;&#30340;&#25945;&#32946;&#31243;&#24230;&#65292;&#25110;&#19981;&#20551;&#35774;&#29305;&#23450;&#30340;&#31038;&#20250;&#32972;&#26223;&#12290;&#22312;&#36825;&#20221;&#36827;&#34892;&#20013;&#30340;&#24037;&#20316;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#25105;&#20204;&#35780;&#20272;&#38544;&#31169;&#25919;&#31574;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#22522;&#26412;&#27861;&#24459;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#30740;&#31350;&#20013;&#30830;&#23450;&#20449;&#24687;&#20844;&#27491;&#24615;&#12289;&#34920;&#29616;&#20844;&#27491;&#24615;&#21644;&#36947;&#24503;/&#20262;&#29702;&#19982;&#38544;&#31169;&#25919;&#31574;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#35780;&#20272;&#25919;&#31574;&#30340;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08115v1 Announce Type: cross  Abstract: Privacy policies are expected to inform data subjects about their data protection rights. They should explain the data controller's data management practices, and make facts such as retention periods or data transfers to third parties transparent. Privacy policies only fulfill their purpose, if they are correctly perceived, interpreted, understood, and trusted by the data subject. Amongst others, this requires that a privacy policy is written in a fair way, e.g., it does not use polarizing terms, does not require a certain education, or does not assume a particular social background. In this work-in-progress paper, we outline our approach to assessing fairness in privacy policies. To this end, we identify from fundamental legal sources and fairness research, how the dimensions informational fairness, representational fairness and ethics/morality are related to privacy policies. We propose options to automatically assess policies in the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22240;&#26524;&#36335;&#24452;&#22270;&#25972;&#21512;&#21040;&#20154;&#31867;&#20013;&#24515;&#35774;&#35745;&#20013;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19987;&#29992;&#25554;&#20214;&#29992;&#20110;&#22312;&#32447;&#21327;&#20316;&#30333;&#26495;&#24179;&#21488;&#65292;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616;&#20854;&#25903;&#25345;&#35774;&#35745;&#36807;&#31243;&#20013;&#30340;&#20998;&#25955;&#21644;&#38598;&#20013;&#38454;&#27573;&#65292;&#20943;&#23569;&#35774;&#35745;&#24072;&#30340;&#35748;&#30693;&#36127;&#33655;&#24182;&#22686;&#21152;&#21019;&#36896;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08111</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#35774;&#35745;&#30340;AI&#36741;&#21161;&#22240;&#26524;&#36335;&#24452;&#22270;
&lt;/p&gt;
&lt;p&gt;
AI-Assisted Causal Pathway Diagram for Human-Centered Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22240;&#26524;&#36335;&#24452;&#22270;&#25972;&#21512;&#21040;&#20154;&#31867;&#20013;&#24515;&#35774;&#35745;&#20013;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19987;&#29992;&#25554;&#20214;&#29992;&#20110;&#22312;&#32447;&#21327;&#20316;&#30333;&#26495;&#24179;&#21488;&#65292;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616;&#20854;&#25903;&#25345;&#35774;&#35745;&#36807;&#31243;&#20013;&#30340;&#20998;&#25955;&#21644;&#38598;&#20013;&#38454;&#27573;&#65292;&#20943;&#23569;&#35774;&#35745;&#24072;&#30340;&#35748;&#30693;&#36127;&#33655;&#24182;&#22686;&#21152;&#21019;&#36896;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22240;&#26524;&#36335;&#24452;&#22270;&#65288;CPD&#65289;&#25972;&#21512;&#21040;&#20154;&#31867;&#20013;&#24515;&#35774;&#35745;&#65288;HCD&#65289;&#20013;&#65292;&#30740;&#31350;&#36825;&#20123;&#22270;&#22914;&#20309;&#22686;&#24378;&#35774;&#35745;&#36807;&#31243;&#30340;&#26089;&#26399;&#38454;&#27573;&#12290;&#24320;&#21457;&#20102;&#19987;&#29992;&#30340;CPD&#25554;&#20214;&#65292;&#29992;&#20110;&#22312;&#32447;&#21327;&#20316;&#30333;&#26495;&#24179;&#21488;Miro&#65292;&#20197;&#31616;&#21270;&#22270;&#30340;&#21019;&#24314;&#24182;&#25552;&#20379;&#23454;&#26102;&#30340;AI&#39537;&#21160;&#25351;&#23548;&#12290;&#36890;&#36807;&#19982;&#35774;&#35745;&#24072;&#65288;N=20&#65289;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;CPD&#30340;&#20998;&#25903;&#21644;&#20854;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#24378;&#35843;&#22312;&#35774;&#35745;&#36807;&#31243;&#30340;&#20998;&#25955;&#21644;&#38598;&#20013;&#38454;&#27573;&#37117;&#24471;&#21040;&#20102;&#25903;&#25345;&#12290;CPD&#20063;&#21487;&#20197;&#20419;&#36827;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#30340;&#27807;&#36890;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#25554;&#20214;&#26174;&#33879;&#20943;&#23569;&#20102;&#35774;&#35745;&#24072;&#30340;&#35748;&#30693;&#36127;&#33655;&#65292;&#24182;&#22312;&#22836;&#33041;&#39118;&#26292;&#36807;&#31243;&#20013;&#22686;&#21152;&#20102;&#20182;&#20204;&#30340;&#21019;&#36896;&#21147;&#65292;&#31361;&#26174;&#20102;AI&#36741;&#21161;&#24037;&#20855;&#22312;&#25903;&#25345;&#21019;&#36896;&#24615;&#24037;&#20316;&#21644;&#22522;&#20110;&#35777;&#25454;&#30340;&#35774;&#35745;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08111v1 Announce Type: cross  Abstract: This paper explores the integration of causal pathway diagrams (CPD) into human-centered design (HCD), investigating how these diagrams can enhance the early stages of the design process. A dedicated CPD plugin for the online collaborative whiteboard platform Miro was developed to streamline diagram creation and offer real-time AI-driven guidance. Through a user study with designers (N=20), we found that CPD's branching and its emphasis on causal connections supported both divergent and convergent processes during design. CPD can also facilitate communication among stakeholders. Additionally, we found our plugin significantly reduces designers' cognitive workload and increases their creativity during brainstorming, highlighting the implications of AI-assisted tools in supporting creative work and evidence-based designs.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Transformer&#27169;&#22411;&#21644;Context-Reverso&#25968;&#25454;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#28165;&#26224;&#24230;&#30340;&#21477;&#23376;</title><link>https://arxiv.org/abs/2403.08103</link><description>&lt;p&gt;
&#21033;&#29992;Context-Reverso&#25968;&#25454;&#20351;&#29992;Transformer&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#28165;&#26224;&#24230;&#30340;&#21477;&#23376;
&lt;/p&gt;
&lt;p&gt;
Contextual Clarity: Generating Sentences with Transformer Models using Context-Reverso Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08103
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Transformer&#27169;&#22411;&#21644;Context-Reverso&#25968;&#25454;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#28165;&#26224;&#24230;&#30340;&#21477;&#23376;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#20016;&#23500;&#30340;&#26102;&#20195;&#65292;&#25552;&#20379;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#19988;&#31616;&#27905;&#30340;&#20449;&#24687;&#23545;&#29992;&#25143;&#33267;&#20851;&#37325;&#35201;&#12290;&#20851;&#38190;&#35789;&#19978;&#19979;&#25991;(KIC)&#29983;&#25104;&#26159;&#22312;&#19968;&#20123;&#24212;&#29992;&#20013;&#25198;&#28436;&#33267;&#20851;&#37325;&#35201;&#35282;&#33394;&#30340;&#20219;&#21153;&#65292;&#27604;&#22914;&#25628;&#32034;&#24341;&#25806;&#12289;&#20010;&#20154;&#21161;&#25163;&#21644;&#20869;&#23481;&#25688;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;T5 transformer&#27169;&#22411;&#29983;&#25104;&#32473;&#23450;&#20851;&#38190;&#35789;&#30340;&#26126;&#30830;&#19988;&#31616;&#27905;&#21477;&#23376;&#19978;&#19979;&#25991;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#20174;Context-Reverso API&#33719;&#21462;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08103v1 Announce Type: cross  Abstract: In the age of information abundance, the ability to provide users with contextually relevant and concise information is crucial. Keyword in Context (KIC) generation is a task that plays a vital role in and generation applications, such as search engines, personal assistants, and content summarization. In this paper, we present a novel approach to generating unambiguous and brief sentence-contexts for given keywords using the T5 transformer model, leveraging data obtained from the Context-Reverso API. The code is available at https://github.com/Rusamus/word2context/tree/main .
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#33258;&#27880;&#24847;&#21147;&#23398;&#20064;&#21040;&#19968;&#20010;&#33258;&#21160;&#26426;&#65292;&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#29983;&#25104;&#26631;&#35760;&#30340;&#20004;&#20010;&#19981;&#21516;&#27493;&#39588;&#26159;&#65306;&#30828;&#26816;&#32034;&#21644;&#36719;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.08081</link><description>&lt;p&gt;
&#20855;&#26377;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Mechanics of Next Token Prediction with Self-Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08081
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#33258;&#27880;&#24847;&#21147;&#23398;&#20064;&#21040;&#19968;&#20010;&#33258;&#21160;&#26426;&#65292;&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#29983;&#25104;&#26631;&#35760;&#30340;&#20004;&#20010;&#19981;&#21516;&#27493;&#39588;&#26159;&#65306;&#30828;&#26816;&#32034;&#21644;&#36719;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#32473;&#23450;&#36755;&#20837;&#24207;&#21015;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#23613;&#31649;&#35757;&#32451;&#30446;&#26631;&#31616;&#21333;&#65292;&#20294;&#23427;&#20204;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#36827;&#23637;&#12290;&#36825;&#19968;&#25104;&#21151;&#30340;&#22522;&#30784;&#26159;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#19968;&#20010;&#21333;&#29420;&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#20174;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#23398;&#21040;&#20102;&#20160;&#20040;&#65311;&#25105;&#20204;&#23637;&#31034;&#65306;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#33258;&#27880;&#24847;&#21147;&#23398;&#20064;&#21040;&#19968;&#20010;&#33258;&#21160;&#26426;&#65292;&#35813;&#33258;&#21160;&#26426;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#27493;&#39588;&#29983;&#25104;&#19979;&#19968;&#20010;&#26631;&#35760;&#65306;(1) &#30828;&#26816;&#32034;&#65306;&#22312;&#32473;&#23450;&#36755;&#20837;&#24207;&#21015;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#27880;&#24847;&#21147;&#31934;&#30830;&#36873;&#25321;&#19982;&#19978;&#19968;&#20010;&#36755;&#20837;&#26631;&#35760;&#30456;&#20851;&#30340;&#39640;&#20248;&#20808;&#32423;&#36755;&#20837;&#26631;&#35760;&#12290;(2) &#36719;&#32452;&#21512;&#65306;&#28982;&#21518;&#65292;&#23427;&#21019;&#24314;&#39640;&#20248;&#20808;&#32423;&#26631;&#35760;&#30340;&#20984;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08081v1 Announce Type: cross  Abstract: Transformer-based language models are trained on large datasets to predict the next token given an input sequence. Despite this simple training objective, they have led to revolutionary advances in natural language processing. Underlying this success is the self-attention mechanism. In this work, we ask: $\textit{What}$ $\textit{does}$ $\textit{a}$ $\textit{single}$ $\textit{self-attention}$ $\textit{layer}$ $\textit{learn}$ $\textit{from}$ $\textit{next-token}$ $\textit{prediction?}$ We show that training self-attention with gradient descent learns an automaton which generates the next token in two distinct steps: $\textbf{(1)}$ $\textbf{Hard}$ $\textbf{retrieval:}$ Given input sequence, self-attention precisely selects the $\textit{high-priority}$ $\textit{input}$ $\textit{tokens}$ associated with the last input token. $\textbf{(2)}$ $\textbf{Soft}$ $\textbf{composition:}$ It then creates a convex combination of the high-priority tok
&lt;/p&gt;</description></item><item><title>FluoroSAM&#26159;&#29992;&#20110;X&#20809;&#22270;&#20687;&#30340;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;X&#20809;&#25104;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.08059</link><description>&lt;p&gt;
FluoroSAM: &#29992;&#20110;X&#20809;&#22270;&#20687;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08059
&lt;/p&gt;
&lt;p&gt;
FluoroSAM&#26159;&#29992;&#20110;X&#20809;&#22270;&#20687;&#30340;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;X&#20809;&#25104;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;X&#20809;&#22270;&#20687;&#20998;&#21106;&#23558;&#21152;&#36895;&#35786;&#26029;&#21644;&#20171;&#20837;&#31934;&#20934;&#21307;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#22270;&#20687;&#20998;&#26512;&#38382;&#39064;&#30340;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#25928;&#29992;&#21463;&#38480;&#20110;&#29305;&#23450;&#20219;&#21153;&#39046;&#22495;&#65292;&#35201;&#25299;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#21017;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#12289;&#26631;&#31614;&#21644;&#37325;&#26032;&#35757;&#32451;&#24037;&#20316;&#12290;&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289; - &#35757;&#32451;&#22312;&#22823;&#37327;&#39640;&#24230;&#21464;&#21270;&#25968;&#25454;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22240;&#27492;&#20351;&#24471;&#24191;&#27867;&#36866;&#29992;&#24615;&#25104;&#20026;&#21487;&#33021; - &#24050;&#32463;&#25104;&#20026;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#30340;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;FMs&#32858;&#28966;&#20110;&#23545;&#35937;&#34987;&#26126;&#26174;&#21487;&#35265;&#36793;&#30028;&#28165;&#26224;&#23450;&#20041;&#30340;&#22330;&#26223;&#21644;&#27169;&#24335;&#65292;&#22914;&#20869;&#31397;&#38236;&#25163;&#26415;&#24037;&#20855;&#20998;&#21106;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;X&#20809;&#25104;&#20687;&#36890;&#24120;&#27809;&#26377;&#25552;&#20379;&#36825;&#31181;&#28165;&#26224;&#30340;&#36793;&#30028;&#25110;&#32467;&#26500;&#20808;&#39564;&#12290;&#22312;X&#20809;&#22270;&#20687;&#24418;&#25104;&#26399;&#38388;&#65292;&#22797;&#26434;&#30340;&#19977;&#32500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08059v1 Announce Type: cross  Abstract: Automated X-ray image segmentation would accelerate research and development in diagnostic and interventional precision medicine. Prior efforts have contributed task-specific models capable of solving specific image analysis problems, but the utility of these models is restricted to their particular task domain, and expanding to broader use requires additional data, labels, and retraining efforts. Recently, foundation models (FMs) -- machine learning models trained on large amounts of highly variable data thus enabling broad applicability -- have emerged as promising tools for automated image analysis. Existing FMs for medical image analysis focus on scenarios and modalities where objects are clearly defined by visually apparent boundaries, such as surgical tool segmentation in endoscopy. X-ray imaging, by contrast, does not generally offer such clearly delineated boundaries or structure priors. During X-ray image formation, complex 3D
&lt;/p&gt;</description></item><item><title>CHAI&#25552;&#20986;&#20102;Clustered Head Attention&#65288;CHAI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36816;&#34892;&#26102;&#32467;&#21512;&#20855;&#26377;&#39640;&#30456;&#20851;&#24615;&#30340;&#27880;&#24847;&#21147;&#22836;&#37096;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#21644;&#35745;&#31639;&#37327;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;&#23384;&#20648;K,V&#32531;&#23384;&#30340;&#20869;&#23384;&#38656;&#27714;&#38477;&#20302;21.4&#65285;&#65292;&#25512;&#29702;&#26102;&#38388;&#24310;&#36831;&#38477;&#20302;1.73&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.08058</link><description>&lt;p&gt;
CHAI&#65306;&#39640;&#25928;LLM&#25512;&#29702;&#30340;&#32858;&#31867;&#22836;&#37096;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
CHAI: Clustered Head Attention for Efficient LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08058
&lt;/p&gt;
&lt;p&gt;
CHAI&#25552;&#20986;&#20102;Clustered Head Attention&#65288;CHAI&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#36816;&#34892;&#26102;&#32467;&#21512;&#20855;&#26377;&#39640;&#30456;&#20851;&#24615;&#30340;&#27880;&#24847;&#21147;&#22836;&#37096;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#21644;&#35745;&#31639;&#37327;&#65292;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;&#23384;&#20648;K,V&#32531;&#23384;&#30340;&#20869;&#23384;&#38656;&#27714;&#38477;&#20302;21.4&#65285;&#65292;&#25512;&#29702;&#26102;&#38388;&#24310;&#36831;&#38477;&#20302;1.73&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25317;&#26377;&#25968;&#30334;&#20159;&#21442;&#25968;&#25913;&#21464;&#20102;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#26102;&#20026;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#26082;&#38656;&#35201;&#35745;&#31639;&#21448;&#38656;&#35201;&#20869;&#23384;&#65292;&#19968;&#20010;&#35831;&#27714;&#21487;&#33021;&#38656;&#35201;&#22810;&#20010;GPU&#21644;&#25968;&#21313;GB&#30340;&#20869;&#23384;&#12290;&#22810;&#22836;&#27880;&#24847;&#21147;&#26159;LLMs&#30340;&#20851;&#38190;&#32452;&#20214;&#20043;&#19968;&#65292;&#21487;&#20197;&#21344;LLMs&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;50%&#20197;&#19978;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#21508;&#22836;&#20043;&#38388;&#23545;&#27880;&#24847;&#21147;&#30340;&#20851;&#27880;&#26377;&#24456;&#39640;&#30340;&#20887;&#20313;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Clustered Head Attention (CHAI)&#12290;CHAI&#22312;&#36816;&#34892;&#26102;&#23558;&#20855;&#26377;&#39640;&#30456;&#20851;&#24615;&#30340;&#22836;&#37096;&#32467;&#21512;&#36827;&#34892;&#33258;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CHAI&#33021;&#22815;&#23558;&#23384;&#20648;K,V&#32531;&#23384;&#30340;&#20869;&#23384;&#38656;&#27714;&#38477;&#20302;&#22810;&#36798;21.4%&#65292;&#25512;&#29702;&#26102;&#24310;&#36831;&#38477;&#20302;&#22810;&#36798;1.73&#20493;&#65292;&#32780;&#26080;&#38656;&#20219;&#20309;&#24494;&#35843;&#12290;CHAI&#23454;&#29616;&#20102;&#26368;&#22810;3.2%&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08058v1 Announce Type: cross  Abstract: Large Language Models (LLMs) with hundreds of billions of parameters have transformed the field of machine learning. However, serving these models at inference time is both compute and memory intensive, where a single request can require multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is one of the key components of LLMs, which can account for over 50% of LLMs memory and compute requirement. We observe that there is a high amount of redundancy across heads on which tokens they pay attention to. Based on this insight, we propose Clustered Head Attention (CHAI). CHAI combines heads with a high amount of correlation for self-attention at runtime, thus reducing both memory and compute. In our experiments, we show that CHAI is able to reduce the memory requirements for storing K,V cache by up to 21.4% and inference time latency by up to 1.73x without any fine-tuning required. CHAI achieves this with a maximum 3.2% deviat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#26088;&#22312;&#29983;&#25104;&#21512;&#21516;&#30340;&#28548;&#28165;&#38382;&#39064;&#65292;&#20197;&#24110;&#21161;&#35782;&#21035;&#21512;&#21516;&#20013;&#30340;&#27495;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.08053</link><description>&lt;p&gt;
&#29983;&#25104;&#29992;&#20110;&#28040;&#38500;&#21512;&#21516;&#27495;&#20041;&#30340;&#28548;&#28165;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Generating Clarification Questions for Disambiguating Contracts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#26088;&#22312;&#29983;&#25104;&#21512;&#21516;&#30340;&#28548;&#28165;&#38382;&#39064;&#65292;&#20197;&#24110;&#21161;&#35782;&#21035;&#21512;&#21516;&#20013;&#30340;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#32463;&#24120;&#31614;&#35746;&#21830;&#19994;&#21512;&#21516;&#65292;&#36825;&#20123;&#21512;&#21516;&#21487;&#20197;&#20316;&#20026;&#39033;&#30446;&#29305;&#23450;&#38656;&#27714;&#30340;&#37325;&#35201;&#26469;&#28304;&#12290;&#21512;&#21516;&#26465;&#27454;&#26159;&#24517;&#39035;&#36981;&#23432;&#30340;&#65292;&#24182;&#19988;&#20174;&#21512;&#21516;&#20013;&#24471;&#20986;&#30340;&#38656;&#27714;&#21487;&#20197;&#35814;&#32454;&#35828;&#26126;&#38750;&#27861;&#21153;&#21033;&#30410;&#30456;&#20851;&#32773;&#65288;&#21253;&#25324;&#38656;&#27714;&#20998;&#26512;&#24072;&#12289;&#24037;&#31243;&#24072;&#21644;&#20132;&#20184;&#20154;&#21592;&#65289;&#38656;&#35201;&#36827;&#34892;&#30340;&#19979;&#28216;&#23454;&#26045;&#27963;&#21160;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24191;&#27867;&#20351;&#29992;&#27861;&#24459;&#26415;&#35821;&#21644;&#21512;&#21516;&#35821;&#35328;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#29702;&#35299;&#21512;&#21516;&#23545;&#36825;&#20123;&#21033;&#30410;&#30456;&#20851;&#32773;&#26469;&#35828;&#26159;&#19968;&#39033;&#35748;&#30693;&#19978;&#30340;&#25361;&#25112;&#65292;&#20063;&#23481;&#26131;&#20986;&#38169;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#30830;&#20445;&#20840;&#38754;&#35206;&#30422;&#65292;&#21512;&#21516;&#36890;&#24120;&#21253;&#21547;&#25514;&#36766;&#21547;&#31946;&#30340;&#26465;&#27454;&#12290;&#30456;&#21453;&#65292;&#38750;&#27861;&#21153;&#21033;&#30410;&#30456;&#20851;&#32773;&#38656;&#35201;&#35814;&#32454;&#26126;&#20102;&#12289;&#26080;&#27495;&#20041;&#22320;&#29702;&#35299;&#21512;&#21516;&#26465;&#27454;&#65292;&#20197;&#21046;&#23450;&#21487;&#25191;&#34892;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#28041;&#21450;&#29983;&#25104;&#21512;&#21516;&#28548;&#28165;&#38382;&#39064;&#30340;&#26032;&#39062;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#36825;&#20123;&#38382;&#39064;&#26088;&#22312;&#35782;&#21035;&#21512;&#21516;&#30340;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08053v1 Announce Type: new  Abstract: Enterprises frequently enter into commercial contracts that can serve as vital sources of project-specific requirements. Contractual clauses are obligatory, and the requirements derived from contracts can detail the downstream implementation activities that non-legal stakeholders, including requirement analysts, engineers, and delivery personnel, need to conduct. However, comprehending contracts is cognitively demanding and error-prone for such stakeholders due to the extensive use of Legalese and the inherent complexity of contract language. Furthermore, contracts often contain ambiguously worded clauses to ensure comprehensive coverage. In contrast, non-legal stakeholders require a detailed and unambiguous comprehension of contractual clauses to craft actionable requirements. In this work, we introduce a novel legal NLP task that involves generating clarification questions for contracts. These questions aim to identify contract ambigui
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#32654;&#22269;384&#20010;&#37117;&#24066;&#22320;&#21306;&#30340;&#24037;&#20316;&#24066;&#22330;&#33021;&#21147;&#65292;&#21457;&#29616;&#37117;&#24066;&#35268;&#27169;&#36234;&#23567;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#36234;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.08046</link><description>&lt;p&gt;
&#22823;&#22478;&#24066;&#20559;&#35265;&#65306;&#35780;&#20272;&#37117;&#24066;&#35268;&#27169;&#23545;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#24037;&#20316;&#24066;&#22330;&#33021;&#21147;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Big City Bias: Evaluating the Impact of Metropolitan Size on Computational Job Market Abilities of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08046
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#32654;&#22269;384&#20010;&#37117;&#24066;&#22320;&#21306;&#30340;&#24037;&#20316;&#24066;&#22330;&#33021;&#21147;&#65292;&#21457;&#29616;&#37117;&#24066;&#35268;&#27169;&#36234;&#23567;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#36234;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#27714;&#32844;&#21305;&#37197;&#30340;&#19968;&#39033;&#26377;&#29992;&#25216;&#26415;&#65292;&#23545;&#27714;&#32844;&#32773;&#21644;&#38599;&#20027;&#37117;&#26377;&#24110;&#21161;&#12290;&#27714;&#32844;&#21305;&#37197;&#36890;&#24120;&#22522;&#20110;&#29305;&#23450;&#22320;&#29702;&#20301;&#32622;&#65292;&#27604;&#22914;&#22478;&#24066;&#25110;&#22320;&#21306;&#12290;&#28982;&#32780;&#65292;LLMs&#20855;&#26377;&#24050;&#30693;&#20559;&#35265;&#65292;&#36890;&#24120;&#28304;&#33258;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#37117;&#24066;&#35268;&#27169;&#20559;&#35265;&#65292;&#35780;&#20272;&#32654;&#22269;384&#20010;&#37117;&#24066;&#22320;&#21306;&#30340;&#38646;&#23556;&#20987;&#34218;&#27700;&#12289;&#38599;&#20027;&#23384;&#22312;&#21644;&#36890;&#21220;&#26102;&#38388;&#39044;&#27979;&#12290;&#22312;&#25152;&#26377;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#37117;&#24066;&#35268;&#27169;&#19982;LLMs&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#36127;&#30456;&#20851;&#65292;&#34920;&#26126;&#36739;&#23567;&#22320;&#21306;&#30340;&#30830;&#21463;&#21040;&#20302;&#20272;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#26368;&#23567;&#30340;10&#20010;&#37117;&#24066;&#22320;&#21306;&#30340;&#22522;&#20934;&#27979;&#35797;&#24615;&#33021;&#35201;&#27604;&#26368;&#22823;&#30340;10&#20010;&#37117;&#24066;&#22320;&#21306;&#20302;300%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08046v1 Announce Type: new  Abstract: Large language models (LLMs) have emerged as a useful technology for job matching, for both candidates and employers. Job matching is often based on a particular geographic location, such as a city or region. However, LLMs have known biases, commonly derived from their training data. In this work, we aim to quantify the metropolitan size bias encoded within large language models, evaluating zero-shot salary, employer presence, and commute duration predictions in 384 of the United States' metropolitan regions. Across all benchmarks, we observe negative correlations between the metropolitan size and the performance of the LLMS, indicating that smaller regions are indeed underrepresented. More concretely, the smallest 10 metropolitan regions show upwards of 300% worse benchmark performance than the largest 10.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20004;&#27493;&#35843;&#25972;&#21644;&#20248;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#65292;&#22312;&#20316;&#32773;&#36716;&#31227;&#21644;&#27597;&#35821;&#39118;&#26684;&#20219;&#21153;&#26041;&#38754;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.08043</link><description>&lt;p&gt;
&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#30340;&#20316;&#32773;&#39118;&#26684;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Authorship Style Transfer with Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08043
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20004;&#27493;&#35843;&#25972;&#21644;&#20248;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#65292;&#22312;&#20316;&#32773;&#36716;&#31227;&#21644;&#27597;&#35821;&#39118;&#26684;&#20219;&#21153;&#26041;&#38754;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#39118;&#26684;&#36716;&#31227;&#30340;&#30446;&#26631;&#26159;&#23558;&#32473;&#23450;&#30340;&#25991;&#26412;&#37325;&#20889;&#25104;&#25351;&#23450;&#30340;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#25991;&#26412;&#30340;&#21547;&#20041;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#37327;&#30446;&#26631;&#39118;&#26684;&#31034;&#20363;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#35270;&#20102;&#30446;&#26631;&#39118;&#26684;&#31034;&#20363;&#25968;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#12290;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#21644;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#30340;&#21457;&#23637;&#34920;&#26126;&#65292;&#36731;&#37327;&#32423;&#30340;&#31574;&#30053;&#20248;&#21270;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#20302;&#36164;&#28304;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#27493;&#35843;&#25972;&#21644;&#20248;&#21270;&#25216;&#26415;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#25991;&#26412;&#39118;&#26684;&#36716;&#31227;&#12290;&#25105;&#20204;&#23558;&#35813;&#25216;&#26415;&#24212;&#29992;&#20110;&#20316;&#32773;&#36716;&#31227;&#20197;&#21450;&#26356;&#22823;&#25968;&#25454;&#30340;&#27597;&#35821;&#39118;&#26684;&#20219;&#21153;&#65292;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#37117;&#21457;&#29616;&#23427;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08043v1 Announce Type: new  Abstract: Authorship style transfer aims to rewrite a given text into a specified target while preserving the original meaning in the source. Existing approaches rely on the availability of a large number of target style exemplars for model training. However, these overlook cases where a limited number of target style examples are available. The development of parameter-efficient transfer learning techniques and policy optimization (PO) approaches suggest lightweight PO is a feasible approach to low-resource style transfer. In this work, we propose a simple two step tune-and-optimize technique for low-resource textual style transfer. We apply our technique to authorship transfer as well as a larger-data native language style task and in both cases find it outperforms state-of-the-art baseline models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22260;&#32469;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26816;&#27979;&#21644;&#20998;&#31867;&#20196;&#20154;&#24974;&#24694;&#25110;&#26377;&#27602;&#20869;&#23481;&#26041;&#38754;&#30340;&#20316;&#29992;&#23637;&#24320;&#25991;&#29486;&#32508;&#36848;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#26088;&#22312;&#25581;&#31034;LLM&#22312;&#35782;&#21035;&#20196;&#20154;&#24974;&#24694;&#20869;&#23481;&#26041;&#38754;&#30340;&#33021;&#21147;&#21450;&#20854;&#24433;&#21709;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.08035</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25171;&#20987;&#32593;&#32476;&#20167;&#24680;&#65306;&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08035
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22260;&#32469;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26816;&#27979;&#21644;&#20998;&#31867;&#20196;&#20154;&#24974;&#24694;&#25110;&#26377;&#27602;&#20869;&#23481;&#26041;&#38754;&#30340;&#20316;&#29992;&#23637;&#24320;&#25991;&#29486;&#32508;&#36848;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#26088;&#22312;&#25581;&#31034;LLM&#22312;&#35782;&#21035;&#20196;&#20154;&#24974;&#24694;&#20869;&#23481;&#26041;&#38754;&#30340;&#33021;&#21147;&#21450;&#20854;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#38500;&#20102;&#35821;&#35328;&#29983;&#25104;&#65292;&#36824;&#21253;&#25324;&#32763;&#35793;&#12289;&#25688;&#35201;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#12290;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#24212;&#29992;&#26159;&#25991;&#26412;&#20998;&#31867;&#12290;&#22312;&#35782;&#21035;&#20196;&#20154;&#24974;&#24694;&#25110;&#26377;&#27602;&#35328;&#35770;&#30340;&#39046;&#22495;&#20013;&#65292;&#36825;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#26159;&#19968;&#20010;&#20805;&#28385;&#25361;&#25112;&#21644;&#20262;&#29702;&#22256;&#22659;&#30340;&#39046;&#22495;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26377;&#20004;&#20010;&#30446;&#26631;&#65306;&#39318;&#20808;&#65292;&#25552;&#20379;&#19968;&#20010;&#22260;&#32469;LLMs&#20316;&#20026;&#20998;&#31867;&#22120;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24378;&#35843;&#23427;&#20204;&#22312;&#26816;&#27979;&#21644;&#20998;&#31867;&#20196;&#20154;&#24974;&#24694;&#25110;&#26377;&#27602;&#20869;&#23481;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#20960;&#31181;LLMs&#22312;&#20998;&#31867;&#20167;&#24680;&#35328;&#35770;&#26041;&#38754;&#30340;&#25928;&#21147;&#65306;&#35782;&#21035;&#21738;&#20123;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#20197;&#21450;&#23427;&#20204;&#30340;&#22522;&#26412;&#23646;&#24615;&#21644;&#35757;&#32451;&#12290;&#20511;&#27492;&#27934;&#23519;&#20419;&#25104;LLM&#22312;&#35782;&#21035;&#20196;&#20154;&#24974;&#24694;&#20869;&#23481;&#26041;&#38754;&#30340;&#20248;&#21155;&#24615;&#25152;&#20381;&#36182;&#30340;&#22240;&#32032;&#12290;&#36890;&#36807;&#32467;&#21512;&#20840;&#38754;&#30340;&#25991;&#29486;&#22238;&#39038;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#35299;&#24320;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36776;&#21035;&#20196;&#20154;&#24974;&#24694;&#20869;&#23481;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08035v1 Announce Type: cross  Abstract: Large language models (LLMs) excel in many diverse applications beyond language generation, e.g., translation, summarization, and sentiment analysis. One intriguing application is in text classification. This becomes pertinent in the realm of identifying hateful or toxic speech -- a domain fraught with challenges and ethical dilemmas. In our study, we have two objectives: firstly, to offer a literature review revolving around LLMs as classifiers, emphasizing their role in detecting and classifying hateful or toxic content. Subsequently, we explore the efficacy of several LLMs in classifying hate speech: identifying which LLMs excel in this task as well as their underlying attributes and training. Providing insight into the factors that contribute to an LLM proficiency (or lack thereof) in discerning hateful content. By combining a comprehensive literature review with an empirical analysis, our paper strives to shed light on the capabil
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#36755;&#20986;&#20013;&#20197;&#27599;&#23618;&#26377;&#30417;&#30563;&#30340;&#26041;&#24335;&#23545;&#21333;&#35789;&#21644;&#23383;&#31526;&#30340;&#35821;&#35328;ID&#26465;&#20214;&#21270;&#21464;&#21387;&#22120;&#23618;&#65292;&#35813;&#26041;&#27861;&#34429;&#28982;&#26410;&#33021;&#26174;&#33879;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#65292;&#20294;&#23637;&#29616;&#20102;&#22312;&#20165;&#20165;&#36890;&#36807;&#21475;&#35821;&#25968;&#25454;&#39044;&#27979;&#27491;&#30830;&#35821;&#35328;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08011</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#25104;&#39044;&#27979;&#21475;&#35821;&#35328;&#35782;&#21035;&#30340;&#21476;&#21513;&#25289;&#29305;&#35821;-&#33521;&#35821;&#28151;&#21512;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Gujarati-English Code-Switching Speech Recognition using ensemble prediction of spoken language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08011
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#36755;&#20986;&#20013;&#20197;&#27599;&#23618;&#26377;&#30417;&#30563;&#30340;&#26041;&#24335;&#23545;&#21333;&#35789;&#21644;&#23383;&#31526;&#30340;&#35821;&#35328;ID&#26465;&#20214;&#21270;&#21464;&#21387;&#22120;&#23618;&#65292;&#35813;&#26041;&#27861;&#34429;&#28982;&#26410;&#33021;&#26174;&#33879;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#65292;&#20294;&#23637;&#29616;&#20102;&#22312;&#20165;&#20165;&#36890;&#36807;&#21475;&#35821;&#25968;&#25454;&#39044;&#27979;&#27491;&#30830;&#35821;&#35328;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#35821;&#38899;&#35782;&#21035;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#22256;&#38590;&#30340;&#20219;&#21153;&#26159;&#35782;&#21035;&#35821;&#35328;&#65292;&#22240;&#20026;&#20004;&#31181;&#35821;&#35328;&#20013;&#30340;&#35768;&#22810;&#35789;&#22312;&#26576;&#20123;&#21475;&#38899;&#19979;&#21548;&#36215;&#26469;&#30456;&#20284;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#36890;&#36807;&#22312;&#36755;&#20986;&#20013;&#20197;&#27599;&#23618;&#26377;&#30417;&#30563;&#30340;&#26041;&#24335;&#23545;&#21333;&#35789;&#21644;&#23383;&#31526;&#30340;&#35821;&#35328;ID&#26465;&#20214;&#21270;&#21464;&#21387;&#22120;&#23618;&#65292;&#20197;&#25913;&#21892;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24341;&#20837;&#35821;&#35328;&#29305;&#23450;&#21442;&#25968;&#21644;&#21487;&#35299;&#37322;&#24615;&#21040;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#26377;&#21161;&#20110;&#20445;&#25345;&#36755;&#20837;&#23545;&#40784;&#36830;&#32493;&#24615;&#30340;&#26102;&#38388;&#25439;&#22833;&#12290;&#23613;&#31649;&#26080;&#27861;&#26174;&#33879;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20102;&#22312;&#20165;&#20165;&#36890;&#36807;&#21475;&#35821;&#25968;&#25454;&#39044;&#27979;&#27491;&#30830;&#35821;&#35328;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#24207;&#21015;&#20013;&#21024;&#38500;LID&#24341;&#20837;&#20102;&#35821;&#35328;&#39044;&#27979;&#30340;&#27491;&#21017;&#21270;&#65292;&#26377;&#21161;&#20110;&#23545;&#40784;&#38271;&#37325;&#22797;&#30340;&#36755;&#20986;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08011v1 Announce Type: cross  Abstract: An important and difficult task in code-switched speech recognition is to recognize the language, as lots of words in two languages can sound similar, especially in some accents. We focus on improving performance of end-to-end Automatic Speech Recognition models by conditioning transformer layers on language ID of words and character in the output in an per layer supervised manner. To this end, we propose two methods of introducing language specific parameters and explainability in the multi-head attention mechanism, and implement a Temporal Loss that helps maintain continuity in input alignment. Despite being unable to reduce WER significantly, our method shows promise in predicting the correct language from just spoken data. We introduce regularization in the language prediction by dropping LID in the sequence, which helps align long repeated output sequences.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Debatrix&#31995;&#32479;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#22810;&#36718;&#36777;&#35770;&#30340;&#20998;&#26512;&#21644;&#35780;&#20272;&#65292;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#30452;&#25509;&#20351;&#29992;LLMs&#65292;&#23454;&#29616;&#20102;&#23545;&#25972;&#22330;&#36777;&#35770;&#30340;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.08010</link><description>&lt;p&gt;
Debatrix:&#22522;&#20110;LLM&#30340;&#22810;&#32500;&#36777;&#35770;&#35780;&#21028;&#31995;&#32479;&#19982;&#36845;&#20195;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08010
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Debatrix&#31995;&#32479;&#65292;&#21033;&#29992;LLMs&#36827;&#34892;&#22810;&#36718;&#36777;&#35770;&#30340;&#20998;&#26512;&#21644;&#35780;&#20272;&#65292;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#30452;&#25509;&#20351;&#29992;LLMs&#65292;&#23454;&#29616;&#20102;&#23545;&#25972;&#22330;&#36777;&#35770;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#26500;&#24314;&#19968;&#20010;&#33258;&#21160;&#21270;&#36777;&#35770;&#35780;&#21028;&#31995;&#32479;&#26469;&#35780;&#20272;&#19968;&#22330;&#24191;&#27867;&#12289;&#20805;&#28385;&#27963;&#21147;&#30340;&#22810;&#36718;&#36777;&#35770;&#65311;&#36825;&#19968;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#35780;&#21028;&#36777;&#35770;&#28041;&#21450;&#22788;&#29702;&#20887;&#38271;&#25991;&#26412;&#12289;&#22797;&#26434;&#30340;&#35770;&#28857;&#20851;&#31995;&#21644;&#22810;&#32500;&#24230;&#35780;&#20272;&#12290;&#24403;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#30701;&#23545;&#35805;&#65292;&#24456;&#23569;&#28041;&#21450;&#23545;&#25972;&#22330;&#36777;&#35770;&#30340;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Debatrix&#65292;&#20351;&#24471;&#22810;&#36718;&#36777;&#35770;&#30340;&#20998;&#26512;&#21644;&#35780;&#20272;&#26356;&#31526;&#21512;&#22823;&#22810;&#25968;&#20154;&#30340;&#20559;&#22909;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;Debatrix&#20855;&#26377;&#22402;&#30452;&#30340;&#12289;&#36845;&#20195;&#24335;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;&#27700;&#24179;&#30340;&#12289;&#22810;&#32500;&#24230;&#30340;&#35780;&#20272;&#21327;&#20316;&#12290;&#20026;&#20102;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#36777;&#35770;&#22330;&#26223;&#20445;&#25345;&#19968;&#33268;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PanelBench&#22522;&#20934;&#65292;&#23558;&#25105;&#20204;&#31995;&#32479;&#30340;&#24615;&#33021;&#19982;&#23454;&#38469;&#36777;&#35770;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#36739;&#20110;&#30452;&#25509;&#20351;&#29992;LLMs&#36827;&#34892;&#36777;&#35770;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#22686;&#24378;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08010v1 Announce Type: new  Abstract: How can we construct an automated debate judge to evaluate an extensive, vibrant, multi-turn debate? This task is challenging, as judging a debate involves grappling with lengthy texts, intricate argument relationships, and multi-dimensional assessments. At the same time, current research mainly focuses on short dialogues, rarely touching upon the evaluation of an entire debate. In this paper, by leveraging Large Language Models (LLMs), we propose Debatrix, which makes the analysis and assessment of multi-turn debates more aligned with majority preferences. Specifically, Debatrix features a vertical, iterative chronological analysis and a horizontal, multi-dimensional evaluation collaboration. To align with real-world debate scenarios, we introduced the PanelBench benchmark, comparing our system's performance to actual debate outcomes. The findings indicate a notable enhancement over directly using LLMs for debate evaluation. Source code
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#22270;&#20687;&#32534;&#36753;&#65292;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#39044;&#22791;&#24037;&#20316;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#21644;DDIM&#21453;&#28436;&#65292;&#33719;&#21462;&#32534;&#36753;&#26041;&#21521;&#23884;&#20837;&#65292;&#36827;&#34892;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#65292;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08004</link><description>&lt;p&gt;
Pix2Pix-OnTheFly: &#21033;&#29992;LLMs&#36827;&#34892;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Pix2Pix-OnTheFly: Leveraging LLMs for Instruction-Guided Image Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#22270;&#20687;&#32534;&#36753;&#65292;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#39044;&#22791;&#24037;&#20316;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#21644;DDIM&#21453;&#28436;&#65292;&#33719;&#21462;&#32534;&#36753;&#26041;&#21521;&#23884;&#20837;&#65292;&#36827;&#34892;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#65292;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#26368;&#36817;&#32467;&#21512;&#35821;&#35328;&#22788;&#29702;&#21644;&#22270;&#20687;&#22788;&#29702;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#21644;DDIM&#21453;&#28436;&#65292;&#33719;&#21462;&#32534;&#36753;&#26041;&#21521;&#23884;&#20837;&#65292;&#36827;&#34892;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#65292;&#32780;&#26080;&#38656;&#39044;&#22791;&#24037;&#20316;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08004v1 Announce Type: cross  Abstract: The combination of language processing and image processing keeps attracting increased interest given recent impressive advances that leverage the combined strengths of both domains of research. Among these advances, the task of editing an image on the basis solely of a natural language instruction stands out as a most challenging endeavour. While recent approaches for this task resort, in one way or other, to some form of preliminary preparation, training or fine-tuning, this paper explores a novel approach: We propose a preparation-free method that permits instruction-guided image editing on the fly. This approach is organized along three steps properly orchestrated that resort to image captioning and DDIM inversion, followed by obtaining the edit direction embedding, followed by image editing proper. While dispensing with preliminary preparation, our approach demonstrates to be effective and competitive, outperforming recent, state 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#21069;&#27839;&#27169;&#22411;&#23578;&#23384;&#22312;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#24046;&#36317;&#65292;&#25506;&#35752;&#20102;&#35757;&#32451;&#24320;&#28304;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20197;&#24357;&#34917;&#20020;&#24202;&#38656;&#27714;&#30340;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.08002</link><description>&lt;p&gt;
&#35757;&#32451;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20197;&#22635;&#34917;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#65306;&#20197;&#25918;&#23556;&#23398;&#25104;&#20687;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Training Small Multimodal Models to Bridge Biomedical Competency Gap: A Case Study in Radiology Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#21069;&#27839;&#27169;&#22411;&#23578;&#23384;&#22312;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#24046;&#36317;&#65292;&#25506;&#35752;&#20102;&#35757;&#32451;&#24320;&#28304;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20197;&#24357;&#34917;&#20020;&#24202;&#38656;&#27714;&#30340;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#22823;&#22522;&#30784;&#27169;&#22411;&#30340;&#23610;&#24230;&#35268;&#24459;&#21644;&#38750;&#20961;&#34920;&#29616;&#28608;&#21169;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#24320;&#21457;&#21644;&#21033;&#29992;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#19968;&#20123;&#29983;&#29289;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26089;&#26399;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20043;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#37325;&#22823;&#25361;&#25112;&#12290;&#20687;GPT-4V&#36825;&#26679;&#30340;&#21069;&#27839;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#20173;&#23384;&#22312;&#37325;&#22823;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#35775;&#38382;&#12289;&#25104;&#26412;&#12289;&#24310;&#36831;&#21644;&#21512;&#35268;&#31561;&#23454;&#38469;&#38382;&#39064;&#20351;&#20020;&#24202;&#21307;&#29983;&#38590;&#20197;&#30452;&#25509;&#22312;&#31169;&#20154;&#24739;&#32773;&#25968;&#25454;&#19978;&#20351;&#29992;&#31169;&#20154;&#25176;&#31649;&#30340;&#26368;&#20808;&#36827;&#22823;&#22411;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#35757;&#32451;&#24320;&#28304;&#23567;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;SMMs&#65289;&#26469;&#22635;&#34917;&#26410;&#28385;&#36275;&#30340;&#20020;&#24202;&#38656;&#27714;&#30340;&#29983;&#29289;&#21307;&#23398;&#33021;&#21147;&#24046;&#36317;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#37319;&#29992;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#23558;&#29992;&#20110;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#26368;&#20808;&#36827;&#39044;&#35757;&#32451;&#27169;&#22411;&#32435;&#20837;&#65292;&#24182;&#20391;&#37325;&#20110;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08002v1 Announce Type: new  Abstract: The scaling laws and extraordinary performance of large foundation models motivate the development and utilization of such large models in biomedicine. However, despite early promising results on some biomedical benchmarks, there are still major challenges that need to be addressed before these models can be used in real-world applications. Frontier models such as GPT-4V still have major competency gaps in multimodal capabilities for biomedical applications. Moreover, pragmatic issues such as access, cost, latency, and compliance make it hard for clinicians to use privately-hosted state-of-the-art large models directly on private patient data. In this paper, we explore training open-source small multimodal models (SMMs) to bridge biomedical competency gaps for unmet clinical needs. To maximize data efficiency, we adopt a modular approach by incorporating state-of-the-art pre-trained models for image and text modalities, and focusing on t
&lt;/p&gt;</description></item><item><title>LiveCodeBench&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#26080;&#27745;&#26579;&#30340;LLMs&#35780;&#20272;&#24037;&#20855;&#65292;&#32858;&#28966;&#20110;&#20174;LeetCode&#12289;AtCoder&#21644;CodeForces&#31561;&#24179;&#21488;&#36830;&#32493;&#25910;&#38598;&#30340;&#26032;&#38382;&#39064;&#65292;&#35206;&#30422;&#33258;&#20462;&#22797;&#12289;&#20195;&#30721;&#25191;&#34892;&#12289;&#27979;&#35797;&#36755;&#20986;&#39044;&#27979;&#31561;&#26356;&#24191;&#27867;&#30340;&#20195;&#30721;&#30456;&#20851;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.07974</link><description>&lt;p&gt;
LiveCodeBench&#65306;&#29992;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#38754;&#21644;&#26080;&#27745;&#26579;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07974
&lt;/p&gt;
&lt;p&gt;
LiveCodeBench&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#26080;&#27745;&#26579;&#30340;LLMs&#35780;&#20272;&#24037;&#20855;&#65292;&#32858;&#28966;&#20110;&#20174;LeetCode&#12289;AtCoder&#21644;CodeForces&#31561;&#24179;&#21488;&#36830;&#32493;&#25910;&#38598;&#30340;&#26032;&#38382;&#39064;&#65292;&#35206;&#30422;&#33258;&#20462;&#22797;&#12289;&#20195;&#30721;&#25191;&#34892;&#12289;&#27979;&#35797;&#36755;&#20986;&#39044;&#27979;&#31561;&#26356;&#24191;&#27867;&#30340;&#20195;&#30721;&#30456;&#20851;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#24212;&#29992;&#31243;&#24207;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#31361;&#20986;&#30340;&#39046;&#22495;&#65292;&#21560;&#24341;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26032;&#30340;&#21644;&#25913;&#36827;&#30340;LLMs&#30340;&#24320;&#21457;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#22522;&#20934;&#65288;&#20363;&#22914;HumanEval&#65292;MBPP&#65289;&#19981;&#20877;&#36275;&#20197;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;LiveCodeBench&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#26080;&#27745;&#26579;&#30340;LLMs&#35780;&#20272;&#24037;&#20855;&#65292;&#29992;&#20110;&#20195;&#30721;&#65292;&#23427;&#20250;&#20174;&#19977;&#20010;&#31454;&#36187;&#24179;&#21488;&#65288;LeetCode&#12289;AtCoder&#21644;CodeForces&#65289;&#19978;&#36830;&#32493;&#22320;&#25910;&#38598;&#26032;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#36824;&#30528;&#37325;&#20851;&#27880;&#26356;&#24191;&#27867;&#30340;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#33021;&#21147;&#65292;&#22914;&#33258;&#20462;&#22797;&#12289;&#20195;&#30721;&#25191;&#34892;&#21644;&#27979;&#35797;&#36755;&#20986;&#39044;&#27979;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#20195;&#30721;&#29983;&#25104;&#12290;&#30446;&#21069;&#65292;LiveCodeBench&#25176;&#31649;&#20102;&#22312;2023&#24180;5&#26376;&#33267;2024&#24180;2&#26376;&#20043;&#38388;&#21457;&#24067;&#30340;400&#20010;&#39640;&#36136;&#37327;&#32534;&#30721;&#38382;&#39064;&#12290;&#25105;&#20204;&#24050;&#32463;&#35780;&#20272;&#20102;9&#20010;&#22522;&#26412;LLMs&#21644;20&#20010;&#25351;&#20196;&#35843;&#25972;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07974v1 Announce Type: cross  Abstract: Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and February 2024. We have evaluated 9 base LLMs and 20 instruction-tuned LLMs o
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#19968;&#31181;&#28151;&#21512;&#20351;&#29992;Sepedi-&#33521;&#35821;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#25506;&#35752;&#20102;&#31471;&#21040;&#31471;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07947</link><description>&lt;p&gt;
&#35780;&#20272;&#19968;&#31181;&#28151;&#21512;&#20351;&#29992;Sepedi-&#33521;&#35821;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The evaluation of a code-switched Sepedi-English automatic speech recognition system
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07947
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#19968;&#31181;&#28151;&#21512;&#20351;&#29992;Sepedi-&#33521;&#35821;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#25506;&#35752;&#20102;&#31471;&#21040;&#31471;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07947v1 &#21457;&#34920;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: &#35821;&#38899;&#25216;&#26415;&#26159;&#19968;&#20010;&#28085;&#30422;&#21508;&#31181;&#25216;&#26415;&#21644;&#24037;&#20855;&#30340;&#39046;&#22495;&#65292;&#29992;&#20110;&#20351;&#35774;&#22791;&#33021;&#22815;&#19982;&#35821;&#38899;&#36827;&#34892;&#20132;&#20114;&#65292;&#20363;&#22914;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#21475;&#35821;&#23545;&#35805;&#31995;&#32479;&#31561;&#65292;&#20801;&#35768;&#35774;&#22791;&#36890;&#36807;&#40614;&#20811;&#39118;&#20174;&#35828;&#35805;&#32773;&#37027;&#37324;&#25429;&#33719;&#35828;&#35805;&#30340;&#35805;&#35821;&#12290;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#22914;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#26159;&#24320;&#21457;ASR&#31995;&#32479;&#20013;&#20351;&#29992;&#26368;&#24191;&#27867;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#29992;&#20110;&#23545;&#20855;&#26377;&#22823;&#37327;&#35821;&#38899;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#36827;&#34892;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#23548;&#33268;&#20302;&#36164;&#28304;&#35821;&#35328;&#30456;&#23545;&#36739;&#23569;&#21457;&#23637;&#12290;&#23613;&#31649;CTC&#26041;&#27861;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#24050;&#25104;&#21151;&#20351;&#29992;&#65292;&#20294;&#23427;&#22312;Sepedi&#35821;&#35328;&#20013;&#30340;&#26377;&#25928;&#24615;&#20173;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28151;&#21512;&#20351;&#29992;Sepedi-&#33521;&#35821;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07947v1 Announce Type: cross  Abstract: Speech technology is a field that encompasses various techniques and tools used to enable machines to interact with speech, such as automatic speech recognition (ASR), spoken dialog systems, and others, allowing a device to capture spoken words through a microphone from a human speaker. End-to-end approaches such as Connectionist Temporal Classification (CTC) and attention-based methods are the most used for the development of ASR systems. However, these techniques were commonly used for research and development for many high-resourced languages with large amounts of speech data for training and evaluation, leaving low-resource languages relatively underdeveloped. While the CTC method has been successfully used for other languages, its effectiveness for the Sepedi language remains uncertain. In this study, we present the evaluation of the Sepedi-English code-switched automatic speech recognition system. This end-to-end system was devel
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#22522;&#20934;&#65288;SRB&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#23545;&#21508;&#31181;&#30772;&#22351;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;&#26576;&#20123;&#24314;&#27169;&#36873;&#25321;&#26377;&#21161;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#19981;&#21516;&#20154;&#21475;&#20122;&#32452;&#19978;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.07937</link><description>&lt;p&gt;
&#35821;&#38899;&#40065;&#26834;&#22522;&#20934;&#65306;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#40065;&#26834;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Speech Robust Bench: A Robustness Benchmark For Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07937
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#22522;&#20934;&#65288;SRB&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#23545;&#21508;&#31181;&#30772;&#22351;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;&#26576;&#20123;&#24314;&#27169;&#36873;&#25321;&#26377;&#21161;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#19981;&#21516;&#20154;&#21475;&#20122;&#32452;&#19978;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#30830;&#20445;&#23427;&#20204;&#22312;&#29289;&#29702;&#19990;&#30028;&#21644;&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#21508;&#31181;&#30772;&#22351;&#19979;&#36827;&#34892;&#21487;&#38752;&#39044;&#27979;&#21464;&#24471;&#24840;&#21457;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#38899;&#40065;&#26834;&#22522;&#20934;&#65288;SRB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;ASR&#27169;&#22411;&#23545;&#21508;&#31181;&#30772;&#22351;&#30340;&#40065;&#26834;&#24615;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;SRB&#30001;69&#20010;&#36755;&#20837;&#25200;&#21160;&#32452;&#25104;&#65292;&#26088;&#22312;&#27169;&#25311;ASR&#27169;&#22411;&#21487;&#33021;&#22312;&#29289;&#29702;&#19990;&#30028;&#21644;&#25968;&#23383;&#19990;&#30028;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#30772;&#22351;&#12290;&#25105;&#20204;&#20351;&#29992;SRB&#26469;&#35780;&#20272;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;ASR&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#27169;&#22411;&#22823;&#23567;&#21644;&#26576;&#20123;&#24314;&#27169;&#36873;&#25321;&#65288;&#22914;&#31163;&#25955;&#34920;&#31034;&#21644;&#33258;&#25105;&#35757;&#32451;&#65289;&#20284;&#20046;&#26377;&#21161;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#27492;&#20998;&#26512;&#25193;&#23637;&#21040;&#34913;&#37327;ASR&#27169;&#22411;&#22312;&#26469;&#33258;&#21508;&#31181;&#20154;&#21475;&#20122;&#32452;&#30340;&#25968;&#25454;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#21363;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20351;&#29992;&#32773;&#20197;&#21450;&#30007;&#24615;&#21644;&#22899;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#22312;&#19981;&#21516;&#20122;&#32452;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07937v1 Announce Type: cross  Abstract: As Automatic Speech Recognition (ASR) models become ever more pervasive, it is important to ensure that they make reliable predictions under corruptions present in the physical and digital world. We propose Speech Robust Bench (SRB), a comprehensive benchmark for evaluating the robustness of ASR models to diverse corruptions. SRB is composed of 69 input perturbations which are intended to simulate various corruptions that ASR models may encounter in the physical and digital world. We use SRB to evaluate the robustness of several state-of-the-art ASR models and observe that model size and certain modeling choices such as discrete representations, and self-training appear to be conducive to robustness. We extend this analysis to measure the robustness of ASR models on data from various demographic subgroups, namely English and Spanish speakers, and males and females, and observed noticeable disparities in the model's robustness across su
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20449;&#24687;&#29109;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#25163;&#26426;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;transformer&#35299;&#30721;&#22120;&#30340;&#29109;&#26469;&#22312;&#35745;&#31639;&#39044;&#31639;&#20869;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;MeRino&#27169;&#22411;&#65292;&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#23637;&#29616;&#20986;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#31454;&#20105;&#24615;&#33021;&#30340;&#29305;&#28857;</title><link>https://arxiv.org/abs/2403.07921</link><description>&lt;p&gt;
Merino&#65306;&#22522;&#20110;&#29109;&#39537;&#21160;&#30340;IoT&#35774;&#22791;&#19978;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Merino: Entropy-driven Design for Generative Language Models on IoT Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07921
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20449;&#24687;&#29109;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#25163;&#26426;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;transformer&#35299;&#30721;&#22120;&#30340;&#29109;&#26469;&#22312;&#35745;&#31639;&#39044;&#31639;&#20869;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;MeRino&#27169;&#22411;&#65292;&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#23637;&#29616;&#20986;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#31454;&#20105;&#24615;&#33021;&#30340;&#29305;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#29616;&#20195;&#26102;&#20195;&#30340;&#38761;&#21629;&#24615;&#36827;&#27493;&#65292;&#28982;&#32780;&#65292;&#30452;&#25509;&#37096;&#32626;LLMs&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#65292;&#27604;&#22914;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#35774;&#22791;&#65292;&#30001;&#20110;&#20854;&#39640;&#35745;&#31639;&#25104;&#26412;&#32780;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20449;&#24687;&#29109;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#25163;&#26426;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35774;&#35745;&#33539;&#24335;&#26159;&#22312;&#32473;&#23450;&#30340;&#35745;&#31639;&#39044;&#31639;&#20869;&#26368;&#22823;&#21270;transformer&#35299;&#30721;&#22120;&#30340;&#29109;&#12290;&#25972;&#20010;&#35774;&#35745;&#36807;&#31243;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#25968;&#23398;&#35268;&#21010;&#65288;MP&#65289;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#20960;&#20998;&#38047;&#20869;&#22312;CPU&#19978;&#23436;&#25104;&#65292;&#20351;&#20854;&#20960;&#20046;&#26159;&#38646;&#25104;&#26412;&#30340;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#35774;&#35745;&#30340;&#27169;&#22411;MeRino&#65292;&#22312;&#20061;&#20010;NLP&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#23545;&#25239;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MeRino&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#33719;&#24471;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#38646;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07921v1 Announce Type: cross  Abstract: Generative Large Language Models (LLMs) stand as a revolutionary advancement in the modern era of artificial intelligence (AI). However, directly deploying LLMs in resource-constrained hardware, such as Internet-of-Things (IoT) devices, is difficult due to their high computational cost. In this paper, we propose a novel information-entropy framework for designing mobile-friendly generative language models. Our key design paradigm is to maximize the entropy of transformer decoders within the given computational budgets. The whole design procedure involves solving a mathematical programming (MP) problem, which can be done on the CPU within minutes, making it nearly zero-cost. We evaluate our designed models, termed MeRino, across nine NLP downstream tasks, showing their competitive performance against the state-of-the-art autoregressive transformer models under the mobile setting. Notably, MeRino achieves similar or better zero performan
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ProtLLM&#65292;&#19968;&#31181;&#20855;&#26377;&#29420;&#29305;&#21160;&#24577;&#34507;&#30333;&#36136;&#35013;&#37197;&#26426;&#21046;&#21450;&#34507;&#30333;&#36136;&#20316;&#20026;&#21333;&#35789;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#30340;&#20132;&#32455;&#24335;&#34507;&#30333;&#36136;-&#35821;&#35328;LLM&#65292;&#24182;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#20132;&#32455;&#24335;&#34507;&#30333;&#36136;-&#25991;&#26412;&#25968;&#25454;&#38598;&#29992;&#20110;&#39044;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.07920</link><description>&lt;p&gt;
ProtLLM&#65306;&#19968;&#31181;&#20855;&#26377;&#34507;&#30333;&#36136;&#20316;&#20026;&#21333;&#35789;&#39044;&#35757;&#32451;&#30340;&#20132;&#32455;&#24335;&#34507;&#30333;&#36136;-&#35821;&#35328;LLM
&lt;/p&gt;
&lt;p&gt;
ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07920
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ProtLLM&#65292;&#19968;&#31181;&#20855;&#26377;&#29420;&#29305;&#21160;&#24577;&#34507;&#30333;&#36136;&#35013;&#37197;&#26426;&#21046;&#21450;&#34507;&#30333;&#36136;&#20316;&#20026;&#21333;&#35789;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#30340;&#20132;&#32455;&#24335;&#34507;&#30333;&#36136;-&#35821;&#35328;LLM&#65292;&#24182;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#20132;&#32455;&#24335;&#34507;&#30333;&#36136;-&#25991;&#26412;&#25968;&#25454;&#38598;&#29992;&#20110;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ProtLLM&#65292;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#36328;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#26082;&#26377;&#34507;&#30333;&#36136;&#20026;&#20013;&#24515;&#21448;&#26377;&#34507;&#30333;&#36136;-&#35821;&#35328;&#20219;&#21153;&#12290;ProtLLM&#20855;&#26377;&#29420;&#29305;&#30340;&#21160;&#24577;&#34507;&#30333;&#36136;&#35013;&#37197;&#26426;&#21046;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#19982;&#20219;&#24847;&#25968;&#37327;&#30340;&#34507;&#30333;&#36136;&#20132;&#32455;&#22312;&#19968;&#36215;&#30340;&#22797;&#26434;&#36755;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34507;&#30333;&#36136;&#20316;&#20026;&#21333;&#35789;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#26469;&#35757;&#32451;ProtLLM&#12290;&#36890;&#36807;&#24320;&#21457;&#19987;&#38376;&#30340;&#34507;&#30333;&#36136;&#35789;&#27719;&#34920;&#65292;&#25105;&#20204;&#36171;&#20104;&#35813;&#27169;&#22411;&#19981;&#20165;&#39044;&#27979;&#33258;&#28982;&#35821;&#35328;&#32780;&#19988;&#39044;&#27979;&#26469;&#33258;&#22823;&#37327;&#20505;&#36873;&#34507;&#30333;&#36136;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20132;&#32455;&#24335;&#34507;&#30333;&#36136;-&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;InterPT&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#12290;&#35813;&#25968;&#25454;&#38598;&#20840;&#38754;&#28085;&#30422;&#20102;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#65288;&#22914;&#34507;&#30333;&#36136;&#27880;&#37322;&#65289;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#65288;&#22914;&#29983;&#29289;&#30740;&#31350;&#35770;&#25991;&#65289;&#65292;&#20174;&#32780;&#36171;&#20104;ProtLLM&#29702;&#35299;&#34507;&#30333;&#36136;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#32463;&#20856;&#25968;&#25454;&#38598;&#19978;&#23545;ProtLLM&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07920v1 Announce Type: cross  Abstract: We propose ProtLLM, a versatile cross-modal large language model (LLM) for both protein-centric and protein-language tasks. ProtLLM features a unique dynamic protein mounting mechanism, enabling it to handle complex inputs where the natural language text is interspersed with an arbitrary number of proteins. Besides, we propose the protein-as-word language modeling approach to train ProtLLM. By developing a specialized protein vocabulary, we equip the model with the capability to predict not just natural language but also proteins from a vast pool of candidates. Additionally, we construct a large-scale interleaved protein-text dataset, named InterPT, for pre-training. This dataset comprehensively encompasses both (1) structured data sources like protein annotations and (2) unstructured data sources like biological research papers, thereby endowing ProtLLM with crucial knowledge for understanding proteins. We evaluate ProtLLM on classic 
&lt;/p&gt;</description></item><item><title>MAGPIE&#26159;&#31532;&#19968;&#20010;&#20026;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#23450;&#21046;&#30340;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#21333;&#19968;&#20219;&#21153;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#24494;&#35843;&#27493;&#39588;&#12290;</title><link>https://arxiv.org/abs/2403.07910</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23186;&#20307;&#20559;&#35265;&#20998;&#26512;&#36890;&#29992;&#21270;&#30340;&#39044;&#35757;&#32451;&#34920;&#36798;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Media-Bias Analysis Generalization for Pre-Trained Identification of Expressions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07910
&lt;/p&gt;
&lt;p&gt;
MAGPIE&#26159;&#31532;&#19968;&#20010;&#20026;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#23450;&#21046;&#30340;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#21333;&#19968;&#20219;&#21153;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#24494;&#35843;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#12289;&#22810;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#20256;&#32479;&#19978;&#36890;&#36807;&#20351;&#29992;&#21333;&#19968;&#20219;&#21153;&#27169;&#22411;&#21644;&#23567;&#22411;&#39046;&#22495;&#20869;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#65292;&#22240;&#27492;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MAGPIE&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#20026;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#23450;&#21046;&#30340;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#35268;&#27169;&#21270;&#30340;&#39044;&#35757;&#32451;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22823;&#20559;&#35265;&#28151;&#21512;&#65288;LBM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;59&#20010;&#19982;&#20559;&#35265;&#30456;&#20851;&#30340;&#20219;&#21153;&#30340;&#32534;&#35793;&#12290;MAGPIE&#22312;Bias Annotation By Experts (BABE)&#25968;&#25454;&#38598;&#19978;&#30340;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;F1&#20998;&#25968;&#30456;&#23545;&#25552;&#39640;&#20102;3.3%&#12290;MAGPIE&#22312;Media Bias Identification Benchmark (MBIB)&#20013;&#30340;8&#20010;&#20219;&#21153;&#20013;&#26377;5&#20010;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20808;&#21069;&#30340;&#27169;&#22411;&#12290;&#20351;&#29992;RoBERTa&#32534;&#30721;&#22120;&#65292;MAGPIE&#20165;&#38656;&#35201;&#30456;&#23545;&#20110;&#21333;&#19968;&#20219;&#21153;&#26041;&#27861;&#30340;15%&#30340;&#24494;&#35843;&#27493;&#39588;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#27604;&#22914;&#20219;&#21153;&#22914;&#24773;&#24863;&#21644;&#24773;&#32490;&#20250;&#22686;&#24378;&#25152;&#26377;&#23398;&#20064;&#65292;&#25152;&#26377;&#20219;&#21153;&#20250;&#22686;&#24378;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07910v1 Announce Type: cross  Abstract: Media bias detection poses a complex, multifaceted problem traditionally tackled using single-task models and small in-domain datasets, consequently lacking generalizability. To address this, we introduce MAGPIE, the first large-scale multi-task pre-training approach explicitly tailored for media bias detection. To enable pre-training at scale, we present Large Bias Mixture (LBM), a compilation of 59 bias-related tasks. MAGPIE outperforms previous approaches in media bias detection on the Bias Annotation By Experts (BABE) dataset, with a relative improvement of 3.3% F1-score. MAGPIE also performs better than previous models on 5 out of 8 tasks in the Media Bias Identification Benchmark (MBIB). Using a RoBERTa encoder, MAGPIE needs only 15% of finetuning steps compared to single-task approaches. Our evaluation shows, for instance, that tasks like sentiment and emotionality boost all learning, all tasks enhance fake news detection, and s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35775;&#38382;&#20869;&#23384;&#26102;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#36890;&#36807;&#32972;&#35829;&#21644;&#32622;&#25442;&#31561;&#25216;&#26415;&#21487;&#20197;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#24320;&#25918;&#22495;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.07805</link><description>&lt;p&gt;
&#36229;&#36234;&#27515;&#35760;&#30828;&#32972;&#65306;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Beyond Memorization: The Challenge of Random Memory Access in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35775;&#38382;&#20869;&#23384;&#26102;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#36890;&#36807;&#32972;&#35829;&#21644;&#32622;&#25442;&#31561;&#25216;&#26415;&#21487;&#20197;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#24320;&#25918;&#22495;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;NLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#20854;&#21442;&#25968;&#20869;&#37096;&#30340;&#30693;&#35782;&#23384;&#20648;&#21644;&#20869;&#23384;&#35775;&#38382;&#26426;&#21046;&#20173;&#28982;&#20196;&#20154;&#36153;&#35299;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-2&#65289;&#26159;&#21542;&#33021;&#22815;&#39034;&#24207;&#25110;&#38543;&#26426;&#22320;&#35775;&#38382;&#20854;&#20869;&#23384;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#21512;&#25104;&#20219;&#21153;&#65292;&#28085;&#30422;&#20840;&#38754;&#32972;&#35829;&#12289;&#36873;&#25321;&#24615;&#32972;&#35829;&#21644;&#22522;&#20110;&#38382;&#39064;&#22238;&#31572;&#30340;&#24773;&#26223;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LMs&#33021;&#22815;&#39034;&#24207;&#35775;&#38382;&#20854;&#20869;&#23384;&#65292;&#21516;&#26102;&#22312;&#38543;&#26426;&#35775;&#38382;&#24050;&#35760;&#24518;&#20869;&#23481;&#26102;&#36935;&#21040;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#32972;&#35829;&#21644;&#32622;&#25442;&#31561;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;LMs&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;&#36825;&#31181;&#24178;&#39044;&#24212;&#29992;&#20110;&#24320;&#25918;&#22495;&#38382;&#39064;&#22238;&#31572;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#36890;&#36807;&#32972;&#35829;&#26469;&#22686;&#24378;&#38543;&#26426;&#35775;&#38382;&#25216;&#26415;&#23545;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07805v1 Announce Type: cross  Abstract: Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks. However, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive. In this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content. We find that techniques including recitation and permutation improve the random memory access capability of LMs. Furthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in questi
&lt;/p&gt;</description></item><item><title>StableToolBench&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#31995;&#32479;&#65292;&#36890;&#36807;&#32531;&#23384;&#31995;&#32479;&#12289;API&#27169;&#25311;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#31283;&#23450;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07714</link><description>&lt;p&gt;
StableToolBench&#65306;&#38754;&#21521;&#22823;&#35268;&#27169;&#31283;&#23450;&#22522;&#20934;&#27979;&#35797;&#30340;&#24037;&#20855;&#23398;&#20064;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
StableToolBench: Towards Stable Large-Scale Benchmarking on Tool Learning of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07714
&lt;/p&gt;
&lt;p&gt;
StableToolBench&#25552;&#20986;&#20102;&#19968;&#31181;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#31995;&#32479;&#65292;&#36890;&#36807;&#32531;&#23384;&#31995;&#32479;&#12289;API&#27169;&#25311;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#31283;&#23450;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20419;&#20351;&#20154;&#20204;&#25506;&#32034;&#24037;&#20855;&#23398;&#20064;&#65292;&#23558;LLMs&#19982;&#22806;&#37096;&#24037;&#20855;&#25972;&#21512;&#20197;&#35299;&#20915;&#21508;&#31181;&#29616;&#23454;&#25361;&#25112;&#12290;&#35780;&#20272;LLMs&#21033;&#29992;&#24037;&#20855;&#30340;&#33021;&#21147;&#38656;&#35201;&#22823;&#35268;&#27169;&#19988;&#31283;&#23450;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;ToolBench&#28436;&#21464;&#32780;&#26469;&#30340;StableToolBench&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21644;&#31283;&#23450;&#35780;&#20272;&#31995;&#32479;&#12290;&#34394;&#25311;API&#26381;&#21153;&#22120;&#21253;&#21547;&#32531;&#23384;&#31995;&#32479;&#21644;API&#27169;&#25311;&#22120;&#65292;&#20114;&#34917;&#20943;&#36731;API&#29366;&#24577;&#21464;&#21270;&#12290;&#21516;&#26102;&#65292;&#31283;&#23450;&#30340;&#35780;&#20272;&#31995;&#32479;&#20351;&#29992;GPT-4&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#35774;&#35745;&#21487;&#35299;&#20915;&#30340;&#36890;&#36807;&#29575;&#21644;&#32988;&#29575;&#65292;&#20197;&#28040;&#38500;&#35780;&#20272;&#36807;&#31243;&#20013;&#30340;&#38543;&#26426;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07714v1 Announce Type: new  Abstract: Large Language Models (LLMs) have witnessed remarkable advancements in recent years, prompting the exploration of tool learning, which integrates LLMs with external tools to address diverse real-world challenges. Assessing the capability of LLMs to utilise tools necessitates large-scale and stable benchmarks. However, previous works relied on either hand-crafted online tools with limited scale, or large-scale real online APIs suffering from instability of API status. To address this problem, we introduce StableToolBench, a benchmark evolving from ToolBench, proposing a virtual API server and stable evaluation system. The virtual API server contains a caching system and API simulators which are complementary to alleviate the change in API status. Meanwhile, the stable evaluation system designs solvable pass and win rates using GPT-4 as the automatic evaluator to eliminate the randomness during evaluation. Experimental results demonstrate 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#25913;&#21464;&#20102;ASIC&#24037;&#31243;&#39046;&#22495;&#65292;&#20294;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#30828;&#20214;&#25551;&#36848;&#20195;&#30721;&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#65292;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36890;&#36807;&#24494;&#35843;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#21644;&#37325;&#32452;HDL&#20195;&#30721;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07039</link><description>&lt;p&gt;
&#20174;&#33521;&#35821;&#21040;ASIC&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30828;&#20214;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
From English to ASIC: Hardware Implementation with Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07039
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#25913;&#21464;&#20102;ASIC&#24037;&#31243;&#39046;&#22495;&#65292;&#20294;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#30828;&#20214;&#25551;&#36848;&#20195;&#30721;&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#65292;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36890;&#36807;&#24494;&#35843;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#21644;&#37325;&#32452;HDL&#20195;&#30721;&#25968;&#25454;&#38598;&#26469;&#25552;&#39640;&#31934;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;ASIC&#24037;&#31243;&#39046;&#22495;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#29616;&#20195;&#25968;&#23383;&#30005;&#36335;&#30340;&#22797;&#26434;&#24615;&#20063;&#22312;&#22686;&#21152;&#65292;&#36825;&#21152;&#21095;&#20102;&#23545;HDL&#32534;&#30721;&#30340;&#35201;&#27714;&#65292;&#38656;&#35201;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#22797;&#26434;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#30828;&#20214;&#25551;&#36848;&#20195;&#30721;&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#65292;&#21152;&#20043;&#30456;&#24212;&#39640;&#36136;&#37327;&#30340;&#20195;&#30721;&#25968;&#25454;&#38598;&#31232;&#32570;&#65292;&#25361;&#25112;&#19981;&#26029;&#12290;&#36825;&#20123;&#25361;&#25112;&#20984;&#26174;&#20102;LLM&#28508;&#21147;&#38761;&#26032;&#25968;&#23383;&#30005;&#36335;&#35774;&#35745;&#19982;&#24403;&#21069;&#33021;&#21147;&#20934;&#30830;&#35299;&#37322;&#21644;&#23454;&#26045;&#30828;&#20214;&#35268;&#33539;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#19987;&#27880;&#20110;&#39046;&#20808;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#21644;HDL&#20195;&#30721;&#25968;&#25454;&#38598;&#37325;&#32452;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07039v1 Announce Type: cross  Abstract: In the realm of ASIC engineering, the landscape has been significantly reshaped by the rapid development of LLM, paralleled by an increase in the complexity of modern digital circuits. This complexity has escalated the requirements for HDL coding, necessitating a higher degree of precision and sophistication. However, challenges have been faced due to the less-than-optimal performance of modern language models in generating hardware description code, a situation further exacerbated by the scarcity of the corresponding high-quality code datasets. These challenges have highlighted the gap between the potential of LLMs to revolutionize digital circuit design and their current capabilities in accurately interpreting and implementing hardware specifications. To address these challenges, a strategy focusing on the fine-tuning of the leading-edge nature language model and the reshuffling of the HDL code dataset has been developed. The fine-tu
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#24403;&#21069;&#35270;&#35273;&#19982;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#31867;&#22312;&#21487;&#33021;&#26631;&#31614;&#30340;&#20998;&#24067;&#19978;&#26174;&#31034;&#20986;&#26497;&#22823;&#20027;&#35266;&#21464;&#24322;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#35270;&#35273;&#23545;&#35937;&#30340;&#21629;&#21517;&#12289;&#25551;&#36848;&#21644;&#37327;&#21270;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.06935</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#23545;&#35937;&#21629;&#21517;&#12289;&#25551;&#36848;&#21644;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Naming, Describing, and Quantifying Visual Objects in Humans and LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06935
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#24403;&#21069;&#35270;&#35273;&#19982;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#31867;&#22312;&#21487;&#33021;&#26631;&#31614;&#30340;&#20998;&#24067;&#19978;&#26174;&#31034;&#20986;&#26497;&#22823;&#20027;&#35266;&#21464;&#24322;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#35270;&#35273;&#23545;&#35937;&#30340;&#21629;&#21517;&#12289;&#25551;&#36848;&#21644;&#37327;&#21270;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35762;&#35805;&#32773;&#22312;&#25551;&#36848;&#22270;&#20687;&#20013;&#30340;&#21516;&#19968;&#23545;&#35937;&#26102;&#20351;&#29992;&#21508;&#31181;&#19981;&#21516;&#30340;&#34920;&#36798;&#26041;&#24335;&#65292;&#36825;&#20135;&#29983;&#20102;&#30001;&#35821;&#29992;&#32422;&#26463;&#39537;&#21160;&#30340;&#21512;&#29702;&#26631;&#31614;&#20998;&#24067;&#65292;&#24403;&#21069;&#35270;&#35273;&#19982;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;VLLMs&#65289;&#33021;&#22815;&#27169;&#20223;&#35821;&#35328;&#20351;&#29992;&#20013;&#36825;&#19968;&#20851;&#38190;&#29305;&#24449;&#30340;&#31243;&#24230;&#23578;&#19981;&#26126;&#30830;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;VLLMs&#65288;FROMAGe&#12289;BLIP-2&#12289;LLaVA&#65289;&#22312;&#20154;&#31867;&#22312;&#21487;&#33021;&#26631;&#31614;&#30340;&#20998;&#24067;&#19978;&#26174;&#31034;&#20986;&#26497;&#22823;&#20027;&#35266;&#21464;&#24322;&#24615;&#30340;&#19977;&#20010;&#31867;&#21035;&#65288;&#21517;&#35789;&#12289;&#23646;&#24615;&#21644;&#37327;&#35789;&#65289;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06935v1 Announce Type: new  Abstract: While human speakers use a variety of different expressions when describing the same object in an image, giving rise to a distribution of plausible labels driven by pragmatic constraints, the extent to which current Vision \&amp; Language Large Language Models (VLLMs) can mimic this crucial feature of language use is an open question. This applies to common, everyday objects, but it is particularly interesting for uncommon or novel objects for which a category label may be lacking or fuzzy. Furthermore, humans show clear production preferences for highly context-sensitive expressions, such as the quantifiers `few' or `most'. In our work, we evaluate VLLMs (FROMAGe, BLIP-2, LLaVA) on three categories (nouns, attributes, and quantifiers) where humans show great subjective variability concerning the distribution over plausible labels, using datasets and resources mostly under-explored in previous work. Our results reveal mixed evidence on the a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21475;&#22836;&#23545;&#35805;&#20013;&#20351;&#29992;&#35821;&#38899;&#27963;&#21160;&#25237;&#24433;&#36827;&#34892;&#22810;&#35821;&#35328;&#20132;&#26367;&#39044;&#27979;&#65292;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#19982;&#21333;&#19968;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#23398;&#20250;&#20102;&#36776;&#21035;&#36755;&#20837;&#20449;&#21495;&#30340;&#35821;&#35328;&#12290;</title><link>https://arxiv.org/abs/2403.06487</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#38899;&#27963;&#21160;&#25237;&#24433;&#36827;&#34892;&#22810;&#35821;&#35328;&#20132;&#26367;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multilingual Turn-taking Prediction Using Voice Activity Projection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21475;&#22836;&#23545;&#35805;&#20013;&#20351;&#29992;&#35821;&#38899;&#27963;&#21160;&#25237;&#24433;&#36827;&#34892;&#22810;&#35821;&#35328;&#20132;&#26367;&#39044;&#27979;&#65292;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#19982;&#21333;&#19968;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#24182;&#19988;&#23398;&#20250;&#20102;&#36776;&#21035;&#36755;&#20837;&#20449;&#21495;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#19978;&#24212;&#29992;&#35821;&#38899;&#27963;&#21160;&#25237;&#24433;&#65288;VAP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#21475;&#22836;&#23545;&#35805;&#30340;&#39044;&#27979;&#24615;&#20132;&#26367;&#27169;&#22411;&#65292;&#28085;&#30422;&#33521;&#35821;&#12289;&#27721;&#35821;&#21644;&#26085;&#35821;&#12290;VAP&#27169;&#22411;&#25345;&#32493;&#39044;&#27979;&#21452;&#20154;&#23545;&#35805;&#20013;&#21442;&#19982;&#32773;&#21363;&#23558;&#21457;&#29983;&#30340;&#35821;&#38899;&#27963;&#21160;&#65292;&#21033;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;Transformer&#25429;&#25417;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#21160;&#24577;&#20114;&#21160;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21333;&#19968;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;VAP&#27169;&#22411;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#30340;&#24212;&#29992;&#19981;&#20250;&#20135;&#29983;&#24456;&#22909;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#19977;&#31181;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#25152;&#26377;&#35821;&#35328;&#19978;&#34920;&#29616;&#20986;&#19982;&#21333;&#35821;&#27169;&#22411;&#30456;&#24403;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#24050;&#23398;&#20250;&#36776;&#21035;&#36755;&#20837;&#20449;&#21495;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#23545;&#38899;&#35843;&#25935;&#24863;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#34987;&#35748;&#20026;&#23545;&#20110;&#20132;&#26367;&#38750;&#24120;&#37325;&#35201;&#30340;&#38901;&#24459;&#32447;&#32034;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06487v1 Announce Type: new  Abstract: This paper investigates the application of voice activity projection (VAP), a predictive turn-taking model for spoken dialogue, on multilingual data, encompassing English, Mandarin, and Japanese. The VAP model continuously predicts the upcoming voice activities of participants in dyadic dialogue, leveraging a cross-attention Transformer to capture the dynamic interplay between participants. The results show that a monolingual VAP model trained on one language does not make good predictions when applied to other languages. However, a multilingual model, trained on all three languages, demonstrates predictive performance on par with monolingual models across all languages. Further analyses show that the multilingual model has learned to discern the language of the input signal. We also analyze the sensitivity to pitch, a prosodic cue that is thought to be important for turn-taking. Finally, we compare two different audio encoders, contrast
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#22810;&#27169;&#24577;&#23567;&#35821;&#35328;&#27169;&#22411;(MSLMs)&#21450;&#25552;&#20986;&#39640;&#25928;&#22810;&#27169;&#24577;&#21161;&#25163;Mipha&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#26041;&#38754;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20987;&#36133;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#24320;&#21457;&#24378;&#22823;MSLMs&#25552;&#20379;&#20102;&#35265;&#35299;&#21644;&#25351;&#21335;</title><link>https://arxiv.org/abs/2403.06199</link><description>&lt;p&gt;
&#36890;&#36807;&#23567;&#35821;&#35328;&#27169;&#22411;&#20840;&#38754;&#25913;&#36896;&#22810;&#27169;&#24577;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Overhaul of Multimodal Assistant with Small Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06199
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#22810;&#27169;&#24577;&#23567;&#35821;&#35328;&#27169;&#22411;(MSLMs)&#21450;&#25552;&#20986;&#39640;&#25928;&#22810;&#27169;&#24577;&#21161;&#25163;Mipha&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#26041;&#38754;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20987;&#36133;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#24320;&#21457;&#24378;&#22823;MSLMs&#25552;&#20379;&#20102;&#35265;&#35299;&#21644;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#23637;&#31034;&#20102;&#22312;&#19982;&#35270;&#35273;&#29702;&#35299;&#21644;&#25512;&#29702;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22521;&#35757;&#21644;&#25512;&#29702;&#38454;&#27573;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#38754;&#20020;&#38556;&#30861;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30740;&#31350;&#21644;&#29992;&#25143;&#31038;&#21306;&#20013;&#21463;&#20247;&#30340;&#33539;&#22260;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#23567;&#35821;&#35328;&#27169;&#22411;&#65288;MSLMs&#65289;&#30340;&#35774;&#35745;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mipha&#30340;&#39640;&#25928;&#22810;&#27169;&#24577;&#21161;&#25163;&#65292;&#26088;&#22312;&#22312;&#22810;&#20010;&#26041;&#38754;&#20043;&#38388;&#21019;&#36896;&#21327;&#21516;&#20316;&#29992;&#65306;&#35270;&#35273;&#34920;&#31034;&#12289;&#35821;&#35328;&#27169;&#22411;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#19981;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;Mipha-3B&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;MLLMs&#65292;&#29305;&#21035;&#26159;LLaVA-1.5-13B&#12290;&#36890;&#36807;&#35814;&#32454;&#35752;&#35770;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#21457;&#23637;&#24378;&#22823;&#30340;MSLMs&#30340;&#35265;&#35299;&#21644;&#25351;&#21335;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;MLLMs&#30340;&#33021;&#21147;&#30456;&#23218;&#32654;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06199v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) have showcased impressive skills in tasks related to visual understanding and reasoning. Yet, their widespread application faces obstacles due to the high computational demands during both the training and inference phases, restricting their use to a limited audience within the research and user communities. In this paper, we investigate the design aspects of Multimodal Small Language Models (MSLMs) and propose an efficient multimodal assistant named Mipha, which is designed to create synergy among various aspects: visual representation, language models, and optimization strategies. We show that without increasing the volume of training data, our Mipha-3B outperforms the state-of-the-art large MLLMs, especially LLaVA-1.5-13B, on multiple benchmarks. Through detailed discussion, we provide insights and guidelines for developing strong MSLMs that rival the capabilities of MLLMs. Our code is availa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04945</link><description>&lt;p&gt;
&#20026;&#25253;&#21578;&#29983;&#25104;&#35843;&#20248;&#24515;&#30005;&#22270;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram Instruction Tuning for Report Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04945
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20316;&#20026;&#24515;&#33039;&#30149;&#24773;&#30417;&#27979;&#30340;&#20027;&#35201;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24037;&#20855;&#65292;&#23545;&#20110;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;ECG&#25968;&#25454;&#23545;&#24515;&#33039;&#30149;&#24773;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#24573;&#30053;&#20102;ECG&#25253;&#21578;&#29983;&#25104;&#65292;&#36825;&#19981;&#20165;&#32791;&#26102;&#65292;&#32780;&#19988;&#38656;&#35201;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;ECG&#25253;&#21578;&#29983;&#25104;&#24182;&#30830;&#20445;&#20854;&#22810;&#21151;&#33021;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;\textit{&#39318;&#27425;}&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#26469;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#35780;&#20272;MEIT&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#21508;&#31181;LLM&#39592;&#24178;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#23545;&#40784;&#20102;ECG&#20449;&#21495;&#21644;&#25253;&#21578;&#30340;&#34920;&#31034;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#35780;&#20272;MEIT&#19982;&#20061;&#20010;&#24320;&#28304;LLMs&#65292;&#20351;&#29992;&#20102;&#36229;&#36807;80&#19975;&#20010;ECG&#25253;&#21578;&#12290;MEIT&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04945v1 Announce Type: new  Abstract: Electrocardiogram (ECG) serves as the primary non-invasive diagnostic tool for cardiac conditions monitoring, are crucial in assisting clinicians. Recent studies have concentrated on classifying cardiac conditions using ECG data but have overlooked ECG report generation, which is not only time-consuming but also requires clinical expertise. To automate ECG report generation and ensure its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework, the \textit{first} attempt to tackle ECG report generation with LLMs and multimodal instructions. To facilitate future research, we establish a benchmark to evaluate MEIT with various LLMs backbones across two large-scale ECG datasets. Our approach uniquely aligns the representations of the ECG signal and the report, and we conduct extensive experiments to benchmark MEIT with nine open source LLMs, using more than 800,000 ECG reports. MEIT's results underscore the superior p
&lt;/p&gt;</description></item><item><title>MuseGraph&#23558;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#21644;&#36890;&#29992;&#30340;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#20351;&#29992;</title><link>https://arxiv.org/abs/2403.04780</link><description>&lt;p&gt;
MuseGraph&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#23548;&#21521;&#25351;&#20196;&#35843;&#25972;&#29992;&#20110;&#36890;&#29992;&#22270;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04780
&lt;/p&gt;
&lt;p&gt;
MuseGraph&#23558;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#21644;&#36890;&#29992;&#30340;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#20016;&#23500;&#23646;&#24615;&#30340;&#22270;&#22312;&#24314;&#27169;&#20114;&#32852;&#23454;&#20307;&#21644;&#25913;&#36827;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#39044;&#27979;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#24120;&#29992;&#20110;&#24314;&#27169;&#24102;&#23646;&#24615;&#30340;&#22270;&#65292;&#20294;&#38656;&#35201;&#22312;&#24212;&#29992;&#20110;&#19981;&#21516;&#22270;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#26102;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24341;&#20837;&#20102;&#26032;&#30340;&#33539;&#20363;&#65292;&#20294;LLMs&#22312;&#22270;&#25366;&#25496;&#20013;&#30340;&#29983;&#25104;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550; MuseGraph&#65292;&#23427;&#26080;&#32541;&#25972;&#21512;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#24182;&#20419;&#36827;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#21644;&#36890;&#29992;&#30340;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#21487;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#20351;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#36755;&#20837;&#29983;&#25104;&#24341;&#20837;&#19968;&#20010;&#32039;&#20945;&#30340;&#22270;&#25551;&#36848;&#65292;&#20197;&#22312;&#35821;&#35328;&#20196;&#29260;&#38480;&#21046;&#30340;&#32422;&#26463;&#19979;&#23553;&#35013;&#26469;&#33258;&#22270;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04780v1 Announce Type: cross  Abstract: Graphs with abundant attributes are essential in modeling interconnected entities and improving predictions in various real-world applications. Traditional Graph Neural Networks (GNNs), which are commonly used for modeling attributed graphs, need to be re-trained every time when applied to different graph tasks and datasets. Although the emergence of Large Language Models (LLMs) has introduced a new paradigm in natural language processing, the generative potential of LLMs in graph mining remains largely under-explored. To this end, we propose a novel framework MuseGraph, which seamlessly integrates the strengths of GNNs and LLMs and facilitates a more effective and generic approach for graph mining across different tasks and datasets. Specifically, we first introduce a compact graph description via the proposed adaptive input generation to encapsulate key information from the graph under the constraints of language token limitations. T
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#21644;&#25216;&#26415;&#39564;&#35777;&#27010;&#24565;&#65292;&#29992;&#20110;&#23545;&#35328;&#35821;&#20013;&#30340;&#38750;&#35328;&#35821;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#21547;&#20041;&#20851;&#32852;&#36215;&#26469;&#65292;&#20174;&#32780;&#20026;&#25506;&#32034;&#34920;&#36798;&#23454;&#29616;&#22810;&#23618;&#38901;&#24459;&#20107;&#20214;&#30340;&#22823;&#22411;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03522</link><description>&lt;p&gt;
&#33258;&#21457;&#24615;&#35328;&#35821;&#20013;&#30340;&#38750;&#35328;&#35821;&#20449;&#24687; - &#26397;&#30528;&#26032;&#30340;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Non-verbal information in spontaneous speech - towards a new framework of analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03522
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#21644;&#25216;&#26415;&#39564;&#35777;&#27010;&#24565;&#65292;&#29992;&#20110;&#23545;&#35328;&#35821;&#20013;&#30340;&#38750;&#35328;&#35821;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#21547;&#20041;&#20851;&#32852;&#36215;&#26469;&#65292;&#20174;&#32780;&#20026;&#25506;&#32034;&#34920;&#36798;&#23454;&#29616;&#22810;&#23618;&#38901;&#24459;&#20107;&#20214;&#30340;&#22823;&#22411;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35328;&#35821;&#20013;&#30340;&#38750;&#35328;&#35821;&#20449;&#21495;&#26159;&#30001;&#38901;&#24459;&#32534;&#30721;&#30340;&#65292;&#25658;&#24102;&#30340;&#20449;&#24687;&#33539;&#22260;&#20174;&#23545;&#35805;&#34892;&#20026;&#21040;&#24577;&#24230;&#21644;&#24773;&#24863;&#12290;&#23613;&#31649;&#20854;&#37325;&#35201;&#24615;&#65292;&#25484;&#25569;&#25484;&#22768;&#32467;&#26500;&#30340;&#21407;&#21017;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#21644;&#25216;&#26415;&#39564;&#35777;&#27010;&#24565;&#65292;&#29992;&#20110;&#23545;&#38901;&#24459;&#20449;&#21495;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#21547;&#20041;&#20851;&#32852;&#36215;&#26469;&#12290;&#35813;&#26694;&#26550;&#35299;&#37322;&#20102;&#22810;&#23618;&#38901;&#24459;&#20107;&#20214;&#30340;&#34920;&#23618;&#34920;&#31034;&#12290;&#20316;&#20026;&#23454;&#26045;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#31867;&#36807;&#31243;&#65292;&#21487;&#20197;&#35299;&#24320;&#19977;&#20010;&#32423;&#21035;&#30340;&#38901;&#24459;&#29616;&#35937;&#12290;&#23427;&#20381;&#36182;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#23454;&#29616;&#21516;&#26102;&#30340;&#22810;&#31867;&#21035;/&#22810;&#26631;&#31614;&#26816;&#27979;&#12290;&#23427;&#21487;&#20197;&#27010;&#25324;&#21508;&#31181;&#21508;&#26679;&#30340;&#33258;&#21457;&#25968;&#25454;&#65292;&#22312;&#19982;&#20154;&#31867;&#27880;&#37322;&#30456;&#24403;&#25110;&#20248;&#20110;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#12290;&#38500;&#20102;&#23545;&#38901;&#24459;&#30340;&#26631;&#20934;&#21270;&#24418;&#24335;&#21270;&#22806;&#65292;&#35299;&#24320;&#38901;&#24459;&#27169;&#24335;&#36824;&#21487;&#20197;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03522v1 Announce Type: cross  Abstract: Non-verbal signals in speech are encoded by prosody and carry information that ranges from conversation action to attitude and emotion. Despite its importance, the principles that govern prosodic structure are not yet adequately understood. This paper offers an analytical schema and a technological proof-of-concept for the categorization of prosodic signals and their association with meaning. The schema interprets surface-representations of multi-layered prosodic events. As a first step towards implementation, we present a classification process that disentangles prosodic phenomena of three orders. It relies on fine-tuning a pre-trained speech recognition model, enabling the simultaneous multi-class/multi-label detection. It generalizes over a large variety of spontaneous data, performing on a par with, or superior to, human annotation. In addition to a standardized formalization of prosody, disentangling prosodic patterns can direct a
&lt;/p&gt;</description></item><item><title>&#22312;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#38469;&#20020;&#24202;&#26696;&#20363;&#19978;&#30340;&#34920;&#29616;&#26159;&#20851;&#38190;&#65292;&#22240;&#27492;&#26500;&#24314;&#20102;&#20004;&#20010;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.18060</link><description>&lt;p&gt;
&#22312;&#22238;&#31572;&#21644;&#35299;&#37322;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21307;&#23398;&#38382;&#39064;&#19978;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Models on Answering and Explaining Challenging Medical Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18060
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23454;&#38469;&#20020;&#24202;&#26696;&#20363;&#19978;&#30340;&#34920;&#29616;&#26159;&#20851;&#38190;&#65292;&#22240;&#27492;&#26500;&#24314;&#20102;&#20004;&#20010;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20363;&#22914;&#36890;&#36807;&#21307;&#23398;&#25191;&#29031;&#32771;&#35797;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20381;&#36182;&#20110;&#22996;&#21592;&#20250;&#32771;&#35797;&#38382;&#39064;&#25110;&#19968;&#33324;&#21307;&#23398;&#38382;&#39064;&#65292;&#26080;&#27861;&#25429;&#25417;&#30495;&#23454;&#20020;&#24202;&#26696;&#20363;&#30340;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#31572;&#26696;&#30340;&#21442;&#32771;&#35299;&#37322;&#38459;&#30861;&#20102;&#23545;&#27169;&#22411;&#35299;&#37322;&#30340;&#35780;&#20272;&#65292;&#36825;&#23545;&#25903;&#25345;&#21307;&#29983;&#20570;&#20986;&#22797;&#26434;&#30340;&#21307;&#30103;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;JAMA&#20020;&#24202;&#25361;&#25112;&#21644;Medbullets&#12290;JAMA&#20020;&#24202;&#25361;&#25112;&#21253;&#21547;&#22522;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20020;&#24202;&#26696;&#20363;&#30340;&#38382;&#39064;&#65292;&#32780;Medbullets&#21253;&#21547;&#31867;&#20284;USMLE Step 2&amp;3&#39118;&#26684;&#30340;&#20020;&#24202;&#38382;&#39064;&#12290;&#20004;&#20010;&#25968;&#25454;&#38598;&#22343;&#20197;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;-&#22238;&#31572;&#20219;&#21153;&#30340;&#32467;&#26500;&#21270;&#24418;&#24335;&#21576;&#29616;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#38468;&#26377;&#19987;&#23478;&#25776;&#20889;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#22312;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#22235;&#20010;LLMs&#12290;&#23454;&#39564;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18060v1 Announce Type: new  Abstract: LLMs have demonstrated impressive performance in answering medical questions, such as passing medical licensing examinations. However, most existing benchmarks rely on board exam questions or general medical questions, falling short in capturing the complexity of realistic clinical cases. Moreover, the lack of reference explanations for answers hampers the evaluation of model explanations, which are crucial to supporting doctors in making complex medical decisions. To address these challenges, we construct two new datasets: JAMA Clinical Challenge and Medbullets. JAMA Clinical Challenge consists of questions based on challenging clinical cases, while Medbullets comprises USMLE Step 2&amp;3 style clinical questions. Both datasets are structured as multiple-choice question-answering tasks, where each question is accompanied by an expert-written explanation. We evaluate four LLMs on the two datasets using various prompts. Experiments demonstrat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#39564;&#35777;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#20998;&#26032;&#24037;&#20855;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#24615;&#33258;&#38382;&#38382;&#39064;&#26469;&#23454;&#29616;&#24037;&#20855;&#36873;&#25321;&#21644;&#21442;&#25968;&#29983;&#25104;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#26032;&#24037;&#20855;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.14158</link><description>&lt;p&gt;
TOOLVERIFIER: &#36890;&#36807;&#33258;&#39564;&#35777;&#23454;&#29616;&#23545;&#26032;&#24037;&#20855;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
TOOLVERIFIER: Generalization to New Tools via Self-Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14158
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#39564;&#35777;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21306;&#20998;&#26032;&#24037;&#20855;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#24615;&#33258;&#38382;&#38382;&#39064;&#26469;&#23454;&#29616;&#24037;&#20855;&#36873;&#25321;&#21644;&#21442;&#25968;&#29983;&#25104;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#26032;&#24037;&#20855;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#25945;&#20250;&#22914;&#20309;&#20351;&#29992;&#24037;&#20855;&#26159;&#36808;&#21521;&#26500;&#24314;&#36890;&#29992;&#21161;&#25163;&#30340;&#37325;&#35201;&#37324;&#31243;&#30865;&#65292;&#20294;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#34429;&#28982;&#22312;&#38024;&#23545;&#29305;&#23450;&#24037;&#20855;&#30340;&#24494;&#35843;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#22312;&#22914;&#20309;&#20174;&#20165;&#26377;&#23569;&#25968;&#31034;&#20363;&#20013;&#24378;&#22823;&#22320;&#20351;&#29992;&#26032;&#24037;&#20855;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#39564;&#35777;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24037;&#20855;&#36873;&#25321;&#21644;&#21442;&#25968;&#29983;&#25104;&#36807;&#31243;&#20013;&#36827;&#34892;&#23545;&#27604;&#24615;&#33258;&#38382;&#38382;&#39064;&#26469;&#21306;&#20998;&#36817;&#20284;&#30340;&#20505;&#36873;&#24037;&#20855;&#12290;&#25105;&#20204;&#21033;&#29992;Llama-2 70B&#26500;&#24314;&#20102;&#21512;&#25104;&#30340;&#39640;&#36136;&#37327;&#33258;&#29983;&#25104;&#25968;&#25454;&#65292;&#29992;&#20110;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25171;&#31639;&#23558;&#20854;&#20844;&#24320;&#21457;&#24067;&#12290;&#22312;ToolBench&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;4&#39033;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21253;&#25324;17&#20010;&#26410;&#35265;&#36807;&#30340;&#24037;&#20855;&#65292;&#23637;&#31034;&#20102;&#22312;&#23569;&#26679;&#26412;&#22522;&#32447;&#27979;&#35797;&#20013;&#24179;&#22343;&#25552;&#21319;&#20102;22%&#65292;&#21363;&#20351;&#22312;&#20505;&#36873;&#24037;&#20855;&#20043;&#38388;&#30340;&#21306;&#21035;&#24494;&#22937;&#20043;&#22788;&#30340;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14158v1 Announce Type: new  Abstract: Teaching language models to use tools is an important milestone towards building general assistants, but remains an open problem. While there has been significant progress on learning to use specific tools via fine-tuning, language models still struggle with learning how to robustly use new tools from only a few demonstrations. In this work we introduce a self-verification method which distinguishes between close candidates by self-asking contrastive questions during (1) tool selection; and (2) parameter generation. We construct synthetic, high-quality, self-generated data for this goal using Llama-2 70B, which we intend to release publicly. Extensive experiments on 4 tasks from the ToolBench benchmark, consisting of 17 unseen tools, demonstrate an average improvement of 22% over few-shot baselines, even in scenarios where the distinctions between candidate tools are finely nuanced.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32479;&#19968;&#30340;&#22522;&#20110;&#20998;&#31867;&#23398;&#25351;&#23548;&#30340;&#25351;&#23548;&#35843;&#25972;&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29616;&#26377;&#20998;&#31867;&#23398;&#36827;&#34892;&#23454;&#20307;&#20851;&#31995;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#23454;&#20307;&#38598;&#25193;&#23637;&#12289;&#20998;&#31867;&#23398;&#25193;&#23637;&#21644;&#31181;&#23376;&#24341;&#23548;&#20998;&#31867;&#23398;&#26500;&#24314;&#19977;&#20010;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.13405</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20110;&#20998;&#31867;&#23398;&#25351;&#23548;&#30340;&#23454;&#20307;&#38598;&#25193;&#23637;&#21644;&#20998;&#31867;&#23398;&#25193;&#23637;&#30340;&#25351;&#23548;&#35843;&#25972;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Taxonomy-Guided Instruction Tuning Framework for Entity Set Expansion and Taxonomy Expansion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13405
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32479;&#19968;&#30340;&#22522;&#20110;&#20998;&#31867;&#23398;&#25351;&#23548;&#30340;&#25351;&#23548;&#35843;&#25972;&#26694;&#26550;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#29616;&#26377;&#20998;&#31867;&#23398;&#36827;&#34892;&#23454;&#20307;&#20851;&#31995;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#23454;&#20307;&#38598;&#25193;&#23637;&#12289;&#20998;&#31867;&#23398;&#25193;&#23637;&#21644;&#31181;&#23376;&#24341;&#23548;&#20998;&#31867;&#23398;&#26500;&#24314;&#19977;&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38598;&#25193;&#23637;&#12289;&#20998;&#31867;&#23398;&#25193;&#23637;&#21644;&#31181;&#23376;&#24341;&#23548;&#20998;&#31867;&#23398;&#26500;&#24314;&#26159;&#19977;&#20010;&#20195;&#34920;&#24615;&#20219;&#21153;&#65292;&#21487;&#20197;&#29992;&#26469;&#33258;&#21160;&#21521;&#29616;&#26377;&#20998;&#31867;&#23398;&#22635;&#20805;&#26032;&#23454;&#20307;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#24322;&#36136;&#25216;&#26415;&#20998;&#21035;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#35270;&#35282;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#20998;&#31867;&#23398;&#32467;&#26500;&#30340;&#35270;&#35282;&#30830;&#35748;&#20102;&#36825;&#20123;&#20219;&#21153;&#25152;&#38656;&#30340;&#20849;&#21516;&#20851;&#38190;&#25216;&#33021;&#8212;&#8212;&#25214;&#21040;&#8220;&#20804;&#24351;&#8221;&#21644;&#25214;&#21040;&#8220;&#29238;&#27597;&#8221;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20110;&#20998;&#31867;&#23398;&#25351;&#23548;&#30340;&#25351;&#23548;&#35843;&#25972;&#26694;&#26550;&#26469;&#20849;&#21516;&#35299;&#20915;&#36825;&#19977;&#20010;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#20998;&#31867;&#23398;&#20316;&#20026;&#20016;&#23500;&#30340;&#23454;&#20307;&#20851;&#31995;&#28304;&#65292;&#25105;&#20204;&#21033;&#29992;&#25351;&#23548;&#35843;&#25972;&#26469;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29983;&#25104;&#29238;&#27597;&#21644;&#20804;&#24351;&#23454;&#20307;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;TaxoInstruct&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13405v1 Announce Type: new  Abstract: Entity Set Expansion, Taxonomy Expansion, and Seed-Guided Taxonomy Construction are three representative tasks that can be used to automatically populate an existing taxonomy with new entities. However, previous approaches often address these tasks separately with heterogeneous techniques, lacking a unified perspective. To tackle this issue, in this paper, we identify the common key skills needed for these tasks from the view of taxonomy structures -- finding 'siblings' and finding 'parents' -- and propose a unified taxonomy-guided instruction tuning framework to jointly solve the three tasks. To be specific, by leveraging the existing taxonomy as a rich source of entity relationships, we utilize instruction tuning to fine-tune a large language model to generate parent and sibling entities. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of TaxoInstruct, which outperforms task-specific baselines across 
&lt;/p&gt;</description></item><item><title>LongAgent&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#23558;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;128K&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#38271;&#25991;&#26412;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11550</link><description>&lt;p&gt;
LongAgent: &#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#23558;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;128K&#19978;&#19979;&#25991;
&lt;/p&gt;
&lt;p&gt;
LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11550
&lt;/p&gt;
&lt;p&gt;
LongAgent&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#23558;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;128K&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#38271;&#25991;&#26412;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#35821;&#35328;&#21644;&#25191;&#34892;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;LLMs&#20197;&#20854;&#26114;&#36149;&#30340;&#35757;&#32451;&#25104;&#26412;&#21644;&#39640;&#25512;&#29702;&#24310;&#36831;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22914;GPT-4&#21644;Claude2&#22312;&#22788;&#29702;&#36229;&#36807;$100k$&#26631;&#35760;&#30340;&#36755;&#20837;&#26102;&#20063;&#32463;&#24120;&#20986;&#38169;&#65292;&#36825;&#31181;&#29616;&#35937;&#20063;&#34987;&#31216;&#20026;\textit{&#20013;&#38388;&#36855;&#22833;}&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#26041;&#27861;\textsc{LongAgent}&#65292;&#23558;LLMs&#65288;&#20363;&#22914;LLaMA&#65289;&#25193;&#23637;&#21040;128K&#19978;&#19979;&#25991;&#65292;&#24182;&#23637;&#31034;&#20986;&#22312;&#38271;&#25991;&#26412;&#22788;&#29702;&#26041;&#38754;&#21487;&#33021;&#20248;&#20110;GPT-4&#30340;&#28508;&#21147;&#12290;&#22312;\textsc{LongAgent}&#20013;&#65292;&#19968;&#20301;&#39046;&#23548;&#32773;&#36127;&#36131;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#24182;&#25351;&#23548;&#22242;&#38431;&#25104;&#21592;&#20174;&#25991;&#26723;&#20013;&#33719;&#21462;&#20449;&#24687;&#12290;&#30001;&#20110;&#25104;&#21592;&#23384;&#22312;&#24187;&#35273;&#65292;&#39046;&#23548;&#32773;&#20174;&#20960;&#21313;&#21040;&#25968;&#30334;&#21517;&#25104;&#21592;&#30340;&#22238;&#24212;&#20013;&#33719;&#21462;&#20934;&#30830;&#20449;&#24687;&#24182;&#38750;&#26131;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11550v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated impressive performance in understanding language and executing complex reasoning tasks. However, LLMs with long context windows have been notorious for their expensive training costs and high inference latency. Even the most advanced models such as GPT-4 and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a phenomenon also known as \textit{lost in the middle}. In this paper, we propose \textsc{LongAgent}, a method based on multi-agent collaboration, which scales LLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority in long-text processing compared to GPT-4. In \textsc{LongAgent}, a leader is responsible for understanding user intent and directing team members to acquire information from documents. Due to members' hallucinations, it is non-trivial for a leader to obtain accurate information from the responses of dozens to hundreds of member
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#20154;&#20307;&#29616;&#20854;&#25198;&#28436;&#35282;&#33394;&#30340;&#31243;&#24230;&#23545;&#20445;&#30041;&#29575;&#30340;&#24433;&#21709;&#26377;&#38480;&#65292;&#32780;&#20854;&#35762;&#35805;&#30340;&#27599;&#20010;&#36718;&#27425;&#30340;&#38271;&#24230;&#26174;&#33879;&#24433;&#21709;&#20445;&#30041;&#29575;</title><link>https://arxiv.org/abs/2402.11522</link><description>&lt;p&gt;
&#25581;&#31034;&#24341;&#20154;&#20837;&#32988;&#23545;&#35805;&#30340;&#31192;&#23494;&#65306;&#35753;&#29992;&#25143;&#27785;&#36855;&#20110;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#30340;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Secrets of Engaging Conversations: Factors that Keep Users Hooked on Role-Playing Dialog Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11522
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#20307;&#29616;&#20854;&#25198;&#28436;&#35282;&#33394;&#30340;&#31243;&#24230;&#23545;&#20445;&#30041;&#29575;&#30340;&#24433;&#21709;&#26377;&#38480;&#65292;&#32780;&#20854;&#35762;&#35805;&#30340;&#27599;&#20010;&#36718;&#27425;&#30340;&#38271;&#24230;&#26174;&#33879;&#24433;&#21709;&#20445;&#30041;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#35805;&#20195;&#29702;&#21464;&#24471;&#36234;&#26469;&#36234;&#31867;&#20154;&#21270;&#65292;&#20154;&#20204;&#29616;&#22312;&#27491;&#22312;&#36827;&#34892;&#25345;&#32493;&#23545;&#35805;&#65292;&#21487;&#20197;&#20174;&#30701;&#26242;&#30340;&#26102;&#21051;&#24310;&#20280;&#21040;&#38271;&#26102;&#38388;&#12290;&#29702;&#35299;&#20419;&#20351;&#36825;&#20123;&#20114;&#21160;&#25345;&#32493;&#30340;&#22240;&#32032;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24456;&#23569;&#25506;&#32034;&#36825;&#31181;&#38271;&#26102;&#38388;&#21644;&#30495;&#23454;&#23545;&#35805;&#30340;&#30701;&#26399;&#27169;&#25311;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24433;&#21709;&#35282;&#33394;&#25198;&#28436;&#27169;&#22411;&#19982;&#30495;&#23454;&#29992;&#25143;&#20043;&#38388;&#20114;&#21160;&#20013;&#20445;&#30041;&#29575;&#30340;&#22240;&#32032;&#12290;&#36890;&#36807;&#20998;&#26512;&#30495;&#23454;&#29992;&#25143;&#21644;&#25968;&#21315;&#20010;&#35282;&#33394;&#20043;&#38388;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#26816;&#26597;&#20102;&#22810;&#20010;&#22240;&#32032;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#29992;&#25143;&#20445;&#30041;&#29575;&#30340;&#24433;&#21709;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#26426;&#22120;&#20154;&#20307;&#29616;&#20854;&#25198;&#28436;&#35282;&#33394;&#30340;&#31243;&#24230;&#23545;&#20445;&#30041;&#29575;&#30340;&#24433;&#21709;&#26377;&#38480;&#65292;&#32780;&#20854;&#35762;&#35805;&#30340;&#27599;&#20010;&#36718;&#27425;&#30340;&#38271;&#24230;&#26174;&#33879;&#24433;&#21709;&#20445;&#30041;&#29575;&#12290;&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#29992;&#25143;&#21442;&#19982;&#24230;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11522v1 Announce Type: new  Abstract: With the growing humanlike nature of dialog agents, people are now engaging in extended conversations that can stretch from brief moments to substantial periods of time. Understanding the factors that contribute to sustaining these interactions is crucial, yet existing studies primarily focusing on short-term simulations that rarely explore such prolonged and real conversations.   In this paper, we investigate the factors influencing retention rates in real interactions with roleplaying models. By analyzing a large dataset of interactions between real users and thousands of characters, we systematically examine multiple factors and assess their impact on user retention rate. Surprisingly, we find that the degree to which the bot embodies the roles it plays has limited influence on retention rates, while the length of each turn it speaks significantly affects retention rates. This study sheds light on the critical aspects of user engageme
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.10659</link><description>&lt;p&gt;
&#22810;&#20010;LLM&#20043;&#38388;&#30340;&#32593;&#32476;&#24418;&#25104;&#19982;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Network Formation and Dynamics Among Multi-LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10659
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#24433;&#21709;&#34892;&#20026;&#12289;&#20559;&#22909;&#21644;&#20851;&#31995;&#65292;&#22312;&#20154;&#31867;&#31038;&#20250;&#20013;&#23545;&#20449;&#24687;&#21644;&#35268;&#33539;&#30340;&#20256;&#25773;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#31038;&#20132;&#21644;&#19987;&#19994;&#29615;&#22659;&#20013;&#65292;&#29702;&#35299;&#23427;&#20204;&#22312;&#31038;&#20132;&#32593;&#32476;&#21644;&#20114;&#21160;&#32972;&#26223;&#19979;&#30340;&#34892;&#20026;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20998;&#26512;&#20102;&#26631;&#20934;&#32593;&#32476;&#32467;&#26500;&#21644;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#30340;&#34892;&#20026;&#65292;&#20197;&#30830;&#23450;&#22810;&#20010;LLMs&#30340;&#21160;&#24577;&#26159;&#21542;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#31038;&#20132;&#32593;&#32476;&#21407;&#21017;&#65292;&#21253;&#25324;&#24494;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#20559;&#29233;&#38468;&#30528;&#12289;&#19977;&#35282;&#38381;&#21512;&#21644;&#21516;&#20284;&#24615;&#65292;&#20197;&#21450;&#23439;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#31038;&#21306;&#32467;&#26500;&#21644;&#23567;&#19990;&#30028;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;&#21521;LLMs&#25552;&#20379;&#32593;&#32476;&#32467;&#26500;&#24182;&#35810;&#38382;&#23427;&#20204;&#23545;&#32593;&#32476;&#24418;&#25104;&#30340;&#20559;&#22909;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#20986;&#25152;&#26377;&#36825;&#20123;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10659v1 Announce Type: cross  Abstract: Social networks influence behaviors, preferences, and relationships and play a crucial role in the dissemination of information and norms within human societies. As large language models (LLMs) increasingly integrate into social and professional environments, understanding their behavior within the context of social networks and interactions becomes essential. Our study analyzes the behaviors of standard network structures and real-world networks to determine whether the dynamics of multiple LLMs align with human social dynamics. We explore various social network principles, including micro-level concepts such as preferential attachment, triadic closure, and homophily, as well as macro-level concepts like community structure and the small-world phenomenon. Our findings suggest that LLMs demonstrate all these principles when they are provided with network structures and asked about their preferences regarding network formation. Furtherm
&lt;/p&gt;</description></item><item><title>SciGLM&#24341;&#20837;&#20102;&#33258;&#25105;&#21453;&#24605;&#25351;&#23548;&#27880;&#37322;&#26694;&#26550;&#65292;&#29992;&#20110;&#24357;&#34917;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#22797;&#26434;&#31185;&#23398;&#27010;&#24565;&#12289;&#25512;&#23548;&#31526;&#21495;&#26041;&#31243;&#24335;&#21644;&#35299;&#20915;&#39640;&#32423;&#25968;&#20540;&#35745;&#31639;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#20197;&#35757;&#32451;&#33021;&#22815;&#36827;&#34892;&#22823;&#23398;&#27700;&#24179;&#31185;&#23398;&#25512;&#29702;&#30340;&#31185;&#23398;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2401.07950</link><description>&lt;p&gt;
SciGLM: &#29992;&#33258;&#25105;&#21453;&#24605;&#25351;&#23548;&#27880;&#37322;&#21644;&#35843;&#25972;&#35757;&#32451;&#31185;&#23398;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SciGLM: Training Scientific Language Models with Self-Reflective Instruction Annotation and Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07950
&lt;/p&gt;
&lt;p&gt;
SciGLM&#24341;&#20837;&#20102;&#33258;&#25105;&#21453;&#24605;&#25351;&#23548;&#27880;&#37322;&#26694;&#26550;&#65292;&#29992;&#20110;&#24357;&#34917;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#22797;&#26434;&#31185;&#23398;&#27010;&#24565;&#12289;&#25512;&#23548;&#31526;&#21495;&#26041;&#31243;&#24335;&#21644;&#35299;&#20915;&#39640;&#32423;&#25968;&#20540;&#35745;&#31639;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#20197;&#35757;&#32451;&#33021;&#22815;&#36827;&#34892;&#22823;&#23398;&#27700;&#24179;&#31185;&#23398;&#25512;&#29702;&#30340;&#31185;&#23398;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#26174;&#31034;&#20986;&#22312;&#21327;&#21161;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;LLMs&#22312;&#29702;&#35299;&#22797;&#26434;&#31185;&#23398;&#27010;&#24565;&#12289;&#25512;&#23548;&#31526;&#21495;&#26041;&#31243;&#24335;&#21644;&#35299;&#20915;&#39640;&#32423;&#25968;&#20540;&#35745;&#31639;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SciGLM&#65292;&#19968;&#22871;&#33021;&#22815;&#36827;&#34892;&#22823;&#23398;&#27700;&#24179;&#31185;&#23398;&#25512;&#29702;&#30340;&#31185;&#23398;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#21453;&#24605;&#25351;&#23548;&#27880;&#37322;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#31185;&#23398;&#39046;&#22495;&#20013;&#25968;&#25454;&#31232;&#32570;&#25361;&#25112;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#29616;&#26377;LLMs&#20026;&#26410;&#26631;&#35760;&#30340;&#31185;&#23398;&#38382;&#39064;&#29983;&#25104;&#36880;&#27493;&#25512;&#29702;&#65292;&#38543;&#21518;&#32463;&#36807;&#33258;&#25105;&#21453;&#24605;&#30340;&#25209;&#35780;&#21644;&#20462;&#25913;&#36807;&#31243;&#12290;&#24212;&#29992;&#36825;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;SciInstruct&#65292;&#36825;&#26159;&#19968;&#20010;&#28085;&#30422;&#29289;&#29702;&#12289;&#21270;&#23398;&#12289;&#25968;&#23398;&#21644;&#24418;&#24335;&#35777;&#26126;&#30340;&#22810;&#26679;&#21270;&#12289;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;SciInstruct&#23545;ChatGLM&#31995;&#21015;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#22686;&#24378;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07950v2 Announce Type: replace  Abstract: Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations. To bridge these gaps, we introduce SciGLM, a suite of scientific language models able to conduct college-level scientific reasoning. Central to our approach is a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain. This framework leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise. Applying this framework, we curated SciInstruct, a diverse and high-quality dataset encompassing physics, chemistry, math, and formal proofs. We fine-tuned the ChatGLM family of language models with SciInstruct, enhancing
&lt;/p&gt;</description></item><item><title>KnowGPT&#26159;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#33218;&#32769;&#34382;&#26426;&#26500;&#24314;&#26368;&#36866;&#21512;&#27599;&#20010;&#38382;&#39064;&#30340;&#25552;&#31034;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25552;&#21319;&#20102;&#30693;&#35782;&#27880;&#20837;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.06185</link><description>&lt;p&gt;
KnowGPT&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
KnowGPT: Black-Box Knowledge Injection for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06185
&lt;/p&gt;
&lt;p&gt;
KnowGPT&#26159;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#33218;&#32769;&#34382;&#26426;&#26500;&#24314;&#26368;&#36866;&#21512;&#27599;&#20010;&#38382;&#39064;&#30340;&#25552;&#31034;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25552;&#21319;&#20102;&#30693;&#35782;&#27880;&#20837;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#25552;&#20379;&#20114;&#21160;&#24335;API&#65292;&#21487;&#20197;&#20197;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#22238;&#31572;&#24120;&#35265;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#38656;&#35201;&#29305;&#23450;&#39046;&#22495;&#25110;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#30340;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20250;&#32473;&#20986;&#19981;&#20934;&#30830;&#25110;&#19981;&#27491;&#30830;&#30340;&#21709;&#24212;&#65292;&#36825;&#20123;&#30693;&#35782;&#24182;&#26410;&#21253;&#21547;&#22312;&#23427;&#20204;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;LLMs&#24182;&#38750;&#24320;&#28304;&#65292;&#36825;&#20351;&#24471;&#20165;&#20351;&#29992;&#27169;&#22411;API&#27880;&#20837;&#30693;&#35782;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;KnowGPT&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#22312;&#38382;&#31572;&#20013;&#30340;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;&#12290;KnowGPT&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#30693;&#35782;&#22270;&#20013;&#25552;&#21462;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#20026;&#27599;&#20010;&#38382;&#39064;&#26500;&#24314;&#26368;&#21512;&#36866;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;KnowGPT&#26174;&#33879;&#22686;&#24378;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;KnowGPT&#24179;&#22343;&#25913;&#36827;&#20102;23%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06185v2 Announce Type: replace-cross  Abstract: Generative Large Language Models (LLMs), such as ChatGPT, offer interactive APIs that can answer common questions at a human-expert level. However, these models often give inaccurate or incorrect responses when faced with questions requiring domain-specific or professional-specific knowledge not covered in their training corpus. Furthermore, many state-of-the-art LLMs are not open-source, making it challenging to inject knowledge with model APIs only. In this work, we introduce KnowGPT, a black-box knowledge injection framework for LLMs in question answering. KnowGPT leverages deep reinforcement learning (RL) to extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed Bandit (MAB) to construct the most suitable prompt for each question. Our extensive experiments on three benchmark datasets showcase that KnowGPT significantly enhances the existing methods. Notably, KnowGPT achieves an average improvement of 23.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#25366;&#25496;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#26412;&#22320;LLMs&#26469;&#35299;&#20915;&#36890;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#38382;&#39064;&#65292;&#30830;&#20445;&#25968;&#25454;&#23433;&#20840;&#24182;&#36827;&#34892;&#36827;&#19968;&#27493;&#35843;&#25972;</title><link>https://arxiv.org/abs/2312.01678</link><description>&lt;p&gt;
Jellyfish&#65306;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jellyfish: A Large Language Model for Data Preprocessing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01678
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#25366;&#25496;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#26412;&#22320;LLMs&#26469;&#35299;&#20915;&#36890;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#38382;&#39064;&#65292;&#30830;&#20445;&#25968;&#25454;&#23433;&#20840;&#24182;&#36827;&#34892;&#36827;&#19968;&#27493;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#25366;&#25496;&#31649;&#36947;&#20013;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;&#26377;&#21033;&#20110;&#31616;&#21333;&#22788;&#29702;&#30340;&#24178;&#20928;&#26684;&#24335;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#65288;DP&#65289;&#20013;LLMs&#30340;&#21033;&#29992;&#12290;&#19982;&#20351;&#29992;LLMs&#20026;DP&#35774;&#35745;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#24341;&#36215;&#20102;&#20852;&#36259;&#30456;&#27604;&#65292;&#26368;&#36817;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#20513;&#35758;&#36890;&#24120;&#20381;&#36182;&#20110;GPT API&#65292;&#24341;&#21457;&#20102;&#19981;&#21487;&#36991;&#20813;&#30340;&#25968;&#25454;&#27844;&#38671;&#25285;&#24551;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#25351;&#23548;&#35843;&#25972;&#26412;&#22320;LLMs&#65288;7-13B&#27169;&#22411;&#65289;&#20316;&#20026;&#36890;&#29992;DP&#38382;&#35299;&#22120;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#20195;&#34920;&#24615;DP&#20219;&#21153;&#30340;&#22235;&#32452;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#38024;&#23545;DP&#23450;&#21046;&#30340;&#24207;&#21015;&#21270;&#21644;&#30693;&#35782;&#27880;&#20837;&#25216;&#26415;&#26500;&#24314;&#20102;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25351;&#23548;&#35843;&#25972;&#30340;LLMs&#20351;&#29992;&#25143;&#33021;&#22815;&#20026;DP&#25163;&#21160;&#21046;&#23450;&#25351;&#23548;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#26412;&#22320;&#12289;&#21333;&#19968;&#21644;&#20215;&#26684;&#20302;&#24265;&#30340;GPU&#19978;&#36816;&#34892;&#65292;&#30830;&#20445;&#25968;&#25454;&#23433;&#20840;&#24182;&#23454;&#29616;&#36827;&#19968;&#27493;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#20026;DP&#25351;&#23548;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01678v4 Announce Type: replace  Abstract: This paper explores the utilization of LLMs for data preprocessing (DP), a crucial step in the data mining pipeline that transforms raw data into a clean format conducive to easy processing. Whereas the use of LLMs has sparked interest in devising universal solutions to DP, recent initiatives in this domain typically rely on GPT APIs, raising inevitable data breach concerns. Unlike these approaches, we consider instruction-tuning local LLMs (7 - 13B models) as universal DP ask solver. We select a collection of datasets across four representative DP tasks and construct instruction-tuning data using serialization and knowledge injection techniques tailored to DP. As such, the instruction-tuned LLMs empower users to manually craft instructions for DP. Meanwhile, they can operate on a local, single, and low-priced GPU, ensuring data security and enabling further tuning. Our experiments show that our dataset constructed for DP instruction
&lt;/p&gt;</description></item><item><title>MLLMs&#36890;&#36807;&#20026;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#24314;&#31435;&#26356;&#20016;&#23500;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20851;&#32852;&#65292;&#20197;&#22686;&#24378;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#8220;&#25991;&#26412;&#21098;&#20999;&#8221;&#26041;&#27861;&#26469;&#36991;&#20813;&#20559;&#35265;&#24341;&#20837;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18765</link><description>&lt;p&gt;
MLLMs&#22686;&#24378;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MLLMs-Augmented Visual-Language Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18765
&lt;/p&gt;
&lt;p&gt;
MLLMs&#36890;&#36807;&#20026;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#24314;&#31435;&#26356;&#20016;&#23500;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20851;&#32852;&#65292;&#20197;&#22686;&#24378;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#8220;&#25991;&#26412;&#21098;&#20999;&#8221;&#26041;&#27861;&#26469;&#36991;&#20813;&#20559;&#35265;&#24341;&#20837;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18765v3 &#20844;&#21578;&#31867;&#22411;: replace-cross &#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#22312;&#35768;&#22810;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#20026;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#24314;&#31435;&#26356;&#20016;&#23500;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20851;&#32852;&#26469;&#21152;&#24378;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24456;&#31616;&#21333;&#65292;&#21033;&#29992;MLLMs&#20026;&#27599;&#20010;&#22270;&#20687;&#25193;&#23637;&#22810;&#20010;&#19981;&#21516;&#30340;&#26631;&#39064;&#12290;&#20026;&#20102;&#38450;&#27490;MLLMs&#30340;&#24187;&#35273;&#21644;&#21333;&#35843;&#35821;&#35328;&#39118;&#26684;&#24341;&#20837;&#30340;&#20559;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#25991;&#26412;&#21098;&#20999;&#8221;&#26469;&#20445;&#25345;&#25193;&#23637;&#26631;&#39064;&#30340;&#36136;&#37327;&#21644;&#21487;&#29992;&#24615;&#12290;&#22312;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#65292;&#22312;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#35843;&#21644;&#38646;-shot&#35774;&#32622;&#19979;&#19968;&#33268;&#22320;&#22312;Recall@1&#19978;&#33719;&#24471;&#20102;5.6 ~ 35.0&#21644;16.8 ~ 46.1&#30340;&#25913;&#36827;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19982;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#30456;&#24403;&#30340;&#38646;-shot&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18765v3 Announce Type: replace-cross  Abstract: Visual-language pre-training has achieved remarkable success in many multi-modal tasks, largely attributed to the availability of large-scale image-text datasets. In this work, we demonstrate that Multi-modal Large Language Models (MLLMs) can enhance visual-language representation learning by establishing richer image-text associations for image-text datasets. Our approach is simple, utilizing MLLMs to extend multiple diverse captions for each image. To prevent the bias introduced by MLLMs' hallucinations and monotonous language styles, we propose "text shearing" to maintain the quality and availability of extended captions. In image-text retrieval, without introducing additional training cost, our method consistently obtains 5.6 ~ 35.0 and 16.8 ~ 46.1 improvement on Recall@1 under the fine-tuning and zero-shot settings, respectively. Notably, we obtain zero-shot results that are comparable to fine-tuning on target datasets, wh
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;</title><link>https://arxiv.org/abs/2311.16480</link><description>&lt;p&gt;
&#30149;&#29702;&#25253;&#21578;&#30340;&#22810;&#23454;&#20363;&#29983;&#25104;&#29992;&#20110;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
WsiCaption: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16480
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#20999;&#29255;&#22270;&#20687;&#26159;&#29992;&#20110;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#25968;&#23383;&#30149;&#29702;&#23398;&#30340;&#22522;&#30784;&#12290;&#25776;&#20889;&#30149;&#29702;&#25253;&#21578;&#23545;&#32463;&#39564;&#19981;&#36275;&#30340;&#30149;&#29702;&#23398;&#23478;&#26469;&#35828;&#26159;&#36153;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#12290;&#20026;&#20102;&#20943;&#23569;&#24037;&#20316;&#37327;&#24182;&#25913;&#21892;&#20020;&#24202;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#29983;&#25104;&#32473;&#23450;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;&#22312;&#25968;&#25454;&#31471;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#26368;&#22823;&#30340;WSI-&#25991;&#26412;&#25968;&#25454;&#38598;&#65288;TCGA-PathoText&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#21644;&#28165;&#29702;TCGA&#20013;&#21465;&#36848;&#35786;&#26029;&#24187;&#28783;&#29255;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#25910;&#38598;&#20102;&#36817;1&#19975;&#23545;&#39640;&#36136;&#37327;&#30340;WSI-&#25991;&#26412;&#37197;&#23545;&#65292;&#20379;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#12290;&#22312;&#27169;&#22411;&#31471;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20197;&#20026;&#21315;&#20159;&#20687;&#32032;WSI&#29983;&#25104;&#30149;&#29702;&#25253;&#21578;&#30340;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#65288;MI-Gen&#65289;&#12290;&#25105;&#20204;&#22312;TCGA-PathoText&#30340;&#26368;&#22823;&#23376;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;&#27492;&#22806;&#65292;WSI-&#25991;&#26412;&#39044;&#27979;&#21487;&#34987;&#35270;&#20026;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16480v2 Announce Type: replace-cross  Abstract: Whole slide images are the foundation of digital pathology for the diagnosis and treatment of carcinomas. Writing pathology reports is laborious and error-prone for inexperienced pathologists. To reduce the workload and improve clinical automation, we investigate how to generate pathology reports given whole slide images. On the data end, we curated the largest WSI-text dataset (TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text pairs for visual-language models by recognizing and cleaning pathology reports which narrate diagnostic slides in TCGA. On the model end, we propose the multiple instance generative model (MI-Gen) which can produce pathology reports for gigapixel WSIs. We benchmark our model on the largest subset of TCGA-PathoText. Experimental results show our model can generate pathology reports which contain multiple clinical clues. Furthermore, WSI-text prediction can be seen as an approac
&lt;/p&gt;</description></item><item><title>SUQL&#26159;&#31532;&#19968;&#20010;&#25903;&#25345;&#22823;&#22411;&#30693;&#35782;&#35821;&#26009;&#24211;&#20013;&#30340;&#28151;&#21512;&#25968;&#25454;&#35775;&#38382;&#30340;&#23545;&#35805;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#30001;&#25991;&#26412;&#22522;&#20803;&#25193;&#23637;&#20102;SQL&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;SUQL&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#20197;&#22788;&#29702;&#28151;&#21512;&#25968;&#25454;&#28304;&#30340;&#35821;&#20041;&#35299;&#26512;&#22120;&#12290;</title><link>https://arxiv.org/abs/2311.09818</link><description>&lt;p&gt;
SUQL&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#23545;&#35805;&#24335;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
SUQL: Conversational Search over Structured and Unstructured Data with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09818
&lt;/p&gt;
&lt;p&gt;
SUQL&#26159;&#31532;&#19968;&#20010;&#25903;&#25345;&#22823;&#22411;&#30693;&#35782;&#35821;&#26009;&#24211;&#20013;&#30340;&#28151;&#21512;&#25968;&#25454;&#35775;&#38382;&#30340;&#23545;&#35805;&#20195;&#29702;&#65292;&#36890;&#36807;&#33258;&#30001;&#25991;&#26412;&#22522;&#20803;&#25193;&#23637;&#20102;SQL&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;SUQL&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#20197;&#22788;&#29702;&#28151;&#21512;&#25968;&#25454;&#28304;&#30340;&#35821;&#20041;&#35299;&#26512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22810;&#25968;&#23545;&#35805;&#20195;&#29702;&#37117;&#22522;&#20110;&#33258;&#30001;&#25991;&#26412;&#25110;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20294;&#35768;&#22810;&#30693;&#35782;&#35821;&#26009;&#24211;&#30001;&#28151;&#21512;&#26469;&#28304;&#32452;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#25903;&#25345;&#22823;&#22411;&#30693;&#35782;&#35821;&#26009;&#24211;&#30340;&#28151;&#21512;&#25968;&#25454;&#35775;&#38382;&#30340;&#23545;&#35805;&#20195;&#29702;&#65292;&#36890;&#36807;&#25105;&#20204;&#24320;&#21457;&#30340;&#19968;&#31181;&#21517;&#20026;SUQL&#65288;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#26597;&#35810;&#35821;&#35328;&#65289;&#30340;&#35821;&#35328;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SUQL&#36890;&#36807;&#33258;&#30001;&#25991;&#26412;&#22522;&#20803;&#65288;&#27719;&#24635;&#21644;&#31572;&#26696;&#65289;&#25193;&#23637;&#20102;SQL&#65292;&#22240;&#27492;&#20449;&#24687;&#26816;&#32034;&#21487;&#20197;&#19982;&#32467;&#26500;&#21270;&#25968;&#25454;&#35775;&#38382;&#20219;&#24847;&#32452;&#21512;&#65292;&#37319;&#29992;&#27491;&#24335;&#12289;&#31616;&#27905;&#12289;&#20934;&#30830;&#21644;&#21487;&#35299;&#37322;&#30340;&#34920;&#31034;&#12290;&#36890;&#36807;SUQL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;LLM&#65292;&#21487;&#20197;&#22788;&#29702;&#28151;&#21512;&#25968;&#25454;&#28304;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#24212;&#29992;&#20110;HybridQA&#25968;&#25454;&#38598;&#26102;&#65292;&#19982;SOTA&#23384;&#22312;8.9%&#30340;&#31934;&#30830;&#21305;&#37197;&#21644;7.1%&#30340;F1&#20540;&#30340;&#24046;&#36317;&#65292;&#21518;&#32773;&#22312;62K&#20010;&#25968;&#25454;&#26679;&#26412;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09818v2 Announce Type: replace  Abstract: While most conversational agents are grounded on either free-text or structured knowledge, many knowledge corpora consist of hybrid sources. This paper presents the first conversational agent that supports the full generality of hybrid data access for large knowledge corpora, through a language we developed called SUQL (Structured and Unstructured Query Language). Specifically, SUQL extends SQL with free-text primitives (summary and answer), so information retrieval can be composed with structured data accesses arbitrarily in a formal, succinct, precise, and interpretable notation. With SUQL, we propose the first semantic parser, an LLM with in-context learning, that can handle hybrid data sources.   Our in-context learning-based approach, when applied to the HybridQA dataset, comes within 8.9% exact match and 7.1% F1 of the SOTA, which was trained on 62K data samples. More significantly, unlike previous approaches, our technique is 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#30340;&#24187;&#35273;&#21644;&#19981;&#24544;&#23454;&#25512;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25506;&#27979;&#26041;&#27861;&#21644;&#22522;&#20934;&#27979;&#35797;&#20197;&#30740;&#31350;LLMs&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26159;&#21542;&#20250;&#37319;&#21462;&#27450;&#39575;&#24615;&#35821;&#20041;&#24555;&#25463;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2311.09702</link><description>&lt;p&gt;
&#25512;&#29702;&#38142;&#19978;&#30340;&#27450;&#39575;&#24615;&#35821;&#20041;&#24555;&#25463;&#26041;&#24335;&#65306;&#27169;&#22411;&#22312;&#27809;&#26377;&#24187;&#35273;&#30340;&#24773;&#20917;&#19979;&#33021;&#36208;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#30340;&#24187;&#35273;&#21644;&#19981;&#24544;&#23454;&#25512;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25506;&#27979;&#26041;&#27861;&#21644;&#22522;&#20934;&#27979;&#35797;&#20197;&#30740;&#31350;LLMs&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26159;&#21542;&#20250;&#37319;&#21462;&#27450;&#39575;&#24615;&#35821;&#20041;&#24555;&#25463;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#26399;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#22312;&#20247;&#22810;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#23384;&#22312;&#24187;&#35273;&#21644;&#19981;&#24544;&#23454;&#25512;&#29702;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30001;&#35821;&#20041;&#20851;&#32852;&#24341;&#36215;&#30340;&#24187;&#35273;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;LLMs&#22312;&#25552;&#31034;&#20013;&#26159;&#21542;&#20250;&#22240;&#20026;&#26576;&#20123;&#20851;&#38190;&#23383;/&#23454;&#20307;&#20559;&#35265;&#32780;&#37319;&#21462;&#25463;&#24452;&#65292;&#32780;&#19981;&#26159;&#36981;&#24490;&#27491;&#30830;&#30340;&#25512;&#29702;&#36335;&#24452;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EureQA&#30340;&#26032;&#22411;&#25506;&#27979;&#26041;&#27861;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#20174;LLMs&#20250;&#20197;&#32477;&#23545;&#30830;&#23450;&#24615;&#27491;&#30830;&#22238;&#31572;&#30340;&#38382;&#39064;&#24320;&#22987;&#65292;&#28982;&#21518;&#36882;&#24402;&#22320;&#29992;&#35777;&#25454;&#21477;&#23376;&#36974;&#34109;&#37325;&#35201;&#23454;&#20307;&#65292;&#35201;&#27714;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#25214;&#21040;&#26681;&#25454;&#35777;&#25454;&#38142;&#26465;&#36974;&#34109;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09702v2 Announce Type: replace-cross  Abstract: Despite the recent advancement in large language models (LLMs) and their high performances across numerous benchmarks, recent research has unveiled that LLMs suffer from hallucinations and unfaithful reasoning. This work studies a specific type of hallucination induced by semantic associations. Specifically, we investigate to what extent LLMs take shortcuts from certain keyword/entity biases in the prompt instead of following the correct reasoning path. To quantify this phenomenon, we propose a novel probing method and benchmark called EureQA. We start from questions that LLMs will answer correctly with utmost certainty, and mask the important entity with evidence sentence recursively, asking models to find masked entities according to a chain of evidence before answering the question.   During the construction of the evidence, we purposefully replace semantic clues (entities) that may lead to the correct answer with distractor
&lt;/p&gt;</description></item><item><title>SCD&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#25512;&#27979;&#35299;&#30721;&#21644;&#23545;&#27604;&#35299;&#30721;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#21033;&#29992;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#23454;&#29616;&#20102;&#35299;&#30721;&#21152;&#36895;&#21644;&#36136;&#37327;&#25552;&#21319;</title><link>https://arxiv.org/abs/2311.08981</link><description>&lt;p&gt;
&#25512;&#27979;&#23545;&#27604;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Speculative Contrastive Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08981
&lt;/p&gt;
&lt;p&gt;
SCD&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#25512;&#27979;&#35299;&#30721;&#21644;&#23545;&#27604;&#35299;&#30721;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#21033;&#29992;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#23454;&#29616;&#20102;&#35299;&#30721;&#21152;&#36895;&#21644;&#36136;&#37327;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#39640;&#35745;&#31639;&#38656;&#27714;&#21644;&#26333;&#20809;&#20559;&#24046;&#65292;&#23427;&#20204;&#30340;&#33258;&#22238;&#24402;&#25512;&#29702;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#21463;&#21040;&#25512;&#27979;&#35299;&#30721;&#21644;&#23545;&#27604;&#35299;&#30721;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#25512;&#27979;&#23545;&#27604;&#35299;&#30721;&#65288;SCD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#21033;&#29992;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#26469;&#23454;&#29616;&#35299;&#30721;&#21152;&#36895;&#21644;&#36136;&#37327;&#25552;&#21319;&#12290;&#23545;&#22235;&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;SCD&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#35299;&#30721;&#25928;&#29575;&#21644;&#36136;&#37327;&#21487;&#20197;&#20860;&#24471;&#19968;&#31181;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08981v2 Announce Type: replace  Abstract: Large language models~(LLMs) exhibit exceptional performance in language tasks, yet their auto-regressive inference is limited due to high computational requirements and is sub-optimal due to the exposure bias. Inspired by speculative decoding and contrastive decoding, we introduce Speculative Contrastive Decoding~(SCD), a straightforward yet powerful decoding approach that leverages predictions from smaller language models~(LMs) to achieve both decoding acceleration and quality improvement. Extensive evaluations and analyses on four diverse language tasks demonstrate the effectiveness of SCD, showing that decoding efficiency and quality can compatibly benefit from one smaller LM.
&lt;/p&gt;</description></item><item><title>Agent Lumos&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35268;&#21010;&#27169;&#22359;&#23398;&#20064;&#39640;&#32423;&#23376;&#30446;&#26631;&#29983;&#25104;&#65292;&#35757;&#32451;&#25509;&#22320;&#27169;&#22359;&#23558;&#20854;&#36716;&#21270;&#20026;&#21160;&#20316;&#65292;&#20419;&#36827;&#24191;&#27867;&#20114;&#21160;&#20219;&#21153;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.05657</link><description>&lt;p&gt;
Agent Lumos: &#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#35757;&#32451;&#24320;&#28304;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Agent Lumos: Unified and Modular Training for Open-Source Language Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05657
&lt;/p&gt;
&lt;p&gt;
Agent Lumos&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35268;&#21010;&#27169;&#22359;&#23398;&#20064;&#39640;&#32423;&#23376;&#30446;&#26631;&#29983;&#25104;&#65292;&#35757;&#32451;&#25509;&#22320;&#27169;&#22359;&#23558;&#20854;&#36716;&#21270;&#20026;&#21160;&#20316;&#65292;&#20419;&#36827;&#24191;&#27867;&#20114;&#21160;&#20219;&#21153;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38381;&#28304;&#20195;&#29702;&#23384;&#22312;&#35832;&#22810;&#38382;&#39064;&#65292;&#22914;&#32570;&#20047;&#36127;&#25285;&#24471;&#36215;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#30340;&#20114;&#21160;&#20219;&#21153;&#20013;&#12290;&#36825;&#20419;&#20351;&#20102;&#24320;&#28304;&#26367;&#20195;&#26041;&#26696;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102; LUMOS&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20026;&#35757;&#32451;&#24320;&#28304; LLM-based &#20195;&#29702;&#32780;&#35774;&#35745;&#30340;&#26694;&#26550;&#20043;&#19968;&#12290;LUMOS&#20855;&#26377;&#21487;&#23398;&#20064;&#12289;&#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#30340;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#23398;&#20064;&#39640;&#32423;&#23376;&#30446;&#26631;&#29983;&#25104;&#30340;&#35268;&#21010;&#27169;&#22359;&#65292;&#20197;&#21450;&#19968;&#20010;&#35757;&#32451;&#26377;&#32032;&#30340;&#25509;&#22320;&#27169;&#22359;&#65292;&#29992;&#20110;&#20351;&#29992;&#25191;&#34892;&#27169;&#22359;&#20013;&#30340;&#21508;&#31181;&#24037;&#20855;&#23558;&#36825;&#20123;&#36716;&#21270;&#20026;&#21160;&#20316;&#12290;&#36825;&#31181;&#35774;&#35745;&#20801;&#35768;&#27169;&#22359;&#21270;&#21319;&#32423;&#65292;&#24182;&#26356;&#24191;&#27867;&#22320;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20114;&#21160;&#20219;&#21153;&#12290;&#20026;&#20102;&#20419;&#36827;&#36890;&#29992;&#20195;&#29702;&#23398;&#20064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#28304;&#33258;&#21508;&#31181;&#22797;&#26434;&#20114;&#21160;&#20219;&#21153;&#20013;&#19981;&#21516;&#22320;&#38754;&#30495;&#23454;&#25512;&#29702;&#21407;&#29702;&#30340;&#22823;&#35268;&#27169;&#12289;&#32479;&#19968;&#21644;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#27880;&#37322;&#12290;&#22312;9&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;LUMOS&#34920;&#29616;&#20986;&#20102;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#65288;1&#65289;LUMOS&#22312;&#22810;&#20010;&#36739;&#22823;&#30340;&#24320;&#28304;a
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05657v2 Announce Type: replace  Abstract: Closed-source agents suffer from several issues such as a lack of affordability, transparency, and reproducibility, particularly on complex interactive tasks. This motivates the development of open-source alternatives. We introduce LUMOS, one of the first frameworks for training open-source LLM-based agents. LUMOS features a learnable, unified, and modular architecture with a planning module that learns high-level subgoal generation, and a grounding module trained to translate these into actions using various tools in the execution module. The design allows for modular upgrades and wider applicability to diverse interactive tasks. To foster generalizable agent learning, we collect large-scale, unified, and high-quality training annotations derived from diverse ground-truth reasoning rationales across various complex interactive tasks. On 9 datasets, LUMOS exhibits several key advantages: (1) LUMOS excels multiple larger open-source a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Octavius&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;MoE&#21644;LoRA&#25216;&#26415;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#35299;&#30721;&#22120;LoRA-MoE&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;2D&#21644;3D&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#32422;20%&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.02684</link><description>&lt;p&gt;
Octavius&#65306;&#36890;&#36807;MoE&#20943;&#36731;MLLM&#20013;&#30340;&#20219;&#21153;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Octavius: Mitigating Task Interference in MLLMs via MoE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02684
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Octavius&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;MoE&#21644;LoRA&#25216;&#26415;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#35299;&#30721;&#22120;LoRA-MoE&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;2D&#21644;3D&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#32422;20%&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#23558;&#23427;&#20204;&#30340;&#38646;-shot&#27867;&#21270;&#33021;&#21147;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#23398;&#20064;&#12290;&#38543;&#30528;&#24341;&#20837;&#26356;&#22810;&#30340;&#24418;&#24335;&#21644;&#19979;&#28216;&#20219;&#21153;&#65292;&#36127;&#38754;&#20914;&#31361;&#21644;&#24178;&#25200;&#21487;&#33021;&#23545;&#24615;&#33021;&#20135;&#29983;&#26356;&#20005;&#37325;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#36825;&#31181;&#29616;&#35937;&#22312;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#34987;&#24573;&#35270;&#20102;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;\mname &#30340;&#26032;&#39062;&#19988;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#19982;Multimodal Large Language Models&#65288;MLLMs&#65289;&#19968;&#36215;&#36827;&#34892;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#20840;&#38754;&#30740;&#31350;&#21644;&#23454;&#39564;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#20247;&#25152;&#21608;&#30693;&#30340;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#21644;&#20195;&#34920;&#24615;PEFT&#25216;&#26415;&#20043;&#19968;&#65292;&#21363;LoRA&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#30340;&#35299;&#30721;&#22120;&#65292;&#31216;&#20026;LoRA-MoE&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#65288;&#32422;20\%&#30340;&#25913;&#36827;&#65289;&#34920;&#26126;&#20102;&#25105;&#20204;&#35774;&#35745;&#22312;&#21508;&#31181;2D&#21644;3D&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;&#20195;&#30721;&#21644;&#30456;&#24212;&#25968;&#25454;&#38598;&#23558;&#24456;&#24555;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02684v1 Announce Type: cross  Abstract: Recent studies have demonstrated Large Language Models (LLMs) can extend their zero-shot generalization capabilities to multimodal learning through instruction tuning. As more modalities and downstream tasks are introduced, negative conflicts and interference may have a worse impact on performance. While this phenomenon has been overlooked in previous work, we propose a novel and extensible framework, called \mname, for comprehensive studies and experimentation on multimodal learning with Multimodal Large Language Models (MLLMs). Specifically, we combine the well-known Mixture-of-Experts (MoE) and one of the representative PEFT techniques, \emph{i.e.,} LoRA, designing a novel LLM-based decoder, called LoRA-MoE, for multimodal learning. The experimental results (about 20\% improvement) have shown the effectiveness and versatility of our design in various 2D and 3D downstream tasks. Code and corresponding dataset will be available soon.
&lt;/p&gt;</description></item><item><title>LitCab &#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26657;&#20934;&#26426;&#21046;&#65292;&#36890;&#36807;&#20165;&#28155;&#21152;&#21407;&#22987;&#27169;&#22411;&#21442;&#25968;&#30340; &lt; 2%&#65292;&#20197;&#19968;&#20010;&#21333;&#19968;&#32447;&#24615;&#23618;&#30340;&#24418;&#24335;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2310.19208</link><description>&lt;p&gt;
LitCab: &#22312;&#30701;&#35821;&#21644;&#38271;&#35821;&#35328;&#27169;&#22411;&#24212;&#31572;&#20013;&#30340;&#36731;&#37327;&#32423;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
LitCab: Lightweight Language Model Calibration over Short- and Long-form Responses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19208
&lt;/p&gt;
&lt;p&gt;
LitCab &#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26657;&#20934;&#26426;&#21046;&#65292;&#36890;&#36807;&#20165;&#28155;&#21152;&#21407;&#22987;&#27169;&#22411;&#21442;&#25968;&#30340; &lt; 2%&#65292;&#20197;&#19968;&#20010;&#21333;&#19968;&#32447;&#24615;&#23618;&#30340;&#24418;&#24335;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#27169;&#22411;&#30340;&#27010;&#29575;&#20272;&#35745;&#19982;&#23454;&#38469;&#36755;&#20986;&#27491;&#30830;&#30340;&#21487;&#33021;&#24615;&#19968;&#33268;&#26102;&#65292;&#35813;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#26657;&#20934;&#33391;&#22909;&#30340;&#12290;&#26657;&#20934;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#22312;&#26816;&#27979;&#21644;&#20943;&#36731;LMs&#30340;&#24187;&#35273;&#20197;&#21450;&#26500;&#24314;&#26356;&#21487;&#20449;&#36182;&#30340;&#27169;&#22411;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LitCab&#65292;&#36825;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#26657;&#20934;&#26426;&#21046;&#65292;&#30001;&#19968;&#20010;&#21333;&#19968;&#32447;&#24615;&#23618;&#32452;&#25104;&#65292;&#23427;&#25509;&#21463;&#36755;&#20837;&#25991;&#26412;&#34920;&#31034;&#24182;&#39044;&#27979;&#19968;&#20010;&#20559;&#24046;&#39033;&#65292;&#28982;&#21518;&#23558;&#20854;&#28155;&#21152;&#21040;LM&#36755;&#20986;logits&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.19208v2 Announce Type: replace  Abstract: A model is considered well-calibrated when its probability estimate aligns with the actual likelihood of the output being correct. Calibrating language models (LMs) is crucial, as it plays a vital role in detecting and mitigating hallucinations of LMs as well as building more trustworthy models. However, standard calibration techniques may not be suited for LM calibration. For instance, post-processing methods such as temperature scaling do not reorder the candidate generations. On the other hand, training-based methods require fine-tuning the entire model, which is impractical for LMs of large scale. We present LitCab, a lightweight calibration mechanism consisting of a single linear layer that takes the input text representation and predicts a bias term, which is then added to the LM output logits. LitCab improves model calibration by only adding &lt; 2% of the original model parameters. For evaluation, we construct CaT, a benchmark c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RoPE&#22522;&#22806;&#25512;&#30340;&#23610;&#24230;&#24459;&#65292;&#36890;&#36807;&#35843;&#25972; RoPE &#20013;&#30340;&#22522;&#25968;&#21644;&#24494;&#35843;&#25991;&#26412;&#38271;&#24230;&#26469;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22806;&#25512;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.05209</link><description>&lt;p&gt;
RoPE&#22522;&#22806;&#25512;&#30340;&#23610;&#24230;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws of RoPE-based Extrapolation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RoPE&#22522;&#22806;&#25512;&#30340;&#23610;&#24230;&#24459;&#65292;&#36890;&#36807;&#35843;&#25972; RoPE &#20013;&#30340;&#22522;&#25968;&#21644;&#24494;&#35843;&#25991;&#26412;&#38271;&#24230;&#26469;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22806;&#25512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Rotary Position Embedding&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22806;&#25512;&#33021;&#21147;&#26159;&#30446;&#21069;&#22791;&#21463;&#20851;&#27880;&#30340;&#35805;&#39064;&#12290;&#29992;&#20110;&#35299;&#20915;LLMs&#22806;&#25512;&#38382;&#39064;&#30340;&#20027;&#27969;&#26041;&#27861;&#26159;&#36890;&#36807;&#23558;RoPE&#20013;&#30340;10000, $\theta_n={10000}^{-2n/d}$&#65292;&#36825;&#20010;&#26059;&#36716;&#22522;&#25968;&#65292;&#26367;&#25442;&#20026;&#26356;&#22823;&#30340;&#20540;&#65292;&#24182;&#25552;&#20379;&#26356;&#38271;&#30340;&#24494;&#35843;&#25991;&#26412;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#22312;&#39044;&#35757;&#32451;&#19978;&#29992;&#36739;&#23567;&#25110;&#36739;&#22823;&#30340;&#22522;&#25968;&#24494;&#35843;RoPE-based LLM&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#22806;&#25512;&#24615;&#33021;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RoPE&#22522;&#22806;&#25512;&#30340;&#23610;&#24230;&#24459;&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;&#21608;&#26399;&#24615;&#30340;&#35282;&#24230;&#25551;&#36848;&#22806;&#25512;&#24615;&#33021;&#19982;&#22522;&#20540;&#20197;&#21450;&#35843;&#25972;&#19978;&#19979;&#25991;&#38271;&#24230;&#20043;&#38388;&#20851;&#31995;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;RoPE&#22522;&#22806;&#25512;&#38382;&#39064;&#30340;&#20851;&#38190;&#32500;&#24230;&#20171;&#32461;&#20102;&#20854;&#36215;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05209v2 Announce Type: replace-cross  Abstract: The extrapolation capability of Large Language Models (LLMs) based on Rotary Position Embedding is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with LLMs involves modifying RoPE by replacing 10000, the rotary base of $\theta_n={10000}^{-2n/d}$ in the original RoPE, with a larger value and providing longer fine-tuning text. In this work, we first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance. After that, we propose \textbf{\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by \textbf{\textit{critical dimension for
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30452;&#25509;&#19982;&#23884;&#20837;&#20132;&#20114;&#65292;&#23558;&#25277;&#35937;&#21521;&#37327;&#36716;&#25442;&#20026;&#21487;&#29702;&#35299;&#30340;&#21465;&#36848;&#65292;&#20351;&#24471;&#22797;&#26434;&#23884;&#20837;&#25968;&#25454;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#24191;&#27867;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.04475</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#23494;&#23884;&#20837;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Demystifying Embedding Spaces using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04475
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30452;&#25509;&#19982;&#23884;&#20837;&#20132;&#20114;&#65292;&#23558;&#25277;&#35937;&#21521;&#37327;&#36716;&#25442;&#20026;&#21487;&#29702;&#35299;&#30340;&#21465;&#36848;&#65292;&#20351;&#24471;&#22797;&#26434;&#23884;&#20837;&#25968;&#25454;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#24191;&#27867;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#24050;&#32463;&#25104;&#20026;&#34920;&#31034;&#26377;&#20851;&#23454;&#20307;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#30340;&#22797;&#26434;&#22810;&#26041;&#38754;&#20449;&#24687;&#30340;&#20851;&#38190;&#25163;&#27573;&#65292;&#20197;&#19968;&#31181;&#32039;&#20945;&#19988;&#26377;&#29992;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38590;&#20197;&#30452;&#25509;&#35299;&#37322;&#12290;&#23613;&#31649;&#19979;&#28216;&#20219;&#21153;&#21033;&#29992;&#20102;&#36825;&#20123;&#21387;&#32553;&#34920;&#31034;&#65292;&#20294;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#38477;&#32500;&#25110;&#19987;&#38376;&#30340;&#26426;&#22120;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30452;&#25509;&#19982;&#23884;&#20837;&#20132;&#20114;&#65292;&#23558;&#25277;&#35937;&#21521;&#37327;&#36716;&#25442;&#20026;&#21487;&#29702;&#35299;&#30340;&#21465;&#36848;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20351;&#36825;&#20123;&#23884;&#20837;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#24191;&#27867;&#23454;&#29992;&#24615;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#23884;&#20837;&#27880;&#20837;LLMs&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#23884;&#20837;&#25968;&#25454;&#30340;&#26597;&#35810;&#21644;&#25506;&#32034;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#65306;&#22686;&#24378;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#12289;&#20256;&#36798;&#26032;&#39062;&#30340;&#23884;&#20837;&#23454;&#20307;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.04475v2 Announce Type: replace-cross  Abstract: Embeddings have become a pivotal means to represent complex, multi-faceted information about entities, concepts, and relationships in a condensed and useful format. Nevertheless, they often preclude direct interpretation. While downstream tasks make use of these compressed representations, meaningful interpretation usually requires visualization using dimensionality reduction or specialized machine learning interpretability methods. This paper addresses the challenge of making such embeddings more interpretable and broadly useful, by employing Large Language Models (LLMs) to directly interact with embeddings -- transforming abstract vectors into understandable narratives. By injecting embeddings into LLMs, we enable querying and exploration of complex embedding data. We demonstrate our approach on a variety of diverse tasks, including: enhancing concept activation vectors (CAVs), communicating novel embedded entities, and decod
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;PECoRe&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#19978;&#19979;&#25991;&#20351;&#29992;&#24773;&#20917;&#65292;&#20174;&#32780;&#35780;&#20272;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2310.01188</link><description>&lt;p&gt;
&#37327;&#21270;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#32972;&#26223;&#20381;&#36182;&#24615;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Plausibility of Context Reliance in Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01188
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;PECoRe&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#19978;&#19979;&#25991;&#20351;&#29992;&#24773;&#20917;&#65292;&#20174;&#32780;&#35780;&#20272;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PECoRe&#30340;&#31471;&#21040;&#31471;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#19978;&#19979;&#25991;&#20351;&#29992;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#27169;&#22411;&#20869;&#37096;&#26469;&#23545;&#27604;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#30446;&#26631;&#20196;&#29260;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#35777;&#26126;&#20854;&#39044;&#27979;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#20351;&#29992;PECORE&#26469;&#37327;&#21270;&#20855;&#26377;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#23558;&#27169;&#22411;&#30340;&#29702;&#30001;&#19982;&#20154;&#31867;&#27880;&#37322;&#22312;&#20960;&#20010;&#23618;&#27425;&#30340;&#35805;&#35821;&#27700;&#24179;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01188v2 Announce Type: replace-cross  Abstract: Establishing whether language models can use contextual information in a human-plausible way is important to ensure their trustworthiness in real-world settings. However, the questions of when and which parts of the context affect model generations are typically tackled separately, with current plausibility evaluations being practically limited to a handful of artificial benchmarks. To address this, we introduce Plausibility Evaluation of Context Reliance (PECoRe), an end-to-end interpretability framework designed to quantify context usage in language models' generations. Our approach leverages model internals to (i) contrastively identify context-sensitive target tokens in generated texts and (ii) link them to contextual cues justifying their prediction. We use \pecore to quantify the plausibility of context-aware machine translation models, comparing model rationales with human annotations across several discourse-level pheno
&lt;/p&gt;</description></item><item><title>CRAFT&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#24037;&#20855;&#21019;&#24314;&#21644;&#26816;&#32034;&#26694;&#26550;&#65292;&#33021;&#22815;&#23450;&#21046;LLMs&#65292;&#20026;&#20854;&#21019;&#24314;&#29305;&#23450;&#20219;&#21153;&#30340;&#24037;&#20855;&#38598;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#38598;&#22686;&#24378;&#20854;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2309.17428</link><description>&lt;p&gt;
CRAFT: &#36890;&#36807;&#21019;&#24314;&#21644;&#26816;&#32034;&#19987;&#19994;&#24037;&#20855;&#38598;&#23450;&#21046;LLMs
&lt;/p&gt;
&lt;p&gt;
CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.17428
&lt;/p&gt;
&lt;p&gt;
CRAFT&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#24037;&#20855;&#21019;&#24314;&#21644;&#26816;&#32034;&#26694;&#26550;&#65292;&#33021;&#22815;&#23450;&#21046;LLMs&#65292;&#20026;&#20854;&#21019;&#24314;&#29305;&#23450;&#20219;&#21153;&#30340;&#24037;&#20855;&#38598;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#38598;&#22686;&#24378;&#20854;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#36890;&#36807;&#29983;&#25104;&#20195;&#30721;&#29255;&#27573;&#24182;&#36890;&#36807;&#29305;&#23450;&#20219;&#21153;&#30340;&#24212;&#29992;&#31243;&#24207;&#32534;&#31243;&#25509;&#21475;&#65288;API&#65289;&#25191;&#34892;&#23427;&#20204;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CRAFT&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;LLMs&#21019;&#24314;&#24037;&#20855;&#38598;&#30340;&#36890;&#29992;&#24037;&#20855;&#21019;&#24314;&#21644;&#26816;&#32034;&#26694;&#26550;&#12290;&#23427;&#20026;&#20219;&#21153;&#21019;&#24314;&#20102;&#29305;&#23450;&#30340;&#24037;&#20855;&#38598;&#65292;&#24182;&#20026;LLMs&#37197;&#22791;&#20102;&#19968;&#20010;&#32452;&#20214;&#65292;&#29992;&#20110;&#20174;&#36825;&#20123;&#38598;&#21512;&#20013;&#26816;&#32034;&#24037;&#20855;&#65292;&#20197;&#22686;&#24378;&#23427;&#20204;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.17428v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) are often augmented with tools to solve complex tasks. By generating code snippets and executing them through task-specific Application Programming Interfaces (APIs), they can offload certain functions to dedicated external modules, such as image encoding and performing calculations. However, most existing approaches to augment LLMs with tools are constrained by general-purpose APIs and lack the flexibility for tailoring them to specific tasks. In this work, we present CRAFT, a general tool creation and retrieval framework for LLMs. It creates toolsets specifically curated for the tasks and equips LLMs with a component that retrieves tools from these sets to enhance their capability to solve complex tasks. For each task, we collect specific code solutions by prompting GPT-4 to solve the training examples. Following a validation step ensuring the correctness, these solutions are abstracted into code 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38142;&#24335;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#20840;&#38754;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#35838;&#31243;&#35268;&#21010;&#12289;&#20010;&#24615;&#21270;&#25351;&#23548;&#21644;&#28789;&#27963;&#27979;&#39564;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2309.08112</link><description>&lt;p&gt;
&#36890;&#36807;&#38142;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#31169;&#20154;&#36741;&#23548;
&lt;/p&gt;
&lt;p&gt;
Empowering Private Tutoring by Chaining Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.08112
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38142;&#24335;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#20840;&#38754;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#35838;&#31243;&#35268;&#21010;&#12289;&#20010;&#24615;&#21270;&#25351;&#23548;&#21644;&#28789;&#27963;&#27979;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#34987;&#24212;&#29992;&#20110;&#22312;&#32447;&#25945;&#32946;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#20197;&#20419;&#36827;&#25945;&#23398;&#21644;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#33268;&#21147;&#20110;&#23436;&#25972;&#30340;AI&#36741;&#23548;&#31995;&#32479;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#30001;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#20840;&#38754;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#24320;&#21457;&#65292;&#28085;&#30422;&#33258;&#21160;&#35838;&#31243;&#35268;&#21010;&#21644;&#35843;&#25972;&#12289;&#23450;&#21046;&#25351;&#23548;&#20197;&#21450;&#28789;&#27963;&#30340;&#27979;&#39564;&#35780;&#20272;&#12290;&#20026;&#20102;&#20351;&#31995;&#32479;&#33021;&#22815;&#32463;&#21463;&#20303;&#38271;&#26102;&#38388;&#20132;&#20114;&#24182;&#28385;&#36275;&#20010;&#24615;&#21270;&#25945;&#32946;&#30340;&#38656;&#27714;&#65292;&#31995;&#32479;&#34987;&#20998;&#35299;&#20026;&#19977;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#26680;&#24515;&#27969;&#31243;-&#20132;&#20114;&#12289;&#21453;&#24605;&#21644;&#21453;&#24212;&#12290;&#27599;&#20010;&#27969;&#31243;&#37117;&#36890;&#36807;&#38142;&#25509;LLM&#39537;&#21160;&#30340;&#24037;&#20855;&#20197;&#21450;&#21160;&#24577;&#26356;&#26032;&#30340;&#35760;&#24518;&#27169;&#22359;&#26469;&#23454;&#29616;&#12290;&#24037;&#20855;&#26159;LLMs&#65292;&#34987;&#25552;&#31034;&#25191;&#34892;&#19968;&#39033;&#29305;&#23450;&#20219;&#21153;&#65292;&#32780;&#35760;&#24518;&#26159;&#22312;&#25945;&#32946;&#36807;&#31243;&#20013;&#26356;&#26032;&#30340;&#25968;&#25454;&#23384;&#20648;&#12290;&#23398;&#20064;&#26085;&#24535;&#20013;&#30340;&#32479;&#35745;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.08112v1 Announce Type: cross  Abstract: Artificial intelligence has been applied in various aspects of online education to facilitate teaching and learning. However, few approaches has been made toward a complete AI-powered tutoring system. In this work, we explore the development of a full-fledged intelligent tutoring system powered by state-of-the-art large language models (LLMs), covering automatic course planning and adjusting, tailored instruction, and flexible quiz evaluation. To make the system robust to prolonged interaction and cater to individualized education, the system is decomposed into three inter-connected core processes-interaction, reflection, and reaction. Each process is implemented by chaining LLM-powered tools along with dynamically updated memory modules. Tools are LLMs prompted to execute one specific task at a time, while memories are data storage that gets updated during education process. Statistical results from learning logs demonstrate the effec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#32534;&#30721;&#22120;&#30340;&#26816;&#27979;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#65292;&#22312;NLP&#39046;&#22495;&#20013;&#23637;&#29616;&#20986;&#20102;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2306.08852</link><description>&lt;p&gt;
BED&#65306;&#22522;&#20110;&#21452;&#32534;&#30721;&#22120;&#30340;&#26816;&#27979;&#22120;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
BED: Bi-Encoder-Based Detectors for Out-of-Distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.08852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#32534;&#30721;&#22120;&#30340;&#26816;&#27979;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#65292;&#22312;NLP&#39046;&#22495;&#20013;&#23637;&#29616;&#20986;&#20102;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#21452;&#32534;&#30721;&#22120;&#30340;&#26816;&#27979;&#22120;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#39033;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;NLP&#20013;&#20351;&#29992;&#19981;&#21516;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#19981;&#21516;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#29305;&#24449;&#25552;&#21462;&#38454;&#27573;&#37319;&#29992;&#36890;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#65288;USE&#65289;&#12289;BERT&#12289;MPNET&#21644;GLOVE&#31561;&#27969;&#34892;&#26041;&#27861;&#65292;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#35780;&#20272;&#26159;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#65292;&#21253;&#25324;CLINC150&#12289;ROSTD-Coarse&#12289;SNIPS&#21644;YELLOW&#12290;&#24615;&#33021;&#26159;&#20351;&#29992;F1-Score&#12289;MCC&#12289;FPR@90&#12289;FPR@95&#12289;AUPR&#21644;AUROC&#31561;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;&#21452;&#32534;&#30721;&#22120;&#30340;&#26816;&#27979;&#22120;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#22343;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#37027;&#20123;&#38656;&#35201;&#35757;&#32451;&#20013;&#30340;OOD&#26631;&#31614;&#30340;&#26041;&#27861;&#36824;&#26159;&#19981;&#38656;&#35201;&#30340;&#26041;&#27861;&#65292;&#22312;NLP&#20013;&#26174;&#31034;&#20986;&#20102;&#24456;&#22823;&#30340;&#24322;&#24120;&#26816;&#27979;&#28508;&#21147;&#12290;&#35757;&#32451;&#36807;&#31243;&#30340;&#31616;&#21333;&#24615;&#21644;&#21331;&#36234;&#30340;&#26816;&#27979;&#24615;&#33021;&#20351;&#20854;&#36866;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.08852v2 Announce Type: replace  Abstract: This paper introduces a novel method leveraging bi-encoder-based detectors along with a comprehensive study comparing different out-of-distribution (OOD) detection methods in NLP using different feature extractors. The feature extraction stage employs popular methods such as Universal Sentence Encoder (USE), BERT, MPNET, and GLOVE to extract informative representations from textual data. The evaluation is conducted on several datasets, including CLINC150, ROSTD-Coarse, SNIPS, and YELLOW. Performance is assessed using metrics such as F1-Score, MCC, FPR@90, FPR@95, AUPR, an AUROC. The experimental results demonstrate that the proposed bi-encoder-based detectors outperform other methods, both those that require OOD labels in training and those that do not, across all datasets, showing great potential for OOD detection in NLP. The simplicity of the training process and the superior detection performance make them applicable to real-world
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#36827;&#34892;&#22810;&#39046;&#22495;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#26102;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;&#32780;&#26080;&#38656;&#29305;&#23450;&#27969;&#27700;&#32447;&#25110;&#26631;&#31614;&#65292;&#21516;&#26102;&#20445;&#25345;&#25104;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2305.14336</link><description>&lt;p&gt;
&#26469;&#33258;&#24322;&#26500;&#34920;&#26684;&#30340;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Schema-Driven Information Extraction from Heterogeneous Tables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14336
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#36827;&#34892;&#22810;&#39046;&#22495;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#26102;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;&#32780;&#26080;&#38656;&#29305;&#23450;&#27969;&#27700;&#32447;&#25110;&#26631;&#31614;&#65292;&#21516;&#26102;&#20445;&#25345;&#25104;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25903;&#25345;&#39640;&#25928;&#22320;&#20174;&#34920;&#26684;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25552;&#21462;&#65292;&#36825;&#26159;&#19968;&#39033;&#23558;&#34920;&#26684;&#25968;&#25454;&#36716;&#25442;&#20026;&#25353;&#29031;&#20154;&#31867;&#32534;&#20889;&#30340;&#27169;&#24335;&#32452;&#32455;&#30340;&#35760;&#24405;&#30340;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#35780;&#20272;&#21508;&#31181;LLM&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#21253;&#25324;&#26469;&#33258;&#22235;&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#34920;&#26684;&#65306;&#26426;&#22120;&#23398;&#20064;&#35770;&#25991;&#12289;&#21270;&#23398;&#25991;&#29486;&#12289;&#26448;&#26009;&#31185;&#23398;&#26399;&#21002;&#21644;&#32593;&#39029;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;&#34920;&#26684;&#38598;&#21512;&#26469;&#35780;&#20272;&#24320;&#28304;&#21644;&#22522;&#20110;API&#30340;&#35821;&#35328;&#27169;&#22411;&#20174;&#28085;&#30422;&#22810;&#31181;&#39046;&#22495;&#21644;&#25968;&#25454;&#26684;&#24335;&#30340;&#34920;&#26684;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#19981;&#38656;&#35201;&#20219;&#21153;&#29305;&#23450;&#30340;&#27969;&#27700;&#32447;&#25110;&#26631;&#31614;&#65292;&#20063;&#21487;&#20197;&#23454;&#29616;&#20986;&#20154;&#24847;&#26009;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;F1&#20998;&#25968;&#33539;&#22260;&#20174;74.2&#21040;96.1&#65292;&#21516;&#26102;&#20445;&#25345;&#25104;&#26412;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#35814;&#32454;&#30340;&#28040;&#34701;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.14336v3 Announce Type: replace  Abstract: In this paper, we explore the question of whether large language models can support cost-efficient information extraction from tables. We introduce schema-driven information extraction, a new task that transforms tabular data into structured records following a human-authored schema. To assess various LLM's capabilities on this task, we present a benchmark comprised of tables from four diverse domains: machine learning papers, chemistry literature, material science journals, and webpages. We use this collection of annotated tables to evaluate the ability of open-source and API-based language models to extract information from tables covering diverse domains and data formats. Our experiments demonstrate that surprisingly competitive performance can be achieved without requiring task-specific pipelines or labels, achieving F1 scores ranging from 74.2 to 96.1, while maintaining cost efficiency. Moreover, through detailed ablation studie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21270;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;Reddit&#19978;&#30340;&#24615;&#21035;&#20449;&#24687;&#27880;&#37322;&#35821;&#26009;&#24211;&#35780;&#20272;&#22810;&#20010;&#29983;&#29289;&#21307;&#23398;NER&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#26126;&#26174;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2212.12799</link><description>&lt;p&gt;
&#21270;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study of Gender Bias in Chemical Named Entity Recognition Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.12799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21270;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;Reddit&#19978;&#30340;&#24615;&#21035;&#20449;&#24687;&#27880;&#37322;&#35821;&#26009;&#24211;&#35780;&#20272;&#22810;&#20010;&#29983;&#29289;&#21307;&#23398;NER&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#26126;&#26174;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#34987;&#20351;&#29992;&#65292;&#20174;&#19981;&#33391;&#33647;&#29289;&#21453;&#24212;&#35782;&#21035;&#21040;&#33647;&#29289;&#27969;&#34892;&#30149;&#23398;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#23545;&#25152;&#26377;&#20154;&#37117;&#36215;&#20316;&#29992;&#12290;&#24615;&#33021;&#24046;&#24322;&#21487;&#33021;&#20250;&#36896;&#25104;&#23454;&#38469;&#20260;&#23475;&#32780;&#19981;&#26159;&#39044;&#26399;&#30340;&#30410;&#22788;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#21270;&#23398;NER&#31995;&#32479;&#20013;&#19982;&#24615;&#21035;&#30456;&#20851;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#26469;&#33258;Reddit&#30340;&#36229;&#36807;92,405&#20010;&#35789;&#30340;&#33258;&#25105;&#35782;&#21035;&#24615;&#21035;&#20449;&#24687;&#30340;&#26032;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#26469;&#34913;&#37327;&#21270;&#23398;NER&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#23545;&#22810;&#20010;&#29983;&#29289;&#21307;&#23398;NER&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#26126;&#26174;&#30340;&#20559;&#35265;&#12290;&#20363;&#22914;&#65292;&#21512;&#25104;&#25968;&#25454;&#34920;&#26126;&#22899;&#24615;&#30456;&#20851;&#21517;&#31216;&#32463;&#24120;&#34987;&#35823;&#20998;&#31867;&#20026;&#21270;&#23398;&#21697;&#65292;&#29305;&#21035;&#26159;&#21697;&#29260;&#21517;&#31216;&#30340;&#25552;&#21450;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#30007;&#24615;&#21644;&#22899;&#24615;&#20851;&#32852;&#25968;&#25454;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#35768;&#22810;&#31995;&#32479;&#26410;&#33021;&#26816;&#27979;&#36991;&#23381;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.12799v2 Announce Type: replace  Abstract: Chemical named entity recognition (NER) models are used in many downstream tasks, from adverse drug reaction identification to pharmacoepidemiology. However, it is unknown whether these models work the same for everyone. Performance disparities can potentially cause harm rather than the intended good. This paper assesses gender-related performance disparities in chemical NER systems. We develop a framework for measuring gender bias in chemical NER models using synthetic data and a newly annotated corpus of over 92,405 words with self-identified gender information from Reddit. Our evaluation of multiple biomedical NER models reveals evident biases. For instance, synthetic data suggests female-related names are frequently misclassified as chemicals, especially for brand name mentions. Additionally, we observe performance disparities between female- and male-associated data in both datasets. Many systems fail to detect contraceptives su
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToPro&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#36755;&#20837;&#21477;&#23376;&#20998;&#35299;&#20026;&#21333;&#20010;&#35789;&#27719;&#24182;&#24212;&#29992;&#25552;&#31034;&#27169;&#26495;&#65292;&#22312;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#20854;&#20182;&#24494;&#35843;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32467;&#26500;&#19981;&#21516;&#30340;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.16589</link><description>&lt;p&gt;
ToPro: &#36328;&#35821;&#35328;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#30340;&#22522;&#20110;Token&#32423;&#21035;&#30340;&#25552;&#31034;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
ToPro: Token-Level Prompt Decomposition for Cross-Lingual Sequence Labeling Tasks. (arXiv:2401.16589v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16589
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ToPro&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#36755;&#20837;&#21477;&#23376;&#20998;&#35299;&#20026;&#21333;&#20010;&#35789;&#27719;&#24182;&#24212;&#29992;&#25552;&#31034;&#27169;&#26495;&#65292;&#22312;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#23454;&#29616;&#20102;&#20248;&#20110;&#20854;&#20182;&#24494;&#35843;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32467;&#26500;&#19981;&#21516;&#30340;&#35821;&#35328;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21477;&#23376;&#32423;&#21035;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#21482;&#26377;&#23569;&#25968;&#32771;&#34385;&#21040;&#20102;&#35789;&#27719;&#32423;&#21035;&#30340;&#26631;&#27880;&#20219;&#21153;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#35789;&#24615;&#26631;&#27880;&#65288;POS&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;Token&#32423;&#21035;&#30340;&#25552;&#31034;&#20998;&#35299;&#65288;ToPro&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#35789;&#27719;&#32423;&#21035;&#30340;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#12290;ToPro&#26041;&#27861;&#23558;&#36755;&#20837;&#21477;&#23376;&#20998;&#35299;&#20026;&#21333;&#20010;&#35789;&#27719;&#65292;&#24182;&#23545;&#27599;&#20010;&#35789;&#27719;&#24212;&#29992;&#19968;&#20010;&#25552;&#31034;&#27169;&#26495;&#12290;&#25105;&#20204;&#22312;&#22810;&#35821;&#35328;NER&#21644;POS&#26631;&#27880;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;ToPro&#30340;&#24494;&#35843;&#22312;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#20248;&#20110;Vanilla&#24494;&#35843;&#21644;Prompt-Tuning&#65292;&#23588;&#20854;&#23545;&#20110;&#32467;&#26500;&#19982;&#28304;&#35821;&#35328;&#33521;&#35821;&#19981;&#21516;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;mT5&#27169;&#22411;&#26102;&#20063;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt-based methods have been successfully applied to multilingual pretrained language models for zero-shot cross-lingual understanding. However, most previous studies primarily focused on sentence-level classification tasks, and only a few considered token-level labeling tasks such as Named Entity Recognition (NER) and Part-of-Speech (POS) tagging. In this paper, we propose Token-Level Prompt Decomposition (ToPro), which facilitates the prompt-based method for token-level sequence labeling tasks. The ToPro method decomposes an input sentence into single tokens and applies one prompt template to each token. Our experiments on multilingual NER and POS tagging datasets demonstrate that ToPro-based fine-tuning outperforms Vanilla fine-tuning and Prompt-Tuning in zero-shot cross-lingual transfer, especially for languages that are typologically different from the source language English. Our method also attains state-of-the-art performance when employed with the mT5 model. Besides, our exp
&lt;/p&gt;</description></item><item><title>GPT-4V(ision)&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32593;&#32476;&#20195;&#29702;&#65292;&#20855;&#26377;&#32508;&#21512;&#35270;&#35273;&#29702;&#35299;&#21644;&#32593;&#39029;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22914;&#26524;&#23558;&#25991;&#26412;&#35745;&#21010;&#36716;&#21270;&#20026;&#23454;&#38469;&#34892;&#21160;&#65292;GPT-4V&#21487;&#20197;&#22312;50%&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;&#36825;&#19968;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01614</link><description>&lt;p&gt;
GPT-4V(ision)&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32593;&#32476;&#20195;&#29702;&#65292;&#22914;&#26524;&#26377;&#22522;&#30784;&#30340;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-4V(ision) is a Generalist Web Agent, if Grounded. (arXiv:2401.01614v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01614
&lt;/p&gt;
&lt;p&gt;
GPT-4V(ision)&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32593;&#32476;&#20195;&#29702;&#65292;&#20855;&#26377;&#32508;&#21512;&#35270;&#35273;&#29702;&#35299;&#21644;&#32593;&#39029;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22914;&#26524;&#23558;&#25991;&#26412;&#35745;&#21010;&#36716;&#21270;&#20026;&#23454;&#38469;&#34892;&#21160;&#65292;GPT-4V&#21487;&#20197;&#22312;50%&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;&#36825;&#19968;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;GPT-4V(ision)&#21644;Gemini&#65292;&#24555;&#36895;&#25512;&#21160;&#20102;&#22810;&#27169;&#22411;&#30340;&#33021;&#21147;&#36793;&#30028;&#36229;&#36234;&#20256;&#32479;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20687;GPT-4V&#36825;&#26679;&#30340;LMM&#20316;&#20026;&#36890;&#29992;&#32593;&#32476;&#20195;&#29702;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SEEACT&#65292;&#19968;&#31181;&#21033;&#29992;LMM&#30340;&#21147;&#37327;&#36827;&#34892;&#32508;&#21512;&#35270;&#35273;&#29702;&#35299;&#21644;&#32593;&#39029;&#25805;&#20316;&#30340;&#36890;&#29992;&#32593;&#32476;&#20195;&#29702;&#12290;&#25105;&#20204;&#22312;&#26368;&#26032;&#30340;MIND2WEB&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#38500;&#20102;&#23545;&#32531;&#23384;&#32593;&#31449;&#30340;&#26631;&#20934;&#31163;&#32447;&#35780;&#20272;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20801;&#35768;&#22312;&#23454;&#26102;&#32593;&#31449;&#19978;&#36816;&#34892;&#32593;&#32476;&#20195;&#29702;&#30340;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#35780;&#20272;&#35774;&#32622;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GPT-4V&#22312;&#32593;&#39029;&#20195;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;-&#22914;&#26524;&#25105;&#20204;&#23558;&#20854;&#25991;&#26412;&#35745;&#21010;&#25163;&#21160;&#22320;&#23454;&#26045;&#20026;&#32593;&#31449;&#19978;&#30340;&#34892;&#21160;&#65292;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#23436;&#25104;50%&#30340;&#20219;&#21153;&#12290;&#27492;&#32467;&#26524;&#26126;&#26174;&#36229;&#36807;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering. In this work, we explore the potential of LMMs like GPT-4V as a generalist web agent that can follow natural language instructions to complete tasks on any given website. We propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual understanding and acting on the web. We evaluate on the recent MIND2WEB benchmark. In addition to standard offline evaluation on cached websites, we enable a new online evaluation setting by developing a tool that allows running web agents on live websites. We show that GPT-4V presents a great potential for web agents - it can successfully complete 50% of the tasks on live websites if we manually ground its textual plans into actions on the websites. This substantially outperforms
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#23454;&#29616;&#23545;&#38544;&#31169;&#25915;&#20987;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24674;&#22797;&#27744;&#21270;&#23618;&#36755;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#19979;&#25552;&#20379;&#26356;&#39640;&#30340;&#25991;&#26412;&#24674;&#22797;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2312.05720</link><description>&lt;p&gt;
&#36229;&#36234;&#26799;&#24230;&#21644;&#20808;&#39564;&#30693;&#35782;&#22312;&#38544;&#31169;&#25915;&#20987;&#20013;&#65306;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#20013;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning. (arXiv:2312.05720v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#23454;&#29616;&#23545;&#38544;&#31169;&#25915;&#20987;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24674;&#22797;&#27744;&#21270;&#23618;&#36755;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#19979;&#25552;&#20379;&#26356;&#39640;&#30340;&#25991;&#26412;&#24674;&#22797;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24378;&#35843;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#36890;&#36807;&#26412;&#22320;&#23384;&#20648;&#25968;&#25454;&#24182;&#20165;&#21457;&#36865;&#27169;&#22411;&#26356;&#26032;&#65292;&#24378;&#35843;&#29992;&#25143;&#38544;&#31169;&#12290;&#26368;&#36817;&#65292;&#19968;&#31995;&#21015;&#26377;&#20851;&#38544;&#31169;&#25915;&#20987;&#30340;&#24037;&#20316;&#36890;&#36807;&#20174;&#32852;&#37030;&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#25935;&#24863;&#30340;&#35757;&#32451;&#25991;&#26412;&#26469;&#25439;&#23475;&#29992;&#25143;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25915;&#20987;&#25216;&#26415;&#38754;&#20020;&#30528;&#19981;&#21516;&#30340;&#38556;&#30861;&#65306;&#19968;&#20123;&#24037;&#20316;&#20027;&#35201;&#20351;&#29992;&#26377;&#38480;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#65288;&#20363;&#22914;&#65292;&#25209;&#22788;&#29702;&#22823;&#23567;&#20026;1&#65289;&#65292;&#32780;&#20854;&#20182;&#25216;&#26415;&#21017;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#38590;&#20197;&#26816;&#27979;&#30340;&#29305;&#28857;&#65292;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#35774;&#32622;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#25991;&#26412;&#24674;&#22797;&#29575;&#12290;&#22522;&#20110;&#22522;&#26412;&#30340;&#26799;&#24230;&#21305;&#37197;&#21644;&#39046;&#22495;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#36890;&#36807;&#24674;&#22797;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#22686;&#24378;&#25915;&#20987;&#33021;&#21147;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29305;&#24449;&#32423;&#21035;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#19982;&#26799;&#24230;&#25968;&#25454;&#19981;&#21516;&#65292;&#36825;&#20123;&#20449;&#21495;&#19981;&#20250;&#22312;&#21477;&#23376;&#21644;&#26631;&#35760;&#20043;&#38388;&#36827;&#34892;&#24179;&#22343;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) emphasizes decentralized training by storing data locally and sending only model updates, underlining user privacy. Recently, a line of works on privacy attacks impairs user privacy by extracting sensitive training text from language models in the context of FL. Yet, these attack techniques face distinct hurdles: some work chiefly with limited batch sizes (e.g., batch size of 1), and others are easily detectable. This paper introduces an innovative approach that is challenging to detect, significantly enhancing the recovery rate of text in various batch-size settings. Building on fundamental gradient matching and domain prior knowledge, we enhance the attack by recovering the input of the Pooler layer of language models, which enables us to provide additional supervised signals at the feature level. Unlike gradient data, these signals do not average across sentences and tokens, thereby offering more nuanced and effective insights. We benchmark our method using t
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07793</link><description>&lt;p&gt;
GenTKG: &#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07793
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;(tKG)&#39046;&#22495;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#20256;&#32479;&#30340;&#22522;&#20110;&#23884;&#20837;&#21644;&#35268;&#21017;&#30340;&#27169;&#22411;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;LLM&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#32467;&#26500;&#21270;&#30340;&#26102;&#38388;&#20851;&#31995;&#25968;&#25454;&#65292;&#24182;&#21462;&#20195;&#23427;&#20204;&#25104;&#20026;&#26102;&#38388;&#20851;&#31995;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#30693;&#35782;&#39044;&#27979;&#24341;&#20837;&#29983;&#25104;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;LLM&#21487;&#20197;&#22788;&#29702;&#30340;&#24207;&#21015;&#33258;&#28982;&#34920;&#36798;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#40511;&#27807;&#65292;&#22312;tKG&#30340;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#24494;&#35843;LLM&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#20063;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;GenTKG&#65292;&#23427;&#22312;tKG&#19978;&#25191;&#34892;&#29983;&#25104;&#24335;&#39044;&#27979;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;GenTKG&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
&lt;/p&gt;</description></item><item><title>PROGrasp&#26159;&#19968;&#20010;&#23454;&#29616;&#29289;&#20307;&#25235;&#21462;&#30340;&#20154;&#26426;&#20132;&#27969;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#38754;&#21521;&#24847;&#22270;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#21644;&#31572;&#26696;&#35299;&#37322;&#27169;&#22359;&#65292;&#26426;&#22120;&#20154;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#24847;&#22270;&#26469;&#35782;&#21035;&#21644;&#25235;&#21462;&#30446;&#26631;&#29289;&#20307;&#12290;</title><link>http://arxiv.org/abs/2309.07759</link><description>&lt;p&gt;
PROGrasp:&#23454;&#29616;&#29289;&#20307;&#25235;&#21462;&#30340;&#20154;&#26426;&#20132;&#27969;
&lt;/p&gt;
&lt;p&gt;
PROGrasp: Pragmatic Human-Robot Communication for Object Grasping. (arXiv:2309.07759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07759
&lt;/p&gt;
&lt;p&gt;
PROGrasp&#26159;&#19968;&#20010;&#23454;&#29616;&#29289;&#20307;&#25235;&#21462;&#30340;&#20154;&#26426;&#20132;&#27969;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#38754;&#21521;&#24847;&#22270;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;&#21644;&#31572;&#26696;&#35299;&#37322;&#27169;&#22359;&#65292;&#26426;&#22120;&#20154;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#24847;&#22270;&#26469;&#35782;&#21035;&#21644;&#25235;&#21462;&#30446;&#26631;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#29289;&#20307;&#25235;&#21462;(IOG)&#26159;&#36890;&#36807;&#20154;&#26426;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#35782;&#21035;&#21644;&#25235;&#21462;&#30446;&#26631;&#29289;&#20307;&#30340;&#20219;&#21153;&#12290;&#24403;&#21069;IOG&#31995;&#32479;&#20551;&#23450;&#20154;&#31867;&#29992;&#25143;&#26368;&#21021;&#25351;&#23450;&#30446;&#26631;&#23545;&#35937;&#30340;&#31867;&#21035;(&#20363;&#22914;&#65292;&#29942;&#23376;)&#12290;&#21463;&#21040;&#35821;&#29992;&#23398;&#30340;&#21551;&#21457;&#65292;&#20154;&#31867;&#24448;&#24448;&#36890;&#36807;&#20381;&#36182;&#19978;&#19979;&#25991;&#26469;&#20256;&#36798;&#24847;&#22270;&#20197;&#23454;&#29616;&#30446;&#26631;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;IOG&#20219;&#21153;&#65292;&#21363;&#23454;&#29992;IOG&#65292;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;&#38754;&#21521;&#24847;&#22270;&#30340;&#22810;&#27169;&#24577;&#23545;&#35805;(IM-Dial)&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#20219;&#21153;&#22330;&#26223;&#20013;&#65292;&#39318;&#20808;&#32473;&#20986;&#19968;&#20010;&#38754;&#21521;&#24847;&#22270;&#30340;&#35805;&#35821;(&#20363;&#22914;&#65292;&#8220;&#25105;&#28212;&#20102;&#8221;)&#12290;&#28982;&#21518;&#65292;&#26426;&#22120;&#20154;&#24212;&#36890;&#36807;&#19982;&#20154;&#31867;&#29992;&#25143;&#20114;&#21160;&#26469;&#35782;&#21035;&#30446;&#26631;&#23545;&#35937;&#12290;&#22522;&#20110;&#20219;&#21153;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#37322;&#29992;&#25143;&#30340;&#24847;&#22270;&#24182;&#25441;&#36215;&#30446;&#26631;&#23545;&#35937;&#65292;&#21363;&#23454;&#29992;&#29289;&#20307;&#25235;&#21462;(PROGrasp)&#12290;PROGrasp&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#23450;&#20301;&#12289;&#38382;&#39064;&#25552;&#38382;&#12289;&#29289;&#20307;&#25235;&#21462;&#20197;&#21450;&#26368;&#37325;&#35201;&#30340;&#65292;&#31572;&#26696;&#35299;&#37322;&#27169;&#22359;&#25191;&#34892;&#23454;&#29992;IOG&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive Object Grasping (IOG) is the task of identifying and grasping the desired object via human-robot natural language interaction. Current IOG systems assume that a human user initially specifies the target object's category (e.g., bottle). Inspired by pragmatics, where humans often convey their intentions by relying on context to achieve goals, we introduce a new IOG task, Pragmatic-IOG, and the corresponding dataset, Intention-oriented Multi-modal Dialogue (IM-Dial). In our proposed task scenario, an intention-oriented utterance (e.g., "I am thirsty") is initially given to the robot. The robot should then identify the target object by interacting with a human user. Based on the task setup, we propose a new robotic system that can interpret the user's intention and pick up the target object, Pragmatic Object Grasping (PROGrasp). PROGrasp performs Pragmatic-IOG by incorporating modules for visual grounding, question asking, object grasping, and most importantly, answer interpre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#39640;&#23545;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#36827;&#34892;&#26816;&#27979;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21306;&#20998;&#24320;&#26469;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#26041;&#38754;&#19982;&#30495;&#23454;&#31185;&#23398;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#31639;&#27861;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.11767</link><description>&lt;p&gt;
&#25552;&#39640;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#26816;&#27979;&#30340;&#26041;&#27861;&#65306;&#24341;&#20837;xFakeBibs&#30417;&#30563;&#23398;&#20064;&#32593;&#32476;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Improving Detection of ChatGPT-Generated Fake Science Using Real Publication Text: Introducing xFakeBibs a Supervised-Learning Network Algorithm. (arXiv:2308.11767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#25552;&#39640;&#23545;ChatGPT&#29983;&#25104;&#30340;&#20551;&#31185;&#23398;&#36827;&#34892;&#26816;&#27979;&#30340;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20934;&#30830;&#22320;&#23558;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21306;&#20998;&#24320;&#26469;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#26041;&#38754;&#19982;&#30495;&#23454;&#31185;&#23398;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#31639;&#27861;&#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#27491;&#22312;&#25104;&#20026;&#29616;&#23454;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21306;&#20998;ChatGPT&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#19982;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#35774;&#35745;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#21644;&#31185;&#23398;&#23478;&#29983;&#25104;&#30340;&#20986;&#29256;&#29289;&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;100&#20010;&#30495;&#23454;&#20986;&#29256;&#29289;&#25688;&#35201;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#37319;&#29992;10&#20493;&#20132;&#21449;&#39564;&#35777;&#26041;&#27861;&#24314;&#31435;&#20102;&#19968;&#20010;&#25509;&#21463;&#33539;&#22260;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;&#19982;ChatGPT&#20869;&#23481;&#36827;&#34892;&#27604;&#36739;&#65292;&#26126;&#26174;&#21487;&#35265;ChatGPT&#20165;&#36129;&#29486;&#20102;23\%&#30340;&#20108;&#20803;&#32452;&#20869;&#23481;&#65292;&#36825;&#27604;&#20854;&#20182;10&#20010;&#20132;&#21449;&#39564;&#35777;&#20013;&#30340;&#20219;&#20309;&#19968;&#20010;&#37117;&#23569;50\%&#12290;&#36825;&#20010;&#20998;&#26512;&#20984;&#26174;&#20102;ChatGPT&#22312;&#25216;&#26415;&#26415;&#35821;&#19978;&#19982;&#30495;&#23454;&#31185;&#23398;&#30340;&#26126;&#26174;&#24046;&#24322;&#12290;&#22312;&#23545;&#27599;&#31687;&#25991;&#31456;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;xFakeBibs&#31639;&#27861;&#20934;&#30830;&#22320;&#23558;98&#31687;&#20986;&#29256;&#29289;&#35782;&#21035;&#20026;&#20551;&#30340;&#65292;&#26377;2&#31687;&#25991;&#29486;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;&#30495;&#23454;&#20986;&#29256;&#29289;&#12290;&#23613;&#31649;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#31639;&#27861;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is becoming a new reality. In this paper, we show how to distinguish ChatGPT-generated publications from counterparts produced by scientists. Using a newly designed supervised Machine Learning algorithm, we demonstrate how to detect machine-generated publications from those produced by scientists. The algorithm was trained using 100 real publication abstracts, followed by a 10-fold calibration approach to establish a lower-upper bound range of acceptance. In the comparison with ChatGPT content, it was evident that ChatGPT contributed merely 23\% of the bigram content, which is less than 50\% of any of the other 10 calibrating folds. This analysis highlights a significant disparity in technical terms where ChatGPT fell short of matching real science. When categorizing the individual articles, the xFakeBibs algorithm accurately identified 98 out of 100 publications as fake, with 2 articles incorrectly classified as real publications. Though this work introduced an algorithmic app
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#21547;&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#20064;&#33021;&#21147;&#20351;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12289;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#32858;&#21512;&#26631;&#31614;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#21450;&#20854;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.12375</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22312;&#23398;&#20064;&#26631;&#31614;&#20851;&#31995;&#19978;&#20855;&#26377;&#21019;&#26032;&#65292;&#20294;&#24182;&#38750;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning. (arXiv:2307.12375v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12375
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#21547;&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#20064;&#33021;&#21147;&#20351;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12289;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#32858;&#21512;&#26631;&#31614;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#21450;&#20854;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#22312;&#21253;&#21547;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#36890;&#24120;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;LLMs&#30340;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26080;&#20849;&#35782;&#65306;&#20363;&#22914;&#65292;&#34429;&#28982;Xie&#31561;&#20154;&#65288;2021&#24180;&#65289;&#23558;ICL&#27604;&#20316;&#19968;&#31181;&#36890;&#29992;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;Min&#31561;&#20154;&#65288;2022b&#24180;&#65289;&#35748;&#20026;ICL&#29978;&#33267;&#19981;&#33021;&#20174;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#23398;&#20064;&#26631;&#31614;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#19979;&#19977;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#32467;&#26524;&#65292;&#65288;2&#65289;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#30340;&#36755;&#20837;-&#26631;&#31614;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#65288;3&#65289;ICL&#22914;&#20309;&#32858;&#21512;&#26469;&#33258;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26631;&#31614;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#36890;&#24120;&#20250;&#25972;&#21512;&#19978;&#19979;&#25991;&#26631;&#31614;&#30340;&#20449;&#24687;&#65292;&#20294;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#26631;&#31614;&#20851;&#31995;&#34987;&#21306;&#21035;&#23545;&#24453;&#65292;&#27169;&#22411;&#19981;&#20250;&#23558;&#25152;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#21516;&#23545;&#24453;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#23545;LLMs&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into underst
&lt;/p&gt;</description></item><item><title>UniTabE&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#34920;&#26684;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#34920;&#26684;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#24182;&#20855;&#26377;&#23545;&#22810;&#26679;&#21270;&#19979;&#28216;&#24212;&#29992;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09249</link><description>&lt;p&gt;
UniTabE: &#38754;&#21521;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#34920;&#26684;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data. (arXiv:2307.09249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09249
&lt;/p&gt;
&lt;p&gt;
UniTabE&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#34920;&#26684;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#34920;&#26684;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#24182;&#20855;&#26377;&#23545;&#22810;&#26679;&#21270;&#19979;&#28216;&#24212;&#29992;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#26126;&#35777;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31361;&#30772;&#24615;&#24433;&#21709;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#23041;&#21147;&#25193;&#23637;&#21040;&#20256;&#32479;&#34987;&#24573;&#35270;&#30340;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#65292;&#35813;&#39046;&#22495;&#30001;&#20110;&#19981;&#21516;&#20219;&#21153;&#22266;&#26377;&#30340;&#20247;&#22810;&#34920;&#26684;&#27169;&#24335;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#24037;&#20316;&#30340;&#20027;&#35201;&#30740;&#31350;&#38382;&#39064;&#22260;&#32469;&#24322;&#26500;&#34920;&#26684;&#32467;&#26500;&#30340;&#36866;&#24212;&#24615;&#12289;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#21327;&#35758;&#30340;&#24314;&#31435;&#12289;&#23398;&#21040;&#30340;&#30693;&#35782;&#22312;&#20219;&#21153;&#20043;&#38388;&#30340;&#27867;&#21270;&#21644;&#21487;&#20256;&#36882;&#24615;&#12289;&#23545;&#22810;&#26679;&#21270;&#19979;&#28216;&#24212;&#29992;&#30340;&#36866;&#24212;&#24615;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#22686;&#37327;&#21015;&#30340;&#32435;&#20837;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UniTabE&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20197;&#19968;&#33268;&#30340;&#26041;&#24335;&#22788;&#29702;&#34920;&#26684;&#65292;&#25670;&#33073;&#20102;&#29305;&#23450;&#34920;&#26684;&#32467;&#26500;&#24378;&#21152;&#30340;&#32422;&#26463;&#12290;UniTabE&#30340;&#26680;&#24515;&#27010;&#24565;&#26159;&#23545;&#27599;&#20010;&#22522;&#26412;&#34920;&#26684;&#36827;&#34892;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks. This study seeks to extend the power of pretraining methodologies to tabular data, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks. The primary research questions underpinning this work revolve around the adaptation to heterogeneous table structures, the establishment of a universal pretraining protocol for tabular data, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time. In response to these challenges, we introduce UniTabE, a pioneering method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures. UniTabE's core concept relies on representing each basic tab
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#26426;&#21046;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#32780;&#30693;&#35782;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#23545;&#20110;&#35760;&#24518;&#24418;&#25104;&#20063;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.09144</link><description>&lt;p&gt;
&#35760;&#24518;&#36824;&#26159;&#24536;&#21364;&#65311;&#28145;&#20837;&#25506;&#35752;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#35760;&#24518;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models. (arXiv:2305.09144v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#26426;&#21046;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#32780;&#30693;&#35782;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#23545;&#20110;&#35760;&#24518;&#24418;&#25104;&#20063;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#26159;&#26368;&#22522;&#26412;&#30340;&#35748;&#30693;&#21151;&#33021;&#20043;&#19968;&#65292;&#26159;&#23384;&#20648;&#19990;&#30028;&#30693;&#35782;&#21644;&#27963;&#21160;&#32463;&#21382;&#30340;&#20648;&#34255;&#24211;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;&#30456;&#21453;&#65292;&#27809;&#26377;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#20445;&#25345;-&#36951;&#24536;&#30340;&#30683;&#30462;&#24182;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#26426;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#30446;&#26631;&#30693;&#35782;&#31867;&#22411;&#12289;&#23398;&#20064;&#31574;&#30053;&#21644;&#23398;&#20064;&#26102;&#38388;&#34920;&#31561;&#65292;&#24320;&#23637;&#20102;&#28145;&#20837;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#32467;&#26524;&#21457;&#29616;&#65306;1&#65289;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#26159;&#23481;&#26131;&#36951;&#24536;&#30340;&#65307;2&#65289;&#39044;&#35757;&#32451;&#21487;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#35760;&#24518;&#33021;&#21147;&#65307;3&#65289;&#30693;&#35782;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#26174;&#33879;&#24433;&#21709;&#35760;&#24518;&#24418;&#25104;&#12290;&#36825;&#20123;&#32467;&#35770;&#26377;&#21161;&#20110;&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#20026;&#35774;&#35745;&#21644;&#35780;&#20272;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#26041;&#27861;&#21644;&#25512;&#29702;&#31639;&#27861;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory is one of the most essential cognitive functions serving as a repository of world knowledge and episodes of activities. In recent years, large-scale pre-trained language models have shown remarkable memorizing ability. On the contrary, vanilla neural networks without pre-training have been long observed suffering from the catastrophic forgetting problem. To investigate such a retentive-forgetful contradiction and understand the memory mechanism of language models, we conduct thorough experiments by controlling the target knowledge types, the learning strategies and the learning schedules. We find that: 1) Vanilla language models are forgetful; 2) Pre-training leads to retentive language models; 3) Knowledge relevance and diversification significantly influence the memory formation. These conclusions are useful for understanding the abilities of pre-trained language models and shed light on designing and evaluating new learning and inference algorithms of language models.
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#31934;&#35843;&#21487;&#20197;&#25552;&#39640;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20294;&#31934;&#35843;&#36890;&#24120;&#20250;&#20351;&#27169;&#22411;&#36807;&#24230;&#19987;&#38376;&#21270;&#65292;&#38477;&#20302;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#27867;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#31934;&#35843;&#26694;&#26550;ProMoT&#21487;&#20197;&#20943;&#23569;&#36825;&#31181;&#26684;&#24335;&#29305;&#21270;&#12290;</title><link>http://arxiv.org/abs/2211.00635</link><description>&lt;p&gt;
&#20004;&#38454;&#27573;LLM&#31934;&#35843;&#26041;&#27861;&#65306;&#26356;&#23569;&#29305;&#21270;&#12289;&#26356;&#22810;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Two-stage LLM Fine-tuning with Less Specialization and More Generalization. (arXiv:2211.00635v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00635
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#31934;&#35843;&#21487;&#20197;&#25552;&#39640;&#29305;&#23450;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20294;&#31934;&#35843;&#36890;&#24120;&#20250;&#20351;&#27169;&#22411;&#36807;&#24230;&#19987;&#38376;&#21270;&#65292;&#38477;&#20302;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#27867;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#31934;&#35843;&#26694;&#26550;ProMoT&#21487;&#20197;&#20943;&#23569;&#36825;&#31181;&#26684;&#24335;&#29305;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#21644;&#25552;&#31034;&#30340;&#36890;&#29992;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#22312;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#31934;&#35843;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#20854;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#31934;&#35843;&#36890;&#24120;&#20351;&#27169;&#22411;&#22312;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36807;&#20110;&#19987;&#38376;&#21270;&#65292;&#24182;&#38477;&#20302;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#27867;&#21270;&#23398;&#20064;&#24615;&#33021;&#65292;&#36825;&#22312;&#38656;&#35201;&#22788;&#29702;&#27809;&#26377;&#31934;&#35843;&#25968;&#25454;&#30340;&#20854;&#20182;&#20219;&#21153;&#26102;&#26159;&#19981;&#21487;&#21462;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#21333;&#20219;&#21153;&#31934;&#35843;&#30830;&#23454;&#20250;&#38477;&#20302;LLM&#30340;&#27867;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#36951;&#24536;&#30340;&#19968;&#20010;&#37325;&#35201;&#21407;&#22240;&#26159;&#26684;&#24335;&#29305;&#21270;&#65292;&#21363;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#20110;&#31934;&#35843;&#20219;&#21153;&#30340;&#26684;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#26684;&#24335;&#29305;&#21270;&#21457;&#29983;&#22312;&#31934;&#35843;&#30340;&#26089;&#26399;&#38454;&#27573;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Prompt Tuning with MOdel Tuning (ProMoT)&#36825;&#19968;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#31934;&#35843;&#26694;&#26550;&#65292;&#21487;&#20197;&#20943;&#23569;&#26684;&#24335;&#29305;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained large language models (LLMs) are general purpose problem solvers applicable to a diverse set of tasks with prompts. They can be further improved towards a specific task by fine-tuning on a specialized dataset. However, fine-tuning usually makes the model narrowly specialized on this dataset with reduced general in-context learning performances, which is undesirable whenever the fine-tuned model needs to handle additional tasks where no fine-tuning data is available. In this work, we first demonstrate that fine-tuning on a single task indeed decreases LLMs' general in-context learning performance. We discover one important cause of such forgetting, format specialization, where the model overfits to the format of the fine-tuned task. We further show that format specialization happens at the very beginning of fine-tuning. To solve this problem, we propose Prompt Tuning with MOdel Tuning (ProMoT), a simple yet effective two-stage fine-tuning framework that reduces format special
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#24212;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2203.11155</link><description>&lt;p&gt;
&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#22312;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification. (arXiv:2203.11155v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11155
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#24212;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#21487;&#34920;&#31034;&#25972;&#20010;&#37327;&#23376;&#31995;&#32479;&#30340;&#20840;&#37096;&#20449;&#24687;&#65292;&#23558;&#23494;&#24230;&#30697;&#38453;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#20219;&#21153;&#21487;&#20197;&#26356;&#21152;&#26377;&#25928;&#22320;&#23454;&#29616;&#38382;&#39064;&#22238;&#31572;&#12290;&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#26032;&#26426;&#21046;&#65292;&#20197;&#24212;&#23545;&#36755;&#20837;&#20026;&#30697;&#38453;&#30340;&#24773;&#20917;&#65292;&#24182;&#23558;&#35813;&#26426;&#21046;&#24212;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;QA&#38382;&#39064;&#30340;&#27714;&#35299;&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#21487;&#20197;&#22686;&#24378;&#32463;&#20856;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#29305;&#24449;&#20449;&#24687;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26032;&#26694;&#26550;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;CNN&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum density matrix represents all the information of the entire quantum system, and novel models of meaning employing density matrices naturally model linguistic phenomena such as hyponymy and linguistic ambiguity, among others in quantum question answering tasks. Naturally, we argue that applying the quantum density matrix into classical Question Answering (QA) tasks can show more effective performance. Specifically, we (i) design a new mechanism based on Long Short-Term Memory (LSTM) to accommodate the case when the inputs are matrixes; (ii) apply the new mechanism to QA problems with Convolutional Neural Network (CNN) and gain the LSTM-based QA model with the quantum density matrix. Experiments of our new model on TREC-QA and WIKI-QA data sets show encouraging results. Similarly, we argue that the quantum density matrix can also enhance the image feature information and the relationship between the features for the classical image classification. Thus, we (i) combine density mat
&lt;/p&gt;</description></item></channel></rss>