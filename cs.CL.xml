<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#36890;&#36807;&#21338;&#24328;&#35770;&#35270;&#35282;&#35780;&#20272;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#22312;&#31283;&#20581;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21017;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.11807</link><description>&lt;p&gt;
LLM&#30340;&#20915;&#31574;&#27700;&#24179;&#22312;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#31350;&#31455;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11807
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21338;&#24328;&#35770;&#35270;&#35282;&#35780;&#20272;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#22312;&#31283;&#20581;&#24615;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;GPT-4&#21017;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#21508;&#31181;&#33021;&#21147;&#65292;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26497;&#22909;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#21338;&#24328;&#35770;&#30340;&#35270;&#35282;&#25506;&#31350;LLMs&#30340;&#20915;&#31574;&#33021;&#21147;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#25903;&#25345;&#22810;&#20010;&#26234;&#33021;&#20307;&#21516;&#26102;&#21442;&#19982;&#30340;&#28216;&#25103;&#65292;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;GAMA-Bench&#65292;&#21253;&#25324;&#20843;&#20010;&#32463;&#20856;&#30340;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#35780;&#20998;&#26041;&#26696;&#65292;&#23450;&#37327;&#35780;&#20272;&#27169;&#22411;&#22312;&#36825;&#20123;&#28216;&#25103;&#20013;&#30340;&#34920;&#29616;&#12290;&#36890;&#36807;GAMA-Bench&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#30340;&#31283;&#20581;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#22686;&#24378;&#31574;&#30053;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;GPT-3.5&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#20854;&#27867;&#21270;&#33021;&#21147;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#19968;&#20123;&#26041;&#27861;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#65292;&#20854;&#24615;&#33021;&#21487;&#20197;&#24471;&#21040;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;GPT-4&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11807v1 Announce Type: new  Abstract: Decision-making, a complicated task requiring various types of abilities, presents an excellent framework for assessing Large Language Models (LLMs). Our research investigates LLMs' decision-making capabilities through the lens of a well-established field, Game Theory. We focus specifically on games that support the participation of more than two agents simultaneously. Subsequently, we introduce our framework, GAMA-Bench, including eight classical multi-agent games. We design a scoring scheme to assess a model's performance in these games quantitatively. Through GAMA-Bench, we investigate LLMs' robustness, generalizability, and enhancement strategies. Results reveal that while GPT-3.5 shows satisfying robustness, its generalizability is relatively limited. However, its performance can be improved through approaches such as Chain-of-Thought. Additionally, we conduct evaluations across various LLMs and find that GPT-4 outperforms other mod
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ChartThinker&#30340;&#21019;&#26032;&#22270;&#34920;&#25688;&#35201;&#26041;&#27861;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#24605;&#32500;&#21644;&#31574;&#30053;&#24615;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#23454;&#29616;&#28145;&#24230;&#20998;&#26512;&#65292;&#26088;&#22312;&#25552;&#39640;&#29983;&#25104;&#25688;&#35201;&#30340;&#36923;&#36753;&#36830;&#36143;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.11236</link><description>&lt;p&gt;
ChartThinker&#65306;&#19968;&#31181;&#20248;&#21270;&#22270;&#34920;&#25688;&#35201;&#30340;&#19978;&#19979;&#25991;&#24605;&#32500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ChartThinker: A Contextual Chain-of-Thought Approach to Optimized Chart Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11236
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ChartThinker&#30340;&#21019;&#26032;&#22270;&#34920;&#25688;&#35201;&#26041;&#27861;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#24605;&#32500;&#21644;&#31574;&#30053;&#24615;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#23454;&#29616;&#28145;&#24230;&#20998;&#26512;&#65292;&#26088;&#22312;&#25552;&#39640;&#29983;&#25104;&#25688;&#35201;&#30340;&#36923;&#36753;&#36830;&#36143;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2403.11236v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#22411; &#25688;&#35201;&#65306;&#25968;&#25454;&#21487;&#35270;&#21270;&#26159;&#21576;&#29616;&#25968;&#25454;&#21644;&#25366;&#25496;&#20854;&#23453;&#36149;&#35265;&#35299;&#30340;&#37325;&#35201;&#25163;&#27573;&#12290;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#36827;&#34892;&#22270;&#34920;&#25688;&#35201;&#30340;&#20219;&#21153;&#26377;&#21161;&#20110;&#28145;&#20837;&#20998;&#26512;&#22270;&#34920;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#35270;&#35273;-&#35821;&#35328;&#21305;&#37197;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#26126;&#26174;&#19981;&#36275;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20840;&#38754;&#30340;&#22270;&#34920;&#26631;&#39064;&#37197;&#23545;&#21644;&#27599;&#20010;&#22270;&#34920;&#30340;&#24494;&#35843;&#35828;&#26126;&#12290;&#30001;&#20110;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#21508;&#31181;&#20027;&#39064;&#21644;&#35270;&#35273;&#39118;&#26684;&#65292;&#21487;&#20197;&#20174;&#35757;&#32451;&#25968;&#25454;&#30340;&#35282;&#24230;&#33719;&#24471;&#26356;&#22909;&#30340;&#21305;&#37197;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22270;&#34920;&#25688;&#35201;&#26041;&#27861;ChartThinker&#65292;&#23427;&#22522;&#20110;&#24605;&#32500;&#38142;&#21644;&#19978;&#19979;&#25991;&#26816;&#32034;&#31574;&#30053;&#32508;&#21512;&#28145;&#24230;&#20998;&#26512;&#65292;&#26088;&#22312;&#25552;&#39640;&#29983;&#25104;&#25688;&#35201;&#30340;&#36923;&#36753;&#36830;&#36143;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#24314;&#31435;&#22312;&#31934;&#24515;&#31574;&#21010;Data
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11236v1 Announce Type: new  Abstract: Data visualization serves as a critical means for presenting data and mining its valuable insights. The task of chart summarization, through natural language processing techniques, facilitates in-depth data analysis of charts. However, there still are notable deficiencies in terms of visual-language matching and reasoning ability for existing approaches. To address these limitations, this study constructs a large-scale dataset of comprehensive chart-caption pairs and fine-tuning instructions on each chart. Thanks to the broad coverage of various topics and visual styles within this dataset, better matching degree can be achieved from the view of training data. Moreover, we propose an innovative chart summarization method, ChartThinker, which synthesizes deep analysis based on chains of thought and strategies of context retrieval, aiming to improve the logical coherence and accuracy of the generated summaries. Built upon the curated datas
&lt;/p&gt;</description></item><item><title>Gemini 1.5 Pro&#26159;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22312;&#25968;&#30334;&#19975;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#24518;&#21644;&#25512;&#29702;&#20449;&#24687;&#65292;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.05530</link><description>&lt;p&gt;
Gemini 1.5&#65306;&#35299;&#38145;&#36328;&#25968;&#30334;&#19975;&#26631;&#35760;&#19978;&#19979;&#25991;&#30340;&#22810;&#27169;&#24577;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05530
&lt;/p&gt;
&lt;p&gt;
Gemini 1.5 Pro&#26159;&#19968;&#31181;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22312;&#25968;&#30334;&#19975;&#26631;&#35760;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#24518;&#21644;&#25512;&#29702;&#20449;&#24687;&#65292;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20221;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Gemini&#23478;&#26063;&#30340;&#26368;&#26032;&#27169;&#22411;Gemini 1.5 Pro&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#35745;&#31639;&#30340;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#33021;&#22815;&#22238;&#24518;&#21644;&#25512;&#29702;&#25968;&#30334;&#19975;&#26631;&#35760;&#19978;&#19979;&#25991;&#20013;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#65292;&#21253;&#25324;&#22810;&#20010;&#38271;&#25991;&#26723;&#21644;&#20960;&#23567;&#26102;&#30340;&#35270;&#39057;&#21644;&#38899;&#39057;&#12290;Gemini 1.5 Pro&#22312;&#21508;&#31181;&#24418;&#24335;&#30340;&#38271;&#19978;&#19979;&#25991;&#26816;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36817;&#20046;&#23436;&#32654;&#30340;&#21484;&#22238;&#29575;&#65292;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#38382;&#31572;&#12289;&#38271;&#35270;&#39057;&#38382;&#31572;&#21644;&#38271;&#19978;&#19979;&#25991;ASR&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#24191;&#27867;&#19968;&#31995;&#21015;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;Gemini 1.0 Ultra&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30456;&#21305;&#25932;&#29978;&#33267;&#36229;&#36807;&#12290;&#22312;&#30740;&#31350;Gemini 1.5 Pro&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#30340;&#26497;&#38480;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#33267;&#23569;10M&#26631;&#35760;&#30340;&#33539;&#22260;&#20869;&#32487;&#32493;&#25913;&#36827;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#20960;&#20046;&#23436;&#32654;&#22320;&#36798;&#21040;&#20102;&#36229;&#36807;99%&#30340;&#26816;&#32034;&#29575;&#65292;&#36825;&#26159;&#23545;&#29616;&#26377;&#27169;&#22411;&#22914;Claude 2.1&#65288;200k&#65289;&#21644;GPT-4 Turbo&#65288;128k&#65289;&#30340;&#19990;&#20195;&#24615;&#39134;&#36291;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#26032;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05530v1 Announce Type: cross  Abstract: In this report, we present the latest model of the Gemini family, Gemini 1.5 Pro, a highly compute-efficient multimodal mixture-of-experts model capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. Gemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks across modalities, improves the state-of-the-art in long-document QA, long-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5 Pro's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (&gt;99%) up to at least 10M tokens, a generational leap over existing models such as Claude 2.1 (200k) and GPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large language models at the
&lt;/p&gt;</description></item><item><title>WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.03218</link><description>&lt;p&gt;
WMDP&#22522;&#20934;&#65306;&#36890;&#36807;&#36951;&#24536;&#27979;&#37327;&#21644;&#20943;&#23569;&#24694;&#24847;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03218
&lt;/p&gt;
&lt;p&gt;
WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#30333;&#23467;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#25919;&#21629;&#20196;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#24694;&#24847;&#34892;&#20026;&#32773;&#24320;&#21457;&#29983;&#29289;&#12289;&#32593;&#32476;&#21644;&#21270;&#23398;&#27494;&#22120;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#20123;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#65292;&#25919;&#24220;&#26426;&#26500;&#21644;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#27491;&#22312;&#24320;&#21457;LLMs&#30340;&#21361;&#38505;&#33021;&#21147;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26159;&#31169;&#20154;&#30340;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#22914;&#20309;&#20943;&#23569;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20165;&#19987;&#27880;&#20110;&#20960;&#26465;&#39640;&#24230;&#29305;&#23450;&#30340;&#24694;&#24847;&#20351;&#29992;&#36884;&#24452;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#22823;&#35268;&#27169;&#26432;&#20260;&#24615;&#27494;&#22120;&#20195;&#29702;&#65288;WMDP&#65289;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;WMDP&#30001;&#19968;&#32452;&#23398;&#26415;&#30028;&#21644;&#25216;&#26415;&#39038;&#38382;&#32852;&#21512;&#24320;&#21457;&#65292;&#24182;&#22312;&#20844;&#24320;&#21457;&#24067;&#21069;&#20005;&#26684;&#36807;&#28388;&#20197;&#28040;&#38500;&#25935;&#24863;&#20449;&#24687;&#12290;WMDP&#26377;&#20004;&#20010;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#36923;&#36753;&#34920;&#36798;&#26469;&#34920;&#24449;&#31867;&#21035;&#21547;&#20041;&#30340;&#35268;&#21017;Prompt&#26041;&#27861;&#65292;&#32467;&#21512;PLMs&#21644;&#33258;&#36845;&#20195;&#36923;&#36753;&#35268;&#21017;&#23454;&#29616;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;</title><link>https://arxiv.org/abs/2403.02932</link><description>&lt;p&gt;
RulePrompt: &#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#19982;&#25552;&#31034;PLMs&#21644;&#33258;&#36845;&#20195;&#36923;&#36753;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
RulePrompt: Weakly Supervised Text Classification with Prompting PLMs and Self-Iterative Logical Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02932
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#36923;&#36753;&#34920;&#36798;&#26469;&#34920;&#24449;&#31867;&#21035;&#21547;&#20041;&#30340;&#35268;&#21017;Prompt&#26041;&#27861;&#65292;&#32467;&#21512;PLMs&#21644;&#33258;&#36845;&#20195;&#36923;&#36753;&#35268;&#21017;&#23454;&#29616;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#65288;WSTC&#65289;&#65292;&#21448;&#31216;&#38646;&#26679;&#26412;&#25110;&#26080;&#25968;&#25454;&#25991;&#26412;&#20998;&#31867;&#65292;&#22312;&#21160;&#24577;&#21644;&#24320;&#25918;&#30340;Web&#29615;&#22659;&#20013;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20165;&#38656;&#35201;&#27599;&#20010;&#31867;&#21035;&#30340;&#26377;&#38480;&#31181;&#23376;&#35789;&#65288;&#26631;&#31614;&#21517;&#31216;&#65289;&#32780;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#23601;&#33021;&#23545;&#22823;&#37327;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#30693;&#35782;&#24418;&#24335;&#65292;&#20351;&#29992;&#36923;&#36753;&#34920;&#36798;&#26469;&#34920;&#24449;&#31867;&#21035;&#30340;&#21547;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20511;&#21161;PLMs&#21644;&#33258;&#36845;&#20195;&#36923;&#36753;&#35268;&#21017;&#26469;&#23454;&#29616;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02932v1 Announce Type: new  Abstract: Weakly supervised text classification (WSTC), also called zero-shot or dataless text classification, has attracted increasing attention due to its applicability in classifying a mass of texts within the dynamic and open Web environment, since it requires only a limited set of seed words (label names) for each category instead of labeled data. With the help of recently popular prompting Pre-trained Language Models (PLMs), many studies leveraged manually crafted and/or automatically identified verbalizers to estimate the likelihood of categories, but they failed to differentiate the effects of these category-indicative words, let alone capture their correlations and realize adaptive adjustments according to the unlabeled corpus. In this paper, in order to let the PLM effectively understand each category, we at first propose a novel form of rule-based knowledge using logical expressions to characterize the meanings of categories. Then, we d
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35780;&#20272;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26415;&#21518;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#19981;&#21516;&#35757;&#32451;&#31574;&#30053;&#30340;&#27169;&#22411;&#22312;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#20013;&#30340;&#28508;&#22312;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.17493</link><description>&lt;p&gt;
&#20026;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#24320;&#20855;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27491;&#30830;&#21058;&#37327;&#26159;&#22810;&#23569;&#65311;
&lt;/p&gt;
&lt;p&gt;
Prescribing Large Language Models for Perioperative Care: What's The Right Dose for Pre-trained Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17493
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26415;&#21518;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#19981;&#21516;&#35757;&#32451;&#31574;&#30053;&#30340;&#27169;&#22411;&#22312;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#20013;&#30340;&#28508;&#22312;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26415;&#21518;&#39118;&#38505;&#39044;&#27979;&#21487;&#20197;&#25351;&#23548;&#26377;&#25928;&#30340;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#31649;&#29702;&#21644;&#35268;&#21010;&#12290;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#39044;&#27979;&#26415;&#21518;&#39118;&#38505;&#12290;&#30740;&#31350;&#20027;&#35201;&#28041;&#21450;2018&#24180;&#33267;2021&#24180;&#38388;&#26469;&#33258;Barnes Jewish&#21307;&#38498;&#31995;&#32479;&#30340;84,875&#20221;&#35760;&#24405;&#12290;&#26041;&#27861;&#22312;Beth Israel Deaconess&#30340;MIMIC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22797;&#21046;&#12290;&#20004;&#39033;&#30740;&#31350;&#30340;&#24179;&#22343;&#38543;&#35775;&#26102;&#38388;&#22522;&#20110;&#26415;&#21518;ICU&#20303;&#38498;&#26102;&#38388;&#23567;&#20110;7&#22825;&#12290;&#23545;&#20110;BJH&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#21253;&#25324;30&#22825;&#27515;&#20129;&#29575;&#12289;&#32954;&#26643;&#22622;&#65288;PE&#65289;&#21644;&#32954;&#28814;&#12290;&#23545;BioGPT&#12289;ClinicalBERT&#21644;BioClinicalBERT&#23454;&#26045;&#20102;&#19977;&#31181;&#22495;&#33258;&#36866;&#24212;&#21644;&#24494;&#35843;&#31574;&#30053;&#65306;&#33258;&#30417;&#30563;&#30446;&#26631;&#65307;&#32467;&#21512;&#21322;&#30417;&#30563;&#24494;&#35843;&#30340;&#26631;&#31614;&#65307;&#20197;&#21450;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#36827;&#34892;&#22522;&#30784;&#24314;&#27169;&#12290;&#27169;&#22411;&#24615;&#33021;&#20351;&#29992;&#25509;&#25910;&#22120;&#25805;&#20316;&#29305;&#24449;&#19979;&#30340;&#38754;&#31215;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17493v1 Announce Type: new  Abstract: Postoperative risk predictions can inform effective perioperative care management and planning. We aimed to assess whether clinical large language models (LLMs) can predict postoperative risks using clinical texts with various training strategies. The main cohort involved 84,875 records from Barnes Jewish Hospital (BJH) system between 2018 and 2021. Methods were replicated on Beth Israel Deaconess's MIMIC dataset. Both studies had mean duration of follow-up based on the length of postoperative ICU stay less than 7 days. For the BJH dataset, outcomes included 30-day mortality, pulmonary embolism (PE) and pneumonia. Three domain adaptation and finetuning strategies were implemented for BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives; incorporating labels with semi-supervised fine-tuning; and foundational modelling through multi-task learning. Model performance was compared using the area under the receiver operating ch
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Dynamic Experienced Expert Modeling&#65288;DEEM&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#32463;&#39564;&#19987;&#23478;&#20351;LLMs&#33021;&#22815;&#20197;&#21322;&#21442;&#25968;&#21270;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;&#22312;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15264</link><description>&lt;p&gt;
DEEM&#65306;&#38754;&#21521;&#31435;&#22330;&#26816;&#27979;&#30340;&#21160;&#24577;&#20307;&#39564;&#19987;&#23478;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
DEEM: Dynamic Experienced Expert Modeling for Stance Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Dynamic Experienced Expert Modeling&#65288;DEEM&#65289;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#32463;&#39564;&#19987;&#23478;&#20351;LLMs&#33021;&#22815;&#20197;&#21322;&#21442;&#25968;&#21270;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#65292;&#25552;&#39640;&#20102;&#22312;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#21021;&#27493;&#23581;&#35797;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;&#31435;&#22330;&#26816;&#27979;&#20219;&#21153;&#65292;&#23637;&#29616;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#31435;&#22330;&#26816;&#27979;&#36890;&#24120;&#38656;&#35201;&#35814;&#32454;&#30340;&#32972;&#26223;&#30693;&#35782;&#65292;&#20256;&#32479;&#30340;&#25512;&#29702;&#26041;&#27861;&#21487;&#33021;&#20250;&#24573;&#35270;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#36827;&#34892;&#19987;&#19994;&#21644;&#20934;&#30830;&#30340;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;LLMs&#30340;&#25512;&#29702;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#23588;&#20854;&#22312;&#21033;&#29992;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#27169;&#25311;&#29305;&#23450;&#19987;&#23478;&#65288;&#21363;&#22810;&#26234;&#33021;&#20307;&#65289;&#26469;&#26816;&#27979;&#31435;&#22330;&#26041;&#38754;&#12290;&#19982;&#29616;&#26377;&#38656;&#35201;&#35814;&#32454;&#25551;&#36848;&#24182;&#20351;&#29992;&#22266;&#23450;&#19987;&#23478;&#30340;&#22810;&#26234;&#33021;&#20307;&#20316;&#21697;&#19981;&#21516;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Dynamic Experienced Expert Modeling&#65288;DEEM&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#29983;&#25104;&#30340;&#32463;&#39564;&#19987;&#23478;&#65292;&#24182;&#35753;LLMs&#20197;&#21322;&#21442;&#25968;&#21270;&#26041;&#24335;&#36827;&#34892;&#25512;&#29702;&#65292;&#20351;&#19987;&#23478;&#26356;&#20855;&#26222;&#36866;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DEEM&#22312;&#19977;&#20010;&#22330;&#26223;&#19978;&#19968;&#30452;&#36798;&#21040;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15264v1 Announce Type: new  Abstract: Recent work has made a preliminary attempt to use large language models (LLMs) to solve the stance detection task, showing promising results. However, considering that stance detection usually requires detailed background knowledge, the vanilla reasoning method may neglect the domain knowledge to make a professional and accurate analysis. Thus, there is still room for improvement of LLMs reasoning, especially in leveraging the generation capability of LLMs to simulate specific experts (i.e., multi-agents) to detect the stance. In this paper, different from existing multi-agent works that require detailed descriptions and use fixed experts, we propose a Dynamic Experienced Expert Modeling (DEEM) method which can leverage the generated experienced experts and let LLMs reason in a semi-parametric way, making the experts more generalizable and reliable. Experimental results demonstrate that DEEM consistently achieves the best results on thre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25776;&#20889;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#20445;&#30041;&#20256;&#32479;&#36741;&#23548;&#31995;&#32479;&#32467;&#26500;&#21644;&#25945;&#23398;&#27861;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09216</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;AutoTutor&#30340;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
Scaling the Authoring of AutoTutors with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25776;&#20889;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#20445;&#30041;&#20256;&#32479;&#36741;&#23548;&#31995;&#32479;&#32467;&#26500;&#21644;&#25945;&#23398;&#27861;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25945;&#32946;&#39046;&#22495;&#26377;&#22810;&#31181;&#29992;&#36884;&#65292;&#20174;&#33258;&#21160;&#39064;&#30446;&#29983;&#25104;&#21040;&#20316;&#25991;&#35780;&#20272;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25776;&#20889;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;LLMs&#30340;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#23427;&#20204;&#23481;&#26131;&#20559;&#31163;&#25152;&#26399;&#26395;&#30340;&#25945;&#23398;&#31574;&#30053;&#65292;&#20363;&#22914;&#27844;&#38706;&#31572;&#26696;&#32473;&#23398;&#29983;&#65292;&#24635;&#20307;&#19978;&#25552;&#20379;&#30340;&#20445;&#35777;&#24456;&#23569;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#34429;&#28982;&#24102;&#26377;&#26576;&#20123;&#38480;&#21046;&#30340;LLMs&#21487;&#20197;&#21462;&#20195;&#23398;&#31185;&#19987;&#23478;&#30340;&#20301;&#32622;&#65292;&#20294;&#25972;&#20307;&#30340;&#25945;&#23398;&#35774;&#35745;&#20173;&#38656;&#25163;&#24037;&#21046;&#20316;&#20197;&#21462;&#24471;&#26368;&#20339;&#23398;&#20064;&#25928;&#26524;&#12290;&#22522;&#20110;&#36825;&#19968;&#21407;&#21017;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#31034;&#20363;&#30340;&#31471;&#21040;&#31471;&#36741;&#23548;&#31995;&#32479;&#65292;&#21629;&#21517;&#20026;MWPTutor&#65292;&#23427;&#20351;&#29992;LLMs&#22635;&#20805;&#39044;&#23450;&#20041;&#26377;&#38480;&#29366;&#24577;&#36716;&#25442;&#22120;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#20445;&#30041;&#20102;&#22810;&#24180;&#26469;&#30001;&#23398;&#20064;&#31185;&#23398;&#23478;&#24320;&#21457;&#30340;&#20256;&#32479;&#36741;&#23548;&#31995;&#32479;&#30340;&#32467;&#26500;&#21644;&#25945;&#23398;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09216v1 Announce Type: new Abstract: Large Language Models (LLMs) have found several use cases in education, ranging from automatic question generation to essay evaluation. In this paper, we explore the potential of using Large Language Models (LLMs) to author Intelligent Tutoring Systems. A common pitfall of LLMs is their straying from desired pedagogical strategies such as leaking the answer to the student, and in general, providing no guarantees. We posit that while LLMs with certain guardrails can take the place of subject experts, the overall pedagogical design still needs to be handcrafted for the best learning results. Based on this principle, we create a sample end-to-end tutoring system named MWPTutor, which uses LLMs to fill in the state space of a pre-defined finite state transducer. This approach retains the structure and the pedagogy of traditional tutoring systems that has been developed over the years by learning scientists but brings in additional flexibility
&lt;/p&gt;</description></item><item><title>ChemLLM&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#21270;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#25351;&#20196;&#26500;&#24314;&#26041;&#27861;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#36716;&#21270;&#20026;&#23545;&#35805;&#24418;&#24335;&#65292;&#20855;&#26377;&#24179;&#28369;&#23545;&#35805;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#21270;&#23398;&#30340;&#19977;&#20010;&#20027;&#35201;&#20219;&#21153;&#20013;&#20987;&#36133;&#20102;GPT-3.5&#12290;</title><link>https://arxiv.org/abs/2402.06852</link><description>&lt;p&gt;
ChemLLM: &#19968;&#20010;&#21270;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChemLLM: A Chemical Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06852
&lt;/p&gt;
&lt;p&gt;
ChemLLM&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#21270;&#23398;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#26032;&#39062;&#30340;&#25351;&#20196;&#26500;&#24314;&#26041;&#27861;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#36716;&#21270;&#20026;&#23545;&#35805;&#24418;&#24335;&#65292;&#20855;&#26377;&#24179;&#28369;&#23545;&#35805;&#20132;&#20114;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#21270;&#23398;&#30340;&#19977;&#20010;&#20027;&#35201;&#20219;&#21153;&#20013;&#20987;&#36133;&#20102;GPT-3.5&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21270;&#23398;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#12289;&#20998;&#23376;&#29983;&#25104;&#12289;&#23454;&#39564;&#21327;&#35758;&#35774;&#35745;&#31561;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#32570;&#20047;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#21270;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22522;&#20110;&#23545;&#35805;&#30340;&#27169;&#22411;&#12290;&#36825;&#20010;&#25361;&#25112;&#26469;&#33258;&#20110;&#20107;&#23454;&#65292;&#22823;&#22810;&#25968;&#21270;&#23398;&#25968;&#25454;&#21644;&#31185;&#23398;&#30693;&#35782;&#20027;&#35201;&#23384;&#20648;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#24211;&#20013;&#65292;&#30452;&#25509;&#20351;&#29992;&#36825;&#20123;&#32467;&#26500;&#21270;&#25968;&#25454;&#20250;&#24433;&#21709;&#27169;&#22411;&#32500;&#25345;&#36830;&#36143;&#23545;&#35805;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#26495;&#30340;&#25351;&#20196;&#26500;&#24314;&#26041;&#27861;&#65292;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#36716;&#21270;&#20026;&#31616;&#27905;&#23545;&#35805;&#24418;&#24335;&#65292;&#36866;&#21512;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;ChemLLM&#65292;&#31532;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#21270;&#23398;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#36827;&#34892;&#24179;&#28369;&#23545;&#35805;&#20132;&#20114;&#12290;ChemLLM&#22312;&#21270;&#23398;&#30340;&#19977;&#20010;&#20027;&#35201;&#20219;&#21153;&#65292;&#21363;&#21517;&#31216;&#36716;&#25442;&#12289;&#20998;&#23376;&#29983;&#25104;&#21644;&#23454;&#39564;&#21327;&#35758;&#35774;&#35745;&#26041;&#38754;&#65292;&#20987;&#36133;&#20102;GPT-3.5&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made impressive progress in chemistry applications, including molecular property prediction, molecular generation, experimental protocol design, etc. However, the community lacks a dialogue-based model specifically designed for chemistry. The challenge arises from the fact that most chemical data and scientific knowledge are primarily stored in structured databases, and the direct use of these structured data compromises the model's ability to maintain coherent dialogue. To tackle this issue, we develop a novel template-based instruction construction method that transforms structured knowledge into plain dialogue, making it suitable for language model training. By leveraging this approach, we develop ChemLLM, the first large language model dedicated to chemistry, capable of performing various tasks across chemical disciplines with smooth dialogue interaction. ChemLLM beats GPT-3.5 on all three principal tasks in chemistry, i.e., name conversion, molecu
&lt;/p&gt;</description></item><item><title>VerAs&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#35780;&#20272;STEM&#23454;&#39564;&#25253;&#21578;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#32500;&#24230;&#30340;&#20998;&#26512;&#35780;&#20272;&#26631;&#20934;&#65292;&#20197;&#21450;&#38024;&#23545;&#23398;&#29983;&#25552;&#20379;&#35814;&#32454;&#21453;&#39304;&#65292;&#24110;&#21161;&#20182;&#20204;&#25552;&#39640;&#31185;&#23398;&#20889;&#20316;&#25216;&#24039;&#12290;</title><link>https://arxiv.org/abs/2402.05224</link><description>&lt;p&gt;
VerAs: &#39564;&#35777;&#28982;&#21518;&#35780;&#20272;STEM&#23454;&#39564;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
VerAs: Verify then Assess STEM Lab Reports
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05224
&lt;/p&gt;
&lt;p&gt;
VerAs&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#35780;&#20272;STEM&#23454;&#39564;&#25253;&#21578;&#12290;&#23427;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#32500;&#24230;&#30340;&#20998;&#26512;&#35780;&#20272;&#26631;&#20934;&#65292;&#20197;&#21450;&#38024;&#23545;&#23398;&#29983;&#25552;&#20379;&#35814;&#32454;&#21453;&#39304;&#65292;&#24110;&#21161;&#20182;&#20204;&#25552;&#39640;&#31185;&#23398;&#20889;&#20316;&#25216;&#24039;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;STEM&#25945;&#32946;&#23545;&#25209;&#21028;&#24615;&#24605;&#32500;&#33021;&#21147;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#31185;&#23398;&#20889;&#20316;&#22312;&#27880;&#37325;&#25506;&#31350;&#25216;&#33021;&#30340;&#35838;&#31243;&#20013;&#21457;&#25381;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26368;&#36817;&#21457;&#24067;&#30340;&#19968;&#20221;&#25968;&#25454;&#38598;&#26159;&#22522;&#20110;&#19968;&#22871;&#25506;&#31350;&#22411;&#29289;&#29702;&#35838;&#31243;&#30340;&#20004;&#32452;&#22823;&#23398;&#27700;&#24179;&#30340;&#23454;&#39564;&#25253;&#21578;&#65292;&#20381;&#36182;&#20110;&#21033;&#29992;&#22810;&#20010;&#32500;&#24230;&#30340;&#20998;&#26512;&#35780;&#20272;&#26631;&#20934;&#65292;&#25351;&#23450;&#23398;&#31185;&#30693;&#35782;&#21644;&#20248;&#31168;&#35299;&#37322;&#30340;&#19968;&#33324;&#32452;&#25104;&#37096;&#20998;&#12290;&#27599;&#20010;&#20998;&#26512;&#32500;&#24230;&#37117;&#20197;6&#20998;&#21046;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#25552;&#20379;&#35814;&#32454;&#21453;&#39304;&#65292;&#24110;&#21161;&#23398;&#29983;&#25552;&#39640;&#31185;&#23398;&#20889;&#20316;&#25216;&#24039;&#12290;&#25163;&#21160;&#35780;&#20272;&#21487;&#33021;&#36739;&#24930;&#65292;&#24182;&#19988;&#22312;&#22823;&#29677;&#20013;&#23545;&#25152;&#26377;&#23398;&#29983;&#36827;&#34892;&#19968;&#33268;&#24615;&#26657;&#20934;&#21487;&#33021;&#24456;&#22256;&#38590;&#12290;&#23613;&#31649;&#22312;STEM&#23398;&#31185;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#30340;&#33258;&#21160;&#35780;&#20272;&#19978;&#24050;&#32463;&#26377;&#24456;&#22810;&#24037;&#20316;&#65292;&#20294;&#22312;&#23454;&#39564;&#25253;&#21578;&#31561;&#38271;&#31687;&#20889;&#20316;&#20013;&#30340;&#24037;&#20316;&#35201;&#23569;&#24471;&#22810;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#29420;&#31435;&#30340;&#39564;&#35777;&#22120;&#21644;&#35780;&#20272;&#27169;&#22359;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;&#24320;&#25918;&#39046;&#22495;&#38382;&#39064;&#22238;&#31572;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With an increasing focus in STEM education on critical thinking skills, science writing plays an ever more important role in curricula that stress inquiry skills. A recently published dataset of two sets of college level lab reports from an inquiry-based physics curriculum relies on analytic assessment rubrics that utilize multiple dimensions, specifying subject matter knowledge and general components of good explanations. Each analytic dimension is assessed on a 6-point scale, to provide detailed feedback to students that can help them improve their science writing skills. Manual assessment can be slow, and difficult to calibrate for consistency across all students in large classes. While much work exists on automated assessment of open-ended questions in STEM subjects, there has been far less work on long-form writing such as lab reports. We present an end-to-end neural architecture that has separate verifier and assessment modules, inspired by approaches to Open Domain Question Answ
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#23545;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35780;&#20272;&#25351;&#26631;UAcc&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#39640;&#20934;&#30830;&#24230;&#30340;LLMs&#21487;&#33021;&#20855;&#26377;&#36739;&#20302;&#30340;&#30830;&#23450;&#24615;&#65292;&#32780;&#22823;&#35268;&#27169;LLMs&#21487;&#33021;&#27604;&#36739;&#23567;&#35268;&#27169;LLMs&#26356;&#19981;&#30830;&#23450;&#12290;&#25351;&#20196;&#24494;&#35843;&#20542;&#21521;&#20110;&#22686;&#21152;LLMs&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12794</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking LLMs via Uncertainty Quantification. (arXiv:2401.12794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12794
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#23545;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35780;&#20272;&#25351;&#26631;UAcc&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#39640;&#20934;&#30830;&#24230;&#30340;LLMs&#21487;&#33021;&#20855;&#26377;&#36739;&#20302;&#30340;&#30830;&#23450;&#24615;&#65292;&#32780;&#22823;&#35268;&#27169;LLMs&#21487;&#33021;&#27604;&#36739;&#23567;&#35268;&#27169;LLMs&#26356;&#19981;&#30830;&#23450;&#12290;&#25351;&#20196;&#24494;&#35843;&#20542;&#21521;&#20110;&#22686;&#21152;LLMs&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21508;&#20010;&#26426;&#26500;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22686;&#22810;&#65292;&#24432;&#26174;&#20102;&#23545;&#32508;&#21512;&#35780;&#20272;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#24179;&#21488;&#65292;&#22914;&#24191;&#20026;&#20154;&#30693;&#30340;HuggingFace&#24320;&#25918;&#30340;LLM&#25490;&#34892;&#27036;&#65292;&#24573;&#35270;&#20102;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;--&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#19981;&#30830;&#23450;&#24615;&#23545;&#20110;&#20840;&#38754;&#35780;&#20272;LLMs&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;LLMs&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#38598;&#25104;&#36827;&#21435;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#20116;&#20010;&#20856;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#20843;&#20010;LLMs&#65288;LLM&#31995;&#21015;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32771;&#34385;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#21644;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#35780;&#20272;&#25351;&#26631;UAcc&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65306;I&#65289;&#20934;&#30830;&#24230;&#36234;&#39640;&#30340;LLMs&#21487;&#33021;&#20250;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#30830;&#23450;&#24615;&#65307;II&#65289;&#35268;&#27169;&#36739;&#22823;&#30340;LLMs&#21487;&#33021;&#19982;&#36739;&#23567;&#30340;LLMs&#30456;&#27604;&#26174;&#31034;&#20986;&#26356;&#22823;&#30340;&#19981;&#30830;&#23450;&#24615;&#65307;III&#65289;&#25351;&#20196;&#24494;&#35843;&#20542;&#21521;&#20110;&#22686;&#21152;LLMs&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#36890;&#36807;&#32771;&#34385;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves eight LLMs (LLM series) spanning five representative natural language processing tasks. Additionally, we introduce an uncertainty-aware evaluation metric, UAcc, which takes into account both prediction accuracy and prediction uncertainty. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs. By taking uncertainty
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#25552;&#31034;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hypotheses-to-Theories (HtT)&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;LLMs&#25512;&#29702;&#30340;&#35268;&#21017;&#24211;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07064</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Large Language Models can Learn Rules. (arXiv:2310.07064v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07064
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#25552;&#31034;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hypotheses-to-Theories (HtT)&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;LLMs&#25512;&#29702;&#30340;&#35268;&#21017;&#24211;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#32473;&#20986;&#19968;&#20123;&#31034;&#20363;&#21644;&#20013;&#38388;&#27493;&#39588;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20381;&#36182;LLM&#20013;&#30340;&#38544;&#24335;&#30693;&#35782;&#30340;&#25552;&#31034;&#26041;&#27861;&#22312;&#38544;&#24335;&#30693;&#35782;&#38169;&#35823;&#25110;&#19982;&#20219;&#21153;&#19981;&#19968;&#33268;&#26102;&#24448;&#24448;&#20250;&#20135;&#29983;&#38169;&#35823;&#30340;&#31572;&#26696;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"&#20551;&#35774;&#21040;&#29702;&#35770;" (HtT) &#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;LLMs&#25512;&#29702;&#30340;&#35268;&#21017;&#24211;&#12290;HtT&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65292;&#24402;&#32435;&#38454;&#27573;&#21644;&#28436;&#32462;&#38454;&#27573;&#12290;&#22312;&#24402;&#32435;&#38454;&#27573;&#65292;&#39318;&#20808;&#35201;&#27714;LLM&#26681;&#25454;&#19968;&#32452;&#35757;&#32451;&#31034;&#20363;&#29983;&#25104;&#21644;&#39564;&#35777;&#35268;&#21017;&#12290;&#20986;&#29616;&#24182;&#23548;&#33268;&#27491;&#30830;&#31572;&#26696;&#30340;&#35268;&#21017;&#23558;&#34987;&#25910;&#38598;&#24418;&#25104;&#19968;&#20010;&#35268;&#21017;&#24211;&#12290;&#22312;&#28436;&#32462;&#38454;&#27573;&#65292;&#28982;&#21518;&#35201;&#27714;LLM&#20351;&#29992;&#23398;&#20064;&#30340;&#35268;&#21017;&#24211;&#36827;&#34892;&#25512;&#29702;&#20197;&#22238;&#31572;&#27979;&#35797;&#38382;&#39064;&#12290;&#22312;&#25968;&#20540;&#25512;&#29702;&#21644;&#20851;&#31995;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;HtT&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#20351;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
When prompted with a few examples and intermediate steps, large language models (LLMs) have demonstrated impressive performance in various reasoning tasks. However, prompting methods that rely on implicit knowledge in an LLM often hallucinate incorrect answers when the implicit knowledge is wrong or inconsistent with the task. To tackle this problem, we present Hypotheses-to-Theories (HtT), a framework that learns a rule library for reasoning with LLMs. HtT contains two stages, an induction stage and a deduction stage. In the induction stage, an LLM is first asked to generate and verify rules over a set of training examples. Rules that appear and lead to correct answers sufficiently often are collected to form a rule library. In the deduction stage, the LLM is then prompted to employ the learned rule library to perform reasoning to answer test questions. Experiments on both numerical reasoning and relational reasoning problems show that HtT improves existing prompting methods, with an 
&lt;/p&gt;</description></item><item><title>SpeechAlign&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#27169;&#22411;&#20013;&#28304;&#30446;&#26631;&#23545;&#40784;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#25351;&#26631;&#65292;&#20026;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35775;&#38382;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#24320;&#28304;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.11585</link><description>&lt;p&gt;
SpeechAlign:&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#32763;&#35793;&#23545;&#40784;&#35780;&#20272;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SpeechAlign: a Framework for Speech Translation Alignment Evaluation. (arXiv:2309.11585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11585
&lt;/p&gt;
&lt;p&gt;
SpeechAlign&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#27169;&#22411;&#20013;&#28304;&#30446;&#26631;&#23545;&#40784;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#25351;&#26631;&#65292;&#20026;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35775;&#38382;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#24320;&#28304;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21040;&#35821;&#38899;&#21644;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#30446;&#21069;&#26159;&#30740;&#31350;&#30340;&#21160;&#24577;&#39046;&#22495;&#12290;&#20026;&#20102;&#23545;&#36825;&#20123;&#39046;&#22495;&#20570;&#20986;&#36129;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SpeechAlign&#65292;&#19968;&#31181;&#35780;&#20272;&#35821;&#38899;&#27169;&#22411;&#20013;&#28304;&#30446;&#26631;&#23545;&#40784;&#36825;&#19968;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#35299;&#20915;&#32570;&#20047;&#21512;&#36866;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Speech Gold Alignment&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#22312;&#33521;&#24503;&#25991;&#26412;&#32763;&#35793;&#30340;&#40644;&#37329;&#23545;&#40784;&#25968;&#25454;&#38598;&#22522;&#30784;&#19978;&#26500;&#24314;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#25351;&#26631;&#65292;&#35821;&#38899;&#23545;&#40784;&#38169;&#35823;&#29575;&#65288;SAER&#65289;&#21644;&#26102;&#38388;&#21152;&#26435;&#35821;&#38899;&#23545;&#40784;&#38169;&#35823;&#29575;&#65288;TW-SAER&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#27169;&#22411;&#20013;&#30340;&#23545;&#40784;&#36136;&#37327;&#12290;&#36890;&#36807;&#21457;&#24067;SpeechAlign&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35775;&#38382;&#30340;&#35780;&#20272;&#26694;&#26550;&#29992;&#20110;&#27169;&#22411;&#35780;&#20272;&#65292;&#24182;&#19988;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#23545;&#24320;&#28304;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech-to-Speech and Speech-to-Text translation are currently dynamic areas of research. To contribute to these fields, we present SpeechAlign, a framework to evaluate the underexplored field of source-target alignment in speech models. Our framework has two core components. First, to tackle the absence of suitable evaluation datasets, we introduce the Speech Gold Alignment dataset, built upon a English-German text translation gold alignment dataset. Secondly, we introduce two novel metrics, Speech Alignment Error Rate (SAER) and Time-weighted Speech Alignment Error Rate (TW-SAER), to evaluate alignment quality in speech models. By publishing SpeechAlign we provide an accessible evaluation framework for model assessment, and we employ it to benchmark open-source Speech Translation models.
&lt;/p&gt;</description></item><item><title>ControlRetriever&#26159;&#19968;&#31181;&#26377;&#21442;&#25968;&#38548;&#31163;&#26550;&#26500;&#30340;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20511;&#21161;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25351;&#20196;&#65292;&#33021;&#22815;&#25511;&#21046;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30452;&#25509;&#25191;&#34892;&#21508;&#31181;&#26816;&#32034;&#20219;&#21153;&#65292;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.10025</link><description>&lt;p&gt;
ControlRetriever: &#21457;&#25381;&#25351;&#20196;&#30340;&#21147;&#37327;&#36827;&#34892;&#21487;&#25511;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
ControlRetriever: Harnessing the Power of Instructions for Controllable Retrieval. (arXiv:2308.10025v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10025
&lt;/p&gt;
&lt;p&gt;
ControlRetriever&#26159;&#19968;&#31181;&#26377;&#21442;&#25968;&#38548;&#31163;&#26550;&#26500;&#30340;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20511;&#21161;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25351;&#20196;&#65292;&#33021;&#22815;&#25511;&#21046;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30452;&#25509;&#25191;&#34892;&#21508;&#31181;&#26816;&#32034;&#20219;&#21153;&#65292;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32570;&#20047;&#19987;&#38376;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#22312;&#21508;&#31181;&#26816;&#32034;&#20219;&#21153;&#20013;&#24448;&#24448;&#38590;&#20197;&#34920;&#29616;&#20986;&#33394;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#26816;&#32034;&#20219;&#21153;&#36890;&#24120;&#28041;&#21450;&#19981;&#21516;&#30340;&#25628;&#32034;&#24847;&#22270;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ControlRetriever&#65292;&#19968;&#31181;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#21442;&#25968;&#38548;&#31163;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#25511;&#21046;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30452;&#25509;&#25191;&#34892;&#21508;&#31181;&#26816;&#32034;&#20219;&#21153;&#65292;&#21457;&#25381;&#33258;&#28982;&#35821;&#35328;&#20013;&#26126;&#30830;&#25551;&#36848;&#26816;&#32034;&#24847;&#22270;&#30340;&#25351;&#20196;&#30340;&#21147;&#37327;&#12290;&#20511;&#21161;&#24050;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#35777;&#26126;&#24378;&#22823;&#30340;ControlNet&#30340;&#22522;&#30784;&#65292;ControlRetriever&#23558;&#19981;&#21516;&#30340;&#26816;&#32034;&#27169;&#22411;&#36171;&#20104;&#20102;&#21487;&#25511;&#24615;&#26816;&#32034;&#30340;&#26032;&#33021;&#21147;&#65292;&#21516;&#26102;&#36824;&#21463;&#21040;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#30340;&#25351;&#23548;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;LLM&#24341;&#23548;&#30340;&#25351;&#20196;&#21512;&#25104;&#21644;&#36845;&#20195;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#24191;&#27867;&#29983;&#25104;&#30340;&#26816;&#32034;&#25968;&#25454;&#36845;&#20195;&#35843;&#25972;ControlRetriever&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown that dense retrieval models, lacking dedicated training data, struggle to perform well across diverse retrieval tasks, as different retrieval tasks often entail distinct search intents. To address this challenge, in this work we introduce ControlRetriever, a generic and efficient approach with a parameter isolated architecture, capable of controlling dense retrieval models to directly perform varied retrieval tasks, harnessing the power of instructions that explicitly describe retrieval intents in natural language. Leveraging the foundation of ControlNet, which has proven powerful in text-to-image generation, ControlRetriever imbues different retrieval models with the new capacity of controllable retrieval, all while being guided by task-specific instructions. Furthermore, we propose a novel LLM guided Instruction Synthesizing and Iterative Training strategy, which iteratively tunes ControlRetriever based on extensive automatically-generated retrieval data wit
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#36328;&#39046;&#22495;&#21644;&#36328;&#20154;&#21475;&#30340;&#25991;&#26412;&#20998;&#31867;&#21160;&#24577;&#65292;&#26500;&#24314;&#26356;&#36890;&#29992;&#30340;&#28389;&#29992;&#20998;&#31867;&#22120;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23569;&#37327;&#22810;&#26679;&#30340;&#25968;&#25454;&#23545;&#20110;&#27169;&#22411;&#30340;&#36890;&#29992;&#21270;&#21644;&#36866;&#24212;&#38750;&#24120;&#26377;&#30410;&#12290;</title><link>http://arxiv.org/abs/2307.16811</link><description>&lt;p&gt;
DoDo&#23398;&#20064;: &#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#26816;&#27979;&#38024;&#23545;&#20844;&#20247;&#20154;&#29289;&#30340;&#28389;&#29992;&#30340;&#39046;&#22495;-&#20154;&#21475;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
DoDo Learning: DOmain-DemOgraphic Transfer in Language Models for Detecting Abuse Targeted at Public Figures. (arXiv:2307.16811v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16811
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#36328;&#39046;&#22495;&#21644;&#36328;&#20154;&#21475;&#30340;&#25991;&#26412;&#20998;&#31867;&#21160;&#24577;&#65292;&#26500;&#24314;&#26356;&#36890;&#29992;&#30340;&#28389;&#29992;&#20998;&#31867;&#22120;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23569;&#37327;&#22810;&#26679;&#30340;&#25968;&#25454;&#23545;&#20110;&#27169;&#22411;&#30340;&#36890;&#29992;&#21270;&#21644;&#36866;&#24212;&#38750;&#24120;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20247;&#20154;&#29289;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#21463;&#21040;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#28389;&#29992;&#65292;&#36825;&#23545;&#20182;&#20204;&#22312;&#20844;&#20247;&#29983;&#27963;&#20013;&#30340;&#31215;&#26497;&#21442;&#19982;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#33258;&#21160;&#21270;&#31995;&#32479;&#21487;&#20197;&#22823;&#35268;&#27169;&#35782;&#21035;&#28389;&#29992;&#65292;&#20294;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#26082;&#26114;&#36149;&#21448;&#22797;&#26434;&#65292;&#21487;&#33021;&#20250;&#36896;&#25104;&#20260;&#23475;&#12290;&#22240;&#27492;&#65292;&#31995;&#32479;&#30340;&#39640;&#25928;&#24615;&#21644;&#36890;&#29992;&#24615;&#26159;&#21487;&#21462;&#30340;&#65292;&#21487;&#20197;&#22788;&#29702;&#22312;&#32447;&#28389;&#29992;&#30340;&#20849;&#20139;&#21644;&#29305;&#23450;&#26041;&#38754;&#12290;&#25105;&#20204;&#25506;&#32034;&#20132;&#21449;&#32676;&#20307;&#25991;&#26412;&#20998;&#31867;&#30340;&#21160;&#24577;&#65292;&#20197;&#20102;&#35299;&#35757;&#32451;&#22312;&#19968;&#20010;&#39046;&#22495;&#25110;&#20154;&#21475;&#32479;&#35745;&#19978;&#30340;&#20998;&#31867;&#22120;&#22312;&#20854;&#20182;&#39046;&#22495;&#25110;&#20154;&#21475;&#32479;&#35745;&#19978;&#30340;&#36716;&#31227;&#24773;&#20917;&#65292;&#20174;&#32780;&#26500;&#24314;&#26356;&#36890;&#29992;&#30340;&#28389;&#29992;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#21019;&#26032;DODO&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;28,000&#20010;&#26631;&#35760;&#26465;&#30446;&#65292;&#22312;&#36328;&#39046;&#22495;&#65288;&#20307;&#32946;&#21644;&#25919;&#27835;&#65289;&#21644;&#36328;&#20154;&#21475;&#65288;&#22899;&#24615;&#21644;&#30007;&#24615;&#65289;&#30340;&#22235;&#20010;&#39046;&#22495;-&#20154;&#21475;&#23545;&#20013;&#65292;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26469;&#20998;&#31867;&#38024;&#23545;&#20844;&#20247;&#20154;&#29289;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#65288;i&#65289;&#23569;&#37327;&#22810;&#26679;&#30340;&#25968;&#25454;&#23545;&#36890;&#29992;&#21270;&#21644;&#27169;&#22411;&#36866;&#24212;&#38750;&#24120;&#26377;&#30410;&#65307;&#65288;ii&#65289;&#27169;&#22411;&#30340;&#36716;&#31227;&#26356;&#23481;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public figures receive a disproportionate amount of abuse on social media, impacting their active participation in public life. Automated systems can identify abuse at scale but labelling training data is expensive, complex and potentially harmful. So, it is desirable that systems are efficient and generalisable, handling both shared and specific aspects of online abuse. We explore the dynamics of cross-group text classification in order to understand how well classifiers trained on one domain or demographic can transfer to others, with a view to building more generalisable abuse classifiers. We fine-tune language models to classify tweets targeted at public figures across DOmains (sport and politics) and DemOgraphics (women and men) using our novel DODO dataset, containing 28,000 labelled entries, split equally across four domain-demographic pairs. We find that (i) small amounts of diverse data are hugely beneficial to generalisation and model adaptation; (ii) models transfer more eas
&lt;/p&gt;</description></item><item><title>Brainformers &#26159;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#23618;&#32423;&#30340;&#32467;&#26500;&#23436;&#21892;&#20102; Transformer &#30340;&#35774;&#35745;&#32570;&#38519;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#25910;&#25947;&#21644;&#26356;&#24555;&#30340;&#27493;&#39588;&#26102;&#38388;&#65292;&#34920;&#29616;&#20986;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00008</link><description>&lt;p&gt;
Brainformers&#65306;&#20197;&#25928;&#29575;&#25442;&#21462;&#31616;&#27905;&#24615;
&lt;/p&gt;
&lt;p&gt;
Brainformers: Trading Simplicity for Efficiency. (arXiv:2306.00008v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00008
&lt;/p&gt;
&lt;p&gt;
Brainformers &#26159;&#19968;&#20010;&#26032;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#23618;&#32423;&#30340;&#32467;&#26500;&#23436;&#21892;&#20102; Transformer &#30340;&#35774;&#35745;&#32570;&#38519;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#25910;&#25947;&#21644;&#26356;&#24555;&#30340;&#27493;&#39588;&#26102;&#38388;&#65292;&#34920;&#29616;&#20986;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#36817;&#25104;&#21151;&#30340;&#26680;&#24515;&#25216;&#26415;&#12290;Transformer &#20855;&#26377;&#19968;&#20010;&#20960;&#20046;&#32479;&#19968;&#30340;&#39592;&#26550;&#65292;&#20854;&#20013;&#23618;&#27425;&#22312;&#21069;&#39304;&#21644;&#33258;&#27880;&#24847;&#21147;&#20043;&#38388;&#20132;&#26367;&#20197;&#24314;&#31435;&#28145;&#24230;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#65292;&#24182;&#21457;&#29616;&#26356;&#22797;&#26434;&#30340;&#22359;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23436;&#25104;&#20219;&#21153;&#12290;&#26681;&#25454;&#36825;&#20010;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#22359;&#65292;&#31216;&#20026; Brainformer&#65292;&#23427;&#30001;&#21508;&#31181;&#24418;&#24335;&#30340;&#23618;&#24402;&#19968;&#21270;&#21644;&#28608;&#27963;&#20989;&#25968;&#12289;&#31232;&#30095;&#38376;&#25511;&#21069;&#39304;&#23618;&#12289;&#23494;&#38598;&#21069;&#39304;&#23618;&#12289;&#27880;&#24847;&#21147;&#23618;&#31561;&#22810;&#26679;&#23618;&#32423;&#32452;&#25104;&#12290;&#22312;&#36136;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#65292;Brainformer &#24635;&#26159;&#20248;&#20110;&#29616;&#26377;&#30340;&#31264;&#23494;&#21644;&#31232;&#30095; Transformer&#12290;&#19968;&#20010;&#20855;&#26377; 80 &#20159;&#20010;&#27599;&#20010;&#26631;&#35760;&#28608;&#27963;&#21442;&#25968;&#30340; Brainformer &#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#20854; GLaM &#23545;&#24212;&#29289;&#65292;&#34920;&#29616;&#20986; 2 &#20493;&#26356;&#24555;&#30340;&#35757;&#32451;&#25910;&#25947;&#21644; 5 &#20493;&#26356;&#24555;&#30340;&#27493;&#39588;&#26102;&#38388;&#12290;&#22312;&#19979;&#28216;&#20219;&#21153;&#35780;&#20272;&#20013;&#65292;Brainformer &#20063;&#34920;&#29616;&#24471;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are central to recent successes in natural language processing and computer vision. Transformers have a mostly uniform backbone where layers alternate between feed-forward and self-attention in order to build a deep network. Here we investigate this design choice and find that more complex blocks that have different permutations of layer primitives can be more efficient. Using this insight, we develop a complex block, named Brainformer, that consists of a diverse sets of layers such as sparsely gated feed-forward layers, dense feed-forward layers, attention layers, and various forms of layer normalization and activation functions. Brainformer consistently outperforms the state-of-the-art dense and sparse Transformers, in terms of both quality and efficiency. A Brainformer model with 8 billion activated parameters per token demonstrates 2x faster training convergence and 5x faster step time compared to its GLaM counterpart. In downstream task evaluation, Brainformer also de
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#20041;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#24403;&#21069;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.12517</link><description>&lt;p&gt;
&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Retrieving Texts based on Abstract Descriptions. (arXiv:2305.12517v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#20041;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#24403;&#21069;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38024;&#23545;&#25991;&#26412;&#30340;&#20449;&#24687;&#25552;&#21462;&#65292;&#25351;&#20196;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23545;&#20110;&#22312;&#22823;&#35268;&#27169;&#25991;&#26723;&#38598;&#21512;&#20013;&#23450;&#20301;&#31526;&#21512;&#32473;&#23450;&#25551;&#36848;&#30340;&#25991;&#26412;&#65288;&#35821;&#20041;&#26816;&#32034;&#65289;&#24182;&#19981;&#36866;&#29992;&#12290;&#22522;&#20110;&#23884;&#20837;&#21521;&#37327;&#30340;&#30456;&#20284;&#24230;&#25628;&#32034;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;&#25191;&#34892;&#26816;&#32034;&#65292;&#20294;&#23884;&#20837;&#20013;&#30340;&#30456;&#20284;&#24230;&#23450;&#20041;&#19981;&#26126;&#30830;&#19988;&#19981;&#19968;&#33268;&#65292;&#24182;&#19988;&#23545;&#20110;&#35768;&#22810;&#29992;&#20363;&#26469;&#35828;&#37117;&#26159;&#27425;&#20248;&#30340;&#12290;&#37027;&#20040;&#65292;&#20160;&#20040;&#26159;&#26377;&#25928;&#26816;&#32034;&#30340;&#22909;&#30340;&#26597;&#35810;&#34920;&#31034;&#65311;&#25105;&#20204;&#30830;&#23450;&#20102;&#26681;&#25454;&#20869;&#23481;&#30340;&#25688;&#35201;&#25551;&#36848;&#26816;&#32034;&#21477;&#23376;&#30340;&#26126;&#30830;&#23450;&#20041;&#19988;&#19968;&#33268;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#25991;&#26412;&#23884;&#20837;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#27169;&#22411;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#30340;&#34920;&#29616;&#26174;&#33879;&#25552;&#21319;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#36890;&#36807;&#25552;&#31034;LLM&#33719;&#24471;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#34429;&#28982;&#24456;&#23481;&#26131;&#20174;LLM&#20013;&#33719;&#24471;&#35757;&#32451;&#26448;&#26009;&#65292;&#20294;LLM&#26080;&#27861;&#30452;&#25509;&#25191;&#34892;&#26816;&#32034;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While instruction-tuned Large Language Models (LLMs) excel at extracting information from text, they are not suitable for locating texts conforming to a given description in a large document collection (semantic retrieval). Similarity search over embedding vectors does allow to perform retrieval by query, but the similarity reflected in the embedding is ill-defined and non-consistent, and is sub-optimal for many use cases. What, then, is a good query representation for effective retrieval?  We identify the well defined and consistent task of retrieving sentences based on abstract descriptions of their content. We demonstrate the inadequacy of current text embeddings and propose an alternative model that significantly improves when used in standard nearest neighbor search. The model is trained using positive and negative pairs sourced through prompting a LLM. While it is easy to source the training material from an LLM, the retrieval task cannot be performed by the LLM directly. This de
&lt;/p&gt;</description></item></channel></rss>