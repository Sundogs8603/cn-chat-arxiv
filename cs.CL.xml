<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#25991;&#31456;&#20171;&#32461;&#20102;&#22914;&#20309;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#27880;&#20837;&#21307;&#23398;&#30693;&#35782;&#65292;&#25552;&#39640;&#20107;&#23454;&#27491;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#22797;&#35786;&#25252;&#29702;&#25351;&#21335;&#30340;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03652</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#27880;&#20837;&#35821;&#35328;&#29983;&#25104;&#65306;&#21307;&#23398;&#23545;&#35805;&#20013;&#33258;&#21160;&#29983;&#25104;&#22797;&#35786;&#25252;&#29702;&#25351;&#21335;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Injecting knowledge into language generation: a case study in auto-charting after-visit care instructions from medical dialogue. (arXiv:2306.03652v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03652
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#20171;&#32461;&#20102;&#22914;&#20309;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#27880;&#20837;&#21307;&#23398;&#30693;&#35782;&#65292;&#25552;&#39640;&#20107;&#23454;&#27491;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#22797;&#35786;&#25252;&#29702;&#25351;&#21335;&#30340;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#39046;&#22495;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#65292;&#20107;&#23454;&#27491;&#30830;&#24615;&#32463;&#24120;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#23454;&#38469;&#24212;&#29992;&#30340;&#23616;&#38480;&#24615;&#12290;&#20445;&#25345;&#20107;&#23454;&#27491;&#30830;&#24615;&#30340;&#22522;&#26412;&#35201;&#27714;&#26159;&#22788;&#29702;&#32597;&#35265;&#26631;&#35760;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#37325;&#28857;&#20171;&#32461;&#20102;&#28304;&#24207;&#21015;&#21644;&#21442;&#32771;&#24207;&#21015;&#20013;&#20986;&#29616;&#30340;&#32597;&#35265;&#26631;&#35760;&#65292;&#22914;&#26524;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#28431;&#25481;&#65292;&#21017;&#20250;&#38477;&#20302;&#36755;&#20986;&#25991;&#26412;&#30340;&#20107;&#23454;&#27491;&#30830;&#24615;&#12290;&#23545;&#20110;&#30693;&#35782;&#20016;&#23500;&#30340;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#21033;&#29992;&#30693;&#35782;&#26469;&#35782;&#21035;&#20986;&#37325;&#35201;&#30340;&#20986;&#29616;&#22312;&#28304;&#21644;&#21442;&#32771;&#20013;&#30340;&#32597;&#35265;&#26631;&#35760;&#65292;&#24182;&#25552;&#39640;&#23427;&#20204;&#30340;&#26465;&#20214;&#27010;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#29983;&#25104;&#32467;&#26524;&#30340;&#20107;&#23454;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Factual correctness is often the limiting factor in practical applications of natural language generation in high-stakes domains such as healthcare. An essential requirement for maintaining factuality is the ability to deal with rare tokens. This paper focuses on rare tokens that appear in both the source and the reference sequences, and which, when missed during generation, decrease the factual correctness of the output text. For high-stake domains that are also knowledge-rich, we show how to use knowledge to (a) identify which rare tokens that appear in both source and reference are important and (b) uplift their conditional probability. We introduce the ``utilization rate'' that encodes knowledge and serves as a regularizer by maximizing the marginal probability of selected tokens. We present a study in a knowledge-rich domain of healthcare, where we tackle the problem of generating after-visit care instructions based on patient-doctor dialogues. We verify that, in our dataset, spec
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#27010;&#29575;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#22810;&#27169;&#24577;&#35773;&#21050;&#12289;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27010;&#29575;&#30340;&#20860;&#23481;&#24615;&#20551;&#35774;&#21644;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03650</link><description>&lt;p&gt;
&#37327;&#23376;&#27010;&#29575;&#39537;&#21160;&#19979;&#30340;&#32852;&#21512;&#22810;&#27169;&#24577;&#35773;&#21050;&#12289;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Quantum Probability Driven Framework for Joint Multi-Modal Sarcasm, Sentiment and Emotion Analysis. (arXiv:2306.03650v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#27010;&#29575;&#39537;&#21160;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#22810;&#27169;&#24577;&#35773;&#21050;&#12289;&#24773;&#24863;&#21644;&#24773;&#32490;&#20998;&#26512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#27010;&#29575;&#30340;&#20860;&#23481;&#24615;&#20551;&#35774;&#21644;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35773;&#21050;&#12289;&#24773;&#24863;&#21644;&#24773;&#32490;&#26159;&#20154;&#31867;&#23545;&#22806;&#37096;&#20107;&#20214;&#20135;&#29983;&#30340;&#19977;&#31181;&#20856;&#22411;&#33258;&#21457;&#24773;&#24863;&#21453;&#24212;&#65292;&#23427;&#20204;&#19982;&#24444;&#27492;&#32806;&#21512;&#12290;&#36825;&#20123;&#20107;&#20214;&#21487;&#20197;&#29992;&#22810;&#31181;&#34920;&#36798;&#26041;&#24335;&#34920;&#36798;&#65292;&#20363;&#22914;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#32852;&#21512;&#20998;&#26512;&#20154;&#31867;&#30340;&#22810;&#27169;&#24577;&#35773;&#21050;&#12289;&#24773;&#24863;&#21644;&#24773;&#32490;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20027;&#39064;&#65292;&#22240;&#20026;&#23427;&#26159;&#19968;&#20010;&#28041;&#21450;&#36328;&#27169;&#24577;&#20132;&#20114;&#21644;&#36328;&#24773;&#24863;&#30456;&#20851;&#24615;&#30340;&#22797;&#26434;&#35748;&#30693;&#36807;&#31243;&#12290;&#20174;&#27010;&#29575;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36328;&#24773;&#24863;&#30456;&#20851;&#24615;&#24847;&#21619;&#30528;&#23545;&#35773;&#21050;&#12289;&#24773;&#24863;&#21644;&#24773;&#32490;&#30340;&#21028;&#26029;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#29616;&#35937;&#30001;&#20110;&#32463;&#20856;&#27010;&#29575;&#29702;&#35770;&#30340;&#20860;&#23481;&#24615;&#20551;&#35774;&#21644;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#32780;&#26080;&#27861;&#34987;&#20805;&#20998;&#24314;&#27169;&#12290;&#32771;&#34385;&#21040;&#37327;&#23376;&#27010;&#29575;&#22312;&#24314;&#27169;&#20154;&#31867;&#35748;&#30693;&#26041;&#38754;&#30340;&#26368;&#36817;&#25104;&#21151;&#65292;&#23588;&#20854;&#26159;&#19978;&#19979;&#25991;&#19981;&#30456;&#23481;&#20915;&#31574;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#37327;&#23376;&#27010;&#29575;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sarcasm, sentiment, and emotion are three typical kinds of spontaneous affective responses of humans to external events and they are tightly intertwined with each other. Such events may be expressed in multiple modalities (e.g., linguistic, visual and acoustic), e.g., multi-modal conversations. Joint analysis of humans' multi-modal sarcasm, sentiment, and emotion is an important yet challenging topic, as it is a complex cognitive process involving both cross-modality interaction and cross-affection correlation. From the probability theory perspective, cross-affection correlation also means that the judgments on sarcasm, sentiment, and emotion are incompatible. However, this exposed phenomenon cannot be sufficiently modelled by classical probability theory due to its assumption of compatibility. Neither do the existing approaches take it into consideration. In view of the recent success of quantum probability (QP) in modeling human cognition, particularly contextual incompatible decisio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35821;&#35328;&#31867;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#29305;&#24449;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#24418;&#24335;&#22312;&#26356;&#20005;&#26684;&#30340;&#31561;&#20215;&#27010;&#24565;&#19979;&#26159;&#31561;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.03628</link><description>&lt;p&gt;
&#25511;&#21046;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#25910;&#25947;&#19982;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence and Diversity in the Control Hierarchy. (arXiv:2306.03628v1 [cs.FL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35821;&#35328;&#31867;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#29305;&#24449;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#24418;&#24335;&#22312;&#26356;&#20005;&#26684;&#30340;&#31561;&#20215;&#27010;&#24565;&#19979;&#26159;&#31561;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Weir&#23450;&#20041;&#20102;&#19968;&#31181;&#35821;&#35328;&#31867;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#20854;&#20013;&#31532;&#20108;&#20010;&#25104;&#21592;&#65288;$ \mathcal{L} _2 $&#65289;&#26159;&#30001;&#26641;&#30456;&#37051;&#35821;&#27861;&#65288;TAG&#65289;&#65292;&#32447;&#24615;&#32034;&#24341;&#35821;&#27861;&#65288;LIG&#65289;&#65292;&#32452;&#21512;&#33539;&#30068;&#35821;&#27861;&#21644;&#22836;&#35821;&#27861;&#29983;&#25104;&#30340;&#12290;&#35813;&#23618;&#27425;&#32467;&#26500;&#20351;&#29992;&#25511;&#21046;&#26426;&#21046;&#33719;&#24471;&#65292;&#24182;&#20351;&#29992;&#30001;&#21478;&#19968;&#20010;CFG&#25511;&#21046;&#30340;CFG&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#33719;&#24471;$ \mathcal{L} _2 $&#12290;&#25105;&#20204;&#23558;Weir&#20851;&#20110;&#21487;&#25511;CFG&#30340;&#23450;&#20041;&#35843;&#25972;&#20026;&#21487;&#25511;&#19979;&#25512;&#33258;&#21160;&#26426;&#65288;PDAs&#65289;&#30340;&#23450;&#20041;&#12290;&#36825;&#20135;&#29983;&#20102;&#19977;&#20010;&#26032;&#30340;$\mathcal{L} _2 $&#30340;&#29305;&#24449;&#65292;&#20316;&#20026;&#30001;&#25511;&#21046;PDAs&#30340;PDAs&#65292;&#25511;&#21046;CFGs&#30340;PDAs&#21644;CFGs&#30340;&#25511;&#21046;PDAs&#29983;&#25104;&#30340;&#35821;&#35328;&#31867;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#22235;&#20010;&#24418;&#24335;&#19981;&#20165;&#26159;&#24369;&#31561;&#25928;&#30340;&#65292;&#32780;&#19988;&#22312;&#25105;&#20204;&#31216;&#20043;&#20026;d-&#24369;&#31561;&#25928;&#30340;&#26356;&#20005;&#26684;&#24847;&#20041;&#19979;&#26159;&#31561;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#26356;&#20005;&#26684;&#30340;&#31561;&#20215;&#27010;&#24565;d-&#24378;&#31561;&#20215;&#65292;&#25105;&#20204;&#26126;&#30830;&#20102;&#25511;&#21046;CFG&#30340;CFG&#23601;&#26159;TAG&#65292;&#25511;&#21046;PDA&#30340;PDA&#23601;&#26159;embedd&#12290;
&lt;/p&gt;
&lt;p&gt;
Weir has defined a hierarchy of language classes whose second member ($\mathcal{L}_2$) is generated by tree-adjoining grammars (TAG), linear indexed grammars (LIG), combinatory categorial grammars, and head grammars. The hierarchy is obtained using the mechanism of control, and $\mathcal{L}_2$ is obtained using a context-free grammar (CFG) whose derivations are controlled by another CFG. We adapt Weir's definition of a controllable CFG to give a definition of controllable pushdown automata (PDAs). This yields three new characterizations of $\mathcal{L}_2$ as the class of languages generated by PDAs controlling PDAs, PDAs controlling CFGs, and CFGs controlling PDAs. We show that these four formalisms are not only weakly equivalent but equivalent in a stricter sense that we call d-weak equivalence. Furthermore, using an even stricter notion of equivalence called d-strong equivalence, we make precise the intuition that a CFG controlling a CFG is a TAG, a PDA controlling a PDA is an embedd
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;CUE&#65292;&#23427;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#32534;&#30721;&#34920;&#31034;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#25200;&#21160;&#28508;&#22312;&#31354;&#38388;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#65292;&#20174;&#32780;&#30830;&#23450;&#27599;&#20010;&#36755;&#20837;&#26631;&#35760;&#23545;&#24635;&#20307;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#24494;&#35843;&#31639;&#27861;&#65292;&#26088;&#22312;&#21033;&#29992;&#26631;&#35760;&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#22120;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03598</link><description>&lt;p&gt;
CUE: &#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#19981;&#30830;&#23450;&#24615;&#35299;&#37322;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CUE: An Uncertainty Interpretation Framework for Text Classifiers Built on Pre-Trained Language Models. (arXiv:2306.03598v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;CUE&#65292;&#23427;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#32534;&#30721;&#34920;&#31034;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#24182;&#36890;&#36807;&#25200;&#21160;&#28508;&#22312;&#31354;&#38388;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#39044;&#27979;&#65292;&#20174;&#32780;&#30830;&#23450;&#27599;&#20010;&#36755;&#20837;&#26631;&#35760;&#23545;&#24635;&#20307;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#24494;&#35843;&#31639;&#27861;&#65292;&#26088;&#22312;&#21033;&#29992;&#26631;&#35760;&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#22120;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#26500;&#24314;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#22312;&#24773;&#24863;&#20998;&#26512;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#38382;&#31572;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20998;&#31867;&#22120;&#20135;&#29983;&#19981;&#30830;&#23450;&#39044;&#27979;&#26102;&#65292;&#23545;&#20110;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#29702;&#35299;PLMs&#30340;&#23398;&#20064;&#29305;&#28857;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#24037;&#20316;&#65292;&#20294;&#26159;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#31350;&#24433;&#21709;PLM&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;CUE&#65292;&#26088;&#22312;&#35299;&#37322;PLM&#27169;&#22411;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23558;PLM&#32534;&#30721;&#34920;&#31034;&#26144;&#23556;&#21040;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#28982;&#21518;&#36890;&#36807;&#25200;&#21160;&#28508;&#22312;&#31354;&#38388;&#29983;&#25104;&#25991;&#26412;&#34920;&#31034;&#65292;&#20174;&#32780;&#23548;&#33268;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#27874;&#21160;&#12290;&#36890;&#36807;&#27604;&#36739;&#25200;&#21160;&#21644;&#21407;&#22987;&#34920;&#31034;&#20043;&#38388;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#24046;&#24322;&#65292;&#21487;&#20197;&#30830;&#23450;&#27599;&#20010;&#36755;&#20837;&#26631;&#35760;&#23545;&#24635;&#20307;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#24494;&#35843;&#31639;&#27861;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21033;&#29992;&#24050;&#35782;&#21035;&#30340;&#26631;&#35760;&#32423;&#19981;&#30830;&#23450;&#24615;&#20174;&#32780;&#36827;&#19968;&#27493;&#25913;&#21892;&#25991;&#26412;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CUE&#21487;&#20197;&#26377;&#25928;&#22320;&#25581;&#31034;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20026;&#29702;&#35299;&#22522;&#20110;PLM&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#34892;&#20026;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#24494;&#35843;&#31639;&#27861;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classifiers built on Pre-trained Language Models (PLMs) have achieved remarkable progress in various tasks including sentiment analysis, natural language inference, and question-answering. However, the occurrence of uncertain predictions by these classifiers poses a challenge to their reliability when deployed in practical applications. Much effort has been devoted to designing various probes in order to understand what PLMs capture. But few studies have delved into factors influencing PLM-based classifiers' predictive uncertainty. In this paper, we propose a novel framework, called CUE, which aims to interpret uncertainties inherent in the predictions of PLM-based models. In particular, we first map PLM-encoded representations to a latent space via a variational auto-encoder. We then generate text representations by perturbing the latent space which causes fluctuation in predictive uncertainty. By comparing the difference in predictive uncertainty between the perturbed and the or
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#27604;&#36739;&#20102;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#21644;&#20799;&#31461;&#30340;&#23398;&#20064;&#36712;&#36857;&#65292;&#21457;&#29616;&#23427;&#20204;&#37117;&#36981;&#24490;&#23558;&#38899;&#38901;&#20316;&#20026;&#36215;&#28857;&#36880;&#27493;&#20064;&#24471;&#35821;&#27861;&#21644;&#35821;&#20041;&#30340;&#27169;&#24335;&#65292;&#32780;&#19988;&#37117;&#34920;&#29616;&#20986;&#23545;&#20110;&#26576;&#20123;&#35821;&#35328;&#32467;&#26500;&#26377;&#20020;&#30028;&#26399;&#30340;&#23398;&#20064;&#24773;&#20917;&#65292;&#20294;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#36824;&#26159;&#23384;&#22312;&#37325;&#35201;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.03586</link><description>&lt;p&gt;
&#35821;&#35328;&#20064;&#24471;&#65306;&#20799;&#31461;&#21644;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#36981;&#24490;&#30456;&#20284;&#30340;&#23398;&#20064;&#38454;&#27573;&#65311;
&lt;/p&gt;
&lt;p&gt;
Language acquisition: do children and language models follow similar learning stages?. (arXiv:2306.03586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03586
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#27604;&#36739;&#20102;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#21644;&#20799;&#31461;&#30340;&#23398;&#20064;&#36712;&#36857;&#65292;&#21457;&#29616;&#23427;&#20204;&#37117;&#36981;&#24490;&#23558;&#38899;&#38901;&#20316;&#20026;&#36215;&#28857;&#36880;&#27493;&#20064;&#24471;&#35821;&#27861;&#21644;&#35821;&#20041;&#30340;&#27169;&#24335;&#65292;&#32780;&#19988;&#37117;&#34920;&#29616;&#20986;&#23545;&#20110;&#26576;&#20123;&#35821;&#35328;&#32467;&#26500;&#26377;&#20020;&#30028;&#26399;&#30340;&#23398;&#20064;&#24773;&#20917;&#65292;&#20294;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#36824;&#26159;&#23384;&#22312;&#37325;&#35201;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#35328;&#20064;&#24471;&#36807;&#31243;&#20013;&#65292;&#20799;&#31461;&#20250;&#25353;&#29031;&#20856;&#22411;&#30340;&#23398;&#20064;&#38454;&#27573;&#39034;&#24207;&#23398;&#20064;&#35821;&#35328;&#65292;&#39318;&#20808;&#23398;&#20064;&#21457;&#38899;&#20998;&#31867;&#65292;&#28982;&#21518;&#21457;&#23637;&#35789;&#27719;&#65292;&#26368;&#32456;&#25484;&#25569;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#21477;&#27861;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#23548;&#33268;&#36825;&#31181;&#23398;&#20064;&#36712;&#36857;&#30340;&#35745;&#31639;&#21407;&#21017;&#20173;&#28982;&#22823;&#37096;&#20998;&#26410;&#30693;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#36712;&#36857;&#21644;&#20799;&#31461;&#30340;&#23398;&#20064;&#36712;&#36857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;GPT-2&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#21542;&#23637;&#29616;&#20986;&#19982;18&#20010;&#26376;&#33267;6&#23681;&#20799;&#31461;&#30456;&#20284;&#30340;&#35821;&#35328;&#20064;&#24471;&#38454;&#27573;&#12290;&#36890;&#36807;&#20174;BLiMP&#12289;Zorro&#21644;BIG-Bench&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;96&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;48&#20010;GPT-2&#27169;&#22411;&#65292;&#24182;&#22312;&#27599;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#35780;&#20272;&#23427;&#20204;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#33021;&#21147;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#35780;&#20272;&#19982;54&#20010;&#20799;&#31461;&#30340;&#35821;&#35328;&#20135;&#29983;&#36807;&#31243;&#34892;&#20026;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20986;&#20102;&#19977;&#20010;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#19982;&#20799;&#31461;&#19968;&#26679;&#65292;&#35821;&#35328;&#27169;&#22411;&#20542;&#21521;&#20110;&#39318;&#20808;&#20064;&#24471;&#38899;&#38901;&#20449;&#24687;&#65292;&#28982;&#21518;&#36880;&#28176;&#23398;&#20064;&#20351;&#29992;&#27491;&#30830;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#29983;&#25104;&#21333;&#35789;&#21644;&#21477;&#23376;&#12290;&#20854;&#27425;&#65292;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#23545;&#26576;&#20123;&#35821;&#35328;&#32467;&#26500;&#30340;&#23398;&#20064;&#26377;&#20020;&#30028;&#26399;&#65292;&#31867;&#20284;&#20110;&#20799;&#31461;&#12290;&#26368;&#21518;&#65292;&#34429;&#28982;&#24635;&#20307;&#19978;&#30340;&#23398;&#20064;&#36712;&#36857;&#30456;&#20284;&#65292;&#20294;&#20063;&#23384;&#22312;&#20799;&#31461;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;&#37325;&#35201;&#24046;&#24322;&#65292;&#36825;&#21487;&#33021;&#25351;&#21521;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#23398;&#20064;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
During language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning trajectory remain largely unknown. To investigate this, we here compare the learning trajectories of deep language models to those of children. Specifically, we test whether, during its training, GPT-2 exhibits stages of language acquisition comparable to those observed in children aged between 18 months and 6 years. For this, we train 48 GPT-2 models from scratch and evaluate their syntactic and semantic abilities at each training step, using 96 probes curated from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these evaluations with the behavior of 54 children during language production. Our analyses reveal three main findings. First, similarly to children, the language models tend
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;2SDiac&#30340;&#22810;&#28304;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#20013;&#20351;&#29992;&#21487;&#36873;&#38899;&#26631;&#26469;&#30830;&#23450;&#25152;&#26377;&#39044;&#27979;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#36890;&#36807;&#24341;&#20837;Guided Learning&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#38543;&#26426;&#25513;&#34109;&#21644;&#32473;&#23450;&#30340;&#36755;&#20837;&#38899;&#26631;&#25552;&#21319;&#26631;&#35760;&#30340;&#27491;&#30830;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#26631;&#35760;&#25991;&#26412;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03557</link><description>&lt;p&gt;
&#21033;&#29992;&#37096;&#20998;&#26631;&#27880;&#30340;&#25991;&#26412;&#25552;&#21319;&#38463;&#25289;&#20271;&#35821;&#38899;&#26631;&#27880;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Take the Hint: Improving Arabic Diacritization with Partially-Diacritized Text. (arXiv:2306.03557v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;2SDiac&#30340;&#22810;&#28304;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#20013;&#20351;&#29992;&#21487;&#36873;&#38899;&#26631;&#26469;&#30830;&#23450;&#25152;&#26377;&#39044;&#27979;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#36890;&#36807;&#24341;&#20837;Guided Learning&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#38543;&#26426;&#25513;&#34109;&#21644;&#32473;&#23450;&#30340;&#36755;&#20837;&#38899;&#26631;&#25552;&#21319;&#26631;&#35760;&#30340;&#27491;&#30830;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#26631;&#35760;&#25991;&#26412;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#30340;&#38463;&#25289;&#20271;&#35821;&#38899;&#26631;&#27880;&#22312;&#24456;&#22810;&#24212;&#29992;&#22330;&#26223;&#20013;&#37117;&#38750;&#24120;&#26377;&#29992;&#65292;&#27604;&#22914;&#23545;&#20110;&#35821;&#35328;&#23398;&#20064;&#32773;&#26469;&#35828;&#65292;&#26631;&#27880;&#21487;&#20197;&#25552;&#20379;&#38405;&#35835;&#25903;&#25345;&#65292;&#32780;&#23545;&#20110;&#35821;&#38899;&#21512;&#25104;&#36825;&#26679;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#26631;&#27880;&#20934;&#30830;&#24615;&#23545;&#20110;&#21457;&#38899;&#39044;&#27979;&#20063;&#38750;&#24120;&#37325;&#35201;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#25968;&#19987;&#27880;&#20110;&#22788;&#29702;&#27809;&#26377;&#38899;&#26631;&#30340;&#21407;&#22987;&#25991;&#26412;&#30340;&#27169;&#22411;&#65292;&#20294;&#26159;&#36890;&#36807;&#32473;&#20154;&#31867;&#25552;&#20379;&#36873;&#23450;&#30340;&#25110;&#37096;&#20998;&#26631;&#27880;&#30340;&#25935;&#24863;&#35789;&#27719;&#65292;&#21487;&#20197;&#20351;&#24471;&#29983;&#20135;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;2SDiac&#30340;&#22810;&#28304;&#27169;&#22411;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25903;&#25345;&#36755;&#20837;&#20013;&#30340;&#21487;&#36873;&#38899;&#26631;&#20197;&#30830;&#23450;&#25152;&#26377;&#39044;&#27979;&#30340;&#36755;&#20986;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Guided Learning&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21487;&#20197;&#21033;&#29992;&#32473;&#23450;&#30340;&#36755;&#20837;&#38899;&#26631;&#21644;&#19981;&#21516;&#31561;&#32423;&#30340;&#38543;&#26426;&#25513;&#34109;&#26469;&#25552;&#21319;&#26631;&#27880;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#27979;&#35797;&#26399;&#38388;&#25552;&#20379;&#30340;&#26631;&#27880;&#33021;&#22815;&#24433;&#21709;&#26356;&#22810;&#30340;&#36755;&#20986;&#20301;&#32622;&#65292;&#23454;&#39564;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#38750;&#26631;&#35760;&#25991;&#26412;&#19978;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#20943;&#23569;60%&#30340;&#21442;&#25968;&#25968;&#30446;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Arabic diacritization is useful in many applications, ranging from reading support for language learners to accurate pronunciation predictor for downstream tasks like speech synthesis. While most of the previous works focused on models that operate on raw non-diacritized text, production systems can gain accuracy by first letting humans partly annotate ambiguous words. In this paper, we propose 2SDiac, a multi-source model that can effectively support optional diacritics in input to inform all predictions. We also introduce Guided Learning, a training scheme to leverage given diacritics in input with different levels of random masking. We show that the provided hints during test affect more output positions than those annotated. Moreover, experiments on two common benchmarks show that our approach i) greatly outperforms the baseline also when evaluated on non-diacritized text; and ii) achieves state-of-the-art results while reducing the parameter count by over 60%.
&lt;/p&gt;</description></item><item><title>SciLit &#26159;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#26816;&#32034;&#12289;&#25688;&#35201;&#21644;&#24341;&#29992;&#30456;&#20851;&#35770;&#25991;&#30340;&#24179;&#21488;&#65292;&#23427;&#21487;&#20197;&#20174;&#25968;&#30334;&#19975;&#31687;&#25991;&#29486;&#20013;&#39640;&#25928;&#22320;&#25512;&#33616;&#35770;&#25991;&#65292;&#24182;&#25552;&#20379;&#20855;&#26377;&#19978;&#19979;&#25991;&#20851;&#32852;&#30340;&#24341;&#29992;&#21477;&#23376;&#12290;</title><link>http://arxiv.org/abs/2306.03535</link><description>&lt;p&gt;
SciLit: &#19968;&#31181;&#32852;&#21512;&#31185;&#23398;&#25991;&#29486;&#21457;&#29616;&#12289;&#25688;&#35201;&#21644;&#24341;&#25991;&#29983;&#25104;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
SciLit: A Platform for Joint Scientific Literature Discovery, Summarization and Citation Generation. (arXiv:2306.03535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03535
&lt;/p&gt;
&lt;p&gt;
SciLit &#26159;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#26816;&#32034;&#12289;&#25688;&#35201;&#21644;&#24341;&#29992;&#30456;&#20851;&#35770;&#25991;&#30340;&#24179;&#21488;&#65292;&#23427;&#21487;&#20197;&#20174;&#25968;&#30334;&#19975;&#31687;&#25991;&#29486;&#20013;&#39640;&#25928;&#22320;&#25512;&#33616;&#35770;&#25991;&#65292;&#24182;&#25552;&#20379;&#20855;&#26377;&#19978;&#19979;&#25991;&#20851;&#32852;&#30340;&#24341;&#29992;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#20889;&#20316;&#28041;&#21450;&#26816;&#32034;&#12289;&#24635;&#32467;&#21644;&#24341;&#29992;&#30456;&#20851;&#35770;&#25991;&#65292;&#36825;&#22312;&#22823;&#35268;&#27169;&#21644;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#21487;&#33021;&#26159;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#36807;&#31243;&#30456;&#20114;&#25805;&#20316;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#25552;&#20379;&#20102;&#21019;&#24314;&#31471;&#21040;&#31471;&#36741;&#21161;&#20889;&#20316;&#24037;&#20855;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SciLit&#65292;&#36825;&#26159;&#19968;&#20010;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#33258;&#21160;&#25512;&#33616;&#30456;&#20851;&#35770;&#25991;&#65292;&#25552;&#21462;&#20142;&#28857;&#65292;&#24182;&#24314;&#35758;&#19968;&#20010;&#24341;&#29992;&#21477;&#23376;&#20316;&#20026;&#35770;&#25991;&#30340;&#24341;&#29992;&#65292;&#32771;&#34385;&#21040;&#29992;&#25143;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#21644;&#20851;&#38190;&#35789;&#12290;SciLit&#21487;&#20197;&#39640;&#25928;&#22320;&#20174;&#25968;&#30334;&#19975;&#31687;&#35770;&#25991;&#30340;&#22823;&#22411;&#25968;&#25454;&#24211;&#20013;&#25512;&#33616;&#35770;&#25991;&#65292;&#20351;&#29992;&#20004;&#38454;&#27573;&#30340;&#39044;&#21462;&#21644;&#37325;&#26032;&#25490;&#21517;&#25991;&#29486;&#25628;&#32034;&#31995;&#32479;&#65292;&#28789;&#27963;&#22788;&#29702;&#35770;&#25991;&#25968;&#25454;&#24211;&#30340;&#28155;&#21152;&#21644;&#21024;&#38500;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26041;&#20415;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#26174;&#31034;&#25512;&#33616;&#30340;&#35770;&#25991;&#20316;&#20026;&#25688;&#35201;&#65292;&#24182;&#25552;&#20379;&#19982;&#25552;&#20379;&#30340;&#19978;&#19979;&#25991;&#23545;&#40784;&#24182;&#25552;&#21040;&#25152;&#36873;&#20851;&#38190;&#35789;&#30340;&#25688;&#35201;&#24341;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific writing involves retrieving, summarizing, and citing relevant papers, which can be time-consuming processes in large and rapidly evolving fields. By making these processes inter-operable, natural language processing (NLP) provides opportunities for creating end-to-end assistive writing tools. We propose SciLit, a pipeline that automatically recommends relevant papers, extracts highlights, and suggests a reference sentence as a citation of a paper, taking into consideration the user-provided context and keywords. SciLit efficiently recommends papers from large databases of hundreds of millions of papers using a two-stage pre-fetching and re-ranking literature search system that flexibly deals with addition and removal of a paper database. We provide a convenient user interface that displays the recommended papers as extractive summaries and that offers abstractively-generated citing sentences which are aligned with the provided context and which mention the chosen keyword(s).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36136;&#37327;&#35780;&#20272;&#30340;&#36807;&#28388;&#26041;&#27861;&#65292;&#20854;&#20174;&#20266;&#24182;&#34892;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#39640;&#36136;&#37327;&#24179;&#34892;&#25968;&#25454;&#65292;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#35757;&#32451;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03507</link><description>&lt;p&gt;
"&#19968;&#28857;&#23601;&#22815;&#20102;"&#65306;&#22522;&#20110;&#23569;&#37327;&#36136;&#37327;&#35780;&#20272;&#30340;&#35821;&#26009;&#36807;&#28388;&#25913;&#36827;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
"A Little is Enough": Few-Shot Quality Estimation based Corpus Filtering improves Machine Translation. (arXiv:2306.03507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36136;&#37327;&#35780;&#20272;&#30340;&#36807;&#28388;&#26041;&#27861;&#65292;&#20854;&#20174;&#20266;&#24182;&#34892;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#39640;&#36136;&#37327;&#24179;&#34892;&#25968;&#25454;&#65292;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#35757;&#32451;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#35780;&#20272;&#65288;QE&#65289;&#30340;&#20219;&#21153;&#26159;&#22312;&#27809;&#26377;&#21442;&#32771;&#32763;&#35793;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;QE&#30340;&#30446;&#26631;&#19982;&#35821;&#26009;&#24211;&#36807;&#28388;&#30340;&#20219;&#21153;&#30456;&#19968;&#33268;&#65292;&#21363;&#20026;&#22312;&#20266;&#24182;&#34892;&#35821;&#26009;&#24211;&#20013;&#30340;&#21477;&#23545;&#20998;&#37197;&#36136;&#37327;&#20998;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36136;&#37327;&#35780;&#20272;&#30340;&#36807;&#28388;&#26041;&#27861;&#65292;&#20197;&#20174;&#20266;&#24182;&#34892;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#39640;&#36136;&#37327;&#24179;&#34892;&#25968;&#25454;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;QE&#26694;&#26550;&#29992;&#20110;&#20174;&#20266;&#24182;&#34892;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#39640;&#36136;&#37327;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#26032;&#39062;&#36866;&#24212;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20010;&#36807;&#28388;&#21518;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#23545;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#33521;&#39532;&#25289;&#38597;&#35821;&#12289;&#20013;&#25991;-&#33521;&#35821;&#21644;&#21360;&#22320;&#35821;-&#23391;&#21152;&#25289;&#35821;&#35821;&#31181;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#22810;&#36798;1.8&#20010;BLEU&#20998;&#25968;&#12290;&#22522;&#20934;&#27169;&#22411;&#26159;&#22312;&#25972;&#20010;&#20266;&#24182;&#34892;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#30340;&#23569;&#26679;&#26412;QE&#27169;&#22411;&#20174;&#33521;&#39532;&#25289;&#38597;&#35821;QE&#27169;&#22411;&#20013;&#20256;&#36882;&#23398;&#20064;&#24182;&#22312;&#20165;500&#20010;&#21360;&#22320;&#35821;-&#23391;&#21152;&#25289;&#35821;&#35757;&#32451;&#23454;&#20363;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#26041;&#27861;&#65292;&#26174;&#31034;&#20986;&#20102;&#23545;&#36807;&#28388;&#21518;&#30340;&#35821;&#26009;&#24211;&#36136;&#37327;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality Estimation (QE) is the task of evaluating the quality of a translation when reference translation is not available. The goal of QE aligns with the task of corpus filtering, where we assign the quality score to the sentence pairs present in the pseudo-parallel corpus. We propose a Quality Estimation based Filtering approach to extract high-quality parallel data from the pseudo-parallel corpus. To the best of our knowledge, this is a novel adaptation of the QE framework to extract quality parallel corpus from the pseudo-parallel corpus. By training with this filtered corpus, we observe an improvement in the Machine Translation (MT) system's performance by up to 1.8 BLEU points, for English-Marathi, Chinese-English, and Hindi-Bengali language pairs, over the baseline model. The baseline model is the one that is trained on the whole pseudo-parallel corpus. Our Few-shot QE model transfer learned from the English-Marathi QE model and fine-tuned on only 500 Hindi-Bengali training inst
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#21046;&#23450;&#23433;&#20840;&#20445;&#38556;&#65292;&#20998;&#26512;LLMs&#30340;&#20869;&#23481;&#29983;&#25104;&#26426;&#21046;&#65292;&#30830;&#23450;&#20102;&#22235;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#21457;&#21644;&#38144;&#21806;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20225;&#19994;&#30340;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2306.03503</link><description>&lt;p&gt;
&#24212;&#29992;&#26631;&#20934;&#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#28216;&#20262;&#29702;
&lt;/p&gt;
&lt;p&gt;
Applying Standards to Advance Upstream &amp; Downstream Ethics in Large Language Models. (arXiv:2306.03503v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#21046;&#23450;&#23433;&#20840;&#20445;&#38556;&#65292;&#20998;&#26512;LLMs&#30340;&#20869;&#23481;&#29983;&#25104;&#26426;&#21046;&#65292;&#30830;&#23450;&#20102;&#22235;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#21457;&#21644;&#38144;&#21806;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20225;&#19994;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;AI&#25152;&#26377;&#32773;&#22914;&#20309;&#20511;&#37492;&#20854;&#20182;&#20869;&#23481;&#21019;&#20316;&#34892;&#19994;&#30340;&#34892;&#20026;&#20934;&#21017;&#21644;&#20262;&#29702;&#26631;&#20934;&#65292;&#20026;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#21046;&#23450;&#23433;&#20840;&#20445;&#38556;&#12290;&#23427;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20262;&#29702;&#24847;&#35782;&#29616;&#29366;&#12290;&#36890;&#36807;&#20998;&#26512;LLMs&#30340;&#20869;&#23481;&#29983;&#25104;&#26426;&#21046;&#65292;&#30830;&#23450;&#20102;&#22235;&#20010;&#20851;&#38190;&#39046;&#22495;&#65288;&#19978;&#19979;&#28216;&#21644;&#29992;&#25143;&#25552;&#31034;/&#22238;&#31572;&#65289;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#21487;&#20197;&#26377;&#25928;&#22320;&#24212;&#29992;&#20445;&#38556;&#25514;&#26045;&#12290;&#38543;&#21518;&#65292;&#23545;&#36825;&#22235;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#21253;&#25324;&#22312;&#25104;&#26412;&#12289;&#26377;&#25928;&#24615;&#21644;&#19982;&#34892;&#19994;&#24815;&#20363;&#30340;&#19968;&#33268;&#24615;&#26041;&#38754;&#35780;&#20272;&#29616;&#26377;&#30340;&#20262;&#29702;&#20445;&#38556;&#25514;&#26045;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#35266;&#28857;&#26159;&#65292;&#29616;&#26377;&#30340;&#19982;IT&#30456;&#20851;&#30340;&#20262;&#29702;&#20934;&#21017;&#34429;&#28982;&#36866;&#29992;&#20110;&#20256;&#32479;&#30340;IT&#24037;&#31243;&#39046;&#22495;&#65292;&#20294;&#19981;&#36275;&#20197;&#24212;&#23545;&#22522;&#20110;LLMs&#20869;&#23481;&#29983;&#25104;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20511;&#37492;&#26032;&#38395;&#19994;&#20869;&#24050;&#26377;&#30340;&#23454;&#36341;&#65292;&#20026;&#20998;&#21457;&#21644;&#38144;&#21806;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20225;&#19994;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores how AI-owners can develop safeguards for AI-generated content by drawing from established codes of conduct and ethical standards in other content-creation industries. It delves into the current state of ethical awareness on Large Language Models (LLMs). By dissecting the mechanism of content generation by LLMs, four key areas (upstream/downstream and at user prompt/answer), where safeguards could be effectively applied, are identified. A comparative analysis of these four areas follows and includes an evaluation of the existing ethical safeguards in terms of cost, effectiveness, and alignment with established industry practices. The paper's key argument is that existing IT-related ethical codes, while adequate for traditional IT engineering, are inadequate for the challenges posed by LLM-based content generation. Drawing from established practices within journalism, we propose potential standards for businesses involved in distributing and selling LLM-generated cont
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#22270;&#20687;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#19982;&#24773;&#33410;&#35760;&#24518;&#26469;&#24494;&#35843;&#27169;&#22411;&#20197;&#36866;&#24212;&#26032;&#25968;&#25454;&#30340;&#30446;&#30340;&#65292;&#32467;&#26524;&#34920;&#26126;&#24773;&#33410;&#35760;&#24518;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.03500</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#19982;&#24773;&#33410;&#35760;&#24518;&#30340;&#21487;&#36866;&#24212;&#20132;&#20114;&#24335;&#22270;&#20687;&#25551;&#36848;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Adaptable and Interactive Image Captioning with Data Augmentation and Episodic Memory. (arXiv:2306.03500v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#22270;&#20687;&#25551;&#36848;&#29983;&#25104;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#19982;&#24773;&#33410;&#35760;&#24518;&#26469;&#24494;&#35843;&#27169;&#22411;&#20197;&#36866;&#24212;&#26032;&#25968;&#25454;&#30340;&#30446;&#30340;&#65292;&#32467;&#26524;&#34920;&#26126;&#24773;&#33410;&#35760;&#24518;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36215;&#21040;&#24456;&#22909;&#30340;&#23398;&#20064;&#25928;&#26524;&#65292;&#22240;&#20026;&#20154;&#31867;&#21453;&#39304;&#21487;&#20197;&#36880;&#27493;&#22320;&#34701;&#20837;&#21040;&#35757;&#32451;&#36807;&#31243;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20132;&#20114;&#24335;&#23398;&#20064;&#30340;&#22270;&#20687;&#25551;&#36848;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#21033;&#29992;&#29992;&#25143;&#36755;&#20837;&#30340;&#20449;&#24687;&#26469;&#36880;&#27493;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20197;&#36866;&#24212;&#26032;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#22312;&#27599;&#20010;&#26032;&#30340;&#36755;&#20837;&#26679;&#20363;&#19978;&#33719;&#21462;&#26356;&#22810;&#30340;&#25968;&#25454;&#24182;&#23454;&#29616;&#20102;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#26469;&#38450;&#27490;&#37325;&#22797;&#26356;&#26032;&#23548;&#33268;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#23558;&#19968;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#22270;&#20687;&#25551;&#36848;&#25968;&#25454;&#38598; VizWiz &#20998;&#25104;&#19981;&#37325;&#21472;&#30340;&#37096;&#20998;&#65292;&#20197;&#27169;&#25311;&#36880;&#27493;&#36755;&#20837;&#27969;&#65292;&#19981;&#26029;&#36866;&#24212;&#26032;&#25968;&#25454;&#30340;&#36807;&#31243;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;&#25968;&#25454;&#22686;&#24378;&#20250;&#23548;&#33268;&#32467;&#26524;&#21464;&#24046;&#65292;&#20294;&#21363;&#20351;&#21482;&#26377;&#30456;&#23545;&#36739;&#23569;&#30340;&#25968;&#25454;&#21487;&#29992;&#65292;&#24773;&#33410;&#35760;&#24518;&#20063;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive machine learning (IML) is a beneficial learning paradigm in cases of limited data availability, as human feedback is incrementally integrated into the training process. In this paper, we present an IML pipeline for image captioning which allows us to incrementally adapt a pre-trained image captioning model to a new data distribution based on user input. In order to incorporate user input into the model, we explore the use of a combination of simple data augmentation methods to obtain larger data batches for each newly annotated data instance and implement continual learning methods to prevent catastrophic forgetting from repeated updates. For our experiments, we split a domain-specific image captioning dataset, namely VizWiz, into non-overlapping parts to simulate an incremental input flow for continually adapting the model to new data. We find that, while data augmentation worsens results, even when relatively small amounts of data are available, episodic memory is an effe
&lt;/p&gt;</description></item><item><title>SciCap+ &#26159;&#19968;&#20221;&#30693;&#35782;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#31185;&#23398;&#22270;&#20363;&#26631;&#39064;&#33258;&#21160;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#20174;&#25552;&#21040;&#22270;&#29255;&#30340;&#27573;&#33853;&#21644;OCR&#26631;&#35760;&#20013;&#25552;&#21462;&#36328;&#27169;&#24577;&#23884;&#20837;&#30340;&#30693;&#35782;&#65292;&#32463;&#23454;&#39564;&#21457;&#29616;&#36825;&#20123;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#22320;&#25552;&#39640;&#33258;&#21160;&#26631;&#20934;&#22270;&#29255;&#30340;&#23383;&#24149;&#29983;&#25104;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.03491</link><description>&lt;p&gt;
SciCap+: &#19968;&#20221;&#30693;&#35782;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#31185;&#23398;&#22270;&#20363;&#26631;&#39064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
SciCap+: A Knowledge Augmented Dataset to Study the Challenges of Scientific Figure Captioning. (arXiv:2306.03491v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03491
&lt;/p&gt;
&lt;p&gt;
SciCap+ &#26159;&#19968;&#20221;&#30693;&#35782;&#22686;&#24378;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#31185;&#23398;&#22270;&#20363;&#26631;&#39064;&#33258;&#21160;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#20174;&#25552;&#21040;&#22270;&#29255;&#30340;&#27573;&#33853;&#21644;OCR&#26631;&#35760;&#20013;&#25552;&#21462;&#36328;&#27169;&#24577;&#23884;&#20837;&#30340;&#30693;&#35782;&#65292;&#32463;&#23454;&#39564;&#21457;&#29616;&#36825;&#20123;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#22320;&#25552;&#39640;&#33258;&#21160;&#26631;&#20934;&#22270;&#29255;&#30340;&#23383;&#24149;&#29983;&#25104;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#26415;&#25991;&#29486;&#20013;&#65292;&#22270;&#20363;&#20026;&#21521;&#35835;&#32773;&#20256;&#36798;&#31185;&#23398;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#25509;&#30340;&#26041;&#24335;&#12290;&#33258;&#21160;&#29983;&#25104;&#22270;&#20363;&#26631;&#39064;&#26377;&#21161;&#20110;&#23558;&#31185;&#23398;&#25991;&#26723;&#27169;&#22411;&#29702;&#35299;&#36229;&#36234;&#25991;&#26412;&#65292;&#24110;&#21161;&#20316;&#32773;&#32534;&#20889;&#26377;&#21161;&#20110;&#20256;&#36798;&#31185;&#23398;&#30740;&#31350;&#32467;&#26524;&#30340;&#20449;&#24687;&#24615;&#26631;&#39064;&#12290;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#31185;&#23398;&#22270;&#20363;&#26631;&#39064;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#22270;&#20687;&#26631;&#39064;&#29983;&#25104;&#20219;&#21153;&#65292;&#27169;&#22411;&#38656;&#35201;&#20351;&#29992;&#36328;&#27169;&#24577;&#23884;&#20837;&#30340;&#30693;&#35782;&#36827;&#34892;&#26631;&#39064;&#29983;&#25104;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22823;&#35268;&#27169;&#30340;SciCap&#25968;&#25454;&#38598;&#65292;&#22686;&#21152;&#20102;&#25552;&#21040;&#22270;&#29255;&#30340;&#27573;&#33853;&#21644;OCR&#26631;&#35760;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#25351;&#38024;&#32593;&#32476;&#30340;&#22810;&#27169;&#24577;&#21464;&#25442;&#27169;&#22411;M4C-Captioner&#20316;&#20026;&#25105;&#20204;&#30740;&#31350;&#30340;&#22522;&#32447;&#65292;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#21040;&#22270;&#29255;&#30340;&#27573;&#33853;&#20316;&#20026;&#38468;&#21152;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#26497;&#22823;&#22320;&#22686;&#24378;&#20102;&#33258;&#21160;&#26631;&#20934;&#22270;&#29255;&#30340;&#23383;&#24149;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In scholarly documents, figures provide a straightforward way of communicating scientific findings to readers. Automating figure caption generation helps move model understandings of scientific documents beyond text and will help authors write informative captions that facilitate communicating scientific findings. Unlike previous studies, we reframe scientific figure captioning as a knowledge-augmented image captioning task that models need to utilize knowledge embedded across modalities for caption generation. To this end, we extended the large-scale SciCap dataset~\cite{hsu-etal-2021-scicap-generating} to SciCap+ which includes mention-paragraphs (paragraphs mentioning figures) and OCR tokens. Then, we conduct experiments with the M4C-Captioner (a multimodal transformer-based model with a pointer network) as a baseline for our study. Our results indicate that mention-paragraphs serves as additional context knowledge, which significantly boosts the automatic standard image caption eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#23383;&#24149;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#29992;&#25143;&#29305;&#23450;&#25968;&#25454;&#12290;&#36825;&#31181;&#31995;&#32479;&#20855;&#26377;&#23454;&#26102;&#26356;&#26032;&#30340;&#21151;&#33021;&#65292;&#21487;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2306.03476</link><description>&lt;p&gt;
&#25918;&#32622;&#20154;&#31867;&#22312;&#22270;&#20687;&#23383;&#24149;&#24490;&#29615;&#20013;
&lt;/p&gt;
&lt;p&gt;
Putting Humans in the Image Captioning Loop. (arXiv:2306.03476v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#23383;&#24149;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#29992;&#25143;&#29305;&#23450;&#25968;&#25454;&#12290;&#36825;&#31181;&#31995;&#32479;&#20855;&#26377;&#23454;&#26102;&#26356;&#26032;&#30340;&#21151;&#33021;&#65292;&#21487;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149; (IC) &#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#33719;&#30410;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#27491;&#22312;&#36827;&#34892;&#36866;&#24212; IC &#31995;&#32479;&#20197;&#38598;&#25104;&#20154;&#31867;&#21453;&#39304;&#30340;&#24037;&#20316;&#65292;&#30446;&#26631;&#26159;&#20351;&#20854;&#26131;&#20110;&#36866;&#24212;&#29992;&#25143;&#29305;&#23450;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#31435;&#22312;&#39044;&#20808;&#35757;&#32451;&#20102; MS COCO &#25968;&#25454;&#38598;&#19978;&#30340;&#22522;&#30784; IC &#27169;&#22411;&#19978;&#65292;&#35813;&#27169;&#22411;&#20026;&#30475;&#19981;&#35265;&#30340;&#22270;&#20687;&#29983;&#25104;&#23383;&#24149;&#12290;&#28982;&#21518;&#65292;&#29992;&#25143;&#23558;&#33021;&#22815;&#25552;&#20379;&#23545;&#22270;&#29255;&#21644;&#29983;&#25104;/&#39044;&#27979;&#23383;&#24149;&#30340;&#21453;&#39304;&#65292;&#36825;&#23558;&#34987;&#22686;&#21152;&#20197;&#21019;&#24314;&#27169;&#22411;&#36866;&#24212;&#30340;&#20854;&#20182;&#35757;&#32451;&#23454;&#20363;&#12290;&#36825;&#20123;&#38468;&#21152;&#23454;&#20363;&#20351;&#29992;&#36880;&#27493;&#26356;&#26032;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#24182;&#20351;&#29992;&#31232;&#30095;&#20869;&#23384;&#22238;&#25918;&#32452;&#20214;&#20197;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#33021;&#23548;&#33268;&#25913;&#36827;&#30340;&#32467;&#26524;&#65292;&#32780;&#19988;&#36824;&#33021;&#20135;&#29983;&#21487;&#23450;&#21046;&#30340; IC &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Captioning (IC) models can highly benefit from human feedback in the training process, especially in cases where data is limited. We present work-in-progress on adapting an IC system to integrate human feedback, with the goal to make it easily adaptable to user-specific data. Our approach builds on a base IC model pre-trained on the MS COCO dataset, which generates captions for unseen images. The user will then be able to offer feedback on the image and the generated/predicted caption, which will be augmented to create additional training instances for the adaptation of the model. The additional instances are integrated into the model using step-wise updates, and a sparse memory replay component is used to avoid catastrophic forgetting. We hope that this approach, while leading to improved results, will also result in customizable IC models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#29305;&#24449;&#30340;&#20107;&#20214;&#25277;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21305;&#37197;&#23454;&#29616;&#20107;&#20214;&#31867;&#22411;&#30340;&#26816;&#27979;&#21644;&#35770;&#20803;&#35282;&#33394;&#30340;&#25552;&#21462;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312; ACE2005 &#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03469</link><description>&lt;p&gt;
&#22522;&#20110;&#32467;&#26500;&#21270;&#35821;&#20041;&#21305;&#37197;&#30340;&#20107;&#20214;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Joint Event Extraction via Structural Semantic Matching. (arXiv:2306.03469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#29305;&#24449;&#30340;&#20107;&#20214;&#25277;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21305;&#37197;&#23454;&#29616;&#20107;&#20214;&#31867;&#22411;&#30340;&#26816;&#27979;&#21644;&#35770;&#20803;&#35282;&#33394;&#30340;&#25552;&#21462;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312; ACE2005 &#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25277;&#21462;&#26159;&#20449;&#24687;&#25277;&#21462;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#26816;&#27979;&#20107;&#20214;&#25552;&#21450;&#65292;&#24182;&#25214;&#21040;&#30456;&#24212;&#30340;&#35770;&#20803;&#35282;&#33394;&#12290;&#26412;&#25991;&#36890;&#36807;&#32534;&#30721;&#20107;&#20214;&#31867;&#22411;&#30340;&#35821;&#20041;&#29305;&#24449;&#24182;&#19982;&#30446;&#26631;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21305;&#37197;&#65292;&#25552;&#20986;&#20102;&#35821;&#20041;&#31867;&#22411;&#23884;&#20837;&#21644;&#21160;&#24577;&#32467;&#26500;&#32534;&#30721;&#22120;&#27169;&#22359;&#12290;&#21516;&#26102;&#26500;&#24314;&#20102;&#32852;&#21512;&#32467;&#26500;&#21270;&#35821;&#20041;&#21305;&#37197;&#27169;&#22411;&#65292;&#36890;&#36807;&#21452;&#21521;&#27880;&#24847;&#21147;&#23618;&#20849;&#21516;&#25191;&#34892;&#20107;&#20214;&#26816;&#27979;&#21644;&#35770;&#28857;&#25552;&#21462;&#20219;&#21153;&#12290;&#22312;ACE2005&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event Extraction (EE) is one of the essential tasks in information extraction, which aims to detect event mentions from text and find the corresponding argument roles. The EE task can be abstracted as a process of matching the semantic definitions and argument structures of event types with the target text. This paper encodes the semantic features of event types and makes structural matching with target text. Specifically, Semantic Type Embedding (STE) and Dynamic Structure Encoder (DSE) modules are proposed. Also, the Joint Structural Semantic Matching (JSSM) model is built to jointly perform event detection and argument extraction tasks through a bidirectional attention layer. The experimental results on the ACE2005 dataset indicate that our model achieves a significant performance improvement
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21150;&#20844;&#23460;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#35821;&#20041;&#35299;&#37322;&#22120;&#23454;&#29616;&#20102;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#24182;&#25191;&#34892;Office&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2306.03460</link><description>&lt;p&gt;
&#36890;&#36807;&#31243;&#24207;&#32508;&#21512;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;
&lt;/p&gt;
&lt;p&gt;
Natural Language Commanding via Program Synthesis. (arXiv:2306.03460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03460
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21150;&#20844;&#23460;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#65292;&#35821;&#20041;&#35299;&#37322;&#22120;&#23454;&#29616;&#20102;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#24182;&#25191;&#34892;Office&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#29992;&#25143;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#20041;&#35299;&#37322;&#22120;&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#21451;&#22909;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#29983;&#20135;&#21147;&#36719;&#20214;&#65292;&#22914;&#24494;&#36719;Office&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36328;&#24212;&#29992;&#31243;&#24207;&#21151;&#33021;&#25191;&#34892;&#29992;&#25143;&#24847;&#22270;&#12290;&#34429;&#28982;LLM&#22312;&#29702;&#35299;&#20197;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#29992;&#25143;&#24847;&#22270;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#38656;&#35201;&#36229;&#36807;&#25991;&#26412;&#21040;&#25991;&#26412;&#36716;&#25442;&#30340;&#24212;&#29992;&#31243;&#24207;&#29305;&#23450;&#29992;&#25143;&#24847;&#22270;&#30340;&#23454;&#29616;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21150;&#20844;&#22495;&#29305;&#23450;&#35821;&#35328;&#65288;ODSL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#27905;&#12289;&#39640;&#32423;&#21035;&#30340;&#35821;&#35328;&#65292;&#19987;&#38376;&#29992;&#20110;&#22312;Office&#24212;&#29992;&#31243;&#24207;&#20013;&#25191;&#34892;&#25805;&#20316;&#24182;&#19982;&#23454;&#20307;&#20132;&#20114;&#12290;&#35821;&#20041;&#35299;&#37322;&#22120;&#21033;&#29992;&#20998;&#26512;&#26816;&#32034;&#25552;&#31034;&#26500;&#36896;&#26041;&#27861;&#19982;LLM&#36827;&#34892;&#31243;&#24207;&#32508;&#21512;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#29992;&#25143;&#35805;&#35821;&#36716;&#25442;&#20026;&#21487;&#20197;&#34987;&#36716;&#25442;&#20026;&#24212;&#29992;&#31243;&#24207;API&#24182;&#25191;&#34892;&#30340;ODSL&#31243;&#24207;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;Microsoft PowerPoint&#30340;&#30740;&#31350;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Semantic Interpreter, a natural language-friendly AI system for productivity software such as Microsoft Office that leverages large language models (LLMs) to execute user intent across application features. While LLMs are excellent at understanding user intent expressed as natural language, they are not sufficient for fulfilling application-specific user intent that requires more than text-to-text transformations. We therefore introduce the Office Domain Specific Language (ODSL), a concise, high-level language specialized for performing actions in and interacting with entities in Office applications. Semantic Interpreter leverages an Analysis-Retrieval prompt construction method with LLMs for program synthesis, translating natural language user utterances to ODSL programs that can be transpiled to application APIs and then executed. We focus our discussion primarily on a research exploration for Microsoft PowerPoint.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#32469;&#21475;&#20196;&#29983;&#25104;&#30340;&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;TwistList&#25968;&#25454;&#38598;&#21644;TwisterMisters&#22522;&#20934;&#31995;&#32479;&#65292;&#24182;&#39564;&#35777;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#21644;&#26174;&#24335;&#38899;&#38901;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03457</link><description>&lt;p&gt;
&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#35821;&#35328;&#29983;&#25104;&#65306;&#20197;&#32469;&#21475;&#20196;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Phonetically-Grounded Language Generation: The Case of Tongue Twisters. (arXiv:2306.03457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;&#32469;&#21475;&#20196;&#29983;&#25104;&#30340;&#22522;&#20110;&#38899;&#38901;&#23398;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;TwistList&#25968;&#25454;&#38598;&#21644;TwisterMisters&#22522;&#20934;&#31995;&#32479;&#65292;&#24182;&#39564;&#35777;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#21644;&#26174;&#24335;&#38899;&#38901;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#38899;&#38901;&#23398;&#35821;&#35328;&#29983;&#25104;&#20027;&#35201;&#38598;&#20013;&#22312;&#35789;&#27468;&#21644;&#35799;&#27468;&#31561;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22260;&#32469;&#32469;&#21475;&#20196;&#29983;&#25104;&#23637;&#24320;&#30340;&#24037;&#20316;&#65292;&#32469;&#21475;&#20196;&#38656;&#35201;&#22312;&#20445;&#25345;&#35821;&#20041;&#27491;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#26368;&#22823;&#21270;&#38899;&#39057;&#37325;&#21472;&#24182;&#20445;&#25345;&#35821;&#27861;&#27491;&#30830;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;TwistList&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;2.1K&#20154;&#24037;&#32534;&#20889;&#30340;&#32469;&#21475;&#20196;&#30340;&#22823;&#22411;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38024;&#23545;&#32469;&#21475;&#20196;&#29983;&#25104;&#25552;&#20986;&#20102;&#19968;&#20123;&#22522;&#20934;&#31995;&#32479;(TwisterMisters)&#65292;&#21253;&#25324;&#38656;&#35201;&#21644;&#19981;&#38656;&#35201;&#22312;&#22495;&#20869;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#30340;&#32467;&#26524;&#26469;&#35777;&#26126;&#29616;&#26377;&#20027;&#27969;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#24615;&#33021;&#20248;&#33391;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#21644;&#26174;&#24335;&#38899;&#38901;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32469;&#21475;&#20196;&#29983;&#25104;&#30340;&#20219;&#21153;&#26159;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous work in phonetically-grounded language generation has mainly focused on domains such as lyrics and poetry. In this paper, we present work on the generation of tongue twisters - a form of language that is required to be phonetically conditioned to maximise sound overlap, whilst maintaining semantic consistency with an input topic, and still being grammatically correct. We present \textbf{TwistList}, a large annotated dataset of tongue twisters, consisting of 2.1K+ human-authored examples. We additionally present several benchmark systems (referred to as TwisterMisters) for the proposed task of tongue twister generation, including models that both do and do not require training on in-domain data. We present the results of automatic and human evaluation to demonstrate the performance of existing mainstream pre-trained models in this task with limited (or no) task specific training and data, and no explicit phonetic knowledge. We find that the task of tongue twister generation is 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;ASR&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#33655;&#20848;&#35821;&#21475;&#22836;&#38405;&#35835;&#20934;&#30830;&#24615;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#19982;&#20154;&#31867;&#35780;&#20272;&#36798;&#21040;&#30456;&#24403;&#30340;&#19968;&#33268;&#24615;&#65288;MCC = 0.63&#65289;&#65292;&#20854;&#35821;&#35328;&#27169;&#22411;&#20013;&#21253;&#25324;&#38405;&#35835;&#38169;&#35823;&#21487;&#20197;&#25552;&#39640;&#35780;&#20272;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03444</link><description>&lt;p&gt;
&#29992;&#20110;&#38405;&#35835;&#35786;&#26029;&#30340;&#21475;&#35821;&#38405;&#35835;&#20934;&#30830;&#24230;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Automatic Assessment of Oral Reading Accuracy for Reading Diagnostics. (arXiv:2306.03444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;ASR&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#33655;&#20848;&#35821;&#21475;&#22836;&#38405;&#35835;&#20934;&#30830;&#24615;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#19982;&#20154;&#31867;&#35780;&#20272;&#36798;&#21040;&#30456;&#24403;&#30340;&#19968;&#33268;&#24615;&#65288;MCC = 0.63&#65289;&#65292;&#20854;&#35821;&#35328;&#27169;&#22411;&#20013;&#21253;&#25324;&#38405;&#35835;&#38169;&#35823;&#21487;&#20197;&#25552;&#39640;&#35780;&#20272;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#33258;&#21160;&#35780;&#20272;&#38405;&#35835;&#27969;&#30021;&#24230;&#23545;&#26089;&#26399;&#26816;&#27979;&#38405;&#35835;&#22256;&#38590;&#20197;&#21450;&#38543;&#21518;&#21450;&#26102;&#24178;&#39044;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#26356;&#26159;&#22914;&#27492;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;Kaldi&#21644;Whisper&#35780;&#20272;&#20102;&#20845;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;ASR&#30340;&#31995;&#32479;&#65292;&#20197;&#33258;&#21160;&#35780;&#20272;&#33655;&#20848;&#35821;&#21475;&#22836;&#38405;&#35835;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#26368;&#25104;&#21151;&#30340;&#31995;&#32479;&#19982;&#20154;&#31867;&#35780;&#20272;&#36798;&#21040;&#20102;&#30456;&#24403;&#30340;&#19968;&#33268;&#24615;&#65288;MCC = 0.63&#65289;&#12290;&#30456;&#21516;&#30340;&#31995;&#32479;&#22312;&#24378;&#21046;&#35299;&#30721;&#32622;&#20449;&#24230;&#24471;&#20998;&#21644;&#21333;&#35789;&#27491;&#30830;&#24615;&#20043;&#38388;&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#30456;&#20851;&#24615;&#65288;r = 0.45&#65289;&#12290;&#35813;&#31995;&#32479;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30001;&#27979;&#35797;&#25968;&#25454;&#30340;&#25163;&#21160;&#27491;&#23383;&#27861;&#36716;&#24405;&#21644;&#38405;&#35835;&#25552;&#31034;&#32452;&#25104;&#65292;&#36825;&#34920;&#26126;&#22312;LM&#20013;&#21253;&#25324;&#38405;&#35835;&#38169;&#35823;&#21487;&#20197;&#25552;&#39640;&#35780;&#20272;&#24615;&#33021;&#12290;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#24320;&#21457;&#33258;&#21160;&#35780;&#20272;&#31995;&#32479;&#30340;&#24847;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#21487;&#33021;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic assessment of reading fluency using automatic speech recognition (ASR) holds great potential for early detection of reading difficulties and subsequent timely intervention. Precise assessment tools are required, especially for languages other than English. In this study, we evaluate six state-of-the-art ASR-based systems for automatically assessing Dutch oral reading accuracy using Kaldi and Whisper. Results show our most successful system reached substantial agreement with human evaluations (MCC = .63). The same system reached the highest correlation between forced decoding confidence scores and word correctness (r = .45). This system's language model (LM) consisted of manual orthographic transcriptions and reading prompts of the test data, which shows that including reading errors in the LM improves assessment performance. We discuss the implications for developing automatic assessment systems and identify possible avenues of future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;ASR&#27169;&#22411;&#33719;&#21462;&#33258;&#21160;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#24739;&#32773;&#30340;&#35821;&#38899;&#36716;&#24405;&#65292;&#25506;&#32034;&#20102;&#21152;&#20837;&#26631;&#28857;&#31526;&#21495;&#21644;&#20572;&#39039;&#20449;&#24687;&#23545;&#20110;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#20572;&#39039;&#32534;&#30721;&#23545;&#20110;&#25163;&#21160;&#21644;ASR&#36716;&#24405;&#22312;&#25152;&#26377;&#25506;&#31350;&#30340;&#26041;&#27861;&#20013;&#26377;&#21161;&#20110;AD&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2306.03443</link><description>&lt;p&gt;
&#22522;&#20110;ASR&#36716;&#24405;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;: &#25506;&#32034;&#26631;&#28857;&#31526;&#21495;&#21644;&#20572;&#39039;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Alzheimer Disease Classification through ASR-based Transcriptions: Exploring the Impact of Punctuation and Pauses. (arXiv:2306.03443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;ASR&#27169;&#22411;&#33719;&#21462;&#33258;&#21160;&#30340;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#24739;&#32773;&#30340;&#35821;&#38899;&#36716;&#24405;&#65292;&#25506;&#32034;&#20102;&#21152;&#20837;&#26631;&#28857;&#31526;&#21495;&#21644;&#20572;&#39039;&#20449;&#24687;&#23545;&#20110;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#34920;&#26126;&#20572;&#39039;&#32534;&#30721;&#23545;&#20110;&#25163;&#21160;&#21644;ASR&#36716;&#24405;&#22312;&#25152;&#26377;&#25506;&#31350;&#30340;&#26041;&#27861;&#20013;&#26377;&#21161;&#20110;AD&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;(AD)&#26159;&#19990;&#30028;&#19978;&#26368;&#24120;&#35265;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#20043;&#19968;&#65292;&#24120;&#24120;&#20276;&#38543;&#30528;&#20132;&#27969;&#22256;&#38590;&#12290;&#20998;&#26512;&#35821;&#38899;&#21487;&#20197;&#20316;&#20026;&#35786;&#26029;&#24037;&#20855;&#26469;&#35782;&#21035;&#35813;&#30149;&#12290;&#26368;&#36817;&#30340;ADReSS&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;AD&#20998;&#31867;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#31361;&#20986;&#20102;&#25163;&#21160;&#36716;&#24405;&#30340;&#23454;&#29992;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#26032;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;Whisper&#26469;&#33719;&#21462;&#36716;&#24405;&#65292;&#20854;&#20013;&#36824;&#21253;&#25324;&#33258;&#21160;&#26631;&#28857;&#31526;&#21495;&#12290;&#20998;&#31867;&#27169;&#22411;&#32467;&#21512;&#39044;&#35757;&#32451;&#30340;FastText&#23383;&#23884;&#20837;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#25163;&#21160;&#21644;ASR&#36716;&#24405;&#20013;&#20998;&#21035;&#23454;&#29616;&#20102;0.854&#21644;0.833&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#24471;&#20998;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#36716;&#24405;&#20013;&#21253;&#21547;&#20572;&#39039;&#20449;&#24687;&#21644;&#26631;&#28857;&#31526;&#21495;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#65292;&#26631;&#28857;&#31526;&#21495;&#21482;&#33021;&#20135;&#29983;&#36731;&#24494;&#30340;&#25913;&#36827;&#65292;&#32780;&#20572;&#39039;&#32534;&#30721;&#21487;&#20197;&#24110;&#21161;&#25163;&#21160;&#21644;ASR&#36716;&#24405;&#22312;&#25152;&#26377;&#25506;&#31350;&#30340;&#26041;&#27861;&#20013;&#36827;&#34892;AD&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Alzheimer's Disease (AD) is the world's leading neurodegenerative disease, which often results in communication difficulties. Analysing speech can serve as a diagnostic tool for identifying the condition. The recent ADReSS challenge provided a dataset for AD classification and highlighted the utility of manual transcriptions. In this study, we used the new state-of-the-art Automatic Speech Recognition (ASR) model Whisper to obtain the transcriptions, which also include automatic punctuation. The classification models achieved test accuracy scores of 0.854 and 0.833 combining the pretrained FastText word embeddings and recurrent neural networks on manual and ASR transcripts respectively. Additionally, we explored the influence of including pause information and punctuation in the transcriptions. We found that punctuation only yielded minor improvements in some cases, whereas pause encoding aided AD classification for both manual and ASR transcriptions across all approaches investigated.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#34917;&#20840;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#24182;&#21457;&#29616;&#36825;&#20123;&#28431;&#27934;&#26174;&#33879;&#38477;&#20302;&#20102;Code-LLMs&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03438</link><description>&lt;p&gt;
&#20195;&#30721;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22635;&#20889;&#21487;&#33021;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#26102;&#23384;&#22312;&#22833;&#36133;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Large Language Models of Code Fail at Completing Code with Potential Bugs. (arXiv:2306.03438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#34917;&#20840;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#24182;&#21457;&#29616;&#36825;&#20123;&#28431;&#27934;&#26174;&#33879;&#38477;&#20302;&#20102;Code-LLMs&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20195;&#30721;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;Code-LLMs&#65289;&#22312;&#20195;&#30721;&#34917;&#20840;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#36825;&#26159;&#32534;&#31243;&#36741;&#21161;&#21644;&#20195;&#30721;&#26234;&#33021;&#30340;&#22522;&#26412;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#24573;&#30053;&#20102;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20195;&#30721;&#19978;&#19979;&#25991;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#28431;&#27934;&#38382;&#39064;&#65292;&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#36825;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#30740;&#31350;&#20102;&#23384;&#22312;&#28431;&#27934;&#30340;&#20195;&#30721;&#34917;&#20840;&#38382;&#39064;&#65292;&#21463;&#23454;&#26102;&#20195;&#30721;&#24314;&#35758;&#30340;&#29616;&#23454;&#22330;&#26223;&#21551;&#21457;&#65292;&#20195;&#30721;&#19978;&#19979;&#25991;&#20013;&#21253;&#21547;&#21487;&#33021;&#30340;&#28431;&#27934;-&#21453;&#27169;&#24335;&#65292;&#36825;&#20123;&#21453;&#27169;&#24335;&#21487;&#20197;&#25104;&#20026;&#23436;&#25104;&#31243;&#24207;&#20013;&#30340;&#28431;&#27934;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#30740;&#31350;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#26159;&#20174;&#35821;&#20041;&#25913;&#21464;&#25805;&#20316;&#20013;&#27966;&#29983;&#30340;&#21512;&#25104;&#28431;&#27934;&#25968;&#25454;&#38598;&#65288;buggy-HumanEval&#65289;&#65292;&#21478;&#19968;&#20010;&#26159;&#20174;&#29992;&#25143;&#25552;&#20132;&#30340;&#32534;&#31243;&#38382;&#39064;&#20013;&#27966;&#29983;&#30340;&#29616;&#23454;&#28431;&#27934;&#25968;&#25454;&#38598;&#65288;buggy-FixEval&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21487;&#33021;&#23384;&#22312;&#28431;&#27934;&#30340;&#24773;&#20917;&#26174;&#33879;&#38477;&#20302;&#20102;&#39640;&#24615;&#33021;Code-LLMs&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;CodeGen-2B-mono&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#36890;&#36807;&#29575;
&lt;/p&gt;
&lt;p&gt;
Large language models of code (Code-LLMs) have recently brought tremendous advances to code completion, a fundamental feature of programming assistance and code intelligence. However, most existing works ignore the possible presence of bugs in the code context for generation, which are inevitable in software development. Therefore, we introduce and study the buggy-code completion problem, inspired by the realistic scenario of real-time code suggestion where the code context contains potential bugs -- anti-patterns that can become bugs in the completed program. To systematically study the task, we introduce two datasets: one with synthetic bugs derived from semantics-altering operator changes (buggy-HumanEval) and one with realistic bugs derived from user submissions to coding problems (buggy-FixEval). We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs. For instance, the passing rates of CodeGen-2B-mono on test 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;Prompt-tuning&#22312;&#27880;&#24847;&#21147;&#26550;&#26500;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#25506;&#32034;&#19978;&#19979;&#25991;&#28151;&#21512;&#27169;&#22411;&#65292;&#34920;&#26126;softmax-prompt-attention&#22312;&#34920;&#36798;&#19978;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#30340;&#20351;&#29992;&#25968;&#25454;&#23398;&#20064;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.03435</link><description>&lt;p&gt;
&#20851;&#27880;&#28857;&#23545;Prompt-tuning&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Role of Attention in Prompt-tuning. (arXiv:2306.03435v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;Prompt-tuning&#22312;&#27880;&#24847;&#21147;&#26550;&#26500;&#20013;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#25506;&#32034;&#19978;&#19979;&#25991;&#28151;&#21512;&#27169;&#22411;&#65292;&#34920;&#26126;softmax-prompt-attention&#22312;&#34920;&#36798;&#19978;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#30340;&#20351;&#29992;&#25968;&#25454;&#23398;&#20064;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt-tuning &#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#31574;&#30053;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064; (&#36719;) &#25552;&#31034;&#21442;&#25968;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#22312; LLM &#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110; Prompt-tuning &#30340;&#33021;&#21147;&#21450;&#20851;&#27880;&#26426;&#21046;&#22312;&#25552;&#31034;&#20013;&#30340;&#20316;&#29992;&#65292;&#29702;&#35770;&#29702;&#35299;&#23578;&#26377;&#38480;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#19968;&#20010;&#27880;&#24847;&#21147;&#26550;&#26500;&#30340; Prompt-tuning&#65292;&#24182;&#30740;&#31350;&#19978;&#19979;&#25991;&#28151;&#21512;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#36755;&#20837;&#34920;&#31034;&#23646;&#20110;&#19978;&#19979;&#25991;&#30456;&#20851;&#25110;&#26080;&#20851;&#38598;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#33258;&#21253;&#21547;&#30340;&#25552;&#31034;-&#27880;&#24847;&#21147;&#27169;&#22411;&#26469;&#38548;&#31163; Prompt-tuning &#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22914;&#19979;&#65306;(1) &#25105;&#20204;&#34920;&#26126;&#22312;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#25968;&#25454;&#27169;&#22411;&#19979;&#65292;softmax-prompt-attention &#22312;&#21487;&#35777;&#26126;&#22320;&#27604;softmax-self-attention &#21644;&#32447;&#24615;&#25552;&#31034;&#27880;&#24847;&#21147;&#26356;&#20855;&#34920;&#36798;&#21147;&#12290;(2) &#25105;&#20204;&#20998;&#26512;&#20102;&#28176;&#21464;&#19979;&#38477;&#30340;&#21021;&#22987;&#36712;&#36857;&#65292;&#24182;&#23637;&#31034;&#21487;&#20197;&#36890;&#36807;&#36817;&#20046;&#26368;&#20248;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#23398;&#20064;&#25552;&#31034;&#21644;&#39044;&#27979;&#22836;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#25552;&#31034;&#21487;&#20197;&#35777;&#26126;&#22320;&#27880;&#24847;&#21040;&#31232;&#30095;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#20449;&#24687;&#12290;(3)
&lt;/p&gt;
&lt;p&gt;
Prompt-tuning is an emerging strategy to adapt large language models (LLM) to downstream tasks by learning a (soft-)prompt parameter from data. Despite its success in LLMs, there is limited theoretical understanding of the power of prompt-tuning and the role of the attention mechanism in prompting. In this work, we explore prompt-tuning for one-layer attention architectures and study contextual mixture-models where each input token belongs to a context-relevant or -irrelevant set. We isolate the role of prompt-tuning through a self-contained prompt-attention model. Our contributions are as follows: (1) We show that softmax-prompt-attention is provably more expressive than softmax-self-attention and linear-prompt-attention under our contextual data model. (2) We analyze the initial trajectory of gradient descent and show that it learns the prompt and prediction head with near-optimal sample complexity and demonstrate how prompt can provably attend to sparse context-relevant tokens. (3) 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24847;&#22270;&#24863;&#30693;FAQ&#26816;&#32034;&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#22312;&#21830;&#21697;&#25628;&#32034;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#24847;&#22270;&#20998;&#31867;&#22120;&#21644;&#37325;&#26500;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#26816;&#32034;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.03411</link><description>&lt;p&gt;
&#21830;&#21697;&#25628;&#32034;&#20013;&#30340;&#24847;&#22270;&#24863;&#30693;FAQ&#26816;&#32034;&#65306;&#29983;&#25104;-&#26816;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generate-then-Retrieve: Intent-Aware FAQ Retrieval in Product Search. (arXiv:2306.03411v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24847;&#22270;&#24863;&#30693;FAQ&#26816;&#32034;&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#22312;&#21830;&#21697;&#25628;&#32034;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#24847;&#22270;&#20998;&#31867;&#22120;&#21644;&#37325;&#26500;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#26816;&#32034;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#21830;&#21697;&#25628;&#32034;&#24341;&#25806;&#20132;&#20114;&#30340;&#23458;&#25143;&#36234;&#26469;&#36234;&#22810;&#22320;&#21046;&#23450;&#20449;&#24687;&#26597;&#35810;&#35831;&#27714;&#12290;&#24120;&#38382;&#38382;&#39064;&#65288;FAQ&#65289;&#26816;&#32034;&#26088;&#22312;&#36890;&#36807;&#38382;&#39064;&#24847;&#22270;&#26469;&#26816;&#32034;&#29992;&#25143;&#26597;&#35810;&#30340;&#24120;&#35265;&#38382;&#39064;-&#31572;&#26696;&#23545;&#12290;&#23558;FAQ&#26816;&#32034;&#19982;&#21830;&#21697;&#25628;&#32034;&#38598;&#25104;&#22312;&#19968;&#36215;&#65292;&#19981;&#20165;&#21487;&#20197;&#20351;&#29992;&#25143;&#20570;&#20986;&#26356;&#26126;&#26234;&#30340;&#36141;&#20080;&#20915;&#31574;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#39640;&#25928;&#30340;&#21806;&#21518;&#25903;&#25345;&#22686;&#24378;&#29992;&#25143;&#20445;&#30041;&#29575;&#12290;&#22312;&#21830;&#21697;&#25628;&#32034;&#20013;&#30830;&#23450;&#20309;&#26102;&#21487;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#30340;FAQ&#26465;&#30446;&#65292;&#32780;&#19981;&#20250;&#25171;&#25200;&#20854;&#36141;&#29289;&#20307;&#39564;&#65292;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24847;&#22270;&#24863;&#30693;FAQ&#26816;&#32034;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#25324;&#65288;1&#65289;&#19968;&#20010;&#24847;&#22270;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#39044;&#27979;FAQ&#26159;&#21542;&#33021;&#22815;&#22238;&#31572;&#29992;&#25143;&#30340;&#38382;&#39064;&#65307;&#65288;2&#65289;&#19968;&#20010;&#37325;&#26500;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#26597;&#35810;&#37325;&#20889;&#20026;&#33258;&#28982;&#38382;&#39064;&#12290;&#31163;&#32447;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#31995;&#32479;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#32034;&#22522;&#20934;FAQ&#26102;&#23558;Hit @ 1&#25552;&#39640;&#20102;13&#65285;&#65292;&#21516;&#26102;&#23558;&#24310;&#36831;&#38477;&#20302;&#20102;95&#65285;&#12290;&#36825;&#20123;&#25913;&#36827;&#32467;&#26524;&#35828;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#24847;&#22270;&#24863;&#30693;FAQ&#26816;&#32034;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customers interacting with product search engines are increasingly formulating information-seeking queries. Frequently Asked Question (FAQ) retrieval aims to retrieve common question-answer pairs for a user query with question intent. Integrating FAQ retrieval in product search can not only empower users to make more informed purchase decisions, but also enhance user retention through efficient post-purchase support. Determining when an FAQ entry can satisfy a user's information need within product search, without disrupting their shopping experience, represents an important challenge. We propose an intent-aware FAQ retrieval system consisting of (1) an intent classifier that predicts when a user's information need can be answered by an FAQ; (2) a reformulation model that rewrites a query into a natural question. Offline evaluation demonstrates that our approach improves Hit@1 by 13% on retrieving ground-truth FAQs, while reducing latency by 95% compared to baseline systems. These impr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21830;&#19994;&#29615;&#22659;&#30340;&#12289;&#33021;&#22815;&#24179;&#34913;&#23545;&#35805;&#27969;&#30021;&#24615;&#21644;&#36235;&#21521;&#20110;&#29702;&#35299;&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#25968;&#25454;&#38598;&#28151;&#21512;&#12289;&#36127;&#35282;&#33394;&#20449;&#24687;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#21450;&#35774;&#35745;&#20010;&#24615;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102; $\textit{WHAT}$&#12289;$\textit{WHEN}$&#21644;$\textit{HOW}$ &#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#23545;&#35805;&#31995;&#32479;&#21709;&#24212;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03361</link><description>&lt;p&gt;
&#35774;&#35745;&#29992;&#25143;&#35282;&#33394;&#24863;&#30693;&#30340;&#23545;&#35805;&#20195;&#29702;&#36827;&#34892;&#26377;&#36259;&#30340;&#23545;&#35805;&#65306;$\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$ to Ground
&lt;/p&gt;
&lt;p&gt;
$\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$ to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue. (arXiv:2306.03361v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21830;&#19994;&#29615;&#22659;&#30340;&#12289;&#33021;&#22815;&#24179;&#34913;&#23545;&#35805;&#27969;&#30021;&#24615;&#21644;&#36235;&#21521;&#20110;&#29702;&#35299;&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#25968;&#25454;&#38598;&#28151;&#21512;&#12289;&#36127;&#35282;&#33394;&#20449;&#24687;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#21450;&#35774;&#35745;&#20010;&#24615;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102; $\textit{WHAT}$&#12289;$\textit{WHEN}$&#21644;$\textit{HOW}$ &#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#23545;&#35805;&#31995;&#32479;&#21709;&#24212;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#31435;&#20010;&#24615;&#21270;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#20197;&#35299;&#20915;&#21830;&#19994;&#35774;&#32622;&#20013;&#28041;&#21450;&#20010;&#24615;&#21270;&#23545;&#35805;&#21709;&#24212;&#19982;&#38750;&#27491;&#24335;&#21709;&#24212;&#20132;&#26367;&#30340;$\textit{WWH}$&#65288;$\textit{WHAT}$&#12289;$\textit{WHEN}$&#21644;$\textit{HOW}$&#65289;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#21152;&#26435;&#25968;&#25454;&#38598;&#28151;&#21512;&#12289;&#36127;&#35282;&#33394;&#20449;&#24687;&#22686;&#24378;&#26041;&#27861;&#20197;&#21450;&#35774;&#35745;&#20010;&#24615;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#20197;&#24212;&#23545;&#20010;&#24615;&#21270;&#12289;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;$\textit{WWH}$&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#26377;&#25928;&#22320;&#24179;&#34913;&#20102;&#23545;&#35805;&#27969;&#30021;&#24615;&#21644;&#36235;&#21521;&#20110;&#29702;&#35299;&#23545;&#35805;&#31995;&#32479;&#65292;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#21709;&#24212;&#31867;&#22411;&#26631;&#31614;&#26469;&#25552;&#39640;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#32452;&#21512;&#23548;&#33268;&#20102;&#26356;&#21152;&#27969;&#30021;&#30340;&#23545;&#35805;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#20027;&#35266;&#20154;&#31867;&#35780;&#20272;&#21644;&#23458;&#35266;&#35780;&#20272;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a method for building a personalized open-domain dialogue system to address the $\textit{WWH}$ ($\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed approach involves weighted dataset blending, negative persona information augmentation methods, and the design of personalized conversation datasets to address the challenges of $\textit{WWH}$ in personalized, open-domain dialogue systems. Our work effectively balances dialogue fluency and tendency to ground, while also introducing a response-type label to improve the controllability and explainability of the grounded responses. The combination of these methods leads to more fluent conversations, as evidenced by subjective human evaluations as well as objective evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;BatchSampler&#65292;&#36890;&#36807;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#37319;&#26679;&#38590;&#20197;&#21306;&#20998;&#30340;&#23454;&#20363;&#30340;&#23567;&#25209;&#37327;&#65292;&#24182;&#21033;&#29992;&#37325;&#21551;&#38543;&#26426;&#28216;&#36208;&#26469;&#24418;&#25104;&#23567;&#25209;&#37327;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.03355</link><description>&lt;p&gt;
BatchSampler&#65306;&#29992;&#20110;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22270;&#24418;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#23567;&#25209;&#37327;&#37319;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
BatchSampler: Sampling Mini-Batches for Contrastive Learning in Vision, Language, and Graphs. (arXiv:2306.03355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;BatchSampler&#65292;&#36890;&#36807;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#37319;&#26679;&#38590;&#20197;&#21306;&#20998;&#30340;&#23454;&#20363;&#30340;&#23567;&#25209;&#37327;&#65292;&#24182;&#21033;&#29992;&#37325;&#21551;&#38543;&#26426;&#28216;&#36208;&#26469;&#24418;&#25104;&#23567;&#25209;&#37327;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
In-Batch&#23545;&#27604;&#23398;&#20064;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#65292;&#23427;&#23558;&#35821;&#20041;&#30456;&#20284;&#30340;&#23454;&#20363;&#32858;&#38598;&#22312;&#19968;&#36215;&#65292;&#21516;&#26102;&#23558;&#19981;&#30456;&#20284;&#30340;&#23454;&#20363;&#25512;&#21040;&#36828;&#31163;mini-batch&#20043;&#22806;&#12290;&#20854;&#25104;&#21151;&#30340;&#20851;&#38190;&#22312;&#20110;&#36127;&#26679;&#26412;&#20849;&#20139;&#31574;&#30053;&#65292;&#20854;&#20013;&#27599;&#20010;&#23454;&#20363;&#37117;&#20316;&#20026;mini-batch&#20013;&#20854;&#20182;&#23454;&#20363;&#30340;&#36127;&#26679;&#26412;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22312;&#24403;&#21069;mini-batch&#33539;&#22260;&#20869;&#37319;&#26679;&#38590;&#36127;&#26679;&#26412;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20854;&#36136;&#37327;&#20165;&#21463;&#38480;&#20110;mini-batch&#26412;&#36523;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#37319;&#26679;mini-batch&#26469;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;BatchSampler&#26469;&#37319;&#26679;&#38590;&#20197;&#21306;&#20998;&#30340;&#65288;&#21363;&#24444;&#27492;&#38590;&#20197;&#21306;&#20998;&#30340;&#22256;&#38590;&#21644;&#30495;&#23454;&#30340;&#36127;&#26679;&#26412;&#65289;&#23454;&#20363;&#30340;&#23567;&#25209;&#37327;&#12290;&#20026;&#20102;&#20351;&#27599;&#20010;&#23567;&#25209;&#37327;&#20855;&#26377;&#26356;&#23569;&#30340;&#20551;&#36127;&#26679;&#26412;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#38543;&#26426;&#36873;&#25321;&#23454;&#20363;&#30340;&#25509;&#36817;&#24230;&#22270;&#12290;&#20026;&#20102;&#24418;&#25104;&#23567;&#25209;&#37327;&#65292;&#25105;&#20204;&#21033;&#29992;&#25509;&#36817;&#24230;&#22270;&#19978;&#30340;&#37325;&#21551;&#38543;&#26426;&#28216;&#36208;&#26469;&#36741;&#21161;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-Batch contrastive learning is a state-of-the-art self-supervised method that brings semantically-similar instances close while pushing dissimilar instances apart within a mini-batch. Its key to success is the negative sharing strategy, in which every instance serves as a negative for the others within the mini-batch. Recent studies aim to improve performance by sampling hard negatives \textit{within the current mini-batch}, whose quality is bounded by the mini-batch itself. In this work, we propose to improve contrastive learning by sampling mini-batches from the input data. We present BatchSampler\footnote{The code is available at \url{https://github.com/THUDM/BatchSampler}} to sample mini-batches of hard-to-distinguish (i.e., hard and true negatives to each other) instances. To make each mini-batch have fewer false negatives, we design the proximity graph of randomly-selected instances. To form the mini-batch, we leverage random walk with restart on the proximity graph to help sam
&lt;/p&gt;</description></item><item><title>Click&#26159;&#19968;&#31181;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#26550;&#26500;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#24207;&#21015;&#20284;&#28982;&#23545;&#27604;&#25439;&#22833;&#26469;&#26681;&#26412;&#20943;&#23569;&#19981;&#33391;&#23646;&#24615;&#30340;&#29983;&#25104;&#27010;&#29575;&#65292;&#21516;&#26102;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26679;&#26412;&#26500;&#36896;&#31574;&#30053;&#26469;&#26500;&#24314;&#23545;&#27604;&#26679;&#26412;&#12290;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#65292;Click&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#19988;&#26679;&#26412;&#26500;&#36896;&#31574;&#30053;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#26356;&#21152;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2306.03350</link><description>&lt;p&gt;
Click: &#24207;&#21015;&#20284;&#28982;&#23545;&#27604;&#23398;&#20064;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Click: Controllable Text Generation with Sequence Likelihood Contrastive Learning. (arXiv:2306.03350v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03350
&lt;/p&gt;
&lt;p&gt;
Click&#26159;&#19968;&#31181;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#26550;&#26500;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#24207;&#21015;&#20284;&#28982;&#23545;&#27604;&#25439;&#22833;&#26469;&#26681;&#26412;&#20943;&#23569;&#19981;&#33391;&#23646;&#24615;&#30340;&#29983;&#25104;&#27010;&#29575;&#65292;&#21516;&#26102;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26679;&#26412;&#26500;&#36896;&#31574;&#30053;&#26469;&#26500;&#24314;&#23545;&#27604;&#26679;&#26412;&#12290;&#22312;&#30456;&#20851;&#20219;&#21153;&#20013;&#65292;Click&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#19988;&#26679;&#26412;&#26500;&#36896;&#31574;&#30053;&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#26356;&#21152;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#35821;&#35328;&#27169;&#22411;&#36991;&#20813;&#29983;&#25104;&#24102;&#26377;&#19981;&#33391;&#23646;&#24615;&#30340;&#25991;&#26412;&#19968;&#30452;&#26159;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#27604;&#22914;&#26377;&#23475;&#30340;&#35821;&#35328;&#21644;&#19981;&#33258;&#28982;&#30340;&#37325;&#22797;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; Click &#26469;&#36827;&#34892;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#65292;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#19988;&#20415;&#20110;&#20351;&#29992;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#12290;&#23427;&#36890;&#36807;&#37319;&#29992;&#24207;&#21015;&#20284;&#28982;&#23545;&#27604;&#25439;&#22833;&#26469;&#26681;&#26412;&#20943;&#23569;&#36127;&#26679;&#26412;&#30340;&#29983;&#25104;&#27010;&#29575;&#65288;&#21363;&#20855;&#26377;&#19981;&#33391;&#23646;&#24615;&#30340;&#29983;&#25104;&#32467;&#26524;&#65289;&#12290;&#21516;&#26102;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20284;&#28982;&#25490;&#21517;&#31574;&#30053;&#26469;&#26500;&#24314;&#23545;&#27604;&#26679;&#26412;&#12290;&#22312;&#35821;&#35328;&#21435;&#27602;&#12289;&#24773;&#24863;&#35843;&#25972;&#21644;&#20943;&#23569;&#37325;&#22797;&#30340;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102; Click &#20778;&#26044;&#24378;&#22522;&#32447;&#30340;&#21487;&#25511;&#25991;&#23383;&#29983;&#25104;&#65292;&#24182;&#23637;&#31034;&#20102; Click &#30340;&#26679;&#26412;&#26500;&#36896;&#31574;&#30053;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has always been an important yet challenging problem to control language models to avoid generating texts with undesirable attributes, such as toxic language and unnatural repetition. We introduce Click for controllable text generation, which needs no modification to the model architecture and facilitates out-of-the-box use of trained models. It employs a contrastive loss on sequence likelihood, which fundamentally decreases the generation probability of negative samples (i.e., generations with undesirable attributes). It also adopts a novel likelihood ranking-based strategy to construct contrastive samples from model generations. On the tasks of language detoxification, sentiment steering, and repetition reduction, we show that Click outperforms strong baselines of controllable text generation and demonstrate the superiority of Click's sample construction strategy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.03341</link><description>&lt;p&gt;
&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#23548;&#20986;&#30495;&#23454;&#30340;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30495;&#23454;&#24615;&#12290;ITI&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27839;&#30528;&#19968;&#32452;&#26041;&#21521;&#31227;&#21160;&#27169;&#22411;&#28608;&#27963;&#65292;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#12290;&#36825;&#31181;&#24178;&#39044;&#26174;&#30528;&#25552;&#39640;&#20102;LLaMA&#27169;&#22411;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#25351;&#20196;&#24494;&#35843;&#30340;LLaMA Alpaca&#19978;&#65292;ITI&#23558;&#20854;&#30495;&#23454;&#24615;&#20174;32.5&#65285;&#25552;&#39640;&#21040;65.1&#65285;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30495;&#23454;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972;&#24178;&#39044;&#24378;&#24230;&#26469;&#24179;&#34913;&#23427;&#12290;ITI &#21462;&#24471;&#20102;&#26368;&#20302;&#31243;&#24230;&#30340;&#24178;&#25200;&#19988;&#35745;&#31639;&#24265;&#20215;&#12290;&#27492;&#22806;&#65292;&#35813;&#25216;&#26415;&#22312;&#25968;&#25454;&#25928;&#29575;&#19978;&#34920;&#29616;&#20248;&#24322;&#65306;&#34429;&#28982;&#20687;RLHF&#36825;&#26679;&#30340;&#26041;&#27861;&#38656;&#35201;&#24191;&#27867;&#27880;&#37322;&#65292;&#20294;&#26159;ITI&#20165;&#20351;&#29992;&#20102;&#20960;&#30334;&#20010;&#20363;&#23376;&#23601;&#33021;&#23450;&#20301;&#30495;&#23454;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#33021;&#20855;&#26377;&#26576;&#31181;&#20869;&#37096;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#26576;&#20107;&#26159;&#30495;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#34920;&#38754;&#19978;&#20135;&#29983;&#20102;&#34394;&#20551;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
&lt;/p&gt;</description></item><item><title>CoSiNES &#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#36866;&#24212;&#24615;&#24378;&#30340;&#23454;&#20307;&#26631;&#20934;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#21452;&#23376;&#32593;&#32476;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#23454;&#20307;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#22312;&#25216;&#26415;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#30456;&#27604;&#20110;&#22522;&#32447;&#31639;&#27861;&#65292;CoSiNES &#30340;&#31934;&#24230;&#26356;&#39640;&#12289;&#36816;&#34892;&#26102;&#38388;&#26356;&#30701;&#12290;</title><link>http://arxiv.org/abs/2306.03316</link><description>&lt;p&gt;
CoSiNES: &#22522;&#20110;&#23545;&#27604;&#21452;&#23376;&#32593;&#32476;&#30340;&#23454;&#20307;&#26631;&#20934;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CoSiNES: Contrastive Siamese Network for Entity Standardization. (arXiv:2306.03316v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03316
&lt;/p&gt;
&lt;p&gt;
CoSiNES &#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#36866;&#24212;&#24615;&#24378;&#30340;&#23454;&#20307;&#26631;&#20934;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#21452;&#23376;&#32593;&#32476;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32467;&#21512;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#23454;&#20307;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#12290;&#22312;&#25216;&#26415;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#30456;&#27604;&#20110;&#22522;&#32447;&#31639;&#27861;&#65292;CoSiNES &#30340;&#31934;&#24230;&#26356;&#39640;&#12289;&#36816;&#34892;&#26102;&#38388;&#26356;&#30701;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#26631;&#20934;&#21270;&#23558;&#26469;&#33258;&#33258;&#30001;&#25991;&#26412;&#20013;&#30340;&#22122;&#22768;&#25552;&#21450;&#26144;&#23556;&#21040;&#30693;&#35782;&#24211;&#20013;&#30340;&#26631;&#20934;&#23454;&#20307;&#12290;&#30456;&#23545;&#20110;&#20854;&#20182;&#23454;&#20307;&#30456;&#20851;&#20219;&#21153;&#65292;&#36825;&#39033;&#20219;&#21153;&#30340;&#21807;&#19968;&#25361;&#25112;&#26159;&#32570;&#20047;&#21608;&#22260;&#29615;&#22659;&#21644;&#25552;&#21450;&#24418;&#24335;&#30340;&#25968;&#31181;&#21464;&#21270;&#65292;&#29305;&#21035;&#26159;&#22312;&#36328;&#39046;&#22495;&#27867;&#21270;&#26102;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#22823;&#22810;&#38598;&#20013;&#20110;&#24320;&#21457;&#35201;&#20040;&#20005;&#37325;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#27169;&#22411;&#65292;&#35201;&#20040;&#19987;&#38376;&#38024;&#23545;&#26576;&#20010;&#29305;&#23450;&#39046;&#22495;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; CoSiNES&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#23545;&#27604;&#21452;&#23376;&#32593;&#32476;&#30340;&#36890;&#29992;&#21644;&#36866;&#24212;&#24615;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#30340;&#23454;&#20307;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#20013;&#12290;&#25105;&#20204;&#22312;&#25216;&#26415;&#39046;&#22495;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;640&#20010;&#25216;&#26415;&#26632;&#23454;&#20307;&#21644;6412&#20010;&#26469;&#33258;&#24037;&#19994;&#20869;&#23481;&#31649;&#29702;&#31995;&#32479;&#30340;&#25552;&#21450;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102; CoSiNES &#27604;&#22522;&#32447;&#31639;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#31934;&#24230;&#21644;&#26356;&#24555;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity standardization maps noisy mentions from free-form text to standard entities in a knowledge base. The unique challenge of this task relative to other entity-related tasks is the lack of surrounding context and numerous variations in the surface form of the mentions, especially when it comes to generalization across domains where labeled data is scarce. Previous research mostly focuses on developing models either heavily relying on context, or dedicated solely to a specific domain. In contrast, we propose CoSiNES, a generic and adaptable framework with Contrastive Siamese Network for Entity Standardization that effectively adapts a pretrained language model to capture the syntax and semantics of the entities in a new domain.  We construct a new dataset in the technology domain, which contains 640 technical stack entities and 6,412 mentions collected from industrial content management systems. We demonstrate that CoSiNES yields higher accuracy and faster runtime than baselines der
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#25945;&#24072;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#26469;&#25913;&#36827;&#23569;&#26679;&#26412;&#27169;&#22411;&#65292;&#23454;&#29616;&#21516;&#26102;&#29983;&#25104;&#20219;&#21153;&#26631;&#31614;&#21644;&#21407;&#29702;&#30340;&#25928;&#26524;&#65307;&#27492;&#22806;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Masked Label Regularization&#65292;&#21487;&#20197;&#26126;&#30830;&#22320;&#24378;&#21046;&#35299;&#37322;&#26126;&#30830;&#22320;&#26465;&#20214;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.03315</link><description>&lt;p&gt;
&#21452;&#25945;&#24072;&#33258;&#25105;&#35757;&#32451;&#30340;&#23569;&#26679;&#26412;&#21407;&#29702;&#29983;&#25104;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Few Shot Rationale Generation using Self-Training with Dual Teachers. (arXiv:2306.03315v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#25945;&#24072;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#36890;&#36807;&#33258;&#25105;&#35757;&#32451;&#26469;&#25913;&#36827;&#23569;&#26679;&#26412;&#27169;&#22411;&#65292;&#23454;&#29616;&#21516;&#26102;&#29983;&#25104;&#20219;&#21153;&#26631;&#31614;&#21644;&#21407;&#29702;&#30340;&#25928;&#26524;&#65307;&#27492;&#22806;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Masked Label Regularization&#65292;&#21487;&#20197;&#26126;&#30830;&#22320;&#24378;&#21046;&#35299;&#37322;&#26126;&#30830;&#22320;&#26465;&#20214;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#35299;&#37322;&#27169;&#22411;&#21516;&#26102;&#20026;&#20854;&#39044;&#27979;&#30340;&#26631;&#31614;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#26159;&#26500;&#24314;&#21487;&#20449;&#36182;&#30340;AI&#24212;&#29992;&#31243;&#24207;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#30001;&#20110;&#20026;&#27880;&#37322;&#26631;&#31614;&#29983;&#25104;&#35299;&#37322;&#26159;&#19968;&#20010;&#36153;&#21147;&#19988;&#25104;&#26412;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#22240;&#27492;&#36817;&#26399;&#30340;&#27169;&#22411;&#20381;&#36182;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20316;&#20026;&#20854;&#39592;&#24178;&#65292;&#24182;&#19988;&#37319;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#65292;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#23569;&#26679;&#26412;&#27169;&#22411;&#65292;&#20551;&#35774;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#37117;&#27809;&#26377;&#20154;&#24037;&#32534;&#20889;&#30340;&#21407;&#29702;&#25110;&#26631;&#27880;&#20219;&#21153;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#25945;&#24072;&#23398;&#20064;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#25105;&#35757;&#32451;&#21644;&#31934;&#28860;&#20102;&#20004;&#20010;&#19987;&#19994;&#30340;&#25945;&#24072;&#27169;&#22411;&#65292;&#29992;&#20110;&#20219;&#21153;&#39044;&#27979;&#21644;&#29702;&#24615;&#21270;&#65292;&#23558;&#23427;&#20204;&#30340;&#30693;&#35782;&#36716;&#21270;&#20026;&#33021;&#22815;&#20849;&#21516;&#29983;&#25104;&#20219;&#21153;&#26631;&#31614;&#21644;&#21407;&#29702;&#30340;&#22810;&#20219;&#21153;&#23398;&#29983;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21046;&#23450;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25513;&#30721;&#26631;&#31614;&#27491;&#21017;&#21270;&#65288;MLR&#65289;&#65292;&#23558;&#35299;&#37322;&#26126;&#30830;&#22320;&#24378;&#21046;&#26465;&#20214;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-rationalizing models that also generate a free-text explanation for their predicted labels are an important tool to build trustworthy AI applications. Since generating explanations for annotated labels is a laborious and costly pro cess, recent models rely on large pretrained language models (PLMs) as their backbone and few-shot learning. In this work we explore a self-training approach leveraging both labeled and unlabeled data to further improve few-shot models, under the assumption that neither human written rationales nor annotated task labels are available at scale. We introduce a novel dual-teacher learning framework, which learns two specialized teacher models for task prediction and rationalization using self-training and distills their knowledge into a multi-tasking student model that can jointly generate the task label and rationale. Furthermore, we formulate a new loss function, Masked Label Regularization (MLR) which promotes explanations to be strongly conditioned on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26495;&#22359;&#25512;&#26029;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#20027;&#39064;&#22411;&#31169;&#21215;&#32929;&#26435;&#22522;&#37329;&#30340;&#25237;&#36164;&#19987;&#19994;&#20154;&#22763;&#25512;&#26029;&#20844;&#21496;&#25152;&#22312;&#30340;&#34892;&#19994;&#26495;&#22359;&#12290;&#35813;&#31995;&#32479;&#24314;&#31435;&#22312;&#20013;&#22411;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;Prompt+&#27169;&#22411;&#24494;&#35843;&#31243;&#24207;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03313</link><description>&lt;p&gt;
&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#36866;&#24212;&#24615;&#24378;&#30340;&#31995;&#32479;&#29992;&#20110;&#25512;&#26029;&#20844;&#21496;&#30340;&#34892;&#19994;&#26495;&#22359;&#65306;&#22522;&#20110;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;Prompt+&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
A Scalable and Adaptive System to Infer the Industry Sectors of Companies: Prompt + Model Tuning of Generative Language Models. (arXiv:2306.03313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26495;&#22359;&#25512;&#26029;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#20027;&#39064;&#22411;&#31169;&#21215;&#32929;&#26435;&#22522;&#37329;&#30340;&#25237;&#36164;&#19987;&#19994;&#20154;&#22763;&#25512;&#26029;&#20844;&#21496;&#25152;&#22312;&#30340;&#34892;&#19994;&#26495;&#22359;&#12290;&#35813;&#31995;&#32479;&#24314;&#31435;&#22312;&#20013;&#22411;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;Prompt+&#27169;&#22411;&#24494;&#35843;&#31243;&#24207;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31169;&#21215;&#32929;&#26435;&#20844;&#21496;&#36890;&#36807;&#25910;&#36141;&#21644;&#31649;&#29702;&#20844;&#21496;&#26469;&#23454;&#29616;&#39640;&#25910;&#30410;&#65292;&#35768;&#22810;&#31169;&#21215;&#32929;&#26435;&#22522;&#37329;&#26159;&#20027;&#39064;&#22411;&#30340;&#65292;&#24847;&#21619;&#30528;&#25237;&#36164;&#19987;&#19994;&#20154;&#22763;&#35201;&#35206;&#30422;&#23613;&#21487;&#33021;&#22810;&#30340;&#34892;&#19994;&#26495;&#22359;&#65292;&#24182;&#22312;&#36825;&#20123;&#26495;&#22359;&#20013;&#36873;&#25321;&#26377;&#21069;&#36884;&#30340;&#20844;&#21496;&#65292;&#22240;&#27492;&#25512;&#26029;&#20844;&#21496;&#30340;&#26495;&#22359;&#23545;&#20027;&#39064;&#22411;&#31169;&#21215;&#32929;&#26435;&#22522;&#37329;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26631;&#20934;&#21270;&#34892;&#19994;&#26495;&#22359;&#26694;&#26550;&#65292;&#24182;&#35752;&#35770;&#20102;&#20856;&#22411;&#30340;&#25361;&#25112;&#65307;&#28982;&#21518;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#26495;&#22359;&#25512;&#26029;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#26159;&#24314;&#31435;&#22312;&#20013;&#22411;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#65292;&#36890;&#36807;Prompt+&#27169;&#22411;&#24494;&#35843;&#31243;&#24207;&#36827;&#34892;&#24494;&#35843;&#12290;&#37096;&#32626;&#30340;&#27169;&#22411;&#23637;&#31034;&#20102;&#27604;&#24120;&#35265;&#22522;&#32447;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#35813;&#31995;&#32479;&#24050;&#32463;&#20026;&#35768;&#22810;&#31169;&#21215;&#32929;&#26435;&#19987;&#19994;&#20154;&#21592;&#26381;&#21153;&#36229;&#36807;&#19968;&#24180;&#65292;&#24182;&#26174;&#31034;&#20986;&#23545;&#25968;&#25454;&#37327;&#30340;&#33391;&#22909;&#21487;&#25193;&#23637;&#24615;&#21644;&#23545;&#34892;&#19994;&#26495;&#22359;&#21644;/&#25110;&#27880;&#37322;&#30340;&#20219;&#20309;&#21464;&#21270;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Private Equity (PE) firms operate investment funds by acquiring and managing companies to achieve a high return upon selling. Many PE funds are thematic, meaning investment professionals aim to identify trends by covering as many industry sectors as possible, and picking promising companies within these sectors. So, inferring sectors for companies is critical to the success of thematic PE funds. In this work, we standardize the sector framework and discuss the typical challenges; we then introduce our sector inference system addressing these challenges. Specifically, our system is built on a medium-sized generative language model, finetuned with a prompt + model tuning procedure. The deployed model demonstrates a superior performance than the common baselines. The system has been serving many PE professionals for over a year, showing great scalability to data volume and adaptability to any change in sector framework and/or annotation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#24352;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28526;&#27969;&#20013;&#65292;&#36824;&#24212;&#25512;&#24191;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20197; StackOverflow &#20026;&#20363;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03268</link><description>&lt;p&gt;
&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65306;&#30456;&#27604;&#19968;&#38149;&#31909;&#24335;&#27169;&#22411;&#65292;&#21315;&#19975;&#19981;&#35201;&#35753;&#39046;&#22495;&#30340;&#20379;&#32473;&#19981;&#36275;&#21463;&#21040;&#27874;&#21450;
&lt;/p&gt;
&lt;p&gt;
Stack Over-Flowing with Results: The Case for Domain-Specific Pre-Training Over One-Size-Fits-All Models. (arXiv:2306.03268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#24352;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#28526;&#27969;&#20013;&#65292;&#36824;&#24212;&#25512;&#24191;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20197; StackOverflow &#20026;&#20363;&#23637;&#31034;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;OpenAI&#30340;GPT&#31995;&#21015;&#65289;&#20026;NLP&#21644;&#36719;&#20214;&#24037;&#31243;&#24102;&#26469;&#20102;&#26497;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#36861;&#27714;&#22823;&#32780;&#20840;&#30340;&#28526;&#27969;&#24212;&#35813;&#19982;&#38024;&#23545;&#29305;&#23450;&#30446;&#30340;&#12289;&#35268;&#27169;&#36866;&#20013;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#26412;&#25991;&#20197;StackOverflow&#20026;&#20363;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#23545;&#20110;&#36890;&#29992;&#27169;&#22411;&#22312;&#39564;&#35777;&#22256;&#24785;&#24230;&#21644;&#36801;&#31227;&#23398;&#20064;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained neural language models have brought immense progress to both NLP and software engineering. Models in OpenAI's GPT series now dwarf Google's BERT and Meta's RoBERTa, which previously set new benchmarks on a wide range of NLP applications. These models are trained on massive corpora of heterogeneous data from web crawls, which enables them to learn general language patterns and semantic relationships. However, the largest models are both expensive to train and deploy and are often closed-source, so we lack access to their data and design decisions. We argue that this trend towards large, general-purpose models should be complemented with single-purpose, more modestly sized pre-trained models. In this work, we take StackOverflow (SO) as a domain example in which large volumes of rich aligned code and text data is available. We adopt standard practices for pre-training large language models, including using a very large context size (2,048 tokens), batch size (0.5M tokens
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#25351;&#23548;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#22686;&#24378;&#20854;&#21307;&#23398;&#30693;&#35782;&#21644;&#22312;&#29305;&#23450;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312; IMPRESSIONS &#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#27604;&#35768;&#22810;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33258;&#36866;&#24212;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#22312; BioNLP 2023 &#30740;&#35752;&#20250;&#30340;&#20219;&#21153;1B&#65306;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#20013;&#25490;&#21517;&#31532;1&#12290;</title><link>http://arxiv.org/abs/2306.03264</link><description>&lt;p&gt;
RadSum23 &#19978;&#30340; shs-nlp: &#38754;&#21521;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#25351;&#23548;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#19979;&#30340;&#21360;&#35937;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs for Radiology Report Impression Generation. (arXiv:2306.03264v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#25351;&#23548;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#22686;&#24378;&#20854;&#21307;&#23398;&#30693;&#35782;&#21644;&#22312;&#29305;&#23450;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312; IMPRESSIONS &#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#27604;&#35768;&#22810;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33258;&#36866;&#24212;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#22312; BioNLP 2023 &#30740;&#35752;&#20250;&#30340;&#20219;&#21153;1B&#65306;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#20013;&#25490;&#21517;&#31532;1&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT &#21644; Bloomz &#36825;&#26679;&#30340;&#25351;&#23548;&#35843;&#25972;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#22312;&#26222;&#36866;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#29702;&#35299;&#25918;&#23556;&#23398;&#25253;&#21578;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20174;&#25152;&#21457;&#29616;&#30340;&#20869;&#23481;&#29983;&#25104; IMPRESSIONS &#37096;&#20998;&#30340;&#20219;&#21153;&#20013;&#12290;&#36825;&#20123;&#27169;&#22411;&#20250;&#29983;&#25104;&#20887;&#38271;&#25110;&#19981;&#23436;&#25972;&#30340; IMPRESSIONS&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#35757;&#32451;&#26399;&#38388;&#23545;&#21307;&#23398;&#25991;&#26412;&#25968;&#25454;&#30340;&#26333;&#20809;&#19981;&#36275;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#21307;&#23398;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#25351;&#23548;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20197;&#22686;&#24378;&#20854;&#21307;&#23398;&#30693;&#35782;&#21644;&#22312;&#29305;&#23450;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#38646;-shot &#35774;&#32622;&#19979;&#65292;&#36825;&#20010;&#31995;&#32479;&#22312; IMPRESSIONS &#29983;&#25104;&#20219;&#21153;&#19978;&#27604;&#35768;&#22810;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33258;&#36866;&#24212;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#22312; BioNLP 2023 &#30740;&#35752;&#20250;&#30340;&#20219;&#21153;1B&#65306;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#20013;&#25490;&#21517;&#31532;1&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned generative Large language models (LLMs) like ChatGPT and Bloomz possess excellent generalization abilities, but they face limitations in understanding radiology reports, particularly in the task of generating the IMPRESSIONS section from the FINDINGS section. They tend to generate either verbose or incomplete IMPRESSIONS, mainly due to insufficient exposure to medical text data during training. We present a system which leverages large-scale medical text data for domain-adaptive pre-training of instruction-tuned LLMs to enhance its medical knowledge and performance on specific medical tasks. We show that this system performs better in a zero-shot setting than a number of pretrain-and-finetune adaptation methods on the IMPRESSIONS generation task, and ranks 1st among participating systems in Task 1B: Radiology Report Summarization at the BioNLP 2023 workshop.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#21270;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#19988;&#27979;&#35797;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#25928;&#26524;&#26174;&#33879;&#65292;&#21516;&#26102;&#26377;&#25928;&#32531;&#35299;&#20102;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#27874;&#21160;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.03241</link><description>&lt;p&gt;
&#29702;&#35299;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#23545;&#35757;&#32451;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effectiveness of Early Weight Averaging for Training Large Language Models. (arXiv:2306.03241v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#21270;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36136;&#37327;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#19988;&#27979;&#35797;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#25928;&#26524;&#26174;&#33879;&#65292;&#21516;&#26102;&#26377;&#25928;&#32531;&#35299;&#20102;&#35757;&#32451;&#20013;&#30340;&#25439;&#22833;&#27874;&#21160;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#20215;&#39640;&#26114;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#33267;&#25910;&#25947;&#24182;&#19981;&#39640;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#24819;&#27861;&#65292;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#27839;&#30528;&#36712;&#36857;&#36827;&#34892;&#26816;&#26597;&#28857;&#24179;&#22343;&#21270;&#65292;&#20197;&#22312;&#27169;&#22411;&#25910;&#25947;&#20043;&#21069;&#25552;&#39640;&#20854;&#36136;&#37327;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35757;&#32451;&#25110;&#25512;&#29702;&#26399;&#38388;&#19981;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20855;&#26377;10&#20159;&#21040;120&#20159;&#21442;&#25968;&#30340;Pythia LLM&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#24182;&#35777;&#26126;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#21644;&#20013;&#26399;&#38454;&#27573;&#65292;&#36825;&#31181;&#24819;&#27861;&#21487;&#20197;&#21152;&#36895;&#25910;&#25947;&#24182;&#25552;&#39640;&#27979;&#35797;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#25928;&#26524;&#12290;&#25439;&#22833;&#27874;&#21160;&#26159;LLM&#35757;&#32451;&#20013;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65307;&#22312;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#20102;&#20004;&#31181;&#22522;&#30784;&#36712;&#36857;&#30340;&#36825;&#31181;&#24773;&#20917;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#24179;&#22343;&#21270;&#21487;&#20197;&#32531;&#35299;&#36825;&#20004;&#31181;&#24773;&#20917;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#19968;&#20010;&#25317;&#26377;69&#20159;&#21442;&#25968;&#30340;LLM&#65292;&#25105;&#20204;&#30340;&#26089;&#26399;&#26435;&#37325;&#24179;&#22343;&#21270;&#37197;&#26041;&#21487;&#20197;&#33410;&#30465;&#39640;&#36798;4200&#23567;&#26102;&#30340;GPU&#26102;&#38388;&#65292;&#36825;&#23545;&#20113;&#35745;&#31639;&#25104;&#26412;&#26469;&#35828;&#26159;&#26174;&#33879;&#30340;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training LLMs is expensive, and recent evidence indicates training all the way to convergence is inefficient. In this paper, we investigate the ability of a simple idea, checkpoint averaging along the trajectory of a training run to improve the quality of models before they have converged. This approach incurs no extra cost during training or inference. Specifically, we analyze the training trajectories of Pythia LLMs with 1 to 12 billion parameters and demonstrate that, particularly during the early to mid stages of training, this idea accelerates convergence and improves both test and zero-shot generalization. Loss spikes are a well recognized problem in LLM training; in our analysis we encountered two instances of this in the underlying trajectories, and both instances were mitigated by our averaging.  For a 6.9B parameter LLM, for example, our early weight averaging recipe can save upto 4200 hours of GPU time, which corresponds to significant savings in cloud compute costs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#25968;&#25454;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#26399;&#23545;&#19981;&#37325;&#35201;&#30340;&#31034;&#20363;&#36827;&#34892;&#25171;&#20998;&#21644;&#25243;&#24323;&#65292;&#20943;&#23569;&#20102;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#26412;&#65292;&#24182;&#19988;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#21644;&#22235;&#20010;&#32852;&#21512;NLU&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.03208</link><description>&lt;p&gt;
&#25968;&#25454;&#39278;&#39135;&#19979;&#30340;NLU&#65306;NLP&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#21160;&#24577;&#25968;&#25454;&#23376;&#38598;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLU on Data Diets: Dynamic Data Subset Selection for NLP Classification Tasks. (arXiv:2306.03208v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21160;&#24577;&#25968;&#25454;&#20462;&#21098;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#26399;&#23545;&#19981;&#37325;&#35201;&#30340;&#31034;&#20363;&#36827;&#34892;&#25171;&#20998;&#21644;&#25243;&#24323;&#65292;&#20943;&#23569;&#20102;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#26412;&#65292;&#24182;&#19988;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#21644;&#22235;&#20010;&#32852;&#21512;NLU&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20250;&#22686;&#21152;NLU&#24212;&#29992;&#30340;&#25104;&#26412;&#65292;&#24182;&#20173;&#26159;&#24320;&#21457;&#21608;&#26399;&#30340;&#29942;&#39048;&#12290;&#26368;&#36817;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#30740;&#31350;&#20351;&#29992;&#25968;&#25454;&#20462;&#21098;&#26469;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;&#37319;&#29992;&#38745;&#24577;&#26041;&#27861;&#36827;&#34892;&#20462;&#21098;&#25968;&#25454;&#36873;&#25321;&#26159;&#22522;&#20110;&#24494;&#35843;&#20043;&#21069;&#20026;&#27599;&#20010;&#35757;&#32451;&#26679;&#20363;&#35745;&#31639;&#30340;&#24471;&#20998;&#65292;&#36825;&#28041;&#21450;&#37325;&#35201;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#35813;&#24471;&#20998;&#21487;&#33021;&#24182;&#19981;&#20195;&#34920;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#26679;&#20363;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#31934;&#32454;&#29256;&#26412;&#30340;&#21160;&#24577;&#25968;&#25454;&#20462;&#21098;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#23450;&#26399;&#23545;&#19981;&#37325;&#35201;&#30340;&#31034;&#20363;&#36827;&#34892;&#25171;&#20998;&#21644;&#25243;&#24323;&#30340;&#35838;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#19968;&#20010;&#25105;&#20204;&#23558;&#20854;&#25193;&#23637;&#21040;&#32852;&#21512;&#24847;&#22270;&#21644;&#27133;&#20998;&#31867;&#20219;&#21153;&#30340;EL2N&#24230;&#37327;&#21644;&#23545;&#23436;&#25972;&#35757;&#32451;&#38598;&#36827;&#34892;&#30340;&#21021;&#22987;&#24494;&#35843;&#38454;&#27573;&#12290;&#25105;&#20204;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#21644;&#22235;&#20010;&#32852;&#21512;NLU&#25968;&#25454;&#38598;&#19978;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#38745;&#24577;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26102;&#38388;-&#20934;&#30830;&#24615;&#26435;&#34913;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;50&#65285;&#26102;&#20445;&#25345;&#23436;&#20840;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning large language models inflates the costs of NLU applications and remains the bottleneck of development cycles. Recent works in computer vision use data pruning to reduce training time. Pruned data selection with static methods is based on a score calculated for each training example prior to finetuning, which involves important computational overhead. Moreover, the score may not necessarily be representative of sample importance throughout the entire training duration. We propose to address these issues with a refined version of dynamic data pruning, a curriculum which periodically scores and discards unimportant examples during finetuning. Our method leverages an EL2N metric that we extend to the joint intent and slot classification task, and an initial finetuning phase on the full train set. Our results on the GLUE benchmark and four joint NLU datasets show a better time-accuracy trade-off compared to static methods. Our method preserves full accuracy while training on 50%
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25277;&#35937;&#35821;&#27861;&#26641;&#30340;&#38745;&#24577;&#35780;&#20215;&#26694;&#26550;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Python&#20195;&#30721;&#34917;&#20840;&#36136;&#37327;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#25191;&#34892;&#30340;&#35780;&#20272;&#26041;&#27861;&#26356;&#21152;&#39640;&#25928;&#65292;&#36866;&#29992;&#20110;&#23454;&#38469;&#20195;&#30721;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#26410;&#23450;&#20041;&#21517;&#31216;&#26159;&#19968;&#20010;&#24120;&#35265;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2306.03203</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#34917;&#20840;&#30340;&#38745;&#24577;&#35780;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Static Evaluation of Code Completion by Large Language Models. (arXiv:2306.03203v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25277;&#35937;&#35821;&#27861;&#26641;&#30340;&#38745;&#24577;&#35780;&#20215;&#26694;&#26550;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Python&#20195;&#30721;&#34917;&#20840;&#36136;&#37327;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#25191;&#34892;&#30340;&#35780;&#20272;&#26041;&#27861;&#26356;&#21152;&#39640;&#25928;&#65292;&#36866;&#29992;&#20110;&#23454;&#38469;&#20195;&#30721;&#12290;&#30740;&#31350;&#25581;&#31034;&#20102;&#26410;&#23450;&#20041;&#21517;&#31216;&#26159;&#19968;&#20010;&#24120;&#35265;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21033;&#29992;&#20195;&#30721;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#29983;&#20135;&#21147;&#30340;&#30740;&#31350;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#25968;&#20010;&#22522;&#20110;&#25191;&#34892;&#30340;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#26469;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#30340;&#20195;&#30721;&#22312;&#31616;&#21333;&#32534;&#31243;&#38382;&#39064;&#19978;&#30340;&#21151;&#33021;&#27491;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#25191;&#34892;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#39033;&#30446;&#19978;&#25191;&#34892;&#21516;&#26679;&#30340;&#35780;&#20272;&#26159;&#38750;&#24120;&#26114;&#36149;&#30340;&#12290;&#19982;&#27492;&#30456;&#21453;&#30340;&#26159;&#65292;&#21487;&#20197;&#38745;&#24577;&#22320;&#26816;&#27979;&#38169;&#35823;&#32780;&#26080;&#38656;&#36816;&#34892;&#31243;&#24207;&#30340;&#35821;&#27861;&#26816;&#26597;&#24037;&#20855;&#65288;&#22914;&#26412;&#25991;&#20013;&#20351;&#29992;&#30340;&#8220;linter&#8221;&#65289;&#65292;&#23578;&#26410;&#34987;&#24191;&#27867;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25277;&#35937;&#35821;&#27861;&#26641;&#30340;&#38745;&#24577;&#35780;&#20215;&#26694;&#26550;&#65292;&#20197;&#37327;&#21270;Python&#20195;&#30721;&#34917;&#20840;&#20013;&#30340;&#38745;&#24577;&#38169;&#35823;&#12290;&#19982;&#22522;&#20110;&#25191;&#34892;&#30340;&#35780;&#20272;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#26356;&#39640;&#25928;&#65292;&#32780;&#19988;&#36866;&#29992;&#20110;&#23454;&#38469;&#20195;&#30721;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20174;&#24320;&#28304;&#20195;&#30721;&#24211;&#20013;&#25910;&#38598;&#20195;&#30721;&#19978;&#19979;&#25991;&#65292;&#20351;&#29992;&#20844;&#20849;&#27169;&#22411;&#29983;&#25104;&#20102;&#19968;&#30334;&#19975;&#20010;&#20989;&#25968;&#20307;&#12290;&#25105;&#20204;&#30340;&#38745;&#24577;&#20998;&#26512;&#25581;&#31034;&#20102;&#26410;&#23450;&#20041;&#30340;&#21517;&#31216;&#65288;Undefined Name&#65289;&#36825;&#20010;&#24120;&#35265;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models trained on code have shown great potential to increase productivity of software developers. Several execution-based benchmarks have been proposed to evaluate functional correctness of model-generated code on simple programming problems. Nevertheless, it is expensive to perform the same evaluation on complex real-world projects considering the execution cost. On the contrary, static analysis tools such as linters, which can detect errors without running the program, haven't been well explored for evaluating code generation models. In this work, we propose a static evaluation framework to quantify static errors in Python code completions, by leveraging Abstract Syntax Trees. Compared with execution-based evaluation, our method is not only more efficient, but also applicable to code in the wild. For experiments, we collect code context from open source repos to generate one million function bodies using public models. Our static analysis reveals that Undefined Name a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24503;&#35821;&#26131;&#35835;&#35821;&#35328;&#65288;LS&#65289;&#30340;&#26368;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#21644;&#36164;&#28304;&#65292;&#24182;&#25506;&#35752;&#20102;&#24503;&#22269;LS&#21644;Einfache Sprache&#65288;ES&#65289;&#29616;&#29366;&#12290;</title><link>http://arxiv.org/abs/2306.03189</link><description>&lt;p&gt;
&#24503;&#22269;&#26131;&#35835;&#24615;&#65306;&#30446;&#21069;&#29366;&#20917;&#21450;&#21487;&#29992;&#36164;&#28304;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Easy-to-Read in Germany: A Survey on its Current State and Available Resources. (arXiv:2306.03189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24503;&#35821;&#26131;&#35835;&#35821;&#35328;&#65288;LS&#65289;&#30340;&#26368;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#21644;&#36164;&#28304;&#65292;&#24182;&#25506;&#35752;&#20102;&#24503;&#22269;LS&#21644;Einfache Sprache&#65288;ES&#65289;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26131;&#35835;&#24615;&#35821;&#35328;&#65288;E2R&#65289;&#26159;&#19968;&#31181;&#21463;&#25511;&#21046;&#30340;&#35821;&#35328;&#21464;&#20307;&#65292;&#36890;&#36807;&#20351;&#29992;&#28165;&#26224;&#12289;&#30452;&#25509;&#21644;&#31616;&#21333;&#30340;&#35821;&#35328;&#65292;&#20351;&#20219;&#20309;&#20070;&#38754;&#25991;&#26412;&#26356;&#26131;&#29702;&#35299;&#12290;&#23427;&#20027;&#35201;&#38754;&#21521;&#35748;&#30693;&#25110;&#26234;&#21147;&#27531;&#30142;&#31561;&#30446;&#26631;&#29992;&#25143;&#32676;&#20307;&#12290;&#32780;&#26222;&#36890;&#35821;&#35328;&#65288;PL&#65289;&#21017;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#35821;&#35328;&#26469;&#20256;&#36798;&#20449;&#24687;&#12290;&#24503;&#35821;&#25317;&#26377;&#20854;&#26131;&#35835;&#35821;&#35328;&#29256;&#26412;Leichte Sprache&#65288;LS&#65289;&#21644;Einfache Sprache&#65288;ES&#65289;&#29256;&#26412;&#30340;PL&#12290;&#36817;&#24180;&#26469;&#65292;&#22312;LS&#39046;&#22495;&#36827;&#34892;&#20102;&#37325;&#35201;&#21457;&#23637;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;LS&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#21644;&#36164;&#28304;&#30340;&#26368;&#26032;&#32508;&#36848;&#12290;&#27492;&#22806;&#65292;&#36824;&#26088;&#22312;&#38416;&#36848;&#24503;&#22269;LS&#21644;ES&#30340;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Easy-to-Read Language (E2R) is a controlled language variant that makes any written text more accessible through the use of clear, direct and simple language. It is mainly aimed at people with cognitive or intellectual disabilities, among other target users. Plain Language (PL), on the other hand, is a variant of a given language, which aims to promote the use of simple language to communicate information. German counts with Leichte Sprache (LS), its version of E2R, and Einfache Sprache (ES), its version of PL. In recent years, important developments have been conducted in the field of LS. This paper offers an updated overview of the existing Natural Language Processing (NLP) tools and resources for LS. Besides, it also aims to set out the situation with regard to LS and ES in Germany.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#34913;&#37327;&#20102;&#33521;&#25991;&#21333;&#20010;&#21333;&#35789;&#21644;&#30456;&#20851;&#25991;&#26412;&#30340;&#24418;&#35937;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#21464;&#24418;&#26816;&#27979;&#27169;&#22411;&#26816;&#27979;&#32452;&#21512;&#21464;&#21270;&#24341;&#36215;&#30340;&#33021;&#21147;&#21464;&#21270;&#65292;&#32467;&#26524;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;&#35745;&#31639;&#25514;&#26045;&#33021;&#22815;&#26356;&#21152;&#19968;&#33268;&#22320;&#21709;&#24212;&#32452;&#21512;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.03168</link><description>&lt;p&gt;
&#12298;&#32452;&#21512;&#19982;&#21464;&#24418;&#65306;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#35780;&#20272;&#24418;&#35937;&#21270;&#33021;&#21147;&#12299;
&lt;/p&gt;
&lt;p&gt;
Composition and Deformance: Measuring Imageability with a Text-to-Image Model. (arXiv:2306.03168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#34913;&#37327;&#20102;&#33521;&#25991;&#21333;&#20010;&#21333;&#35789;&#21644;&#30456;&#20851;&#25991;&#26412;&#30340;&#24418;&#35937;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#21464;&#24418;&#26816;&#27979;&#27169;&#22411;&#26816;&#27979;&#32452;&#21512;&#21464;&#21270;&#24341;&#36215;&#30340;&#33021;&#21147;&#21464;&#21270;&#65292;&#32467;&#26524;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;&#35745;&#31639;&#25514;&#26045;&#33021;&#22815;&#26356;&#21152;&#19968;&#33268;&#22320;&#21709;&#24212;&#32452;&#21512;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#23398;&#23398;&#32773;&#21644;&#24515;&#29702;&#23398;&#23478;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#30740;&#31350;&#35821;&#35328;&#23383;&#31526;&#20018;&#22312;&#21548;&#32773;&#25110;&#35835;&#32773;&#20013;&#24341;&#21457;&#24515;&#29702;&#22270;&#20687;&#30340;&#20542;&#21521;&#65292;&#20294;&#22823;&#22810;&#25968;&#35745;&#31639;&#30740;&#31350;&#20165;&#23558;&#36825;&#31181;&#24418;&#35937;&#21270;&#30340;&#27010;&#24565;&#24212;&#29992;&#20110;&#23396;&#31435;&#30340;&#21333;&#35789;&#12290;&#26412;&#25991;&#21033;&#29992;&#26368;&#36817;&#21457;&#23637;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;DALLE mini&#65289;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#29983;&#25104;&#30340;&#22270;&#20687;&#26469;&#27979;&#37327;&#21333;&#20010;&#33521;&#35821;&#21333;&#35789;&#21644;&#30456;&#20851;&#25991;&#26412;&#30340;&#24418;&#35937;&#21270;&#33021;&#21147;&#30340;&#35745;&#31639;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#19977;&#20010;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#25991;&#26412;&#25552;&#31034;&#65292;&#21253;&#25324;&#20154;&#31867;&#29983;&#25104;&#30340;&#22270;&#20687;&#26631;&#39064;&#12289;&#26032;&#38395;&#25991;&#31456;&#21477;&#23376;&#21644;&#35799;&#27468;&#34892;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#25552;&#31034;&#36827;&#34892;&#19981;&#21516;&#30340;&#21464;&#24418;&#65292;&#20197;&#26816;&#26597;&#27169;&#22411;&#26816;&#27979;&#21040;&#30340;&#30001;&#32452;&#21512;&#21464;&#21270;&#24341;&#36215;&#30340;&#24418;&#35937;&#21270;&#33021;&#21147;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#25552;&#20986;&#30340;&#24418;&#35937;&#21270;&#33021;&#21147;&#30340;&#35745;&#31639;&#25514;&#26045;&#19982;&#20154;&#31867;&#23545;&#21333;&#35789;&#30340;&#21028;&#26029;&#20043;&#38388;&#23384;&#22312;&#36739;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#25152;&#25552;&#20986;&#30340;&#25514;&#26045;&#26356;&#19968;&#33268;&#22320;&#21709;&#24212;&#32452;&#21512;&#21464;&#21270;&#32780;&#19981;&#26159;&#22522;&#32447;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21487;&#33021;&#30340;&#24212;&#29992;&#21644;&#26410;&#26469;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although psycholinguists and psychologists have long studied the tendency of linguistic strings to evoke mental images in hearers or readers, most computational studies have applied this concept of imageability only to isolated words. Using recent developments in text-to-image generation models, such as DALLE mini, we propose computational methods that use generated images to measure the imageability of both single English words and connected text. We sample text prompts for image generation from three corpora: human-generated image captions, news article sentences, and poem lines. We subject these prompts to different deformances to examine the model's ability to detect changes in imageability caused by compositional change. We find high correlation between the proposed computational measures of imageability and human judgments of individual words. We also find the proposed measures more consistently respond to changes in compositionality than baseline approaches. We discuss possible 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#30456;&#20851;&#24615;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#21152;&#26435;&#19981;&#21516;&#23545;&#20598;&#30340;&#23545;&#27604;&#25439;&#22833;&#65292;&#20197;&#25913;&#21892;&#26080;&#30417;&#30563;&#26816;&#32034;&#27169;&#22411;&#24615;&#33021;&#65292;&#36827;&#19968;&#27493;&#30340;&#25506;&#32034;&#34920;&#26126;&#23427;&#21487;&#20197;&#20987;&#36133;BM25&#65292;&#20316;&#20026;&#24456;&#22909;&#30340;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#22120;&#12290;</title><link>http://arxiv.org/abs/2306.03166</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#31264;&#23494;&#26816;&#32034;&#21450;&#30456;&#20851;&#24615;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Dense Retrieval with Relevance-Aware Contrastive Pre-Training. (arXiv:2306.03166v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#30456;&#20851;&#24615;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#21152;&#26435;&#19981;&#21516;&#23545;&#20598;&#30340;&#23545;&#27604;&#25439;&#22833;&#65292;&#20197;&#25913;&#21892;&#26080;&#30417;&#30563;&#26816;&#32034;&#27169;&#22411;&#24615;&#33021;&#65292;&#36827;&#19968;&#27493;&#30340;&#25506;&#32034;&#34920;&#26126;&#23427;&#21487;&#20197;&#20987;&#36133;BM25&#65292;&#20316;&#20026;&#24456;&#22909;&#30340;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31264;&#23494;&#26816;&#32034;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#23545;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#35201;&#27714;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#22330;&#26223;&#12290;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#20174;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#26500;&#24314;&#20266;&#27491;&#21521;&#31034;&#20363;&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#22686;&#24191;&#21046;&#20316;&#30340;&#20266;&#27491;&#21521;&#31034;&#20363;&#21487;&#33021;&#19981;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#30456;&#20851;&#24615;&#24863;&#30693;&#23545;&#27604;&#23398;&#20064;&#12290;&#23427;&#23558;&#20013;&#38388;&#35757;&#32451;&#30340;&#27169;&#22411;&#26412;&#36523;&#20316;&#20026;&#19981;&#23436;&#32654;&#30340;&#39044;&#27979;&#22120;&#26469;&#20272;&#35745;&#27491;&#23545;&#20598;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#26681;&#25454;&#20272;&#35745;&#30340;&#30456;&#20851;&#24615;&#33258;&#36866;&#24212;&#22320;&#21152;&#26435;&#19981;&#21516;&#23545;&#20598;&#30340;&#23545;&#27604;&#25439;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;BEIR&#21644;&#24320;&#25918;&#39046;&#22495;&#30340;QA&#26816;&#32034;&#22522;&#20934;&#19978;&#22987;&#32456;&#20248;&#20110;SOTA&#26080;&#30417;&#30563;&#26816;&#32034;&#27169;&#22411;Contriever&#12290;&#36827;&#19968;&#27493;&#30340;&#25506;&#32034;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#22312;&#30446;&#26631;&#35821;&#26009;&#24211;&#19978;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#21518;&#20987;&#36133;BM25&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#20316;&#20026;&#24456;&#22909;&#30340;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#22120;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#22312;https://github.com/Yibin-Lei/ReContriever&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrievers have achieved impressive performance, but their demand for abundant training data limits their application scenarios. Contrastive pre-training, which constructs pseudo-positive examples from unlabeled data, has shown great potential to solve this problem. However, the pseudo-positive examples crafted by data augmentations can be irrelevant. To this end, we propose relevance-aware contrastive learning. It takes the intermediate-trained model itself as an imperfect oracle to estimate the relevance of positive pairs and adaptively weighs the contrastive loss of different pairs according to the estimated relevance. Our method consistently improves the SOTA unsupervised Contriever model on the BEIR and open-domain QA retrieval benchmarks. Further exploration shows that our method can not only beat BM25 after further pre-training on the target corpus but also serves as a good few-shot learner. Our code is publicly available at https://github.com/Yibin-Lei/ReContriever.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#19979;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#25277;&#26679;&#21644;&#25490;&#24207;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#25968;&#23383;&#22696;&#27700;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#36136;&#37327;&#65292;&#36825;&#39033;&#30740;&#31350;&#22312;&#25968;&#23383;&#22696;&#27700;&#39046;&#22495;&#20013;&#39318;&#27425;&#36827;&#34892;&#65292;&#20854;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#20123;&#25216;&#26415;&#33021;&#26174;&#30528;&#25552;&#39640;&#21512;&#25104;&#22696;&#27700;&#30340;&#35782;&#21035;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03103</link><description>&lt;p&gt;
&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#25968;&#23383;&#31508;&#36857;&#29983;&#25104;&#30340;&#25277;&#26679;&#21644;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Sampling and Ranking for Digital Ink Generation on a tight computational budget. (arXiv:2306.03103v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03103
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#19979;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#25277;&#26679;&#21644;&#25490;&#24207;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#25968;&#23383;&#22696;&#27700;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#36136;&#37327;&#65292;&#36825;&#39033;&#30740;&#31350;&#22312;&#25968;&#23383;&#22696;&#27700;&#39046;&#22495;&#20013;&#39318;&#27425;&#36827;&#34892;&#65292;&#20854;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#20123;&#25216;&#26415;&#33021;&#26174;&#30528;&#25552;&#39640;&#21512;&#25104;&#22696;&#27700;&#30340;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#22696;&#27700;&#65288;&#22312;&#32447;&#25163;&#20889;&#65289;&#29983;&#25104;&#26377;&#35768;&#22810;&#28508;&#22312;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#20070;&#20889;&#33258;&#21160;&#23436;&#25104;&#21151;&#33021;&#12289;&#25340;&#20889;&#32416;&#27491;&#21644;&#32654;&#21270;&#31561;&#12290;&#22240;&#20026;&#20889;&#20316;&#26159;&#20010;&#20154;&#30340;&#65292;&#25152;&#20197;&#22788;&#29702;&#36890;&#24120;&#22312;&#35774;&#22791;&#19978;&#23436;&#25104;&#12290;&#22240;&#27492;&#65292;&#22696;&#27700;&#29983;&#25104;&#27169;&#22411;&#38656;&#35201;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#19979;&#24555;&#36895;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20869;&#23481;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#25512;&#29702;&#26102;&#38388;&#39044;&#31639;&#20869;&#26368;&#22823;&#21270;&#35757;&#32451;&#30340;&#25968;&#23383;&#22696;&#27700;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#36136;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#21644;&#27604;&#36739;&#22810;&#31181;&#25277;&#26679;&#21644;&#25490;&#24207;&#25216;&#26415;&#30340;&#25928;&#26524;&#65292;&#36825;&#26159;&#25968;&#23383;&#22696;&#27700;&#39046;&#22495;&#31532;&#19968;&#27425;&#36825;&#26679;&#30340;&#28040;&#34701;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#25105;&#20204;&#30340;&#21457;&#29616;&#8212;&#8212;&#33521;&#35821;&#21644;&#36234;&#21335;&#35821;&#20070;&#20889;&#20197;&#21450;&#25968;&#23398;&#20844;&#24335;&#8212;&#8212;&#20351;&#29992;&#20004;&#31181;&#27169;&#22411;&#31867;&#22411;&#21644;&#20004;&#31181;&#24120;&#35265;&#30340;&#22696;&#27700;&#25968;&#25454;&#34920;&#31034;&#12290;&#22312;&#25152;&#26377;&#32452;&#21512;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#21512;&#25104;&#22696;&#27700;&#30340;&#21487;&#35782;&#21035;&#24615;&#30340;&#26174;&#30528;&#25552;&#39640;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#23383;&#31526;&#25968;&#37327;&#20943;&#23569;&#20102;&#19968;&#21322;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital ink (online handwriting) generation has a number of potential applications for creating user-visible content, such as handwriting autocompletion, spelling correction, and beautification. Writing is personal and usually the processing is done on-device. Ink generative models thus need to produce high quality content quickly, in a resource constrained environment.  In this work, we study ways to maximize the quality of the output of a trained digital ink generative model, while staying within an inference time budget. We use and compare the effect of multiple sampling and ranking techniques, in the first ablation study of its kind in the digital ink domain.  We confirm our findings on multiple datasets - writing in English and Vietnamese, as well as mathematical formulas - using two model types and two common ink data representations. In all combinations, we report a meaningful improvement in the recognizability of the synthetic inks, in some cases more than halving the character
&lt;/p&gt;</description></item><item><title>Video-LLaMA&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#21033;&#29992;&#24050;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#21644;&#21548;&#35273;&#30340;&#29702;&#35299;&#38382;&#39064;&#65292;&#20854;&#20013;Video Q-former&#21644;Audio Q-former&#29992;&#20110;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#19982;&#26102;&#38388;&#21464;&#21270;&#21644;&#38899;&#39057;&#20449;&#21495;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.02858</link><description>&lt;p&gt;
Video-LLaMA&#65306;&#29992;&#20110;&#35270;&#39057;&#29702;&#35299;&#30340;&#25351;&#20196;&#35843;&#25972;&#30340;&#35821;&#38899;-&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding. (arXiv:2306.02858v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02858
&lt;/p&gt;
&lt;p&gt;
Video-LLaMA&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#21033;&#29992;&#24050;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#21644;&#21548;&#35273;&#30340;&#29702;&#35299;&#38382;&#39064;&#65292;&#20854;&#20013;Video Q-former&#21644;Audio Q-former&#29992;&#20110;&#22788;&#29702;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#19982;&#26102;&#38388;&#21464;&#21270;&#21644;&#38899;&#39057;&#20449;&#21495;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26694;&#26550;Video-LLaMA&#65292;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29702;&#35299;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#21644;&#21548;&#35273;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;Video-LLaMA&#20174;&#24050;&#32463;&#39044;&#35757;&#32451;&#22909;&#30340;&#35270;&#35273;&#21644;&#38899;&#39057;&#32534;&#30721;&#22120;&#20197;&#21450;&#24050;&#32463;&#20923;&#32467;&#30340;LLMs&#36827;&#34892;&#36328;&#27169;&#24577;&#35757;&#32451;&#12290;&#30456;&#27604;&#20110;&#20043;&#21069;&#19987;&#27880;&#20110;&#38745;&#24577;&#22270;&#20687;&#29702;&#35299;&#30340;&#35270;&#35273;-LLMs&#65292;&#22914;MiniGPT-4&#21644;LLaVA&#65292;Video-LLaMA&#20027;&#35201;&#35299;&#20915;&#20004;&#20010;&#35270;&#39057;&#29702;&#35299;&#26041;&#38754;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#25429;&#25417;&#35270;&#35273;&#22330;&#26223;&#20013;&#30340;&#26102;&#38388;&#21464;&#21270;&#65292;&#65288;2&#65289;&#38598;&#25104;&#38899;&#39057;&#35270;&#35273;&#20449;&#21495;&#12290;&#20026;&#20102;&#20811;&#26381;&#31532;&#19968;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;Video Q-former&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#32452;&#35013;&#21040;&#25105;&#20204;&#30340;&#35270;&#39057;&#32534;&#30721;&#22120;&#20013;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#35270;&#39057;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#26469;&#23398;&#20064;&#35270;&#39057;-&#35821;&#35328;&#23545;&#24212;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#31532;&#20108;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;ImageBind&#65292;&#19968;&#20010;&#23558;&#22810;&#31181;&#27169;&#24577;&#23545;&#40784;&#30340;&#36890;&#29992;&#23884;&#20837;&#27169;&#22411;&#65292;&#20316;&#20026;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#24182;&#22312;ImageBind&#20043;&#19978;&#24341;&#20837;&#19968;&#20010;Audio Q-former&#65292;&#23398;&#20064;&#21512;&#29702;&#30340;&#21548;&#35273;&#26597;&#35810;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual &amp; audio encoders and the frozen LLMs. Unlike previous vision-LLMs that focus on static image comprehensions such as MiniGPT-4 and LLaVA, Video-LLaMA mainly tackles two challenges in video understanding: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble the pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities as the pre-trained audio encoder, and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for
&lt;/p&gt;</description></item><item><title>Polyglot-Ko&#26159;&#19968;&#31181;&#38889;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#27604;mBERT&#21644;XGLM&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#26377;&#21161;&#20110;&#25913;&#21892;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.02254</link><description>&lt;p&gt;
Polyglot-Ko: &#24320;&#28304;&#22823;&#35268;&#27169;&#38889;&#35821;&#35821;&#35328;&#27169;&#22411;&#30340;&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean Language Models. (arXiv:2306.02254v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02254
&lt;/p&gt;
&lt;p&gt;
Polyglot-Ko&#26159;&#19968;&#31181;&#38889;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#27604;mBERT&#21644;XGLM&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#26377;&#21161;&#20110;&#25913;&#21892;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Polyglot&#26159;&#19968;&#20010;&#26088;&#22312;&#22686;&#24378;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#38750;&#33521;&#35821;&#35821;&#35328;&#24615;&#33021;&#30340;&#24320;&#21019;&#24615;&#39033;&#30446;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Polyglot Korean&#27169;&#22411;&#65292;&#23427;&#26159;&#19987;&#27880;&#20110;&#38889;&#35821;&#32780;&#19981;&#26159;&#22810;&#35821;&#35328;&#24615;&#36136;&#30340;&#12290;&#25105;&#20204;&#19982;TUNiB&#21512;&#20316;&#65292;&#25910;&#38598;&#20102;1.2TB&#30340;&#31934;&#24515;&#31579;&#36873;&#30340;&#38889;&#35821;&#25968;&#25454;&#65292;&#37325;&#28857;&#24320;&#21457;&#20102;&#38889;&#35821;&#27169;&#22411;&#12290; Polyglot-Ko&#27169;&#22411;&#21253;&#25324;&#22810;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#22823;&#23567;&#21644;&#29305;&#24615;&#20197;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#20010;&#38889;&#35821;NLP&#20219;&#21153;&#19978;&#34920;&#29616;&#27604;mBERT&#21644;XGLM&#26356;&#22909;&#65292;&#36827;&#19968;&#27493;&#25903;&#25345;&#20855;&#26377;&#19987;&#38376;&#38024;&#23545;&#26576;&#19968;&#35821;&#35328;&#30340;&#27169;&#22411;&#30340;&#38656;&#27714;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polyglot is a pioneering project aimed at enhancing the non-English language performance of multilingual language models. Despite the availability of various multilingual models such as mBERT (Devlin et al., 2019), XGLM (Lin et al., 2022), and BLOOM (Scao et al., 2022), researchers and developers often resort to building monolingual models in their respective languages due to the dissatisfaction with the current multilingual models non-English language capabilities. Addressing this gap, we seek to develop advanced multilingual language models that offer improved performance in non-English languages. In this paper, we introduce the Polyglot Korean models, which represent a specific focus rather than being multilingual in nature. In collaboration with TUNiB, our team collected 1.2TB of Korean data meticulously curated for our research journey. We made a deliberate decision to prioritize the development of Korean models before venturing into multilingual models. This choice was motivated 
&lt;/p&gt;</description></item><item><title>MultiLegalPile&#26159;&#19968;&#20010;689GB&#30340;&#22810;&#35821;&#35328;&#27861;&#24459;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;24&#31181;&#35821;&#35328;&#30340;&#19981;&#21516;&#27861;&#24459;&#25968;&#25454;&#28304;&#65292;&#20801;&#35768;&#22312;&#20844;&#24179;&#20351;&#29992;&#19979;&#38024;&#23545;&#39044;&#35757;&#32451;NLP&#27169;&#22411;&#12290;&#35813;&#35821;&#26009;&#24211;&#20026;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#22312;LexGLUE&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2306.02069</link><description>&lt;p&gt;
MultiLegalPile&#65306;689GB&#30340;&#22810;&#35821;&#35328;&#27861;&#24459;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
MultiLegalPile: A 689GB Multilingual Legal Corpus. (arXiv:2306.02069v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02069
&lt;/p&gt;
&lt;p&gt;
MultiLegalPile&#26159;&#19968;&#20010;689GB&#30340;&#22810;&#35821;&#35328;&#27861;&#24459;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;24&#31181;&#35821;&#35328;&#30340;&#19981;&#21516;&#27861;&#24459;&#25968;&#25454;&#28304;&#65292;&#20801;&#35768;&#22312;&#20844;&#24179;&#20351;&#29992;&#19979;&#38024;&#23545;&#39044;&#35757;&#32451;NLP&#27169;&#22411;&#12290;&#35813;&#35821;&#26009;&#24211;&#20026;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#22312;LexGLUE&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20026;&#27490;&#65292;&#19987;&#19994;&#39046;&#22495;&#65288;&#22914;&#27861;&#24459;&#65289;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#24456;&#23569;&#65292;&#32780;&#19988;&#32463;&#24120;&#20165;&#38480;&#20110;&#33521;&#35821;&#12290;&#25105;&#20204;&#25972;&#29702;&#24182;&#21457;&#24067;&#20102;MultiLegalPile&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#30340;24&#31181;&#35821;&#35328;&#30340;689GB&#35821;&#26009;&#24211;&#12290;MultiLegalPile&#35821;&#26009;&#24211;&#21253;&#25324;&#21508;&#31181;&#35768;&#21487;&#35777;&#30340;&#19981;&#21516;&#27861;&#24459;&#25968;&#25454;&#28304;&#65292;&#20801;&#35768;&#22312;&#20844;&#24179;&#20351;&#29992;&#19979;&#38024;&#23545;&#39044;&#35757;&#32451;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#27169;&#22411;&#65292;&#23545;&#20110;Eurlex Resources&#21644;Legal mC4&#23376;&#38598;&#25317;&#26377;&#26356;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;RoBERTa&#27169;&#22411;&#21644;&#19968;&#20010;&#22810;&#35821;&#35328;Longformer&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#20998;&#21035;&#22312;&#27599;&#31181;&#29305;&#23450;&#35821;&#35328;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;24&#20010;&#21333;&#35821;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;LEXTREME&#19978;&#23545;&#23427;&#20204;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;LexGLUE&#19978;&#23545;&#33521;&#35821;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;LEXTREME&#19978;&#21019;&#36896;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;(SotA)&#65292;&#33521;&#35821;&#27169;&#22411;&#21017;&#22312;LexGLUE&#19978;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#23558;&#25968;&#25454;&#38598;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#20195;&#30721;&#20840;&#37096;&#37322;&#25918;&#22312;&#26368;&#24320;&#25918;&#30340;&#35768;&#21487;&#35777;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large, high-quality datasets are crucial for training Large Language Models (LLMs). However, so far, there are few datasets available for specialized critical domains such as law and the available ones are often only for the English language. We curate and release MultiLegalPile, a 689GB corpus in 24 languages from 17 jurisdictions. The MultiLegalPile corpus, which includes diverse legal data sources with varying licenses, allows for pretraining NLP models under fair use, with more permissive licenses for the Eurlex Resources and Legal mC4 subsets. We pretrain two RoBERTa models and one Longformer multilingually, and 24 monolingual models on each of the language-specific subsets and evaluate them on LEXTREME. Additionally, we evaluate the English and multilingual models on LexGLUE. Our multilingual models set a new SotA on LEXTREME and our English models on LexGLUE. We release the dataset, the trained models, and all of the code under the most open possible licenses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#20851;&#31995;&#25277;&#21462;&#39046;&#22495;&#30340;&#24212;&#29992;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#35752;&#35770;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#24212;&#23545;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.02051</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#20851;&#31995;&#25277;&#21462;&#39046;&#22495;&#30340;&#32508;&#36848;&#65306;&#26368;&#26032;&#36827;&#23637;&#19982;&#26032;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Deep Learning for Relation Extraction: Recent Advances and New Frontiers. (arXiv:2306.02051v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#20851;&#31995;&#25277;&#21462;&#39046;&#22495;&#30340;&#24212;&#29992;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#35752;&#35770;&#20102;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#24212;&#23545;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#26159;&#25351;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#35782;&#21035;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20851;&#31995;&#25277;&#21462;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#22522;&#30784;&#65292;&#20363;&#22914;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12289;&#38382;&#31572;&#21644;&#20449;&#24687;&#26816;&#32034;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20851;&#31995;&#25277;&#21462;&#39046;&#22495;&#21344;&#25454;&#20102;&#20027;&#23548;&#22320;&#20301;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#38543;&#21518;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23558;&#20851;&#31995;&#25277;&#21462;&#30340;&#26368;&#26032;&#25216;&#26415;&#25512;&#21521;&#20102;&#19968;&#20010;&#26032;&#30340;&#39640;&#24230;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20851;&#31995;&#25277;&#21462;&#36164;&#28304;&#65292;&#21253;&#25324;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#20174;&#25991;&#26412;&#34920;&#31034;&#12289;&#19978;&#19979;&#25991;&#32534;&#30721;&#21644;&#19977;&#20803;&#32452;&#39044;&#27979;&#19977;&#20010;&#26041;&#38754;&#23545;&#29616;&#26377;&#24037;&#20316;&#36827;&#34892;&#20998;&#31867;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20851;&#31995;&#25277;&#21462;&#38754;&#20020;&#30340;&#19968;&#20123;&#37325;&#35201;&#25361;&#25112;&#65292;&#24182;&#24635;&#32467;&#20102;&#21487;&#33021;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20123;&#20855;&#26377;&#28508;&#22312;&#21069;&#26223;&#30340;&#26410;&#26469;&#26041;&#21521;&#21644;&#23637;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) involves identifying the relations between entities from unstructured texts. RE serves as the foundation for many natural language processing (NLP) applications, such as knowledge graph completion, question answering, and information retrieval. In recent years, deep neural networks have dominated the field of RE and made noticeable progress. Subsequently, the large pre-trained language models (PLMs) have taken the state-of-the-art of RE to a new level. This survey provides a comprehensive review of existing deep learning techniques for RE. First, we introduce RE resources, including RE datasets and evaluation metrics. Second, we propose a new taxonomy to categorize existing works from three perspectives (text representation, context encoding, and triplet prediction). Third, we discuss several important challenges faced by RE and summarize potential techniques to tackle these challenges. Finally, we outline some promising future directions and prospects in this 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23558;&#30456;&#23545;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#27880;&#20837;&#20855;&#26377;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#21512;&#25104;&#31639;&#26415;&#20219;&#21153;&#65288;MsAT&#65289;&#65292;&#20174;&#32780;&#25552;&#39640;LM&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.01707</link><description>&lt;p&gt;
&#20174;&#31639;&#26415;&#20219;&#21153;&#20013;&#23398;&#20064;&#22810;&#27493;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Multi-step Reasoning from Arithmetic Task. (arXiv:2306.01707v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#23558;&#30456;&#23545;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#27880;&#20837;&#20855;&#26377;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#30340;&#21512;&#25104;&#31639;&#26415;&#20219;&#21153;&#65288;MsAT&#65289;&#65292;&#20174;&#32780;&#25552;&#39640;LM&#22312;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#34987;&#35748;&#20026;&#26159;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24517;&#35201;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;LM&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#25104;&#21151;&#24402;&#22240;&#20110;&#23427;&#20204;&#30340;&#36830;&#32493;&#24605;&#32771;&#65288;CoT&#65289;&#25512;&#29702;&#33021;&#21147;&#65292;&#21363;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#25104;&#36880;&#27493;&#25512;&#29702;&#38142;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#31181;&#33021;&#21147;&#20284;&#20046;&#21482;&#20986;&#29616;&#22312;&#20855;&#26377;&#20016;&#23500;&#21442;&#25968;&#30340;&#27169;&#22411;&#20013;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#22914;&#20309;&#23558;&#30456;&#23545;&#36739;&#23567;&#30340;LM&#19982;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;MsAT&#65288;&#22810;&#27493;&#31639;&#26415;&#20219;&#21153;&#65289;&#36827;&#34892;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#26469;&#27880;&#20837;&#36825;&#31181;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#23398;&#24212;&#29992;&#39064;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#22686;&#24378;LM&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning is regarded as a necessary ability for Language Models (LMs). Recent works demonstrate large LMs' impressive performance in solving math problems. The success is attributed to their Chain-of-Thought (CoT) reasoning abilities, i.e., the ability to decompose complex questions into step-by-step reasoning chains, but such ability seems only to emerge from models with abundant parameters. This work investigates how to incorporate relatively small LMs with the capabilities of multi-step reasoning. We propose to inject such abilities by continually pre-training LMs on a synthetic dataset MsAT, which stands for Multi-step Arithmetic Task. Our experiments on four math word problem datasets show the effectiveness of the proposed method in enhancing LMs' math reasoning abilities.
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;ChatGPT&#29983;&#25104;&#21453;&#20107;&#23454;&#24605;&#32771;&#21551;&#21457;&#38382;&#39064;&#65292;&#25552;&#39640;&#32534;&#31243;&#30005;&#23376;&#25945;&#26448;&#30340;&#23548;&#33322;&#24615;&#21644;&#20114;&#21160;&#24615;&#65292;&#28608;&#21457;&#23398;&#29983;&#30340;&#25209;&#21028;&#24615;&#24605;&#32500;&#12290;</title><link>http://arxiv.org/abs/2306.00551</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#24605;&#32771;&#21551;&#21457;&#38382;&#39064;&#25552;&#21319;&#32534;&#31243;&#30005;&#23376;&#25945;&#26448;
&lt;/p&gt;
&lt;p&gt;
Enhancing Programming eTextbooks with ChatGPT Generated Counterfactual-Thinking-Inspired Questions. (arXiv:2306.00551v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00551
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;ChatGPT&#29983;&#25104;&#21453;&#20107;&#23454;&#24605;&#32771;&#21551;&#21457;&#38382;&#39064;&#65292;&#25552;&#39640;&#32534;&#31243;&#30005;&#23376;&#25945;&#26448;&#30340;&#23548;&#33322;&#24615;&#21644;&#20114;&#21160;&#24615;&#65292;&#28608;&#21457;&#23398;&#29983;&#30340;&#25209;&#21028;&#24615;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#25945;&#26448;&#24050;&#32463;&#25104;&#20026;&#26085;&#24120;&#23398;&#20064;&#20219;&#21153;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#25968;&#23383;&#25945;&#26448;&#29992;&#20110;&#32534;&#31243;&#35838;&#31243;&#12290;&#36890;&#24120;&#65292;&#23398;&#29983;&#20204;&#38590;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#32534;&#31243;&#25945;&#26448;&#65292;&#21487;&#33021;&#30340;&#21407;&#22240;&#26159;&#36825;&#20123;&#25945;&#26448;&#20013;&#29992;&#20110;&#35828;&#26126;&#27010;&#24565;&#30340;&#31034;&#20363;&#20195;&#30721;&#23545;&#23398;&#29983;&#30340;&#20114;&#21160;&#24615;&#19981;&#36275;&#65292;&#22240;&#27492;&#19981;&#36275;&#20197;&#28608;&#21457;&#23398;&#29983;&#36827;&#19968;&#27493;&#25506;&#32034;&#25110;&#29702;&#35299;&#36825;&#20123;&#32534;&#31243;&#31034;&#20363;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21033;&#29992;&#8220;&#21453;&#20107;&#23454;&#8221;&#38382;&#39064;&#22686;&#24378;&#26234;&#33021;&#25945;&#26448;&#23548;&#33322;&#24615;&#30340;&#24819;&#27861;&#65292;&#20351;&#23398;&#29983;&#23545;&#36825;&#20123;&#31243;&#24207;&#36827;&#34892;&#25209;&#21028;&#24615;&#24605;&#32771;&#24182;&#25552;&#39640;&#21487;&#33021;&#30340;&#31243;&#24207;&#29702;&#35299;&#12290;&#21463;&#20197;&#21069;&#26377;&#20851;&#25945;&#32946;&#39046;&#22495;&#30340;&#21453;&#20107;&#23454;&#24605;&#32771;&#35825;&#23548;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;GPT&#29983;&#25104;&#38382;&#39064;&#22686;&#24378;&#25968;&#23383;&#25945;&#31185;&#20070;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Digital textbooks have become an integral part of everyday learning tasks. In this work, we consider the use of digital textbooks for programming classes. Generally, students struggle with utilizing textbooks on programming to the maximum, with a possible reason being that the example programs provided as illustration of concepts in these textbooks don't offer sufficient interactivity for students, and thereby not sufficiently motivating to explore or understand these programming examples better. In our work, we explore the idea of enhancing the navigability of intelligent textbooks with the use of ``counterfactual'' questions, to make students think critically about these programs and enhance possible program comprehension. Inspired from previous works on nudging students on counter factual thinking, we present the possibility to enhance digital textbooks with questions generated using GPT.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.00477</link><description>&lt;p&gt;
&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#21487;&#36870;&#24615;&#65306;&#20174;&#21442;&#25968;&#21040;&#20869;&#23384;&#39640;&#25928;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Make Your Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. (arXiv:2306.00477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23581;&#35797;&#23454;&#29616;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36816;&#29992;&#21487;&#36870;&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24494;&#35843;&#65292;&#24182;&#21457;&#29616;&#22312;&#21021;&#22987;&#21270;&#24494;&#35843;&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#25104;&#21151;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35757;&#32451;&#23569;&#37327;&#21442;&#25968;&#32780;&#19981;&#20250;&#38477;&#20302;&#24615;&#33021;&#65292;&#24182;&#38543;&#30528;PLM&#36234;&#26469;&#36234;&#22823;&#32780;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PEFT&#26041;&#27861;&#19981;&#20855;&#22791;&#20869;&#23384;&#25928;&#29575;&#65292;&#22240;&#20026;&#23427;&#20204;&#20173;&#38656;&#35201;&#23384;&#20648;&#22823;&#37096;&#20998;&#20013;&#38388;&#28608;&#27963;&#20540;&#20197;&#20415;&#35745;&#31639;&#26799;&#24230;&#65292;&#31867;&#20284;&#20110;&#24494;&#35843;&#12290;&#19968;&#20010;&#20943;&#23569;&#28608;&#27963;&#20869;&#23384;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#24212;&#29992;&#21487;&#36870;&#27169;&#22411;&#65292;&#36825;&#26679;&#20013;&#38388;&#28608;&#27963;&#20540;&#23601;&#26080;&#38656;&#32531;&#23384;&#65292;&#21487;&#20197;&#37325;&#26032;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#23558;PLM&#20462;&#25913;&#20026;&#23427;&#30340;&#21487;&#36870;&#21464;&#20307;&#24182;&#36827;&#34892;PEFT&#24182;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#65292;&#22240;&#20026;&#21487;&#36870;&#27169;&#22411;&#20855;&#26377;&#19982;&#24403;&#21069;&#21457;&#24067;&#30340;PLM&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#25991;&#39318;&#20808;&#35843;&#26597;&#29616;&#26377;PEFT&#26041;&#27861;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#35748;&#35782;&#21040;&#22312;&#21021;&#22987;&#21270;PEFT&#26102;&#20445;&#30041;PLM&#30340;&#36215;&#28857;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant with PEFT is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#23383;&#24149;&#65292;&#32780;&#19981;&#30452;&#25509;&#22788;&#29702;&#22270;&#20687;&#12290;&#22312;CIDEr&#25351;&#26631;&#19978;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.00301</link><description>&lt;p&gt;
CapText: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#20869;&#23481;&#21644;&#25551;&#36848;&#29983;&#25104;&#23383;&#24149;
&lt;/p&gt;
&lt;p&gt;
CapText: Large Language Model-based Caption Generation From Image Context and Description. (arXiv:2306.00301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00301
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#23383;&#24149;&#65292;&#32780;&#19981;&#30452;&#25509;&#22788;&#29702;&#22270;&#20687;&#12290;&#22312;CIDEr&#25351;&#26631;&#19978;&#65292;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#22270;&#29255;&#23383;&#24149;&#24448;&#24448;&#26159;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#65292;&#24182;&#19988;&#25552;&#20379;&#26377;&#20851;&#22270;&#20687;&#30340;&#34917;&#20805;&#20449;&#24687;&#65292;&#32780;&#27169;&#22411;&#24448;&#24448;&#29983;&#25104;&#25551;&#36848;&#22270;&#20687;&#35270;&#35273;&#29305;&#24449;&#30340;&#8220;&#25551;&#36848;&#8221;&#12290;&#22312;&#23383;&#24149;&#29983;&#25104;&#26041;&#38754;&#30340;&#30740;&#31350;&#24050;&#25506;&#32034;&#20102;&#20351;&#29992;&#27169;&#22411;&#22312;&#25552;&#20379;&#23545;&#24212;&#30340;&#25551;&#36848;&#25110;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#23383;&#24149;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#19978;&#19979;&#25991;&#20013;&#29983;&#25104;&#23383;&#24149;&#65292;&#32780;&#19981;&#30452;&#25509;&#22788;&#29702;&#22270;&#20687;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#24494;&#35843;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312; CIDEr &#25351;&#26631;&#19978;&#32988;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#65292;&#22914; OSCAR-VinVL&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep-learning models have been shown to perform well on image-to-text datasets, it is difficult to use them in practice for captioning images. This is because \textit{captions} traditionally tend to be context-dependent and offer complementary information about an image, while models tend to produce \textit{descriptions} that describe the visual features of the image. Prior research in caption generation has explored the use of models that generate captions when provided with the images alongside their respective descriptions or contexts. We propose and evaluate a new approach, which leverages existing large language models to generate captions from textual descriptions and context alone, without ever processing the image directly. We demonstrate that after fine-tuning, our approach outperforms current state-of-the-art image-text alignment models like OSCAR-VinVL on this task on the CIDEr metric.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.00107</link><description>&lt;p&gt;
MERT:&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#35821;&#38899;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#30340;&#19968;&#31181;&#24456;&#26377;&#21069;&#26223;&#30340;&#33539;&#20363;&#65292;&#23545;&#20110;&#36328;&#36234;&#38899;&#20048;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35843;&#24615;&#21644;&#38899;&#39640;&#36825;&#26679;&#30340;&#29305;&#27530;&#38899;&#20048;&#30693;&#35782;&#30340;&#24314;&#27169;&#39047;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;&#65292;&#21363;MERT&#12290;&#22312;&#25105;&#20204;&#30340;&#25506;&#32034;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20248;&#31168;&#30340;&#25945;&#24072;&#27169;&#22411;&#32452;&#21512;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#23383;&#31215;&#26497;&#24230;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#32763;&#35793;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.16806</link><description>&lt;p&gt;
GPT&#26159;&#21542;&#20250;&#20135;&#29983;&#26356;&#19981;&#20934;&#30830;&#30340;&#32763;&#35793;?
&lt;/p&gt;
&lt;p&gt;
Do GPTs Produce Less Literal Translations?. (arXiv:2305.16806v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#23383;&#31215;&#26497;&#24230;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#32763;&#35793;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3&#65292;&#24050;&#32463;&#25104;&#20026;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#25110;&#29702;&#35299;&#20219;&#21153;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20219;&#21153;&#20013;&#65292;&#24050;&#26377;&#22810;&#39033;&#30740;&#31350;&#25506;&#32034;&#21033;&#29992;few-shot&#25552;&#31034;&#26426;&#21046;&#20174;LLMs&#20013;&#24341;&#20986;&#26356;&#22909;&#30340;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#30456;&#23545;&#36739;&#23569;&#22320;&#20851;&#27880;&#36825;&#31181;&#32763;&#35793;&#19982;&#26631;&#20934;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#29983;&#25104;&#32763;&#35793;&#30340;&#36136;&#37327;&#24046;&#24322;&#12290;&#26412;&#30740;&#31350;&#20174;&#25991;&#23383;&#23545;&#40784;&#21644;&#21333;&#35843;&#24615;&#31561;&#26041;&#38754;&#65292;&#27604;&#36739;&#20102;GPT&#21644;NMT&#29983;&#25104;&#32763;&#35793;&#30340;&#25991;&#26412;&#25991;&#23383;&#31215;&#26497;&#24230;&#65292;&#21457;&#29616;GPT&#20174;&#33521;&#35821;&#65288;E-X&#65289;&#32763;&#35793;&#30340;&#25991;&#26412;&#26356;&#19981;&#20934;&#30830;&#65292;&#20294;&#22312;MT&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#19968;&#32467;&#26524;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#20063;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#21516;&#26102;&#65292;&#24403;&#32763;&#35793;&#21477;&#23376;&#38271;&#24230;&#22686;&#21152;&#26102;&#65292;&#36825;&#31181;&#24046;&#21035;&#23601;&#23588;&#20026;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose language models capable of addressing many natural language generation or understanding tasks. On the task of Machine Translation (MT), multiple works have investigated few-shot prompting mechanisms to elicit better translations from LLMs. However, there has been relatively little investigation on how such translations differ qualitatively from the translations generated by standard Neural Machine Translation (NMT) models. In this work, we investigate these differences in terms of the literalness of translations produced by the two systems. Using literalness measures involving word alignment and monotonicity, we find that translations out of English (E-X) from GPTs tend to be less literal, while exhibiting similar or better scores on MT quality metrics. We demonstrate that this finding is borne out in human evaluations as well. We then show that these differences are especially pronounced when translating senten
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#39044;&#27979;&#32534;&#30721;&#27169;&#22411;&#65292;&#21487;&#20197;&#25226;&#35828;&#35805;&#32773;&#21644;&#35821;&#38899;&#20449;&#24687;&#32534;&#30721;&#22312;&#27491;&#20132;&#23376;&#31354;&#38388;&#65292;&#36827;&#32780;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#36716;&#24405;&#30340;&#35828;&#35805;&#32773;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;&#35828;&#35805;&#32773;&#20449;&#24687;&#65292;&#24182;&#22312;&#38899;&#20301;&#36776;&#21035;&#20219;&#21153;&#20013;&#20248;&#20110;&#20043;&#21069;&#30340;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.12464</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#27979;&#32534;&#30721;&#27169;&#22411;&#29992;&#27491;&#20132;&#23376;&#31354;&#38388;&#32534;&#30721;&#35828;&#35805;&#32773;&#21644;&#35821;&#38899;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Predictive Coding Models Encode Speaker and Phonetic Information in Orthogonal Subspaces. (arXiv:2305.12464v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12464
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#39044;&#27979;&#32534;&#30721;&#27169;&#22411;&#65292;&#21487;&#20197;&#25226;&#35828;&#35805;&#32773;&#21644;&#35821;&#38899;&#20449;&#24687;&#32534;&#30721;&#22312;&#27491;&#20132;&#23376;&#31354;&#38388;&#65292;&#36827;&#32780;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#36716;&#24405;&#30340;&#35828;&#35805;&#32773;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#26377;&#25928;&#28040;&#38500;&#20102;&#35828;&#35805;&#32773;&#20449;&#24687;&#65292;&#24182;&#22312;&#38899;&#20301;&#36776;&#21035;&#20219;&#21153;&#20013;&#20248;&#20110;&#20043;&#21069;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#32534;&#30721;&#20102;&#35828;&#35805;&#32773;&#21644;&#35821;&#38899;&#20449;&#24687;&#65292;&#20294;&#23427;&#20204;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20998;&#24067;&#24773;&#20917;&#20173;&#26410;&#28145;&#20837;&#30740;&#31350;&#12290;&#25105;&#20204;&#20551;&#35774;&#23427;&#20204;&#34987;&#32534;&#30721;&#22312;&#27491;&#20132;&#23376;&#31354;&#38388;&#20013;&#65292;&#36825;&#31181;&#29305;&#24615;&#26377;&#21161;&#20110;&#31616;&#21333;&#30340;&#35299;&#32544;&#12290;&#24212;&#29992;&#20027;&#25104;&#20998;&#20998;&#26512;&#21040;&#20004;&#20010;&#39044;&#27979;&#32534;&#30721;&#27169;&#22411;&#30340;&#34920;&#31034;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#23376;&#31354;&#38388;&#65292;&#25429;&#25417;&#21040;&#30340;&#26159;&#35828;&#35805;&#32773;&#21644;&#35821;&#38899;&#21464;&#21270;&#65292;&#24182;&#30830;&#35748;&#23427;&#20204;&#20960;&#20046;&#27491;&#20132;&#12290;&#22522;&#20110;&#36825;&#20010;&#29305;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35828;&#35805;&#32773;&#24402;&#19968;&#21270;&#26041;&#27861;&#65292;&#23427;&#25240;&#21472;&#32534;&#30721;&#35828;&#35805;&#32773;&#20449;&#24687;&#30340;&#23376;&#31354;&#38388;&#65292;&#26080;&#38656;&#36716;&#24405;&#12290;&#25506;&#26512;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#28040;&#38500;&#20102;&#35828;&#35805;&#32773;&#20449;&#24687;&#65292;&#24182;&#22312;&#38899;&#20301;&#36776;&#21035;&#20219;&#21153;&#20013;&#20248;&#20110;&#20043;&#21069;&#30340;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#65292;&#21487;&#29992;&#20110;&#28040;&#38500;&#26410;&#30693;&#35828;&#35805;&#32773;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised speech representations are known to encode both speaker and phonetic information, but how they are distributed in the high-dimensional space remains largely unexplored. We hypothesize that they are encoded in orthogonal subspaces, a property that lends itself to simple disentanglement. Applying principal component analysis to representations of two predictive coding models, we identify two subspaces that capture speaker and phonetic variances, and confirm that they are nearly orthogonal. Based on this property, we propose a new speaker normalization method which collapses the subspace that encodes speaker information, without requiring transcriptions. Probing experiments show that our method effectively eliminates speaker information and outperforms a previous baseline in phone discrimination tasks. Moreover, the approach generalizes and can be used to remove information of unseen speakers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#65288;NLP&#65289;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#38656;&#35201;&#29702;&#35299;&#24182;&#22312;&#25991;&#26412;&#20013;&#30456;&#24212;&#36827;&#34892;&#25805;&#20316;&#30340;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoS&#65292;&#20351;&#29992;&#31616;&#21270;&#30340;&#31526;&#21495;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2305.10276</link><description>&lt;p&gt;
&#36830;&#38145;&#31526;&#21495;&#25552;&#31034;&#28608;&#21457;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35268;&#21010;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models. (arXiv:2305.10276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#65288;NLP&#65289;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#38656;&#35201;&#29702;&#35299;&#24182;&#22312;&#25991;&#26412;&#20013;&#30456;&#24212;&#36827;&#34892;&#25805;&#20316;&#30340;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoS&#65292;&#20351;&#29992;&#31616;&#21270;&#30340;&#31526;&#21495;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;LLMs&#22312;&#38656;&#35201;&#29702;&#35299;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#27169;&#25311;&#30340;&#34394;&#25311;&#31354;&#38388;&#29615;&#22659;&#24182;&#22312;&#25991;&#26412;&#20013;&#30456;&#24212;&#36827;&#34892;&#25805;&#20316;&#30340;&#22797;&#26434;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#33258;&#28982;&#35821;&#35328;&#35268;&#21010;&#65288;NLP&#65289;&#30340;&#22522;&#20934;&#65292;&#23427;&#30001;&#19968;&#32452;&#26032;&#39062;&#30340;&#20219;&#21153;&#32452;&#25104;&#65306;Brick World&#12289;&#22522;&#20110;NLVR&#30340;&#25805;&#20316;&#21644;&#33258;&#28982;&#35821;&#35328;&#23548;&#33322;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#27969;&#34892;&#30340;LLMs&#65288;&#22914;ChatGPT&#65289;&#20173;&#28982;&#32570;&#20047;&#22797;&#26434;&#35268;&#21010;&#30340;&#33021;&#21147;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#8212;&#8212;LLMs&#26159;&#21542;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#25551;&#36848;&#30340;&#29615;&#22659;&#26377;&#33391;&#22909;&#30340;&#29702;&#35299;&#65292;&#25110;&#32773;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#65288;&#22914;&#31526;&#21495;&#34920;&#31034;&#65289;&#26159;&#21542;&#26356;&#21152;&#31616;&#21333;&#65292;&#22240;&#27492;&#26356;&#23481;&#26131;&#34987;LLMs&#29702;&#35299;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CoS&#65288;Chain-of-Symbol Prompting&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#38142;&#24335;&#20013;&#38388;&#24605;&#32771;&#27493;&#39588;&#20013;&#20351;&#29992;&#31616;&#21270;&#30340;&#31526;&#21495;&#31354;&#38388;&#34920;&#31034;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;CoS&#26131;&#20110;&#20351;&#29992;&#65292;&#19981;&#38656;&#35201;&#23545;LLMs&#36827;&#34892;&#39069;&#22806;&#30340;&#22521;&#35757;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we take the initiative to investigate the performance of LLMs on complex planning tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act correspondingly in text. We propose a benchmark named Natural Language Planning (NLP) composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and Natural Language Navigation. We found that current popular LLMs such as ChatGPT still lack abilities in complex planning. This arises a question -- do the LLMs have a good understanding of the environments described in natural language, or maybe other alternatives such as symbolic representations are neater and hence better to be understood by LLMs? To this end, we propose a novel method called CoS (Chain-of-Symbol Prompting) that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps. CoS is easy to use and does not need additional training on LLMs. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#8212;&#8212;&#35299;&#37322;&#24615;&#24494;&#35843;&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#22312;&#32473;&#20986;&#31572;&#26696;&#30340;&#21516;&#26102;&#29983;&#25104;&#25903;&#25345;&#35813;&#31572;&#26696;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#65292;&#26469;&#20943;&#36731;LLMs&#20381;&#36182;&#34394;&#20551;&#20851;&#32852;&#65292;&#20351;&#24471;&#27169;&#22411;&#23545;&#34394;&#20551;&#25552;&#31034;&#26356;&#21152;&#24378;&#38887;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04990</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#24494;&#35843;&#20351;&#27169;&#22411;&#23545;&#34394;&#20551;&#25552;&#31034;&#26356;&#24378;&#38887;
&lt;/p&gt;
&lt;p&gt;
Explanation-based Finetuning Makes Models More Robust to Spurious Cues. (arXiv:2305.04990v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#8212;&#8212;&#35299;&#37322;&#24615;&#24494;&#35843;&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#22312;&#32473;&#20986;&#31572;&#26696;&#30340;&#21516;&#26102;&#29983;&#25104;&#25903;&#25345;&#35813;&#31572;&#26696;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#65292;&#26469;&#20943;&#36731;LLMs&#20381;&#36182;&#34394;&#20551;&#20851;&#32852;&#65292;&#20351;&#24471;&#27169;&#22411;&#23545;&#34394;&#20551;&#25552;&#31034;&#26356;&#21152;&#24378;&#38887;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38750;&#24120;&#24378;&#22823;&#65292;&#26377;&#26102;&#20250;&#23398;&#20064;&#21040;&#26631;&#31614;&#21644;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#35299;&#37322;&#24615;&#24494;&#35843;&#20316;&#20026;&#20943;&#36731;LLMs&#20381;&#36182;&#34394;&#20551;&#20851;&#32852;&#30340;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#19982;&#26631;&#20934;&#24494;&#35843;&#21482;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#31572;&#26696;&#19981;&#21516;&#65292;&#25105;&#20204;&#24494;&#35843;&#27169;&#22411;&#20197;&#29983;&#25104;&#25903;&#25345;&#20854;&#31572;&#26696;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#20154;&#24037;&#26500;&#24314;&#30340;&#35757;&#32451;&#38598;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#35813;&#35757;&#32451;&#38598;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#30340;&#34394;&#20551;&#25552;&#31034;&#65292;&#24182;&#22312;&#27809;&#26377;&#36825;&#20123;&#25552;&#31034;&#30340;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#19982;&#26631;&#20934;&#24494;&#35843;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#26041;&#38754;&#20351;&#27169;&#22411;&#26497;&#20854;&#24378;&#38887;&#65306;ComVE&#65288;+1.2&#65289;&#65292;CREAK&#65288;+9.1&#65289;&#65292;e-SNLI&#65288;+15.4&#65289;&#21644;SBIC&#65288;+6.5&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#37322;&#21516;&#26679;&#26377;&#25928;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data. We propose explanation-based finetuning as a novel and general approach to mitigate LLMs' reliance on spurious correlations. Unlike standard finetuning where the model only predicts the answer given the input, we finetune the model to additionally generate a free-text explanation supporting its answer. To evaluate our method, we finetune the model on artificially constructed training sets containing different types of spurious cues, and test it on a test set without these cues. Compared to standard finetuning, our method makes models remarkably more robust against spurious cues in terms of accuracy drop across four classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC (+6.5). Moreover, our method works equally well with explanations generated by the model, implyin
&lt;/p&gt;</description></item><item><title>SI-LSTM&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#30340;&#24490;&#29615;&#32467;&#26500;&#65292;&#21487;&#20197;&#36861;&#36394;&#19981;&#21516;&#35828;&#35805;&#20154;&#30340;&#24773;&#24863;&#29366;&#24577;&#65292;&#20174;&#32780;&#22686;&#24378;&#23545;&#35805;&#24773;&#24863;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.03506</link><description>&lt;p&gt;
SI-LSTM: &#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#30340;&#35828;&#35805;&#20154;&#28151;&#21512;&#38271;&#30701;&#26399;&#35760;&#24518;&#21644;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
SI-LSTM: Speaker Hybrid Long-short Term Memory and Cross Modal Attention for Emotion Recognition in Conversation. (arXiv:2305.03506v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03506
&lt;/p&gt;
&lt;p&gt;
SI-LSTM&#26159;&#19968;&#31181;&#29992;&#20110;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#30340;&#24490;&#29615;&#32467;&#26500;&#65292;&#21487;&#20197;&#36861;&#36394;&#19981;&#21516;&#35828;&#35805;&#20154;&#30340;&#24773;&#24863;&#29366;&#24577;&#65292;&#20174;&#32780;&#22686;&#24378;&#23545;&#35805;&#24773;&#24863;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#30340;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#23545;&#20110;&#26234;&#33021;&#21307;&#30103;&#12289;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#21644;&#32842;&#22825;&#21382;&#21490;&#35266;&#28857;&#25366;&#25496;&#31561;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35828;&#35805;&#20154;&#20449;&#24687;&#22686;&#24378;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;SI-LSTM&#65289;&#30340;&#24490;&#29615;&#32467;&#26500;&#65292;&#21487;&#20197;&#36861;&#36394;&#19981;&#21516;&#35828;&#35805;&#20154;&#30340;&#24773;&#24863;&#29366;&#24577;&#65292;&#20174;&#32780;&#22686;&#24378;&#23545;&#35805;&#24773;&#24863;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation~(ERC) across modalities is of vital importance for a variety of applications, including intelligent healthcare, artificial intelligence for conversation, and opinion mining over chat history. The crux of ERC is to model both cross-modality and cross-time interactions throughout the conversation. Previous methods have made progress in learning the time series information of conversation while lacking the ability to trace down the different emotional states of each speaker in a conversation. In this paper, we propose a recurrent structure called Speaker Information Enhanced Long-Short Term Memory (SI-LSTM) for the ERC task, where the emotional states of the distinct speaker can be tracked in a sequential way to enhance the learning of the emotion in conversation. Further, to improve the learning of multimodal features in ERC, we utilize a cross-modal attention component to fuse the features between different modalities and model the interaction of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20420;&#20044;&#25112;&#20105;&#26399;&#38388;&#20044;&#20811;&#20848;&#20154;&#22312; Twitter &#19978;&#30340;&#35821;&#35328;&#20351;&#29992;&#65292;&#21457;&#29616;&#22312;&#25112;&#20105;&#29190;&#21457;&#21069;&#24050;&#32463;&#20986;&#29616;&#20174;&#20420;&#35821;&#21521;&#20044;&#20811;&#20848;&#35821;&#36716;&#21464;&#30340;&#36235;&#21183;&#65292;&#32780;&#25112;&#20105;&#29190;&#21457;&#21518;&#36825;&#31181;&#36235;&#21183;&#21152;&#36895;&#20102;&#65292;&#24182;&#19988;&#35768;&#22810;&#20351;&#29992;&#20420;&#35821;&#30340;&#29992;&#25143;&#22312;&#25112;&#20105;&#26399;&#38388;&#36716;&#21464;&#25104;&#20351;&#29992;&#20044;&#20811;&#20848;&#35821;&#12290;</title><link>http://arxiv.org/abs/2305.02770</link><description>&lt;p&gt;
&#35821;&#35328;&#36873;&#25321;&#30340;&#25919;&#27835;&#65306;&#20420;&#20044;&#25112;&#20105;&#22914;&#20309;&#24433;&#21709;&#20044;&#20811;&#20848;&#20154;&#22312; Twitter &#19978;&#30340;&#35821;&#35328;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Politics of Language Choice: How the Russian-Ukrainian War Influences Ukrainians' Language Use on Twitter. (arXiv:2305.02770v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20420;&#20044;&#25112;&#20105;&#26399;&#38388;&#20044;&#20811;&#20848;&#20154;&#22312; Twitter &#19978;&#30340;&#35821;&#35328;&#20351;&#29992;&#65292;&#21457;&#29616;&#22312;&#25112;&#20105;&#29190;&#21457;&#21069;&#24050;&#32463;&#20986;&#29616;&#20174;&#20420;&#35821;&#21521;&#20044;&#20811;&#20848;&#35821;&#36716;&#21464;&#30340;&#36235;&#21183;&#65292;&#32780;&#25112;&#20105;&#29190;&#21457;&#21518;&#36825;&#31181;&#36235;&#21183;&#21152;&#36895;&#20102;&#65292;&#24182;&#19988;&#35768;&#22810;&#20351;&#29992;&#20420;&#35821;&#30340;&#29992;&#25143;&#22312;&#25112;&#20105;&#26399;&#38388;&#36716;&#21464;&#25104;&#20351;&#29992;&#20044;&#20811;&#20848;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20351;&#29992;&#22825;&#29983;&#26159;&#25919;&#27835;&#30340;&#65292;&#24182;&#32463;&#24120;&#29992;&#20316;&#25991;&#21270;&#36523;&#20221;&#30340;&#36733;&#20307;&#65292;&#21516;&#26102;&#20063;&#26159;&#22269;&#23478;&#24314;&#35774;&#30340;&#22522;&#30784;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20420;&#20044;&#25112;&#20105;&#26399;&#38388;&#65288;2020&#24180;1&#26376;&#33267;2022&#24180;10&#26376;&#65289;&#65292;&#22522;&#20110;&#36229;&#36807;62,000&#20301;&#29992;&#25143;&#21457;&#24067;&#30340;400&#19975;&#26465;&#22320;&#29702;&#26631;&#35760;&#25512;&#25991;&#20013;&#65292;&#20044;&#20811;&#20848;&#20844;&#27665;&#30340;&#35821;&#35328;&#36873;&#25321;&#21644;&#25512;&#25991;&#27963;&#21160;&#12290;&#20351;&#29992;&#32479;&#35745;&#27169;&#22411;&#65292;&#21306;&#20998;&#20102;Twitter&#19978;&#29992;&#25143;&#30340;&#27969;&#20837;&#27969;&#20986;&#25152;&#24341;&#36215;&#30340;&#26679;&#26412;&#25928;&#24212;&#21644;&#29992;&#25143;&#34892;&#20026;&#21464;&#21270;&#25152;&#24341;&#36215;&#30340;&#34892;&#20026;&#25928;&#24212;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#25112;&#20105;&#29190;&#21457;&#20043;&#21069;&#24050;&#32463;&#26377;&#19968;&#20010;&#31283;&#23450;&#30340;&#20174;&#20420;&#35821;&#21521;&#20044;&#20811;&#20848;&#35821;&#30340;&#36716;&#21464;&#65292;&#32780;&#36825;&#19968;&#36807;&#31243;&#22312;&#25112;&#20105;&#29190;&#21457;&#21518;&#36805;&#36895;&#21152;&#36895;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#21464;&#21270;&#20027;&#35201;&#24402;&#22240;&#20110;&#29992;&#25143;&#34892;&#20026;&#30340;&#25913;&#21464;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35768;&#22810;&#20351;&#29992;&#20420;&#35821;&#30340;&#29992;&#25143;&#22312;&#25112;&#20105;&#26399;&#38388;&#20250;&#36716;&#21464;&#25104;&#20351;&#29992;&#20044;&#20811;&#20848;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of language is innately political and often a vehicle of cultural identity as well as the basis for nation building. Here, we examine language choice and tweeting activity of Ukrainian citizens based on more than 4 million geo-tagged tweets from over 62,000 users before and during the Russian-Ukrainian War, from January 2020 to October 2022. Using statistical models, we disentangle sample effects, arising from the in- and outflux of users on Twitter, from behavioural effects, arising from behavioural changes of the users. We observe a steady shift from the Russian language towards the Ukrainian language already before the war, which drastically speeds up with its outbreak. We attribute these shifts in large part to users' behavioural changes. Notably, we find that many Russian-tweeting users perform a hard-switch to Ukrainian as a result of the war.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27969;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;Common Crawl&#65292;&#20174;&#20114;&#32852;&#32593;&#19978;&#25910;&#38598;PDF&#25991;&#20214;&#65292;&#26500;&#24314;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#22810;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;&#19968;&#20010;CCpdf&#35821;&#26009;&#24211;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#36827;&#34892;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#30740;&#31350;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2304.14953</link><description>&lt;p&gt;
CCpdf&#65306;&#20174;&#32593;&#32476;&#29228;&#34411;&#25968;&#25454;&#20013;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
CCpdf: Building a High Quality Corpus for Visually Rich Documents from Web Crawl Data. (arXiv:2304.14953v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27969;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;Common Crawl&#65292;&#20174;&#20114;&#32852;&#32593;&#19978;&#25910;&#38598;PDF&#25991;&#20214;&#65292;&#26500;&#24314;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#22810;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;&#19968;&#20010;CCpdf&#35821;&#26009;&#24211;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#36827;&#34892;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#30740;&#31350;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#25991;&#26723;&#29702;&#35299;&#39046;&#22495;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#36825;&#20123;&#36827;&#23637;&#30340;&#19968;&#37096;&#20998;&#24471;&#30410;&#20110;&#20351;&#29992;&#39044;&#35757;&#32451;&#20110;&#22823;&#37327;&#25991;&#26723;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25991;&#26723;&#29702;&#35299;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#36890;&#24120;&#21333;&#19968;&#39046;&#22495;&#12289;&#21333;&#35821;&#35328;&#12289;&#25110;&#19981;&#20844;&#24320;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;Common Crawl&#65292;&#20174;&#20114;&#32852;&#32593;&#19978;&#25910;&#38598;PDF&#25991;&#20214;&#65292;&#26500;&#24314;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#22810;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#23545;&#26500;&#24314;&#27969;&#31243;&#30340;&#25152;&#26377;&#27493;&#39588;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#25968;&#25454;&#36136;&#37327;&#21644;&#22788;&#29702;&#26102;&#38388;&#20043;&#38388;&#24179;&#34913;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20998;&#20139;&#20102;&#19968;&#20010;CCpdf&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;PDF&#25991;&#20214;&#30340;&#32034;&#24341;&#21644;&#19979;&#36733;&#33050;&#26412;&#65292;&#21487;&#20197;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#25152;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#21644;&#24037;&#20855;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#36827;&#34892;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#30740;&#31350;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the field of document understanding has progressed a lot. A significant part of this progress has been possible thanks to the use of language models pretrained on large amounts of documents. However, pretraining corpora used in the domain of document understanding are single domain, monolingual, or nonpublic. Our goal in this paper is to propose an efficient pipeline for creating a big-scale, diverse, multilingual corpus of PDF files from all over the Internet using Common Crawl, as PDF files are the most canonical types of documents as considered in document understanding. We analysed extensively all of the steps of the pipeline and proposed a solution which is a trade-off between data quality and processing time. We also share a CCpdf corpus in a form or an index of PDF files along with a script for downloading them, which produces a collection useful for language model pretraining. The dataset and tools published with this paper offer researchers the opportunity to 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36923;&#36753;&#35821;&#27861;&#23884;&#20837;&#27169;&#22411;(LGE)&#65292;&#23427;&#21487;&#20197;&#20174;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26080;&#30417;&#30563;&#36827;&#34892;&#25512;&#29702;&#65292;&#20135;&#29983;&#31616;&#26126;&#26131;&#25026;&#30340;&#36755;&#20986;&#65292;&#33021;&#22815;&#36879;&#26126;&#22320;&#29983;&#25104;&#26032;&#21477;&#23376;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#20165;&#26377;&#19968;&#30334;&#21477;&#35805;&#30340;&#35821;&#26009;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.14590</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#35821;&#27861;&#30340;&#36923;&#36753;&#35789;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A logical word embedding for learning grammar. (arXiv:2304.14590v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14590
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36923;&#36753;&#35821;&#27861;&#23884;&#20837;&#27169;&#22411;(LGE)&#65292;&#23427;&#21487;&#20197;&#20174;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26080;&#30417;&#30563;&#36827;&#34892;&#25512;&#29702;&#65292;&#20135;&#29983;&#31616;&#26126;&#26131;&#25026;&#30340;&#36755;&#20986;&#65292;&#33021;&#22815;&#36879;&#26126;&#22320;&#29983;&#25104;&#26032;&#21477;&#23376;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#20165;&#26377;&#19968;&#30334;&#21477;&#35805;&#30340;&#35821;&#26009;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36923;&#36753;&#35821;&#27861;&#23884;&#20837;(LGE)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21463;&#21040;&#20102;&#32452;&#21512;&#35821;&#27861;&#21644;&#31867;&#21035;&#35821;&#27861;&#30340;&#21551;&#21457;&#65292;&#21487;&#20197;&#20174;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#23545;&#35789;&#27719;&#31867;&#21035;&#21644;&#21477;&#27861;&#35268;&#21017;&#36827;&#34892;&#26080;&#30417;&#30563;&#25512;&#29702;&#12290; LGE&#20135;&#29983;&#20102;&#31616;&#26126;&#26131;&#25026;&#30340;&#36755;&#20986;&#65292;&#33021;&#22815;&#36879;&#26126;&#22320;&#29983;&#25104;&#26032;&#21477;&#23376;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#20165;&#26377;&#19968;&#30334;&#21477;&#35805;&#30340;&#35821;&#26009;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the logical grammar emdebbing (LGE), a model inspired by pregroup grammars and categorial grammars to enable unsupervised inference of lexical categories and syntactic rules from a corpus of text. LGE produces comprehensible output summarizing its inferences, has a completely transparent process for producing novel sentences, and can learn from as few as a hundred sentences.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;NAIST-SIC-Aligned&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#23545;&#40784;&#30340;&#33521;&#26085;&#24179;&#34892;&#21516;&#22768;&#20256;&#35793;&#25968;&#25454;&#38598;&#12290;&#35813;&#35770;&#25991;&#20351;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#32463;&#36807;&#23450;&#37327;&#25110;&#23450;&#24615;&#39564;&#35777;&#30340;&#27599;&#20010;&#27493;&#39588;&#65292;&#20197;&#30830;&#20445;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#24179;&#34892;SI&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.11766</link><description>&lt;p&gt;
NAIST-SIC-Aligned&#65306;&#33258;&#21160;&#23545;&#40784;&#30340;&#33521;&#26085;&#21516;&#22768;&#20256;&#35793;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
NAIST-SIC-Aligned: Automatically-Aligned English-Japanese Simultaneous Interpretation Corpus. (arXiv:2304.11766v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;NAIST-SIC-Aligned&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#23545;&#40784;&#30340;&#33521;&#26085;&#24179;&#34892;&#21516;&#22768;&#20256;&#35793;&#25968;&#25454;&#38598;&#12290;&#35813;&#35770;&#25991;&#20351;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#32463;&#36807;&#23450;&#37327;&#25110;&#23450;&#24615;&#39564;&#35777;&#30340;&#27599;&#20010;&#27493;&#39588;&#65292;&#20197;&#30830;&#20445;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#24179;&#34892;SI&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#21033;&#29992;&#21516;&#22768;&#20256;&#35793;&#65288;SI&#65289;&#25968;&#25454;&#26469;&#24433;&#21709;&#21516;&#22768;&#26426;&#22120;&#32763;&#35793;&#65288;SiMT&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#30001;&#20110;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#65292;&#30740;&#31350;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;NAIST-SIC-Aligned&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#23545;&#40784;&#30340;&#33521;&#26085;&#24179;&#34892;&#21516;&#22768;&#20256;&#35793;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#19968;&#20010;&#38750;&#23545;&#40784;&#35821;&#26009;&#24211;NAIST-SIC&#24320;&#22987;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23545;&#40784;&#26041;&#27861;&#65292;&#20351;&#35821;&#26009;&#24211;&#20855;&#26377;&#24179;&#34892;&#24615;&#65292;&#20174;&#32780;&#36866;&#21512;&#27169;&#22411;&#35757;&#32451;&#12290;&#31532;&#19968;&#38454;&#27573;&#26159;&#31895;&#30053;&#23545;&#40784;&#65292;&#22312;&#27492;&#27493;&#39588;&#20013;&#65292;&#25105;&#20204;&#22312;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#25191;&#34892;&#19968;&#20010;&#22810;&#23545;&#22810;&#30340;&#26144;&#23556;&#65307;&#31532;&#20108;&#38454;&#27573;&#26159;&#32454;&#31890;&#24230;&#23545;&#40784;&#65292;&#22312;&#27492;&#27493;&#39588;&#20013;&#65292;&#25105;&#20204;&#25191;&#34892;&#35821;&#21477;&#20869;&#37096;&#21644;&#35821;&#21477;&#38388;&#36807;&#28388;&#26469;&#25552;&#39640;&#23545;&#40784;&#23545;&#30340;&#36136;&#37327;&#12290;&#20026;&#30830;&#20445;&#35821;&#26009;&#24211;&#30340;&#36136;&#37327;&#65292;&#27599;&#20010;&#27493;&#39588;&#37117;&#32463;&#36807;&#20102;&#23450;&#37327;&#25110;&#23450;&#24615;&#30340;&#39564;&#35777;&#12290;&#36825;&#26159;&#25991;&#29486;&#20013;&#31532;&#19968;&#20010;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#24179;&#34892;SI&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#25163;&#21160;&#31934;&#36873;&#20102;&#19968;&#20010;&#23567;&#22411;&#27979;&#35797;&#38598;&#29992;&#20110;&#35780;&#20272;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
It remains a question that how simultaneous interpretation (SI) data affects simultaneous machine translation (SiMT). Research has been limited due to the lack of a large-scale training corpus. In this work, we aim to fill in the gap by introducing NAIST-SIC-Aligned, which is an automatically-aligned parallel English-Japanese SI dataset. Starting with a non-aligned corpus NAIST-SIC, we propose a two-stage alignment approach to make the corpus parallel and thus suitable for model training. The first stage is coarse alignment where we perform a many-to-many mapping between source and target sentences, and the second stage is fine-grained alignment where we perform intra- and inter-sentence filtering to improve the quality of aligned pairs. To ensure the quality of the corpus, each step has been validated either quantitatively or qualitatively. This is the first open-sourced large-scale parallel SI dataset in the literature. We also manually curated a small test set for evaluation purpose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#21644;&#24207;&#21015;&#32423;&#21035;&#30340;&#19981;&#21487;&#33021;&#24615;&#25439;&#22833;&#65292;&#20197;&#21450;&#22312;&#22521;&#35757;&#26399;&#38388;&#30340;&#37325;&#22797;&#24809;&#32602;&#12289;&#25512;&#29702;&#21644;&#21518;&#22788;&#29702;&#31561;&#22810;&#23618;&#38754;&#26041;&#27861;&#26469;&#25233;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37325;&#22797;&#65292;&#24182;&#36991;&#20813;&#29983;&#25104;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.10611</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26041;&#38754;&#37325;&#22797;&#25233;&#21046;&#21644;&#20869;&#23481;&#35843;&#25511;
&lt;/p&gt;
&lt;p&gt;
Multi-aspect Repetition Suppression and Content Moderation of Large Language Models. (arXiv:2304.10611v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#21644;&#24207;&#21015;&#32423;&#21035;&#30340;&#19981;&#21487;&#33021;&#24615;&#25439;&#22833;&#65292;&#20197;&#21450;&#22312;&#22521;&#35757;&#26399;&#38388;&#30340;&#37325;&#22797;&#24809;&#32602;&#12289;&#25512;&#29702;&#21644;&#21518;&#22788;&#29702;&#31561;&#22810;&#23618;&#38754;&#26041;&#27861;&#26469;&#25233;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#37325;&#22797;&#65292;&#24182;&#36991;&#20813;&#29983;&#25104;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#22312;NLP&#39046;&#22495;&#26159;&#26368;&#20855;&#24433;&#21709;&#21147;&#30340;&#39046;&#22495;&#20043;&#19968;&#65292;&#36817;&#24180;&#26469;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24102;&#26469;&#30340;&#36827;&#27493;&#24471;&#21040;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#20316;&#20026;&#32534;&#20889;&#21161;&#25163;&#24212;&#29992;&#31243;&#24207;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#23427;&#20204;&#36890;&#24120;&#23481;&#26131;&#22797;&#21046;&#25110;&#25193;&#23637;&#36755;&#20837;&#20013;&#25552;&#20379;&#30340;&#20855;&#26377;&#25915;&#20987;&#24615;&#30340;&#20869;&#23481;&#12290;&#22312;&#20302;&#36164;&#28304;&#25968;&#25454;&#29615;&#22659;&#20013;&#65292;&#23427;&#20204;&#20063;&#21487;&#33021;&#23548;&#33268;&#36755;&#20986;&#37325;&#22797;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31934;&#30830;&#21644;&#38750;&#31934;&#30830;&#37325;&#22797;&#25233;&#21046;&#30340;&#32467;&#21512;&#26041;&#27861;&#65292;&#20351;&#29992;&#26631;&#35760;&#21644;&#24207;&#21015;&#32423;&#21035;&#30340;&#19981;&#21487;&#33021;&#24615;&#25439;&#22833;&#65292;&#22521;&#35757;&#26399;&#38388;&#30340;&#37325;&#22797;&#24809;&#32602;&#12289;&#25512;&#29702;&#21644;&#21518;&#22788;&#29702;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22810;&#32423;&#19981;&#21487;&#33021;&#24615;&#25439;&#22833;&#30340;&#33539;&#22260;&#65292;&#20197;&#36171;&#20104;&#27169;&#22411;&#36991;&#20813;&#20174;&#19968;&#24320;&#22987;&#20135;&#29983;&#25915;&#20987;&#24615;&#35789;&#27719;&#21644;&#30701;&#35821;&#30340;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#22312;&#22810;&#20010;&#24230;&#37327;&#26631;&#20934;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language generation is one of the most impactful fields in NLP, and recent years have witnessed its evolution brought about by large language models (LLMs). As the key instrument for writing assistance applications, they are generally prone to replicating or extending offensive content provided in the input. In low-resource data regime, they can also lead to repetitive outputs (Holtzman et al., 2019) [1]. Usually, offensive content and repetitions are mitigated with post-hoc methods, including n-gram level blocklists, top-k and nucleus sampling. In this paper, we introduce a combination of exact and non-exact repetition suppression using token and sequence level unlikelihood loss, repetition penalty during training, inference, and post-processing respectively. We further explore multi-level unlikelihood loss to the extent that it endows the model with abilities to avoid generating offensive words and phrases from the beginning. Finally, with comprehensive experiments, we demons
&lt;/p&gt;</description></item><item><title>G2T&#26159;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#33258;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;G2T&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.06653</link><description>&lt;p&gt;
G2T: &#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
G2T: A simple but versatile framework for topic modeling based on pretrained language model and community detection. (arXiv:2304.06653v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06653
&lt;/p&gt;
&lt;p&gt;
G2T&#26159;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#33258;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;G2T&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#32858;&#31867;&#30340;&#20027;&#39064;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36866;&#24403;&#30340;&#35789;&#35821;&#31579;&#36873;&#26041;&#27861;&#32858;&#31867;&#39640;&#36136;&#37327;&#30340;&#21477;&#23376;&#23884;&#20837;&#65292;&#29983;&#25104;&#27604;&#29983;&#25104;&#24335;&#27010;&#29575;&#20027;&#39064;&#27169;&#22411;&#26356;&#22909;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#36873;&#25321;&#21512;&#36866;&#21442;&#25968;&#30340;&#22256;&#38590;&#20197;&#21450;&#19981;&#23436;&#25972;&#30340;&#27169;&#22411;&#24573;&#30053;&#21333;&#35789;&#19982;&#20027;&#39064;&#21450;&#20027;&#39064;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#23450;&#37327;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#20294;&#26377;&#25928;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21363;&#22270;&#20027;&#39064;&#65288;G2T&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been reported that clustering-based topic models, which cluster high-quality sentence embeddings with an appropriate word selection method, can generate better topics than generative probabilistic topic models. However, these approaches suffer from the inability to select appropriate parameters and incomplete models that overlook the quantitative relation between words with topics and topics with text. To solve these issues, we propose graph to topic (G2T), a simple but effective framework for topic modelling. The framework is composed of four modules. First, document representation is acquired using pretrained language models. Second, a semantic graph is constructed according to the similarity between document representations. Third, communities in document semantic graphs are identified, and the relationship between topics and documents is quantified accordingly. Fourth, the word--topic distribution is computed based on a variant of TFIDF. Automatic evaluation suggests that G2
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26723;VQA&#25968;&#25454;&#38598;PDF-VQA&#65292;&#20197;&#22810;&#20010;&#39029;&#38754;&#30340;&#23436;&#25972;&#25991;&#26723;&#20316;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#19982;&#22788;&#29702;&#25991;&#26723;&#20803;&#32032;&#12289;&#32467;&#26500;&#21644;&#20869;&#23481;&#31561;&#26041;&#38754;&#65292;&#20026;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2304.06447</link><description>&lt;p&gt;
PDF-VQA: &#19968;&#20010;&#26032;&#30340;&#29992;&#20110;PDF&#25991;&#20214;&#30495;&#23454;&#19990;&#30028;VQA&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PDF-VQA: A New Dataset for Real-World VQA on PDF Documents. (arXiv:2304.06447v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26723;VQA&#25968;&#25454;&#38598;PDF-VQA&#65292;&#20197;&#22810;&#20010;&#39029;&#38754;&#30340;&#23436;&#25972;&#25991;&#26723;&#20316;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#19982;&#22788;&#29702;&#25991;&#26723;&#20803;&#32032;&#12289;&#32467;&#26500;&#21644;&#20869;&#23481;&#31561;&#26041;&#38754;&#65292;&#20026;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26723;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#30740;&#31350;&#25991;&#26723;&#22270;&#20687;&#30340;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#25991;&#26723;&#30340;VQA&#25968;&#25454;&#38598;PDF-VQA&#65292;&#20174;&#25991;&#26723;&#20803;&#32032;&#35782;&#21035;&#12289;&#25991;&#26723;&#24067;&#23616;&#32467;&#26500;&#29702;&#35299;&#20197;&#21450;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#31561;&#21508;&#20010;&#26041;&#38754;&#20840;&#38754;&#25506;&#35752;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;PDF-VQA&#25968;&#25454;&#38598;&#23558;&#25991;&#26723;&#29702;&#35299;&#30340;&#35268;&#27169;&#20174;&#21333;&#20010;&#25991;&#26723;&#39029;&#38754;&#25193;&#23637;&#21040;&#35810;&#38382;&#22810;&#20010;&#39029;&#38754;&#30340;&#23436;&#25972;&#25991;&#26723;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;VQA&#27169;&#22411;&#65292;&#26126;&#30830;&#22320;&#38598;&#25104;&#20102;&#19981;&#21516;&#25991;&#26723;&#20803;&#32032;&#20043;&#38388;&#30340;&#31354;&#38388;&#21644;&#23618;&#27425;&#32467;&#26500;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#25991;&#26723;&#32467;&#26500;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#35813;&#24615;&#33021;&#19982;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#36739;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#38382;&#39064;&#31867;&#22411;&#21644;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-based Visual Question Answering examines the document understanding of document images in conditions of natural language questions. We proposed a new document-based VQA dataset, PDF-VQA, to comprehensively examine the document understanding from various aspects, including document element recognition, document layout structural understanding as well as contextual understanding and key information extraction. Our PDF-VQA dataset extends the current scale of document understanding that limits on the single document page to the new scale that asks questions over the full document of multiple pages. We also propose a new graph-based VQA model that explicitly integrates the spatial and hierarchically structural relationships between different document elements to boost the document structural understanding. The performances are compared with several baselines over different question types and tasks\footnote{The full dataset will be released after paper acceptance.
&lt;/p&gt;</description></item><item><title>oBERTa&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#12289;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17612</link><description>&lt;p&gt;
oBERTa: &#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#21644;&#21098;&#26525;&#26469;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes. (arXiv:2303.17612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17612
&lt;/p&gt;
&lt;p&gt;
oBERTa&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#12289;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;oBERTa&#35821;&#35328;&#27169;&#22411;&#30340;&#33539;&#22260;&#65292;&#23427;&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20801;&#35768;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20174;&#19994;&#32773;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;3.8&#21040;24.3&#20493;&#30340;&#26356;&#24555;&#36895;&#30340;&#27169;&#22411;&#12290;oBERTa&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#21098;&#26525;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#24037;&#20316;&#65292;&#24182;&#21033;&#29992;&#20923;&#32467;&#30340;&#23884;&#20837;&#26469;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#65292;&#24182;&#25913;&#36827;&#27169;&#22411;&#21021;&#22987;&#21270;&#65292;&#20197;&#22312;&#24191;&#27867;&#30340;&#20256;&#36882;&#20219;&#21153;&#19978;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#29983;&#25104;oBERTa&#26102;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#39640;&#24230;&#20248;&#21270;&#30340;RoBERTa&#19982;BERT&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26399;&#38388;&#21098;&#26525;&#26041;&#38754;&#30340;&#19981;&#21516;&#20043;&#22788;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#24494;&#35843;&#26399;&#38388;&#19981;&#22826;&#36866;&#21512;&#21387;&#32553;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;oBERTa&#22312;&#19971;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;NLP&#20219;&#21153;&#19978;&#30340;&#20351;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#36827;&#30340;&#21387;&#32553;&#25216;&#26415;&#20351;&#24471;&#32463;&#36807;&#21098;&#26525;&#30340;oBERTa&#27169;&#22411;&#33021;&#22815;&#21305;&#37197;BERTBASE&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;SQUAD V1.1&#38382;&#31572;&#25968;&#25454;&#30340;Prune OFA Large&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the range of oBERTa language models, an easy-to-use set of language models, which allows Natural Language Processing (NLP) practitioners to obtain between 3.8 and 24.3 times faster models without expertise in model compression. Specifically, oBERTa extends existing work on pruning, knowledge distillation, and quantization and leverages frozen embeddings to improve knowledge distillation, and improved model initialization to deliver higher accuracy on a a broad range of transfer tasks. In generating oBERTa, we explore how the highly optimized RoBERTa differs from the BERT with respect to pruning during pre-training and fine-tuning and find it less amenable to compression during fine-tuning. We explore the use of oBERTa on a broad seven representative NLP tasks and find that the improved compression techniques allow a pruned oBERTa model to match the performance of BERTBASE and exceed the performance of Prune OFA Large on the SQUAD V1.1 Question Answering data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;f-DPG&#65292;&#29992;&#20110;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20559;&#22909;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#35780;&#20272;&#20219;&#20309;&#30446;&#26631;&#20998;&#24067;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#21508;&#31181;&#26694;&#26550;&#21644;&#36924;&#36817;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08215</link><description>&lt;p&gt;
&#36890;&#36807;f-&#25955;&#24230;&#26368;&#23567;&#21270;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Aligning Language Models with Preferences through f-divergence Minimization. (arXiv:2302.08215v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;f-DPG&#65292;&#29992;&#20110;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20559;&#22909;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#35780;&#20272;&#20219;&#20309;&#30446;&#26631;&#20998;&#24067;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#21508;&#31181;&#26694;&#26550;&#21644;&#36924;&#36817;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#21644;&#20559;&#22909;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#23545;&#30446;&#26631;&#20998;&#24067;&#36827;&#34892;&#36924;&#36817;&#65292;&#20197;&#26399;&#36798;&#21040;&#26576;&#31181;&#25152;&#38656;&#34892;&#20026;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#30446;&#26631;&#20998;&#24067;&#30340;&#20989;&#25968;&#24418;&#24335;&#21644;&#29992;&#20110;&#36924;&#36817;&#30446;&#26631;&#20998;&#24067;&#30340;&#31639;&#27861;&#19978;&#23384;&#22312;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;f-DPG&#65292;&#35813;&#26041;&#27861;&#20801;&#35768;&#20351;&#29992;&#20219;&#20309;&#21487;&#35780;&#20272;&#30340;f-&#25955;&#24230;&#36924;&#36817;&#20219;&#20309;&#30446;&#26631;&#20998;&#24067;&#65292;&#20174;&#32780;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#21508;&#31181;&#26694;&#26550;&#21644;&#36924;&#36817;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21508;&#31181;&#25955;&#24230;&#30446;&#26631;&#30340;&#23454;&#38469;&#22909;&#22788;&#65292;&#24182;&#35777;&#26126;&#20102;&#27809;&#26377;&#26222;&#36866;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning language models with preferences can be posed as approximating a target distribution representing some desired behavior. Existing approaches differ both in the functional form of the target distribution and the algorithm used to approximate it. For instance, Reinforcement Learning from Human Feedback (RLHF) corresponds to minimizing a reverse KL from an implicit target distribution arising from a KL penalty in the objective. On the other hand, Generative Distributional Control (GDC) has an explicit target distribution and minimizes a forward KL from it using the Distributional Policy Gradient (DPG) algorithm. In this paper, we propose a new approach, f-DPG, which allows the use of any f-divergence to approximate any target distribution that can be evaluated. f-DPG unifies both frameworks (RLHF, GDC) and the approximation methods (DPG, RL with KL penalties). We show the practical benefits of various choices of divergence objectives and demonstrate that there is no universally o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;DP-BART&#65292;&#37319;&#29992;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#27861;&#36827;&#34892;&#25991;&#26412;&#37325;&#20889;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;LDP&#31995;&#32479;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#38656;&#35201;DP&#20445;&#35777;&#30340;&#22122;&#38899;&#37327;&#65292;&#23454;&#39564;&#34920;&#26126;&#22312;&#19979;&#28216;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.07636</link><description>&lt;p&gt;
&#22522;&#20110;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#30340;&#25991;&#26412;&#37325;&#20889;DP-BART&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
DP-BART for Privatized Text Rewriting under Local Differential Privacy. (arXiv:2302.07636v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31995;&#32479;DP-BART&#65292;&#37319;&#29992;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#30340;&#26041;&#27861;&#36827;&#34892;&#25991;&#26412;&#37325;&#20889;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;LDP&#31995;&#32479;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#38656;&#35201;DP&#20445;&#35777;&#30340;&#22122;&#38899;&#37327;&#65292;&#23454;&#39564;&#34920;&#26126;&#22312;&#19979;&#28216;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#23616;&#37096;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#30340;&#31169;&#23494;&#25991;&#26412;&#37325;&#20889;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#27491;&#24335;&#20445;&#35777;&#23545;&#20010;&#20154;&#30340;&#38544;&#31169;&#20445;&#25252;&#30340;&#21516;&#26102;&#65292;&#20849;&#20139;&#25935;&#24863;&#30340;&#25991;&#26412;&#25991;&#26723;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31995;&#32479;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#65292;&#22914;&#24418;&#24335;&#19978;&#25968;&#23398;&#32570;&#38519;&#12289;&#19981;&#20999;&#23454;&#38469;&#30340;&#38544;&#31169;&#20445;&#35777;&#12289;&#20165;&#38024;&#23545;&#20010;&#21035;&#21333;&#35789;&#30340;&#31169;&#23494;&#21270;&#65292;&#20197;&#21450;&#32570;&#20047;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#31995;&#32479;&#8220;DP-BART&#8221;&#65292;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;LDP&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#21098;&#36753;&#26041;&#27861;&#12289;&#36845;&#20195;&#21098;&#26525;&#20197;&#21450;&#36827;&#19968;&#27493;&#22521;&#35757;&#20869;&#37096;&#34920;&#31034;&#65292;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#38656;&#35201;DP&#20445;&#35777;&#30340;&#22122;&#38899;&#37327;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#22823;&#23567;&#19981;&#21516;&#30340;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#23454;&#39564;&#65292;&#22312;&#19981;&#21516;&#30340;&#38544;&#31169;&#20445;&#35777;&#19979;&#37325;&#20889;&#36825;&#20123;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#32463;&#36807;&#37325;&#20889;&#30340;&#25991;&#26412;&#30340;&#19979;&#28216;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#31169;&#23494;&#25991;&#26412;&#37325;&#20889;&#26041;&#27861;&#21450;&#20854;&#38480;&#21046;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35752;&#35770;&#65292;&#21253;&#25324;&#20005;&#26684;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privatized text rewriting with local differential privacy (LDP) is a recent approach that enables sharing of sensitive textual documents while formally guaranteeing privacy protection to individuals. However, existing systems face several issues, such as formal mathematical flaws, unrealistic privacy guarantees, privatization of only individual words, as well as a lack of transparency and reproducibility. In this paper, we propose a new system 'DP-BART' that largely outperforms existing LDP systems. Our approach uses a novel clipping method, iterative pruning, and further training of internal representations which drastically reduces the amount of noise required for DP guarantees. We run experiments on five textual datasets of varying sizes, rewriting them at different privacy guarantees and evaluating the rewritten texts on downstream text classification tasks. Finally, we thoroughly discuss the privatized text rewriting approach and its limitations, including the problem of the stric
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26080;&#20851;&#19978;&#19979;&#25991;&#30340;&#24178;&#25200;&#24615;&#12290;&#20182;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#26080;&#20851;&#20449;&#24687;&#30340;&#31639;&#26415;&#25512;&#29702;&#25968;&#25454;&#38598;GSM-IC&#26469;&#34913;&#37327;&#36825;&#31181;&#21487;&#24178;&#25200;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21253;&#21547;&#26080;&#20851;&#20449;&#24687;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#20250;&#24613;&#21095;&#19979;&#38477;&#65292;&#20294;&#20351;&#29992;&#33258;&#25105;&#19968;&#33268;&#24615;&#36827;&#34892;&#35299;&#30721;&#24182;&#28155;&#21152;&#19968;&#20010;&#25351;&#20196;&#21487;&#20197;&#32531;&#35299;&#36825;&#19968;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2302.00093</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#26080;&#20851;&#19978;&#19979;&#25991;&#30340;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Be Easily Distracted by Irrelevant Context. (arXiv:2302.00093v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#26080;&#20851;&#19978;&#19979;&#25991;&#30340;&#24178;&#25200;&#24615;&#12290;&#20182;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#26080;&#20851;&#20449;&#24687;&#30340;&#31639;&#26415;&#25512;&#29702;&#25968;&#25454;&#38598;GSM-IC&#26469;&#34913;&#37327;&#36825;&#31181;&#21487;&#24178;&#25200;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21253;&#21547;&#26080;&#20851;&#20449;&#24687;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#20250;&#24613;&#21095;&#19979;&#38477;&#65292;&#20294;&#20351;&#29992;&#33258;&#25105;&#19968;&#33268;&#24615;&#36827;&#34892;&#35299;&#30721;&#24182;&#28155;&#21152;&#19968;&#20010;&#25351;&#20196;&#21487;&#20197;&#32531;&#35299;&#36825;&#19968;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#23427;&#20204;&#20027;&#35201;&#22312;&#25152;&#26377;&#36755;&#20837;&#19978;&#19979;&#25991;&#20449;&#24687;&#37117;&#19982;&#35299;&#20915;&#20219;&#21153;&#30456;&#20851;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#24178;&#25200;&#24615;&#65292;&#21363;&#19981;&#30456;&#20851;&#19978;&#19979;&#25991;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#20934;&#30830;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#26080;&#20851;&#20449;&#24687;&#30340;&#31639;&#26415;&#25512;&#29702;&#25968;&#25454;&#38598;GSM-IC&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#34913;&#37327;&#26368;&#23574;&#31471;&#30340;&#25552;&#31034;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#24178;&#25200;&#24615;&#65292;&#21457;&#29616;&#24403;&#21253;&#21547;&#26080;&#20851;&#20449;&#24687;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#20250;&#24613;&#21095;&#19979;&#38477;&#12290;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#20960;&#31181;&#32531;&#35299;&#36825;&#31181;&#19981;&#36275;&#30340;&#26041;&#27861;&#65292;&#22914;&#20351;&#29992;&#33258;&#25105;&#19968;&#33268;&#24615;&#36827;&#34892;&#35299;&#30721;&#65292;&#24182;&#22312;&#25552;&#31034;&#20013;&#28155;&#21152;&#19968;&#26465;&#25351;&#20196;&#65292;&#21578;&#35785;&#35821;&#35328;&#27169;&#22411;&#24573;&#30053;&#26080;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant in
&lt;/p&gt;</description></item><item><title>AutoPEFT&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;PEFT&#65288;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65289;&#37197;&#32622;&#25628;&#32034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#22320;&#25214;&#21040;&#26368;&#20339;&#30340;PEFT&#27169;&#22359;&#21644;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;&#22312;&#20856;&#22411;&#30340;NLP&#20219;&#21153;&#20013;&#65292;AutoPEFT&#34920;&#29616;&#20986;&#27604;&#25163;&#21160;&#35774;&#35745;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.12132</link><description>&lt;p&gt;
AutoPEFT&#65306;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#33258;&#21160;&#37197;&#32622;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
AutoPEFT: Automatic Configuration Search for Parameter-Efficient Fine-Tuning. (arXiv:2301.12132v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12132
&lt;/p&gt;
&lt;p&gt;
AutoPEFT&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;PEFT&#65288;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65289;&#37197;&#32622;&#25628;&#32034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#33258;&#21160;&#22320;&#25214;&#21040;&#26368;&#20339;&#30340;PEFT&#27169;&#22359;&#21644;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#20248;&#21270;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;&#22312;&#20856;&#22411;&#30340;NLP&#20219;&#21153;&#20013;&#65292;AutoPEFT&#34920;&#29616;&#20986;&#27604;&#25163;&#21160;&#35774;&#35745;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#19987;&#38376;&#30340;&#24494;&#35843;&#29992;&#20110;&#19979;&#28216;NLP&#20219;&#21153;&#65292;&#20294;&#36825;&#26679;&#30340;&#36807;&#31243;&#21487;&#33021;&#24456;&#26114;&#36149;&#12290;&#26368;&#36817;&#65292;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#36890;&#36807;&#26356;&#26032;&#27604;&#23436;&#25972;&#27169;&#22411;&#24494;&#35843;&#65288;FFT&#65289;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;PEFT&#37197;&#32622;&#26041;&#38754;&#20570;&#20986;&#26126;&#26234;&#30340;&#35774;&#35745;&#36873;&#25321;&#26159;&#19981;&#23481;&#26131;&#30340;&#65292;&#20363;&#22914;&#23427;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#21487;&#35843;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#29978;&#33267;&#26159;PEFT&#27169;&#22359;&#25554;&#20837;&#30340;&#22270;&#23618;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340;&#25163;&#21160;&#35774;&#35745;&#37197;&#32622;&#24456;&#21487;&#33021;&#22312;&#24615;&#33021;&#25928;&#29575;&#26435;&#34913;&#26041;&#38754;&#26159;&#27425;&#20248;&#30340;&#12290;&#21463;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#30340;&#36827;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoPEFT&#26469;&#33258;&#21160;&#36873;&#25321;PEFT&#37197;&#32622;&#65306;&#39318;&#20808;&#35774;&#35745;&#20855;&#26377;&#22810;&#20010;&#20195;&#34920;&#24615;PEFT&#27169;&#22359;&#30340;&#34920;&#36798;&#37197;&#32622;&#25628;&#32034;&#31354;&#38388;&#12290;&#28982;&#21518;&#20351;&#29992;&#22810;&#30446;&#26631;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#20302;&#25104;&#26412;&#30340;&#35774;&#32622;&#65292;&#20174;&#32780;&#21457;&#29616;&#20248;&#21270;&#20219;&#21153;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#30340;Pareto&#20248;&#21270;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#20856;&#22411;&#30340;NLP&#20219;&#21153;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#31867;&#12289;&#38382;&#31572;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#19978;&#35780;&#20272;&#20102;AutoPEFT&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#20248;&#20110;&#25163;&#21160;&#35774;&#35745;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pretrained language models are widely used in downstream NLP tasks via task-specific fine-tuning, but such procedures can be costly. Recently, Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task performance while updating a much smaller number of parameters compared to full model fine-tuning (FFT). However, it is non-trivial to make informed design choices on the PEFT configurations, such as their architecture, the number of tunable parameters, and even the layers in which the PEFT modules are inserted. Consequently, it is highly likely that the current, manually designed configurations are suboptimal in terms of their performance-efficiency trade-off. Inspired by advances in neural architecture search, we propose AutoPEFT for automatic PEFT configuration selection: we first design an expressive configuration search space with multiple representative PEFT modules as building blocks. Using multi-objective Bayesian optimisation in a low-cost setup, we then disc
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#31867;&#20284;&#20110;&#20799;&#31461;&#35821;&#35328;&#36755;&#20837;&#25968;&#25454;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#32423;&#27010;&#25324;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#27010;&#25324;&#30340;&#33021;&#21147;&#19982;&#27491;&#30830;&#30340;&#23618;&#32423;&#35268;&#21017;&#19981;&#31526;&#65292;&#20174;&#32467;&#35770;&#21487;&#20197;&#30475;&#20986;&#65292;&#20165;&#20174;&#35821;&#26009;&#24211;&#20013;&#36827;&#34892;&#31867;&#20154;&#27010;&#25324;&#38656;&#35201;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#24378;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2301.11462</link><description>&lt;p&gt;
&#35770;&#35821;&#35328;&#36755;&#20837;&#36139;&#20047;&#31243;&#24230;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#23618;&#32423;&#27010;&#25324;&#33021;&#21147;&#30340;&#24433;&#21709;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech. (arXiv:2301.11462v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11462
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#31867;&#20284;&#20110;&#20799;&#31461;&#35821;&#35328;&#36755;&#20837;&#25968;&#25454;&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#20004;&#31181;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#32423;&#27010;&#25324;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#27010;&#25324;&#30340;&#33021;&#21147;&#19982;&#27491;&#30830;&#30340;&#23618;&#32423;&#35268;&#21017;&#19981;&#31526;&#65292;&#20174;&#32467;&#35770;&#21487;&#20197;&#30475;&#20986;&#65292;&#20165;&#20174;&#35821;&#26009;&#24211;&#20013;&#36827;&#34892;&#31867;&#20154;&#27010;&#25324;&#38656;&#35201;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#24378;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#23545;&#20110;&#23398;&#20064;&#35821;&#27861;&#26102;&#36890;&#24120;&#20250;&#36873;&#25321;&#23618;&#27425;&#24615;&#35268;&#21017;&#65292;&#20154;&#20204;&#24819;&#30693;&#36947;&#36825;&#31181;&#20559;&#22909;&#26159;&#30001;&#20110;&#23398;&#20064;&#23618;&#32423;&#32467;&#26500;&#30340;&#20559;&#24046;&#36824;&#26159;&#20854;&#20182;&#19968;&#20123;&#24773;&#20917;&#12290;&#36890;&#36807;&#35757;&#32451;&#20004;&#31181;&#19981;&#24102;&#23618;&#32423;&#20559;&#35265;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#21487;&#33021;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#20004;&#31181;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#27010;&#25324;&#30340;&#33021;&#21147;&#26356;&#31526;&#21512;&#38169;&#35823;&#32447;&#24615;&#35268;&#21017;&#32780;&#19981;&#26159;&#27491;&#30830;&#30340;&#23618;&#32423;&#35268;&#21017;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#20165;&#20165;&#20174;&#35821;&#26009;&#24211;&#20013;&#36827;&#34892;&#31867;&#20154;&#27010;&#25324;&#38656;&#35201;&#27604;&#29616;&#26377;&#27169;&#22411;&#26356;&#24378;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
When acquiring syntax, children consistently choose hierarchical rules over competing non-hierarchical possibilities. Is this preference due to a learning bias for hierarchical structure, or due to more general biases that interact with hierarchical cues in children's linguistic input? We explore these possibilities by training LSTMs and Transformers - two types of neural networks without a hierarchical bias - on data similar in quantity and content to children's linguistic input: text from the CHILDES corpus. We then evaluate what these models have learned about English yes/no questions, a phenomenon for which hierarchical structure is crucial. We find that, though they perform well at capturing the surface statistics of child-directed speech (as measured by perplexity), both model types generalize in a way more consistent with an incorrect linear rule than the correct hierarchical rule. These results suggest that human-like generalization from text alone requires stronger biases than
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#27700;&#21360;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#22312;&#19981;&#38477;&#20302;&#25991;&#26412;&#36136;&#37327;&#30340;&#21069;&#25552;&#19979;&#23884;&#20837;&#20449;&#21495;&#65292;&#19988;&#21487;&#20197;&#20351;&#29992;&#39640;&#25928;&#30340;&#24320;&#28304;&#31639;&#27861;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#35813;&#25216;&#26415;&#21313;&#20998;&#40065;&#26834;&#21644;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2301.10226</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
A Watermark for Large Language Models. (arXiv:2301.10226v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#27700;&#21360;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#35813;&#25216;&#26415;&#21487;&#20197;&#22312;&#19981;&#38477;&#20302;&#25991;&#26412;&#36136;&#37327;&#30340;&#21069;&#25552;&#19979;&#23884;&#20837;&#20449;&#21495;&#65292;&#19988;&#21487;&#20197;&#20351;&#29992;&#39640;&#25928;&#30340;&#24320;&#28304;&#31639;&#27861;&#36827;&#34892;&#26816;&#27979;&#65292;&#24182;&#19988;&#35813;&#25216;&#26415;&#21313;&#20998;&#40065;&#26834;&#21644;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#23884;&#20837;&#20449;&#21495;&#65292;&#21363;&#23558;&#27700;&#21360;&#25216;&#26415;&#24212;&#29992;&#20110;&#27169;&#22411;&#36755;&#20986;&#65292;&#21487;&#20197;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28508;&#22312;&#30340;&#21361;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#27700;&#21360;&#25216;&#26415;&#26694;&#26550;&#12290;&#27700;&#21360;&#21487;&#20197;&#23884;&#20837;&#21040;&#25991;&#26412;&#20013;&#65292;&#23545;&#25991;&#26412;&#36136;&#37327;&#30340;&#24433;&#21709;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#39640;&#25928;&#30340;&#24320;&#28304;&#31639;&#27861;&#22312;&#19981;&#35775;&#38382;&#35821;&#35328;&#27169;&#22411;API&#25110;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26816;&#27979;&#12290;&#27700;&#21360;&#25216;&#26415;&#36890;&#36807;&#22312;&#29983;&#25104;&#21333;&#35789;&#20043;&#21069;&#36873;&#25321;&#19968;&#32452;&#38543;&#26426;&#30340;&#8220;&#32511;&#33394;&#8221;&#26631;&#35760;&#65292;&#28982;&#21518;&#22312;&#25277;&#26679;&#36807;&#31243;&#20013;&#36719;&#24615;&#22320;&#25512;&#24191;&#20351;&#29992;&#36825;&#20123;&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;P&#20540;&#32479;&#35745;&#26816;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#27700;&#21360;&#25216;&#26415;&#65292; &#24182;&#25512;&#23548;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#26694;&#26550;&#26469;&#20998;&#26512;&#27700;&#21360;&#25216;&#26415;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;Open Pretrained Transformer&#65288;OPT&#65289;&#23478;&#26063;&#30340;&#19968;&#20010;&#25968;&#21313;&#20159;&#21442;&#25968;&#27169;&#22411;&#26469;&#27979;&#35797;&#27700;&#21360;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.
&lt;/p&gt;</description></item><item><title>NarrowBERT&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;Transformer&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#31232;&#30095;&#21270;&#27169;&#22411;&#24182;&#20165;&#23545;&#25513;&#30721;&#20196;&#29260;&#36827;&#34892;&#25805;&#20316;&#65292;&#22312;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#25552;&#39640;&#20102;$2\times$&#20197;&#19978;&#30340;&#21534;&#21520;&#37327;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#23558;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;&#22810;&#36798;$3.5\times$&#12290;NarrowBERT&#22312;&#20960;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19982;&#26631;&#20934;BERT&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2301.04761</link><description>&lt;p&gt;
NarrowBERT: &#21152;&#36895;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#21644;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NarrowBERT: Accelerating Masked Language Model Pretraining and Inference. (arXiv:2301.04761v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04761
&lt;/p&gt;
&lt;p&gt;
NarrowBERT&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;Transformer&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#31232;&#30095;&#21270;&#27169;&#22411;&#24182;&#20165;&#23545;&#25513;&#30721;&#20196;&#29260;&#36827;&#34892;&#25805;&#20316;&#65292;&#22312;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#25552;&#39640;&#20102;$2\times$&#20197;&#19978;&#30340;&#21534;&#21520;&#37327;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#23558;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;&#22810;&#36798;$3.5\times$&#12290;NarrowBERT&#22312;&#20960;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19982;&#26631;&#20934;BERT&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26159;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38750;&#24120;&#25104;&#21151;&#30340;&#24418;&#24335;&#65292;&#20294;&#38543;&#30528;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#19981;&#26029;&#22686;&#22823;&#65292;&#25191;&#34892;&#39044;&#35757;&#32451;&#30340;&#25104;&#26412;&#21464;&#24471;&#36234;&#26469;&#36234;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NarrowBERT&#65292;&#19968;&#31181;&#25913;&#36827;&#30340;Transformer&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;$2\times$&#20197;&#19978;&#30340;&#36895;&#24230;&#25552;&#39640;&#20102;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21534;&#21520;&#37327;&#12290;NarrowBERT&#31232;&#30095;&#21270;&#20102;Transformer&#27169;&#22411;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#65292;&#33258;&#27880;&#24847;&#21147;&#26597;&#35810;&#21644;&#21069;&#39304;&#23618;&#20165;&#23545;&#27599;&#20010;&#21477;&#23376;&#30340;&#25513;&#30721;&#20196;&#29260;&#36827;&#34892;&#25805;&#20316;&#65292;&#32780;&#19981;&#26159;&#20687;&#36890;&#24120;&#30340;Transformer&#32534;&#30721;&#22120;&#37027;&#26679;&#23545;&#25152;&#26377;&#20196;&#29260;&#36827;&#34892;&#25805;&#20316;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;NarrowBERT&#22312;&#25512;&#29702;&#26102;&#23558;&#21534;&#21520;&#37327;&#25552;&#39640;&#20102;&#22810;&#36798;$3.5\times$&#65292;&#22312;&#20687;MNLI&#36825;&#26679;&#30340;&#21477;&#23376;&#32534;&#30721;&#20219;&#21153;&#19978;&#65292;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#25110;&#27809;&#26377;&#26126;&#26174;&#38477;&#20302;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#23519;&#20102;NarrowBERT&#22312;IMDB&#21644;Amazon&#35780;&#35770;&#20998;&#31867;&#20197;&#21450;CoNLL NER&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20854;&#19982;&#26631;&#20934;BERT&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale language model pretraining is a very successful form of self-supervised learning in natural language processing, but it is increasingly expensive to perform as the models and pretraining corpora have become larger over time. We propose NarrowBERT, a modified transformer encoder that increases the throughput for masked language model pretraining by more than $2\times$. NarrowBERT sparsifies the transformer model such that the self-attention queries and feedforward layers only operate on the masked tokens of each sentence during pretraining, rather than all of the tokens as with the usual transformer encoder. We also show that NarrowBERT increases the throughput at inference time by as much as $3.5\times$ with minimal (or no) performance degradation on sentence encoding tasks like MNLI. Finally, we examine the performance of NarrowBERT on the IMDB and Amazon reviews classification and CoNLL NER tasks and show that it is also comparable to standard BERT performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCO&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21487;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#20107;&#23454;&#25968;&#25454;&#65292;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31561;&#20219;&#21153;&#30340;&#22240;&#26524;&#25512;&#29702;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#25968;&#25454;&#35757;&#32451;&#26041;&#27861;&#65292;&#25928;&#26524;&#26356;&#22909;&#19988;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2212.10534</link><description>&lt;p&gt;
DISCO: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#28860;&#30701;&#35821;&#21453;&#20107;&#23454;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
DISCO: Distilling Phrasal Counterfactuals with Large Language Models. (arXiv:2212.10534v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DISCO&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21487;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#20107;&#23454;&#25968;&#25454;&#65292;&#29992;&#20110;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#31561;&#20219;&#21153;&#30340;&#22240;&#26524;&#25512;&#29702;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#25968;&#25454;&#35757;&#32451;&#26041;&#27861;&#65292;&#25928;&#26524;&#26356;&#22909;&#19988;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#21453;&#20107;&#23454;&#22686;&#24191;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#20219;&#21153;&#30340;&#22240;&#26524;&#32467;&#26500;&#34920;&#36798;&#65292;&#20174;&#32780;&#23454;&#29616;&#31283;&#20581;&#30340;&#27867;&#21270;&#12290;&#20294;&#23545;&#20110;&#22823;&#22810;&#25968;&#20219;&#21153;&#32780;&#35328;&#65292;&#39640;&#36136;&#37327;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#24456;&#23569;&#19988;&#38590;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#12290;&#24403;&#20351;&#29992;&#20247;&#21253;&#26041;&#27861;&#36827;&#34892;&#29983;&#25104;&#26102;&#65292;&#36890;&#24120;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#37117;&#26377;&#38480;&#12290;&#24403;&#20351;&#29992;&#26377;&#30417;&#30563;&#26041;&#27861;&#26102;&#65292;&#35201;&#23558;&#20854;&#25193;&#23637;&#21040;&#26032;&#30340;&#21453;&#20107;&#23454;&#32500;&#24230;&#26159;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DISCO&#65288;DIStilled COunterfactual Data&#65289;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#22312;&#35268;&#27169;&#19978;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#12290;DISCO&#24037;&#31243;&#24072;&#20351;&#29992;&#22823;&#22411;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#20197;&#29983;&#25104;&#30701;&#35821;&#25200;&#21160;&#12290;&#28982;&#21518;&#65292;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#25945;&#24072;&#27169;&#22411;&#36807;&#28388;&#36825;&#20123;&#29983;&#25104;&#65292;&#20197;&#25552;&#21462;&#39640;&#36136;&#37327;&#30340;&#21453;&#20107;&#23454;&#25968;&#25454;&#12290;&#34429;&#28982;&#26159;&#38754;&#21521;&#20219;&#21153;&#30340;&#65292;&#25105;&#20204;&#24212;&#29992;&#25105;&#20204;&#30340;&#27969;&#31243;&#26469;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;&#22312;&#20687;NLI&#21387;&#21147;&#27979;&#35797;&#36825;&#26679;&#30340;&#25361;&#25112;&#24615;&#35780;&#20272;&#20013;&#65292;&#29992;DISCO&#29983;&#25104;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#30456;&#23545;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#27604;&#20351;&#29992;&#20256;&#32479;&#65288;&#38750;&#21453;&#20107;&#23454;&#22686;&#24378;&#65289;&#30340;&#25968;&#25454;&#35757;&#32451;&#30340;&#22823;&#27169;&#22411;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#29983;&#25104;&#21453;&#20107;&#23454;&#25968;&#25454;&#65292;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#30340;&#22240;&#26524;&#25512;&#29702;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models trained with counterfactually augmented data learn representations of the causal structure of tasks, enabling robust generalization. However, high-quality counterfactual data is scarce for most tasks and not easily generated at scale. When crowdsourced, such data is typically limited in scale and diversity; when generated using supervised methods, it is computationally expensive to extend to new counterfactual dimensions. In this work, we introduce DISCO (DIStilled COunterfactual Data), a new method for automatically generating high quality counterfactual data at scale. DISCO engineers prompts to generate phrasal perturbations with a large general language model. Then, a task-specific teacher model filters these generations to distill high-quality counterfactual data. While task-agnostic, we apply our pipeline to the task of natural language inference (NLI) and find that on challenging evaluations such as the NLI stress test, comparatively smaller student models trained with DIS
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#29983;&#25104;&#27491;&#30830;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#25351;&#26631;VISOR&#20197;&#34913;&#37327;&#29983;&#25104;&#22270;&#20687;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#21457;&#29616;&#24403;&#21069;T2I&#27169;&#22411;&#23613;&#31649;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20294;&#20854;&#31354;&#38388;&#19978;&#20934;&#30830;&#30340;&#22270;&#20687;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#22312;&#31354;&#38388;&#35859;&#35789;&#21644;&#22330;&#26223;&#20851;&#31995;&#29702;&#35299;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2212.10015</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#31354;&#38388;&#20851;&#31995;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Spatial Relationships in Text-to-Image Generation. (arXiv:2212.10015v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#29983;&#25104;&#27491;&#30830;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#25351;&#26631;VISOR&#20197;&#34913;&#37327;&#29983;&#25104;&#22270;&#20687;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#21457;&#29616;&#24403;&#21069;T2I&#27169;&#22411;&#23613;&#31649;&#21487;&#20197;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20294;&#20854;&#31354;&#38388;&#19978;&#20934;&#30830;&#30340;&#22270;&#20687;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#22312;&#31354;&#38388;&#35859;&#35789;&#21644;&#22330;&#26223;&#20851;&#31995;&#29702;&#35299;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#29702;&#35299;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#22522;&#26412;&#26041;&#38754;&#65292;&#23545;&#20110;&#20154;&#31867;&#32423;&#21035;&#30340;&#22270;&#20687;&#25512;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#26159;&#22522;&#30784;&#35821;&#35328;&#29702;&#35299;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#26368;&#36817;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#65288;T2I&#65289;&#27169;&#22411;&#22312;&#36924;&#30495;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#21487;&#38752;&#31354;&#38388;&#29702;&#35299;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;T2I&#27169;&#22411;&#29983;&#25104;&#27491;&#30830;&#31354;&#38388;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;VISOR&#35780;&#20272;&#25351;&#26631;&#65292;&#23427;&#25429;&#25417;&#20102;&#25991;&#26412;&#20013;&#25551;&#36848;&#30340;&#31354;&#38388;&#20851;&#31995;&#22312;&#22270;&#20687;&#20013;&#26159;&#21542;&#20934;&#30830;&#29983;&#25104;&#12290;&#20026;&#20102;&#22522;&#20934;&#29616;&#26377;&#27169;&#22411;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;&#25551;&#36848;&#20004;&#20010;&#23545;&#35937;&#21450;&#23427;&#20204;&#20043;&#38388;&#31354;&#38388;&#20851;&#31995;&#30340;&#21477;&#23376;&#25968;&#25454;&#38598;SR2D&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#35780;&#20272;&#27969;&#31243;&#26469;&#35782;&#21035;&#29289;&#20307;&#21450;&#20854;&#31354;&#38388;&#20851;&#31995;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#35780;&#20272;T2I&#27169;&#22411;&#26102;&#37319;&#29992;&#23427;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#21457;&#29616;&#65292;&#20063;&#23601;&#26159;&#23613;&#31649;&#26368;&#26032;&#30340;T2I&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#65292;&#20294;&#23427;&#20204;&#29983;&#25104;&#31354;&#38388;&#19978;&#20934;&#30830;&#30340;&#22270;&#20687;&#33021;&#21147;&#20173;&#28982;&#19981;&#36275;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#22312;&#31354;&#38388;&#35859;&#35789;&#65288;&#22914;'&#22312;&#21069;&#38754;'&#21644;'&#22312;&#21518;&#38754;'&#65289;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#19988;&#22312;&#22330;&#26223;&#30340;&#20851;&#31995;&#29702;&#35299;&#26041;&#38754;&#20063;&#26377;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial understanding is a fundamental aspect of computer vision and integral for human-level reasoning about images, making it an important component for grounded language understanding. While recent text-to-image synthesis (T2I) models have shown unprecedented improvements in photorealism, it is unclear whether they have reliable spatial understanding capabilities. We investigate the ability of T2I models to generate correct spatial relationships among objects and present VISOR, an evaluation metric that captures how accurately the spatial relationship described in text is generated in the image. To benchmark existing models, we introduce a dataset, SR2D, that contains sentences describing two objects and the spatial relationship between them. We construct an automated evaluation pipeline to recognize objects and their spatial relationships, and employ it in a large-scale evaluation of T2I models. Our experiments reveal a surprising finding that, although state-of-the-art T2I models 
&lt;/p&gt;</description></item><item><title>DuNST&#26159;&#19968;&#31181;&#21452;&#37325;&#22122;&#22768;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25200;&#21160;&#29983;&#25104;&#30340;&#20266;&#25991;&#26412;&#65292;&#23558;&#20266;&#25991;&#26412;&#26631;&#35760;&#21644;&#26080;&#26631;&#31614;&#30340;&#20266;&#26631;&#31614;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#19988;&#21487;&#20197;&#32531;&#35299;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#31354;&#38388;&#30340;&#38480;&#21046;&#24615;&#27867;&#21270;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2212.08724</link><description>&lt;p&gt;
DuNST&#65306;&#21452;&#37325;&#22122;&#22768;&#33258;&#35757;&#32451;&#29992;&#20110;&#21322;&#30417;&#30563;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DuNST: Dual Noisy Self Training for Semi-Supervised Controllable Text Generation. (arXiv:2212.08724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08724
&lt;/p&gt;
&lt;p&gt;
DuNST&#26159;&#19968;&#31181;&#21452;&#37325;&#22122;&#22768;&#33258;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#30417;&#30563;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25200;&#21160;&#29983;&#25104;&#30340;&#20266;&#25991;&#26412;&#65292;&#23558;&#20266;&#25991;&#26412;&#26631;&#35760;&#21644;&#26080;&#26631;&#31614;&#30340;&#20266;&#26631;&#31614;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#19988;&#21487;&#20197;&#32531;&#35299;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#31354;&#38388;&#30340;&#38480;&#21046;&#24615;&#27867;&#21270;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35821;&#35328;&#29702;&#35299;&#65292;&#33258;&#35757;&#32451;&#65288;ST&#65289;&#36890;&#36807;&#22686;&#21152;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#27425;&#25968;&#26469;&#25193;&#20805;&#26631;&#35760;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#65292;&#26377;&#20102;&#36739;&#22823;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#24102;&#23646;&#24615;&#25511;&#21046;&#30340;&#35821;&#35328;&#29983;&#25104;&#20013;&#65292;&#23558;ST&#32435;&#20837;&#20854;&#20013;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21482;&#33021;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#20266;&#25991;&#26412;&#36827;&#34892;&#22686;&#24378;&#30340;&#29983;&#25104;&#27169;&#22411;&#20250;&#36807;&#24230;&#24378;&#35843;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#31354;&#38388;&#65292;&#21463;&#21040;&#21463;&#38480;&#30340;&#27867;&#21270;&#36793;&#30028;&#25152;&#22256;&#25200;&#12290;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;ST&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;DuNST&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;DuNST&#36890;&#36807;&#19968;&#20010;&#20849;&#20139;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#32852;&#21512;&#29983;&#25104;&#25991;&#26412;&#21644;&#23545;&#24212;&#30340;&#20998;&#31867;&#26631;&#31614;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#28789;&#27963;&#30340;&#22122;&#22768;&#26469;&#25200;&#20081;&#29983;&#25104;&#30340;&#20266;&#25991;&#26412;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#26500;&#24314;&#24182;&#21033;&#29992;&#26469;&#33258;&#32473;&#23450;&#26631;&#31614;&#30340;&#20266;&#25991;&#26412;&#20197;&#21450;&#26469;&#33258;&#21487;&#29992;&#26080;&#26631;&#31614;&#25991;&#26412;&#30340;&#20266;&#26631;&#31614;&#65292;&#22312;ST&#36807;&#31243;&#20013;&#36880;&#28176;&#25913;&#36827;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;DuNST&#21487;&#20197;&#34987;&#35270;&#20026;&#21521;&#28508;&#22312;&#30495;&#23454;&#25991;&#26412;&#30340;&#25506;&#32034;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training (ST) has prospered again in language understanding by augmenting the fine-tuning of pre-trained language models when labeled data is insufficient. However, it remains challenging to incorporate ST into attribute-controllable language generation. Augmented by only self-generated pseudo text, generation models over-emphasize exploitation of the previously learned space, suffering from a constrained generalization boundary. We revisit ST and propose a novel method, DuNST to alleviate this problem. DuNST jointly models text generation and classification with a shared Variational AutoEncoder and corrupts the generated pseudo text by two kinds of flexible noise to disturb the space. In this way, our model could construct and utilize both pseudo text from given labels and pseudo labels from available unlabeled text, which are gradually refined during the ST process. We theoretically demonstrate that DuNST can be regarded as enhancing exploration towards the potential real text s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25688;&#35201;&#26174;&#35201;&#24615;&#21327;&#35758;&#65292;ACUs&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#35780;&#20272;&#21327;&#35758;&#20302;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#24314;&#31435;&#20102;&#19968;&#20010;&#36229;&#22823;&#35268;&#27169;&#35780;&#20272;&#25968;&#25454;&#38598;RoSE&#24182;&#36827;&#34892;&#20102;&#20154;&#31867;&#35780;&#20272;&#21644;&#33258;&#21160;&#35780;&#20272;&#65292;&#20026;&#25688;&#35201;&#35780;&#20272;&#30340;&#30456;&#20851;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2212.07981</link><description>&lt;p&gt;
&#37325;&#35775;&#40644;&#37329;&#26631;&#20934;&#65306;&#20197;&#20581;&#22766;&#30340;&#20154;&#31867;&#35780;&#20272;&#20026;&#22522;&#30784;&#30340;&#25688;&#35201;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Gold Standard: Grounding Summarization Evaluation with Robust Human Evaluation. (arXiv:2212.07981v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07981
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25688;&#35201;&#26174;&#35201;&#24615;&#21327;&#35758;&#65292;ACUs&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#35780;&#20272;&#21327;&#35758;&#20302;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#24314;&#31435;&#20102;&#19968;&#20010;&#36229;&#22823;&#35268;&#27169;&#35780;&#20272;&#25968;&#25454;&#38598;RoSE&#24182;&#36827;&#34892;&#20102;&#20154;&#31867;&#35780;&#20272;&#21644;&#33258;&#21160;&#35780;&#20272;&#65292;&#20026;&#25688;&#35201;&#35780;&#20272;&#30340;&#30456;&#20851;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35780;&#20272;&#26159;&#33258;&#21160;&#25688;&#35201;&#31995;&#32479;&#21644;&#35780;&#20272;&#25351;&#26631;&#35780;&#20272;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25688;&#35201;&#20154;&#31867;&#35780;&#20272;&#30740;&#31350;&#35201;&#20040;&#23384;&#22312;&#24456;&#20302;&#30340;&#35780;&#20998;&#21592;&#19968;&#33268;&#24615;&#65292;&#35201;&#20040;&#35268;&#27169;&#19981;&#36275;&#65292;&#24182;&#19988;&#32570;&#20047;&#28145;&#20837;&#30340;&#20154;&#31867;&#35780;&#20272;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20174;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#35299;&#20915;&#20102;&#29616;&#26377;&#25688;&#35201;&#35780;&#20272;&#30340;&#32570;&#28857;&#65306;&#65288;1&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#36807;&#30340;&#25688;&#35201;&#26174;&#35201;&#24615;&#21327;&#35758;&#65292;&#21363;&#21407;&#23376;&#20869;&#23481;&#21333;&#20301;&#65288;ACUs&#65289;&#65292;&#23427;&#22522;&#20110;&#32454;&#31890;&#24230;&#35821;&#20041;&#21333;&#20803;&#65292;&#20801;&#35768;&#33719;&#24471;&#36739;&#39640;&#30340;&#35780;&#20998;&#21592;&#19968;&#33268;&#24615;&#12290;&#65288;2&#65289;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#20154;&#31867;&#35780;&#20272;&#25968;&#25454;&#38598;RoSE&#65292;&#20854;&#20013;&#21253;&#25324;&#36229;&#36807;28&#20010;&#34920;&#29616;&#26368;&#20339;&#30340;&#31995;&#32479;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;22,000&#20010;&#25688;&#35201;&#32423;&#21035;&#27880;&#37322;&#12290;&#65288;3&#65289;&#25105;&#20204;&#23545;&#22235;&#31181;&#20154;&#31867;&#35780;&#20272;&#21327;&#35758;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;&#35774;&#32622;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#65288;4&#65289;&#25105;&#20204;&#20351;&#29992;&#20849;&#29616;&#30697;&#38453;&#20316;&#20026;&#20154;&#31867;&#35780;&#20272;&#30340;&#20195;&#29702;&#65292;&#35780;&#20272;&#20102;50&#20010;&#33258;&#21160;&#25351;&#26631;&#21450;&#20854;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human evaluation is the foundation upon which the evaluation of both summarization systems and automatic metrics rests. However, existing human evaluation studies for summarization either exhibit a low inter-annotator agreement or have insufficient scale, and an in-depth analysis of human evaluation is lacking. Therefore, we address the shortcomings of existing summarization evaluation along the following axes: (1) We propose a modified summarization salience protocol, Atomic Content Units (ACUs), which is based on fine-grained semantic units and allows for a high inter-annotator agreement. (2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large human evaluation dataset consisting of 22,000 summary-level annotations over 28 top-performing systems on three datasets. (3) We conduct a comparative study of four human evaluation protocols, underscoring potential confounding factors in evaluation setups. (4) We evaluate 50 automatic metrics and their variants using the co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#22312;&#22330;&#23398;&#20064;&#32773;&#23398;&#20064;&#26032;&#25216;&#33021;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#65292;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.01692</link><description>&lt;p&gt;
&#22312;&#22330;&#23398;&#20064;&#32773;&#33021;&#21542;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#25512;&#29702;&#27010;&#24565;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can In-context Learners Learn a Reasoning Concept from Demonstrations?. (arXiv:2212.01692v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#22312;&#22330;&#23398;&#20064;&#32773;&#23398;&#20064;&#26032;&#25216;&#33021;&#12290;&#36890;&#36807;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#65292;&#36825;&#20010;&#26041;&#27861;&#21487;&#20197;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20174;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26032;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22330;&#23398;&#20064;&#32773;&#22823;&#37096;&#20998;&#20381;&#36182;&#20110;&#20182;&#20204;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#65292;&#22914;&#26631;&#31614;&#30340;&#24773;&#24863;&#65292;&#32780;&#19981;&#26159;&#22312;&#36755;&#20837;&#20013;&#25214;&#21040;&#26032;&#30340;&#20851;&#32852;&#24615;&#12290;&#28982;&#32780;&#65292;&#24120;&#29992;&#30340;&#23569;&#26679;&#26412;&#35780;&#20272;&#35774;&#32622;&#20351;&#29992;&#38543;&#26426;&#36873;&#25321;&#30340;&#22312;&#22330;&#28436;&#31034;&#26080;&#27861;&#21306;&#20998;&#27169;&#22411;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#25216;&#33021;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#22823;&#37096;&#20998;&#38543;&#26426;&#36873;&#25321;&#30340;&#28436;&#31034;&#24182;&#19981;&#21576;&#29616;&#36229;&#36234;&#26292;&#38706;&#20110;&#26032;&#20219;&#21153;&#20998;&#24067;&#30340;&#39044;&#27979;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#22312;&#27169;&#22411;&#35760;&#24518;&#29420;&#31435;&#30340;&#24773;&#20917;&#19979;&#21306;&#20998;&#27169;&#22411;&#30340;&#22312;&#22330;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#65292;&#36873;&#25321;&#19982;&#39044;&#27979;&#31034;&#20363;&#20849;&#20139;&#21487;&#33021;&#20449;&#24687;&#30340;&#28436;&#31034;&#12290;&#25105;&#20204;&#20174;&#27880;&#37322;&#35299;&#37322;&#20013;&#25552;&#21462;&#20102;&#19968;&#32452;&#36825;&#26679;&#30340;&#27010;&#24565;&#65292;&#24182;&#27979;&#37327;&#20102;&#27169;&#22411;&#23637;&#31034;&#36825;&#20123;&#27010;&#24565;&#21487;&#20197;&#33719;&#24471;&#22810;&#23569;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models show an emergent ability to learn a new task from a small number of input-output demonstrations. However, recent work shows that in-context learners largely rely on their pre-trained knowledge, such as the sentiment of the labels, instead of finding new associations in the input. However, the commonly-used few-shot evaluation settings using a random selection of in-context demonstrations can not disentangle models' ability to learn a new skill from demonstrations, as most of the randomly-selected demonstrations do not present relations informative for prediction beyond exposing the new task distribution.  To disentangle models' in-context learning ability independent of models' memory, we introduce a Conceptual few-shot learning method selecting the demonstrations sharing a possibly-informative concept with the predicted sample. We extract a set of such concepts from annotated explanations and measure how much can models benefit from presenting these concepts in f
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#24212;&#29992;&#20110;&#35821;&#38899;&#20998;&#31867;&#38382;&#39064;&#21644;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#30340;&#20869;&#30465;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;Transformer&#27880;&#24847;&#21147;&#22270;&#21644;&#23884;&#20837;&#30340;&#25299;&#25169;&#21644;&#20195;&#25968;&#29305;&#24449;&#12290;&#22312;&#36825;&#20123;&#29305;&#24449;&#22522;&#30784;&#19978;&#26500;&#24314;&#30340;&#31616;&#21333;&#32447;&#24615;&#20998;&#31867;&#22120;&#32988;&#36807;&#31934;&#35843;&#20998;&#31867;&#22120;&#22836;&#37096;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#26368;&#20248;&#24615;&#33021;&#12290;&#25299;&#25169;&#29305;&#24449;&#33021;&#22815;&#25581;&#31034;&#35821;&#38899;Transformer&#22836;&#30340;&#21151;&#33021;&#35282;&#33394;&#65292;&#36825;&#34920;&#26126;TDA&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35821;&#38899;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.17223</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#35821;&#38899;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Topological Data Analysis for Speech Processing. (arXiv:2211.17223v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#24212;&#29992;&#20110;&#35821;&#38899;&#20998;&#31867;&#38382;&#39064;&#21644;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#30340;&#20869;&#30465;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#22522;&#20110;Transformer&#27880;&#24847;&#21147;&#22270;&#21644;&#23884;&#20837;&#30340;&#25299;&#25169;&#21644;&#20195;&#25968;&#29305;&#24449;&#12290;&#22312;&#36825;&#20123;&#29305;&#24449;&#22522;&#30784;&#19978;&#26500;&#24314;&#30340;&#31616;&#21333;&#32447;&#24615;&#20998;&#31867;&#22120;&#32988;&#36807;&#31934;&#35843;&#20998;&#31867;&#22120;&#22836;&#37096;&#65292;&#24182;&#23454;&#29616;&#20102;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#26368;&#20248;&#24615;&#33021;&#12290;&#25299;&#25169;&#29305;&#24449;&#33021;&#22815;&#25581;&#31034;&#35821;&#38899;Transformer&#22836;&#30340;&#21151;&#33021;&#35282;&#33394;&#65292;&#36825;&#34920;&#26126;TDA&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35821;&#38899;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#24212;&#29992;&#20110;&#35821;&#38899;&#20998;&#31867;&#38382;&#39064;&#21450;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;HuBERT&#30340;&#20869;&#30465;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20123;&#22522;&#20110;Transformer&#27880;&#24847;&#21147;&#22270;&#21644;&#23884;&#20837;&#30340;&#25299;&#25169;&#21644;&#20195;&#25968;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36825;&#20123;&#29305;&#24449;&#22522;&#30784;&#19978;&#26500;&#24314;&#30340;&#31616;&#21333;&#32447;&#24615;&#20998;&#31867;&#22120;&#32988;&#36807;&#31934;&#35843;&#20998;&#31867;&#22120;&#22836;&#37096;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#22235;&#20010;&#24120;&#35265;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#32422;9%&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#21644;5%&#30340;ERR&#25552;&#39640;&#12290;&#22312;CREMA-D&#25968;&#25454;&#38598;&#19978;&#65292;&#25552;&#20986;&#30340;&#29305;&#24449;&#38598;&#36798;&#21040;&#20102;&#20934;&#30830;&#29575;80.155&#30340;&#26032;&#30340;&#26368;&#20248;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25299;&#25169;&#29305;&#24449;&#33021;&#22815;&#25581;&#31034;&#35821;&#38899;Transformer&#22836;&#30340;&#21151;&#33021;&#35282;&#33394;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#27809;&#26377;&#20219;&#20309;&#19979;&#28216;&#31934;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#22836;&#21487;&#21306;&#20998;&#26679;&#26412;&#26469;&#28304;&#65288;&#33258;&#28982;/&#21512;&#25104;&#65289;&#25110;&#22768;&#38899;&#23545;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;TDA&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35821;&#38899;&#20998;&#26512;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#38656;&#35201;&#32467;&#26500;&#39044;&#27979;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#38468;&#24405;&#12289;&#23545;TDA&#21644;HuBERT&#27169;&#22411;&#30340;&#20171;&#32461;&#20197;&#21450;&#20351;&#29992;&#20854;&#20182;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We apply topological data analysis (TDA) to speech classification problems and to the introspection of a pretrained speech model, HuBERT. To this end, we introduce a number of topological and algebraic features derived from Transformer attention maps and embeddings. We show that a simple linear classifier built on top of such features outperforms a fine-tuned classification head. In particular, we achieve an improvement of about $9\%$ accuracy and $5\%$ ERR on four common datasets; on CREMA-D, the proposed feature set reaches a new state of the art performance with accuracy $80.155$. We also show that topological features are able to reveal functional roles of speech Transformer heads; e.g., we find the heads capable to distinguish between pairs of sample sources (natural/synthetic) or voices without any downstream fine-tuning. Our results demonstrate that TDA is a promising new approach for speech analysis, especially for tasks that require structural prediction. Appendices, an introd
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20351;&#20854;&#21487;&#20197;&#20174;&#22806;&#37096;&#35760;&#24518;&#26816;&#32034;&#30456;&#20851;&#22270;&#29255;&#21644;&#25991;&#26412;&#20449;&#24687;&#65292;&#20197;&#26356;&#21487;&#25193;&#23637;&#21644;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#38598;&#25104;&#30693;&#35782;&#65292;&#25552;&#20379;&#26356;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.12561</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Retrieval-Augmented Multimodal Language Modeling. (arXiv:2211.12561v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20351;&#20854;&#21487;&#20197;&#20174;&#22806;&#37096;&#35760;&#24518;&#26816;&#32034;&#30456;&#20851;&#22270;&#29255;&#21644;&#25991;&#26412;&#20449;&#24687;&#65292;&#20197;&#26356;&#21487;&#25193;&#23637;&#21644;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#38598;&#25104;&#30693;&#35782;&#65292;&#25552;&#20379;&#26356;&#39640;&#36136;&#37327;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22914;DALL-E&#21644;CM3&#22312;&#25991;&#26412;&#19982;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#23558;&#25152;&#26377;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#65288;&#20363;&#22914;&#22467;&#33778;&#23572;&#38081;&#22612;&#30340;&#22806;&#35266;&#65289;&#23384;&#20648;&#22312;&#27169;&#22411;&#21442;&#25968;&#20013;&#65292;&#38656;&#35201;&#36234;&#26469;&#36234;&#22823;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#25968;&#25454;&#26469;&#25429;&#25417;&#26356;&#22810;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#20197;&#26356;&#21487;&#25193;&#23637;&#21644;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#38598;&#25104;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20351;&#22522;&#26412;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#29983;&#25104;&#22120;&#65289;&#33021;&#22815;&#21442;&#32771;&#30001;&#26816;&#32034;&#22120;&#20174;&#22806;&#37096;&#35760;&#24518;&#65288;&#20363;&#22914;&#32593;&#19978;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#65289;&#20013;&#25552;&#21462;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;CLIP&#20316;&#20026;&#26816;&#32034;&#22120;&#65292;&#20351;&#29992;&#22312;LAION&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;CM3 Transformer&#20316;&#20026;&#29983;&#25104;&#22120;&#12290;&#25105;&#20204;&#24471;&#21040;&#30340;&#27169;&#22411;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;CM3&#65288;RA-CM3&#65289;&#65292;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#26816;&#32034;&#21644;&#29983;&#25104;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;RA-CM3&#22312;&#22270;&#20687;&#21644;&#22270;&#29255;&#29983;&#25104;&#26041;&#38754;&#22343;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#22810;&#27169;&#24577;&#27169;&#22411;&#22914;DALL-E&#21644;CM3&#65292;&#24182;&#19988;&#20854;&#26816;&#32034;&#27169;&#22359;&#21487;&#20197;&#26377;&#25928;&#22320;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#25152;&#29983;&#25104;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent multimodal models such as DALL-E and CM3 have achieved remarkable progress in text-to-image and image-to-text generation. However, these models store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the model parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a retrieval-augmented multimodal model, which enables a base multimodal model (generator) to refer to relevant text and images fetched by a retriever from external memory (e.g., documents on the web). Specifically, for the retriever, we use a pretrained CLIP, and for the generator, we train a CM3 Transformer on the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate both text and images. We show that RA-CM3 significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption gen
&lt;/p&gt;</description></item><item><title>SmoothQuant&#26159;&#19968;&#31181;&#35757;&#32451;&#26080;&#38656;&#30340;&#36890;&#29992;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;8&#20301;&#26435;&#37325;&#12289;8&#20301;&#28608;&#27963;&#65288;W8A8&#65289;&#37327;&#21270;&#12290;SmoothQuant&#36890;&#36807;&#25968;&#23398;&#31561;&#25928;&#36716;&#25442;&#23558;&#37327;&#21270;&#38590;&#24230;&#20174;&#28608;&#27963;&#31227;&#21040;&#26435;&#37325;&#65292;&#20351;&#24471;&#25152;&#26377;&#30697;&#38453;&#20056;&#27861;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;INT8&#37327;&#21270;&#25104;&#20026;&#21487;&#33021;&#65292;&#20855;&#26377;&#26368;&#39640;1.56&#20493;&#21152;&#36895;&#21644;2&#20493;&#20869;&#23384;&#20943;&#23569;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.10438</link><description>&lt;p&gt;
SmoothQuant&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#30830;&#39640;&#25928;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models. (arXiv:2211.10438v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10438
&lt;/p&gt;
&lt;p&gt;
SmoothQuant&#26159;&#19968;&#31181;&#35757;&#32451;&#26080;&#38656;&#30340;&#36890;&#29992;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#31934;&#24230;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;8&#20301;&#26435;&#37325;&#12289;8&#20301;&#28608;&#27963;&#65288;W8A8&#65289;&#37327;&#21270;&#12290;SmoothQuant&#36890;&#36807;&#25968;&#23398;&#31561;&#25928;&#36716;&#25442;&#23558;&#37327;&#21270;&#38590;&#24230;&#20174;&#28608;&#27963;&#31227;&#21040;&#26435;&#37325;&#65292;&#20351;&#24471;&#25152;&#26377;&#30697;&#38453;&#20056;&#27861;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;INT8&#37327;&#21270;&#25104;&#20026;&#21487;&#33021;&#65292;&#20855;&#26377;&#26368;&#39640;1.56&#20493;&#21152;&#36895;&#21644;2&#20493;&#20869;&#23384;&#20943;&#23569;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#21644;&#20869;&#23384;&#12290;&#37327;&#21270;&#21487;&#20197;&#20943;&#23569;&#20869;&#23384;&#24182;&#21152;&#36895;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#22312;&#20445;&#25345;&#31934;&#24230;&#21644;&#30828;&#20214;&#25928;&#29575;&#30340;&#21516;&#26102;&#32500;&#25345;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SmoothQuant&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#12289;&#20445;&#25345;&#31934;&#24230;&#21644;&#36890;&#29992;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;LLMs&#30340;8&#20301;&#26435;&#37325;&#12289;8&#20301;&#28608;&#27963;&#65288;W8A8&#65289;&#37327;&#21270;&#12290;&#22522;&#20110;&#26435;&#37325;&#26131;&#20110;&#37327;&#21270;&#32780;&#28608;&#27963;&#19981;&#26131;&#37327;&#21270;&#30340;&#20107;&#23454;&#65292;SmoothQuant&#36890;&#36807;&#25968;&#23398;&#31561;&#25928;&#36716;&#25442;&#23558;&#37327;&#21270;&#38590;&#24230;&#20174;&#28608;&#27963;&#31227;&#33267;&#26435;&#37325;&#65292;&#36890;&#36807;&#31163;&#32447;&#24179;&#28369;&#28608;&#27963;&#30340;&#24322;&#24120;&#20540;&#26469;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;SmoothQuant&#20351;&#25152;&#26377;&#30697;&#38453;&#20056;&#27861;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;INT8&#37327;&#21270;&#25104;&#20026;&#21487;&#33021;&#65292;&#21253;&#25324;OPT&#12289;BLOOM&#12289;GLM&#12289;MT-NLG&#21644;LLaMA&#31995;&#21015;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;LLMs&#30340;&#26368;&#39640;1.56&#20493;&#21152;&#36895;&#21644;2&#20493;&#20869;&#23384;&#20943;&#23569;&#65292;&#24182;&#19988;&#20960;&#20046;&#19981;&#20250;&#26377;&#31934;&#24230;&#25439;&#22833;&#12290;SmoothQuant&#21487;&#20197;&#20026;530B LLM&#25552;&#20379;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and LLaMA family. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM wi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#36890;&#29992;&#37492;&#21035;&#22120;&#65292;&#36890;&#36807;&#22312;NLP&#20219;&#21153;&#20013;&#20351;&#29992;&#21333;&#19968;&#30340;&#37492;&#21035;&#22120;&#65292;&#21487;&#23454;&#29616;&#27604;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#21462;&#24471;&#20102;&#22312;T0&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.08099</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#36890;&#29992;&#37492;&#21035;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Universal Discriminator for Zero-Shot Generalization. (arXiv:2211.08099v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08099
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#36890;&#29992;&#37492;&#21035;&#22120;&#65292;&#36890;&#36807;&#22312;NLP&#20219;&#21153;&#20013;&#20351;&#29992;&#21333;&#19968;&#30340;&#37492;&#21035;&#22120;&#65292;&#21487;&#23454;&#29616;&#27604;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#21462;&#24471;&#20102;&#22312;T0&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24314;&#27169;&#19968;&#30452;&#26159;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23637;&#31034;&#22312;&#22823;&#37327;NLP&#20219;&#21153;&#20013;&#65292;&#37492;&#21035;&#26041;&#27861;&#27604;&#29983;&#25104;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#25361;&#25112;&#36825;&#31181;&#24815;&#20363;&#12290;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#37492;&#21035;&#22120;&#26469;&#39044;&#27979;&#25991;&#26412;&#26679;&#26412;&#26159;&#21542;&#26469;&#33258;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#65292;&#31867;&#20284;&#20110;GAN&#12290;&#30001;&#20110;&#35768;&#22810;NLP&#20219;&#21153;&#21487;&#20197;&#34920;&#31034;&#20026;&#20174;&#20960;&#20010;&#36873;&#39033;&#20013;&#36873;&#25321;&#65292;&#22240;&#27492;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#37492;&#21035;&#22120;&#26469;&#39044;&#27979;&#36755;&#20837;&#21644;&#21738;&#20010;&#36873;&#39033;&#19982;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#30340;&#27010;&#29575;&#26368;&#22823;&#12290;&#36825;&#31181;&#31616;&#21333;&#30340;&#20844;&#24335;&#22312;T0&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#32467;&#26524;&#65292;&#20998;&#21035;&#22312;&#19981;&#21516;&#35268;&#27169;&#19978;&#27604;T0&#39640;16.0&#65285;&#65292;7.8&#65285;&#21644;11.5&#65285;&#12290;&#22312;&#24494;&#35843;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#20165;&#21344;&#20043;&#21069;&#26041;&#27861;&#30340;1/4&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38656;&#35201;&#26368;&#23567;&#30340;&#22122;&#22768;&#21644;&#26550;&#26500;&#24037;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative modeling has been the dominant approach for large-scale pretraining and zero-shot generalization. In this work, we challenge this convention by showing that discriminative approaches perform substantially better than generative ones on a large number of NLP tasks. Technically, we train a single discriminator to predict whether a text sample comes from the true data distribution, similar to GANs. Since many NLP tasks can be formulated as selecting from a few options, we use this discriminator to predict the concatenation of input and which option has the highest probability of coming from the true data distribution. This simple formulation achieves state-of-the-art zero-shot results on the T0 benchmark, outperforming T0 by 16.0\%, 7.8\%, and 11.5\% respectively on different scales. In the finetuning setting, our approach also achieves new state-of-the-art results on a wide range of NLP tasks, with only 1/4 parameters of previous methods. Meanwhile, our approach requires minim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#20197;&#20462;&#27491;&#39044;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;F1&#24230;&#37327;&#19978;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;2.35&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2210.07523</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#33258;&#36866;&#24212;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Self-Adaptive Named Entity Recognition by Retrieving Unstructured Knowledge. (arXiv:2210.07523v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#20197;&#20462;&#27491;&#39044;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;F1&#24230;&#37327;&#19978;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;2.35&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21487;&#20197;&#29992;&#20110;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#29305;&#23450;&#39046;&#22495;&#30340;&#23454;&#20307;&#65288;&#20363;&#22914;&#38899;&#20048;&#39046;&#22495;&#20013;&#30340;&#33402;&#26415;&#23478;&#65289;&#65292;&#20294;&#26159;&#21019;&#24314;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#32467;&#26500;&#21270;&#30693;&#35782;&#24211;&#26469;&#25191;&#34892;&#30446;&#26631;&#39046;&#22495;&#30340;&#20934;&#30830;NER&#26159;&#24456;&#26114;&#36149;&#30340;&#12290;&#20026;&#20102;&#26816;&#32034;&#26410;&#34987;&#24456;&#22909;&#22320;&#23398;&#20064;&#30340;&#23454;&#20307;&#30340;&#29992;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;NER&#26041;&#27861;&#65292;&#23427;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#12290;&#20026;&#20102;&#26816;&#32034;NER&#30340;&#26377;&#29992;&#30693;&#35782;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#65292;&#20351;&#29992;&#19981;&#30830;&#23450;&#30340;&#23454;&#20307;&#20316;&#20026;&#26597;&#35810;&#26469;&#26816;&#32034;&#38750;&#32467;&#26500;&#21270;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#39044;&#27979;&#36755;&#20837;&#20013;&#30340;&#23454;&#20307;&#65292;&#28982;&#21518;&#25214;&#21040;&#37027;&#20123;&#39044;&#27979;&#19981;&#33258;&#20449;&#30340;&#23454;&#20307;&#12290;&#28982;&#21518;&#65292;&#23427;&#20351;&#29992;&#36825;&#20123;&#19981;&#30830;&#23450;&#30340;&#23454;&#20307;&#20316;&#20026;&#26597;&#35810;&#26469;&#26816;&#32034;&#30693;&#35782;&#65292;&#24182;&#23558;&#26816;&#32034;&#21040;&#30340;&#25991;&#26412;&#36830;&#25509;&#21040;&#21407;&#22987;&#36755;&#20837;&#20197;&#20462;&#27491;&#39044;&#27979;&#12290;&#22312;CrossNER&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;F1&#24230;&#37327;&#19978;&#27604;&#24378;&#22522;&#32447;&#27169;&#22411;&#39640;2.35&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although named entity recognition (NER) helps us to extract domain-specific entities from text (e.g., artists in the music domain), it is costly to create a large amount of training data or a structured knowledge base to perform accurate NER in the target domain. Here, we propose self-adaptive NER, which retrieves external knowledge from unstructured text to learn the usages of entities that have not been learned well. To retrieve useful knowledge for NER, we design an effective two-stage model that retrieves unstructured knowledge using uncertain entities as queries. Our model predicts the entities in the input and then finds those of which the prediction is not confident. Then, it retrieves knowledge by using these uncertain entities as queries and concatenates the retrieved text to the original input to revise the prediction. Experiments on CrossNER datasets demonstrated that our model outperforms strong baselines by 2.35 points in F1 metric.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#26694;&#26550;&#65292;&#22522;&#20110;&#24120;&#35782;&#25512;&#29702;&#33021;&#22815;&#23454;&#29616;&#38544;&#24335;&#24847;&#22270;&#30340;&#39044;&#27979;&#21644;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;-shot&#25552;&#31034;&#20197;&#35302;&#21457;&#30456;&#24212;&#30340;&#20219;&#21153;&#23548;&#21521;&#26426;&#22120;&#20154;&#65292;&#25552;&#39640;&#20102;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.05901</link><description>&lt;p&gt;
&#22522;&#20110;&#24120;&#35782;&#25512;&#29702;&#30340;&#38544;&#24335;&#24847;&#22270;&#39044;&#27979;&#21644;&#24314;&#35758;&#30340;&#38646;-shot&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Prompting for Implicit Intent Prediction and Recommendation with Commonsense Reasoning. (arXiv:2210.05901v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#26694;&#26550;&#65292;&#22522;&#20110;&#24120;&#35782;&#25512;&#29702;&#33021;&#22815;&#23454;&#29616;&#38544;&#24335;&#24847;&#22270;&#30340;&#39044;&#27979;&#21644;&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;-shot&#25552;&#31034;&#20197;&#35302;&#21457;&#30456;&#24212;&#30340;&#20219;&#21153;&#23548;&#21521;&#26426;&#22120;&#20154;&#65292;&#25552;&#39640;&#20102;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#26234;&#33021;&#34394;&#25311;&#21161;&#25163;&#21482;&#33021;&#36890;&#36807;&#26126;&#30830;&#25552;&#21040;&#30340;&#20219;&#21153;&#25110;&#26381;&#21153;&#26469;&#25191;&#34892;&#30456;&#20851;&#39046;&#22495;&#25110;&#20219;&#21153;&#65292;&#38656;&#35201;&#36890;&#36807;&#38271;&#26102;&#38388;&#30340;&#20250;&#35805;&#21644;&#22810;&#20010;&#26126;&#30830;&#24847;&#22270;&#26469;&#25191;&#34892;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#21161;&#25163;&#33021;&#22815;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#25512;&#26029;&#20986;&#29992;&#25143;&#35805;&#35821;&#30340;&#65288;&#22810;&#20010;&#65289;&#38544;&#24335;&#24847;&#22270;&#65292;&#20174;&#32780;&#20943;&#23569;&#22797;&#26434;&#30340;&#20132;&#20114;&#24182;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#29992;&#25143;&#35805;&#35821;&#33258;&#21160;&#25512;&#26029;&#38544;&#24335;&#24847;&#22270;&#65292;&#28982;&#21518;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;-shot&#25552;&#31034;&#65292;&#20174;&#32780;&#35302;&#21457;&#36866;&#24403;&#30340;&#21333;&#20010;&#20219;&#21153;&#23548;&#21521;&#26426;&#22120;&#20154;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#38646;-shot&#26041;&#24335;&#19979;&#23454;&#29616;&#20102;&#38544;&#24335;&#24847;&#22270;&#21644;&#30456;&#20851;&#26426;&#22120;&#20154;&#30340;&#24314;&#35758;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent virtual assistants are currently designed to perform tasks or services explicitly mentioned by users, so multiple related domains or tasks need to be performed one by one through a long conversation with many explicit intents. Instead, human assistants are capable of reasoning (multiple) implicit intents based on user utterances via commonsense knowledge, reducing complex interactions and improving practicality. Therefore, this paper proposes a framework of multi-domain dialogue systems, which can automatically infer implicit intents based on user utterances and then perform zero-shot prompting using a large pre-trained language model to trigger suitable single task-oriented bots. The proposed framework is demonstrated effective to realize implicit intents and recommend associated bots in a zero-shot manner.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#20999;&#32447;&#26680; (NTK) &#22312;&#25551;&#36848;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;14&#20010;NLP&#20219;&#21153;&#20013;&#20351;&#29992;&#25513;&#30721;&#35789;&#39044;&#27979;&#38382;&#39064;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#21487;&#20197;&#21462;&#24471;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.05643</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#20989;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A Kernel-Based View of Language Model Fine-Tuning. (arXiv:2210.05643v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#20999;&#32447;&#26680; (NTK) &#22312;&#25551;&#36848;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;14&#20010;NLP&#20219;&#21153;&#20013;&#20351;&#29992;&#25513;&#30721;&#35789;&#39044;&#27979;&#38382;&#39064;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#21487;&#20197;&#21462;&#24471;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (LMs) &#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#35299;&#20915; NLP &#20219;&#21153;&#24050;&#32463;&#25104;&#20026;&#26631;&#20934;&#20570;&#27861;&#12290;&#20294;&#26159;&#65292;&#30446;&#21069;&#23545;&#20110;&#32463;&#39564;&#25104;&#21151;&#32972;&#21518;&#30340;&#29702;&#35770;&#26426;&#21046;&#20102;&#35299;&#24456;&#23569;&#65292;&#20363;&#22914;&#20026;&#20160;&#20040;&#22312;&#20960;&#21313;&#20010;&#35757;&#32451;&#28857;&#19978;&#24494;&#35843;&#19968;&#20010;&#26377; $10^8$ &#20010;&#25110;&#26356;&#22810;&#21442;&#25968;&#30340;&#27169;&#22411;&#19981;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#20999;&#32447;&#26680; (NTK) &#22312;&#25551;&#36848;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102; NTK &#24418;&#24335;&#21270;&#26041;&#27861;&#20197;&#24212;&#29992;&#20110; Adam&#65292;&#24182;&#20351;&#29992; Tensor Programs &#25551;&#36848;&#20102; NTK &#36866;&#29992;&#20110;&#25551;&#36848;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26356;&#26032;&#30340;&#26465;&#20214;&#12290;&#25105;&#20204;&#22312; 14 &#20010; NLP &#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#25552;&#31034;&#23558;&#19979;&#28216;&#20219;&#21153;&#34920;&#36848;&#20026;&#25513;&#30721;&#35789;&#39044;&#27979;&#38382;&#39064;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has become standard to solve NLP tasks by fine-tuning pre-trained language models (LMs), especially in low-data settings. There is minimal theoretical understanding of empirical success, e.g., why fine-tuning a model with $10^8$ or more parameters on a couple dozen training points does not result in overfitting. We investigate whether the Neural Tangent Kernel (NTK) - which originated as a model to study the gradient descent dynamics of infinitely wide networks with suitable random initialization - describes fine-tuning of pre-trained LMs. This study was inspired by the decent performance of NTK for computer vision tasks (Wei et al., 2022). We extend the NTK formalism to Adam and use Tensor Programs (Yang, 2020) to characterize conditions under which the NTK lens may describe fine-tuning updates to pre-trained language models. Extensive experiments on 14 NLP tasks validate our theory and show that formulating the downstream task as a masked word prediction problem through prompting 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#23545;&#35805;&#27169;&#25311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#24102;&#27880;&#37322;&#30340;&#31034;&#20363;&#33258;&#21160;&#21019;&#24314;&#22823;&#37327;&#23545;&#35805;&#25968;&#25454;&#65292;&#27604;&#20247;&#21253;&#26356;&#21152;&#25104;&#26412;&#25928;&#30410;&#21644;&#33410;&#30465;&#26102;&#38388;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#20351;&#29992;&#27169;&#25311;&#23545;&#35805;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.04185</link><description>&lt;p&gt;
&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21487;&#25511;&#23545;&#35805;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Controllable Dialogue Simulation with In-Context Learning. (arXiv:2210.04185v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#23545;&#35805;&#27169;&#25311;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#24102;&#27880;&#37322;&#30340;&#31034;&#20363;&#33258;&#21160;&#21019;&#24314;&#22823;&#37327;&#23545;&#35805;&#25968;&#25454;&#65292;&#27604;&#20247;&#21253;&#26356;&#21152;&#25104;&#26412;&#25928;&#30410;&#21644;&#33410;&#30465;&#26102;&#38388;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#20351;&#29992;&#27169;&#25311;&#23545;&#35805;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#23545;&#35805;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#30340;&#24102;&#27880;&#37322;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#38598;&#36890;&#24120;&#36890;&#36807;&#20247;&#21253;&#21019;&#24314;&#65292;&#36153;&#26102;&#36153;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Dialogic &#30340;&#26032;&#22411;&#23545;&#35805;&#27169;&#25311;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#22823;&#23610;&#24230;&#35821;&#35328;&#27169;&#22411;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20197;&#33258;&#21160;&#21270;&#30340;&#26041;&#24335;&#21019;&#24314;&#25968;&#25454;&#38598;&#12290;&#22312;&#23569;&#37327;&#24102;&#27880;&#37322;&#30340;&#23545;&#35805;&#31034;&#20363;&#30340;&#21551;&#21457;&#19979;&#65292;Dialogic &#33258;&#21160;&#36873;&#25321;&#19978;&#19979;&#25991;&#20013;&#30340;&#31034;&#20363;&#65292;&#20419;&#20351; GPT-3 &#25511;&#21046;&#29983;&#25104;&#26032;&#30340;&#23545;&#35805;&#21644;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24555;&#36895;&#25193;&#23637;&#23569;&#37327;&#30340;&#23545;&#35805;&#25968;&#25454;&#65292;&#20960;&#20046;&#27809;&#26377;&#20154;&#31867;&#20171;&#20837;&#21644;&#21442;&#25968;&#26356;&#26032;&#65292;&#22240;&#27492;&#27604;&#20247;&#21253;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#21644;&#33410;&#30465;&#26102;&#38388;&#12290;&#22522;&#20110; MultiWOZ &#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#65292;&#20351;&#29992;&#27169;&#25311;&#23545;&#35805;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#29978;&#33267;&#27604;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#20154;&#24037;&#29983;&#25104;&#30340;&#23545;&#35805;&#26356;&#22909;&#65292;&#20165;&#20351;&#29992; 85 &#26465;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building dialogue systems requires a large corpus of annotated dialogues. Such datasets are usually created via crowdsourcing, which is expensive and time-consuming. In this paper, we propose \textsc{Dialogic}, a novel dialogue simulation method based on large language model in-context learning to automate dataset creation. Seeded with a few annotated dialogues, \textsc{Dialogic} automatically selects in-context examples for demonstration and prompts GPT-3 to generate new dialogues and annotations in a controllable way. Our method can rapidly expand a small set of dialogue data with minimum or zero \textit{human involvement} and \textit{parameter update} and is thus much more cost-efficient and time-saving than crowdsourcing. Experimental results on the MultiWOZ dataset demonstrate that training a model on the simulated dialogues leads to even better performance than using the same amount of human-generated dialogues under the challenging low-resource settings, with as few as 85 dialog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#32763;&#36716;&#23398;&#20064;&#8221;&#30340;&#20803;&#35757;&#32451;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20219;&#21153;&#25351;&#20196;&#65292;&#21487;&#20197;&#22312;&#38646;&#26679;&#26412;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#21253;&#21547;&#26410;&#35265;&#26631;&#31614;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#12290;Flipped&#22312;14&#20010;BIG-bench&#22522;&#20934;&#27979;&#35797;&#20219;&#21153;&#20013;&#24179;&#22343;&#27604;3-shot GPT-3&#39640;&#20986;8.4%&#21644;9.7%&#30340;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2210.02969</link><description>&lt;p&gt;
&#29468;&#27979;&#25351;&#20196;&#65281;&#32763;&#36716;&#23398;&#20064;&#20351;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#24378;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners. (arXiv:2210.02969v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#32763;&#36716;&#23398;&#20064;&#8221;&#30340;&#20803;&#35757;&#32451;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20219;&#21153;&#25351;&#20196;&#65292;&#21487;&#20197;&#22312;&#38646;&#26679;&#26412;&#20219;&#21153;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#21253;&#21547;&#26410;&#35265;&#26631;&#31614;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#20013;&#12290;Flipped&#22312;14&#20010;BIG-bench&#22522;&#20934;&#27979;&#35797;&#20219;&#21153;&#20013;&#24179;&#22343;&#27604;3-shot GPT-3&#39640;&#20986;8.4%&#21644;9.7%&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#35757;&#32451;&#36890;&#36807;&#26368;&#22823;&#21270;&#32473;&#23450;&#20219;&#21153;&#25351;&#20196;&#21644;&#36755;&#20837;&#23454;&#20363;&#30340;&#30446;&#26631;&#26631;&#31614;&#20284;&#28982;&#26469;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;(LM)&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#20219;&#21153;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20803;&#35757;&#32451;&#30340;LM&#20173;&#28982;&#38590;&#20197;&#25512;&#24191;&#21040;&#21253;&#21547;&#22312;&#20803;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#26631;&#31614;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#32763;&#36716;&#23398;&#20064;&#8221;&#30340;&#20803;&#35757;&#32451;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#35757;&#32451;LM&#22312;&#32473;&#23450;&#36755;&#20837;&#23454;&#20363;&#21644;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20219;&#21153;&#25351;&#20196;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#20351;&#29992;&#32763;&#36716;&#23398;&#20064;&#35757;&#32451;&#30340;LM(&#31216;&#20026;&#8220;Flipped&#8221;)&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#29983;&#25104;&#20219;&#21153;&#25351;&#20196;&#30340;&#26631;&#31614;&#36873;&#39033;&#12290;&#22312;BIG-bench&#22522;&#20934;&#27979;&#35797;&#30340;14&#20010;&#20219;&#21153;&#20013;&#65292;&#22823;&#23567;&#20026;11B&#30340;Flipped&#22312;&#24179;&#22343;&#20540;&#26041;&#38754;&#20248;&#20110;&#38646;&#26679;&#26412;T0-11B&#29978;&#33267;&#27604;16&#20493;&#22823;&#30340;3-shot GPT-3(175B)&#39640;&#20986;8.4%&#21644;9.7%&#30340;&#20998;&#25968;&#12290;Flipped&#22312;&#20855;&#26377;&#26410;&#30693;&#26631;&#31614;&#30340;&#20219;&#21153;&#19978;&#23588;&#20854;&#34920;&#29616;&#31361;&#20986;&#65292;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#27604;T0-11B&#39640;&#20986;20%&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-training, which fine-tunes the language model (LM) on various downstream tasks by maximizing the likelihood of the target label given the task instruction and input instance, has improved the zero-shot task generalization performance. However, meta-trained LMs still struggle to generalize to challenging tasks containing novel labels unseen during meta-training. In this paper, we propose Flipped Learning, an alternative method of meta-training which trains the LM to generate the task instruction given the input instance and label. During inference, the LM trained with Flipped Learning, referred to as Flipped, selects the label option that is most likely to generate the task instruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized Flipped outperforms zero-shot T0-11B and even a 16 times larger 3-shot GPT-3 (175B) on average by 8.4% and 9.7% points, respectively. Flipped gives particularly large improvements on tasks with unseen labels, outperforming T0-11B by up to +20% av
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#20998;&#23618;&#33976;&#39311;&#26041;&#27861;&#65288;TED&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#20219;&#21153;&#24863;&#30693;&#30340;&#28388;&#27874;&#22120;&#26469;&#23545;&#40784;&#23398;&#29983;&#21644;&#25945;&#24072;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#36873;&#25321;&#26377;&#29992;&#30340;&#30693;&#35782;&#65292;&#20943;&#23569;&#30693;&#35782;&#24046;&#36317;&#65292;&#20351;&#23398;&#29983;&#27169;&#22411;&#26356;&#22909;&#22320;&#36866;&#24212;&#30446;&#26631;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#23569;&#30340;&#21442;&#25968;&#19979;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.01351</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810;&#65306;&#38754;&#21521;&#20219;&#21153;&#30340;&#20998;&#23618;&#33976;&#39311;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Less is More: Task-aware Layer-wise Distillation for Language Model Compression. (arXiv:2210.01351v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#20998;&#23618;&#33976;&#39311;&#26041;&#27861;&#65288;TED&#65289;&#65292;&#36890;&#36807;&#35774;&#35745;&#20219;&#21153;&#24863;&#30693;&#30340;&#28388;&#27874;&#22120;&#26469;&#23545;&#40784;&#23398;&#29983;&#21644;&#25945;&#24072;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#36873;&#25321;&#26377;&#29992;&#30340;&#30693;&#35782;&#65292;&#20943;&#23569;&#30693;&#35782;&#24046;&#36317;&#65292;&#20351;&#23398;&#29983;&#27169;&#22411;&#26356;&#22909;&#22320;&#36866;&#24212;&#30446;&#26631;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#23569;&#30340;&#21442;&#25968;&#19979;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#33976;&#39311;&#26159;&#23558;&#22823;&#27169;&#22411;&#65288;&#21363;&#25945;&#24072;&#27169;&#22411;&#65289;&#21387;&#32553;&#20026;&#23567;&#27169;&#22411;&#65288;&#21363;&#23398;&#29983;&#27169;&#22411;&#65289;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23398;&#29983;&#36890;&#36807;&#22312;&#27599;&#20010;&#20013;&#38388;&#23618;&#27169;&#20223;&#25945;&#24072;&#30340;&#38544;&#34255;&#34920;&#31034;&#26469;&#20174;&#25945;&#24072;&#20013;&#33976;&#39311;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20998;&#23618;&#33976;&#39311;&#20063;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;&#30001;&#20110;&#23398;&#29983;&#30340;&#27169;&#22411;&#23481;&#37327;&#27604;&#25945;&#24072;&#23567;&#65292;&#23427;&#36890;&#24120;&#20250;&#20986;&#29616;&#27424;&#25311;&#21512;;&#27492;&#22806;&#65292;&#25945;&#24072;&#30340;&#38544;&#34255;&#34920;&#31034;&#21253;&#21547;&#20102;&#23398;&#29983;&#26410;&#24517;&#38656;&#35201;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20219;&#21153;&#30340;&#20998;&#23618;&#33976;&#39311;&#65288;TED&#65289;&#26041;&#27861;&#12290;TED&#35774;&#35745;&#20219;&#21153;&#24863;&#30693;&#28388;&#27874;&#22120;&#26469;&#23545;&#40784;&#27599;&#19968;&#23618;&#30340;&#23398;&#29983;&#21644;&#25945;&#24072;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#36825;&#20123;&#28388;&#27874;&#22120;&#20174;&#38544;&#34255;&#34920;&#31034;&#20013;&#36873;&#25321;&#23545;&#30446;&#26631;&#20219;&#21153;&#26377;&#29992;&#30340;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;TED&#20943;&#23569;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#24182;&#24110;&#21161;&#23398;&#29983;&#26356;&#22909;&#22320;&#36866;&#24212;&#30446;&#26631;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;TED&#65292;&#24182;&#34920;&#26126;&#23427;&#21487;&#20197;&#22312;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#23569;&#24471;&#22810;&#30340;&#21442;&#25968;&#24773;&#20917;&#19979;&#23454;&#29616;&#21487;&#27604;&#25110;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Layer-wise distillation is a powerful tool to compress large models (i.e. teacher models) into small ones (i.e., student models). The student distills knowledge from the teacher by mimicking the hidden representations of the teacher at every intermediate layer. However, layer-wise distillation is difficult. Since the student has a smaller model capacity than the teacher, it is often under-fitted. Furthermore, the hidden representations of the teacher contain redundant information that the student does not necessarily need for the target task's learning. To address these challenges, we propose a novel Task-aware layEr-wise Distillation (TED). TED designs task-aware filters to align the hidden representations of the student and the teacher at each layer. The filters select the knowledge that is useful for the target task from the hidden representations. As such, TED reduces the knowledge gap between the two models and helps the student to fit better on the target task. We evaluate TED in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22522;&#30784;&#38382;&#31572;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36339;&#36291;&#26816;&#32034;&#30456;&#20851;&#19978;&#19979;&#25991;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#34701;&#21512;&#26426;&#21046;&#65292;&#23454;&#29616;&#20855;&#26377;&#28789;&#27963;&#24615;&#12289;&#35206;&#30422;&#38754;&#21644;&#32467;&#26500;&#25512;&#29702;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#30693;&#35782;&#22522;&#30784;&#26500;&#24314;&#26041;&#27861;&#65292;&#24182;&#36798;&#21040;&#19982;&#20381;&#36182;&#20110;&#22806;&#37096;&#20449;&#24687;&#36164;&#28304;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.08284</link><description>&lt;p&gt;
&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#22522;&#30784;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Structured Knowledge Grounding for Question Answering. (arXiv:2209.08284v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22522;&#30784;&#38382;&#31572;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36339;&#36291;&#26816;&#32034;&#30456;&#20851;&#19978;&#19979;&#25991;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#34701;&#21512;&#26426;&#21046;&#65292;&#23454;&#29616;&#20855;&#26377;&#28789;&#27963;&#24615;&#12289;&#35206;&#30422;&#38754;&#21644;&#32467;&#26500;&#25512;&#29702;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#30693;&#35782;&#22522;&#30784;&#26500;&#24314;&#26041;&#27861;&#65292;&#24182;&#36798;&#21040;&#19982;&#20381;&#36182;&#20110;&#22806;&#37096;&#20449;&#24687;&#36164;&#28304;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#33021;&#21542;&#36890;&#36807;&#22266;&#26377;&#30340;&#20851;&#31995;&#25512;&#29702;&#33021;&#21147;&#22312;&#30693;&#35782;&#24211;&#20013;&#23545;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#30693;&#35782;&#22522;&#30784;&#26500;&#24314;&#65311;&#34429;&#28982;&#20043;&#21069;&#21482;&#20351;&#29992;LM&#30340;&#27169;&#22411;&#22312;&#35768;&#22810;&#38382;&#31572;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#19968;&#23450;&#25104;&#21151;&#65292;&#20294;&#26368;&#36817;&#30340;&#19968;&#20123;&#26041;&#27861;&#21253;&#25324;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#65292;&#36890;&#36807;&#20854;&#26356;&#20855;&#36923;&#36753;&#39537;&#21160;&#30340;&#38544;&#21547;&#30693;&#35782;&#26469;&#34917;&#20805;LM&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;KG&#65289;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#20351;&#24471;LM&#20445;&#25345;&#38754;&#21521;&#26410;&#30693;&#38382;&#39064;&#30340;&#28789;&#27963;&#24615;&#21644;&#24191;&#24230;&#65292;&#30446;&#21069;&#20173;&#26159;&#20010;&#26410;&#35299;&#20043;&#35868;&#65292;&#24182;&#19988;&#24403;&#21069;&#30340;&#27169;&#22411;&#20381;&#36182;&#20110;&#22270;&#25216;&#26415;&#26469;&#25552;&#21462;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20165;&#21033;&#29992;LM&#26469;&#32467;&#21512;&#35821;&#35328;&#21644;&#30693;&#35782;&#65292;&#20197;&#23454;&#29616;&#20855;&#26377;&#28789;&#27963;&#24615;&#12289;&#35206;&#30422;&#38754;&#21644;&#32467;&#26500;&#25512;&#29702;&#30340;&#30693;&#35782;&#22522;&#30784;&#38382;&#31572;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#30693;&#35782;&#26500;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#36339;&#36291;&#26469;&#26816;&#32034;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65292;&#36825;&#31181;&#26041;&#27861;&#34920;&#29616;&#20986;&#20102;&#27604;&#20256;&#32479;&#22522;&#20110;GNN&#25216;&#26415;&#26356;&#20840;&#38754;&#30340;&#34920;&#29616;&#21147;&#12290;&#24182;&#19988;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#28145;&#24230;&#34701;&#21512;&#26426;&#21046;&#65292;&#36827;&#19968;&#27493;&#24357;&#21512;&#20102;&#35821;&#35328;&#21644;&#30693;&#35782;&#27169;&#24577;&#20043;&#38388;&#20449;&#24687;&#20132;&#25442;&#30340;&#29942;&#39048;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#30693;&#35782;&#22522;&#30784;&#26500;&#24314;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20986;&#19982;&#20381;&#36182;&#20110;&#22806;&#37096;&#20449;&#24687;&#36164;&#28304;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#30456;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can language models (LM) ground question-answering (QA) tasks in the knowledge base via inherent relational reasoning ability? While previous models that use only LMs have seen some success on many QA tasks, more recent methods include knowledge graphs (KG) to complement LMs with their more logic-driven implicit knowledge. However, effectively extracting information from structured data, like KGs, empowers LMs to remain an open question, and current models rely on graph techniques to extract knowledge. In this paper, we propose to solely leverage the LMs to combine the language and knowledge for knowledge based question-answering with flexibility, breadth of coverage and structured reasoning. Specifically, we devise a knowledge construction method that retrieves the relevant context with a dynamic hop, which expresses more comprehensivenes than traditional GNN-based techniques. And we devise a deep fusion mechanism to further bridge the information exchanging bottleneck between the lan
&lt;/p&gt;</description></item><item><title>GigaST&#26159;&#19968;&#20221;&#22823;&#35268;&#27169;&#30340;&#20266;&#35821;&#38899;&#32763;&#35793;(ST)&#35821;&#26009;&#24211;&#65292;&#20351;&#29992;&#35813;&#35821;&#26009;&#24211;&#25152;&#35757;&#32451;&#20986;&#30340;ST&#27169;&#22411;&#22312;MuST-C&#33521;&#35821;-&#24503;&#35821;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2204.03939</link><description>&lt;p&gt;
GigaST: &#19968;&#20221;10,000&#23567;&#26102;&#30340;&#20266;&#35821;&#38899;&#32763;&#35793;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
GigaST: A 10,000-hour Pseudo Speech Translation Corpus. (arXiv:2204.03939v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.03939
&lt;/p&gt;
&lt;p&gt;
GigaST&#26159;&#19968;&#20221;&#22823;&#35268;&#27169;&#30340;&#20266;&#35821;&#38899;&#32763;&#35793;(ST)&#35821;&#26009;&#24211;&#65292;&#20351;&#29992;&#35813;&#35821;&#26009;&#24211;&#25152;&#35757;&#32451;&#20986;&#30340;ST&#27169;&#22411;&#22312;MuST-C&#33521;&#35821;-&#24503;&#35821;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GigaST&#65292;&#19968;&#20221;&#22823;&#35268;&#27169;&#30340;&#20266;&#35821;&#38899;&#32763;&#35793;(ST)&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#33521;&#35821;ASR&#35821;&#26009;&#24211;GigaSpeech&#20013;&#30340;&#25991;&#26412;&#32763;&#35793;&#25104;&#24503;&#35821;&#21644;&#20013;&#25991;&#26469;&#21019;&#24314;&#35813;&#35821;&#26009;&#24211;&#12290;&#35757;&#32451;&#38598;&#30001;&#24378;&#22823;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#32763;&#35793;&#65292;&#27979;&#35797;&#38598;&#30001;&#20154;&#31867;&#36827;&#34892;&#32763;&#35793;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;ST&#27169;&#22411;&#22312;MuST-C&#33521;&#35821;-&#24503;&#35821;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#33719;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#32763;&#35793;&#36807;&#31243;&#25551;&#36848;&#24182;&#39564;&#35777;&#20102;&#20854;&#36136;&#37327;&#12290;&#25105;&#20204;&#20844;&#24320;&#20102;&#32763;&#35793;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#24182;&#24076;&#26395;&#20419;&#36827;&#35821;&#38899;&#32763;&#35793;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;NeurST&#19978;&#21457;&#24067;&#20102;&#35757;&#32451;&#33050;&#26412;&#65292;&#20197;&#20415;&#36731;&#26494;&#22797;&#21046;&#25105;&#20204;&#30340;&#31995;&#32479;&#12290; GigaST&#25968;&#25454;&#38598;&#21487;&#22312;https://st-benchmark.github.io/resources/GigaST&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces GigaST, a large-scale pseudo speech translation (ST) corpus. We create the corpus by translating the text in GigaSpeech, an English ASR corpus, into German and Chinese. The training set is translated by a strong machine translation system and the test set is translated by human. ST models trained with an addition of our corpus obtain new state-of-the-art results on the MuST-C English-German benchmark test set. We provide a detailed description of the translation process and verify its quality. We make the translated text data public and hope to facilitate research in speech translation. Additionally, we also release the training scripts on NeurST to make it easy to replicate our systems. GigaST dataset is available at https://st-benchmark.github.io/resources/GigaST.
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#24179;&#21488;&#19978;&#30340;&#26377;&#23475;&#20869;&#23481;&#31181;&#31867;&#21644;&#24179;&#21488;&#38656;&#27714;&#19982;&#33258;&#21160;&#26816;&#27979;&#26377;&#23475;&#20869;&#23481;&#30340;&#30740;&#31350;&#26041;&#21521;&#23384;&#22312;&#24046;&#24322;&#65292;&#38656;&#35201;&#26356;&#28145;&#20837;&#30340;&#30740;&#31350;&#21644;&#26356;&#22909;&#30340;&#24179;&#21488;&#31649;&#29702;&#65292;&#20197;&#20943;&#23569;&#31038;&#20250;&#21361;&#23475;&#21644;&#21019;&#24314;&#26356;&#23436;&#25972;&#30340;&#29992;&#25143;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2103.00153</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#24179;&#21488;&#19978;&#26816;&#27979;&#26377;&#23475;&#20869;&#23481;&#65306;&#24179;&#21488;&#38656;&#27714;&#19982;&#30740;&#31350;&#26041;&#21521;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Detecting Harmful Content On Online Platforms: What Platforms Need Vs. Where Research Efforts Go. (arXiv:2103.00153v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.00153
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24179;&#21488;&#19978;&#30340;&#26377;&#23475;&#20869;&#23481;&#31181;&#31867;&#21644;&#24179;&#21488;&#38656;&#27714;&#19982;&#33258;&#21160;&#26816;&#27979;&#26377;&#23475;&#20869;&#23481;&#30340;&#30740;&#31350;&#26041;&#21521;&#23384;&#22312;&#24046;&#24322;&#65292;&#38656;&#35201;&#26356;&#28145;&#20837;&#30340;&#30740;&#31350;&#21644;&#26356;&#22909;&#30340;&#24179;&#21488;&#31649;&#29702;&#65292;&#20197;&#20943;&#23569;&#31038;&#20250;&#21361;&#23475;&#21644;&#21019;&#24314;&#26356;&#23436;&#25972;&#30340;&#29992;&#25143;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#24179;&#21488;&#19978;&#26377;&#23475;&#20869;&#23481;&#30340;&#27867;&#28389;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#21253;&#25324;&#20167;&#24680;&#35328;&#35770;&#12289;&#25915;&#20987;&#24615;&#35821;&#35328;&#12289;&#27450;&#20940;&#21644;&#39578;&#25200;&#12289;&#38169;&#35823;&#20449;&#24687;&#12289;&#22403;&#22334;&#37038;&#20214;&#12289;&#26292;&#21147;&#12289;&#38706;&#39592;&#20869;&#23481;&#12289;&#24615;&#34384;&#24453;&#12289;&#33258;&#27531;&#31561;&#22810;&#31181;&#24418;&#24335;&#12290;&#22312;&#32447;&#24179;&#21488;&#23547;&#27714;&#38480;&#21046;&#36825;&#20123;&#20869;&#23481;&#20197;&#20943;&#23569;&#31038;&#20250;&#21361;&#23475;&#65292;&#36981;&#23432;&#27861;&#24459;&#27861;&#35268;&#65292;&#24182;&#20026;&#29992;&#25143;&#21019;&#24314;&#26356;&#20855;&#21253;&#23481;&#24615;&#30340;&#29615;&#22659;&#12290;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#26816;&#27979;&#26377;&#23475;&#20869;&#23481;&#65292;&#36890;&#24120;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#23376;&#38382;&#39064;&#25110;&#29421;&#31364;&#30340;&#31038;&#21306;&#19978;&#65292;&#22240;&#20026;&#26377;&#23475;&#20869;&#23481;&#30340;&#23450;&#20041;&#36890;&#24120;&#21462;&#20915;&#20110;&#24179;&#21488;&#21644;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24403;&#21069;&#23384;&#22312;&#22312;&#32447;&#24179;&#21488;&#23547;&#27714;&#36943;&#21046;&#30340;&#26377;&#23475;&#20869;&#23481;&#31867;&#22411;&#65292;&#19982;&#33258;&#21160;&#26816;&#27979;&#27492;&#31867;&#20869;&#23481;&#30340;&#30740;&#31350;&#26041;&#21521;&#20043;&#38388;&#23384;&#22312;&#20108;&#20803;&#23545;&#31435;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#27492;&#35843;&#26597;&#29616;&#26377;&#26041;&#27861;&#20197;&#21450;&#22312;&#32447;&#24179;&#21488;&#30340;&#20869;&#23481;&#31649;&#29702;&#25919;&#31574;&#65292;&#24182;&#38024;&#23545;&#24615;&#25552;&#20986;&#30740;&#31350;&#26041;&#21521;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of harmful content on online platforms is a major societal problem, which comes in many different forms including hate speech, offensive language, bullying and harassment, misinformation, spam, violence, graphic content, sexual abuse, self harm, and many other. Online platforms seek to moderate such content to limit societal harm, to comply with legislation, and to create a more inclusive environment for their users. Researchers have developed different methods for automatically detecting harmful content, often focusing on specific sub-problems or on narrow communities, as what is considered harmful often depends on the platform and on the context. We argue that there is currently a dichotomy between what types of harmful content online platforms seek to curb, and what research efforts there are to automatically detect such content. We thus survey existing methods as well as content moderation policies by online platforms in this light and we suggest directions for fu
&lt;/p&gt;</description></item></channel></rss>