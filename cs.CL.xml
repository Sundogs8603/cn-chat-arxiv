<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25991;&#26723;&#32534;&#30721;&#22120;&#65292;&#20351;&#29992;&#32500;&#22522;&#30334;&#31185;&#20316;&#20026;&#25968;&#25454;&#26469;&#28304;&#65292;&#24182;&#37319;&#29992;&#36328;&#35821;&#35328;&#23545;&#27604;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#29992;&#20110;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#25991;&#26723;&#32423;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.07016</link><description>&lt;p&gt;
&#36890;&#29992;&#22810;&#35821;&#35328;&#25991;&#26723;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
A General-Purpose Multilingual Document Encoder. (arXiv:2305.07016v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25991;&#26723;&#32534;&#30721;&#22120;&#65292;&#20351;&#29992;&#32500;&#22522;&#30334;&#31185;&#20316;&#20026;&#25968;&#25454;&#26469;&#28304;&#65292;&#24182;&#37319;&#29992;&#36328;&#35821;&#35328;&#23545;&#27604;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#29992;&#20110;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#25991;&#26723;&#32423;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;MMTs&#65289;&#24050;&#32463;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#22810;&#35821;&#35328;NLP&#21644;&#29305;&#21035;&#26159;NLP&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#23613;&#31649;&#35768;&#22810;&#24037;&#20316;&#21033;&#29992;MMTs&#26469;&#25366;&#25496;&#24179;&#34892;&#25968;&#25454;&#21644;&#35825;&#23548;&#21452;&#35821;&#25991;&#26723;&#23884;&#20837;&#65292;&#20294;&#24456;&#23569;&#26377;&#24037;&#20316;&#19987;&#38376;&#29992;&#20110;&#35757;&#32451;&#36890;&#29992;&#65288;&#22823;&#35268;&#27169;&#65289;&#22810;&#35821;&#35328;&#25991;&#26723;&#32534;&#30721;&#22120;&#65292;&#21487;&#29992;&#20110;&#30417;&#30563;&#21644;&#38750;&#30417;&#30563;&#25991;&#26723;&#32423;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39044;&#20808;&#35757;&#32451;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25991;&#26723;&#32534;&#30721;&#22120;&#65292;&#20316;&#20026;&#19968;&#20010;&#20998;&#23618;&#36716;&#25442;&#22120;&#27169;&#22411;&#65288;HMDE&#65289;&#65292;&#20854;&#20013;&#19968;&#20010;&#27973;&#23618;&#25991;&#26723;&#36716;&#25442;&#22120;&#23545;&#26368;&#20808;&#36827;&#30340;&#39044;&#20808;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#21477;&#23376;&#32534;&#30721;&#22120;&#20135;&#29983;&#30340;&#21477;&#23376;&#34920;&#31034;&#36827;&#34892;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;&#25105;&#20204;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#20316;&#20026;&#21487;&#29992;&#30340;&#21487;&#27604;&#36739;&#25991;&#20214;&#28304;&#26469;&#21019;&#24314;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#36328;&#35821;&#35328;&#23545;&#27604;&#30446;&#26631;&#26469;&#35757;&#32451;HMDE&#65292;&#36827;&#19968;&#27493;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#30340;&#31867;&#21035;&#23618;&#27425;&#32467;&#26500;&#26469;&#21019;&#24314;&#38590;&#20197;&#21306;&#20998;&#30340;&#36127;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massively multilingual pretrained transformers (MMTs) have tremendously pushed the state of the art on multilingual NLP and cross-lingual transfer of NLP models in particular. While a large body of work leveraged MMTs to mine parallel data and induce bilingual document embeddings, much less effort has been devoted to training general-purpose (massively) multilingual document encoder that can be used for both supervised and unsupervised document-level tasks. In this work, we pretrain a massively multilingual document encoder as a hierarchical transformer model (HMDE) in which a shallow document transformer contextualizes sentence representations produced by a state-of-the-art pretrained multilingual sentence encoder. We leverage Wikipedia as a readily available source of comparable documents for creating training data, and train HMDE by means of a cross-lingual contrastive objective, further exploiting the category hierarchy of Wikipedia for creation of difficult negatives. We evaluate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;&#30340;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#65292;&#37319;&#29992;&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#12289;&#32858;&#28966;&#25439;&#22833;&#21644;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#31561;&#25216;&#26415;&#65292;&#22312;LVIS&#19978;&#21462;&#24471;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.07011</link><description>&lt;p&gt;
&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#65306;&#35270;&#35273;&#21464;&#21387;&#22120;&#19979;&#30340;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers. (arXiv:2305.07011v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;&#30340;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#65292;&#37319;&#29992;&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#12289;&#32858;&#28966;&#25439;&#22833;&#21644;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#31561;&#25216;&#26415;&#65292;&#22312;LVIS&#19978;&#21462;&#24471;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21306;&#22495;&#24863;&#30693;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;RO-ViT&#65289;&#65292;&#19968;&#31181;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#22635;&#34917;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#24314;&#35758;&#38543;&#26426;&#35009;&#21098;&#24182;&#35843;&#25972;&#20301;&#32622;&#23884;&#20837;&#30340;&#21306;&#22495;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#25972;&#20010;&#22270;&#20687;&#20301;&#32622;&#23884;&#20837;&#12290;&#36825;&#26356;&#22909;&#22320;&#21305;&#37197;&#20102;&#26816;&#27979;&#24494;&#35843;&#38454;&#27573;&#20013;&#21306;&#22495;&#32423;&#21035;&#19978;&#20351;&#29992;&#20301;&#32622;&#23884;&#20837;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29992;&#32858;&#28966;&#25439;&#22833;&#26367;&#25442;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;softmax&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#37027;&#20123;&#26377;&#20449;&#24687;&#37327;&#20294;&#38590;&#20197;&#25429;&#25417;&#30340;&#20363;&#23376;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20197;&#25913;&#36827;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;LVIS&#21644;COCO&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#23436;&#25972;&#27169;&#22411;&#21644;&#38646;-shot&#36716;&#31227;&#24615;&#33021;&#12290;RO-ViT&#22312;LVIS&#19978;&#23454;&#29616;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#65292;&#36229;&#36807;&#29616;&#26377;&#26368;&#20339;&#26041;&#27861;5.8&#20010;&#30334;&#20998;&#28857;&#65292;&#21516;&#26102;&#36824;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#38646;-shot&#36716;&#31227;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - a contrastive image-text pretraining recipe to bridge the gap between image-level pretraining and open-vocabulary object detection. At the pretraining phase, we propose to randomly crop and resize regions of positional embeddings instead of using the whole image positional embeddings. This better matches the use of positional embeddings at region-level in the detection finetuning phase. In addition, we replace the common softmax cross entropy loss in contrastive learning with focal loss to better learn the informative yet difficult examples. Finally, we leverage recent advances in novel object proposals to improve open-vocabulary detection finetuning. We evaluate our full model on the LVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer. RO-ViT achieves a state-of-the-art 32.1 $AP_r$ on LVIS, surpassing the best existing approach by +5.8 points in addition to competitive zero-shot transfer detec
&lt;/p&gt;</description></item><item><title>&#23376;&#35789;&#32423;&#20998;&#27573;&#26426;&#22120;&#32763;&#35793;&#65288;SSMT&#65289;&#26159;&#19968;&#31181;&#23558;&#23376;&#35789;&#20998;&#21106;&#21644;MT&#22312;&#19968;&#20010;&#21487;&#35757;&#32451;&#27169;&#22411;&#20013;&#32479;&#19968;&#36215;&#26469;&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#20998;&#21106;&#30446;&#26631;&#21477;&#23376;&#35789;&#65292;&#21516;&#26102;&#32852;&#21512;&#23398;&#20064;&#29983;&#25104;&#30446;&#26631;&#21477;&#23376;&#12290;&#22312;6&#20010;&#32763;&#35793;&#26041;&#21521;&#30340;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;SSMT&#25552;&#39640;&#20102;&#35789;&#32032;&#20016;&#23500;&#30340;&#32858;&#38598;&#24615;&#35821;&#35328;&#30340;chrF&#20998;&#25968;&#65292;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07005</link><description>&lt;p&gt;
&#23376;&#35789;&#32423;&#20998;&#27573;&#26426;&#22120;&#32763;&#35793;&#65306;&#32479;&#19968;&#20998;&#35789;&#21644;&#30446;&#26631;&#21477;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Subword Segmental Machine Translation: Unifying Segmentation and Target Sentence Generation. (arXiv:2305.07005v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07005
&lt;/p&gt;
&lt;p&gt;
&#23376;&#35789;&#32423;&#20998;&#27573;&#26426;&#22120;&#32763;&#35793;&#65288;SSMT&#65289;&#26159;&#19968;&#31181;&#23558;&#23376;&#35789;&#20998;&#21106;&#21644;MT&#22312;&#19968;&#20010;&#21487;&#35757;&#32451;&#27169;&#22411;&#20013;&#32479;&#19968;&#36215;&#26469;&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#20998;&#21106;&#30446;&#26631;&#21477;&#23376;&#35789;&#65292;&#21516;&#26102;&#32852;&#21512;&#23398;&#20064;&#29983;&#25104;&#30446;&#26631;&#21477;&#23376;&#12290;&#22312;6&#20010;&#32763;&#35793;&#26041;&#21521;&#30340;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;SSMT&#25552;&#39640;&#20102;&#35789;&#32032;&#20016;&#23500;&#30340;&#32858;&#38598;&#24615;&#35821;&#35328;&#30340;chrF&#20998;&#25968;&#65292;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#35789;&#20998;&#35789;&#22120;&#65288;&#22914;BPE&#65289;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#21644;&#20854;&#20182;&#65288;&#26465;&#20214;&#65289;&#35821;&#35328;&#27169;&#22411;&#20013;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;&#22312;&#35757;&#32451;&#20043;&#21069;&#24212;&#29992;&#23427;&#20204;&#20110;&#25968;&#25454;&#38598;&#19978;&#65292;&#22240;&#27492;&#32763;&#35793;&#25110;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#21462;&#20915;&#20110;&#20998;&#35789;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#23376;&#35789;&#32423;&#20998;&#27573;&#26426;&#22120;&#32763;&#35793;&#65288;SSMT&#65289;&#30340;&#20559;&#31163;&#20110;&#27492;&#33539;&#20363;&#30340;&#26041;&#27861;&#12290;SSMT&#23558;&#23376;&#35789;&#20998;&#21106;&#21644;MT&#22312;&#19968;&#20010;&#21487;&#35757;&#32451;&#27169;&#22411;&#20013;&#32479;&#19968;&#36215;&#26469;&#12290;&#23427;&#23398;&#20064;&#20998;&#21106;&#30446;&#26631;&#21477;&#23376;&#35789;&#65292;&#21516;&#26102;&#32852;&#21512;&#23398;&#20064;&#29983;&#25104;&#30446;&#26631;&#21477;&#23376;&#12290;&#20026;&#20102;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20351;&#29992;SSMT&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#35299;&#30721;&#65292;&#36825;&#26159;&#19968;&#31181;&#38543;&#30528;&#29983;&#25104;&#32763;&#35793;&#36866;&#24212;&#20998;&#21106;&#30340;&#25991;&#26412;&#29983;&#25104;&#31639;&#27861;&#12290;&#22312;6&#20010;&#32763;&#35793;&#26041;&#21521;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SSMT&#25552;&#39640;&#20102;&#35789;&#32032;&#20016;&#23500;&#30340;&#32858;&#38598;&#24615;&#35821;&#35328;&#30340;chrF&#20998;&#25968;&#12290;&#22686;&#30410;&#22312;&#38750;&#24120;&#20302;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#26368;&#24378;&#12290;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;SSMT&#36824;&#23398;&#20064;&#21040;&#20102;&#26356;&#25509;&#36817;&#35821;&#32032;&#30340;&#23376;&#35789;&#65292;&#24182;&#19988;&#22312;&#29992;&#20110;&#35780;&#20272;&#24418;&#24577;&#30340;&#27979;&#35797;&#38598;&#19978;&#35777;&#26126;&#20102;&#26356;&#22823;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subword segmenters like BPE operate as a preprocessing step in neural machine translation and other (conditional) language models. They are applied to datasets before training, so translation or text generation quality relies on the quality of segmentations. We propose a departure from this paradigm, called subword segmental machine translation (SSMT). SSMT unifies subword segmentation and MT in a single trainable model. It learns to segment target sentence words while jointly learning to generate target sentences. To use SSMT during inference we propose dynamic decoding, a text generation algorithm that adapts segmentations as it generates translations. Experiments across 6 translation directions show that SSMT improves chrF scores for morphologically rich agglutinative languages. Gains are strongest in the very low-resource scenario. SSMT also learns subwords that are closer to morphemes compared to baselines and proves more robust on a test set constructed for evaluating morphologic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#24605;&#32500;&#25552;&#31034;&#26041;&#27861;&#65292;&#21517;&#20026;XLT&#65292;&#29992;&#20110;&#25552;&#39640;LLMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21508;&#31181;&#22810;&#35821;&#35328;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#19981;&#21516;&#35821;&#35328;&#20013;&#20219;&#21153;&#24615;&#33021;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2305.07004</link><description>&lt;p&gt;
LLM &#19981;&#21516;&#35821;&#35328;&#30340;&#24615;&#33021;&#34920;&#29616;&#19981;&#19968;: &#36890;&#36807;&#36328;&#35821;&#35328;&#24605;&#32500;&#25552;&#31034;&#25552;&#39640;&#22810;&#35821;&#35328;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting. (arXiv:2305.07004v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07004
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#24605;&#32500;&#25552;&#31034;&#26041;&#27861;&#65292;&#21517;&#20026;XLT&#65292;&#29992;&#20110;&#25552;&#39640;LLMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21508;&#31181;&#22810;&#35821;&#35328;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#19981;&#21516;&#35821;&#35328;&#20013;&#20219;&#21153;&#24615;&#33021;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#20855;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#36328;&#35821;&#35328;&#24605;&#32500;&#25552;&#31034;(XLT)&#65292;&#20197;&#31995;&#32479;&#22320;&#25552;&#39640;LLMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;XLT&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#27169;&#26495;&#25552;&#31034;&#65292;&#21487;&#20197;&#21050;&#28608;&#36328;&#35821;&#35328;&#21644;&#36923;&#36753;&#25512;&#29702;&#25216;&#33021;&#65292;&#20197;&#22686;&#24378;&#19981;&#21516;&#35821;&#35328;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#39640;&#36164;&#28304;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;7&#20010;&#20856;&#22411;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#30340;&#20840;&#38754;&#35780;&#20272;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XLT&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#26174;&#33879;&#20943;&#23569;&#20102;&#19981;&#21516;&#35821;&#35328;&#20013;&#27599;&#20010;&#20219;&#21153;&#24179;&#22343;&#24615;&#33021;&#21644;&#26368;&#20339;&#24615;&#33021;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;XLT&#22312;&#31639;&#26415;&#25512;&#29702;&#21644;&#24320;&#25918;&#22495;&#38382;&#31572;&#26041;&#38754;&#24102;&#26469;&#20102;&#36229;&#36807;10&#20010;&#30334;&#20998;&#28857;&#30340;&#24179;&#22343;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate impressive multilingual capability, but their performance varies substantially across different languages. In this work, we introduce a simple yet effective method, called cross-lingual-thought prompting (XLT), to systematically improve the multilingual capability of LLMs. Specifically, XLT is a generic template prompt that stimulates cross-lingual and logical reasoning skills to enhance task performance across languages. We conduct comprehensive evaluations on 7 typical benchmarks related to reasoning, understanding, and generation tasks, covering both high-resource and low-resource languages. Experimental results show that XLT not only remarkably enhances the performance of various multilingual tasks but also significantly reduces the gap between the average performance and the best performance of each task in different languages. Notably, XLT brings over 10 points of average improvement in arithmetic reasoning and open-domain question-answeri
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25351;&#20196;&#36981;&#24490;&#20026;&#26041;&#27861;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#20559;&#22909;&#25110;&#38656;&#27714;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#36827;&#32780;&#25552;&#39640;&#25512;&#33616;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.07001</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20316;&#20026;&#25351;&#20196;&#36981;&#24490;&#30340;&#26041;&#27861;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recommendation as Instruction Following: A Large Language Model Empowered Recommendation Approach. (arXiv:2305.07001v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07001
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25351;&#20196;&#36981;&#24490;&#20026;&#26041;&#27861;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#20559;&#22909;&#25110;&#38656;&#27714;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#36827;&#32780;&#25552;&#39640;&#25512;&#33616;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#25512;&#33616;&#31995;&#32479;&#22312;&#30740;&#31350;&#21644;&#20135;&#19994;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#19988;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#24320;&#21457;&#26377;&#25928;&#30340;&#25512;&#33616;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#20174;&#21382;&#21490;&#34892;&#20026;&#25968;&#25454;&#20013;&#23398;&#20064;&#28508;&#22312;&#30340;&#29992;&#25143;&#20559;&#22909;&#65292;&#36827;&#32780;&#20272;&#35745;&#29992;&#25143;-&#39033;&#30446;&#21305;&#37197;&#20851;&#31995;&#20197;&#36827;&#34892;&#25512;&#33616;&#12290;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#26399;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#25512;&#33616;&#27169;&#22411;&#65292;&#23558;&#25512;&#33616;&#35270;&#20026;LLMs&#30340;&#25351;&#20196;&#36981;&#24490;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#29992;&#25143;&#30340;&#20559;&#22909;&#25110;&#38656;&#27714;&#21487;&#20197;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65288;&#31216;&#20026;&#25351;&#20196;&#65289;&#26469;&#34920;&#36798;&#65292;&#20174;&#32780;LLMs&#21487;&#20197;&#29702;&#35299;&#24182;&#36827;&#19968;&#27493;&#25191;&#34892;&#25351;&#20196;&#20197;&#36798;&#21040;&#25512;&#33616;&#20219;&#21153;&#30340;&#30446;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#25351;&#20196;&#24494;&#35843;&#24320;&#28304;LLM&#65288;3B Flan-T5-XL&#65289;&#26469;&#24320;&#21457;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#20351;LLMs&#36866;&#24212;&#25512;&#33616;&#31995;&#32479;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#36716;&#25442;&#26041;&#27861;&#65292;&#23558;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#36716;&#21270;&#20026;&#25351;&#20196;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#26041;&#27861;&#65292;&#22312;&#25512;&#33616;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past decades, recommender systems have attracted much attention in both research and industry communities, and a large number of studies have been devoted to developing effective recommendation models. Basically speaking, these models mainly learn the underlying user preference from historical behavior data, and then estimate the user-item matching relationships for recommendations. Inspired by the recent progress on large language models (LLMs), we take a different approach to developing the recommendation models, considering recommendation as instruction following by LLMs. The key idea is that the preferences or needs of a user can be expressed in natural language descriptions (called instructions), so that LLMs can understand and further execute the instruction for fulfilling the recommendation task. Instead of using public APIs of LLMs, we instruction tune an open-source LLM (3B Flan-T5-XL), in order to better adapt LLMs to recommender systems. For this purpose, we first des
&lt;/p&gt;</description></item><item><title>SMATCH++&#26159;&#19968;&#20010;&#29992;&#20110;&#35821;&#20041;&#22270;&#26631;&#20934;&#21270;&#21644;&#25193;&#23637;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24230;&#37327;&#20998;&#20026;&#39044;&#22788;&#29702;&#12289;&#23545;&#40784;&#21644;&#35780;&#20998;&#19977;&#20010;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#24230;&#37327;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23545;&#40784;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06993</link><description>&lt;p&gt;
SMATCH++: &#35821;&#20041;&#22270;&#30340;&#26631;&#20934;&#21270;&#21644;&#25193;&#23637;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
SMATCH++: Standardized and Extended Evaluation of Semantic Graphs. (arXiv:2305.06993v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06993
&lt;/p&gt;
&lt;p&gt;
SMATCH++&#26159;&#19968;&#20010;&#29992;&#20110;&#35821;&#20041;&#22270;&#26631;&#20934;&#21270;&#21644;&#25193;&#23637;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24230;&#37327;&#20998;&#20026;&#39044;&#22788;&#29702;&#12289;&#23545;&#40784;&#21644;&#35780;&#20998;&#19977;&#20010;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#24230;&#37327;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#23545;&#40784;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Smatch&#24230;&#37327;&#26159;&#35780;&#20272;&#22270;&#24418;&#36317;&#31163;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#20363;&#22914;&#35780;&#20272;&#35821;&#20041;&#22270;&#35299;&#26512;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35813;&#24230;&#37327;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#22914;&#19981;&#36879;&#26126;&#30340;&#39044;&#22788;&#29702;&#36873;&#25321;&#21487;&#33021;&#20250;&#24433;&#21709;&#32467;&#26524;&#65292;&#24403;&#21069;&#30340;&#22270;&#24418;&#23545;&#40784;&#35299;&#31639;&#22120;&#27809;&#26377;&#25552;&#20379;&#19978;&#30028;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#19978;&#30028;&#65292;&#20844;&#24179;&#30340;&#35780;&#20272;&#19981;&#33021;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;Smatch&#30340;&#33258;&#36866;&#24212;&#25193;&#23637;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#32454;&#31890;&#24230;&#35821;&#20041;&#30456;&#20284;&#24615;&#65289;&#20998;&#25955;&#65292;&#24182;&#32570;&#20047;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#26816;&#26597;&#65292;&#25105;&#20204;&#23558;&#24230;&#37327;&#20998;&#20026;&#19977;&#20010;&#27169;&#22359;&#65306;&#39044;&#22788;&#29702;&#12289;&#23545;&#40784;&#21644;&#35780;&#20998;&#12290;&#26816;&#26597;&#27599;&#20010;&#27169;&#22359;&#65292;&#25105;&#20204;&#25351;&#23450;&#23427;&#30340;&#30446;&#26631;&#24182;&#35786;&#26029;&#28508;&#22312;&#38382;&#39064;&#65292;&#20026;&#27492;&#25105;&#20204;&#35752;&#35770;&#24182;&#27979;&#35797;&#32531;&#35299;&#31574;&#30053;&#12290;&#23545;&#20110;&#39044;&#22788;&#29702;&#65292;&#25105;&#20204;&#23637;&#31034;&#22914;&#20309;&#23436;&#20840;&#31526;&#21512;&#20801;&#35768;&#32467;&#26500;&#19981;&#21516;&#20294;&#26377;&#25928;&#30340;&#22270;&#24418;&#27880;&#37322;&#25351;&#21335;&#12290;&#20026;&#20102;&#26356;&#23433;&#20840;&#21644;&#22686;&#24378;&#30340;&#23545;&#40784;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#27969;&#24418;&#31639;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Smatch metric is a popular method for evaluating graph distances, as is necessary, for instance, to assess the performance of semantic graph parsing systems. However, we observe some issues in the metric that jeopardize meaningful evaluation. E.g., opaque pre-processing choices can affect results, and current graph-alignment solvers do not provide us with upper-bounds. Without upper-bounds, however, fair evaluation is not guaranteed. Furthermore, adaptions of Smatch for extended tasks (e.g., fine-grained semantic similarity) are spread out, and lack a unifying framework.  For better inspection, we divide the metric into three modules: pre-processing, alignment, and scoring. Examining each module, we specify its goals and diagnose potential issues, for which we discuss and test mitigation strategies. For pre-processing, we show how to fully conform to annotation guidelines that allow structurally deviating but valid graphs. For safer and enhanced alignment, we show the feasibility o
&lt;/p&gt;</description></item><item><title>SeViLA&#26159;&#19968;&#20010;&#21033;&#29992;&#21333;&#20010;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#22312;&#35270;&#39057;&#23450;&#20301;&#21644;&#38382;&#31572;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#33258;&#25105;&#38142;&#25509;&#31574;&#30053;&#35757;&#32451;&#23616;&#37096;&#21270;&#22120;&#21644;&#22238;&#31572;&#22120;&#27169;&#22359;&#20197;&#23450;&#20301;&#26368;&#20855;&#20449;&#24687;&#30340;&#20851;&#38190;&#24103;&#20197;&#22238;&#31572;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06988</link><description>&lt;p&gt;
&#33258;&#25105;&#38142;&#24335;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#35270;&#39057;&#23450;&#20301;&#19982;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Self-Chained Image-Language Model for Video Localization and Question Answering. (arXiv:2305.06988v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06988
&lt;/p&gt;
&lt;p&gt;
SeViLA&#26159;&#19968;&#20010;&#21033;&#29992;&#21333;&#20010;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#22312;&#35270;&#39057;&#23450;&#20301;&#21644;&#38382;&#31572;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#33258;&#25105;&#38142;&#25509;&#31574;&#30053;&#35757;&#32451;&#23616;&#37096;&#21270;&#22120;&#21644;&#22238;&#31572;&#22120;&#27169;&#22359;&#20197;&#23450;&#20301;&#26368;&#20855;&#20449;&#24687;&#30340;&#20851;&#38190;&#24103;&#20197;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35270;&#39057;&#38382;&#31572;&#33021;&#22815;&#21462;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#34429;&#28982;&#36825;&#20123;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#21551;&#21160;&#35270;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#23558;&#22343;&#21248;&#37319;&#26679;&#30340;&#35270;&#39057;&#24103;&#20316;&#20026;&#35270;&#35273;&#36755;&#20837;&#36827;&#34892;&#20018;&#25509;&#65292;&#32780;&#26410;&#36827;&#34892;&#26174;&#24335;&#30340;&#35821;&#35328;&#24863;&#30693;&#21644;&#26102;&#38388;&#24314;&#27169;&#12290;&#24403;&#35270;&#39057;&#36755;&#20837;&#20013;&#21482;&#26377;&#19968;&#37096;&#20998;&#19982;&#35821;&#35328;&#26597;&#35810;&#30456;&#20851;&#26102;&#65292;&#36825;&#31181;&#22343;&#21248;&#24103;&#37319;&#26679;&#36890;&#24120;&#20250;&#23548;&#33268;&#37325;&#35201;&#30340;&#35270;&#35273;&#32447;&#32034;&#20002;&#22833;&#12290;&#23613;&#31649;&#20154;&#31867;&#36890;&#24120;&#20250;&#25214;&#21040;&#35270;&#39057;&#20013;&#35201;&#20851;&#27880;&#30340;&#29255;&#27573;&#24182;&#20498;&#24102;&#29255;&#21051;&#26469;&#22238;&#31572;&#38382;&#39064;&#65292;&#20294;&#35757;&#32451;&#19968;&#20010;&#26126;&#30830;&#30340;&#35270;&#39057;&#29255;&#27573;&#23616;&#37096;&#21270;&#22120;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;&#27880;&#37322;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SeViLA&#26694;&#26550;&#65292;&#21033;&#29992;&#21333;&#20010;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#65288;BLIP-2&#65289;&#26469;&#22788;&#29702;&#35270;&#39057;&#30340;&#26102;&#38388;&#20851;&#38190;&#24103;&#23450;&#20301;&#21644;&#38382;&#31572;&#12290;SeViLA&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#23616;&#37096;&#21270;&#22120;&#21644;&#22238;&#31572;&#22120;&#65292;&#20004;&#32773;&#20849;&#20139;&#30456;&#21516;&#30340;&#22270;&#20687;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#38142;&#25509;&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23450;&#20301;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#24103;&#20197;&#22238;&#31572;&#32473;&#23450;&#30340;&#38382;&#39064;&#12290;&#22312;TVQA&#12289;TVR&#21644;How2QA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SeViLA&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#27880;&#37322;&#23601;&#33021;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown promising results on utilizing pre-trained image-language models for video question answering. While these image-language models can efficiently bootstrap the representation learning of video-language models, they typically concatenate uniformly sampled video frames as visual inputs without explicit language-aware, temporal modeling. When only a portion of a video input is relevant to the language query, such uniform frame sampling can often lead to missing important visual cues. Although humans often find a video moment to focus on and rewind the moment to answer questions, training a query-aware video moment localizer often requires expensive annotations and high computational costs. To address this issue, we propose Self-Chained Video Localization-Answering (SeViLA), a novel framework that leverages a single image-language model (BLIP-2) to tackle both temporal keyframe localization and QA on videos. SeViLA framework consists of two modules: Localizer and A
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#35789;&#27719;&#21305;&#37197;&#20316;&#20026;&#35780;&#20272;&#26041;&#27861;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#26377;&#38480;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#20013;&#19968;&#20010;&#38646;-shot&#27169;&#22411;&#30340;&#24615;&#33021;&#22823;&#24133;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.06984</link><description>&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#35780;&#20272;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Evaluating Open-Domain Question Answering in the Era of Large Language Models. (arXiv:2305.06984v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#35789;&#27719;&#21305;&#37197;&#20316;&#20026;&#35780;&#20272;&#26041;&#27861;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#26377;&#38480;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#20013;&#19968;&#20010;&#38646;-shot&#27169;&#22411;&#30340;&#24615;&#33021;&#22823;&#24133;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#21305;&#37197;&#20173;&#26159;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65288;QA&#65289;&#30340;&#20107;&#23454;&#35780;&#20272;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#19968;&#20010;&#21512;&#29702;&#30340;&#20505;&#36873;&#31572;&#26696;&#26410;&#20986;&#29616;&#22312;&#37329;&#26631;&#20934;&#31572;&#26696;&#21015;&#34920;&#20013;&#26102;&#65292;&#35789;&#27719;&#21305;&#37197;&#23436;&#20840;&#22833;&#36133;&#65292;&#38543;&#30528;&#25105;&#20204;&#20174;&#25277;&#21462;&#27169;&#22411;&#36716;&#21521;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#31181;&#24773;&#20917;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;QA&#20013;&#30340;&#26368;&#36817;&#25104;&#21151;&#21152;&#21095;&#20102;&#35789;&#27719;&#21305;&#37197;&#30340;&#22833;&#36133;&#65292;&#22240;&#20026;&#20505;&#36873;&#31572;&#26696;&#21464;&#24471;&#26356;&#38271;&#65292;&#22240;&#27492;&#19982;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#21305;&#37197;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32570;&#20047;&#20934;&#30830;&#30340;&#35780;&#20272;&#65292;&#24320;&#25918;&#39046;&#22495;QA&#30340;&#30495;&#27491;&#36827;&#23637;&#20173;&#28982;&#26410;&#30693;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;NQ-open&#30340;&#19968;&#20010;&#23376;&#38598;&#19978;&#25163;&#21160;&#35780;&#20272;&#21508;&#31181;&#24320;&#25918;&#39046;&#22495;QA&#27169;&#22411;&#65288;&#21253;&#25324;LLMs&#65289;&#30340;&#31572;&#26696;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#25581;&#31034;&#65292;&#23613;&#31649;&#25152;&#26377;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#33021;&#34987;&#26174;&#30528;&#20302;&#20272;&#65292;&#20294;InstructGPT&#65288;&#38646;-shot&#65289;LLM&#30340;&#24615;&#33021;&#22686;&#21152;&#20102;&#36817;60&#65285;&#65292;&#20351;&#20854;&#19982;&#29616;&#26377;&#30340;&#39030;&#32423;&#27169;&#22411;&#24182;&#39550;&#40784;&#39537;&#65292;&#32780;I
&lt;/p&gt;
&lt;p&gt;
Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more challenging. Without accurate evaluation, the true progress in open-domain QA remains unknown. In this paper, we conduct a thorough analysis of various open-domain QA models, including LLMs, by manually evaluating their answers on a subset of NQ-open, a popular benchmark. Our assessments reveal that while the true performance of all models is significantly underestimated, the performance of the InstructGPT (zero-shot) LLM increases by nearly +60%, making it on par with existing top models, and the I
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26356;&#32039;&#23494;&#22320;&#38598;&#25104;&#20102;&#20027;&#21160;&#26816;&#32034;&#21644;&#29983;&#25104;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19968;&#32452;&#21477;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06983</link><description>&lt;p&gt;
&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Active Retrieval Augmented Generation. (arXiv:2305.06983v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26356;&#32039;&#23494;&#22320;&#38598;&#25104;&#20102;&#20027;&#21160;&#26816;&#32034;&#21644;&#29983;&#25104;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19968;&#32452;&#21477;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20855;&#26377;&#29702;&#35299;&#21644;&#29983;&#25104;&#35821;&#35328;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#20135;&#29983;&#34394;&#20551;&#30340;&#21644;&#38169;&#35823;&#30340;&#36755;&#20986;&#12290;&#20174;&#22806;&#37096;&#30693;&#35782;&#36164;&#28304;&#20013;&#26816;&#32034;&#20449;&#24687;&#26469;&#22686;&#24378;LM&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26816;&#32034;&#22686;&#24378;&#30340;LM&#37319;&#29992;&#19968;&#31181;&#26816;&#32034;&#21644;&#29983;&#25104;&#30340;&#35774;&#32622;&#65292;&#20165;&#22522;&#20110;&#36755;&#20837;&#19968;&#27425;&#26816;&#32034;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#29983;&#25104;&#38271;&#25991;&#26412;&#30340;&#26356;&#26222;&#36941;&#30340;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#19981;&#26029;&#22320;&#25910;&#38598;&#20449;&#24687;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36807;&#21435;&#26377;&#19968;&#20123;&#26816;&#32034;&#20449;&#24687;&#65292;&#21516;&#26102;&#29983;&#25104;&#36755;&#20986;&#30340;&#21162;&#21147;&#65292;&#22823;&#22810;&#25968;&#37117;&#26159;&#20351;&#29992;&#21069;&#19968;&#20010;&#19978;&#19979;&#25991;&#20316;&#20026;&#26597;&#35810;&#65292;&#22312;&#22266;&#23450;&#30340;&#26102;&#38388;&#38388;&#38548;&#20869;&#26816;&#32034;&#25991;&#26723;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#24191;&#20041;&#35270;&#22270;&#65292;&#21363;&#22312;&#25972;&#20010;&#29983;&#25104;&#36807;&#31243;&#20013;&#20027;&#21160;&#20915;&#23450;&#20309;&#26102;&#20197;&#21450;&#20309;&#26102;&#20174;&#21738;&#37324;&#26816;&#32034;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21069;&#30651;&#24615;&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;FLARE&#65289;&#65292;&#23427;&#36890;&#36807;&#20801;&#35768;&#29983;&#25104;&#22120;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#20027;&#21160;&#26597;&#35810;&#26816;&#32034;&#32452;&#20214;&#26469;&#26356;&#32039;&#23494;&#22320;&#38598;&#25104;&#20027;&#21160;&#26816;&#32034;&#21644;&#29983;&#25104;&#12290;FLARE&#22312;&#19968;&#32452;&#21477;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20027;&#21160;&#26816;&#32034;&#22312;&#25972;&#20010;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval-augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout the generation process is essential. There have been some past efforts to retrieve information multiple times while generating outputs, which mostly retrieve documents at fixed intervals using the previous context as queries. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval aug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#20010;&#23545;&#20256;&#32479;&#35748;&#35782;&#30340;&#39072;&#35206;&#24615;&#35266;&#28857;&#65306;&#22312;&#35745;&#31639;&#26426;&#32534;&#31243;&#39046;&#22495;&#30340;&#20856;&#22411;ChatGPT&#20219;&#21153;&#20013;&#65292;&#20154;&#31867;&#31243;&#24207;&#21592;&#20173;&#28982;&#27604;ChatGPT&#26356;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2305.06934</link><description>&lt;p&gt;
&#20154;&#31867;&#20173;&#28982;&#27604;ChatGPT&#26356;&#20248;&#31168;&#65306;&#20197;IEEEXtreme&#32534;&#31243;&#31454;&#36187;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition. (arXiv:2305.06934v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#20010;&#23545;&#20256;&#32479;&#35748;&#35782;&#30340;&#39072;&#35206;&#24615;&#35266;&#28857;&#65306;&#22312;&#35745;&#31639;&#26426;&#32534;&#31243;&#39046;&#22495;&#30340;&#20856;&#22411;ChatGPT&#20219;&#21153;&#20013;&#65292;&#20154;&#31867;&#31243;&#24207;&#21592;&#20173;&#28982;&#27604;ChatGPT&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;ChatGPT&#21457;&#24067;&#20197;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#31361;&#20986;&#20102;ChatGPT&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#23427;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#39046;&#22495;&#20013;&#24448;&#24448;&#21487;&#20197;&#19982;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#36890;&#36807;&#23637;&#31034;&#20154;&#31867;&#34920;&#29616;&#20248;&#24322;&#30340;&#23454;&#20363;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#31435;&#30340;&#35266;&#28857;&#65292;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#26426;&#32534;&#31243;&#39046;&#22495;&#30340;&#20856;&#22411;ChatGPT&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#23558;IEEExtreme&#32534;&#31243;&#25361;&#25112;&#36187;&#20316;&#20026;&#22522;&#20934;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#22797;&#26434;&#24230;&#38382;&#39064;&#30340;&#30693;&#21517;&#22269;&#38469;&#32534;&#31243;&#31454;&#36187;&#12290;&#20026;&#20102;&#36827;&#34892;&#24443;&#24213;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;102&#20010;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;IEEExtreme&#29256;&#26412;&#30340;&#25361;&#25112;&#65292;&#20351;&#29992;&#19977;&#31181;&#20027;&#35201;&#32534;&#31243;&#35821;&#35328;Python&#12289;Java&#21644;C++&#36827;&#34892;&#25191;&#34892;&#12290;&#25105;&#20204;&#30340;&#32463;&#39564;&#20998;&#26512;&#25552;&#20379;&#20102;&#35777;&#25454;&#65292;&#35777;&#26126;&#19982;&#26222;&#36941;&#35748;&#20026;&#30340;&#30456;&#21453;&#65292;&#20154;&#31867;&#31243;&#24207;&#21592;&#22312;&#32534;&#31243;&#29615;&#22659;&#20013;&#30340;&#38382;&#39064;&#35299;&#20915;&#30340;&#26576;&#20123;&#26041;&#38754;&#19978;&#20173;&#28982;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the release of ChatGPT, numerous studies have highlighted the remarkable performance of ChatGPT, which often rivals or even surpasses human capabilities in various tasks and domains. However, this paper presents a contrasting perspective by demonstrating an instance where human performance excels in typical tasks suited for ChatGPT, specifically in the domain of computer programming. We utilize the IEEExtreme Challenge competition as a benchmark, a prestigious, annual international programming contest encompassing a wide range of problems with different complexities. To conduct a thorough evaluation, we selected and executed a diverse set of 102 challenges, drawn from five distinct IEEExtreme editions, using three major programming languages: Python, Java, and C++. Our empirical analysis provides evidence that contrary to popular belief, human programmers maintain a competitive edge over ChatGPT in certain aspects of problem-solving within the programming context. In fact, we fou
&lt;/p&gt;</description></item><item><title>AfriQA&#26159;&#31532;&#19968;&#20010;&#19987;&#27880;&#20110;&#38750;&#27954;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;QA&#25968;&#25454;&#38598;&#65292;&#24357;&#34917;&#20102;&#38750;&#27954;&#35821;&#35328;&#25968;&#23383;&#21270;&#20869;&#23481;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#33258;&#21160;&#32763;&#35793;&#21644;&#22810;&#35821;&#35328;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#36739;&#24046;&#65292;&#38656;&#35201;&#25903;&#25345;&#36328;&#35821;&#35328;&#25512;&#29702;&#21644;&#36716;&#31227;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.06897</link><description>&lt;p&gt;
AfriQA&#65306;&#38024;&#23545;&#38750;&#27954;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#24320;&#25918;&#26816;&#32034;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
AfriQA: Cross-lingual Open-Retrieval Question Answering for African Languages. (arXiv:2305.06897v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06897
&lt;/p&gt;
&lt;p&gt;
AfriQA&#26159;&#31532;&#19968;&#20010;&#19987;&#27880;&#20110;&#38750;&#27954;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;QA&#25968;&#25454;&#38598;&#65292;&#24357;&#34917;&#20102;&#38750;&#27954;&#35821;&#35328;&#25968;&#23383;&#21270;&#20869;&#23481;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#33258;&#21160;&#32763;&#35793;&#21644;&#22810;&#35821;&#35328;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#36739;&#24046;&#65292;&#38656;&#35201;&#25903;&#25345;&#36328;&#35821;&#35328;&#25512;&#29702;&#21644;&#36716;&#31227;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21270;&#30340;&#38750;&#27954;&#35821;&#35328;&#20869;&#23481;&#36828;&#36828;&#19981;&#36275;&#65292;&#36825;&#20351;&#24471;&#38382;&#31572;&#31995;&#32479;&#38590;&#20197;&#28385;&#36275;&#29992;&#25143;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#36328;&#35821;&#35328;&#24320;&#25918;&#26816;&#32034;&#38382;&#31572;&#65288;XOR QA&#65289;&#31995;&#32479;--&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#22312;&#20026;&#20154;&#20204;&#25552;&#20379;&#26412;&#22320;&#35821;&#35328;&#26381;&#21153;&#30340;&#21516;&#26102;&#20174;&#20854;&#20182;&#35821;&#35328;&#20013;&#33719;&#21462;&#31572;&#26696;&#20869;&#23481;--&#25552;&#20379;&#20102;&#19968;&#31181;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#30340;&#25163;&#27573;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;AfriQA&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19987;&#27880;&#20110;&#38750;&#27954;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;QA&#25968;&#25454;&#38598;&#12290;AfriQA&#21253;&#25324;10&#31181;&#38750;&#27954;&#35821;&#35328;&#30340;12,000&#22810;&#20010;XOR QA&#31034;&#20363;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#20851;&#27880;&#20132;&#21449;&#35821;&#35328;QA&#22686;&#24378;&#30446;&#26631;&#35821;&#35328;&#35206;&#30422;&#33539;&#22260;&#30340;&#35821;&#35328;&#65292;&#20294;AfriQA&#20391;&#37325;&#20110;&#20132;&#21449;&#35821;&#35328;&#31572;&#26696;&#20869;&#23481;&#26159;&#21807;&#19968;&#39640;&#35206;&#30422;&#33539;&#22260;&#31572;&#26696;&#20869;&#23481;&#30340;&#35821;&#35328;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#38750;&#27954;&#35821;&#35328;&#26159;XOR QA&#20013;&#26368;&#37325;&#35201;&#21644;&#26368;&#29616;&#23454;&#30340;&#29992;&#20363;&#20043;&#19968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#33258;&#21160;&#32763;&#35793;&#21644;&#22810;&#35821;&#35328;&#26816;&#32034;&#31995;&#32479;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#31361;&#26174;&#20102;&#38656;&#35201;&#25903;&#25345;&#36328;&#35821;&#35328;&#25512;&#29702;&#21644;&#36716;&#31227;&#23398;&#20064;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
African languages have far less in-language content available digitally, making it challenging for question answering systems to satisfy the information needs of users. Cross-lingual open-retrieval question answering (XOR QA) systems -- those that retrieve answer content from other languages while serving people in their native language -- offer a means of filling this gap. To this end, we create AfriQA, the first cross-lingual QA dataset with a focus on African languages. AfriQA includes 12,000+ XOR QA examples across 10 African languages. While previous datasets have focused primarily on languages where cross-lingual QA augments coverage from the target language, AfriQA focuses on languages where cross-lingual answer content is the only high-coverage source of answer content. Because of this, we argue that African languages are one of the most important and realistic use cases for XOR QA. Our experiments demonstrate the poor performance of automatic translation and multilingual retri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;Transformer&#21644;&#20219;&#21153;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#35774;&#35745;&#20102;&#33258;&#21160;&#31995;&#32479;&#26469;&#26816;&#27979;&#21644;&#20998;&#31867;&#22312;&#32447;&#31354;&#38388;&#20013;&#30340;&#24615;&#21035;&#27495;&#35270;&#20869;&#23481;&#65292;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#19982;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;SemEval-2023&#20219;&#21153;10&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#36798;&#21040;&#20102;83&#65285;&#65292;64&#65285;&#21644;47&#65285;&#30340;F1&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.06892</link><description>&lt;p&gt;
IUST_NLP&#22312;SemEval-2023&#20219;&#21153;10&#20013;&#30340;&#24212;&#29992;&#65306;&#20351;&#29992;Transformer&#21644;&#20219;&#21153;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#24615;&#21035;&#27495;&#35270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
IUST_NLP at SemEval-2023 Task 10: Explainable Detecting Sexism with Transformers and Task-adaptive Pretraining. (arXiv:2305.06892v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;Transformer&#21644;&#20219;&#21153;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#35774;&#35745;&#20102;&#33258;&#21160;&#31995;&#32479;&#26469;&#26816;&#27979;&#21644;&#20998;&#31867;&#22312;&#32447;&#31354;&#38388;&#20013;&#30340;&#24615;&#21035;&#27495;&#35270;&#20869;&#23481;&#65292;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#19982;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;SemEval-2023&#20219;&#21153;10&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#36798;&#21040;&#20102;83&#65285;&#65292;64&#65285;&#21644;47&#65285;&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;SemEval-2023&#20219;&#21153;10&#20013;&#30340;&#31995;&#32479;&#65306;&#22312;&#32447;&#24615;&#21035;&#27495;&#35270;&#30340;&#21487;&#35299;&#37322;&#24615;&#26816;&#27979;&#65288;EDOS&#65289;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35774;&#35745;&#19968;&#31181;&#33258;&#21160;&#31995;&#32479;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#20998;&#31867;&#22312;&#32447;&#31354;&#38388;&#20013;&#30340;&#24615;&#21035;&#27495;&#35270;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#20219;&#21153;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#21644;&#38598;&#25104;&#23398;&#20064;&#12290;&#25105;&#20204;&#31995;&#32479;&#30340;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#20998;&#26512;&#19981;&#21516;&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#24182;&#32467;&#21512;&#36825;&#20123;&#27169;&#22411;&#65292;&#20197;&#21450;&#25552;&#20379;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#20960;&#31181;&#20854;&#20182;&#31574;&#30053;&#12290;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#23376;&#20219;&#21153;A&#65292;B&#21644;C&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;83&#65285;&#65292;64&#65285;&#21644;47&#65285;&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our system on SemEval-2023 Task 10: Explainable Detection of Online Sexism (EDOS). This work aims to design an automatic system for detecting and classifying sexist content in online spaces. We propose a set of transformer-based pre-trained models with task-adaptive pretraining and ensemble learning. The main contributions of our system include analyzing the performance of different transformer-based pre-trained models and combining these models, as well as providing an efficient method using large amounts of unlabeled data for model adaptive pretraining. We have also explored several other strategies. On the test dataset, our system achieves F1-scores of 83%, 64%, and 47% on subtasks A, B, and C, respectively.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#27169;&#22411;&#20381;&#36182;&#24050;&#30693;&#34394;&#20551;&#29305;&#24449;&#30340;&#25216;&#26415;&#65292;&#24182;&#35780;&#20272;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#38382;&#31572;&#27169;&#22411;&#21644;&#21435;&#20559;&#32622;&#26041;&#27861;&#23545;&#22823;&#37327;&#24050;&#30693;&#21644;&#26032;&#21457;&#29616;&#30340;&#39044;&#27979;&#20559;&#24046;&#30340;&#40065;&#26834;&#24615;&#12290;&#20854;&#21457;&#29616;&#21435;&#20559;&#32622;&#26041;&#27861;&#19981;&#33021;&#36890;&#36807;&#20943;&#36731;&#23545;&#20559;&#24046;&#29305;&#24449;&#30340;&#20381;&#36182;&#26469;&#35299;&#37322;OOD&#25910;&#30410;&#65292;&#34920;&#26126;&#20559;&#24046;&#22312;QA&#25968;&#25454;&#38598;&#20013;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2305.06841</link><description>&lt;p&gt;
&#19977;&#24605;&#32780;&#21518;&#34892;&#65306;&#34913;&#37327;&#28040;&#38500;&#38382;&#31572;&#27169;&#22411;&#39044;&#27979;&#24555;&#25463;&#26041;&#24335;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models. (arXiv:2305.06841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06841
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#27169;&#22411;&#20381;&#36182;&#24050;&#30693;&#34394;&#20551;&#29305;&#24449;&#30340;&#25216;&#26415;&#65292;&#24182;&#35780;&#20272;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#38382;&#31572;&#27169;&#22411;&#21644;&#21435;&#20559;&#32622;&#26041;&#27861;&#23545;&#22823;&#37327;&#24050;&#30693;&#21644;&#26032;&#21457;&#29616;&#30340;&#39044;&#27979;&#20559;&#24046;&#30340;&#40065;&#26834;&#24615;&#12290;&#20854;&#21457;&#29616;&#21435;&#20559;&#32622;&#26041;&#27861;&#19981;&#33021;&#36890;&#36807;&#20943;&#36731;&#23545;&#20559;&#24046;&#29305;&#24449;&#30340;&#20381;&#36182;&#26469;&#35299;&#37322;OOD&#25910;&#30410;&#65292;&#34920;&#26126;&#20559;&#24046;&#22312;QA&#25968;&#25454;&#38598;&#20013;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLMs&#65289;&#20027;&#23548;&#20102;&#22823;&#37096;&#20998;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24314;&#27169;&#20551;&#28151;&#28102;&#30340;&#25903;&#25345;&#19979;, &#22240;&#27492;&#26377;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26576;&#20123;&#36825;&#20123;&#32467;&#26524;&#26159;&#30001;&#24314;&#27169;&#34394;&#20551;&#30456;&#20851;&#24615;&#23454;&#29616;&#30340;&#12290;&#20316;&#32773;&#24120;&#24120;&#36890;&#36807;&#35780;&#20272;&#21516;&#19968;&#20219;&#21153;&#30340;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#38598;&#21487;&#33021;&#20855;&#26377;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#30456;&#21516;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#27169;&#22411;&#20381;&#36182;&#20110;&#20219;&#20309;&#24050;&#30693;&#34394;&#20551;&#29305;&#24449;&#30340;&#23610;&#24230;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#21435;&#20559;&#32622;&#26041;&#27861;&#22312;&#38382;&#31572;&#65288;QA&#65289;&#20013;&#23545;&#22823;&#37327;&#24050;&#30693;&#21644;&#26032;&#21457;&#29616;&#30340;&#39044;&#27979;&#20559;&#24046;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#21435;&#20559;&#32622;&#26041;&#27861;&#30340;&#25253;&#21578;OOD&#25910;&#30410;&#19981;&#33021;&#36890;&#36807;&#20943;&#36731;&#23545;&#26377;&#20559;&#29305;&#24449;&#30340;&#20381;&#36182;&#26469;&#35299;&#37322;&#65292;&#36825;&#34920;&#26126;&#20559;&#24046;&#22312;QA&#25968;&#25454;&#38598;&#20013;&#20849;&#20139;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;OOD&#27169;&#22411;&#30340;&#34920;&#29616;&#21462;&#20915;&#20110;&#20559;&#24046;&#29305;&#24449;&#65292;&#19982;ID&#27169;&#22411;&#30456;&#24403;&#65292;&#36827;&#32780;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#65292;&#36825;&#25512;&#21160;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the Large Language Models (LLMs) dominate a majority of language understanding tasks, previous work shows that some of these results are supported by modelling spurious correlations of training datasets. Authors commonly assess model robustness by evaluating their models on out-of-distribution (OOD) datasets of the same task, but these datasets might share the bias of the training dataset.  We propose a simple method for measuring a scale of models' reliance on any identified spurious feature and assess the robustness towards a large set of known and newly found prediction biases for various pre-trained models and debiasing methods in Question Answering (QA). We find that the reported OOD gains of debiasing methods can not be explained by mitigated reliance on biased features, suggesting that biases are shared among QA datasets. We further evidence this by measuring that performance of OOD models depends on bias features comparably to the ID model, motivating future work to refin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#19968;&#20010;&#25991;&#26412;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#26631;&#27880;&#20102;&#21361;&#38505;&#24773;&#22659;&#21450;&#35282;&#33394;&#30340;&#24656;&#24807;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#26080;&#30417;&#30563;&#30340;&#22522;&#30784;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20449;&#21495;&#26469;&#26816;&#27979;&#36825;&#20123;&#24773;&#22659;&#65292;&#20294;&#35201;&#36827;&#19968;&#27493;&#28145;&#20837;&#20998;&#26512;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#21361;&#38505;&#21644;&#24656;&#24807;&#30340;&#25551;&#36848;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#12290;</title><link>http://arxiv.org/abs/2305.06818</link><description>&lt;p&gt;
&#25506;&#32034;&#24748;&#30097;&#30340;&#35745;&#31639;&#26426;&#20998;&#26512;&#65306;&#26816;&#27979;&#21361;&#38505;&#24773;&#22659;
&lt;/p&gt;
&lt;p&gt;
Towards a Computational Analysis of Suspense: Detecting Dangerous Situations. (arXiv:2305.06818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#19968;&#20010;&#25991;&#26412;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#26631;&#27880;&#20102;&#21361;&#38505;&#24773;&#22659;&#21450;&#35282;&#33394;&#30340;&#24656;&#24807;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#26080;&#30417;&#30563;&#30340;&#22522;&#30784;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20449;&#21495;&#26469;&#26816;&#27979;&#36825;&#20123;&#24773;&#22659;&#65292;&#20294;&#35201;&#36827;&#19968;&#27493;&#28145;&#20837;&#20998;&#26512;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#21361;&#38505;&#21644;&#24656;&#24807;&#30340;&#25551;&#36848;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24748;&#30097;&#26159;&#20445;&#25345;&#35835;&#32773;&#21442;&#19982;&#24182;&#24819;&#32487;&#32493;&#38405;&#35835;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#35745;&#31639;&#26426;&#25991;&#23398;&#30740;&#31350;&#39046;&#22495;&#23545;&#27492;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#12290;&#26412;&#25991;&#30528;&#37325;&#25506;&#35752;&#20316;&#32773;&#21487;&#20197;&#21033;&#29992;&#26469;&#22686;&#21152;&#24748;&#30097;&#30340;&#20803;&#32032;&#20043;&#19968;&#65306;&#21361;&#38505;&#24773;&#22659;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25991;&#26412;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#26631;&#27880;&#20102;&#21361;&#38505;&#24773;&#22659;&#65292;&#20998;&#36776;&#20102;7&#31181;&#31867;&#22411;&#30340;&#21361;&#38505;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26631;&#27880;&#20102;&#25991;&#26412;&#20013;&#25551;&#36848;&#35282;&#33394;&#25152;&#32463;&#21382;&#30340;&#24656;&#24807;&#30340;&#37096;&#20998;&#65292;&#19981;&#31649;&#23454;&#38469;&#19978;&#23384;&#22312;&#21361;&#38505;&#19982;&#21542;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#33258;&#21160;&#26816;&#27979;&#36825;&#20123;&#24773;&#22659;&#30340;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#26080;&#30417;&#30563;&#30340;&#22522;&#30784;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#20449;&#21495;&#65292;&#20294;&#36827;&#19968;&#27493;&#20998;&#26512;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#12290;&#19981;&#20986;&#25152;&#26009;&#65292;&#21361;&#38505;&#21644;&#24656;&#24807;&#30340;&#25551;&#36848;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#65292;&#26080;&#35770;&#26159;&#23616;&#37096;&#30340;(&#20363;&#22914;&#65292;&#21482;&#25552;&#21040;&#20294;&#23454;&#38469;&#19978;&#24182;&#19981;&#23384;&#22312;&#21361;&#38505;&#30340;&#24773;&#22659;)&#36824;&#26159;&#20840;&#23616;&#30340;(&#20363;&#22914;&#65292;&#8220;&#39118;&#26292;&#8221;&#22312;&#23383;&#38754;&#19978;&#20351;&#29992;&#30340;&#24773;&#22659;)&#12290;
&lt;/p&gt;
&lt;p&gt;
Suspense is an important tool in storytelling to keep readers engaged and wanting to read more. However, it has so far not been studied extensively in Computational Literary Studies. In this paper, we focus on one of the elements authors can use to build up suspense: dangerous situations. We introduce a corpus of texts annotated with dangerous situations, distinguishing between 7 types of danger. Additionally, we annotate parts of the text that describe fear experienced by a character, regardless of the actual presence of danger. We present experiments towards the automatic detection of these situations, finding that unsupervised baseline methods can provide valuable signals for the detection, but more complex methods are necessary for further analysis. Not unexpectedly, the description of danger and fear often relies heavily on the context, both local (e.g., situations where danger is only mentioned, but not actually present) and global (e.g., "storm" being used in a literal sense in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;THUIR&#22242;&#38431;&#22312;COLIEE 2023&#27861;&#24459;&#26696;&#20363;&#34164;&#28085;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#23581;&#35797;&#20102;&#20256;&#32479;&#30340;&#35789;&#27719;&#21305;&#37197;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#26356;&#22810;&#30340;&#21442;&#25968;&#21644;&#27861;&#24459;&#30693;&#35782;&#23545;&#27861;&#24459;&#26696;&#20363;&#34164;&#28085;&#20219;&#21153;&#26377;&#25152;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.06817</link><description>&lt;p&gt;
THUIR@COLIEE 2023&#65306;&#26356;&#22810;&#21442;&#25968;&#21644;&#27861;&#24459;&#30693;&#35782;&#29992;&#20110;&#27861;&#24459;&#26696;&#20363;&#34164;&#21547;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
THUIR@COLIEE 2023: More Parameters and Legal Knowledge for Legal Case Entailment. (arXiv:2305.06817v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;THUIR&#22242;&#38431;&#22312;COLIEE 2023&#27861;&#24459;&#26696;&#20363;&#34164;&#28085;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#23581;&#35797;&#20102;&#20256;&#32479;&#30340;&#35789;&#27719;&#21305;&#37197;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#26356;&#22810;&#30340;&#21442;&#25968;&#21644;&#27861;&#24459;&#30693;&#35782;&#23545;&#27861;&#24459;&#26696;&#20363;&#34164;&#28085;&#20219;&#21153;&#26377;&#25152;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;THUIR&#22242;&#38431;&#22312;COLIEE 2023&#27861;&#24459;&#26696;&#20363;&#34164;&#28085;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#12290;&#35813;&#20219;&#21153;&#35201;&#27714;&#21442;&#19982;&#32773;&#20174;&#32473;&#23450;&#30340;&#25903;&#25345;&#26696;&#20363;&#20013;&#35782;&#21035;&#19968;&#20010;&#29305;&#23450;&#27573;&#33853;&#65292;&#35813;&#27573;&#33853;&#34164;&#21547;&#20102;&#26597;&#35810;&#26696;&#20363;&#30340;&#20915;&#23450;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#20256;&#32479;&#30340;&#35789;&#27719;&#21305;&#37197;&#26041;&#27861;&#21644;&#20855;&#26377;&#19981;&#21516;&#22823;&#23567;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#37319;&#29992;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#24182;&#19981;&#26159;&#24456;&#20581;&#22766;&#65292;&#36825;&#34920;&#26126;&#31572;&#26696;&#27573;&#33853;&#19981;&#33021;&#31616;&#21333;&#22320;&#36890;&#36807;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#30830;&#23450;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26356;&#22810;&#30340;&#21442;&#25968;&#21644;&#27861;&#24459;&#30693;&#35782;&#23545;&#27861;&#24459;&#26696;&#20363;&#30340;&#34164;&#28085;&#20219;&#21153;&#26377;&#25152;&#36129;&#29486;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;COLIEE 2023&#27604;&#36187;&#20013;&#33719;&#24471;&#31532;&#19977;&#21517;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#23454;&#29616;&#21487;&#20197;&#22312;https://github.com/CSHaitao/THUIR-COLIEE2023&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the approach of the THUIR team at the COLIEE 2023 Legal Case Entailment task. This task requires the participant to identify a specific paragraph from a given supporting case that entails the decision for the query case. We try traditional lexical matching methods and pre-trained language models with different sizes. Furthermore, learning-to-rank methods are employed to further improve performance. However, learning-to-rank is not very robust on this task. which suggests that answer passages cannot simply be determined with information retrieval techniques. Experimental results show that more parameters and legal knowledge contribute to the legal case entailment task. Finally, we get the third place in COLIEE 2023. The implementation of our method can be found at https://github.com/CSHaitao/THUIR-COLIEE2023.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;THUIR&#22312;COLIEE 2023&#27604;&#36187;&#20013;&#30340;&#20896;&#20891;&#26041;&#26696;&#65292;&#20854;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#34701;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#21551;&#21457;&#24335;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#37319;&#29992;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#21512;&#24182;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20855;&#26377;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06812</link><description>&lt;p&gt;
THUIR@COLIEE 2023: &#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#34701;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
THUIR@COLIEE 2023: Incorporating Structural Knowledge into Pre-trained Language Models for Legal Case Retrieval. (arXiv:2305.06812v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;THUIR&#22312;COLIEE 2023&#27604;&#36187;&#20013;&#30340;&#20896;&#20891;&#26041;&#26696;&#65292;&#20854;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#34701;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#21551;&#21457;&#24335;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#37319;&#29992;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#21512;&#24182;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20855;&#26377;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#25216;&#26415;&#22312;&#29616;&#20195;&#26234;&#33021;&#27861;&#24459;&#31995;&#32479;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#32780;&#20316;&#20026;&#19968;&#39033;&#24180;&#24230;&#30693;&#21517;&#22269;&#38469;&#27604;&#36187;&#65292;COLIEE&#26088;&#22312;&#23454;&#29616;&#38024;&#23545;&#27861;&#24459;&#25991;&#26412;&#30340;&#26368;&#20808;&#36827;&#26816;&#32034;&#27169;&#22411;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#20896;&#20891;&#22242;&#38431;THUIR&#22312;COLIEE 2023&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#32467;&#26500;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#23545;&#27861;&#24459;&#26696;&#20363;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21551;&#21457;&#24335;&#39044;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#26041;&#27861;&#20197;&#20943;&#23569;&#26080;&#20851;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#23558;&#20855;&#26377;&#19981;&#21516;&#32500;&#24230;&#30340;&#29305;&#24449;&#21512;&#24182;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#26696;&#20855;&#26377;&#21331;&#36234;&#30340;&#20248;&#21183;&#65292;&#23448;&#26041;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#36816;&#34892;&#25928;&#26524;&#22312;&#25152;&#26377;&#25552;&#20132;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#21487;&#22312;https://github.com/CSHaitao/THUIR-COLIEE2023&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Legal case retrieval techniques play an essential role in modern intelligent legal systems. As an annually well-known international competition, COLIEE is aiming to achieve the state-of-the-art retrieval model for legal texts. This paper summarizes the approach of the championship team THUIR in COLIEE 2023. To be specific, we design structure-aware pre-trained language models to enhance the understanding of legal cases. Furthermore, we propose heuristic pre-processing and post-processing approaches to reduce the influence of irrelevant messages. In the end, learning-to-rank methods are employed to merge features with different dimensions. Experimental results demonstrate the superiority of our proposal. Official results show that our run has the best performance among all submissions. The implementation of our method can be found at https://github.com/CSHaitao/THUIR-COLIEE2023.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#20041;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#35786;&#30103;&#26415;&#35821;&#20013;&#30340;&#20064;&#35821;&#24615;&#22810;&#35789;&#34920;&#36798;&#24335;&#65292;&#21152;&#24378;&#20102;UMLS&#26412;&#20307;&#21644;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.06801</link><description>&lt;p&gt;
&#22522;&#20110;&#23450;&#20041;&#34920;&#31034;&#23398;&#20064;&#30340;&#35786;&#30103;&#26415;&#35821;&#20013;&#20064;&#35821;&#24615;&#22810;&#35789;&#34920;&#36798;&#24335;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Detecting Idiomatic Multiword Expressions in Clinical Terminology using Definition-Based Representation Learning. (arXiv:2305.06801v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23450;&#20041;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#35786;&#30103;&#26415;&#35821;&#20013;&#30340;&#20064;&#35821;&#24615;&#22810;&#35789;&#34920;&#36798;&#24335;&#65292;&#21152;&#24378;&#20102;UMLS&#26412;&#20307;&#21644;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#23450;&#20041;&#35821;&#20041;&#27169;&#22411;&#22312;&#35786;&#30103;&#26415;&#35821;&#20013;&#26816;&#27979;&#20064;&#35821;&#24615;&#21644;&#21322;&#20064;&#35821;&#24615;&#22810;&#35789;&#34920;&#36798;&#24335;&#65288;MWEs&#65289;&#30340;&#28508;&#21147;&#12290;&#30740;&#31350;&#32858;&#28966;&#20110;UMLS&#26412;&#20307;&#20013;&#23450;&#20041;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#65292;&#24182;&#26088;&#22312;&#24110;&#21161;&#20248;&#20808;&#32763;&#35793;&#36825;&#20123;&#23454;&#20307;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20998;&#29983;&#29289;&#21307;&#23398;MWEs&#30340;&#20064;&#35821;&#24615;&#65292;&#35813;&#24037;&#20855;&#22522;&#20110;&#37027;&#20123;MWEs&#30340;&#35821;&#20041;&#34920;&#31034;&#21644;&#20854;&#32452;&#25104;&#37096;&#20998;&#30340;&#34920;&#31034;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#32463;&#36807;&#35757;&#32451;&#30340;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;BioLORD&#23454;&#29616;&#20102;&#36825;&#19968;&#30446;&#26631;&#12290;&#19982;&#20004;&#20010;&#22522;&#20110;Transformer&#30340;&#26368;&#20808;&#36827;&#30340;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;SapBERT&#21644;CODER&#30456;&#27604;&#65292;&#23450;&#20041;&#34920;&#31034;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#24471;&#21040;&#20102;&#20984;&#26174;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;BioLORD&#27169;&#22411;&#20855;&#26377;&#36739;&#24378;&#30340;&#35782;&#21035;&#20064;&#35821;MWEs&#30340;&#33021;&#21147;&#65292;&#19981;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper shines a light on the potential of definition-based semantic models for detecting idiomatic and semi-idiomatic multiword expressions (MWEs) in clinical terminology. Our study focuses on biomedical entities defined in the UMLS ontology and aims to help prioritize the translation efforts of these entities. In particular, we develop an effective tool for scoring the idiomaticity of biomedical MWEs based on the degree of similarity between the semantic representations of those MWEs and a weighted average of the representation of their constituents. We achieve this using a biomedical language model trained to produce similar representations for entity names and their definitions, called BioLORD. The importance of this definition-based approach is highlighted by comparing the BioLORD model to two other state-of-the-art biomedical language models based on Transformer: SapBERT and CODER. Our results show that the BioLORD model has a strong ability to identify idiomatic MWEs, not rep
&lt;/p&gt;</description></item><item><title>COCKATIEL&#26159;&#19968;&#31181;&#36830;&#32493;&#27010;&#24565;&#25490;&#21517;&#24102;&#24402;&#22240;&#24615;&#35299;&#37322;&#30340;&#25216;&#26415;&#65292;&#22522;&#20110;&#27010;&#24565;&#65292;&#29992;&#20110;&#20174;NLP&#20998;&#31867;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#20013;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#65292;&#19988;&#19981;&#20250;&#24433;&#21709;&#20934;&#30830;&#24615;&#25110;&#38656;&#35201;&#26032;&#27169;&#22411;&#65292;&#24050;&#35777;&#26126;&#27604;&#29616;&#26377;&#26041;&#27861;&#20135;&#29983;&#26356;&#26377;&#20449;&#24687;&#37327;&#21644;&#21487;&#38752;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.06754</link><description>&lt;p&gt;
COCKATIEL:&#29992;&#21487;&#35299;&#37322;&#20803;&#32032;&#23545;NLP&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36827;&#34892;&#36830;&#32493;&#27010;&#24565;&#25490;&#21517;&#24102;&#24402;&#22240;&#24615;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP tasks. (arXiv:2305.06754v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06754
&lt;/p&gt;
&lt;p&gt;
COCKATIEL&#26159;&#19968;&#31181;&#36830;&#32493;&#27010;&#24565;&#25490;&#21517;&#24102;&#24402;&#22240;&#24615;&#35299;&#37322;&#30340;&#25216;&#26415;&#65292;&#22522;&#20110;&#27010;&#24565;&#65292;&#29992;&#20110;&#20174;NLP&#20998;&#31867;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#20013;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#65292;&#19988;&#19981;&#20250;&#24433;&#21709;&#20934;&#30830;&#24615;&#25110;&#38656;&#35201;&#26032;&#27169;&#22411;&#65292;&#24050;&#35777;&#26126;&#27604;&#29616;&#26377;&#26041;&#27861;&#20135;&#29983;&#26356;&#26377;&#20449;&#24687;&#37327;&#21644;&#21487;&#38752;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#32467;&#26500;&#22797;&#26434;&#65292;&#20854;&#22312;NLP&#20013;&#30340;&#20351;&#29992;&#34429;&#28982;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#20294;&#20854;&#21487;&#35299;&#37322;&#24615;&#25110;&#21487;&#35299;&#37322;&#24615;&#36739;&#20026;&#26840;&#25163;&#12290;&#26368;&#36817;&#30340;&#20105;&#35770;&#34920;&#26126;&#65292;&#27880;&#24847;&#21147;&#22270;&#21644;&#24402;&#22240;&#26041;&#27861;&#19981;&#21487;&#38752;&#65292;&#32780;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20171;&#32461;&#20102;&#20854;&#20013;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;COCKATIEL&#36825;&#19968;&#26032;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#65292;&#23427;&#26159;&#19968;&#31181;&#21518;&#26399;&#26041;&#27861;&#65292;&#22522;&#20110;&#27010;&#24565;&#65292;&#29992;&#20110;&#20174;&#32463;&#36807;NLP&#20998;&#31867;&#20219;&#21153;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#20013;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#26469;&#21457;&#29616;&#27169;&#22411;&#21033;&#29992;&#26469;&#36827;&#34892;&#39044;&#27979;&#30340;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#25935;&#24863;&#24615;&#20998;&#26512;&#26469;&#20934;&#30830;&#20272;&#35745;&#27599;&#20010;&#27010;&#24565;&#23545;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#24213;&#23618;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25110;&#38656;&#35201;&#35757;&#32451;&#26032;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21333;&#19968;&#21644;&#22810;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;COCKATIEL&#27604;&#29616;&#26377;&#26041;&#27861;&#20135;&#29983;&#26356;&#26377;&#20449;&#24687;&#37327;&#21644;&#21487;&#38752;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer architectures are complex and their use in NLP, while it has engendered many successes, makes their interpretability or explainability challenging. Recent debates have shown that attention maps and attribution methods are unreliable (Pruthi et al., 2019; Brunner et al., 2019). In this paper, we present some of their limitations and introduce COCKATIEL, which successfully addresses some of them. COCKATIEL is a novel, post-hoc, concept-based, model-agnostic XAI technique that generates meaningful explanations from the last layer of a neural net model trained on an NLP classification task by using Non-Negative Matrix Factorization (NMF) to discover the concepts the model leverages to make predictions and by exploiting a Sensitivity Analysis to estimate accurately the importance of each of these concepts for the model. It does so without compromising the accuracy of the underlying model or requiring a new one to be trained. We conduct experiments in single and multi-aspect sent
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#39318;&#20010;&#29992;&#20110;&#24211;&#23572;&#24503;&#25163;&#35821;&#21644;&#21475;&#35821;&#24211;&#23572;&#24503;&#20043;&#38388;&#24179;&#34892;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#32479;&#35745;&#32763;&#35793;&#25216;&#26415;&#36827;&#34892;&#20102;&#33258;&#21160;&#32763;&#35793;&#65292;&#32467;&#26524;&#20934;&#30830;&#29575;&#20026;53.8%&#12290;</title><link>http://arxiv.org/abs/2305.06747</link><description>&lt;p&gt;
&#24211;&#23572;&#24503;&#25163;&#35821;&#30340;&#31532;&#19968;&#20010;&#24179;&#34892;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
The First Parallel Corpora for Kurdish Sign Language. (arXiv:2305.06747v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06747
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#39318;&#20010;&#29992;&#20110;&#24211;&#23572;&#24503;&#25163;&#35821;&#21644;&#21475;&#35821;&#24211;&#23572;&#24503;&#20043;&#38388;&#24179;&#34892;&#32763;&#35793;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#32479;&#35745;&#32763;&#35793;&#25216;&#26415;&#36827;&#34892;&#20102;&#33258;&#21160;&#32763;&#35793;&#65292;&#32467;&#26524;&#20934;&#30830;&#29575;&#20026;53.8%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24211;&#23572;&#24503;&#25163;&#35821;(KuSL)&#26159;&#24211;&#23572;&#24503;&#32843;&#20154;&#30340;&#33258;&#28982;&#35821;&#35328;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#33258;&#21160;&#32763;&#35793;&#21475;&#22836;&#24211;&#23572;&#24503;&#35821;&#21644;KuSL&#20043;&#38388;&#30340;&#25991;&#26412;&#12290;&#25163;&#35821;&#35821;&#35328;&#30340;&#21457;&#23637;&#36895;&#24230;&#24456;&#24555;&#65292;&#24182;&#19988;&#36981;&#24490;&#19982;&#21475;&#35821;&#35821;&#35328;&#19981;&#21516;&#30340;&#35821;&#27861;&#35268;&#21017;&#12290;&#22240;&#27492;&#65292;&#22312;&#20219;&#20309;&#32763;&#35793;&#20013;&#65292;&#24212;&#32771;&#34385;&#36825;&#20123;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22836;&#20687;&#30340;Kurdish Sorani&#26041;&#35328;&#25991;&#26412;&#33258;&#21160;&#32763;&#35793;&#25104;Kurdish Sign Language&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#35813;&#35821;&#35328;&#23545;&#30340;&#31532;&#19968;&#20010;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#35757;&#32451;&#32479;&#35745;&#26426;&#22120;&#32763;&#35793;(SMT)&#24341;&#25806;&#12290;&#25105;&#20204;&#20351;&#29992;&#21452;&#35821;&#35780;&#20272;&#39564;&#31639;(BLEU)&#27979;&#35797;&#20102;&#36755;&#20986;&#30340;&#21487;&#29702;&#35299;&#24615;&#24182;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;53.8%&#30340;&#20934;&#30830;&#29575;&#12290;&#19982;&#35813;&#39046;&#22495;&#20197;&#21069;&#30340;&#23454;&#39564;&#30456;&#27604;&#65292;&#32467;&#26524;&#30456;&#24403;&#39640;&#12290;&#25105;&#20204;&#24576;&#30097;&#21407;&#22240;&#26159;&#20004;&#20010;&#23545;&#30340;&#32467;&#26500;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#35745;&#21010;&#22312;Kurdish-BLARK&#19978;&#20197;CC BY-NC-SA 4.0&#35768;&#21487;&#35777;&#20844;&#24320;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kurdish Sign Language (KuSL) is the natural language of the Kurdish Deaf people. We work on automatic translation between spoken Kurdish and KuSL. Sign languages evolve rapidly and follow grammatical rules that differ from spoken languages. Consequently,those differences should be considered during any translation. We proposed an avatar-based automatic translation of Kurdish texts in the Sorani (Central Kurdish) dialect into the Kurdish Sign language. We developed the first parallel corpora for that pair that we use to train a Statistical Machine Translation (SMT) engine. We tested the outcome understandability and evaluated it using the Bilingual Evaluation Understudy (BLEU). Results showed 53.8% accuracy. Compared to the previous experiments in the field, the result is considerably high. We suspect the reason to be the similarity between the structure of the two pairs. We plan to make the resources publicly available under CC BY-NC-SA 4.0 license on the Kurdish-BLARK (https://kurdish
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110; Transformer &#30340; Albertina PT-* &#27169;&#22411;&#36827;&#34892;&#20102;&#33889;&#33796;&#29273;&#35821;&#30340;&#31070;&#32463;&#32534;&#30721;&#65292;&#21019;&#26032;&#24615;&#22320;&#25552;&#21319;&#20102;&#35813;&#35821;&#35328;&#22312;&#25968;&#23383;&#26102;&#20195;&#30340;&#25216;&#26415;&#20934;&#22791;&#27700;&#24179;&#65292;&#23588;&#20854;&#26159;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#21644;&#24052;&#35199;&#30340;&#32654;&#27954;&#33889;&#33796;&#29273;&#35821;&#20004;&#20010;&#21464;&#31181;&#12290;</title><link>http://arxiv.org/abs/2305.06721</link><description>&lt;p&gt;
&#22522;&#20110; Transformer Albertina PT-* &#25552;&#21319;&#33889;&#33796;&#29273;&#35821;&#30340;&#31070;&#32463;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Advancing Neural Encoding of Portuguese with Transformer Albertina PT-*. (arXiv:2305.06721v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110; Transformer &#30340; Albertina PT-* &#27169;&#22411;&#36827;&#34892;&#20102;&#33889;&#33796;&#29273;&#35821;&#30340;&#31070;&#32463;&#32534;&#30721;&#65292;&#21019;&#26032;&#24615;&#22320;&#25552;&#21319;&#20102;&#35813;&#35821;&#35328;&#22312;&#25968;&#23383;&#26102;&#20195;&#30340;&#25216;&#26415;&#20934;&#22791;&#27700;&#24179;&#65292;&#23588;&#20854;&#26159;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#21644;&#24052;&#35199;&#30340;&#32654;&#27954;&#33889;&#33796;&#29273;&#35821;&#20004;&#20010;&#21464;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25512;&#36827;&#33889;&#33796;&#29273;&#35821;&#65288;PT&#65289;&#30340;&#31070;&#32463;&#32534;&#30721;&#65292;&#20026;&#35813;&#35821;&#35328;&#22312;&#25968;&#23383;&#26102;&#20195;&#30340;&#25216;&#26415;&#20934;&#22791;&#25171;&#19979;&#22522;&#30784;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110; Transformer &#30340; Albertina PT-* &#22522;&#30784;&#27169;&#22411;&#65292;&#20026;&#20854;&#20004;&#20010;&#21464;&#31181;&#65288;&#33889;&#33796;&#29273;&#30340;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#65288;PT-PT&#65289;&#21644;&#24052;&#35199;&#30340;&#32654;&#27954;&#33889;&#33796;&#29273;&#35821;&#65288;PT-BR&#65289;&#65289;&#30340;&#31070;&#32463;&#32534;&#30721;&#21019;&#19979;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#24378;&#22823;&#30340;&#27169;&#22411;&#20316;&#20026;&#36215;&#28857;&#65292;&#21363;DeBERTa&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#25910;&#38598;&#30340;PT-PT&#25968;&#25454;&#38598;&#21644;brWaC&#35821;&#26009;&#24211;&#23545;&#20854;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#36866;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#30340;&#33879;&#21517;&#19979;&#28216;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#26469;&#35780;&#20272;Albertina&#21644;&#31454;&#20105;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290; Albertina PT-PT&#21644;PT-BR&#29256;&#26412;&#22343;&#21487;&#20813;&#36153;&#20998;&#21457;&#65292;&#24182;&#22312;&#26368;&#23485;&#26494;&#30340;&#35768;&#21487;&#20197;&#19979;&#36816;&#34892;&#20110;&#28040;&#36153;&#32423;&#30828;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
To advance the neural encoding of Portuguese (PT), and a fortiori the technological preparation of this language for the digital age, we developed a Transformer-based foundation model that sets a new state of the art in this respect for two of its variants, namely European Portuguese from Portugal (PT-PT) and American Portuguese from Brazil (PT-BR).  To develop this encoder, which we named Albertina PT-*, a strong model was used as a starting point, DeBERTa, and its pre-training was done over data sets of Portuguese, namely over a data set we gathered for PT-PT and over the brWaC corpus for PT-BR. The performance of Albertina and competing models was assessed by evaluating them on prominent downstream language processing tasks adapted for Portuguese.  Both Albertina PT-PT and PT-BR versions are distributed free of charge and under the most permissive license possible and can be run on consumer-grade hardware, thus seeking to contribute to the advancement of research and innovation in l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38754;&#21521;&#22522;&#20110;&#36328;&#24230;&#30340;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#30340;&#25104;&#26412;&#25928;&#30410;&#20247;&#21253;&#31639;&#27861;&#65292;&#20351;&#29992;&#20102;&#32452;&#21512;&#22810;&#33218;&#32769;&#34382;&#26426;&#26041;&#27861;&#36827;&#34892;&#24037;&#20154;&#36873;&#25321;&#65292;&#24182;&#29992;&#31227;&#20301;&#12289;&#25193;&#23637;&#21644;&#25910;&#32553;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27880;&#37322;&#36136;&#37327;&#21644;&#38477;&#20302;&#25104;&#26412;&#65292;F1&#24471;&#20998;&#30456;&#23545;&#20110;&#20165;&#19987;&#23478;&#30340;&#22522;&#32447;&#25552;&#39640;&#20102;100.04&#65285;&#65292;&#25104;&#26412;&#33410;&#32422;&#39640;&#36798;65.97&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.06683</link><description>&lt;p&gt;
&#38754;&#21521;&#22522;&#20110;&#36328;&#24230;&#30340;&#24207;&#21015;&#26631;&#27880;&#30340;&#25104;&#26412;&#25928;&#30410;&#20247;&#21253;&#65306;&#24037;&#20154;&#36873;&#25321;&#21644;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Cost-efficient Crowdsourcing for Span-based Sequence Labeling: Worker Selection and Data Augmentation. (arXiv:2305.06683v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38754;&#21521;&#22522;&#20110;&#36328;&#24230;&#30340;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#30340;&#25104;&#26412;&#25928;&#30410;&#20247;&#21253;&#31639;&#27861;&#65292;&#20351;&#29992;&#20102;&#32452;&#21512;&#22810;&#33218;&#32769;&#34382;&#26426;&#26041;&#27861;&#36827;&#34892;&#24037;&#20154;&#36873;&#25321;&#65292;&#24182;&#29992;&#31227;&#20301;&#12289;&#25193;&#23637;&#21644;&#25910;&#32553;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27880;&#37322;&#36136;&#37327;&#21644;&#38477;&#20302;&#25104;&#26412;&#65292;F1&#24471;&#20998;&#30456;&#23545;&#20110;&#20165;&#19987;&#23478;&#30340;&#22522;&#32447;&#25552;&#39640;&#20102;100.04&#65285;&#65292;&#25104;&#26412;&#33410;&#32422;&#39640;&#36798;65.97&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20154;&#36873;&#25321;&#31639;&#27861;&#65292;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20110;&#36328;&#24230;&#30340;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#30340;&#27880;&#37322;&#36136;&#37327;&#24182;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;&#19982;&#20197;&#21069;&#38024;&#23545;&#31616;&#21333;&#20219;&#21153;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#26412;&#30740;&#31350;&#28041;&#21450;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#20013;&#30340;&#26631;&#31614;&#30456;&#20114;&#20381;&#36182;&#24615;&#22797;&#26434;&#24615;&#12290;&#25152;&#25552;&#35758;&#30340;&#31639;&#27861;&#20351;&#29992;&#32452;&#21512;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;CMAB&#65289;&#26041;&#27861;&#36827;&#34892;&#24037;&#20154;&#36873;&#25321;&#12290;&#35299;&#20915;&#20102;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#25361;&#25112;&#65292;&#35813;&#25361;&#25112;&#38459;&#30861;&#20102;&#24037;&#20154;&#36873;&#25321;&#30340;&#31163;&#32447;&#27169;&#25311;&#65292;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;&#31227;&#20301;&#12289;&#25193;&#23637;&#21644;&#25910;&#32553;&#65288;SES&#65289;&#30340;&#21019;&#26032;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;SES&#26041;&#27861;&#19987;&#38376;&#20026;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#35774;&#35745;&#12290;&#22312;CoNLL 2003 NER&#21644;&#20013;&#25991;OEI&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20005;&#26684;&#27979;&#35797;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#25928;&#29575;&#65292;F1&#24471;&#20998;&#30456;&#23545;&#20110;&#20165;&#19987;&#23478;&#30340;&#22522;&#32447;&#25552;&#39640;&#20102;100.04&#65285;&#65292;&#25104;&#26412;&#33410;&#32422;&#39640;&#36798;65.97&#65285;&#12290;&#26412;&#25991;&#36824;&#21253;&#25324;&#19968;&#20010;&#29420;&#31435;&#20110;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel worker selection algorithm, enhancing annotation quality and reducing costs in challenging span-based sequence labeling tasks in Natural Language Processing (NLP). Unlike previous studies targeting simpler tasks, this study contends with the complexities of label interdependencies in sequence labeling tasks. The proposed algorithm utilizes a Combinatorial Multi-Armed Bandit (CMAB) approach for worker selection. The challenge of dealing with imbalanced and small-scale datasets, which hinders offline simulation of worker selection, is tackled using an innovative data augmentation method termed shifting, expanding, and shrinking (SES). The SES method is designed specifically for sequence labeling tasks. Rigorous testing on CoNLL 2003 NER and Chinese OEI datasets showcased the algorithm's efficiency, with an increase in F1 score up to 100.04% of the expert-only baseline, alongside cost savings up to 65.97%. The paper also encompasses a dataset-independent test
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23376;&#38598;&#26469;&#39640;&#25928;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.06677</link><description>&lt;p&gt;
INGENIOUS&#65306;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23376;&#38598;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39640;&#25928;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models. (arXiv:2305.06677v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#23376;&#38598;&#26469;&#39640;&#25928;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20943;&#23569;&#20102;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26174;&#30528;&#29305;&#28857;&#26159;&#22312;&#20854;&#27867;&#21270;&#33021;&#21147;&#21644;&#26032;&#33021;&#21147;&#26041;&#38754;&#38543;&#30528;&#27169;&#22411;&#23481;&#37327;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#22686;&#21152;&#32780; achieved. &#28982;&#32780;&#65292;&#24517;&#39035;&#35748;&#35782;&#21040;&#36825;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20102;&#36807;&#38271;&#30340;&#35757;&#32451;&#26102;&#38388;&#12289;&#36807;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#26377;&#23475;&#30340;&#29615;&#22659;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#26159;&#21542;&#21487;&#33021;&#20165;&#20351;&#29992;&#39640;&#24230;&#20449;&#24687;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#26469;&#35757;&#32451; PTLM&#65292;&#24182;&#21516;&#26102;&#20445;&#25345;&#20854;&#19979;&#28216;&#24615;&#33021;&#65311;
&lt;/p&gt;
&lt;p&gt;
A salient characteristic of large pre-trained language models (PTLMs) is a remarkable improvement in their generalization capability and emergence of new capabilities with increasing model capacity and pre-training dataset size. Consequently, we are witnessing the development of enormous models pushing the state-of-the-art. It is, however, imperative to realize that this inevitably leads to prohibitively long training times, extortionate computing costs, and a detrimental environmental impact. Significant efforts are underway to make PTLM training more efficient through innovations in model architectures, training pipelines, and loss function design, with scant attention being paid to optimizing the utility of training data. The key question that we ask is whether it is possible to train PTLMs by employing only highly informative subsets of the training data while maintaining downstream performance? Building upon the recent progress in informative data subset selection, we show how we 
&lt;/p&gt;</description></item><item><title>QURG&#26159;&#19968;&#31181;&#24110;&#21161;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#27169;&#22411;&#23454;&#29616;&#19978;&#19979;&#25991;&#29702;&#35299;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#33021;&#22312;SParC&#21644;CoSQL&#31561;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38590;&#20197;&#22788;&#29702;&#21644;&#38271;&#36718;&#27425;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06655</link><description>&lt;p&gt;
QURG: &#38382;&#39064;&#37325;&#20889;&#24341;&#23548;&#19979;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
QURG: Question Rewriting Guided Context-Dependent Text-to-SQL Semantic Parsing. (arXiv:2305.06655v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06655
&lt;/p&gt;
&lt;p&gt;
QURG&#26159;&#19968;&#31181;&#24110;&#21161;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#27169;&#22411;&#23454;&#29616;&#19978;&#19979;&#25991;&#29702;&#35299;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#33021;&#22312;SParC&#21644;CoSQL&#31561;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38590;&#20197;&#22788;&#29702;&#21644;&#38271;&#36718;&#27425;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#25991;&#26412;&#21040;SQL&#30340;&#30446;&#26631;&#26159;&#23558;&#22810;&#36718;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#32763;&#35793;&#25104;SQL&#26597;&#35810;&#35821;&#21477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QURG&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38382;&#39064;&#37325;&#20889;&#24341;&#23548;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#23454;&#29616;&#36275;&#22815;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#38382;&#39064;&#37325;&#20889;&#27169;&#22411;&#65292;&#22312;&#38382;&#39064;&#19978;&#19979;&#25991;&#30340;&#22522;&#30784;&#19978;&#23436;&#25104;&#24403;&#21069;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#37325;&#20889;&#32534;&#36753;&#30697;&#38453;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#27969;&#30697;&#38453;&#32534;&#30721;&#22120;&#65292;&#26469;&#20849;&#21516;&#24314;&#27169;&#38382;&#39064;&#21644;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#37325;&#20889;&#20851;&#31995;&#65292;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#21644;&#32467;&#26500;&#21270;&#27169;&#24335;&#20043;&#38388;&#30340;&#27169;&#24335;&#38142;&#25509;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;QURG&#26174;&#33879;&#25552;&#39640;&#20102;&#20004;&#20010;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#25968;&#25454;&#38598;SParC&#21644;CoSQL&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38590;&#20197;&#22788;&#29702;&#21644;&#38271;&#36718;&#27425;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context-dependent Text-to-SQL aims to translate multi-turn natural language questions into SQL queries. Despite various methods have exploited context-dependence information implicitly for contextual SQL parsing, there are few attempts to explicitly address the dependencies between current question and question context. This paper presents QURG, a novel Question Rewriting Guided approach to help the models achieve adequate contextual understanding. Specifically, we first train a question rewriting model to complete the current question based on question context, and convert them into a rewriting edit matrix. We further design a two-stream matrix encoder to jointly model the rewriting relations between question and context, and the schema linking relations between natural language and structured schema. Experimental results show that QURG significantly improves the performances on two large-scale context-dependent datasets SParC and CoSQL, especially for hard and long-turn questions.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30701;&#35821;&#32423;&#22797;&#21046;&#26426;&#21046;-PROM&#65292;&#21487;&#20197;&#22686;&#24378;&#23545;n-gram&#30340;&#27880;&#24847;&#21147;&#65292;&#29992;&#20110;&#20855;&#26377;&#39044;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#36136;&#37327;&#21644;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.06647</link><description>&lt;p&gt;
PROM&#65306;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#30701;&#35821;&#32423;&#22797;&#21046;&#26426;&#21046;&#65292;&#29992;&#20110;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
PROM: A Phrase-level Copying Mechanism with Pre-training for Abstractive Summarization. (arXiv:2305.06647v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06647
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30701;&#35821;&#32423;&#22797;&#21046;&#26426;&#21046;-PROM&#65292;&#21487;&#20197;&#22686;&#24378;&#23545;n-gram&#30340;&#27880;&#24847;&#21147;&#65292;&#29992;&#20110;&#20855;&#26377;&#39044;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#36136;&#37327;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#25688;&#35201;&#29983;&#25104;&#26041;&#38754;&#30340;&#26174;&#33879;&#25104;&#23601;&#65292;&#22797;&#21046;&#26426;&#21046;&#36890;&#36807;&#25552;&#39640;&#20107;&#23454;&#24615;&#12289;&#31283;&#23450;&#24615;&#21644;&#25972;&#20307;&#24615;&#33021;&#26041;&#38754;&#30340;&#25913;&#36827;&#35777;&#26126;&#20854;&#26377;&#21161;&#20110;&#36825;&#19968;&#36827;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#30701;&#35821;&#32423;&#22797;&#21046;&#26426;&#21046;-PROM&#65292;&#23427;&#22686;&#24378;&#20102;&#23545;n-gram&#30340;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20855;&#26377;&#39044;&#35757;&#32451;&#30340;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#20013;&#12290;PROM&#28155;&#21152;&#20102;&#19968;&#20010;&#25351;&#31034;&#22120;&#23618;&#65292;&#20197;&#26126;&#30830;&#20174;&#28304;&#20013;&#21487;&#20197;&#22797;&#21046;&#30340;n-gram&#20013;&#30340;&#20196;&#29260;&#65292;&#24182;&#35745;&#31639;&#22797;&#21046;&#39044;&#27979;&#30340;&#36741;&#21161;&#25439;&#22833;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22522;&#20934;&#24494;&#35843;&#20013;&#65292;PROM&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#65292;PROM&#29992;&#20110;&#23545;&#21407;&#22987;&#35821;&#26009;&#24211;&#36827;&#34892;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#26032;&#30340;&#36890;&#29992;&#22522;&#32447;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;PROM&#25191;&#34892;&#26356;&#21512;&#29702;&#30340;&#22797;&#21046;&#24182;&#26377;&#21161;&#20110;&#20445;&#25345;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Based on the remarkable achievements of pre-trained language models in abstractive summarization, the copying mechanism has proved helpful by improving the factuality, stability, and overall performance. This work proposes PROM, a new PhRase-level cOpying Mechanism that enhances attention on n-grams, which can be applied to zero-shot summarization with pre-training. PROM adds an indicator layer to explicitly pick up tokens in n-gram that can be copied from the source, and calculates an auxiliary loss for the copying prediction. Empirical studies show that PROM makes significant improvements in fine-tuning on benchmarks. In zero-shot setting, PROM is utilized in the self-supervised pre-training on raw corpora and provides new general baselines on a wide range of summarization datasets. Further analysis shows that PROM performs more reasonable copying and contributes to faithfulness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#21333;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30446;&#26631;&#32676;&#20307;&#30340;&#39044;&#27979;&#65292;&#27169;&#25311;&#20102;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#22312;&#32447;&#24847;&#35265;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06626</link><description>&lt;p&gt;
&#24403;&#22810;&#25968;&#20154;&#26159;&#38169;&#35823;&#30340;&#65306;&#21033;&#29992;&#26631;&#27880;&#32773;&#19981;&#19968;&#33268;&#24615;&#36827;&#34892;&#20027;&#35266;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
When the Majority is Wrong: Leveraging Annotator Disagreement for Subjective Tasks. (arXiv:2305.06626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#21333;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30446;&#26631;&#32676;&#20307;&#30340;&#39044;&#27979;&#65292;&#27169;&#25311;&#20102;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#65292;&#36890;&#36807;&#20351;&#29992;&#20182;&#20204;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#21644;&#22312;&#32447;&#24847;&#35265;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#34429;&#28982;&#36890;&#24120;&#20351;&#29992;&#26631;&#27880;&#32773;&#30340;&#22810;&#25968;&#25237;&#31080;&#26469;&#30830;&#23450;&#26631;&#31614;&#65292;&#20294;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31561;&#20027;&#35266;&#20219;&#21153;&#20013;&#65292;&#26631;&#27880;&#32773;&#20043;&#38388;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#21487;&#33021;&#21453;&#26144;&#20986;&#32676;&#20307;&#35266;&#28857;&#30340;&#24046;&#24322;&#65292;&#32780;&#19981;&#26159;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#19968;&#20010;&#35821;&#21477;&#26159;&#21542;&#20882;&#29359;&#20102;&#23427;&#25152;&#38024;&#23545;&#30340;&#20154;&#32676;&#65292;&#32780;&#36825;&#21487;&#33021;&#21482;&#21344;&#26631;&#27880;&#32773;&#27744;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#39044;&#27979;&#21487;&#33021;&#20855;&#26377;&#20882;&#29359;&#24615;&#25991;&#26412;&#19978;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#65292;&#24182;&#32467;&#21512;&#25991;&#26412;&#30340;&#39044;&#27979;&#30446;&#26631;&#32676;&#20307;&#26469;&#27169;&#25311;&#30446;&#26631;&#32676;&#20307;&#25104;&#21592;&#30340;&#24847;&#35265;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31995;&#21015;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#21253;&#25324;&#25552;&#39640;&#20102;22&#65285;&#22312;&#39044;&#27979;&#27599;&#20010;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#19978;&#30340;&#24615;&#33021;&#65292;&#25552;&#39640;&#20102;33&#65285;&#22312;&#39044;&#27979;&#26631;&#27880;&#32773;&#20043;&#38388;&#26041;&#24046;&#19978;&#30340;&#24615;&#33021;&#65292;&#36825;&#25552;&#20379;&#20102;&#19979;&#28216;&#29992;&#26469;&#34913;&#37327;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21487;&#20197;&#20351;&#29992;&#26631;&#27880;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#21644;&#20854;&#22312;&#32447;&#24847;&#35265;&#26469;&#39044;&#27979;&#26631;&#27880;&#32773;&#30340;&#25171;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Though majority vote among annotators is typically used for ground truth labels in natural language processing, annotator disagreement in tasks such as hate speech detection may reflect differences among group opinions, not noise. Thus, a crucial problem in hate speech detection is whether a statement is offensive to the demographic group that it targets, which may constitute a small fraction of the annotator pool. We construct a model that predicts individual annotator ratings on potentially offensive text and combines this information with the predicted target group of the text to model the opinions of target group members. We show gains across a range of metrics, including raising performance over the baseline by 22% at predicting individual annotators' ratings and 33% at predicting variance among annotators, which provides a method of measuring model uncertainty downstream. We find that annotators' ratings can be predicted using their demographic information and opinions on online 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31867;&#27604;&#20851;&#31995;&#30340;&#36830;&#32493;&#25277;&#21462;&#27169;&#22411;&#65292;&#36890;&#36807;&#35774;&#35745;&#35760;&#24518;&#26080;&#20851;&#30340;&#20851;&#31995;&#21407;&#22411;&#21644;&#35760;&#24518;&#22686;&#24378;&#20197;&#20811;&#26381;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36827;&#32780;&#24341;&#20837;&#32508;&#21512;&#35757;&#32451;&#21644;&#28966;&#28857;&#30693;&#35782;&#33976;&#39311;&#20197;&#22686;&#24378;&#22312;&#31867;&#27604;&#20851;&#31995;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06620</link><description>&lt;p&gt;
&#21306;&#20998;&#31867;&#27604;&#35821;&#20041;&#20197;&#25552;&#21319;&#36830;&#32493;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Improving Continual Relation Extraction by Distinguishing Analogous Semantics. (arXiv:2305.06620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31867;&#27604;&#20851;&#31995;&#30340;&#36830;&#32493;&#25277;&#21462;&#27169;&#22411;&#65292;&#36890;&#36807;&#35774;&#35745;&#35760;&#24518;&#26080;&#20851;&#30340;&#20851;&#31995;&#21407;&#22411;&#21644;&#35760;&#24518;&#22686;&#24378;&#20197;&#20811;&#26381;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36827;&#32780;&#24341;&#20837;&#32508;&#21512;&#35757;&#32451;&#21644;&#28966;&#28857;&#30693;&#35782;&#33976;&#39311;&#20197;&#22686;&#24378;&#22312;&#31867;&#27604;&#20851;&#31995;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#20851;&#31995;&#25277;&#21462;&#26088;&#22312;&#23398;&#20064;&#19981;&#26029;&#20986;&#29616;&#30340;&#20851;&#31995;&#65292;&#21516;&#26102;&#36991;&#20813;&#36951;&#24536;&#24050;&#23398;&#20064;&#30340;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#23384;&#20648;&#23569;&#37327;&#20856;&#22411;&#26679;&#26412;&#29992;&#20110;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#20197;&#32531;&#35299;&#36951;&#24536;&#12290;&#28982;&#32780;&#65292;&#21453;&#22797;&#37325;&#25918;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#23548;&#33268;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#26412;&#25991;&#23545;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#31867;&#27604;&#20851;&#31995;&#20005;&#37325;&#24433;&#21709;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#31867;&#27604;&#20851;&#31995;&#30340;&#26032;&#22411;&#36830;&#32493;&#25277;&#21462;&#27169;&#22411;&#65292;&#20855;&#26377;&#35760;&#24518;&#26080;&#20851;&#30340;&#20851;&#31995;&#21407;&#22411;&#21644;&#35760;&#24518;&#22686;&#24378;&#31561;&#35774;&#35745;&#65292;&#20197;&#20811;&#26381;&#36807;&#25311;&#21512;&#38382;&#39064;&#65307;&#21516;&#26102;&#65292;&#24341;&#20837;&#32508;&#21512;&#35757;&#32451;&#21644;&#28966;&#28857;&#30693;&#35782;&#33976;&#39311;&#26469;&#22686;&#24378;&#22312;&#31867;&#27604;&#20851;&#31995;&#19978;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#21331;&#36234;&#65292;&#19988;&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;&#31867;&#27604;&#20851;&#31995;&#21644;&#20811;&#26381;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual relation extraction (RE) aims to learn constantly emerging relations while avoiding forgetting the learned relations. Existing works store a small number of typical samples to re-train the model for alleviating forgetting. However, repeatedly replaying these samples may cause the overfitting problem. We conduct an empirical study on existing works and observe that their performance is severely affected by analogous relations. To address this issue, we propose a novel continual extraction model for analogous relations. Specifically, we design memory-insensitive relation prototypes and memory augmentation to overcome the overfitting problem. We also introduce integrated training and focal knowledge distillation to enhance the performance on analogous relations. Experimental results show the superiority of our model and demonstrate its effectiveness in distinguishing analogous relations and overcoming overfitting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; SCKD &#27169;&#22411;&#65292;&#20351;&#29992;&#24207;&#21015;&#30693;&#35782;&#33976;&#39311;&#19982;&#23545;&#27604;&#23398;&#20064;&#65292;&#23454;&#29616;&#36830;&#32493;&#30340;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#26087;&#20851;&#31995;&#36951;&#24536;&#21644;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06616</link><description>&lt;p&gt;
&#24207;&#21015;&#23545;&#27604;&#30693;&#35782;&#33976;&#39311;&#65292;&#29992;&#20110;&#36830;&#32493;&#30340;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Serial Contrastive Knowledge Distillation for Continual Few-shot Relation Extraction. (arXiv:2305.06616v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; SCKD &#27169;&#22411;&#65292;&#20351;&#29992;&#24207;&#21015;&#30693;&#35782;&#33976;&#39311;&#19982;&#23545;&#27604;&#23398;&#20064;&#65292;&#23454;&#29616;&#36830;&#32493;&#30340;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#26087;&#20851;&#31995;&#36951;&#24536;&#21644;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#30340;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26088;&#22312;&#36890;&#36807;&#23569;&#37327;&#26631;&#27880;&#30340;&#35757;&#32451;&#26679;&#26412;&#19981;&#26029;&#22320;&#20026;&#26032;&#20851;&#31995;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#20013;&#26368;&#22823;&#30340;&#25361;&#25112;&#26159;&#26087;&#20851;&#31995;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#30001;&#25968;&#25454;&#31232;&#30095;&#24615;&#36896;&#25104;&#30340;&#36807;&#24230;&#25311;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;SCKD&#65292;&#26469;&#23436;&#25104;&#36830;&#32493;&#30340;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#24207;&#21015;&#30693;&#35782;&#33976;&#39311;&#26469;&#20445;&#23384;&#20197;&#21069;&#27169;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#20266;&#26679;&#26412;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#20445;&#25345;&#19981;&#21516;&#20851;&#31995;&#26679;&#26412;&#30340;&#34920;&#31034;&#36275;&#22815;&#21487;&#21306;&#20998;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;SCKD&#22312;&#36830;&#32493;&#30340;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#20854;&#22312;&#30693;&#35782;&#36716;&#31227;&#21644;&#20869;&#23384;&#21033;&#29992;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual few-shot relation extraction (RE) aims to continuously train a model for new relations with few labeled training data, of which the major challenges are the catastrophic forgetting of old relations and the overfitting caused by data sparsity. In this paper, we propose a new model, namely SCKD, to accomplish the continual few-shot RE task. Specifically, we design serial knowledge distillation to preserve the prior knowledge from previous models and conduct contrastive learning with pseudo samples to keep the representations of samples in different relations sufficiently distinguishable. Our experiments on two benchmark datasets validate the effectiveness of SCKD for continual few-shot RE and its superiority in knowledge transfer and memory utilization over state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#25991;&#26412;&#20013;&#30340;&#33258;&#30456;&#20851;&#34928;&#20943;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#29992;&#38480;&#21046;&#23494;&#20999;&#30456;&#20851;&#65292;&#29983;&#25104;&#25991;&#26412;&#30340;&#33258;&#30456;&#20851;&#34928;&#20943;&#19982;&#25991;&#23398;&#25991;&#26412;&#30340;&#19981;&#21516;&#65292;&#34920;&#29616;&#20986;&#39532;&#23572;&#31185;&#22827;&#34892;&#20026;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#22312;&#22788;&#29702;&#38271;&#25991;&#26412;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06615</link><description>&lt;p&gt;
&#25991;&#26412;&#20013;&#30340;&#33258;&#30456;&#20851;&#34928;&#20943;&#21450;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#29992;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Autocorrelations Decay in Texts and Applicability Limits of Language Models. (arXiv:2305.06615v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06615
&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20013;&#30340;&#33258;&#30456;&#20851;&#34928;&#20943;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#29992;&#38480;&#21046;&#23494;&#20999;&#30456;&#20851;&#65292;&#29983;&#25104;&#25991;&#26412;&#30340;&#33258;&#30456;&#20851;&#34928;&#20943;&#19982;&#25991;&#23398;&#25991;&#26412;&#30340;&#19981;&#21516;&#65292;&#34920;&#29616;&#20986;&#39532;&#23572;&#31185;&#22827;&#34892;&#20026;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#23384;&#22312;&#22312;&#22788;&#29702;&#38271;&#25991;&#26412;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#25991;&#26412;&#20013;&#33258;&#30456;&#20851;&#34928;&#20943;&#23450;&#24459;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#36866;&#29992;&#38480;&#21046;&#23494;&#20999;&#30456;&#20851;&#12290;&#20351;&#29992;&#20998;&#24067;&#35821;&#20041;&#23398;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#25991;&#26412;&#20013;&#21333;&#35789;&#30340;&#33258;&#30456;&#20851;&#24615;&#25353;&#29031;&#24130;&#24459;&#34928;&#20943;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20998;&#24067;&#35821;&#20041;&#23398;&#20026;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#19968;&#33268;&#30340;&#33258;&#30456;&#20851;&#34928;&#20943;&#25351;&#25968;&#12290;&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#33258;&#30456;&#20851;&#34928;&#20943;&#22312;&#25968;&#37327;&#19978;&#21644;&#36136;&#37327;&#19978;&#36890;&#24120;&#19982;&#25991;&#23398;&#25991;&#26412;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#34920;&#29616;&#20986;&#39532;&#23572;&#31185;&#22827;&#34892;&#20026;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#22411;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#20998;&#26512;&#25110;&#29983;&#25104;&#38271;&#25991;&#26412;&#26102;&#21487;&#33021;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that the laws of autocorrelations decay in texts are closely related to applicability limits of language models. Using distributional semantics we empirically demonstrate that autocorrelations of words in texts decay according to a power law. We show that distributional semantics provides coherent autocorrelations decay exponents for texts translated to multiple languages. The autocorrelations decay in generated texts is quantitatively and often qualitatively different from the literary texts. We conclude that language models exhibiting Markov behavior, including large autoregressive language models, may have limitations when applied to long texts, whether analysis or generation.
&lt;/p&gt;</description></item><item><title>BanglaBook &#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23391;&#21152;&#25289;&#35821;&#20070;&#35780;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324; 158,065 &#20010;&#26679;&#26412;&#65292;&#38024;&#23545;&#24773;&#24863;&#20998;&#26512;&#20998;&#20026;&#19977;&#20010;&#22823;&#31867;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#21462;&#20195;&#25163;&#21160;&#26500;&#24314;&#29305;&#24449;&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#26174;&#30528;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06595</link><description>&lt;p&gt;
BanglaBook: &#19968;&#31181;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#22823;&#35268;&#27169;&#23391;&#21152;&#25289;&#35821;&#20070;&#35780;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BanglaBook: A Large-scale Bangla Dataset for Sentiment Analysis from Book Reviews. (arXiv:2305.06595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06595
&lt;/p&gt;
&lt;p&gt;
BanglaBook &#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23391;&#21152;&#25289;&#35821;&#20070;&#35780;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324; 158,065 &#20010;&#26679;&#26412;&#65292;&#38024;&#23545;&#24773;&#24863;&#20998;&#26512;&#20998;&#20026;&#19977;&#20010;&#22823;&#31867;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#21462;&#20195;&#25163;&#21160;&#26500;&#24314;&#29305;&#24449;&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#26174;&#30528;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28040;&#36153;&#32773;&#24773;&#24863;&#20998;&#26512;&#21487;&#20197;&#36890;&#36807;&#35780;&#35770;&#34920;&#36798;&#25552;&#20379;&#26377;&#20851;&#20135;&#21697;&#36136;&#37327;&#30340;&#20016;&#23500;&#35265;&#35299;&#12290;&#23613;&#31649;&#24773;&#24863;&#20998;&#26512;&#30340;&#30740;&#31350;&#22312;&#35768;&#22810;&#27969;&#34892;&#35821;&#35328;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#25506;&#32034;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#30456;&#20851;&#25968;&#25454;&#21644;&#36328;&#39046;&#22495;&#36866;&#24212;&#24615;&#65292;&#30456;&#23545;&#36739;&#23569;&#20851;&#27880;&#23391;&#21152;&#25289;&#35821;&#35328;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; BanglaBook&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#23391;&#21152;&#25289;&#35821;&#20070;&#35780;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324; 158,065 &#20010;&#26679;&#26412;&#65292;&#20998;&#20026;&#19977;&#20010;&#22823;&#31867;&#65306;&#31215;&#26497;&#12289;&#28040;&#26497;&#21644;&#20013;&#24615;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#24182;&#20351;&#29992;&#19968;&#31995;&#21015;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24314;&#31435;&#20102;&#22522;&#32447;&#65292;&#21253;&#25324; SVM&#12289;LSTM &#21644; Bangla-BERT&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20381;&#36182;&#25163;&#21160;&#26500;&#24314;&#29305;&#24449;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#20855;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#24378;&#35843;&#20102;&#22312;&#27492;&#39046;&#22495;&#38656;&#35201;&#39069;&#22806;&#30340;&#22521;&#35757;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;&#24773;&#24863;&#38169;&#35823;&#20998;&#31867;&#26469;&#36827;&#34892;&#28145;&#20837;&#30340;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#26377;&#20851;&#23391;&#21152;&#25289;&#24773;&#24863;&#20998;&#26512;&#24615;&#36136;&#30340;&#36827;&#19968;&#27493;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of consumer sentiment, as expressed through reviews, can provide a wealth of insight regarding the quality of a product. While the study of sentiment analysis has been widely explored in many popular languages, relatively less attention has been given to the Bangla language, mostly due to a lack of relevant data and cross-domain adaptability. To address this limitation, we present BanglaBook, a large-scale dataset of Bangla book reviews consisting of 158,065 samples classified into three broad categories: positive, negative, and neutral. We provide a detailed statistical analysis of the dataset and employ a range of machine learning models to establish baselines including SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial performance advantage of pre-trained models over models that rely on manually crafted features, emphasizing the necessity for additional training resources in this domain. Additionally, we conduct an in-depth error analysis by examining se
&lt;/p&gt;</description></item><item><title>FactKG&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#65292;&#21253;&#21547;108k&#20010;&#33258;&#28982;&#35821;&#35328;&#22768;&#26126;&#21644;&#20116;&#31181;&#25512;&#29702;&#31867;&#22411;&#65292;&#21487;&#24110;&#21161;&#31038;&#21306;&#26356;&#22909;&#22320;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.06590</link><description>&lt;p&gt;
FactKG: &#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
FactKG: Fact Verification via Reasoning on Knowledge Graphs. (arXiv:2305.06590v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06590
&lt;/p&gt;
&lt;p&gt;
FactKG&#26159;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#65292;&#21253;&#21547;108k&#20010;&#33258;&#28982;&#35821;&#35328;&#22768;&#26126;&#21644;&#20116;&#31181;&#25512;&#29702;&#31867;&#22411;&#65292;&#21487;&#24110;&#21161;&#31038;&#21306;&#26356;&#22909;&#22320;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#22312;&#21508;&#31181;&#39046;&#22495;&#65288;&#22914;&#21307;&#30103;&#24212;&#29992;&#21644;&#23545;&#35805;&#20195;&#29702;&#65289;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#20107;&#23454;&#39564;&#35777;&#26041;&#38754;&#65292;KG&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#20316;&#20026;&#30693;&#35782;&#28304;&#12290;KG&#21487;&#20197;&#25104;&#20026;&#20107;&#23454;&#39564;&#35777;&#30340;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#26469;&#28304;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#21487;&#38752;&#24615;&#21644;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;KG&#30001;&#33410;&#28857;&#21644;&#36793;&#32452;&#25104;&#65292;&#28165;&#26224;&#22320;&#23637;&#31034;&#20102;&#27010;&#24565;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20351;&#24471;&#26426;&#22120;&#21487;&#20197;&#25512;&#29702;&#20986;&#19968;&#31995;&#21015;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#29702;&#35299;&#36825;&#20123;&#26426;&#22120;&#21487;&#35835;&#30340;&#27010;&#24565;&#22914;&#20309;&#26144;&#23556;&#21040;&#25991;&#26412;&#20013;&#30340;&#20449;&#24687;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#12290;&#20026;&#20102;&#20351;&#31038;&#21306;&#26356;&#22909;&#22320;&#21033;&#29992;KG&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;FactKG:&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#65292;&#23427;&#21253;&#21547;108k&#20010;&#33258;&#28982;&#35821;&#35328;&#22768;&#26126;&#20197;&#21450;&#20116;&#31181;&#25512;&#29702;&#31867;&#22411;&#65306;&#21333;&#36339;&#12289;&#21512;&#21462;&#12289;&#23384;&#22312;&#12289;&#22810;&#36339;&#21644;&#21542;&#23450;&#12290;&#27492;&#22806;&#65292;FactKG&#21253;&#21547;&#21508;&#31181;&#35821;&#35328;&#27169;&#24335;&#65292;&#21253;&#25324;&#21475;&#35821;&#39118;&#26684;&#30340;&#22768;&#26126;&#21644;&#20070;&#38754;&#39118;&#26684;&#30340;&#22768;&#26126;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real world applications, knowledge graphs (KG) are widely used in various domains (e.g. medical applications and dialogue agents). However, for fact verification, KGs have not been adequately utilized as a knowledge source. KGs can be a valuable knowledge source in fact verification due to their reliability and broad applicability. A KG consists of nodes and edges which makes it clear how concepts are linked together, allowing machines to reason over chains of topics. However, there are many challenges in understanding how these machine-readable concepts map to information in text. To enable the community to better use KGs, we introduce a new dataset, FactKG: Fact Verification via Reasoning on Knowledge Graphs. It consists of 108k natural language claims with five types of reasoning: One-hop, Conjunction, Existence, Multi-hop, and Negation. Furthermore, FactKG contains various linguistic patterns, including colloquial style claims as well as written style claims to increase practica
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SemEval-2023 Task 2&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;MultiCoNER V2&#25968;&#25454;&#38598;&#65292;&#35782;&#21035;12&#31181;&#35821;&#35328;&#20013;&#22797;&#26434;&#30340;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#12290;&#26368;&#20248;&#26041;&#27861;&#26159;&#23558;&#22806;&#37096;&#30693;&#35782;&#34701;&#20837;transformer&#27169;&#22411;&#65292;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#26159;&#23186;&#20307;&#26631;&#39064;&#21644;&#20135;&#21697;&#21517;&#31216;&#31561;&#23454;&#20307;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.06586</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;2&#65306;&#32454;&#31890;&#24230;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;MultiCoNER 2&#65289;
&lt;/p&gt;
&lt;p&gt;
SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2). (arXiv:2305.06586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06586
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SemEval-2023 Task 2&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;MultiCoNER V2&#25968;&#25454;&#38598;&#65292;&#35782;&#21035;12&#31181;&#35821;&#35328;&#20013;&#22797;&#26434;&#30340;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#12290;&#26368;&#20248;&#26041;&#27861;&#26159;&#23558;&#22806;&#37096;&#30693;&#35782;&#34701;&#20837;transformer&#27169;&#22411;&#65292;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#26159;&#23186;&#20307;&#26631;&#39064;&#21644;&#20135;&#21697;&#21517;&#31216;&#31561;&#23454;&#20307;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SemEval-2023&#20219;&#21153;2&#20851;&#20110;&#32454;&#31890;&#24230;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;MultiCoNER 2&#65289;&#30340;&#30740;&#31350;&#21457;&#29616;&#12290;&#35813;&#20219;&#21153;&#20998;&#20026;13&#20010;&#36712;&#36947;&#65292;&#37325;&#28857;&#20851;&#27880;12&#31181;&#35821;&#35328;&#21644;&#21333;&#35821;&#12289;&#22810;&#35821;&#21644;&#22024;&#26434;&#29615;&#22659;&#19979;&#35782;&#21035;&#22797;&#26434;&#30340;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#65288;&#22914;WRITTENWORK&#12289;VEHICLE&#12289;MUSICALGRP&#65289;&#30340;&#26041;&#27861;&#12290;&#20219;&#21153;&#20351;&#29992;MultiCoNER V2&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;Bangla&#12289;Chinese&#12289;English&#12289;Farsi&#12289;French&#12289;German&#12289;Hindi&#12289;Italian&#12289;Portuguese&#12289;Spanish&#12289;Swedish&#21644;Ukrainian&#32452;&#25104;&#65292;&#20849;&#26377;220&#19975;&#20010;&#23454;&#20363;&#12290;MultiCoNER 2&#26159;SemEval-2023&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#21560;&#24341;&#20102;47&#20010;&#38431;&#20237;&#25552;&#20132;842&#20010;&#32467;&#26524;&#65292;&#20854;&#20013;34&#20010;&#38431;&#20237;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23186;&#20307;&#26631;&#39064;&#21644;&#20135;&#21697;&#21517;&#31216;&#31561;&#22797;&#26434;&#23454;&#20307;&#31867;&#22411;&#26159;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#65292;&#23558;&#22806;&#37096;&#30693;&#35782;&#34701;&#20837;transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;Creative Work&#21644;Group&#31867;&#21035;&#19978;&#33719;&#24471;&#20102;&#26368;&#22823;&#22686;&#30410;&#65292;&#21363;&#20351;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the findings of SemEval-2023 Task 2 on Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2). Divided into 13 tracks, the task focused on methods to identify complex fine-grained named entities (like WRITTENWORK, VEHICLE, MUSICALGRP) across 12 languages, in both monolingual and multilingual scenarios, as well as noisy settings. The task used the MultiCoNER V2 dataset, composed of 2.2 million instances in Bangla, Chinese, English, Farsi, French, German, Hindi, Italian., Portuguese, Spanish, Swedish, and Ukrainian. MultiCoNER 2 was one of the most popular tasks of SemEval-2023. It attracted 842 submissions from 47 teams, and 34 teams submitted system papers. Results showed that complex entity types such as media titles and product names were the most challenging. Methods fusing external knowledge into transformer models achieved the best performance, and the largest gains were on the Creative Work and Group classes, which are still challenging even with external kn
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#23383;&#20856;&#38142;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#33021;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.06575</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23383;&#20856;&#38142;&#25552;&#31034;&#22312;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Dictionary Prompting Elicits Translation in Large Language Models. (arXiv:2305.06575v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06575
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#28155;&#21152;&#23383;&#20856;&#38142;&#25552;&#31034;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#33021;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(MNMT)&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#27809;&#26377;&#24179;&#34892;&#25968;&#25454;&#20063;&#33021;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35757;&#32451;&#25968;&#25454;&#37327;&#24040;&#22823;&#65292;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#32763;&#35793;&#31232;&#26377;&#35789;&#27719;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#65292;&#24456;&#38590;&#26816;&#32034;&#21040;&#30456;&#20851;&#31034;&#33539;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36825;&#38480;&#21046;&#20102;LLMs&#22312;&#32763;&#35793;&#26041;&#38754;&#30340;&#23454;&#38469;&#24212;&#29992;&#8212;&#8212;&#25105;&#20204;&#35813;&#22914;&#20309;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;CoD&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#35821;&#35328;&#23383;&#20856;&#38142;&#20026;&#19968;&#37096;&#20998;&#36755;&#20837;&#21333;&#35789;&#22686;&#21152;LLMs&#30340;&#20808;&#21069;&#30693;&#35782;&#65292;&#20174;&#32780;&#20419;&#36827;LLMs&#30340;&#32763;&#35793;&#33021;&#21147;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;FLORES-200&#20840;&#24320;&#21457;&#27979;&#35797;&#38598;&#19978;&#65292;&#36890;&#36807;&#23558;CoD&#21644;ChatGPT&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#33719;&#24471;&#39640;&#36798;13&#20493;&#30340;MNMT ChrF++&#20998;&#25968;&#30340;&#25910;&#30410;&#65288;&#33521;&#35821;&#21040;&#22622;&#23572;&#32500;&#20122;&#35821;&#65292;&#35199;&#37324;&#23572;&#23383;&#27597;&#20070;&#20889;&#65292;ChrF ++&#20998;&#25968;&#20174;3.08&#22686;&#21152;&#21040;42.63&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#20854;&#20182;&#25968;&#25454;&#38598;&#19978;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown surprisingly good performance in multilingual neural machine translation (MNMT) even when trained without parallel data. Yet, despite the fact that the amount of training data is gigantic, they still struggle with translating rare words, particularly for low-resource languages. Even worse, it is usually unrealistic to retrieve relevant demonstrations for in-context learning with low-resource languages on LLMs, which restricts the practical use of LLMs for translation -- how should we mitigate this problem? To this end, we present a novel method, CoD, which augments LLMs with prior knowledge with the chains of multilingual dictionaries for a subset of input words to elicit translation abilities for LLMs. Extensive experiments indicate that augmenting ChatGPT with CoD elicits large gains by up to 13x ChrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in Cyrillic script) on FLORES-200 full devtest set. We further demonstrate the im
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550; FGWEA &#65292;&#23427;&#21033;&#29992;&#34701;&#21512;&#26684;&#32599;&#33707;&#22827; - &#29926;&#28909;&#26031;&#22374;&#36317;&#31163;&#65292;&#23454;&#29616;&#20102;&#23454;&#20307;&#35821;&#20041;&#21644;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#30340;&#32508;&#21512;&#27604;&#36739;&#21644;&#23545;&#40784;&#65292;&#36890;&#36807;&#19977;&#38454;&#27573;&#30340;&#28176;&#36827;&#20248;&#21270;&#31639;&#27861;&#26469;&#25552;&#39640;&#21305;&#37197;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06574</link><description>&lt;p&gt;
&#19968;&#31181;&#34701;&#21512;&#26684;&#32599;&#33707;&#22827; - &#29926;&#28909;&#26031;&#22374;&#26694;&#26550;&#30340;&#26080;&#30417;&#30563;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Fused Gromov-Wasserstein Framework for Unsupervised Knowledge Graph Entity Alignment. (arXiv:2305.06574v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550; FGWEA &#65292;&#23427;&#21033;&#29992;&#34701;&#21512;&#26684;&#32599;&#33707;&#22827; - &#29926;&#28909;&#26031;&#22374;&#36317;&#31163;&#65292;&#23454;&#29616;&#20102;&#23454;&#20307;&#35821;&#20041;&#21644;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#30340;&#32508;&#21512;&#27604;&#36739;&#21644;&#23545;&#40784;&#65292;&#36890;&#36807;&#19977;&#38454;&#27573;&#30340;&#28176;&#36827;&#20248;&#21270;&#31639;&#27861;&#26469;&#25552;&#39640;&#21305;&#37197;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#26159;&#22312;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20013;&#35782;&#21035;&#30456;&#24212;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#22522;&#20110;&#23884;&#20837;&#30340;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#20805;&#20998;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FGWEA&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;&#65292;&#21033;&#29992;&#34701;&#21512;&#26684;&#32599;&#33707;&#22827; - &#29926;&#28909;&#26031;&#22374;&#65288;FGW&#65289;&#36317;&#31163;&#65292;&#20801;&#35768;&#22312;&#32852;&#21512;&#20248;&#21270;&#26694;&#26550;&#20869;&#20840;&#38754;&#27604;&#36739;&#23454;&#20307;&#35821;&#20041;&#21644;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#12290;&#20026;&#20102;&#35299;&#20915;&#20248;&#21270;FGW&#25152;&#28041;&#21450;&#30340;&#35745;&#31639;&#25361;&#25112;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#28176;&#36827;&#20248;&#21270;&#31639;&#27861;&#12290;&#23427;&#20174;&#22522;&#26412;&#35821;&#20041;&#23884;&#20837;&#21305;&#37197;&#24320;&#22987;&#65292;&#26681;&#25454;&#39640;&#32622;&#20449;&#24230;&#23454;&#20307;&#38142;&#25509;&#30340;&#36845;&#20195;&#26356;&#26032;&#65292;&#36880;&#27493;&#36817;&#20284;&#36328;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#21644;&#20851;&#31995;&#30456;&#20284;&#24615;&#21305;&#37197;&#65292;&#26368;&#32456;&#22312;&#30693;&#35782;&#22270;&#35889;&#38388;&#36827;&#34892;&#20840;&#23616;&#32467;&#26500;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;14&#20010;&#23454;&#20307;&#23545;&#40784;&#25968;&#25454;&#38598;&#30340;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity alignment is the task of identifying corresponding entities across different knowledge graphs (KGs). Although recent embedding-based entity alignment methods have shown significant advancements, they still struggle to fully utilize KG structural information. In this paper, we introduce FGWEA, an unsupervised entity alignment framework that leverages the Fused Gromov-Wasserstein (FGW) distance, allowing for a comprehensive comparison of entity semantics and KG structures within a joint optimization framework. To address the computational challenges associated with optimizing FGW, we devise a three-stage progressive optimization algorithm. It starts with a basic semantic embedding matching, proceeds to approximate cross-KG structural and relational similarity matching based on iterative updates of high-confidence entity links, and ultimately culminates in a global structural comparison between KGs. We perform extensive experiments on four entity alignment datasets covering 14 dist
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06569</link><description>&lt;p&gt;
&#22914;&#20309;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#32034;&#24341;&#39033;&#30446;ID
&lt;/p&gt;
&lt;p&gt;
How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23558;&#25512;&#33616;&#20219;&#21153;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25512;&#33616;&#12290;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#24314;&#35758;&#30340;&#39033;&#30446;&#32780;&#19981;&#26159;&#35745;&#31639;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#20013;&#27599;&#20010;&#20505;&#36873;&#39033;&#30446;&#30340;&#25490;&#21517;&#24471;&#20998;&#65292;&#31616;&#21270;&#20102;&#25512;&#33616;&#31649;&#36947;&#65292;&#36991;&#20813;&#20102;&#22810;&#27573;&#36807;&#28388;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#20915;&#23450;&#35201;&#25512;&#33616;&#21738;&#20123;&#39033;&#30446;&#26102;&#29983;&#25104;&#36807;&#38271;&#30340;&#25991;&#26412;&#65292;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#21019;&#24314;LLM&#20860;&#23481;&#30340;&#39033;&#30446;ID&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#65292;&#20197;P5&#20026;&#20195;&#34920;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32034;&#24341;&#26041;&#27861;&#22797;&#21046;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#20960;&#31181;&#24494;&#19981;&#36275;&#36947;&#30340;&#39033;&#30446;&#32034;&#24341;&#26041;&#27861;&#65288;&#22914;&#29420;&#31435;&#32034;&#24341;&#12289;&#26631;&#39064;&#32034;&#24341;&#21644;&#38543;&#26426;&#32034;&#24341;&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#32034;&#24341;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32034;&#24341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;GENRE&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#20041;&#30693;&#35782;&#20016;&#23500;&#26032;&#38395;&#25968;&#25454;&#65292;&#36890;&#36807;&#20174;&#27169;&#22411;&#35774;&#35745;&#36716;&#31227;&#21040;&#25552;&#31034;&#35774;&#35745;&#25552;&#20379;&#28789;&#27963;&#32780;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#26032;&#38395;&#29983;&#25104;&#12289;&#29992;&#25143;&#30011;&#20687;&#21644;&#26032;&#38395;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.06566</link><description>&lt;p&gt;
LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#21021;&#25506;
&lt;/p&gt;
&lt;p&gt;
A First Look at LLM-Powered Generative News Recommendation. (arXiv:2305.06566v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;GENRE&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#20041;&#30693;&#35782;&#20016;&#23500;&#26032;&#38395;&#25968;&#25454;&#65292;&#36890;&#36807;&#20174;&#27169;&#22411;&#35774;&#35745;&#36716;&#31227;&#21040;&#25552;&#31034;&#35774;&#35745;&#25552;&#20379;&#28789;&#27963;&#32780;&#32479;&#19968;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#20010;&#24615;&#21270;&#26032;&#38395;&#29983;&#25104;&#12289;&#29992;&#25143;&#30011;&#20687;&#21644;&#26032;&#38395;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#20026;&#29992;&#25143;&#27983;&#35272;&#28023;&#37327;&#22312;&#32447;&#26032;&#38395;&#20869;&#23481;&#25152;&#24517;&#38656;&#30340;&#24037;&#20855;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#26032;&#38395;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30528;&#20919;&#21551;&#21160;&#38382;&#39064;&#12289;&#29992;&#25143;&#30011;&#20687;&#24314;&#27169;&#21644;&#26032;&#38395;&#20869;&#23481;&#29702;&#35299;&#31561;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#27169;&#22411;&#35774;&#35745;&#36981;&#24490;&#19968;&#31181;&#19981;&#28789;&#27963;&#30340;&#20363;&#34892;&#31243;&#24207;&#26469;&#35299;&#20915;&#29305;&#23450;&#30340;&#25361;&#25112;&#65292;&#20294;&#22312;&#29702;&#35299;&#26032;&#38395;&#20869;&#23481;&#21644;&#25429;&#25417;&#29992;&#25143;&#20852;&#36259;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GENRE&#65292;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#29983;&#25104;&#24335;&#26032;&#38395;&#25512;&#33616;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#35821;&#20041;&#30693;&#35782;&#26469;&#20016;&#23500;&#26032;&#38395;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20174;&#27169;&#22411;&#35774;&#35745;&#36716;&#31227;&#21040;&#25552;&#31034;&#35774;&#35745;&#26469;&#25552;&#20379;&#19968;&#31181;&#28789;&#27963;&#32780;&#32479;&#19968;&#30340;&#26032;&#38395;&#25512;&#33616;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GENRE&#22312;&#20010;&#24615;&#21270;&#26032;&#38395;&#29983;&#25104;&#12289;&#29992;&#25143;&#30011;&#20687;&#21644;&#26032;&#38395;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;&#12290;&#20351;&#29992;&#21508;&#31181;&#27969;&#34892;&#30340;&#25512;&#33616;&#27169;&#22411;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;GENRE&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized news recommendation systems have become essential tools for users to navigate the vast amount of online news content, yet existing news recommenders face significant challenges such as the cold-start problem, user profile modeling, and news content understanding. Previous works have typically followed an inflexible routine to address a particular challenge through model design, but are limited in their ability to understand news content and capture user interests. In this paper, we introduce GENRE, an LLM-powered generative news recommendation framework, which leverages pretrained semantic knowledge from large language models to enrich news data. Our aim is to provide a flexible and unified solution for news recommendation by moving from model design to prompt design. We showcase the use of GENRE for personalized news generation, user profiling, and news summarization. Extensive experiments with various popular recommendation models demonstrate the effectiveness of GENRE. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#38271;&#23614;&#20998;&#24067;&#25968;&#25454;&#30340;Open Long-Tailed QA (OLTQA)&#27169;&#22411;&#65292;&#40723;&#21169;&#22836;&#37096;&#12289;&#23614;&#37096;&#21644;&#26410;&#30693;&#20219;&#21153;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#65292;&#24182;&#20174;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26126;&#30830;&#25366;&#25496;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;QA&#26041;&#27861;&#20013;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06557</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#20013;&#30340;&#38271;&#23614;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Long-Tailed Question Answering in an Open World. (arXiv:2305.06557v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#38271;&#23614;&#20998;&#24067;&#25968;&#25454;&#30340;Open Long-Tailed QA (OLTQA)&#27169;&#22411;&#65292;&#40723;&#21169;&#22836;&#37096;&#12289;&#23614;&#37096;&#21644;&#26410;&#30693;&#20219;&#21153;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#65292;&#24182;&#20174;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26126;&#30830;&#25366;&#25496;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;QA&#26041;&#27861;&#20013;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#24320;&#25918;&#38271;&#23614;&#20998;&#24067;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#32479;&#19968;&#30340;QA&#27169;&#22411;&#20197;&#25903;&#25345;&#21508;&#31181;&#20219;&#21153;&#23545;&#20110;&#23454;&#38469;&#30340;QA&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#25193;&#23637;&#20197;&#21069;&#30340;QA&#26041;&#27861;&#24182;&#19981;&#23481;&#26131;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#38656;&#35201;&#35775;&#38382;&#36275;&#22815;&#26679;&#26412;&#30340;&#24050;&#30693;&#20219;&#21153;&#65292;&#35201;&#20040;&#19981;&#26126;&#30830;&#22320;&#23545;&#26410;&#30693;&#20219;&#21153;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;Open Long-Tailed QA (OLTQA)&#23450;&#20041;&#20026;&#23398;&#20064;&#38271;&#23614;&#20998;&#24067;&#25968;&#25454;&#24182;&#22312;&#24050;&#30693;&#21644;&#26410;&#30693;QA&#20219;&#21153;&#19978;&#20248;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;OLTQA&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#40723;&#21169;&#22836;&#37096;&#12289;&#23614;&#37096;&#21644;&#26410;&#30693;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#65292;&#24182;&#20174;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26126;&#30830;&#25366;&#25496;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#32452;&#32454;&#31890;&#24230;&#30340;&#32452;&#20214;&#26469;&#32452;&#32455;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#21160;&#24577;&#32452;&#21512;&#36825;&#20123;&#32452;&#20214;&#20197;&#26041;&#20415;&#30693;&#35782;&#20849;&#20139;&#12290;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#20010;&#26816;&#32034;-&#37325;&#25490;&#26694;&#26550;&#26469;&#36873;&#25321;&#19978;&#19979;&#25991;&#20363;&#23376;&#65292;&#36825;&#20123;&#20363;&#23376;&#25351;&#23548;LM&#29983;&#25104;&#34920;&#36798;QA&#20219;&#21153;&#30693;&#35782;&#30340;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21160;&#24577;&#27744;&#21270;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data often have an open long-tailed distribution, and building a unified QA model supporting various tasks is vital for practical QA applications. However, it is non-trivial to extend previous QA approaches since they either require access to seen tasks of adequate samples or do not explicitly model samples from unseen tasks. In this paper, we define Open Long-Tailed QA (OLTQA) as learning from long-tailed distributed data and optimizing performance over seen and unseen QA tasks. We propose an OLTQA model that encourages knowledge sharing between head, tail and unseen tasks, and explicitly mines knowledge from a large pre-trained language model (LM). Specifically, we organize our model through a pool of fine-grained components and dynamically combine these components for an input to facilitate knowledge sharing. A retrieve-then-rerank frame is further introduced to select in-context examples, which guild the LM to generate text that express knowledge for QA tasks. Moreover, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Diana&#27169;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#26550;&#26500;&#30340;&#29983;&#21629;&#21608;&#26399;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#22235;&#31181;&#23618;&#27425;&#21270;&#32452;&#32455;&#30340;&#25552;&#31034;&#26469;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#20854;&#20013;&#65292;&#20219;&#21153;&#32423;&#25552;&#31034;&#29992;&#20110;&#25429;&#33719;&#20219;&#21153;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#23454;&#20363;&#32423;&#25552;&#31034;&#29992;&#20110;&#23398;&#20064;&#36328;&#36755;&#20837;&#26679;&#26412;&#20849;&#20139;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06555</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#19979;&#30340;&#39046;&#22495;&#22686;&#37327;&#29983;&#26426;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Domain Incremental Lifelong Learning in an Open World. (arXiv:2305.06555v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Diana&#27169;&#22411;&#65292;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#26550;&#26500;&#30340;&#29983;&#21629;&#21608;&#26399;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#22235;&#31181;&#23618;&#27425;&#21270;&#32452;&#32455;&#30340;&#25552;&#31034;&#26469;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#20854;&#20013;&#65292;&#20219;&#21153;&#32423;&#25552;&#31034;&#29992;&#20110;&#25429;&#33719;&#20219;&#21153;&#29305;&#23450;&#30340;&#30693;&#35782;&#65292;&#23454;&#20363;&#32423;&#25552;&#31034;&#29992;&#20110;&#23398;&#20064;&#36328;&#36755;&#20837;&#26679;&#26412;&#20849;&#20139;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#26159;NLP&#27169;&#22411;&#19981;&#26029;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#37325;&#35201;&#33021;&#21147;&#12290;&#22522;&#20110;&#26550;&#26500;&#30340;&#26041;&#27861;&#34987;&#25253;&#36947;&#20026;LL&#27169;&#22411;&#30340;&#26377;&#25928;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23558;&#20808;&#21069;&#30340;&#26041;&#27861;&#25193;&#23637;&#21040;&#39046;&#22495;&#22686;&#37327;LL&#22330;&#26223;&#24182;&#38750;&#26131;&#20107;&#65292;&#22240;&#20026;&#23427;&#20204;&#35201;&#20040;&#38656;&#35201;&#22312;&#27979;&#35797;&#38454;&#27573;&#35775;&#38382;&#20219;&#21153;&#36523;&#20221;&#65292;&#35201;&#20040;&#26080;&#27861;&#22788;&#29702;&#26469;&#33258;&#26410;&#35265;&#20219;&#21153;&#30340;&#26679;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;Diana&#65306;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#26550;&#26500;&#30340;&#29983;&#21629;&#21608;&#26399;&#23398;&#20064;&#27169;&#22411;&#65292;&#35797;&#22270;&#36890;&#36807;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#26469;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290; Diana&#20351;&#29992;&#22235;&#31181;&#23618;&#27425;&#21270;&#32452;&#32455;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#19981;&#21516;&#31890;&#24230;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning (LL) is an important ability for NLP models to learn new tasks continuously. Architecture-based approaches are reported to be effective implementations for LL models. However, it is non-trivial to extend previous approaches to domain incremental LL scenarios since they either require access to task identities in the testing phase or cannot handle samples from unseen tasks. In this paper, we propose \textbf{Diana}: a \underline{d}ynam\underline{i}c \underline{a}rchitecture-based lifelo\underline{n}g le\underline{a}rning model that tries to learn a sequence of tasks with a prompt-enhanced language model. Four types of hierarchically organized prompts are used in Diana to capture knowledge from different granularities. Specifically, we dedicate task-level prompts to capture task-specific knowledge to retain high LL performances and maintain instance-level prompts to learn knowledge shared across input samples to improve the model's generalization performance. Moreover, w
&lt;/p&gt;</description></item><item><title>GeoGLUE&#26159;&#19968;&#20010;&#26032;&#30340;&#22320;&#29702;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#20845;&#20010;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#24182;&#19988;&#32463;&#36807;&#20102;&#26377;&#25928;&#24615;&#21644;&#37325;&#35201;&#24615;&#30340;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.06545</link><description>&lt;p&gt;
GeoGLUE&#65306;&#19968;&#20010;&#22320;&#29702;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
GeoGLUE: A GeoGraphic Language Understanding Evaluation Benchmark. (arXiv:2305.06545v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06545
&lt;/p&gt;
&lt;p&gt;
GeoGLUE&#26159;&#19968;&#20010;&#26032;&#30340;&#22320;&#29702;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#20845;&#20010;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#24182;&#19988;&#32463;&#36807;&#20102;&#26377;&#25928;&#24615;&#21644;&#37325;&#35201;&#24615;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22320;&#29702;&#24212;&#29992;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#33258;&#21160;&#21270;&#21644;&#26234;&#33021;&#21270;&#30340;&#27169;&#22411;&#26159;&#22788;&#29702;&#22823;&#37327;&#20449;&#24687;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#32773;&#20851;&#27880;&#22320;&#29702;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#20063;&#20174;&#26410;&#24314;&#31435;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#26631;&#20934;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GeoGLUE&#30340;&#22320;&#29702;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;&#12290;&#25105;&#20204;&#20174;&#20844;&#24320;&#21457;&#24067;&#30340;&#22320;&#29702;&#36164;&#28304;&#25910;&#38598;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20845;&#20010;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#21253;&#25324;&#22522;&#20110;&#22238;&#35843;&#30340;&#22320;&#29702;&#25991;&#26412;&#30456;&#20284;&#24230;&#12289;&#22522;&#20110;&#37325;&#26032;&#25490;&#24207;&#30340;&#22320;&#29702;&#25991;&#26412;&#30456;&#20284;&#24230;&#12289;&#22320;&#29702;&#20803;&#32032;&#26631;&#35760;&#12289;&#22320;&#29702;&#32452;&#21512;&#20998;&#26512;&#12289;&#22320;&#29702;&#20309;&#22788;&#20998;&#31163;&#12289;&#22320;&#29702;&#23454;&#20307;&#23545;&#40784;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35780;&#20272;&#23454;&#39564;&#21644;&#19968;&#33324;&#24615;&#22522;&#32447;&#30340;&#20998;&#26512;&#65292;&#34920;&#26126;GeoGLUE&#22522;&#20934;&#30340;&#26377;&#25928;&#24615;&#21644;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a fast developing pace of geographic applications, automatable and intelligent models are essential to be designed to handle the large volume of information. However, few researchers focus on geographic natural language processing, and there has never been a benchmark to build a unified standard. In this work, we propose a GeoGraphic Language Understanding Evaluation benchmark, named GeoGLUE. We collect data from open-released geographic resources and introduce six natural language understanding tasks, including geographic textual similarity on recall, geographic textual similarity on rerank, geographic elements tagging, geographic composition analysis, geographic where what cut, and geographic entity alignment. We also pro vide evaluation experiments and analysis of general baselines, indicating the effectiveness and significance of the GeoGLUE benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#32422;&#23450;&#22914;&#20309;&#36716;&#31227;&#21040;&#19982;&#20043;&#21069;&#23436;&#20840;&#19981;&#21516;&#30340;&#23545;&#35937;&#65292;&#36890;&#36807;&#35843;&#26597;&#21487;&#21629;&#21517;&#24615;&#22914;&#20309;&#24433;&#21709;&#32422;&#23450;&#30340;&#24418;&#25104;&#20197;&#21450;&#26032;&#30340;&#35268;&#21017;&#22914;&#20309;&#27867;&#21270;&#21040;&#26032;&#30340;&#25351;&#28041;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.06539</link><description>&lt;p&gt;
&#35821;&#20041;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#24815;&#20363;&#22312;&#26032;&#25351;&#28041;&#29289;&#19978;&#30340;&#25512;&#24191;
&lt;/p&gt;
&lt;p&gt;
Semantic uncertainty guides the extension of conventions to new referents. (arXiv:2305.06539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#32422;&#23450;&#22914;&#20309;&#36716;&#31227;&#21040;&#19982;&#20043;&#21069;&#23436;&#20840;&#19981;&#21516;&#30340;&#23545;&#35937;&#65292;&#36890;&#36807;&#35843;&#26597;&#21487;&#21629;&#21517;&#24615;&#22914;&#20309;&#24433;&#21709;&#32422;&#23450;&#30340;&#24418;&#25104;&#20197;&#21450;&#26032;&#30340;&#35268;&#21017;&#22914;&#20309;&#27867;&#21270;&#21040;&#26032;&#30340;&#25351;&#28041;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#24515;&#29702;&#23398;&#30340;&#38271;&#26399;&#30740;&#31350;&#20256;&#32479;&#19968;&#30452;&#22312;&#30740;&#31350;&#25351;&#31216;&#28216;&#25103;&#20013;&#30340;&#29305;&#27530;&#24815;&#20363;&#30340;&#24418;&#25104;&#21644;&#27867;&#21270;&#65292;&#23637;&#31034;&#20102;&#26032;&#30446;&#26631;&#30340;&#20064;&#24471;&#30340;&#24815;&#20363;&#22914;&#20309;&#36716;&#31227;&#21040;&#26032;&#30340;&#25351;&#28041;&#24773;&#22659;&#20013;&#12290;&#20294;&#26159;&#65292;&#21478;&#19968;&#20010;&#24191;&#27867;&#21270;&#30340;&#36724;&#21364;&#40092;&#26377;&#30740;&#31350;&#65306;&#24403;&#29305;&#23450;&#35789;&#27719;&#36873;&#25321;&#19981;&#22826;&#21487;&#33021;&#37325;&#22797;&#26102;&#65292;&#24418;&#25104;&#30340;&#32422;&#23450;&#22914;&#20309;&#36716;&#31227;&#21040;&#23436;&#20840;&#19981;&#21516;&#30340;&#30446;&#26631;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#21452;&#20154;&#30740;&#31350;&#65288;N=240&#65289;&#65292;&#30528;&#37325;&#25506;&#35752;&#21487;&#21629;&#21517;&#24615;&#30340;&#20316;&#29992;&#8212;&#8212;&#20004;&#20010;&#20010;&#20307;&#20849;&#20139;&#30456;&#21516;&#26631;&#31614;&#30340;&#20808;&#39564;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#21457;&#24067;&#30340;KiloGram&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20043;&#21069;&#21487;&#29992;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#25968;&#37327;&#32423;&#65292;&#23637;&#31034;&#20102;&#39640;&#21487;&#21629;&#21517;&#24615;&#31561;&#23646;&#24615;&#30340;&#39640;&#22810;&#26679;&#24615;&#25277;&#35937;&#22270;&#26696;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#21487;&#21629;&#21517;&#24615;&#22914;&#20309;&#24418;&#25104;&#24815;&#20363;&#65292;&#32780;&#31532;&#20108;&#39033;&#21017;&#25506;&#35752;&#20102;&#26032;&#30340;&#35268;&#21017;&#22914;&#20309;&#27867;&#21270;&#21040;&#23436;&#20840;&#19981;&#21516;&#30340;&#25351;&#28041;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
A long tradition of studies in psycholinguistics has examined the formation and generalization of ad hoc conventions in reference games, showing how newly acquired conventions for a given target transfer to new referential contexts. However, another axis of generalization remains understudied: how do conventions formed for one target transfer to completely distinct targets, when specific lexical choices are unlikely to repeat? This paper presents two dyadic studies (N = 240) that address this axis of generalization, focusing on the role of nameability -- the a priori likelihood that two individuals will share the same label. We leverage the recently-released KiloGram dataset, a collection of abstract tangram images that is orders of magnitude larger than previously available, exhibiting high diversity of properties like nameability. Our first study asks how nameability shapes convention formation, while the second asks how new conventions generalize to entirely new targets of reference
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#24046;&#24322;&#23545;&#40784;&#30340;&#36890;&#29992;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;KGA&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#22330;&#26223;&#20013;&#30340;&#36951;&#24536;&#21151;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;NLP&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.06535</link><description>&lt;p&gt;
KGA: &#22522;&#20110;&#30693;&#35782;&#24046;&#24322;&#23545;&#40784;&#30340;&#36890;&#29992;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment. (arXiv:2305.06535v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#30693;&#35782;&#24046;&#24322;&#23545;&#40784;&#30340;&#36890;&#29992;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;KGA&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#22330;&#26223;&#20013;&#30340;&#36951;&#24536;&#21151;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;NLP&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#12298;&#34987;&#36951;&#24536;&#26435;&#12299;&#31435;&#27861;&#24341;&#36215;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#30740;&#31350;&#20852;&#36259;&#65292;&#20854;&#20013;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#24536;&#35760;&#26377;&#20851;&#29305;&#23450;&#35757;&#32451;&#23454;&#20363;&#20449;&#24687;&#30340;&#21151;&#33021;&#65292;&#23601;&#20687;&#23427;&#20204;&#20174;&#26410;&#23384;&#22312;&#20110;&#35757;&#32451;&#38598;&#20013;&#19968;&#26679;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#20027;&#35201;&#20851;&#27880;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#22330;&#26223;&#65292;&#24182;&#19988;&#22312;NLP&#39046;&#22495;&#20013;&#24573;&#30053;&#20102;&#36951;&#24536;&#30340;&#35201;&#32032;&#65292;&#25991;&#26412;&#25968;&#25454;&#27604;&#22270;&#20687;&#21253;&#21547;&#26356;&#22810;&#26126;&#30830;&#19988;&#25935;&#24863;&#30340;&#20010;&#20154;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36951;&#24536;&#26694;&#26550;KGA&#26469;&#35825;&#21457;&#36951;&#24536;&#12290;&#19982;&#20197;&#24448;&#35797;&#22270;&#24674;&#22797;&#26799;&#24230;&#25110;&#24378;&#21046;&#27169;&#22411;&#25191;&#34892;&#25509;&#36817;&#20110;&#19968;&#20010;&#29305;&#23450;&#20998;&#24067;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;KGA&#32500;&#25252;&#20998;&#24067;&#24046;&#24322;&#65288;&#21363;&#65292;&#30693;&#35782;&#24046;&#24322;&#65289;&#12290;&#36825;&#25918;&#23485;&#20102;&#20998;&#24067;&#20551;&#35774;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39318;&#27425;&#23558;&#36951;&#24536;&#26041;&#27861;&#24212;&#29992;&#20110;&#21508;&#31181;NLP&#20219;&#21153;&#65288;&#21363;&#65292;&#20998;&#31867;&#65292;&#32763;&#35793;&#65292;&#21709;&#24212;&#29983;&#25104;&#65289;&#24182;&#25552;&#20986;&#20102;&#20960;&#20010;&#20855;&#26377;&#30456;&#20851;&#24615;&#30340;&#36951;&#24536;&#35780;&#20272;&#25351;&#26631;&#12290;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;KGA&#26694;&#26550;&#22312;NLP&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#36866;&#24403;&#30340;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent legislation of the "right to be forgotten" has led to the interest in machine unlearning, where the learned models are endowed with the function to forget information about specific training instances as if they have never existed in the training set. Previous work mainly focuses on computer vision scenarios and largely ignores the essentials of unlearning in NLP field, where text data contains more explicit and sensitive personal information than images. In this paper, we propose a general unlearning framework called KGA to induce forgetfulness. Different from previous work that tries to recover gradients or forces models to perform close to one specific distribution, KGA maintains distribution differences (i.e., knowledge gap). This relaxes the distribution assumption. Furthermore, we first apply the unlearning method to various NLP tasks (i.e., classification, translation, response generation) and propose several unlearning evaluation metrics with pertinence. Experiments on l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#21830;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#36234;&#19981;&#21516;&#35821;&#35328;&#31995;&#21644;&#22320;&#29702;&#21306;&#22495;&#30340;&#20843;&#31181;&#38750;&#27954;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#22312;&#38750;&#27954;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30053;&#20302;&#12290;&#21628;&#21505;&#30830;&#20445;&#38750;&#27954;&#35821;&#35328;&#22312;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24471;&#21040;&#20805;&#20998;&#30340;&#37325;&#35270;&#12290;</title><link>http://arxiv.org/abs/2305.06530</link><description>&lt;p&gt;
&#21830;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#27954;&#35821;&#31181;&#19978;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Good are Commercial Large Language Models on African Languages?. (arXiv:2305.06530v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21830;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#36234;&#19981;&#21516;&#35821;&#35328;&#31995;&#21644;&#22320;&#29702;&#21306;&#22495;&#30340;&#20843;&#31181;&#38750;&#27954;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#22312;&#38750;&#27954;&#35821;&#35328;&#19978;&#30340;&#34920;&#29616;&#30053;&#20302;&#12290;&#21628;&#21505;&#30830;&#20445;&#38750;&#27954;&#35821;&#35328;&#22312;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24471;&#21040;&#20805;&#20998;&#30340;&#37325;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#20419;&#20351;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26222;&#21450;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#26410;&#30693;&#20219;&#21153;&#21644;&#35821;&#35328;&#19978;&#20063;&#33021;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#23427;&#20204;&#20063;&#34987;&#20316;&#20026;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#26381;&#21153;&#30340;&#21830;&#19994;API&#25152;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#38750;&#27954;&#35821;&#31181;&#19978;&#30340;&#34920;&#29616;&#23578;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#23545;&#36328;&#36234;&#19981;&#21516;&#35821;&#35328;&#31995;&#21644;&#22320;&#29702;&#21306;&#22495;&#30340;&#20843;&#31181;&#38750;&#27954;&#35821;&#35328;&#19978;&#30340;&#20004;&#39033;&#20219;&#21153;&#65288;&#26426;&#22120;&#32763;&#35793;&#21644;&#25991;&#26412;&#20998;&#31867;&#65289;&#19978;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#22312;&#38750;&#27954;&#35821;&#31181;&#19978;&#30340;&#24615;&#33021;&#30053;&#20302;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#23427;&#20204;&#22312;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#30340;&#34920;&#29616;&#26356;&#22909;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21628;&#21505;&#30830;&#20445;&#38750;&#27954;&#35821;&#35328;&#22312;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24471;&#21040;&#20805;&#20998;&#30340;&#37325;&#35270;&#65292;&#36825;&#20063;&#26159;&#38750;&#27954;&#35821;&#31181;&#36880;&#28176;&#27969;&#34892;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Natural Language Processing (NLP) has led to the proliferation of large pretrained language models. These models have been shown to yield good performance, using in-context learning, even on unseen tasks and languages. They have also been exposed as commercial APIs as a form of language-model-as-a-service, with great adoption. However, their performance on African languages is largely unknown. We present a preliminary analysis of commercial large language models on two tasks (machine translation and text classification) across eight African languages, spanning different language families and geographical areas. Our results suggest that commercial language models produce below-par performance on African languages. We also find that they perform better on text classification than machine translation. In general, our findings present a call-to-action to ensure African languages are well represented in commercial large language models, given their growing popularity.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550; RSMI&#65292;&#32467;&#21512;&#20102;&#38543;&#26426;&#24179;&#28369;&#21644;&#25513;&#30721;&#25512;&#29702;&#65292;&#20197;&#25552;&#39640; NLP &#31995;&#32479;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#32463;&#36807;&#22522;&#20934;&#25968;&#25454;&#38598;&#27979;&#35797;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#23558;&#23545;&#25239;&#40065;&#26834;&#24615;&#25552;&#39640;2&#21040;3&#20493;&#12290;</title><link>http://arxiv.org/abs/2305.06522</link><description>&lt;p&gt;
&#38543;&#26426;&#24179;&#28369;&#21644;&#25513;&#30721;&#25512;&#29702;&#29992;&#20110;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications. (arXiv:2305.06522v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550; RSMI&#65292;&#32467;&#21512;&#20102;&#38543;&#26426;&#24179;&#28369;&#21644;&#25513;&#30721;&#25512;&#29702;&#65292;&#20197;&#25552;&#39640; NLP &#31995;&#32479;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#32463;&#36807;&#22522;&#20934;&#25968;&#25454;&#38598;&#27979;&#35797;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#23558;&#23545;&#25239;&#40065;&#26834;&#24615;&#25552;&#39640;2&#21040;3&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181; NLP &#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20063;&#34987;&#30693;&#36947;&#23545;&#29305;&#23450;&#30340;&#23545;&#25239;&#24615;&#20363;&#23376;&#23384;&#22312;&#33030;&#24369;&#24615;&#65292;&#22240;&#27492;&#36234;&#26469;&#36234;&#22810;&#20851;&#27880; NLP &#31995;&#32479;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102; RSMI&#65292;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#23427;&#23558;&#38543;&#26426;&#24179;&#28369;&#65288;RS&#65289;&#19982;&#25513;&#30721;&#25512;&#29702;&#65288;MI&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640; NLP &#31995;&#32479;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;RS&#23558;&#20998;&#31867;&#22120;&#36716;&#25442;&#20026;&#24179;&#28369;&#30340;&#20998;&#31867;&#22120;&#65292;&#20197;&#33719;&#24471;&#31283;&#20581;&#30340;&#34920;&#31034;&#65292;&#32780;MI&#24378;&#21046;&#27169;&#22411;&#21033;&#29992;&#36755;&#20837;&#24207;&#21015;&#20013;&#19968;&#20010;&#25513;&#34109;&#26631;&#35760;&#30340;&#21608;&#22260;&#19978;&#19979;&#25991;&#12290;RSMI&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#23558;&#23545;&#25239;&#40065;&#26834;&#24615;&#25552;&#39640;2&#21040;3&#20493;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#23450;&#24615;&#20998;&#26512;&#65292;&#20197;&#39564;&#35777; RSMI &#19981;&#21516;&#38454;&#27573;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#28040;&#34701;&#30740;&#31350;&#25506;&#31350;&#20854;&#26500;&#25104;&#37096;&#20998;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#23454;&#35777;&#35777;&#26126; RSMI &#30340;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#25512;&#21521;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems. We introduce RSMI, a novel two-stage framework that combines randomized smoothing (RS) with masked inference (MI) to improve the adversarial robustness of NLP systems. RS transforms a classifier into a smoothed classifier to obtain robust representations, whereas MI forces a model to exploit the surrounding context of a masked token in an input sequence. RSMI improves adversarial robustness by 2 to 3 times over existing state-of-the-art methods on benchmark datasets. We also perform in-depth qualitative analysis to validate the effectiveness of the different stages of RSMI and probe the impact of its components through extensive ablations. By empirically proving the stability of RSMI, we put it f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#36523;&#21270;&#23436;&#25104;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#21270;&#20219;&#21153;&#35745;&#21010;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#33021;&#22815;&#20351;&#29992;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#39044;&#27979;&#26356;&#22909;&#30340;&#35745;&#21010;&#65292;&#21478;&#22806;&#35745;&#21010;&#39044;&#27979;&#21644;&#35745;&#21010;&#25191;&#34892;&#27169;&#22359;&#21487;&#33021;&#30456;&#20114;&#20381;&#36182;&#65292;&#23436;&#20840;&#35299;&#32806;&#21487;&#33021;&#19981;&#26159;&#29702;&#24819;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.06485</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#21270;&#20219;&#21153;&#35745;&#21010;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multimodal Contextualized Plan Prediction for Embodied Task Completion. (arXiv:2305.06485v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20855;&#36523;&#21270;&#23436;&#25104;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#21270;&#20219;&#21153;&#35745;&#21010;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#33021;&#22815;&#20351;&#29992;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#39044;&#27979;&#26356;&#22909;&#30340;&#35745;&#21010;&#65292;&#21478;&#22806;&#35745;&#21010;&#39044;&#27979;&#21644;&#35745;&#21010;&#25191;&#34892;&#27169;&#22359;&#21487;&#33021;&#30456;&#20114;&#20381;&#36182;&#65292;&#23436;&#20840;&#35299;&#32806;&#21487;&#33021;&#19981;&#26159;&#29702;&#24819;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#35268;&#21010;&#26159;&#20256;&#32479;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#32452;&#21512;&#32454;&#31890;&#24230;&#25216;&#33021;&#26469;&#25191;&#34892;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#26500;&#24314;&#31995;&#32479;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;&#21487;&#25191;&#34892;&#34892;&#21160;&#65292;&#20197;&#22312;&#27169;&#25311;&#30340;&#20855;&#36523;&#21270;&#20195;&#29702;&#20013;&#23436;&#25104;&#20219;&#21153;&#65292;&#37325;&#28857;&#26159;&#30452;&#25509;&#39044;&#27979;&#21487;&#36890;&#36807;&#29289;&#29702;&#26426;&#22120;&#20154;&#30452;&#25509;&#25191;&#34892;&#30340;&#20302;&#32423;&#21035;&#34892;&#21160;&#24207;&#21015;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36716;&#32780;&#19987;&#27880;&#20110;&#39044;&#27979;&#36739;&#39640;&#23618;&#27425;&#30340;&#35745;&#21010;&#34920;&#31034;&#65292;&#29992;&#20110;TEACh&#36825;&#26679;&#30340;&#20855;&#36523;&#21270;&#23436;&#25104;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#20551;&#35774;&#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#33719;&#24471;&#39640;&#23618;&#27425;&#35745;&#21010;&#39044;&#27979;&#30340;&#25216;&#26415;&#39044;&#35745;&#23545;&#29289;&#29702;&#26426;&#22120;&#20154;&#31995;&#32479;&#26356;&#20855;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#20351;&#29992;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#39044;&#27979;&#26356;&#22909;&#30340;&#35745;&#21010;&#65292;&#24182;&#19988;&#35745;&#21010;&#39044;&#27979;&#21644;&#35745;&#21010;&#25191;&#34892;&#27169;&#22359;&#21487;&#33021;&#30456;&#20114;&#20381;&#36182;&#65292;&#22240;&#27492;&#23436;&#20840;&#35299;&#32806;&#21487;&#33021;&#19981;&#26159;&#29702;&#24819;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#29702;&#24819;&#35745;&#21010;&#30340;&#25191;&#34892;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#37327;&#21270;&#35745;&#21010;&#39044;&#27979;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task planning is an important component of traditional robotics systems enabling robots to compose fine grained skills to perform more complex tasks. Recent work building systems for translating natural language to executable actions for task completion in simulated embodied agents is focused on directly predicting low level action sequences that would be expected to be directly executable by a physical robot. In this work, we instead focus on predicting a higher level plan representation for one such embodied task completion dataset - TEACh, under the assumption that techniques for high-level plan prediction from natural language are expected to be more transferable to physical robot systems. We demonstrate that better plans can be predicted using multimodal context, and that plan prediction and plan execution modules are likely dependent on each other and hence it may not be ideal to fully decouple them. Further, we benchmark execution of oracle plans to quantify the scope for improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20027;&#35201;&#21360;&#24230;&#35821;&#35328;&#25991;&#26412;&#36716;&#25442;&#25104;Bharti&#30450;&#25991;&#23383;&#31526;&#30340;&#26041;&#26696;&#65292;&#20854;&#37319;&#29992;&#28151;&#21512;&#26041;&#27861;&#24182;&#36890;&#36807;LSTM&#27169;&#22411;&#35299;&#20915;&#27495;&#20041;&#65292;&#27979;&#35797;&#34920;&#26126;&#35813;&#27169;&#22411;&#20135;&#29983;&#20102;&#25509;&#36817;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06475</link><description>&lt;p&gt;
&#19968;&#31181;&#23558;&#21360;&#24230;&#35821;&#25991;&#26412;&#32763;&#35793;&#25104;Bharti&#30450;&#25991;&#23383;&#31526;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Model for Translation of Text from Indian Languages to Bharti Braille Characters. (arXiv:2305.06475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20027;&#35201;&#21360;&#24230;&#35821;&#35328;&#25991;&#26412;&#36716;&#25442;&#25104;Bharti&#30450;&#25991;&#23383;&#31526;&#30340;&#26041;&#26696;&#65292;&#20854;&#37319;&#29992;&#28151;&#21512;&#26041;&#27861;&#24182;&#36890;&#36807;LSTM&#27169;&#22411;&#35299;&#20915;&#27495;&#20041;&#65292;&#27979;&#35797;&#34920;&#26126;&#35813;&#27169;&#22411;&#20135;&#29983;&#20102;&#25509;&#36817;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#21147;&#21463;&#25439;&#32773;&#22312;&#23398;&#20064;&#26102;&#38754;&#20020;&#24456;&#22810;&#22256;&#38590;&#65292;&#20854;&#20013;&#19968;&#22823;&#21407;&#22240;&#26159;&#32570;&#20047;Bharti&#30450;&#25991;&#33050;&#26412;&#30340;&#21487;&#29992;&#25991;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20027;&#35201;&#21360;&#24230;&#35821;&#35328;&#30340;&#25991;&#26412;&#36716;&#25442;&#20026;Bharti&#30450;&#25991;&#30340;&#26041;&#26696;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#28151;&#21512;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#21360;&#24230;&#35821;&#25991;&#26412;&#36755;&#20837;&#21040;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;&#20013;&#65292;&#22914;&#26377;&#20219;&#20309;&#27495;&#20041;&#65292;&#21017;&#36890;&#36807;&#24212;&#29992;&#22522;&#20110;LSTM&#30340;&#27169;&#22411;&#35299;&#20915;&#12290;&#24320;&#21457;&#30340;&#27169;&#22411;&#20063;&#32463;&#36807;&#27979;&#35797;&#65292;&#24182;&#34987;&#21457;&#29616;&#20135;&#29983;&#20102;&#25509;&#36817;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
People who are visually impaired face a lot of difficulties while studying. One of the major causes to this is lack of available text in Bharti Braille script. In this paper, we have suggested a scheme to convert text in major Indian languages into Bharti Braille. The system uses a hybrid approach where at first the text in Indian language is given to a rule based system and in case if there is any ambiguity then it is resolved by applying a LSTM based model. The developed model has also been tested and found to have produced near accurate results.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;LSF-Models&#65289;&#22914;ChatGPT&#21644;DALLE-E&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#65288;PHM&#65289;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22823;&#37327;&#25968;&#25454;&#21644;&#36229;&#22823;&#27169;&#22411;&#33539;&#24335;&#65292;&#25104;&#20026;AI-2.0&#30340;&#26032;&#26102;&#20195;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2305.06472</link><description>&lt;p&gt;
ChatGPT&#24335;&#30340;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps. (arXiv:2305.06472v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;LSF-Models&#65289;&#22914;ChatGPT&#21644;DALLE-E&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#65288;PHM&#65289;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22823;&#37327;&#25968;&#25454;&#21644;&#36229;&#22823;&#27169;&#22411;&#33539;&#24335;&#65292;&#25104;&#20026;AI-2.0&#30340;&#26032;&#26102;&#20195;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#25216;&#26415;&#22312;&#24037;&#19994;&#29983;&#20135;&#21644;&#35774;&#22791;&#32500;&#25252;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#36890;&#36807;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;PHM&#25216;&#26415;&#35782;&#21035;&#21644;&#39044;&#27979;&#35774;&#22791;&#25925;&#38556;&#21644;&#25439;&#22351;&#12290;&#29616;&#22312;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;LSF-Models&#65289;&#22914;ChatGPT&#21644;DALLE-E&#30340;AI&#25216;&#26415;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#36229;&#22823;&#27169;&#22411;&#33539;&#24335;&#65292;&#25104;&#20026;AI-2.0&#30340;&#26032;&#26102;&#20195;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;&#36825;&#31181;&#25216;&#26415;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24037;&#19994;&#39046;&#22495;&#65292;&#22914;&#38081;&#36335;&#12289;&#33021;&#28304;&#21644;&#33322;&#31354;&#31561;&#65292;&#20197;&#25552;&#39640;&#35774;&#22791;&#30340;&#26381;&#21153;&#23551;&#21629;&#21644;&#21487;&#38752;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#29983;&#20135;&#25104;&#26412;&#21644;&#20572;&#26426;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prognostics and health management (PHM) technology plays a critical role in industrial production and equipment maintenance by identifying and predicting possible equipment failures and damages, thereby allowing necessary maintenance measures to be taken to enhance equipment service life and reliability while reducing production costs and downtime. In recent years, PHM technology based on artificial intelligence (AI) has made remarkable achievements in the context of the industrial IoT and big data, and it is widely used in various industries, such as railway, energy, and aviation, for condition monitoring, fault prediction, and health management. The emergence of large-scale foundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of AI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved from a research paradigm of single-modal, single-task, and limited-data to a multi-modal, multi-task, massive data, and super-large model paradigm. ChatGPT r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#35821;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#22270;&#22806;&#25991;&#26723;&#26102;&#36827;&#34892;&#24402;&#32435;&#25512;&#29702;&#65307;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#34920;&#26126;&#20102;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#32852;&#21512;&#24314;&#27169;&#35789;&#32423;&#21644;&#25991;&#26723;&#32423;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06434</link><description>&lt;p&gt;
&#22522;&#20110;&#35789;&#35821;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Word Grounded Graph Convolutional Network. (arXiv:2305.06434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35789;&#35821;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#22788;&#29702;&#22270;&#22806;&#25991;&#26723;&#26102;&#36827;&#34892;&#24402;&#32435;&#25512;&#29702;&#65307;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#34920;&#26126;&#20102;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#32852;&#21512;&#24314;&#27169;&#35789;&#32423;&#21644;&#25991;&#26723;&#32423;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#22312;&#23398;&#20064;&#25991;&#26412;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#24314;&#27169;&#22270;&#32467;&#26500;&#25968;&#25454;&#65288;&#22914;&#25991;&#29486;&#24341;&#29992;&#32593;&#32476;&#65289;&#26041;&#38754;&#65292;&#23545;&#20110;&#21508;&#31181;&#20219;&#21153;&#22914;&#25991;&#26412;&#20998;&#31867;&#31561;&#37117;&#26377;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;GCNs&#20165;&#38480;&#20110;&#22788;&#29702;&#39044;&#23450;&#20041;&#22270;&#20013;&#30340;&#25991;&#26723;&#65292;&#21363;&#19981;&#33021;&#25512;&#24191;&#21040;&#22270;&#22806;&#25991;&#26723;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#25991;&#26723;&#22270;&#36716;&#21270;&#20026;&#35789;&#22270;&#65292;&#36890;&#36807;&#20351;&#29992;&#29420;&#31435;&#20110;&#25991;&#26723;&#30340;&#22270;&#26469;&#35299;&#32806;&#25968;&#25454;&#26679;&#26412;&#65288;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#38598;&#20013;&#30340;&#25991;&#26723;&#65289;&#21644;GCN&#27169;&#22411;&#12290;&#36825;&#31181;&#22522;&#20110;&#35789;&#32423;&#30340;GCN&#22240;&#27492;&#21487;&#20197;&#33258;&#28982;&#22320;&#24402;&#32435;&#22320;&#25512;&#29702;&#20986;&#22270;&#22806;&#25991;&#26723;&#12290;&#25552;&#20986;&#20102;&#22522;&#20110;WGraph&#30340;&#24402;&#32435;&#35789;&#35821;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;WGCN&#65289;&#65292;&#23427;&#21487;&#20197;&#23545;&#21333;&#35789;&#32423;&#25991;&#26412;&#23454;&#20363;&#65288;&#22914;&#19981;&#22312;&#35821;&#26009;&#24211;&#20013;&#30340;&#25991;&#26723;&#65289;&#36827;&#34892;&#24402;&#32435;&#25512;&#29702;&#12290;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;WGCN&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#24418;&#26041;&#27861;&#32852;&#21512;&#24314;&#27169;&#35789;&#32423;&#21644;&#25991;&#26723;&#32423;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Convolutional Networks (GCNs) have shown strong performance in learning text representations for various tasks such as text classification, due to its expressive power in modeling graph structure data (e.g., a literature citation network). Most existing GCNs are limited to deal with documents included in a pre-defined graph, i.e., it cannot be generalized to out-of-graph documents. To address this issue, we propose to transform the document graph into a word graph, to decouple data samples (i.e., documents in training and test sets) and a GCN model by using a document-independent graph. Such word-level GCN could therefore naturally inference out-of-graph documents in an inductive way. The proposed Word-level Graph (WGraph) can not only implicitly learning word presentation with commonly-used word co-occurrences in corpora, but also incorporate extra global semantic dependency derived from inter-document relationships (e.g., literature citations). An inductive Word-grounded Graph 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLAIR&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#21644;&#22238;&#31572;&#26469;&#26816;&#27979;ChatGPT&#20013;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30495;&#23454;&#24615;&#65292;&#21487;&#20197;&#20998;&#31867;&#20154;&#21644;&#26426;&#22120;&#20154;&#12290;&#21333;&#38382;&#39064;&#20998;&#20026;&#23545;&#20110;&#20154;&#31867;&#32780;&#35328;&#23481;&#26131;&#20294;&#23545;&#20110;&#26426;&#22120;&#20154;&#24456;&#38590;&#21644;&#23545;&#20110;&#26426;&#22120;&#20154;&#32780;&#35328;&#23481;&#26131;&#20294;&#23545;&#20110;&#20154;&#31867;&#24456;&#38590;&#20004;&#20010;&#31867;&#21035;&#65292;&#20998;&#21035;&#36827;&#34892;&#26816;&#27979;&#12290; &#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06424</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#36824;&#26159;&#20154;&#31867;&#65311;&#29992;&#19968;&#20010;&#38382;&#39064;&#26816;&#27979;ChatGPT&#20882;&#21517;&#39030;&#26367;&#32773;
&lt;/p&gt;
&lt;p&gt;
Bot or Human? Detecting ChatGPT Imposters with A Single Question. (arXiv:2305.06424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLAIR&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#21644;&#22238;&#31572;&#26469;&#26816;&#27979;ChatGPT&#20013;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30495;&#23454;&#24615;&#65292;&#21487;&#20197;&#20998;&#31867;&#20154;&#21644;&#26426;&#22120;&#20154;&#12290;&#21333;&#38382;&#39064;&#20998;&#20026;&#23545;&#20110;&#20154;&#31867;&#32780;&#35328;&#23481;&#26131;&#20294;&#23545;&#20110;&#26426;&#22120;&#20154;&#24456;&#38590;&#21644;&#23545;&#20110;&#26426;&#22120;&#20154;&#32780;&#35328;&#23481;&#26131;&#20294;&#23545;&#20110;&#20154;&#31867;&#24456;&#38590;&#20004;&#20010;&#31867;&#21035;&#65292;&#20998;&#21035;&#36827;&#34892;&#26816;&#27979;&#12290; &#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#26368;&#36817;&#23637;&#31034;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20351;&#24471;&#32763;&#35793;&#12289;&#20889;&#20316;&#21644;&#38386;&#32842;&#31561;&#21508;&#31181;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#25285;&#24515;&#23427;&#20204;&#21487;&#33021;&#34987;&#28389;&#29992;&#20110;&#27450;&#35784;&#25110;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#31561;&#24694;&#24847;&#29992;&#36884;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26816;&#27979;&#32842;&#22825;&#20013;&#28041;&#21450;&#30340;&#21478;&#19968;&#26041;&#26159;&#26426;&#22120;&#20154;&#36824;&#26159;&#20154;&#31867;&#30340;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLAIR&#30340;&#26694;&#26550;&#65292;&#21363;&#36890;&#36807;&#21333;&#20010;&#38382;&#39064;&#21644;&#22238;&#31572;&#26469;&#26597;&#25214;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#65292;&#20197;&#22312;&#32447;&#26041;&#24335;&#26816;&#27979;&#20250;&#35805;&#20013;&#30340;&#23545;&#35805;&#26426;&#22120;&#20154;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#38024;&#23545;&#19968;&#20010;&#21333;&#19968;&#38382;&#39064;&#22330;&#26223;&#65292;&#35813;&#22330;&#26223;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#20154;&#31867;&#29992;&#25143;&#21644;&#26426;&#22120;&#20154;&#12290;&#36825;&#20123;&#38382;&#39064;&#20998;&#20026;&#20004;&#31867;&#65306;&#23545;&#20110;&#20154;&#31867;&#32780;&#35328;&#23481;&#26131;&#20294;&#23545;&#20110;&#26426;&#22120;&#20154;&#24456;&#38590;&#65288;&#20363;&#22914;&#35745;&#25968;&#12289;&#26367;&#25442;&#12289;&#23450;&#20301;&#12289;&#22122;&#38899;&#36807;&#28388;&#21644;ASCII&#33402;&#26415;&#65289;&#65292;&#20197;&#21450;&#23545;&#20110;&#26426;&#22120;&#20154;&#32780;&#35328;&#23481;&#26131;&#20294;&#23545;&#20110;&#20154;&#31867;&#24456;&#38590;&#65288;&#20363;&#22914;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#35782;&#21035;&#65289;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;FLAIR&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models like ChatGPT have recently demonstrated impressive capabilities in natural language understanding and generation, enabling various applications including translation, essay writing, and chit-chatting. However, there is a concern that they can be misused for malicious purposes, such as fraud or denial-of-service attacks. Therefore, it is crucial to develop methods for detecting whether the party involved in a conversation is a bot or a human. In this paper, we propose a framework named FLAIR, Finding Large language model Authenticity via a single Inquiry and Response, to detect conversational bots in an online manner. Specifically, we target a single question scenario that can effectively differentiate human users from bots. The questions are divided into two categories: those that are easy for humans but difficult for bots (e.g., counting, substitution, positioning, noise filtering, and ASCII art), and those that are easy for bots but difficult for humans (e.g., m
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#24207;&#21015;&#21040;&#24207;&#21015;&#21464;&#25442;&#27169;&#22411;&#36827;&#34892;&#21307;&#38498;&#36807;&#31243;&#23567;&#32467;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#20197;&#32531;&#35299;&#21307;&#29983;&#36807;&#21171;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#24615;&#19978;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#30450;&#35780;&#20272;&#34920;&#26126;62%&#30340;&#33258;&#21160;&#21270;&#25688;&#35201;&#31526;&#21512;&#26631;&#20934;&#65292;&#20855;&#26377;&#20020;&#24202;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.06416</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#31070;&#32463;&#31185;&#24739;&#32773;&#20986;&#38498;&#23567;&#32467;&#21307;&#38498;&#36807;&#31243;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Method to Automate the Discharge Summary Hospital Course for Neurology Patients. (arXiv:2305.06416v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06416
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#24207;&#21015;&#21040;&#24207;&#21015;&#21464;&#25442;&#27169;&#22411;&#36827;&#34892;&#21307;&#38498;&#36807;&#31243;&#23567;&#32467;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#20197;&#32531;&#35299;&#21307;&#29983;&#36807;&#21171;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#24615;&#19978;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#30450;&#35780;&#20272;&#34920;&#26126;62%&#30340;&#33258;&#21160;&#21270;&#25688;&#35201;&#31526;&#21512;&#26631;&#20934;&#65292;&#20855;&#26377;&#20020;&#24202;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20020;&#24202;&#31508;&#35760;&#30340;&#29983;&#25104;&#34987;&#25552;&#20986;&#20316;&#20026;&#32531;&#35299;&#21307;&#29983;&#36807;&#21171;&#30340;&#31574;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#30149;&#20154;&#20303;&#38498;&#26399;&#38388;&#33258;&#21160;&#21270;&#21465;&#36848;&#24615;&#25688;&#35201;&#21487;&#20316;&#20026;&#20303;&#38498;&#21307;&#29983;&#22312;&#30005;&#23376;&#30149;&#21382;&#31995;&#32479;&#20013;&#35760;&#24405;&#30340;&#20986;&#38498;&#23567;&#32467;&#20013;&#30340;&#21307;&#38498;&#36807;&#31243;&#37096;&#20998;&#30340;&#34917;&#20805;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#24207;&#21015;&#21040;&#24207;&#21015;&#21464;&#25442;&#27169;&#22411;&#36827;&#34892;&#21307;&#38498;&#36807;&#31243;&#23567;&#32467;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;BERT&#21644;BART&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#24182;&#36890;&#36807;&#38480;&#21046;&#27874;&#26463;&#25628;&#32034;&#36827;&#34892;&#20102;&#23454;&#38469;&#24615;&#20248;&#21270;&#65292;&#37319;&#29992;&#20102;&#20174;&#23398;&#26415;&#21307;&#30103;&#20013;&#24515;&#31070;&#32463;&#31185;&#30149;&#20154;&#30340;&#30005;&#23376;&#30149;&#21382;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;ROUGE&#20998;&#25968;&#21644;13.76&#30340;R-2&#12290;&#22312;&#30450;&#35780;&#20272;&#20013;&#65292;&#20004;&#20301;&#33891;&#20107;&#20250;&#35748;&#35777;&#30340;&#21307;&#29983;&#35780;&#20215;62%&#30340;&#33258;&#21160;&#21270;&#25688;&#35201;&#31526;&#21512;&#26631;&#20934;&#65292;&#36825;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#33021;&#22312;&#20020;&#24202;&#19978;&#26377;&#29992;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#30740;&#31350;&#26159;&#20854;&#20013;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generation of automated clinical notes have been posited as a strategy to mitigate physician burnout. In particular, an automated narrative summary of a patient's hospital stay could supplement the hospital course section of the discharge summary that inpatient physicians document in electronic health record (EHR) systems. In the current study, we developed and evaluated an automated method for summarizing the hospital course section using encoder-decoder sequence-to-sequence transformer models. We fine tuned BERT and BART models and optimized for factuality through constraining beam search, which we trained and tested using EHR data from patients admitted to the neurology unit of an academic medical center. The approach demonstrated good ROUGE scores with an R-2 of 13.76. In a blind evaluation, two board-certified physicians rated 62% of the automated summaries as meeting the standard of care, which suggests the method may be useful clinically. To our knowledge, this study is among th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;LACoS-BLOOM&#65292;&#37319;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#12289;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#21644;Siamese&#26550;&#26500;&#65292;&#33021;&#22815;&#29983;&#25104;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#21333;&#35789;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2305.06404</link><description>&lt;p&gt;
LACoS-BLOOM&#65306;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LACoS-BLOOM: Low-rank Adaptation with Contrastive objective on 8 bits Siamese-BLOOM. (arXiv:2305.06404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;LACoS-BLOOM&#65292;&#37319;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#12289;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#21644;Siamese&#26550;&#26500;&#65292;&#33021;&#22815;&#29983;&#25104;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#21333;&#35789;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#26159;&#20960;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#31243;&#24207;&#30340;&#26377;&#29992;&#29305;&#24449;&#65292;&#20363;&#22914;&#21477;&#23376;&#30456;&#20284;&#24615;&#12289;&#25991;&#26412;&#32858;&#31867;&#21644;&#35821;&#20041;&#25628;&#32034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#30340;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#22312;&#20855;&#26377;8&#20301;Siamese-BLOOM&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#65292;&#35813;&#27169;&#22411;&#20248;&#21270;&#20197;&#29983;&#25104;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#21333;&#35789;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text embeddings are useful features for several NLP applications, such as sentence similarity, text clustering, and semantic search. In this paper, we present a Low-rank Adaptation with a Contrastive objective on top of 8-bit Siamese-BLOOM, a multilingual large language model optimized to produce semantically meaningful word embeddings. The innovation is threefold. First, we cast BLOOM weights to 8-bit values. Second, we fine-tune BLOOM with a scalable adapter (LoRA) and 8-bit Adam optimizer for sentence similarity classification. Third, we apply a Siamese architecture on BLOOM model with a contrastive objective to ease the multi-lingual labeled data scarcity. The experiment results show the quality of learned embeddings from LACoS-BLOOM is proportional to the number of model parameters and the amount of unlabeled training data. With the parameter efficient fine-tuning design, we are able to run BLOOM 7.1 billion parameters end-to-end on a single GPU machine with 32GB memory. Compared 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;UVLN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#32763;&#35793;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#35270;&#35273;-&#35821;&#35328;&#23548;&#33322;&#65292;&#20854;&#32467;&#21512;&#20102;&#26368;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#23884;&#20837;&#25216;&#26415;&#65292;&#26088;&#22312;&#23558;&#25351;&#20196;&#36319;&#38543;&#20195;&#29702;&#31243;&#24207;&#30340;&#25104;&#21151;&#25512;&#24191;&#21040;&#38750;&#33521;&#35821;&#35821;&#35328;&#65292;&#25552;&#39640;&#20854;&#26131;&#25805;&#20316;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06358</link><description>&lt;p&gt;
&#8220;&#21487;&#35775;&#38382;&#30340;&#25351;&#20196;&#36319;&#38543;&#26426;&#22120;&#20154;&#8221;
&lt;/p&gt;
&lt;p&gt;
Accessible Instruction-Following Agent. (arXiv:2305.06358v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06358
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;UVLN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#32763;&#35793;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#35270;&#35273;-&#35821;&#35328;&#23548;&#33322;&#65292;&#20854;&#32467;&#21512;&#20102;&#26368;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#24418;&#23884;&#20837;&#25216;&#26415;&#65292;&#26088;&#22312;&#23558;&#25351;&#20196;&#36319;&#38543;&#20195;&#29702;&#31243;&#24207;&#30340;&#25104;&#21151;&#25512;&#24191;&#21040;&#38750;&#33521;&#35821;&#35821;&#35328;&#65292;&#25552;&#39640;&#20854;&#26131;&#25805;&#20316;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#26681;&#25454;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#20449;&#21495;&#21644;&#25351;&#20196;&#21512;&#20316;&#24182;&#23436;&#25104;&#20219;&#21153;&#12290;&#35757;&#32451;&#36825;&#26679;&#30340;&#26426;&#22120;&#20154;&#24456;&#38590;&#65292;&#29305;&#21035;&#26159;&#30001;&#20110;&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#21644;&#22797;&#26434;&#30340;&#29615;&#22659;&#12290;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#35821;&#26009;&#24211;&#20351;&#24471;&#20808;&#21069;&#30340;&#25351;&#20196;&#36319;&#38543;&#20195;&#29702;&#31243;&#24207;&#20559;&#21521;&#33521;&#35821;&#65292;&#20351;&#20854;&#26080;&#27861;&#24212;&#29992;&#20110;&#20351;&#29992;&#22810;&#31181;&#35821;&#35328;&#29978;&#33267;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#29992;&#25143;&#12290;&#28982;&#32780;&#65292;&#25351;&#20196;&#36319;&#38543;&#20195;&#29702;&#31243;&#24207;&#26159;&#22312;&#20551;&#35774;&#29992;&#25143;&#21487;&#20197;&#35266;&#23519;&#21040;&#29615;&#22659;&#30340;&#27169;&#24335;&#19979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#23558;&#25351;&#20196;&#36319;&#38543;&#20195;&#29702;&#31243;&#24207;&#30340;&#25104;&#21151;&#25512;&#24191;&#21040;&#38750;&#33521;&#35821;&#35821;&#35328;&#65292;&#24182;&#25913;&#21892;&#20854;&#19981;&#21487;&#25805;&#20316;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;UVLN (&#36890;&#29992;&#35270;&#35273;-&#35821;&#35328;&#23548;&#33322;), &#35813;&#26694;&#26550;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#32763;&#35793;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#35270;&#35273;-&#35821;&#35328;&#23548;&#33322;&#65292;&#20854;&#32467;&#21512;&#20102;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (&#22914; GPT3) &#19982;&#22270;&#24418;&#23884;&#20837;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can collaborate and complete tasks based on visual signals and instruction from the environment. Training such a robot is difficult especially due to the understanding of the instruction and the complicated environment. Previous instruction-following agents are biased to English-centric corpus, making it unrealizable to be applied to users that use multiple languages or even low-resource languages. Nevertheless, the instruction-following agents are pre-trained in a mode that assumes the user can observe the environment, which limits its accessibility. In this work, we're trying to generalize the success of instruction-following agents to non-English languages with little corpus resources, and improve its intractability and accessibility. We introduce UVLN (Universal Vision-Language Navigation), a novel machine-translation instructional augmented framework for cross-lingual vision-language navigation, with a novel composition of state-of-the-art large language model (GPT3) with t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;GPT-3&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#29983;&#25104;&#25991;&#31456;&#25688;&#35201;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#23545;&#21333;&#20010;&#25991;&#31456;&#30340;&#24635;&#32467;&#21644;&#31616;&#21270;&#25928;&#26524;&#36739;&#22909;&#65292;&#20294;&#22312;&#32508;&#21512;&#22810;&#31687;&#25991;&#31456;&#20013;&#25152;&#25253;&#21578;&#30340;&#35777;&#25454;&#26041;&#38754;&#34920;&#29616;&#27424;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.06299</link><description>&lt;p&gt;
&#20351;&#29992;GPT-3&#23545;&#21307;&#23398;&#35777;&#25454;&#36827;&#34892;&#24635;&#32467;&#12289;&#31616;&#21270;&#21644;&#32508;&#21512;&#65288;&#25104;&#26524;&#21442;&#24046;&#19981;&#40784;&#65289;
&lt;/p&gt;
&lt;p&gt;
Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success). (arXiv:2305.06299v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;GPT-3&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#29983;&#25104;&#25991;&#31456;&#25688;&#35201;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23427;&#23545;&#21333;&#20010;&#25991;&#31456;&#30340;&#24635;&#32467;&#21644;&#31616;&#21270;&#25928;&#26524;&#36739;&#22909;&#65292;&#20294;&#22312;&#32508;&#21512;&#22810;&#31687;&#25991;&#31456;&#20013;&#25152;&#25253;&#21578;&#30340;&#35777;&#25454;&#26041;&#38754;&#34920;&#29616;&#27424;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;GPT-3&#65292;&#33021;&#22815;&#22312;&#20960;&#20046;&#27809;&#26377;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#19968;&#27969;&#30340;&#26222;&#36890;&#39046;&#22495;&#26032;&#38395;&#25991;&#31456;&#25688;&#35201;&#12290;&#20294;&#26159;&#65292;&#23578;&#19981;&#28165;&#26970;&#36825;&#26679;&#30340;&#27169;&#22411;&#26159;&#21542;&#22312;&#26356;&#19987;&#19994;&#21644;&#39640;&#39118;&#38505;&#30340;&#39046;&#22495;&#65292;&#22914;&#29983;&#29289;&#21307;&#23398;&#20013;&#21516;&#26679;&#20855;&#22791;&#36825;&#26679;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35831;&#39046;&#22495;&#19987;&#23478;&#65288;&#20855;&#22791;&#21307;&#23398;&#22521;&#35757;&#30340;&#20154;&#65289;&#35780;&#20272;&#30001;GPT-3&#29983;&#25104;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#25688;&#35201;&#65292;&#24182;&#32771;&#34385;&#21333;&#19968;&#21644;&#22810;&#25991;&#26723;&#25688;&#35201;&#24773;&#20917;&#12290;&#21069;&#32773;&#20013;&#65292;GPT-3&#30340;&#20219;&#21153;&#26159;&#29983;&#25104;&#25551;&#36848;&#38543;&#26426;&#23545;&#29031;&#35797;&#39564;&#30340;&#25991;&#31456;&#30340;&#24120;&#35268;&#21644;&#31616;&#26126;&#35821;&#35328;&#25688;&#35201;&#65307;&#21518;&#32773;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;GPT-3&#22312;&#25972;&#20010;&#25991;&#31456;&#38598;&#20013;&#32508;&#21512;&#25253;&#21578;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#27880;&#37322;&#26041;&#26696;&#26469;&#35780;&#20272;&#27169;&#22411;&#36755;&#20986;&#65292;&#24182;&#37325;&#28857;&#35780;&#20272;&#29983;&#25104;&#25688;&#35201;&#30340;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;GPT-3&#33021;&#22815;&#24544;&#23454;&#22320;&#24635;&#32467;&#21644;&#31616;&#21270;&#21333;&#20010;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#65292;&#20294;&#23427;&#22312;&#32508;&#21512;&#22810;&#20010;&#25991;&#31456;&#25152;&#25552;&#20379;&#30340;&#35777;&#25454;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models, particularly GPT-3, are able to produce high quality summaries of general domain news articles in few- and zero-shot settings. However, it is unclear if such models are similarly capable in more specialized, high-stakes domains such as biomedicine. In this paper, we enlist domain experts (individuals with medical training) to evaluate summaries of biomedical articles generated by GPT-3, given zero supervision. We consider both single- and multi-document settings. In the former, GPT-3 is tasked with generating regular and plain-language summaries of articles describing randomized controlled trials; in the latter, we assess the degree to which GPT-3 is able to \emph{synthesize} evidence reported across a collection of articles. We design an annotation scheme for evaluating model outputs, with an emphasis on assessing the factual accuracy of generated summaries. We find that while GPT-3 is able to summarize and simplify single biomedical articles faithfully, it stru
&lt;/p&gt;</description></item><item><title>CodeIE&#25552;&#20986;&#20102;&#20351;&#29992;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65288;Code-LLMs&#65289;&#20195;&#26367;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65288;NL-LLMs&#65289;&#23545;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#36825;&#31867;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#21462;&#24471;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#39640;&#36798;4.5%&#30340;&#32477;&#23545;&#31934;&#24230;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.05711</link><description>&lt;p&gt;
CodeIE: &#22823;&#22411;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#20248;&#20110;&#23569;&#26679;&#26412;&#20449;&#24687;&#25552;&#21462;&#22120;
&lt;/p&gt;
&lt;p&gt;
CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors. (arXiv:2305.05711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05711
&lt;/p&gt;
&lt;p&gt;
CodeIE&#25552;&#20986;&#20102;&#20351;&#29992;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65288;Code-LLMs&#65289;&#20195;&#26367;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65288;NL-LLMs&#65289;&#23545;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#36825;&#31867;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#21462;&#24471;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#39640;&#36798;4.5%&#30340;&#32477;&#23545;&#31934;&#24230;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39044;&#35757;&#32451;&#26041;&#38754;&#65292;&#24050;&#32463;&#34920;&#29616;&#20986;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#20855;&#26377;&#24778;&#20154;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#24120;&#30340;&#20570;&#27861;&#26159;&#23558;&#20219;&#21153;&#37325;&#26500;&#20026;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#26684;&#24335;&#65292;&#20197;&#20415;&#33258;&#28982;&#35821;&#35328;&#30340;&#29983;&#25104;&#24335;LLMs&#65288;&#22914;GPT-3&#65289;&#21487;&#20197;&#34987;&#25552;&#31034;&#35299;&#20915;&#23427;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;NL-LLMs&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#20219;&#21153;&#26159;&#19981;&#26131;&#30340;&#65292;&#22240;&#20026;IE&#20219;&#21153;&#30340;&#36755;&#20986;&#36890;&#24120;&#26159;&#32467;&#26500;&#21270;&#30340;&#65292;&#22240;&#27492;&#24456;&#38590;&#36716;&#25442;&#25104;&#32431;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20195;&#30721;&#24418;&#24335;&#32780;&#38750;&#33258;&#28982;&#35821;&#35328;&#26469;&#34920;&#36798;&#32467;&#26500;&#21270;&#30340;&#36755;&#20986;&#65292;&#24182;&#21033;&#29992;&#20195;&#30721;&#29983;&#25104;LLMs&#65288;&#22914;Codex&#65289;&#26469;&#25191;&#34892;IE&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#12290;&#19982;NL-LLMs&#30456;&#27604;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#35774;&#35745;&#20195;&#30721;&#39118;&#26684;&#30340;&#25552;&#31034;&#21644;&#23558;&#36825;&#20123;IE&#20219;&#21153;&#26356;&#25913;&#20026;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;Code-LLMs&#21487;&#20197;&#19982;&#36825;&#20123;IE&#20219;&#21153;&#24456;&#22909;&#22320;&#23545;&#40784;&#12290;&#22312;&#19971;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#29615;&#22659;&#19979;&#19968;&#30452;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#36798;4.5%&#30340;&#32477;&#23545;&#31934;&#24230;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text. In this paper, we propose to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction. In contrast to NL-LLMs, we show that Code-LLMs can be well-aligned with these IE tasks by designing code-style prompts and formulating these IE tasks as code generation tasks. Experiment results on seven benchmarks show that our method consistently outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;SemEval 2023&#30340;&#20219;&#21153;&#19971;&#65292;&#26088;&#22312;&#36827;&#34892;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#35813;&#20219;&#21153;&#38590;&#24230;&#36739;&#22823;&#65292;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#30456;&#23545;&#20110;&#34164;&#21547;&#20219;&#21153;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.02993</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;7: &#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data. (arXiv:2305.02993v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;SemEval 2023&#30340;&#20219;&#21153;&#19971;&#65292;&#26088;&#22312;&#36827;&#34892;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65292;&#35813;&#20219;&#21153;&#38590;&#24230;&#36739;&#22823;&#65292;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#30456;&#23545;&#20110;&#34164;&#21547;&#20219;&#21153;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20171;&#32461;SemEval 2023&#20219;&#21153;7&#30340;&#32467;&#26524;&#65292;&#35813;&#20219;&#21153;&#20027;&#35201;&#28041;&#21450;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#20013;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI4CT&#65289;&#65292;&#30001;&#20004;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#65306;&#19968;&#20010;&#26159;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20219;&#21153;&#65292;&#21478;&#19968;&#20010;&#26159;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#12290;&#36825;&#20004;&#20010;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#21307;&#23398;&#21644;&#25968;&#23383;&#25512;&#29702;&#65292;&#36825;&#23545;&#20110;&#24320;&#21457;&#33021;&#22815;&#36827;&#34892;&#22823;&#35268;&#27169;&#21307;&#30103;&#35777;&#25454;&#35299;&#37322;&#21644;&#26816;&#32034;&#12289;&#25552;&#20379;&#20010;&#24615;&#21270;&#22522;&#20110;&#35777;&#25454;&#30340;&#20445;&#20581;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#31532;1&#20010;&#23376;&#20219;&#21153;&#8220;&#34164;&#21547;&#20219;&#21153;&#8221;&#25910;&#21040;&#20102;&#26469;&#33258;40&#20301;&#21442;&#36187;&#32773;&#30340;643&#20221;&#25552;&#20132;&#65292;&#31532;2&#20010;&#23376;&#20219;&#21153;&#8220;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#8221;&#25910;&#21040;&#20102;&#26469;&#33258;23&#20301;&#21442;&#36187;&#32773;&#30340;364&#20221;&#25552;&#20132;&#12290;&#36825;&#20004;&#20010;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22823;&#37096;&#20998;&#25552;&#20132;&#30340;&#31995;&#32479;&#22312;&#34164;&#21547;&#20219;&#21153;&#19978;&#26410;&#33021;&#26126;&#26174;&#20248;&#20110;&#22823;&#22810;&#25968;&#31867;&#22522;&#32447;&#65292;&#32780;&#25105;&#20204;&#35266;&#23519;&#21040;&#35777;&#25454;&#36873;&#25321;&#20219;&#21153;&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#34164;&#21547;&#20219;&#21153;&#12290;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#20250;&#23548;&#33268;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#26356;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the results of SemEval 2023 task 7 -- Multi-Evidence Natural Language Inference for Clinical Trial Data (NLI4CT) -- consisting of 2 tasks, a Natural Language Inference (NLI) task, and an evidence selection task on clinical trial data. The proposed challenges require multi-hop biomedical and numerical reasoning, which are of significant importance to the development of systems capable of large-scale interpretation and retrieval of medical evidence, to provide personalized evidence-based care.  Task 1, the entailment task, received 643 submissions from 40 participants, and Task 2, the evidence selection task, received 364 submissions from 23 participants. The tasks are challenging, with the majority of submitted systems failing to significantly outperform the majority class baseline on the entailment task, and we observe significantly better performance on the evidence selection task than on the entailment task. Increasing the number of model parameters leads to a di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25688;&#35201;&#25552;&#21462;&#26041;&#27861;DiffuSum&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25277;&#21462;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01735</link><description>&lt;p&gt;
DiffuSum&#65306;&#22522;&#20110;&#25193;&#25955;&#22686;&#24378;&#30340;&#25688;&#35201;&#25552;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffuSum: Generation Enhanced Extractive Summarization with Diffusion. (arXiv:2305.01735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25688;&#35201;&#25552;&#21462;&#26041;&#27861;DiffuSum&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#25277;&#21462;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#21462;&#24335;&#25688;&#35201;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#20174;&#28304;&#25991;&#20214;&#20013;&#25552;&#21462;&#21477;&#23376;&#26469;&#24418;&#25104;&#25688;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DiffuSum&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#25152;&#38656;&#30340;&#25688;&#35201;&#21477;&#23376;&#34920;&#31034;&#65292;&#24182;&#22522;&#20110;&#21477;&#23376;&#34920;&#31034;&#21305;&#37197;&#25552;&#21462;&#21477;&#23376;&#12290;&#27492;&#22806;&#65292;DiffuSum&#20849;&#21516;&#20248;&#21270;&#20102;&#23545;&#27604;&#21477;&#23376;&#32534;&#30721;&#22120;&#21644;&#21305;&#37197;&#25439;&#22833;&#65292;&#29992;&#20110;&#21477;&#23376;&#34920;&#31034;&#23545;&#40784;&#65292;&#20197;&#21450;&#29992;&#20110;&#34920;&#31034;&#22810;&#26679;&#24615;&#30340;&#22810;&#31867;&#23545;&#27604;&#25439;&#22833;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DiffuSum&#22312;CNN/DailyMail&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#25277;&#21462;&#32467;&#26524;&#65292;&#22312;ROUGE&#20998;&#25968;&#26041;&#38754;&#36798;&#21040;&#20102; $44.83/22.56/40.56$&#12290;&#23545;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#25688;&#35201;&#38271;&#24230;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#20063;&#35777;&#26126;&#20102;DiffuSum&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#24378;&#22823;&#24615;&#33021;&#34920;&#26126;&#20102;&#36866;&#24212;&#24403;&#21069;&#24773;&#20917;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extractive summarization aims to form a summary by directly extracting sentences from the source document. Existing works mostly formulate it as a sequence labeling problem by making individual sentence label predictions. This paper proposes DiffuSum, a novel paradigm for extractive summarization, by directly generating the desired summary sentence representations with diffusion models and extracting sentences based on sentence representation matching. In addition, DiffuSum jointly optimizes a contrastive sentence encoder with a matching loss for sentence representation alignment and a multi-class contrastive loss for representation diversity. Experimental results show that DiffuSum achieves the new state-of-the-art extractive results on CNN/DailyMail with ROUGE scores of $44.83/22.56/40.56$. Experiments on the other two datasets with different summary lengths also demonstrate the effectiveness of DiffuSum. The strong performance of our framework shows the great potential of adapting g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#20302;&#36164;&#28304;&#36866;&#24212;&#39064;&#30446;&#20013;&#20351;&#29992;&#30340;ASR&#21644;NLU&#30340;&#31649;&#36947;&#26041;&#27861;&#12290;&#22312;ASR&#20013;&#65292;&#20351;&#29992;&#19978;&#37319;&#26679;&#30340;Whisper&#23545;&#27599;&#20010;&#39046;&#22495;&#36827;&#34892;Feine-tune&#65307;&#22312;NLU&#20013;&#65292;&#20351;&#29992;MLM&#25216;&#26415;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#24182;&#20351;&#29992;&#22522;&#20110;&#26816;&#32034;&#30340;&#26041;&#27861;&#25193;&#20805;&#25968;&#25454;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#22312;&#25552;&#37266;/&#22825;&#27668;&#39046;&#22495;&#33719;&#24471;&#20102;&#39640;&#31934;&#30830;&#21305;&#37197;&#20934;&#30830;&#24230;&#24182;&#33719;&#24471;&#20102;&#25361;&#25112;&#30340;&#31532;&#19968;&#21517;&#12290;</title><link>http://arxiv.org/abs/2305.01194</link><description>&lt;p&gt;
&#22522;&#20110;MLM&#25968;&#25454;&#22686;&#24378;&#30340;ASR&#21644;NLU&#31649;&#36947;&#31995;&#32479;&#24212;&#23545;STOP&#20302;&#36164;&#28304;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
The Pipeline System of ASR and NLU with MLM-based Data Augmentation toward STOP Low-resource Challenge. (arXiv:2305.01194v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#20302;&#36164;&#28304;&#36866;&#24212;&#39064;&#30446;&#20013;&#20351;&#29992;&#30340;ASR&#21644;NLU&#30340;&#31649;&#36947;&#26041;&#27861;&#12290;&#22312;ASR&#20013;&#65292;&#20351;&#29992;&#19978;&#37319;&#26679;&#30340;Whisper&#23545;&#27599;&#20010;&#39046;&#22495;&#36827;&#34892;Feine-tune&#65307;&#22312;NLU&#20013;&#65292;&#20351;&#29992;MLM&#25216;&#26415;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#24182;&#20351;&#29992;&#22522;&#20110;&#26816;&#32034;&#30340;&#26041;&#27861;&#25193;&#20805;&#25968;&#25454;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#22312;&#25552;&#37266;/&#22825;&#27668;&#39046;&#22495;&#33719;&#24471;&#20102;&#39640;&#31934;&#30830;&#21305;&#37197;&#20934;&#30830;&#24230;&#24182;&#33719;&#24471;&#20102;&#25361;&#25112;&#30340;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;ICASSP&#20449;&#21495;&#22788;&#29702;&#22823;&#36187;2023&#30340;&#21475;&#35821;&#29702;&#35299;&#22823;&#25361;&#25112;&#65288;Spoken Language Understanding Grand Challenge&#65289;&#20302;&#36164;&#28304;&#39046;&#22495;&#36866;&#24212;&#36187;&#36947;&#65288;Track3&#65289;&#20013;&#37319;&#29992;&#30340;ASR&#21644;NLU&#30340;&#31649;&#36947;&#26041;&#27861;&#12290;&#38024;&#23545;ASR&#65292;&#25105;&#20204;&#20351;&#29992;&#19978;&#37319;&#26679; fine-tune Whisper &#20197;&#36866;&#24212;&#27599;&#20010;&#39046;&#22495;&#12290;&#38024;&#23545;NLU&#65292;&#25105;&#20204; fine-tune BART &#22312;&#25152;&#26377; Track3 &#25968;&#25454;&#19978;&#65292;&#28982;&#21518;&#22312;&#20302;&#36164;&#28304;&#22495;&#25968;&#25454;&#19978;&#36827;&#34892; fine-tune&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#22522;&#20110;&#36974;&#30422;&#30340;LM&#65288;MLM&#65289;&#25968;&#25454;&#22686;&#24378;&#65292;&#20854;&#20013;&#19968;&#20123;&#36755;&#20837;&#26631;&#35760;&#21644;&#30456;&#24212;&#30340;&#30446;&#26631;&#26631;&#31614;&#20351;&#29992; MLM &#36827;&#34892;&#26367;&#25442;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#22522;&#20110;&#26816;&#32034;&#30340;&#26041;&#27861;&#65292;&#27169;&#22411;&#36755;&#20837;&#19982;&#31867;&#20284;&#30340;&#35757;&#32451;&#26679;&#26412;&#19968;&#36215;&#36827;&#34892;&#22686;&#24378;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#22312;&#25552;&#37266;/&#22825;&#27668;&#39046;&#22495;&#23454;&#29616;&#20102;63.3 / 75.0&#65288;&#24179;&#22343;&#65306;69.15&#65289;&#30340;&#31934;&#30830;&#21305;&#37197;&#65288;EM&#65289;&#20934;&#30830;&#24230;&#65292;&#33719;&#24471;&#20102;&#35813;&#25361;&#25112;&#30340;&#31532;&#19968;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20026;&#23545;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;2023&#65289;&#30340;&#22238;&#24212;&#65292;&#20173;&#28982;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#22823;&#37327;&#30340;L2&#29992;&#25143;&#24433;&#21709;&#35821;&#35328;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00217</link><description>&lt;p&gt;
&#23545;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;2023&#24180;&#65289;&#30340;&#22238;&#24212;&#65306;&#20173;&#28982;&#27809;&#26377;&#35777;&#25454;&#35777;&#26126;&#38750;&#27597;&#35821;&#29992;&#25143;&#27604;&#20363;&#23545;&#35821;&#35328;&#22797;&#26434;&#24230;&#26377;&#24433;&#21709;&#65288;arXiv:2305.00217v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Still no evidence for an effect of the proportion of non-native speakers on language complexity -- A response to Kauhanen, Einhaus &amp; Walkden (2023). (arXiv:2305.00217v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20026;&#23545;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;2023&#65289;&#30340;&#22238;&#24212;&#65292;&#20173;&#28982;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#22823;&#37327;&#30340;L2&#29992;&#25143;&#24433;&#21709;&#35821;&#35328;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22312;&#12298;&#35821;&#35328;&#36827;&#21270;&#26434;&#24535;&#12299;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#20013;&#65292;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;https://doi.org/10.1093/jole/lzad005&#65292;KEW&#65289;&#25361;&#25112;&#20102;&#25105;&#22312;&#19968;&#31687;&#35770;&#25991;&#20013;&#65288;Koplenig&#65292;Royal Society Open Science&#65292;6&#65292;181274&#65288;2019&#65289;&#65292;https://doi.org/10.1098/rsos.181274&#65289;&#25152;&#21576;&#29616;&#30340;&#32467;&#26524;&#12290;&#22312;&#35813;&#35770;&#25991;&#20013;&#65292;&#25105;&#35797;&#22270;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#32479;&#35745;&#20998;&#26512;&#26469;&#34920;&#26126;&#22823;&#37327;L2&#65288;&#31532;&#20108;&#35821;&#35328;&#65289;&#29992;&#25143;&#20284;&#20046;&#19981;&#20250;&#24433;&#21709;&#35821;&#35328;&#30340;&#65288;&#35821;&#27861;&#25110;&#32479;&#35745;&#65289;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#19987;&#27880;&#20110;Ethnologue&#35780;&#20272;&#35821;&#35328;&#22320;&#20301;&#30340;&#26041;&#24335;&#65306;&#22914;&#26524;&#19968;&#31181;&#35821;&#35328;&#38500;&#20102;&#34987;L1&#65288;&#31532;&#19968;&#35821;&#35328;&#65289;&#20351;&#29992;&#32773;&#20043;&#22806;&#65292;&#36824;&#24212;&#35813;&#26377;&#22823;&#37327;&#30340;L2&#20351;&#29992;&#32773;&#65292;&#37027;&#20040;&#35813;&#35821;&#35328;&#23601;&#34987;&#25551;&#36848;&#20026;&#20256;&#25773;&#24615;&#30340;&#12290;KEW&#25209;&#35780;&#20102;&#23558;&#20256;&#25773;&#24615;&#20316;&#20026;&#35821;&#35328;&#26159;&#21542;&#25317;&#26377;&#22823;&#37327;L2&#20351;&#29992;&#32773;&#65288;&#20108;&#20803;&#65289;&#25351;&#26631;&#30340;&#20351;&#29992;&#65292;&#20197;&#21450;&#22312;&#30452;&#25509;&#20272;&#35745;L2&#27604;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;L2&#29992;&#25143;&#27604;&#20363;&#24402;&#20026;&#38750;&#20256;&#25773;&#24615;&#35821;&#35328;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a recent paper published in the Journal of Language Evolution, Kauhanen, Einhaus &amp; Walkden (https://doi.org/10.1093/jole/lzad005, KEW) challenge the results presented in one of my papers (Koplenig, Royal Society Open Science, 6, 181274 (2019), https://doi.org/10.1098/rsos.181274), in which I tried to show through a series of statistical analyses that large numbers of L2 (second language) speakers do not seem to affect the (grammatical or statistical) complexity of a language. To this end, I focus on the way in which the Ethnologue assesses language status: a language is characterised as vehicular if, in addition to being used by L1 (first language) speakers, it should also have a significant number of L2 users. KEW criticise both the use of vehicularity as a (binary) indicator of whether a language has a significant number of L2 users and the idea of imputing a zero proportion of L2 speakers to non-vehicular languages whenever a direct estimate of that proportion is unavailable. Whi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#21644;GPT-4&#65292;&#21253;&#25324;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21069;&#26223;&#24212;&#29992;&#65292;&#24182;&#30528;&#37325;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#21019;&#26032;&#12290;ChatGPT/GPT-4&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#39046;&#22495;&#20063;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01852</link><description>&lt;p&gt;
ChatGPT/GPT-4&#30740;&#31350;&#32508;&#36848;&#21450;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#26410;&#26469;&#30340;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models. (arXiv:2304.01852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#21644;GPT-4&#65292;&#21253;&#25324;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21069;&#26223;&#24212;&#29992;&#65292;&#24182;&#30528;&#37325;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#21019;&#26032;&#12290;ChatGPT/GPT-4&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#39046;&#22495;&#20063;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26469;&#33258;GPT&#31995;&#21015;&#30340;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;ChatGPT&#21644;GPT-4&#21450;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21069;&#26223;&#24212;&#29992;&#12290;&#23454;&#38469;&#19978;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26159;&#25552;&#39640;LLMs&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#30340;&#37325;&#35201;&#21019;&#26032;&#12290;&#25105;&#20204;&#22312;arXiv&#19978;&#28145;&#20837;&#20998;&#26512;&#20102;194&#31687;&#30456;&#20851;&#25991;&#29486;&#65292;&#21253;&#25324;&#36235;&#21183;&#20998;&#26512;&#12289;&#35789;&#20113;&#34920;&#29616;&#21644;&#22312;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#20998;&#24067;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;ChatGPT/GPT-4&#30740;&#31350;&#26174;&#33879;&#22686;&#38271;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#30452;&#25509;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#19978;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#22312;&#20174;&#25945;&#32946;&#21644;&#21382;&#21490;&#21040;&#25968;&#23398;&#12289;&#21307;&#23398;&#21644;&#29289;&#29702;&#31561;&#39046;&#22495;&#20855;&#26377;&#30456;&#24403;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#26377;&#20851;ChatGPT&#30340;&#33021;&#21147;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive survey of ChatGPT and GPT-4, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT/GPT-4 research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Beta-with-Spikes&#36924;&#36817;&#30340;Wright-Fisher&#27169;&#22411;&#30340;&#33258;&#21253;&#21547;&#25512;&#26029;&#26041;&#27861;&#65292;&#21487;&#20272;&#31639;&#36827;&#21270;&#21442;&#25968;&#65292;&#31283;&#20581;&#24615;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.04691</link><description>&lt;p&gt;
Wright-Fisher&#27169;&#22411;&#19979;&#33258;&#21253;&#21547;Beta-with-Spikes&#36924;&#36817;&#30340;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-contained Beta-with-Spikes Approximation for Inference Under a Wright-Fisher Model. (arXiv:2303.04691v2 [q-bio.PE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Beta-with-Spikes&#36924;&#36817;&#30340;Wright-Fisher&#27169;&#22411;&#30340;&#33258;&#21253;&#21547;&#25512;&#26029;&#26041;&#27861;&#65292;&#21487;&#20272;&#31639;&#36827;&#21270;&#21442;&#25968;&#65292;&#31283;&#20581;&#24615;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#20174;&#26102;&#24207;&#25968;&#25454;&#20013;&#20272;&#31639;Wright-Fisher&#27169;&#22411;&#20869;&#30340;&#36827;&#21270;&#21442;&#25968;&#65292;&#35813;&#27169;&#22411;&#25551;&#36848;&#20102;&#30001;&#20110;&#36873;&#25321;&#21644;&#22522;&#22240;&#28418;&#21464;&#23548;&#33268;&#30340;&#31561;&#20301;&#22522;&#22240;&#39057;&#29575;&#30340;&#21464;&#21270;&#12290;&#29983;&#29289;&#31181;&#32676;&#30340;&#25968;&#25454;&#23384;&#22312;&#36825;&#26679;&#30340;&#25968;&#25454;&#65292;&#20363;&#22914;&#36890;&#36807;&#20154;&#24037;&#36827;&#21270;&#23454;&#39564;&#65292;&#20197;&#21450;&#34892;&#20026;&#30340;&#25991;&#21270;&#36827;&#21270;&#65292;&#20363;&#22914;&#35760;&#36733;&#24102;&#26377;&#31867;&#20284;&#21547;&#20041;&#30340;&#19981;&#21516;&#21333;&#35789;&#21382;&#21490;&#29992;&#27861;&#30340;&#35821;&#35328;&#25991;&#29486;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26041;&#27861;&#24314;&#31435;&#22312;Wright-Fisher&#27169;&#22411;&#39044;&#27979;&#30340;&#31561;&#20301;&#22522;&#22240;&#39057;&#29575;&#20998;&#24067;&#30340;Beta-with-Spikes&#36924;&#36817;&#22522;&#30784;&#20043;&#19978;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#21253;&#21547;&#30340;&#26041;&#26696;&#26469;&#20272;&#35745;&#36924;&#36817;&#20013;&#30340;&#21442;&#25968;&#65292;&#24182;&#29992;&#21512;&#25104;&#25968;&#25454;&#28436;&#31034;&#20102;&#20854;&#22312;&#24378;&#36873;&#25321;&#21644;&#25509;&#36817;&#28781;&#32477;&#30340;&#24773;&#20917;&#19979;&#30340;&#31283;&#20581;&#24615;&#65292;&#36825;&#26159;&#20197;&#21069;&#26041;&#27861;&#22833;&#36133;&#30340;&#22320;&#26041;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24212;&#29992;&#20110;&#38754;&#21253;&#37237;&#27597;&#65288;Saccharomyces cerevisiae&#65289;&#31561;&#20301;&#22522;&#22240;&#39057;&#29575;&#25968;&#25454;&#65292;&#21457;&#29616;&#20102;&#25903;&#25345;&#29420;&#31435;&#35777;&#25454;&#30340;&#36873;&#25321;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
We construct a reliable estimation of evolutionary parameters within the Wright-Fisher model, which describes changes in allele frequencies due to selection and genetic drift, from time-series data. Such data exists for biological populations, for example via artificial evolution experiments, and for the cultural evolution of behavior, such as linguistic corpora that document historical usage of different words with similar meanings. Our method of analysis builds on a Beta-with-Spikes approximation to the distribution of allele frequencies predicted by the Wright-Fisher model. We introduce a self-contained scheme for estimating the parameters in the approximation, and demonstrate its robustness with synthetic data, especially in the strong-selection and near-extinction regimes where previous approaches fail. We further apply to allele frequency data for baker's yeast (Saccharomyces cerevisiae), finding a significant signal of selection in cases where independent evidence supports such 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#23383;&#31526;&#32423;&#21035;&#21644;&#23376;&#35789;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#23383;&#31526;&#32423;&#21035;&#24314;&#27169;&#22312;&#24418;&#20284;&#21333;&#35789;&#21644;&#31232;&#26377;&#21333;&#35789;&#30340;&#32763;&#35793;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2302.14220</link><description>&lt;p&gt;
&#23383;&#31526;&#32423;&#21035;&#30340;&#32763;&#35793;&#26159;&#21542;&#20540;&#24471;&#31561;&#24453;&#65311;&#23558;&#23383;&#31526;&#32423;&#21035;&#21644;&#23376;&#35789;&#32423;&#21035;&#27169;&#22411;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Are Character-level Translations Worth the Wait? Comparing Character- and Subword-level Models for Machine Translation. (arXiv:2302.14220v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#23383;&#31526;&#32423;&#21035;&#21644;&#23376;&#35789;&#32423;&#21035;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26426;&#22120;&#32763;&#35793;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#23383;&#31526;&#32423;&#21035;&#24314;&#27169;&#22312;&#24418;&#20284;&#21333;&#35789;&#21644;&#31232;&#26377;&#21333;&#35789;&#30340;&#32763;&#35793;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#39044;&#20808;&#35757;&#32451;&#30340;&#23383;&#31526;&#32423;&#21035;&#35821;&#35328;&#27169;&#22411;&#19982;&#27969;&#34892;&#30340;&#23376;&#35789;&#27169;&#22411;&#20855;&#26377;&#30456;&#24403;&#30340;&#31454;&#20105;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#38754;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#40092;&#26377;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#22312;&#22810;&#31181;&#35821;&#35328;&#21644;&#23454;&#39564;&#26465;&#20214;&#19979;&#65292;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;&#23383;&#31526;&#32423;&#21035;&#21644;&#23376;&#35789;&#32423;&#21035;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;&#20998;&#21035;&#20026;ByT5&#21644;mT5&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#25928;&#26524;&#65292;&#32467;&#26524;&#34920;&#26126;&#23383;&#31526;&#32423;&#21035;&#24314;&#27169;&#22312;&#32763;&#35793;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23383;&#31526;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#21319;&#22312;&#20110;&#26356;&#22909;&#22320;&#32763;&#35793;&#20102;&#24418;&#20284;&#21333;&#35789;&#21644;&#31232;&#26377;&#21333;&#35789;&#12290;&#22312;&#35780;&#20272;&#28304;&#25991;&#26412;&#22312;&#39537;&#21160;&#27169;&#22411;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#26102;&#65292;&#25105;&#20204;&#31361;&#20986;ByT5&#21333;&#35789;&#32423;&#21035;&#27169;&#24335;&#34920;&#26126;&#23383;&#31526;&#32423;&#21035;&#24314;&#27169;&#30340;&#28508;&#22312;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained character-level language models were recently shown to be competitive with popular subword models across a range of NLP tasks. However, there has been little research on their effectiveness for neural machine translation (NMT). This work performs an extensive comparison across multiple languages and experimental conditions of state-of-the-art character- and subword-level pre-trained models (ByT5 and mT5, respectively) on NMT, showing the effectiveness of character-level modeling in translation, particularly in cases where training data is limited. In our analysis, we show how character models' performance gains are reflected in better translations of orthographically similar words and rare words. While evaluating the importance of source texts in driving model predictions, we highlight ByT5 word-level patterns suggesting an ability to modulate word and character-level information during the translation, providing insights into a potential weakness of character-level modeling
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#22312;&#27861;&#23450;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#20854;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#26368;&#20339;&#32467;&#26524;&#65292;&#20294;&#20173;&#23384;&#22312;&#38169;&#35823;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;GPT-3&#23545;&#23454;&#38469;&#27861;&#35268;&#23384;&#22312;&#32570;&#38519;&#65292;&#19988;&#22312;&#23545;&#20110;&#21512;&#25104;&#27861;&#35268;&#30340;&#38382;&#39064;&#22238;&#31572;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2302.06100</link><description>&lt;p&gt;
GPT-3&#33021;&#36827;&#34892;&#27861;&#23450;&#25512;&#29702;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can GPT-3 Perform Statutory Reasoning?. (arXiv:2302.06100v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06100
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#22312;&#27861;&#23450;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#20854;&#34920;&#29616;&#20248;&#20110;&#20043;&#21069;&#26368;&#20339;&#32467;&#26524;&#65292;&#20294;&#20173;&#23384;&#22312;&#38169;&#35823;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;GPT-3&#23545;&#23454;&#38469;&#27861;&#35268;&#23384;&#22312;&#32570;&#38519;&#65292;&#19988;&#22312;&#23545;&#20110;&#21512;&#25104;&#27861;&#35268;&#30340;&#38382;&#39064;&#22238;&#31572;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#23450;&#25512;&#29702;&#26159;&#19968;&#31181;&#21033;&#29992;&#20107;&#23454;&#21644;&#30001;&#31435;&#27861;&#26426;&#26500;&#29992;&#33258;&#28982;&#35821;&#35328;&#20070;&#20889;&#30340;&#35268;&#21017;&#65288;&#21363;&#27861;&#35268;&#65289;&#36827;&#34892;&#25512;&#29702;&#30340;&#22522;&#26412;&#27861;&#24459;&#25216;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#24378;&#22823;&#30340;GPT-3&#27169;&#22411;text-davinci-003&#22312;&#19968;&#20010;&#21517;&#20026;SARA&#30340;&#24050;&#24314;&#31435;&#30340;&#27861;&#23450;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#21160;&#24577;&#23569;&#37327;&#31034;&#20363;&#25552;&#31034;&#12289;&#24605;&#32500;&#38142;&#25552;&#31034;&#21644;&#38646;&#26679;&#26412;&#25552;&#31034;&#12290;&#34429;&#28982;&#25105;&#20204;&#21462;&#24471;&#20102;&#27604;&#20808;&#21069;&#26368;&#20339;&#21457;&#34920;&#32467;&#26524;&#26356;&#22909;&#30340;GPT-3&#32467;&#26524;&#65292;&#20294;&#25105;&#20204;&#20063;&#30830;&#35748;&#20102;&#20854;&#20986;&#29616;&#20102;&#20960;&#31181;&#26126;&#26174;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#20123;&#38169;&#35823;&#30340;&#21407;&#22240;&#65292;&#24182;&#21457;&#29616;GPT-3&#23545;SARA&#22522;&#20110;&#23454;&#38469;&#32654;&#22269;&#27861;&#35268;&#30340;&#20808;&#39564;&#30693;&#35782;&#23384;&#22312;&#32570;&#38519;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#31616;&#21333;&#30340;&#21512;&#25104;&#27861;&#35268;&#65292;&#30830;&#20445;GPT-3&#22312;&#35757;&#32451;&#26399;&#38388;&#20174;&#26410;&#35265;&#36807;&#12290;&#25105;&#20204;&#21457;&#29616;GPT-3&#22312;&#22238;&#31572;&#20851;&#20110;&#36825;&#20123;&#31616;&#21333;&#21512;&#25104;&#27861;&#35268;&#30340;&#30452;&#25130;&#20102;&#24403;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statutory reasoning is the task of reasoning with facts and statutes, which are rules written in natural language by a legislature. It is a basic legal skill. In this paper we explore the capabilities of the most capable GPT-3 model, text-davinci-003, on an established statutory-reasoning dataset called SARA. We consider a variety of approaches, including dynamic few-shot prompting, chain-of-thought prompting, and zero-shot prompting. While we achieve results with GPT-3 that are better than the previous best published results, we also identify several types of clear errors it makes. We investigate why these errors happen. We discover that GPT-3 has imperfect prior knowledge of the actual U.S. statutes on which SARA is based. More importantly, we create simple synthetic statutes, which GPT-3 is guaranteed not to have seen during training. We find GPT-3 performs poorly at answering straightforward questions about these simple synthetic statutes.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyPe&#30340;&#24494;&#35843;&#25216;&#26415;&#65292;&#36890;&#36807;&#25200;&#21160;Transformer&#23618;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#25913;&#21892;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#25311;&#21512;&#25110;&#34920;&#31034;&#23849;&#28291;&#31561;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#26222;&#36890;&#24494;&#35843;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.08853</link><description>&lt;p&gt;
HyPe&#65306;&#20351;&#29992;&#38544;&#34255;&#34920;&#31034;&#25200;&#21160;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation. (arXiv:2212.08853v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HyPe&#30340;&#24494;&#35843;&#25216;&#26415;&#65292;&#36890;&#36807;&#25200;&#21160;Transformer&#23618;&#30340;&#38544;&#34255;&#34920;&#31034;&#65292;&#25913;&#21892;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#25311;&#21512;&#25110;&#34920;&#31034;&#23849;&#28291;&#31561;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#26222;&#36890;&#24494;&#35843;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#32467;&#26500;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#20173;&#28982;&#23384;&#22312;&#30528;&#36807;&#25311;&#21512;&#25110;&#34920;&#31034;&#23849;&#28291;&#31561;&#38382;&#39064;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;HyPe&#25216;&#26415;&#65292;&#36890;&#36807;&#25200;&#21160;Transformer&#23618;&#30340;&#38544;&#34255;&#34920;&#31034;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#19982;&#20808;&#21069;&#21482;&#28155;&#21152;&#22122;&#22768;&#21040;&#36755;&#20837;&#25110;&#21442;&#25968;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#35748;&#20026;Transformer&#23618;&#30340;&#38544;&#34255;&#34920;&#31034;&#20256;&#36798;&#20102;&#26356;&#22810;&#19981;&#21516;&#19988;&#26377;&#24847;&#20041;&#30340;&#35821;&#35328;&#20449;&#24687;&#65292;&#22240;&#27492;&#35753;Transformer&#23618;&#26356;&#21152;&#40065;&#26834;&#20110;&#38544;&#34255;&#34920;&#31034;&#30340;&#25200;&#21160;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;PLMs&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#22312;GLUE&#21644;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;HyPe&#20248;&#20110;&#26222;&#36890;&#30340;&#24494;&#35843;&#65292;&#24182;&#22686;&#24378;&#20102;&#26469;&#33258;&#19981;&#21516;&#23618;&#30340;&#38544;&#34255;&#34920;&#31034;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models with the Transformers structure have shown great performance in natural language processing. However, there still poses problems when fine-tuning pre-trained language models on downstream tasks, such as over-fitting or representation collapse. In this work, we propose HyPe, a simple yet effective fine-tuning technique to alleviate such problems by perturbing hidden representations of Transformers layers. Unlike previous works that only add noise to inputs or parameters, we argue that the hidden representations of Transformers layers convey more diverse and meaningful language information. Therefore, making the Transformers layers more robust to hidden representation perturbations can further benefit the fine-tuning of PLMs en bloc. We conduct extensive experiments and analyses on GLUE and other natural language inference datasets. Results demonstrate that HyPe outperforms vanilla fine-tuning and enhances generalization of hidden representations from different layers. In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21516;&#22768;&#32763;&#35793;&#20013;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31574;&#30053;&#65288;EDAtt&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#23454;&#26102;&#24341;&#23548;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#24471;&#20998;&#26469;&#25552;&#39640;&#21516;&#22768;&#32763;&#35793;&#24615;&#33021;&#12290;&#22312;&#33521;&#35793;&#24503;&#21644;&#33521;&#35793;&#35199;&#20219;&#21153;&#20013;&#65292;EDAtt &#31574;&#30053;&#20855;&#26377;&#35745;&#31639;&#24863;&#30693;&#24310;&#36831;&#20248;&#21183;&#24182;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.07850</link><description>&lt;p&gt;
&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#25552;&#21319;&#21516;&#22768;&#32763;&#35793;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Attention as a Guide for Simultaneous Speech Translation. (arXiv:2212.07850v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21516;&#22768;&#32763;&#35793;&#20013;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31574;&#30053;&#65288;EDAtt&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#23454;&#26102;&#24341;&#23548;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#24471;&#20998;&#26469;&#25552;&#39640;&#21516;&#22768;&#32763;&#35793;&#24615;&#33021;&#12290;&#22312;&#33521;&#35793;&#24503;&#21644;&#33521;&#35793;&#35199;&#20219;&#21153;&#20013;&#65292;EDAtt &#31574;&#30053;&#20855;&#26377;&#35745;&#31639;&#24863;&#30693;&#24310;&#36831;&#20248;&#21183;&#24182;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#30740;&#31350;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#27987;&#21402;&#30340;&#20852;&#36259;&#65292;&#22914;&#35821;&#35328;&#27169;&#22411;&#21644;&#26426;&#22120;&#32763;&#35793;&#12290;&#26412;&#25991;&#30740;&#31350;&#38024;&#23545;&#21516;&#22768;&#32763;&#35793;&#20013;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31574;&#30053;&#65288;EDAtt&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#23454;&#26102;&#24341;&#23548;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27880;&#24847;&#21147;&#24471;&#20998;&#26469;&#25552;&#39640;&#21516;&#22768;&#32763;&#35793;&#24615;&#33021;&#12290;&#22312;&#33521;&#35793;&#24503;&#21644;&#33521;&#35793;&#35199;&#20219;&#21153;&#20013;&#65292;&#19982;&#29616;&#26377;&#21516;&#22768;&#32763;&#35793;&#25216;&#26415;&#30456;&#27604;&#65292;EDAtt &#31574;&#30053;&#22312;&#35745;&#31639;&#24863;&#30693;&#24310;&#36831;&#31561;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study of the attention mechanism has sparked interest in many fields, such as language modeling and machine translation. Although its patterns have been exploited to perform different tasks, from neural network understanding to textual alignment, no previous work has analysed the encoder-decoder attention behavior in speech translation (ST) nor used it to improve ST on a specific task. In this paper, we fill this gap by proposing an attention-based policy (EDAtt) for simultaneous ST (SimulST) that is motivated by an analysis of the existing attention relations between audio input and textual output. Its goal is to leverage the encoder-decoder attention scores to guide inference in real time. Results on en-&gt;{de, es} show that the EDAtt policy achieves overall better results compared to the SimulST state of the art, especially in terms of computational-aware latency.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#21644;&#20869;&#22312;&#35821;&#20041;&#30340;&#21407;&#22411;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#21644;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#30340;&#36328;&#39046;&#22495;&#23567;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#26377;&#25928;&#22320;&#25552;&#21462;&#26032;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2212.02560</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#31034;&#23398;&#20064;&#21644;&#39046;&#22495;&#36866;&#24212;&#30340;&#22810;&#39046;&#22495;&#23567;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Few-Shot Relation Extraction via Representation Learning and Domain Adaptation. (arXiv:2212.02560v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#39564;&#30693;&#35782;&#21644;&#20869;&#22312;&#35821;&#20041;&#30340;&#21407;&#22411;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#21644;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#30340;&#36328;&#39046;&#22495;&#23567;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#26377;&#25928;&#22320;&#25552;&#21462;&#26032;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26088;&#22312;&#22312;&#27599;&#20010;&#20851;&#31995;&#20013;&#20165;&#26377;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#26032;&#30340;&#20851;&#31995;&#12290;&#20043;&#21069;&#22522;&#20110;&#24230;&#37327;&#30340;&#23569;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#31639;&#27861;&#36890;&#36807;&#23558;&#30001;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#23884;&#20837;&#29983;&#25104;&#30340;&#21407;&#22411;&#19982;&#20351;&#29992;&#35757;&#32451;&#30340;&#24230;&#37327;&#20989;&#25968;&#20998;&#26512;&#26597;&#35810;&#35821;&#21477;&#30340;&#23884;&#20837;&#36827;&#34892;&#27604;&#36739;&#20197;&#35782;&#21035;&#36825;&#20123;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#39046;&#22495;&#22987;&#32456;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#39046;&#22495;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#23545;&#26410;&#35265;&#20851;&#31995;&#30340;&#27867;&#21270;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20174;&#20808;&#39564;&#30693;&#35782;&#21644;&#20851;&#31995;&#30340;&#20869;&#22312;&#35821;&#20041;&#20013;&#23398;&#20064;&#26356;&#26131;&#35299;&#37322;&#21644;&#39640;&#25928;&#30340;&#21407;&#22411;&#65292;&#20197;&#26356;&#26377;&#25928;&#22320;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#25552;&#21462;&#26032;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20351;&#29992;&#20808;&#21069;&#20449;&#24687;&#25506;&#32034;&#20851;&#31995;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#20851;&#31995;&#30340;&#21407;&#22411;&#34920;&#31034;&#12290;&#36890;&#36807;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36328;&#39046;&#22495;&#23567;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#24102;&#26377;&#23569;&#37327;&#26631;&#35760;&#31034;&#20363;&#30340;&#26410;&#35265;&#39046;&#22495;&#20013;&#25552;&#21462;&#26032;&#30340;&#20851;&#31995;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#36328;&#39046;&#22495;&#23567;&#26679;&#26412;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot relation extraction aims to recognize novel relations with few labeled sentences in each relation. Previous metric-based few-shot relation extraction algorithms identify relationships by comparing the prototypes generated by the few labeled sentences embedding with the embeddings of the query sentences using a trained metric function. However, as these domains always have considerable differences from those in the training dataset, the generalization ability of these approaches on unseen relations in many domains is limited. Since the prototype is necessary for obtaining relationships between entities in the latent space, we suggest learning more interpretable and efficient prototypes from prior knowledge and the intrinsic semantics of relations to extract new relations in various domains more effectively. By exploring the relationships between relations using prior information, we effectively improve the prototype representation of relations. By using contrastive learning to 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;GLM2FSA&#65292;&#33021;&#22815;&#33258;&#21160;&#20174;&#20219;&#21153;&#30446;&#26631;&#30340;&#31616;&#30701;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20219;&#21153;&#30693;&#35782;&#24182;&#26500;&#24314;&#19968;&#20010;&#32534;&#30721;&#39640;&#23618;&#27425;&#20219;&#21153;&#30693;&#35782;&#30340;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65292;&#26500;&#24314;&#30340;&#33258;&#21160;&#26426;&#21487;&#20197;&#34987;&#27491;&#24335;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2212.01944</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#26426;&#34920;&#31034;&#20219;&#21153;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Automaton-Based Representations of Task Knowledge from Generative Language Models. (arXiv:2212.01944v3 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01944
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;GLM2FSA&#65292;&#33021;&#22815;&#33258;&#21160;&#20174;&#20219;&#21153;&#30446;&#26631;&#30340;&#31616;&#30701;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20219;&#21153;&#30693;&#35782;&#24182;&#26500;&#24314;&#19968;&#20010;&#32534;&#30721;&#39640;&#23618;&#27425;&#20219;&#21153;&#30693;&#35782;&#30340;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65292;&#26500;&#24314;&#30340;&#33258;&#21160;&#26426;&#21487;&#20197;&#34987;&#27491;&#24335;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#26426;&#30340;&#20219;&#21153;&#30693;&#35782;&#34920;&#31034;&#22312;&#25511;&#21046;&#21644;&#35268;&#21010;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#26500;&#24314;&#27492;&#31867;&#33258;&#21160;&#26426;&#25152;&#38656;&#30340;&#39640;&#23618;&#27425;&#20219;&#21153;&#30693;&#35782;&#36890;&#24120;&#24456;&#22256;&#38590;&#12290;&#21516;&#26102;&#65292;&#22823;&#35268;&#27169;&#33258;&#21160;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#30456;&#20851;&#20219;&#21153;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#36755;&#20986;&#19981;&#33021;&#27491;&#24335;&#39564;&#35777;&#25110;&#29992;&#20110;&#39034;&#24207;&#20915;&#31574;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;GLM2FSA&#30340;&#26032;&#31639;&#27861;&#65292;&#23427;&#20174;&#20219;&#21153;&#30446;&#26631;&#30340;&#31616;&#30701;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#26500;&#24314;&#19968;&#20010;&#32534;&#30721;&#39640;&#23618;&#27425;&#20219;&#21153;&#30693;&#35782;&#30340;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#65288;FSA&#65289;&#12290;GLM2FSA&#39318;&#20808;&#21521;GLM&#21457;&#36865;&#26597;&#35810;&#20197;&#25552;&#21462;&#25991;&#26412;&#24418;&#24335;&#30340;&#20219;&#21153;&#30693;&#35782;&#65292;&#28982;&#21518;&#23427;&#24314;&#31435;&#19968;&#20010;FSA&#26469;&#34920;&#31034;&#36825;&#31181;&#22522;&#20110;&#25991;&#26412;&#30340;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22635;&#34917;&#20102;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#21644;&#33258;&#21160;&#26426;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#26500;&#24314;&#30340;FSA&#21487;&#20197;&#38024;&#23545;&#29992;&#25143;&#23450;&#20041;&#30340;&#35268;&#26684;&#36827;&#34892;&#27491;&#24335;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automaton-based representations of task knowledge play an important role in control and planning for sequential decision-making problems. However, obtaining the high-level task knowledge required to build such automata is often difficult. Meanwhile, large-scale generative language models (GLMs) can automatically generate relevant task knowledge. However, the textual outputs from GLMs cannot be formally verified or used for sequential decision-making. We propose a novel algorithm named GLM2FSA, which constructs a finite state automaton (FSA) encoding high-level task knowledge from a brief natural-language description of the task goal. GLM2FSA first sends queries to a GLM to extract task knowledge in textual form, and then it builds an FSA to represent this text-based knowledge. The proposed algorithm thus fills the gap between natural-language task descriptions and automaton-based representations, and the constructed FSA can be formally verified against user-defined specifications. We a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;NLP&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20854;&#20013;CF&#39044;&#38450;&#12289;&#30693;&#35782;&#36801;&#31227;&#21644;&#36328;&#20219;&#21153;&#31867;&#20998;&#31163;&#31561;&#26041;&#38754;&#23545;NLP&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2211.12701</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#25345;&#32493;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Continual Learning of Natural Language Processing Tasks: A Survey. (arXiv:2211.12701v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;NLP&#20013;&#25345;&#32493;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20854;&#20013;CF&#39044;&#38450;&#12289;&#30693;&#35782;&#36801;&#31227;&#21644;&#36328;&#20219;&#21153;&#31867;&#20998;&#31163;&#31561;&#26041;&#38754;&#23545;NLP&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#27169;&#25311;&#20154;&#31867;&#19981;&#26029;&#23398;&#20064;&#21644;&#31215;&#32047;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#19981;&#20250;&#24536;&#35760;&#20043;&#21069;&#23398;&#36807;&#30340;&#30693;&#35782;&#65292;&#24182;&#23558;&#23398;&#21040;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#26032;&#20219;&#21153;&#26356;&#22909;&#22320;&#23398;&#20064;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;NLP&#20013;CL&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23427;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;CL&#26377;&#26174;&#30528;&#21306;&#21035;&#12290;&#23427;&#28085;&#30422;&#20102;&#65288;1&#65289;&#25152;&#26377;CL&#35774;&#32622;&#21450;&#29616;&#26377;&#25216;&#26415;&#20998;&#31867;&#65307;&#65288;2&#65289;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65307;&#65288;3&#65289;&#30693;&#35782;&#36801;&#31227;&#65288;KT&#65289;&#65292;&#23545;NLP&#20219;&#21153;&#23588;&#20854;&#37325;&#35201;&#65307;&#20197;&#21450;&#65288;4&#65289;&#19968;&#20123;&#29702;&#35770;&#21644;&#20132;&#20219;&#21153;&#31867;&#20998;&#31163;&#65288;ICS&#65289;&#30340;&#38544;&#21547;&#25361;&#25112;&#12290;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#30340;&#19968;&#20123;&#21015;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) is a learning paradigm that emulates the human capability of learning and accumulating knowledge continually without forgetting the previously learned knowledge and also transferring the learned knowledge to help learn new tasks better. This survey presents a comprehensive review and analysis of the recent progress of CL in NLP, which has significant differences from CL in computer vision and machine learning. It covers (1) all CL settings with a taxonomy of existing techniques; (2) catastrophic forgetting (CF) prevention, (3) knowledge transfer (KT), which is particularly important for NLP tasks; and (4) some theory and the hidden challenge of inter-task class separation (ICS). (1), (3) and (4) have not been included in the existing survey. Finally, a list of future directions is discussed.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#38477;&#20302;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#27979;&#35797;&#22312;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2211.08794</link><description>&lt;p&gt;
&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#30340;&#40065;&#26834;&#24615;&#20302;&#36164;&#28304;&#24494;&#35843;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Low-Resource Fine-Tuning with Multi-View Compressed Representations. (arXiv:2211.08794v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#38477;&#20302;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#36807;&#25311;&#21512;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#27979;&#35797;&#22312;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21442;&#25968;&#30340;&#24040;&#22823;&#25968;&#37327;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24494;&#35843;&#23481;&#26131;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#20986;&#29616;&#36807;&#24230;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;PLM&#30340;&#38544;&#34255;&#34920;&#31034;&#19978;&#25805;&#20316;&#65292;&#20197;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;PLM&#30340;&#38544;&#34255;&#23618;&#20043;&#38388;&#25554;&#20837;&#38543;&#26426;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#26469;&#33258;&#21069;&#19968;&#23618;&#30340;&#28608;&#27963;&#36716;&#25442;&#20026;&#22810;&#35270;&#35282;&#21387;&#32553;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#39304;&#36865;&#21040;&#19978;&#23618;&#12290;&#24494;&#35843;&#32467;&#26463;&#21518;&#65292;&#33258;&#32534;&#30721;&#22120;&#20250;&#34987;&#31227;&#38500;&#25481;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#20250;&#22686;&#21152;&#39069;&#22806;&#30340;&#21442;&#25968;&#25110;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#24207;&#21015;&#21644;&#26631;&#35760;&#32423;&#21035;&#30340;&#20302;&#36164;&#28304;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the huge amount of parameters, fine-tuning of pretrained language models (PLMs) is prone to overfitting in the low resource scenarios. In this work, we present a novel method that operates on the hidden representations of a PLM to reduce overfitting. During fine-tuning, our method inserts random autoencoders between the hidden layers of a PLM, which transform activations from the previous layers into a multi-view compressed representation before feeding it into the upper layers. The autoencoders are plugged out after fine-tuning, so our method does not add extra parameters or increase computation cost during inference. Our method demonstrates promising performance improvement across a wide range of sequence- and token-level low-resource NLP tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#32852;&#21512;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#35328;&#19981;&#27969;&#30021;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#22768;&#23398;&#20449;&#24687;&#20351;&#19981;&#27969;&#30021;&#26816;&#27979;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#38750;&#35821;&#35328;&#32447;&#32034;&#65292;&#38477;&#20302;&#20102;&#24310;&#36831;&#21644;&#25512;&#29702;&#36127;&#25285;&#12290;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#36755;&#20986;&#21487;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#29575;&#30340;&#19981;&#27969;&#30021;&#26816;&#27979;&#65292;&#24182;&#36991;&#20813;&#30001;&#20110;&#19981;&#27969;&#30021;&#24341;&#36215;&#30340;&#35782;&#21035;&#38169;&#35823;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.08726</link><description>&lt;p&gt;
&#23454;&#26102;&#32852;&#21512;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#35328;&#19981;&#27969;&#30021;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Streaming Joint Speech Recognition and Disfluency Detection. (arXiv:2211.08726v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#32852;&#21512;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#35328;&#19981;&#27969;&#30021;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#22768;&#23398;&#20449;&#24687;&#20351;&#19981;&#27969;&#30021;&#26816;&#27979;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#38750;&#35821;&#35328;&#32447;&#32034;&#65292;&#38477;&#20302;&#20102;&#24310;&#36831;&#21644;&#25512;&#29702;&#36127;&#25285;&#12290;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#36755;&#20986;&#21487;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#29575;&#30340;&#19981;&#27969;&#30021;&#26816;&#27979;&#65292;&#24182;&#36991;&#20813;&#30001;&#20110;&#19981;&#27969;&#30021;&#24341;&#36215;&#30340;&#35782;&#21035;&#38169;&#35823;&#65292;&#25552;&#39640;&#20102;&#25972;&#20307;&#35821;&#38899;&#35782;&#21035;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#19981;&#27969;&#30021;&#26816;&#27979;&#20027;&#35201;&#37319;&#29992;&#31649;&#36947;&#22788;&#29702;&#65292;&#20316;&#20026;&#35821;&#38899;&#35782;&#21035;&#30340;&#21518;&#22788;&#29702;&#27493;&#39588;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110; Transformer &#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#32852;&#21512;&#35299;&#20915;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#35328;&#19981;&#27969;&#30021;&#26816;&#27979;&#65292;&#20197;&#27969;&#24335;&#26041;&#24335;&#24037;&#20316;&#12290;&#19982;&#31649;&#36947;&#22788;&#29702;&#30456;&#27604;&#65292;&#32852;&#21512;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#22768;&#23398;&#20449;&#24687;&#65292;&#20351;&#35821;&#35328;&#19981;&#27969;&#30021;&#26816;&#27979;&#23545;&#35782;&#21035;&#38169;&#35823;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20379;&#38750;&#35821;&#35328;&#32447;&#32034;&#12290;&#27492;&#22806;&#65292;&#32852;&#21512;&#24314;&#27169;&#26377;&#20302;&#24310;&#36831;&#21644;&#36731;&#37327;&#32423;&#25512;&#29702;&#30340;&#29305;&#28857;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#27969;&#24335;&#35821;&#35328;&#19981;&#27969;&#30021;&#26816;&#27979;&#32852;&#21512;&#27169;&#22411;&#65306;&#25991;&#26412;&#22686;&#24378;&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#27169;&#22411;&#12290;&#25991;&#26412;&#22686;&#24378;&#27169;&#22411;&#26159;&#22312;&#24102;&#26377;&#29305;&#27530;&#26631;&#35760;&#30340;&#25991;&#26412;&#19978;&#35757;&#32451;&#30340;&#65292;&#25351;&#31034;&#19981;&#27969;&#30021;&#37096;&#20998;&#30340;&#36215;&#28857;&#21644;&#32456;&#28857;&#12290;&#28982;&#32780;&#65292;&#23427;&#23384;&#22312;&#26469;&#33258;&#39069;&#22806;&#30340;&#19981;&#27969;&#30021;&#26631;&#35760;&#30340;&#24310;&#36831;&#21644;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#30340;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#21516;&#26102;&#20855;&#26377;&#35821;&#38899;&#35782;&#21035;&#21644;&#35821;&#35328;&#19981;&#27969;&#30021;&#26816;&#27979;&#20004;&#20010;&#36755;&#20986;&#23618;&#12290;&#22810;&#20219;&#21153;&#27169;&#22411;&#36890;&#36807;&#36991;&#20813;&#30001;&#20110;&#19981;&#27969;&#30021;&#32780;&#24341;&#36215;&#30340;&#35782;&#21035;&#38169;&#35823;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#29575;&#30340;&#19981;&#27969;&#30021;&#26816;&#27979;&#65292;&#24182;&#25913;&#21892;&#20102;&#25972;&#20307;&#30340;&#35821;&#38899;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disfluency detection has mainly been solved in a pipeline approach, as post-processing of speech recognition. In this study, we propose Transformer-based encoder-decoder models that jointly solve speech recognition and disfluency detection, which work in a streaming manner. Compared to pipeline approaches, the joint models can leverage acoustic information that makes disfluency detection robust to recognition errors and provide non-verbal clues. Moreover, joint modeling results in low-latency and lightweight inference. We investigate two joint model variants for streaming disfluency detection: a transcript-enriched model and a multi-task model. The transcript-enriched model is trained on text with special tags indicating the starting and ending points of the disfluent part. However, it has problems with latency and standard language model adaptation, which arise from the additional disfluency tags. We propose a multi-task model to solve such problems, which has two output layers at the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#26174;&#33879;&#24615;&#26041;&#27861;&#25581;&#31034;&#31070;&#32463;NLP&#27169;&#22411;&#40657;&#21283;&#23376;&#65292;&#21327;&#35758;&#35780;&#20272;&#24182;&#19981;&#33021;&#20445;&#35777;&#21487;&#38752;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;Pearson-r&#26356;&#36866;&#21512;&#30340;&#26367;&#20195;&#26041;&#26696;&#23454;&#29616;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#27491;&#21017;&#21270;&#25216;&#26415;&#25552;&#39640;&#27880;&#24847;&#21147;&#35299;&#37322;&#30340;&#24544;&#23454;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.08369</link><description>&lt;p&gt;
&#26131;&#20110;&#20915;&#23450;&#65292;&#38590;&#20197;&#36798;&#25104;&#19968;&#33268;&#65306;&#20943;&#23569;&#26174;&#33879;&#24615;&#26041;&#27861;&#20043;&#38388;&#30340;&#20998;&#27495;
&lt;/p&gt;
&lt;p&gt;
Easy to Decide, Hard to Agree: Reducing Disagreements Between Saliency Methods. (arXiv:2211.08369v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21033;&#29992;&#26174;&#33879;&#24615;&#26041;&#27861;&#25581;&#31034;&#31070;&#32463;NLP&#27169;&#22411;&#40657;&#21283;&#23376;&#65292;&#21327;&#35758;&#35780;&#20272;&#24182;&#19981;&#33021;&#20445;&#35777;&#21487;&#38752;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;Pearson-r&#26356;&#36866;&#21512;&#30340;&#26367;&#20195;&#26041;&#26696;&#23454;&#29616;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#36890;&#36807;&#27491;&#21017;&#21270;&#25216;&#26415;&#25552;&#39640;&#27880;&#24847;&#21147;&#35299;&#37322;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#31070;&#32463;NLP&#27169;&#22411;&#40657;&#21283;&#23376;&#30340;&#27969;&#34892;&#26041;&#27861;&#26159;&#21033;&#29992;&#26174;&#33879;&#24615;&#26041;&#27861;&#65292;&#23427;&#20204;&#20026;&#27599;&#20010;&#36755;&#20837;&#32452;&#20214;&#20998;&#37197;&#26631;&#37327;&#37325;&#35201;&#24615;&#20998;&#25968;&#12290;&#35780;&#20272;&#35299;&#37322;&#24615;&#26041;&#27861;&#26159;&#21542;&#24544;&#23454;&#30340;&#24120;&#35265;&#20570;&#27861;&#26159;&#20351;&#29992;&#8220;&#21327;&#35758;&#35780;&#20272;&#8221;&#8212;&#8212;&#22914;&#26524;&#22810;&#31181;&#26041;&#27861;&#23545;&#35299;&#37322;&#36798;&#25104;&#19968;&#33268;&#65292;&#20854;&#21487;&#20449;&#24230;&#23601;&#20250;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#21516;&#19968;&#27169;&#22411;&#23454;&#20363;&#65292;&#26174;&#33879;&#24615;&#26041;&#27861;&#20063;&#34920;&#29616;&#20986;&#36739;&#24369;&#30340;&#31209;&#30456;&#20851;&#24615;&#65292;&#24182;&#20513;&#23548;&#20351;&#29992;&#26367;&#20195;&#35786;&#26029;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31209;&#30456;&#20851;&#24615;&#19981;&#36866;&#21512;&#35780;&#20272;&#19968;&#33268;&#24615;&#65292;&#24182;&#35748;&#20026;Pearson-r&#26159;&#26356;&#36866;&#21512;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#22686;&#21152;&#27880;&#24847;&#21147;&#35299;&#37322;&#30340;&#24544;&#23454;&#24230;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#20063;&#20250;&#22686;&#21152;&#26174;&#33879;&#24615;&#26041;&#27861;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#21457;&#29616;&#19982;&#22522;&#20110;&#22521;&#35757;&#21160;&#24577;&#30340;&#23454;&#20363;&#31867;&#21035;&#30456;&#36830;&#25509;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#26576;&#20123;&#31867;&#21035;&#30340;&#26174;&#33879;&#24615;&#26041;&#27861;&#35299;&#37322;&#30340;&#19968;&#33268;&#24615;&#38750;&#24120;&#20302;&#65292;&#20363;&#22914;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
A popular approach to unveiling the black box of neural NLP models is to leverage saliency methods, which assign scalar importance scores to each input component. A common practice for evaluating whether an interpretability method is faithful has been to use evaluation-by-agreement -- if multiple methods agree on an explanation, its credibility increases. However, recent work has found that saliency methods exhibit weak rank correlations even when applied to the same model instance and advocated for the use of alternative diagnostic methods. In our work, we demonstrate that rank correlation is not a good fit for evaluating agreement and argue that Pearson-$r$ is a better-suited alternative. We further show that regularization techniques that increase faithfulness of attention explanations also increase agreement between saliency methods. By connecting our findings to instance categories based on training dynamics, we show that the agreement of saliency method explanations is very low f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FolkScope&#24847;&#22270;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#26426;&#20132;&#20114;&#27880;&#37322;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#29992;&#20110;&#25581;&#31034;&#26377;&#20851;&#36141;&#20080;&#29289;&#21697;&#30340;&#20154;&#31867;&#24605;&#32500;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2211.08316</link><description>&lt;p&gt;
FolkScope: &#38754;&#21521;&#30005;&#21830;&#24120;&#35782;&#21457;&#29616;&#30340;&#24847;&#22270;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery. (arXiv:2211.08316v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FolkScope&#24847;&#22270;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#26426;&#20132;&#20114;&#27880;&#37322;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#29992;&#20110;&#25581;&#31034;&#26377;&#20851;&#36141;&#20080;&#29289;&#21697;&#30340;&#20154;&#31867;&#24605;&#32500;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#30005;&#21830;&#24179;&#21488;&#29992;&#25143;&#30340;&#24847;&#22270;&#38656;&#35201;&#24120;&#35782;&#30693;&#35782;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FolkScope&#65292;&#19968;&#31181;&#24847;&#22270;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#26694;&#26550;&#65292;&#29992;&#20110;&#25581;&#31034;&#26377;&#20851;&#36141;&#20080;&#29289;&#21697;&#30340;&#20154;&#31867;&#24605;&#32500;&#32467;&#26500;&#12290;&#30001;&#20110;&#24120;&#35782;&#30693;&#35782;&#36890;&#24120;&#19981;&#26131;&#35328;&#20256;&#19988;&#26410;&#26126;&#30830;&#34920;&#36798;&#65292;&#22240;&#27492;&#25191;&#34892;&#20449;&#24687;&#25552;&#21462;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#20154;&#26426;&#20132;&#20114;&#27880;&#37322;&#26469;&#21322;&#33258;&#21160;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#12290;&#39318;&#20808;&#65292;LLM&#36890;&#36807;&#30005;&#21830;&#29305;&#23450;&#25552;&#31034;&#29983;&#25104;&#24847;&#22270;&#26029;&#35328;&#65292;&#20197;&#35299;&#37322;&#36141;&#29289;&#34892;&#20026;&#65292;&#20854;&#20013;&#24847;&#22270;&#21487;&#20197;&#26159;&#24320;&#25918;&#21407;&#22240;&#25110;&#33853;&#20837;&#19982;ConceptNet&#23545;&#40784;&#30340;18&#20010;&#31867;&#21035;&#20043;&#19968;&#30340;&#35859;&#35789;&#65292;&#20363;&#22914;&#65306;&#26159;&#65292;&#30001;...&#21046;&#25104;&#65292;&#29992;&#20110;...&#31561;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#38543;&#26426;&#25277;&#26679;&#30340;&#24847;&#22270;&#36827;&#34892;&#21512;&#29702;&#24615;&#21644;&#20856;&#22411;&#24615;&#26631;&#27880;&#65292;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#20415;&#23558;&#20154;&#31867;&#21028;&#26029;&#24212;&#29992;&#20110;&#25152;&#26377;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#23558;&#26029;&#35328;&#32467;&#26500;&#21270;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26080;&#21521;&#22270;&#31070;&#32463;&#32593;&#32476;&#23558;&#24847;&#22270;&#20851;&#31995;&#26144;&#23556;&#21040;&#30693;&#35782;&#22270;&#35889;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding users' intentions in e-commerce platforms requires commonsense knowledge. In this paper, we present FolkScope, an intention knowledge graph construction framework to reveal the structure of humans' minds about purchasing items. As commonsense knowledge is usually ineffable and not expressed explicitly, it is challenging to perform information extraction. Thus, we propose a new approach that leverages the generation power of large language models~(LLMs) and human-in-the-loop annotation to semi-automatically construct the knowledge graph. LLMs first generate intention assertions via e-commerce-specific prompts to explain shopping behaviors, where the intention can be an open reason or a predicate falling into one of 18 categories aligning with ConceptNet, e.g., IsA, MadeOf, UsedFor, etc. Then we annotate plausibility and typicality labels of sampled intentions as training data in order to populate human judgments to all automatic generations. Last, to structurize the assert
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;OLDS&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20026;&#23545;&#35805;&#25688;&#35201;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30465;&#30053;&#26631;&#31614;&#12290;&#36890;&#36807;&#20998;&#26512;&#35813;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#36890;&#36807;&#20026;&#25688;&#35201;&#27169;&#22411;&#25552;&#20379;&#30495;&#23454;&#30465;&#30053;&#26631;&#31614;&#65292;&#24182;&#22312;&#25688;&#35201;&#36807;&#31243;&#20013;&#26174;&#24335;&#22320;&#24314;&#27169;&#21644;&#35299;&#20915;&#30465;&#30053;&#38382;&#39064;&#65292;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#25688;&#35201;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.07145</link><description>&lt;p&gt;
&#25506;&#31350;&#23545;&#35805;&#25688;&#35201;&#20013;&#30340;&#30465;&#30053;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Omission in Dialogue Summarization. (arXiv:2211.07145v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07145
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;OLDS&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20026;&#23545;&#35805;&#25688;&#35201;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#30465;&#30053;&#26631;&#31614;&#12290;&#36890;&#36807;&#20998;&#26512;&#35813;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#36890;&#36807;&#20026;&#25688;&#35201;&#27169;&#22411;&#25552;&#20379;&#30495;&#23454;&#30465;&#30053;&#26631;&#31614;&#65292;&#24182;&#22312;&#25688;&#35201;&#36807;&#31243;&#20013;&#26174;&#24335;&#22320;&#24314;&#27169;&#21644;&#35299;&#20915;&#30465;&#30053;&#38382;&#39064;&#65292;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#25688;&#35201;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25688;&#35201;&#30340;&#30446;&#26631;&#26159;&#23558;&#20887;&#38271;&#30340;&#23545;&#35805;&#21387;&#32553;&#25104;&#31616;&#32451;&#30340;&#25688;&#35201;&#65292;&#24182;&#22312;&#36817;&#26399;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#36317;&#31163;&#20196;&#20154;&#28385;&#24847;&#20173;&#26377;&#24456;&#22823;&#24046;&#36317;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30465;&#30053;&#26159;&#24433;&#21709;&#25688;&#35201;&#36136;&#37327;&#30340;&#20027;&#35201;&#22240;&#32032;&#20043;&#19968;&#65292;&#20294;&#26159;&#24456;&#23569;&#26377;&#30740;&#31350;&#36827;&#19968;&#27493;&#25506;&#35752;&#30465;&#30053;&#38382;&#39064;&#65292;&#20363;&#22914;&#30465;&#30053;&#22914;&#20309;&#24433;&#21709;&#25688;&#35201;&#32467;&#26524;&#20197;&#21450;&#22914;&#20309;&#26816;&#27979;&#30465;&#30053;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#20943;&#23569;&#30465;&#30053;&#24182;&#25552;&#39640;&#25688;&#35201;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#20998;&#26512;&#21644;&#26816;&#27979;&#30465;&#30053;&#20381;&#36182;&#20110;&#20855;&#26377;&#30465;&#30053;&#26631;&#31614;&#30340;&#25688;&#35201;&#25968;&#25454;&#38598;&#65288;&#21363;&#65292;&#21738;&#20123;&#23545;&#35805;&#35805;&#35821;&#22312;&#25688;&#35201;&#20013;&#34987;&#30465;&#30053;&#65289;&#65292;&#32780;&#24403;&#21069;&#25991;&#29486;&#20013;&#24182;&#27809;&#26377;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;OLDS&#25968;&#25454;&#38598;&#65292;&#20026;&#23545;&#35805;&#25688;&#35201;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#30465;&#30053;&#26631;&#31614;&#12290;&#36890;&#36807;&#20998;&#26512;&#35813;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#20026;&#25688;&#35201;&#27169;&#22411;&#25552;&#20379;&#30495;&#23454;&#30465;&#30053;&#26631;&#31614;&#65292;&#24182;&#22312;&#25688;&#35201;&#36807;&#31243;&#20013;&#26174;&#24335;&#22320;&#24314;&#27169;&#21644;&#35299;&#20915;&#30465;&#30053;&#38382;&#39064;&#65292;&#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#25688;&#35201;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue summarization aims to condense the lengthy dialogue into a concise summary, and has recently achieved significant progress. However, the result of existing methods is still far from satisfactory. Previous works indicated that omission is a major factor in affecting the quality of summarization, but few of them have further explored the omission problem, such as how omission affects summarization results and how to detect omission, which is critical for reducing omission and improving summarization quality. Moreover, analyzing and detecting omission relies on summarization datasets with omission labels (i.e., which dialogue utterances are omitted in the summarization), which are not available in the current literature. In this paper, we propose the OLDS dataset, which provides high-quality Omission Labels for Dialogue Summarization. By analyzing this dataset, we find that a large improvement in summarization quality can be achieved by providing ground-truth omission labels for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21551;&#31034;&#31070;&#32463;&#32593;&#32476;&#65288;ENN&#65289;&#26469;&#20248;&#20808;&#32771;&#34385;&#26377;&#20449;&#24687;&#20215;&#20540;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#26356;&#23569;&#30340;&#26631;&#31614;&#25968;&#25454;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#20351;&#29992; ENN &#37117;&#27604;&#24120;&#35268;&#30340;&#21551;&#21457;&#24335;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2211.01568</link><description>&lt;p&gt;
&#36890;&#36807;&#21551;&#31034;&#31070;&#32463;&#32593;&#32476;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning Language Models via Epistemic Neural Networks. (arXiv:2211.01568v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21551;&#31034;&#31070;&#32463;&#32593;&#32476;&#65288;ENN&#65289;&#26469;&#20248;&#20808;&#32771;&#34385;&#26377;&#20449;&#24687;&#20215;&#20540;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;&#26356;&#23569;&#30340;&#26631;&#31614;&#25968;&#25454;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#20351;&#29992; ENN &#37117;&#27604;&#24120;&#35268;&#30340;&#21551;&#21457;&#24335;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20250;&#22312;&#22823;&#35268;&#27169;&#30340;&#26080;&#30417;&#30563;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#30340;&#24494;&#35843;&#26041;&#27861;&#24182;&#19981;&#37325;&#35270;&#25152;&#24494;&#35843;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#20320;&#33021;&#22815;&#23558;&#26377;&#20449;&#24687;&#20215;&#20540;&#30340;&#35757;&#32451;&#25968;&#25454;&#25918;&#22312;&#20248;&#20808;&#32771;&#34385;&#30340;&#20301;&#32622;&#19978;&#65292;&#23601;&#21487;&#20197;&#22312;&#20351;&#29992;&#26356;&#23569;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#25105;&#20204;&#23558;&#35821;&#35328;&#27169;&#22411;&#22686;&#24191;&#20102;&#19968;&#20010; epinet&#65292;&#36825;&#26159;&#19968;&#20010;&#36741;&#21161;&#20272;&#31639;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#24182;&#24418;&#25104;&#19968;&#20010;&#21551;&#31034;&#31070;&#32463;&#32593;&#32476;&#65288;ENN&#65289;&#30340;&#23567;&#22411;&#39069;&#22806;&#32593;&#32476;&#12290;ENN&#26159;&#33021;&#22815;&#30693;&#36947;&#33258;&#24049;&#30340;&#19981;&#36275;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010; epinet &#26469;&#20248;&#20808;&#32771;&#34385;&#19981;&#30830;&#23450;&#25968;&#25454;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558; BERT &#23545; GLUE &#20219;&#21153;&#30340;&#24494;&#35843;&#24615;&#33021;&#25552;&#39640;&#21040;&#19982;&#19981;&#36827;&#34892;&#20248;&#20808;&#32771;&#34385;&#35757;&#32451;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#25968;&#25454;&#26631;&#31614;&#25968;&#37327;&#20943;&#21322;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#21512;&#25104;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992; epinet &#37117;&#20248;&#20110;&#21551;&#21457;&#24335;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models often pre-train on large unsupervised text corpora, then fine-tune on additional task-specific data. However, typical fine-tuning schemes do not prioritize the examples that they tune on. We show that, if you can prioritize informative training data, you can achieve better performance while using fewer labels. To do this we augment a language model with an epinet: a small additional network that helps to estimate model uncertainty and forms an \textit{epistemic neural network} (ENN). ENNs are neural networks that can know what they don't know. Using an epinet to prioritize uncertain data, we can fine-tune BERT on GLUE tasks to the same performance while using 2x less data than training without prioritization. We also investigate performance in synthetic neural network generative models designed to build understanding. In each setting, using an epinet outperforms heuristic active learning schemes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#32452;&#21512;&#27880;&#24847;&#21147;&#35821;&#27861;&#8221;&#65288;CAGs&#65289;&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#35813;&#32467;&#26500;&#36890;&#36807;&#32452;&#21512;&#20989;&#25968;&#23558;&#23376;&#26641;&#36882;&#24402;&#22320;&#32452;&#21512;&#20026;&#21333;&#20010;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#20808;&#21069;&#30340;&#32467;&#26500;&#20449;&#24687;&#65307;&#32467;&#26524;&#34920;&#26126;&#65292;&#32452;&#21512;&#20989;&#25968;&#21644;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#37117;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#20351;LMs&#26356;&#21152;&#31867;&#20284;&#20110;&#20154;&#31867;&#65292;&#24182;&#20801;&#35768;&#21477;&#27861;&#29305;&#24449;&#32780;&#19981;&#20801;&#35768;&#35821;&#20041;&#29305;&#24449;&#28183;&#36879;&#21040;&#23376;&#26641;&#34920;&#31034;&#20013;&#12290;</title><link>http://arxiv.org/abs/2210.12958</link><description>&lt;p&gt;
&#32452;&#21512;&#12289;&#27880;&#24847;&#21147;&#25110;&#20004;&#32773;&#20860;&#22791;&#65311;
&lt;/p&gt;
&lt;p&gt;
Composition, Attention, or Both?. (arXiv:2210.12958v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#32452;&#21512;&#27880;&#24847;&#21147;&#35821;&#27861;&#8221;&#65288;CAGs&#65289;&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#35813;&#32467;&#26500;&#36890;&#36807;&#32452;&#21512;&#20989;&#25968;&#23558;&#23376;&#26641;&#36882;&#24402;&#22320;&#32452;&#21512;&#20026;&#21333;&#20010;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#20808;&#21069;&#30340;&#32467;&#26500;&#20449;&#24687;&#65307;&#32467;&#26524;&#34920;&#26126;&#65292;&#32452;&#21512;&#20989;&#25968;&#21644;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#37117;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#20351;LMs&#26356;&#21152;&#31867;&#20284;&#20110;&#20154;&#31867;&#65292;&#24182;&#20801;&#35768;&#21477;&#27861;&#29305;&#24449;&#32780;&#19981;&#20801;&#35768;&#35821;&#20041;&#29305;&#24449;&#28183;&#36879;&#21040;&#23376;&#26641;&#34920;&#31034;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#32452;&#21512;&#27880;&#24847;&#21147;&#35821;&#27861;&#8221;&#65288;CAGs&#65289;&#30340;&#26032;&#22411;&#32467;&#26500;&#65292;&#35813;&#32467;&#26500;&#36890;&#36807;&#32452;&#21512;&#20989;&#25968;&#23558;&#23376;&#26641;&#36882;&#24402;&#22320;&#32452;&#21512;&#20026;&#21333;&#20010;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#36873;&#25321;&#24615;&#22320;&#20851;&#27880;&#20808;&#21069;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#20123;&#32452;&#20214;&#8212;&#8212;&#32452;&#21512;&#20989;&#25968;&#21644;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#8212;&#8212;&#26159;&#21542;&#37117;&#21487;&#20197;&#24341;&#36215;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#21477;&#27861;&#24402;&#32435;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#35880;&#24910;&#25511;&#21046;&#30340;&#27169;&#22411;&#22823;&#23567;&#23545;&#20855;&#26377;&#21644;&#19981;&#20855;&#26377;&#36825;&#20004;&#20010;&#32452;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#38024;&#23545;SyntaxGym&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20845;&#20010;&#27979;&#35797;&#30005;&#36335;&#35780;&#20272;&#20854;&#21477;&#27861;&#24402;&#32435;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32452;&#21512;&#20989;&#25968;&#21644;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#37117;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#20351;LMs&#26356;&#21152;&#31867;&#20284;&#20110;&#20154;&#31867;&#65292;&#24182;&#23545;&#35821;&#35328;&#29616;&#35937;&#36827;&#34892;&#26356;&#36817;&#19968;&#27493;&#30340;&#26816;&#26597;&#65292;&#26263;&#31034;&#32452;&#21512;&#20989;&#25968;&#20801;&#35768;&#21477;&#27861;&#29305;&#24449;&#32780;&#19981;&#20801;&#35768;&#35821;&#20041;&#29305;&#24449;&#28183;&#36879;&#21040;&#23376;&#26641;&#34920;&#31034;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel architecture called Composition Attention Grammars (CAGs) that recursively compose subtrees into a single vector representation with a composition function, and selectively attend to previous structural information with a self-attention mechanism. We investigate whether these components -- the composition function and the self-attention mechanism -- can both induce human-like syntactic generalization. Specifically, we train language models (LMs) with and without these two components with the model sizes carefully controlled, and evaluate their syntactic generalization performance against six test circuits on the SyntaxGym benchmark. The results demonstrated that the composition function and the self-attention mechanism both play an important role to make LMs more human-like, and closer inspection of linguistic phenomenon implied that the composition function allowed syntactic features, but not semantic features, to percolate into subtree representation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#12289;&#24369;&#30417;&#30563;&#21644;&#23436;&#20840;&#30417;&#30563;&#30340;&#35757;&#32451;&#26041;&#27861;&#23545;&#22810;&#39046;&#22495;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#22312;&#26500;&#24314;&#35821;&#26009;&#24211;&#26102;&#39046;&#22495;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.12921</link><description>&lt;p&gt;
&#25506;&#35752;&#22810;&#39046;&#22495;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#33258;&#30417;&#30563;&#12289;&#24369;&#30417;&#30563;&#21644;&#23436;&#20840;&#30417;&#30563;&#30340;&#35757;&#32451;&#26041;&#27861;&#65306;&#23391;&#21152;&#25289;&#24067;&#23572;&#35821;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Investigating self-supervised, weakly supervised and fully supervised training approaches for multi-domain automatic speech recognition: a study on Bangladeshi Bangla. (arXiv:2210.12921v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12921
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#12289;&#24369;&#30417;&#30563;&#21644;&#23436;&#20840;&#30417;&#30563;&#30340;&#35757;&#32451;&#26041;&#27861;&#23545;&#22810;&#39046;&#22495;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#24433;&#21709;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#22312;&#26500;&#24314;&#35821;&#26009;&#24211;&#26102;&#39046;&#22495;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#25913;&#36827;&#65292;&#20294;&#30001;&#20110;&#39046;&#22495;&#36716;&#31227;&#32780;&#23548;&#33268;&#30340;&#40065;&#26834;&#24615;&#21644;&#36890;&#29992;&#24615;&#38382;&#39064;&#20173;&#28982;&#22256;&#25200;&#30528;ASR&#31995;&#32479;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#22312;&#32534;&#35793;ASR&#25968;&#25454;&#38598;&#26102;&#65292;&#36890;&#24120;&#19981;&#36275;&#22815;&#22320;&#35782;&#21035;&#21644;&#26816;&#26597;&#20027;&#35201;&#30340;&#35821;&#26009;&#24211;&#35774;&#35745;&#26631;&#20934;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26368;&#20808;&#36827;&#30340;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#33258;&#30417;&#30563; wav2vec 2.0 &#21644;&#24369;&#30417;&#30563; Whisper&#65289;&#20197;&#21450;&#23436;&#20840;&#30417;&#30563;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#22810;&#39046;&#22495;ASR&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#22312;&#19968;&#20010;&#26032;&#30340;&#22810;&#39046;&#22495;&#23391;&#21152;&#25289;&#22269;&#24067;&#23572;&#35821;ASR&#35780;&#20272;&#22522;&#20934;BanSpeech&#19978;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#26469;&#35777;&#26126;&#22312;&#26500;&#24314;&#35821;&#26009;&#24211;&#26102;&#39046;&#22495;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;BanSpeech&#21253;&#21547;&#22823;&#32422;6.52&#23567;&#26102;&#30340;&#20154;&#24037;&#26631;&#27880;&#35821;&#38899;&#21644;&#26469;&#33258;13&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;8085&#20010;&#21457;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite huge improvements in automatic speech recognition (ASR) employing neural networks, ASR systems still suffer from a lack of robustness and generalizability issues due to domain shifting. This is mainly because principal corpus design criteria are often not identified and examined adequately while compiling ASR datasets. In this study, we investigate the robustness of the state-of-the-art transfer learning approaches such as self-supervised wav2vec 2.0 and weakly supervised Whisper as well as fully supervised convolutional neural networks (CNNs) for multi-domain ASR. We also demonstrate the significance of domain selection while building a corpus by assessing these models on a novel multi-domain Bangladeshi Bangla ASR evaluation benchmark - BanSpeech, which contains approximately 6.52 hours of human-annotated speech and 8085 utterances from 13 distinct domains. SUBAK.KO, a mostly read speech corpus for the morphologically rich language Bangla, has been used to train the ASR syste
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#26694;&#26550;&#30340;&#30693;&#35782;&#22270;&#35889;&#26102;&#38388;&#38382;&#31572;&#26597;&#35810;&#29983;&#25104;&#26041;&#27861;&#65292;&#21463;&#21040;&#26102;&#38388;&#32422;&#26463;&#30340;&#38480;&#21046;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#38382;&#31572;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.04490</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#20041;&#26694;&#26550;&#30340;&#30693;&#35782;&#22270;&#35889;&#26102;&#38388;&#38382;&#31572;&#26597;&#35810;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Semantic Framework based Query Generation for Temporal Question Answering over Knowledge Graphs. (arXiv:2210.04490v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#26694;&#26550;&#30340;&#30693;&#35782;&#22270;&#35889;&#26102;&#38388;&#38382;&#31572;&#26597;&#35810;&#29983;&#25104;&#26041;&#27861;&#65292;&#21463;&#21040;&#26102;&#38388;&#32422;&#26463;&#30340;&#38480;&#21046;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#38382;&#31572;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30693;&#35782;&#22270;&#35889;&#19978;&#20851;&#20110;&#26102;&#38388;&#30340;&#20107;&#23454;&#38382;&#31572;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#29983;&#25104;&#26102;&#38388;&#26597;&#35810;&#26102;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#26041;&#27861;&#24573;&#30053;&#20102;&#19968;&#20123;&#20869;&#22312;&#30340;&#20107;&#20214;&#32852;&#31995;&#65292;&#36825;&#20123;&#32852;&#31995;&#20351;&#20107;&#20214;&#22312;&#26102;&#38388;&#19978;&#30456;&#20851;&#65292;&#21487;&#33021;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#26102;&#38388;&#32422;&#26463;&#30340;&#21487;&#33021;&#35299;&#37322;&#65292;&#24182;&#23558;&#35299;&#37322;&#32467;&#26500;&#24402;&#32435;&#20026;&#26102;&#38388;&#32422;&#26463;&#30340;&#35821;&#20041;&#26694;&#26550;&#65288;SF-TCons&#65289;&#12290;&#22522;&#20110;&#35821;&#20041;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#38382;&#31572;&#26041;&#27861;&#65292;SF-TQA&#65292;&#36890;&#36807;&#25506;&#32034;&#25152;&#25552;&#21450;&#23454;&#20307;&#30340;&#30456;&#20851;&#20107;&#23454;&#26469;&#29983;&#25104;&#26597;&#35810;&#22270;&#65292;&#25506;&#32034;&#36807;&#31243;&#21463;&#21040;SF-TCons&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;SF-TQA&#22312;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering factual questions with temporal intent over knowledge graphs (temporal KGQA) attracts rising attention in recent years. In the generation of temporal queries, existing KGQA methods ignore the fact that some intrinsic connections between events can make them temporally related, which may limit their capability. We systematically analyze the possible interpretation of temporal constraints and conclude the interpretation structures as the Semantic Framework of Temporal Constraints, SF-TCons. Based on the semantic framework, we propose a temporal question answering method, SF-TQA, which generates query graphs by exploring the relevant facts of mentioned entities, where the exploring process is restricted by SF-TCons. Our evaluations show that SF-TQA significantly outperforms existing methods on two benchmarks over different knowledge graphs.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;&#20070;&#31821;&#30340;&#20840;&#25991;&#20869;&#23481;&#36827;&#34892;&#21487;&#35270;&#21270;&#21644;&#20998;&#31867;&#20219;&#21153;&#65292;&#30740;&#31350;&#39044;&#27979;&#20070;&#31821;&#26159;&#21542;&#20250;&#25104;&#20026;&#30021;&#38144;&#20070;&#12290;&#20351;&#29992;&#20102; SemAxis &#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#36827;&#34892;&#25968;&#25454;&#21021;&#27493;&#25506;&#32034;&#65292;&#37319;&#29992;&#22810;&#31181;&#20998;&#31867;&#22120;&#33719;&#24471;&#23450;&#37327;&#21644;&#26356;&#21152;&#23458;&#35266;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.02334</link><description>&lt;p&gt;
&#20351;&#29992;&#20840;&#25991;&#20869;&#23481;&#34920;&#24449;&#21644;&#35782;&#21035;&#30021;&#38144;&#20070;
&lt;/p&gt;
&lt;p&gt;
Using Full-Text Content to Characterize and Identify Best Seller Books. (arXiv:2210.02334v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02334
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;&#20070;&#31821;&#30340;&#20840;&#25991;&#20869;&#23481;&#36827;&#34892;&#21487;&#35270;&#21270;&#21644;&#20998;&#31867;&#20219;&#21153;&#65292;&#30740;&#31350;&#39044;&#27979;&#20070;&#31821;&#26159;&#21542;&#20250;&#25104;&#20026;&#30021;&#38144;&#20070;&#12290;&#20351;&#29992;&#20102; SemAxis &#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#36827;&#34892;&#25968;&#25454;&#21021;&#27493;&#25506;&#32034;&#65292;&#37319;&#29992;&#22810;&#31181;&#20998;&#31867;&#22120;&#33719;&#24471;&#23450;&#37327;&#21644;&#26356;&#21152;&#23458;&#35266;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33402;&#26415;&#20316;&#21697;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#36827;&#34892;&#30740;&#31350;&#65292;&#20854;&#20013;&#19968;&#20010;&#20363;&#23376;&#26159;&#23427;&#20204;&#22312;&#35835;&#32773;&#20013;&#30340;&#25509;&#21463;&#24773;&#20917;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#25991;&#33402;&#20316;&#21697;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#29305;&#21035;&#26159;&#35780;&#20272;&#39044;&#27979;&#20070;&#31821;&#26159;&#21542;&#20250;&#25104;&#20026;&#30021;&#38144;&#20070;&#30340;&#20219;&#21153;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20070;&#31821;&#30340;&#20840;&#25991;&#20869;&#23481;&#65292;&#24182;&#32771;&#34385;&#20102;&#21487;&#35270;&#21270;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992; SemAxis &#21644;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#36827;&#34892;&#25968;&#25454;&#32467;&#26500;&#21644;&#23646;&#24615;&#30340;&#21021;&#27493;&#25506;&#32034;&#12290;&#20026;&#20102;&#33719;&#24471;&#23450;&#37327;&#21644;&#26356;&#21152;&#23458;&#35266;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21508;&#31181;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;&#26041;&#27861;&#19982;&#25968;&#25454;&#38598;&#19968;&#36215;&#20351;&#29992;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20174;1895&#24180;&#21040;1924&#24180;&#20986;&#29256;&#30340;&#20070;&#31821;&#65292;&#24182;&#34987;&#12298;&#20986;&#29256;&#21608;&#21002;&#30021;&#38144;&#20070;&#27036;&#12299;&#30830;&#23450;&#20026;&#30021;&#38144;&#20070;&#21644;&#22312;&#21516;&#19968;&#26102;&#26399;&#20986;&#29256;&#20294;&#26410;&#34987;&#25552;&#21450;&#30340;&#25991;&#23398;&#20316;&#21697;&#12290;&#25105;&#20204;&#26041;&#27861;&#27604;&#36739;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#22909;&#30340;&#25104;&#32489;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Artistic pieces can be studied from several perspectives, one example being their reception among readers over time. In the present work, we approach this interesting topic from the standpoint of literary works, particularly assessing the task of predicting whether a book will become a best seller. Dissimilarly from previous approaches, we focused on the full content of books and considered visualization and classification tasks. We employed visualization for the preliminary exploration of the data structure and properties, involving SemAxis and linear discriminant analyses. Then, to obtain quantitative and more objective results, we employed various classifiers. Such approaches were used along with a dataset containing (i) books published from 1895 to 1924 and consecrated as best sellers by the Publishers Weekly Bestseller Lists and (ii) literary works published in the same period but not being mentioned in that list. Our comparison of methods revealed that the best-achieved result 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#20102;&#20004;&#20010;PLMs&#65292;&#21363;LegalBERT&#21644;CaseLawBERT&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#30340;&#35789;&#27719;&#34920;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20960;&#39033;&#22522;&#20934;&#27861;&#24459;NLP&#20219;&#21153;&#20013;&#65292;&#23545;&#21360;&#24230;&#21644;&#38750;&#21360;&#24230;&#30340;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2209.06049</link><description>&lt;p&gt;
&#38754;&#21521;&#27861;&#24459;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65306;&#20197;&#21360;&#24230;&#27861;&#24459;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law. (arXiv:2209.06049v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#20102;&#20004;&#20010;PLMs&#65292;&#21363;LegalBERT&#21644;CaseLawBERT&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#30340;&#35789;&#27719;&#34920;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20960;&#39033;&#22522;&#20934;&#27861;&#24459;NLP&#20219;&#21153;&#20013;&#65292;&#23545;&#21360;&#24230;&#21644;&#38750;&#21360;&#24230;&#30340;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#22686;&#22810;&#65292;&#29305;&#21035;&#26159;&#22312;&#27431;&#32654;&#27861;&#24459;&#25991;&#26412;&#26041;&#38754;&#65292;PLMs&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#21360;&#24230;&#31561;&#20854;&#20182;&#22269;&#23478;&#30340;&#27861;&#24459;&#25991;&#26412;&#20855;&#26377;&#24456;&#22810;&#29305;&#27530;&#29305;&#24449;&#65292;&#22240;&#27492;&#20063;&#38656;&#35201;&#22312;&#36825;&#20123;&#26041;&#38754;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#23581;&#35797;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21360;&#24230;&#27861;&#24459;&#25968;&#25454;&#19978;&#37325;&#26032;&#35757;&#32451;&#65288;&#32487;&#32493;&#39044;&#35757;&#32451;&#65289;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;&#27861;&#24459;PLMs, LegalBERT&#21644;CaseLawBERT&#65292;&#20197;&#21450;&#20351;&#29992;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#30340;&#35789;&#27719;&#34920;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;PLMs&#24212;&#29992;&#20110;&#19977;&#20010;&#22522;&#20934;&#27861;&#24459;NLP&#20219;&#21153;&#8212;&#8212;&#20174;&#20107;&#23454;&#20013;&#35782;&#21035;&#27861;&#24459;&#27861;&#35268;&#12289;&#23545;&#27861;&#38498;&#21028;&#20915;&#25991;&#20214;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#20197;&#21450;&#39044;&#27979;&#27861;&#38498;&#19978;&#35785;&#21028;&#20915;--&#22312;&#21360;&#24230;&#21644;&#38750;&#21360;&#24230;&#30340;&#25991;&#26412;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLP in the legal domain has seen increasing success with the emergence of Transformer-based Pre-trained Language Models (PLMs) pre-trained on legal text. PLMs trained over European and US legal text are available publicly; however, legal text from other domains (countries), such as India, have a lot of distinguishing characteristics. With the rapidly increasing volume of Legal NLP applications in various countries, it has become necessary to pre-train such LMs over legal text of other countries as well. In this work, we attempt to investigate pre-training in the Indian legal domain. We re-train (continue pre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indian legal data, as well as train a model from scratch with a vocabulary based on Indian legal text. We apply these PLMs over three benchmark legal NLP tasks -Legal Statute Identification from facts, Semantic Segmentation of Court Judgment Documents, and Court Appeal Judgment Prediction -- over both Indian and non-
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#31350;&#20102;LGBTQ&#22312;&#32447;&#31038;&#32676;&#22312;COVID-19&#30123;&#24773;&#26399;&#38388;&#32463;&#21382;&#30340;&#23569;&#25968;&#32676;&#20307;&#21387;&#21147;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#26816;&#27979;Twitter&#19978;&#34920;&#29616;&#20986;&#30340;&#23569;&#25968;&#32676;&#20307;&#21387;&#21147;&#65292;&#27604;&#36739;&#30123;&#24773;&#21069;&#21518;&#30340;&#35821;&#35328;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2205.09511</link><description>&lt;p&gt;
LGBTQ&#22312;&#32447;&#31038;&#32676;&#22312;COVID-19&#30123;&#24773;&#26399;&#38388;&#32463;&#21382;&#30340;&#23569;&#25968;&#32676;&#20307;&#21387;&#21147;
&lt;/p&gt;
&lt;p&gt;
Minority Stress Experienced by LGBTQ Online Communities during the COVID-19 Pandemic. (arXiv:2205.09511v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.09511
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#31350;&#20102;LGBTQ&#22312;&#32447;&#31038;&#32676;&#22312;COVID-19&#30123;&#24773;&#26399;&#38388;&#32463;&#21382;&#30340;&#23569;&#25968;&#32676;&#20307;&#21387;&#21147;&#12290;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#26816;&#27979;Twitter&#19978;&#34920;&#29616;&#20986;&#30340;&#23569;&#25968;&#32676;&#20307;&#21387;&#21147;&#65292;&#27604;&#36739;&#30123;&#24773;&#21069;&#21518;&#30340;&#35821;&#35328;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#30340;&#24433;&#21709;&#22312;&#23569;&#25968;&#26063;&#32676;&#20013;&#20135;&#29983;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#24433;&#21709;&#65292;&#22914;LGBTQ&#31038;&#32676;&#65288;&#22899;&#21516;&#24615;&#24651;&#12289;&#30007;&#21516;&#24615;&#24651;&#12289;&#21452;&#24615;&#24651;&#12289;&#36328;&#24615;&#21035;&#21644;&#37239;&#20799;&#65289;&#30340;&#25104;&#21592;&#65292;&#22240;&#20026;&#20182;&#20204;&#26412;&#36523;&#23384;&#22312;&#31038;&#20250;&#19981;&#21033;&#21644;&#20581;&#24247;&#24046;&#24322;&#12290;&#34429;&#28982;&#23545;&#20110;COVID-19&#30123;&#24773;&#23545;&#22823;&#20247;&#29983;&#27963;&#21508;&#20010;&#26041;&#38754;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;LGBTQ&#26063;&#32676;&#12290;&#26412;&#25991;&#21033;&#29992;&#30123;&#24773;&#21069;&#21644;&#30123;&#24773;&#26399;&#38388;&#30340;&#25968;&#25454;&#38598;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#20004;&#32452;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#20197;&#35782;&#21035;&#22312;Twitter&#19978;&#34920;&#29616;&#20986;&#23569;&#25968;&#32676;&#20307;&#21387;&#21147;&#30340;&#24086;&#23376;&#12290;&#23569;&#25968;&#32676;&#20307;&#21387;&#21147;&#26159;LGBTQ&#26063;&#32676;&#25104;&#21592;&#30001;&#20110;&#20854;&#24615;&#21035;&#21644;&#24615;&#21035;&#35748;&#21516;&#32780;&#38754;&#20020;&#30340;&#29420;&#29305;&#21387;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26368;&#20339;&#30340;&#30123;&#24773;&#21069;&#21644;&#30123;&#24773;&#26399;&#38388;&#27169;&#22411;&#34920;&#29616;&#20986;&#24378;&#22823;&#21644;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#26816;&#27979;&#21253;&#21547;&#23569;&#25968;&#32676;&#20307;&#21387;&#21147;&#30340;&#24086;&#23376;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#30123;&#24773;&#21069;&#21644;&#30123;&#24773;&#26399;&#38388;&#23569;&#25968;&#32676;&#20307;&#21387;&#21147;&#24086;&#23376;&#30340;&#35821;&#35328;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#24868;&#24594;&#26159;&#22312;&#30123;&#24773;&#26399;&#38388;&#26368;&#26174;&#33879;&#30340;&#24773;&#32490;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has disproportionately impacted the lives of minorities, such as members of the LGBTQ community (lesbian, gay, bisexual, transgender, and queer) due to pre-existing social disadvantages and health disparities. Although extensive research has been carried out on the impact of the COVID-19 pandemic on different aspects of the general population's lives, few studies are focused on the LGBTQ population. In this paper, we develop and evaluate two sets of machine learning classifiers using a pre-pandemic and a during-pandemic dataset to identify Twitter posts exhibiting minority stress, which is a unique pressure faced by the members of the LGBTQ population due to their sexual and gender identities. We demonstrate that our best pre- and during-pandemic models show strong and stable performance for detecting posts that contain minority stress. We investigate the linguistic differences in minority stress posts across pre- and during-pandemic periods. We find that anger wo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22810;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#29305;&#24449;&#65292;&#22635;&#34917;&#20102;&#25512;&#26029;&#21644;&#25913;&#21464;&#36825;&#20123;&#27169;&#22411;&#25152;&#32487;&#25215;&#20154;&#26684;&#29305;&#24449;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2204.12000</link><description>&lt;p&gt;
&#35780;&#20272;&#30333;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Estimating the Personality of White-Box Language Models. (arXiv:2204.12000v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12000
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22810;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#26684;&#29305;&#24449;&#65292;&#22635;&#34917;&#20102;&#25512;&#26029;&#21644;&#25913;&#21464;&#36825;&#20123;&#27169;&#22411;&#25152;&#32487;&#25215;&#20154;&#26684;&#29305;&#24449;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38754;&#21521;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#34394;&#25311;&#21161;&#25163;&#21040;&#23545;&#35805;&#26426;&#22120;&#20154;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#21487;&#20197;&#20135;&#29983;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#28982;&#32780;&#29616;&#26377;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20250;&#21453;&#26144;&#20154;&#31867;&#20559;&#35265;&#12290;&#34429;&#28982;&#35768;&#22810;&#20559;&#35265;&#23588;&#20854;&#26159;&#37027;&#20123;&#21487;&#33021;&#23548;&#33268;&#21361;&#23475;&#30340;&#20559;&#35265;&#24050;&#32463;&#34987;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#25512;&#26029;&#21644;&#25913;&#21464;&#36825;&#20123;&#27169;&#22411;&#25152;&#32487;&#25215;&#30340;&#20154;&#26684;&#29305;&#24449;&#30340;&#30740;&#31350;&#21364;&#24456;&#23569;&#25110;&#26681;&#26412;&#19981;&#23384;&#22312;&#12290;&#26412;&#25991;&#23558;&#36890;&#36807;&#25506;&#32034;&#29992;&#20110;&#29983;&#25104;&#24320;&#25918;&#24335;&#25991;&#26412;&#30340;&#22810;&#20010;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#20154;&#26684;&#29305;&#24449;&#65292;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#22522;&#20110;&#27969;&#34892;&#30340;&#20116;&#20010;&#22823;&#22240;&#32032;&#21644;&#21457;&#23637;&#20986;&#20581;&#22766;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#20123;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#20154;&#26684;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Technology for open-ended language generation, a key application of artificial intelligence, has advanced to a great extent in recent years. Large-scale language models, which are trained on large corpora of text, are being used in a wide range of applications everywhere, from virtual assistants to conversational bots. While these language models output fluent text, existing research shows that these models can and do capture human biases. Many of these biases, especially those that could potentially cause harm, are being well-investigated. On the other hand, studies that infer and change human personality traits inherited by these models have been scarce or non-existent. Our work seeks to address this gap by exploring the personality traits of several large-scale language models designed for open-ended text generation and the datasets used for training them. We build on the popular Big Five factors and develop robust methods that quantify the personality traits of these models and the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23581;&#35797;&#36890;&#36807;&#21033;&#29992;&#20845;&#31181;&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#22522;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#36890;&#29992;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#65292;&#21033;&#29992;210,000&#20221;&#27861;&#24459;&#35785;&#35772;&#25991;&#26723;&#36827;&#34892;&#24494;&#35843;&#21644;&#19987;&#19994;&#21270;&#35757;&#32451;&#65292;&#35299;&#20915;&#27861;&#24459;&#25991;&#20214;&#30456;&#20284;&#24230;&#38382;&#39064;&#65292;&#20174;&#32780;&#21327;&#21161;&#24555;&#36895;&#35299;&#20915;&#21496;&#27861;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2204.07182</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#29992;&#20110;&#27861;&#38498;&#25991;&#20214;&#30340;&#30456;&#20284;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysing similarities between legal court documents using natural language processing approaches based on Transformers. (arXiv:2204.07182v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23581;&#35797;&#36890;&#36807;&#21033;&#29992;&#20845;&#31181;&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#22522;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#36890;&#29992;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#65292;&#21033;&#29992;210,000&#20221;&#27861;&#24459;&#35785;&#35772;&#25991;&#26723;&#36827;&#34892;&#24494;&#35843;&#21644;&#19987;&#19994;&#21270;&#35757;&#32451;&#65292;&#35299;&#20915;&#27861;&#24459;&#25991;&#20214;&#30456;&#20284;&#24230;&#38382;&#39064;&#65292;&#20174;&#32780;&#21327;&#21161;&#24555;&#36895;&#35299;&#20915;&#21496;&#27861;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#25104;&#20026;&#27861;&#24459;&#39046;&#22495;&#20013;&#21327;&#21161;&#24555;&#36895;&#35299;&#20915;&#21496;&#27861;&#31243;&#24207;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#26412;&#25991;&#20197;&#24052;&#35199;&#21496;&#27861;&#31995;&#32479;&#30340;&#26696;&#20363;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#36816;&#29992;&#20845;&#31181;&#22522;&#20110;Transformer&#32467;&#26500;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#35299;&#20915;&#27861;&#24459;&#25991;&#20214;&#30456;&#20284;&#24230;&#38382;&#39064;&#12290;&#21253;&#25324;BERT&#12289;GPT-2&#12289;RoBERTa&#31561;NLP&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#29992;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#36890;&#29992;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#21033;&#29992;210,000&#20221;&#27861;&#24459;&#35785;&#35772;&#25991;&#26723;&#36827;&#34892;&#24494;&#35843;&#21644;&#19987;&#19994;&#21270;&#35757;&#32451;&#12290;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#25991;&#26723;&#30340;&#23884;&#20837;&#21521;&#37327;&#34920;&#24449;&#65292;&#36816;&#29992;&#32858;&#31867;&#26041;&#27861;&#23545;&#35785;&#35772;&#26696;&#20214;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#35745;&#31639;&#27599;&#20010;&#27169;&#22411;&#30340;&#21697;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Artificial Intelligence (AI) have leveraged promising results in solving complex problems in the area of Natural Language Processing (NLP), being an important tool to help in the expeditious resolution of judicial proceedings in the legal area. In this context, this work targets the problem of detecting the degree of similarity between judicial documents that can be achieved in the inference group, by applying six NLP techniques based on the transformers architecture to a case study of legal proceedings in the Brazilian judicial system. The NLP transformer-based models, namely BERT, GPT-2 and RoBERTa, were pre-trained using a general purpose corpora of the Brazilian Portuguese language, and then were fine-tuned and specialised for the legal sector using 210,000 legal proceedings. Vector representations of each legal document were calculated based on their embeddings, which were used to cluster the lawsuits, calculating the quality of each model based on the cosine of
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25506;&#31350;&#20102;&#23545;&#27604;&#34920;&#31034;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;Time Control(TC)&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#30041;&#38271;&#25991;&#26412;&#30340;&#32467;&#26500;&#65292;&#24182;&#22312;&#23398;&#20064;&#21477;&#23376;&#34920;&#31034;&#20197;&#33719;&#24471;&#35805;&#35821;&#36830;&#36143;&#24615;&#26041;&#38754;&#34920;&#29616;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2203.11370</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#36807;&#31243;&#30340;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Language modeling via stochastic processes. (arXiv:2203.11370v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25506;&#31350;&#20102;&#23545;&#27604;&#34920;&#31034;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;Time Control(TC)&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#30041;&#38271;&#25991;&#26412;&#30340;&#32467;&#26500;&#65292;&#24182;&#22312;&#23398;&#20064;&#21477;&#23376;&#34920;&#31034;&#20197;&#33719;&#24471;&#35805;&#35821;&#36830;&#36143;&#24615;&#26041;&#38754;&#34920;&#29616;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#30701;&#25991;&#26412;&#65292;&#20294;&#26159;&#24403;&#23427;&#20204;&#29983;&#25104;&#36739;&#38271;&#25991;&#26412;&#26102;&#65292;&#24448;&#24448;&#26174;&#24471;&#20887;&#26434;&#25110;&#32773;&#19981;&#36830;&#36143;&#12290;&#36825;&#20123;&#38382;&#39064;&#28304;&#33258;next-token-only&#30340;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#12290;&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#24037;&#20316;&#34920;&#26126;&#65292;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#21040;&#22909;&#30340;&#28508;&#22312;&#34920;&#24449;&#65292;&#36825;&#23545;&#20110;&#21306;&#20998;&#24615;&#20219;&#21153;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20998;&#26512;&#20102;&#23545;&#27604;&#34920;&#31034;&#24212;&#29992;&#20110;&#29983;&#25104;&#20219;&#21153;&#65288;&#22914;&#38271;&#25991;&#26412;&#29983;&#25104;&#65289;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Time Control (TC)&#12290;TC&#39318;&#20808;&#23398;&#20064;&#30446;&#26631;&#25991;&#26412;&#39046;&#22495;&#30340;&#23545;&#27604;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#36825;&#20123;&#34920;&#31034;&#35299;&#30721;&#29983;&#25104;&#25991;&#26412;&#12290;&#19982;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#26041;&#27861;&#21644;&#36328;&#36234;&#21508;&#31181;&#25991;&#26412;&#39046;&#22495;&#30340;fine-tuning GPT2&#30456;&#27604;&#65292;TC&#22312;&#23398;&#20064;&#21477;&#23376;&#34920;&#31034;&#20197;&#33719;&#24471;&#35805;&#35821;&#36830;&#36143;&#24615;&#26041;&#38754;&#34920;&#29616;&#31454;&#20105;&#21147;&#12290;&#22312;&#38271;&#25991;&#26412;&#29983;&#25104;&#35774;&#32622;&#20013;&#65292;TC&#20445;&#30041;&#20102;&#25991;&#26412;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language modeling objective. Recent work in self-supervised learning suggests that models can learn good latent representations via contrastive learning, which can be effective for discriminative tasks. Our work analyzes the application of contrastive representations for generative tasks, like long text generation. We propose one approach for leveraging constrastive representations, which we call Time Control (TC). TC first learns a contrastive representation of the target text domain, then generates text by decoding from these representations. Compared to domain-specific methods and fine-tuning GPT2 across a variety of text domains, TC performs competitively to methods specific for learning sentence representations on discourse coherence. On long text generation settings, TC preserves the text structure bo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23558;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#24212;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2203.11155</link><description>&lt;p&gt;
&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#22312;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Quantum Density Matrix in Classical Question Answering and Classical Image Classification. (arXiv:2203.11155v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.11155
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23558;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#24212;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#21644;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#21487;&#34920;&#31034;&#25972;&#20010;&#37327;&#23376;&#31995;&#32479;&#30340;&#20840;&#37096;&#20449;&#24687;&#65292;&#23558;&#23494;&#24230;&#30697;&#38453;&#29992;&#20110;&#32463;&#20856;&#38382;&#31572;&#20219;&#21153;&#21487;&#20197;&#26356;&#21152;&#26377;&#25928;&#22320;&#23454;&#29616;&#38382;&#39064;&#22238;&#31572;&#12290;&#26412;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;LSTM&#30340;&#26032;&#26426;&#21046;&#65292;&#20197;&#24212;&#23545;&#36755;&#20837;&#20026;&#30697;&#38453;&#30340;&#24773;&#20917;&#65292;&#24182;&#23558;&#35813;&#26426;&#21046;&#24212;&#29992;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;QA&#38382;&#39064;&#30340;&#27714;&#35299;&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#37327;&#23376;&#23494;&#24230;&#30697;&#38453;&#21487;&#20197;&#22686;&#24378;&#32463;&#20856;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#29305;&#24449;&#20449;&#24687;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26032;&#26694;&#26550;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;CNN&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum density matrix represents all the information of the entire quantum system, and novel models of meaning employing density matrices naturally model linguistic phenomena such as hyponymy and linguistic ambiguity, among others in quantum question answering tasks. Naturally, we argue that applying the quantum density matrix into classical Question Answering (QA) tasks can show more effective performance. Specifically, we (i) design a new mechanism based on Long Short-Term Memory (LSTM) to accommodate the case when the inputs are matrixes; (ii) apply the new mechanism to QA problems with Convolutional Neural Network (CNN) and gain the LSTM-based QA model with the quantum density matrix. Experiments of our new model on TREC-QA and WIKI-QA data sets show encouraging results. Similarly, we argue that the quantum density matrix can also enhance the image feature information and the relationship between the features for the classical image classification. Thus, we (i) combine density mat
&lt;/p&gt;</description></item><item><title>&#20998;&#23618;&#32467;&#26500;&#19982;&#24038;&#35282;&#26550;&#26500;&#26159;&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;&#26356;&#20855;&#35748;&#30693;&#21487;&#20449;&#24230;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2109.04939</link><description>&lt;p&gt;
&#21033;&#29992;&#24038;&#35282;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#35821;&#27861;&#27169;&#25311;&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars. (arXiv:2109.04939v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.04939
&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#32467;&#26500;&#19982;&#24038;&#35282;&#26550;&#26500;&#26159;&#20154;&#31867;&#21477;&#23376;&#22788;&#29702;&#26356;&#20855;&#35748;&#30693;&#21487;&#20449;&#24230;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#20013;&#65292;&#24050;&#32463;&#34920;&#26126;&#20998;&#32423;&#32467;&#26500;&#20351;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26356;&#20687;&#20154;&#31867;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#25991;&#29486;&#23545;&#20998;&#23618;&#27169;&#22411;&#30340;&#20998;&#26512;&#31574;&#30053;&#25345;&#26080;&#30693;&#24577;&#24230;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#23618;&#32467;&#26500;&#26159;&#21542;&#20351;LM&#26356;&#20687;&#20154;&#31867;&#65292;&#22914;&#26524;&#26159;&#65292;&#26368;&#20855;&#35748;&#30693;&#21487;&#20449;&#24230;&#30340;&#35299;&#26512;&#31574;&#30053;&#26159;&#20160;&#20040;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#20855;&#26377;&#22836;&#32456;&#24038;&#21521;&#32467;&#26500;&#30340;&#26085;&#35821;&#38405;&#35835;&#26102;&#38388;&#19982;&#38271;&#30701;&#26102;&#35760;&#24518;&#65288;LSTM&#65289;&#20316;&#20026;&#24207;&#21015;&#27169;&#22411;&#21644;&#20174;&#19978;&#21040;&#19979;&#21644;&#20174;&#24038;&#35282;RNNG&#20316;&#20026;&#20998;&#23618;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#20102;&#19977;&#20010;LM&#12290;&#25105;&#20204;&#30340;&#35745;&#31639;&#24314;&#27169;&#34920;&#26126;&#65292;&#24038;&#19978;&#26041;RNNG&#20248;&#20110;&#20174;&#19978;&#21040;&#19979;&#30340;RNNG&#21644;LSTM&#65292;&#36825;&#34920;&#26126;&#20998;&#23618;&#21644;&#24038;&#35282;&#26550;&#26500;&#27604;&#20174;&#19978;&#21040;&#19979;&#25110;&#24207;&#21015;&#26550;&#26500;&#26356;&#20855;&#35748;&#30693;&#21487;&#20449;&#24230;&#12290;&#27492;&#22806;&#65292;&#35748;&#30693;&#21487;&#20449;&#24230;&#19982;&#65288;i&#65289;&#21477;&#23376;&#38271;&#24230;&#65292;&#65288;ii&#65289;&#21477;&#23376;&#32467;&#26500;&#21644;&#65288;iii&#65289;&#35835;&#32773;&#21475;&#35821;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#20063;&#24471;&#21040;&#20102;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computational linguistics, it has been shown that hierarchical structures make language models (LMs) more human-like. However, the previous literature has been agnostic about a parsing strategy of the hierarchical models. In this paper, we investigated whether hierarchical structures make LMs more human-like, and if so, which parsing strategy is most cognitively plausible. In order to address this question, we evaluated three LMs against human reading times in Japanese with head-final left-branching structures: Long Short-Term Memory (LSTM) as a sequential model and Recurrent Neural Network Grammars (RNNGs) with top-down and left-corner parsing strategies as hierarchical models. Our computational modeling demonstrated that left-corner RNNGs outperformed top-down RNNGs and LSTM, suggesting that hierarchical and left-corner architectures are more cognitively plausible than top-down or sequential architectures. In addition, the relationships between the cognitive plausibility and (i) p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21387;&#32553;&#23376;&#23618;&#30340;&#39640;&#25928;Transformer&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#20943;&#23569;&#23376;&#23618;&#24182;&#25552;&#39640;&#24182;&#34892;&#24615;&#33021;&#22815;&#36798;&#21040;1.42&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#30830;&#20445;&#24615;&#33021;&#19982;&#22522;&#32447;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2101.00542</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#21387;&#32553;&#23376;&#23618;&#30340;&#39640;&#25928;Transformer&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
An Efficient Transformer Decoder with Compressed Sub-layers. (arXiv:2101.00542v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.00542
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21387;&#32553;&#23376;&#23618;&#30340;&#39640;&#25928;Transformer&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#20943;&#23569;&#23376;&#23618;&#24182;&#25552;&#39640;&#24182;&#34892;&#24615;&#33021;&#22815;&#36798;&#21040;1.42&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#21516;&#26102;&#30830;&#20445;&#24615;&#33021;&#19982;&#22522;&#32447;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30001;&#20110;&#20854;&#39640;&#25928;&#32780;&#27969;&#34892;&#30340;&#22823;&#22411;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#65288;Transformer&#65289;&#30340;&#35299;&#30721;&#22120;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#65292;&#23548;&#33268;&#25928;&#29575;&#38382;&#39064;&#12290;&#36890;&#36807;&#26597;&#30475;&#35299;&#30721;&#22120;&#30340;&#25968;&#23398;&#20844;&#24335;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#21387;&#32553;&#20854;&#23376;&#23618;&#65288;Transformer&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#65289;&#31616;&#21270;&#26550;&#26500;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#24182;&#34892;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21387;&#32553;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#20854;&#35299;&#30721;&#22120;&#23618;&#20165;&#21253;&#21547;&#19968;&#20010;&#23376;&#23618;&#32780;&#19981;&#26159;&#19977;&#20010;&#23376;&#23618;&#12290;&#22312;14&#20010;WMT&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#24378;&#22522;&#32447;&#24555;1.42&#20493;&#65292;&#24182;&#19988;&#24615;&#33021;&#30456;&#24403;&#12290;&#32780;&#36825;&#20010;&#24378;&#22522;&#32447;&#24050;&#32463;&#27604;&#24191;&#27867;&#20351;&#29992;&#30340;&#26631;&#20934;&#22522;&#32447;&#24555;2&#20493;&#32780;&#19988;&#24615;&#33021;&#19981;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large attention-based encoder-decoder network (Transformer) has become prevailing recently due to its effectiveness. But the high computation complexity of its decoder raises the inefficiency issue. By examining the mathematic formulation of the decoder, we show that under some mild conditions, the architecture could be simplified by compressing its sub-layers, the basic building block of Transformer, and achieves a higher parallelism. We thereby propose Compressed Attention Network, whose decoder layer consists of only one sub-layer instead of three. Extensive experiments on 14 WMT machine translation tasks show that our model is 1.42x faster with performance on par with a strong baseline. This strong baseline is already 2x faster than the widely used standard baseline without loss in performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;Lambek&#28436;&#31639;&#24341;&#20837;&#20102;&#19968;&#20010;&#30456;&#20851;&#27169;&#24577;!L*&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#20313;&#20195;&#25968;&#27169;&#24577;&#30340;&#33539;&#30068;&#21521;&#37327;&#31354;&#38388;&#35821;&#20041;&#23398;&#32473;&#20986;&#20102;&#35813;&#27169;&#24577;&#19979;&#30340;&#30701;&#35821;&#27966;&#29983;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2005.03074</link><description>&lt;p&gt;
&#20855;&#26377;&#30456;&#20851;&#27169;&#24577;&#30340;Lambek&#28436;&#31639;&#30340;&#33539;&#30068;&#21521;&#37327;&#31354;&#38388;&#35821;&#20041;&#23398;
&lt;/p&gt;
&lt;p&gt;
Categorical Vector Space Semantics for Lambek Calculus with a Relevant Modality. (arXiv:2005.03074v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.03074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;Lambek&#28436;&#31639;&#24341;&#20837;&#20102;&#19968;&#20010;&#30456;&#20851;&#27169;&#24577;!L*&#65292;&#24182;&#36890;&#36807;&#26500;&#24314;&#20855;&#26377;&#20313;&#20195;&#25968;&#27169;&#24577;&#30340;&#33539;&#30068;&#21521;&#37327;&#31354;&#38388;&#35821;&#20041;&#23398;&#32473;&#20986;&#20102;&#35813;&#27169;&#24577;&#19979;&#30340;&#30701;&#35821;&#27966;&#29983;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#20855;&#26377;&#26377;&#38480;&#21387;&#32553;&#21644;&#32622;&#25442;&#35268;&#21017;&#30340;&#30456;&#20851;&#27169;&#24577;! L *&#30340;Lambek&#28436;&#31639;&#24320;&#21457;&#20102;&#19968;&#31181;&#33539;&#30068;&#32452;&#21512;&#20998;&#24067;&#35821;&#20041;&#23398;&#12290;&#35821;&#20041;&#23398;&#30340;&#33539;&#30068;&#37096;&#20998;&#26159;&#19968;&#20010;&#20855;&#26377;&#20313;&#20195;&#25968;&#27169;&#24577;&#30340;&#19968;&#33324;&#21452;&#38381;&#33539;&#30068;&#65292;&#38750;&#24120;&#31867;&#20284;&#20110;&#24494;&#20998;&#33539;&#30068;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#8220;&#37327;&#23376;&#21270;&#8221;&#20989;&#23376;&#23558;&#36825;&#20010;&#33539;&#30068;&#23454;&#20363;&#21270;&#20026;&#26377;&#38480;&#32500;&#21521;&#37327;&#31354;&#38388;&#21644;&#32447;&#24615;&#26144;&#23556;&#65292;&#24182;&#21033;&#29992;&#19977;&#31181;&#20855;&#20307;&#30340;&#20313;&#20195;&#25968;&#27169;&#24577;&#26469;&#36827;&#34892;&#25805;&#20316;&#12290;&#25105;&#20204;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;&#26500;&#24314;&#30456;&#20851;&#27169;&#24577;! L *&#30340;&#19968;&#20010;&#31034;&#20363;&#20013;&#65306;&#20855;&#26377;&#23492;&#29983;&#20195;&#30340;&#30701;&#35821;&#27966;&#29983;&#12290;&#20351;&#29992;BERT&#12289;Word2Vec&#21644;FastText&#21521;&#37327;&#20197;&#21450;&#20851;&#31995;&#24352;&#37327;&#65292;&#25105;&#20204;&#23545;&#20855;&#20307;&#35299;&#37322;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36890;&#36807;&#23545;&#25193;&#23637;&#20102;&#30340;&#21477;&#23376;&#28040;&#23696;&#25968;&#25454;&#38598;&#19978;&#30340;&#23492;&#29983;&#20195;&#30701;&#35821;&#36827;&#34892;&#20219;&#21153;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a categorical compositional distributional semantics for Lambek Calculus with a Relevant Modality !L*, which has a limited edition of the contraction and permutation rules. The categorical part of the semantics is a monoidal biclosed category with a coalgebra modality, very similar to the structure of a Differential Category. We instantiate this category to finite dimensional vector spaces and linear maps via "quantisation" functors and work with three concrete interpretations of the coalgebra modality. We apply the model to construct categorical and concrete semantic interpretations for the motivating example of !L*: the derivation of a phrase with a parasitic gap. The effectiveness of the concrete interpretations are evaluated via a disambiguation task, on an extension of a sentence disambiguation dataset to parasitic gap phrases, using BERT, Word2Vec, and FastText vectors and Relational tensors.
&lt;/p&gt;</description></item></channel></rss>