<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>LLM-Grounder&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#12289;&#24320;&#25918;&#35789;&#27719;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;3D&#35270;&#35273;&#23450;&#20301;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20998;&#35299;&#26597;&#35810;&#24182;&#20351;&#29992;&#35270;&#35273;&#23450;&#20301;&#24037;&#20855;&#35782;&#21035;&#29289;&#20307;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#26032;&#22330;&#26223;&#21644;&#25991;&#26412;&#26597;&#35810;&#30340;&#26377;&#25928;&#23450;&#20301;&#12290;&#22312;ScanRefer&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.12311</link><description>&lt;p&gt;
LLM-Grounder: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#30340;&#24320;&#25918;&#35789;&#27719;3D&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent. (arXiv:2309.12311v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12311
&lt;/p&gt;
&lt;p&gt;
LLM-Grounder&#26159;&#19968;&#31181;&#38646;&#26679;&#26412;&#12289;&#24320;&#25918;&#35789;&#27719;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;3D&#35270;&#35273;&#23450;&#20301;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20998;&#35299;&#26597;&#35810;&#24182;&#20351;&#29992;&#35270;&#35273;&#23450;&#20301;&#24037;&#20855;&#35782;&#21035;&#29289;&#20307;&#65292;&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#26032;&#22330;&#26223;&#21644;&#25991;&#26412;&#26597;&#35810;&#30340;&#26377;&#25928;&#23450;&#20301;&#12290;&#22312;ScanRefer&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#35270;&#35273;&#23450;&#20301;&#26159;&#23478;&#29992;&#26426;&#22120;&#20154;&#30340;&#37325;&#35201;&#33021;&#21147;&#65292;&#21487;&#20197;&#20351;&#20854;&#22312;&#29615;&#22659;&#20013;&#23548;&#33322;&#12289;&#25805;&#20316;&#29289;&#20307;&#24182;&#26681;&#25454;&#29615;&#22659;&#22238;&#31572;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#22823;&#37327;&#26631;&#35760;&#30340;&#25968;&#25454;&#65292;&#25110;&#32773;&#22312;&#22788;&#29702;&#22797;&#26434;&#35821;&#35328;&#26597;&#35810;&#26102;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LLM-Grounder&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#12289;&#24320;&#25918;&#35789;&#27719;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;3D&#35270;&#35273;&#23450;&#20301;&#27969;&#31243;&#12290;LLM-Grounder&#21033;&#29992;&#19968;&#20010;LLM&#23558;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#20998;&#35299;&#20026;&#35821;&#20041;&#25104;&#20998;&#65292;&#24182;&#20351;&#29992;&#35832;&#22914;OpenScene&#25110;LERF&#20043;&#31867;&#30340;&#35270;&#35273;&#23450;&#20301;&#24037;&#20855;&#26469;&#35782;&#21035;3D&#22330;&#26223;&#20013;&#30340;&#29289;&#20307;&#12290;&#28982;&#21518;&#65292;LLM&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#29289;&#20307;&#20043;&#38388;&#30340;&#31354;&#38388;&#21644;&#24120;&#35782;&#20851;&#31995;&#65292;&#20197;&#20570;&#20986;&#26368;&#32456;&#30340;&#23450;&#20301;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26032;&#30340;3D&#22330;&#26223;&#21644;&#20219;&#24847;&#25991;&#26412;&#26597;&#35810;&#12290;&#25105;&#20204;&#22312;ScanRefer&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;LLM-Grounder&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;LLMs&#22312;3D&#35270;&#35273;&#23450;&#20301;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D visual grounding is a critical skill for household robots, enabling them to navigate, manipulate objects, and answer questions based on their environment. While existing approaches often rely on extensive labeled data or exhibit limitations in handling complex language queries, we propose LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model (LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to decompose complex natural language queries into semantic constituents and employs a visual grounding tool, such as OpenScene or LERF, to identify objects in a 3D scene. The LLM then evaluates the spatial and commonsense relations among the proposed objects to make a final grounding decision. Our method does not require any labeled training data and can generalize to novel 3D scenes and arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and demonstrate state-of-the-art zero-shot grounding accuracy. Our findings indicate that LLMs si
&lt;/p&gt;</description></item><item><title>&#28436;&#32451;&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#20914;&#31361;&#21644;&#25552;&#20379;&#21453;&#39304;&#65292;&#25945;&#25480;&#29992;&#25143;&#20914;&#31361;&#35299;&#20915;&#30340;&#25216;&#33021;&#12290;&#21033;&#29992;&#28436;&#32451;&#65292;&#29992;&#25143;&#21487;&#20197;&#32451;&#20064;&#22788;&#29702;&#21508;&#31181;&#20914;&#31361;&#22330;&#26223;&#65292;&#24182;&#23398;&#20064;&#22914;&#20309;&#36816;&#29992;&#20914;&#31361;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2309.12309</link><description>&lt;p&gt;
&#28436;&#32451;&#65306;&#36890;&#36807;&#27169;&#25311;&#20914;&#31361;&#26469;&#25945;&#25480;&#20914;&#31361;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rehearsal: Simulating Conflict to Teach Conflict Resolution. (arXiv:2309.12309v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12309
&lt;/p&gt;
&lt;p&gt;
&#28436;&#32451;&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#27169;&#25311;&#20914;&#31361;&#21644;&#25552;&#20379;&#21453;&#39304;&#65292;&#25945;&#25480;&#29992;&#25143;&#20914;&#31361;&#35299;&#20915;&#30340;&#25216;&#33021;&#12290;&#21033;&#29992;&#28436;&#32451;&#65292;&#29992;&#25143;&#21487;&#20197;&#32451;&#20064;&#22788;&#29702;&#21508;&#31181;&#20914;&#31361;&#22330;&#26223;&#65292;&#24182;&#23398;&#20064;&#22914;&#20309;&#36816;&#29992;&#20914;&#31361;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#38469;&#20914;&#31361;&#26159;&#19968;&#31181;&#20196;&#20154;&#19981;&#33298;&#26381;&#20294;&#19981;&#21487;&#36991;&#20813;&#30340;&#29983;&#27963;&#20107;&#23454;&#12290;&#25104;&#21151;&#22320;&#22788;&#29702;&#20914;&#31361;&#26159;&#19968;&#31181;&#25216;&#33021;&#65292;&#21487;&#20197;&#36890;&#36807;&#21051;&#24847;&#32451;&#20064;&#26469;&#23398;&#20064;&#65292;&#20294;&#26159;&#24456;&#23569;&#26377;&#20154;&#33021;&#22815;&#33719;&#24471;&#26377;&#25928;&#30340;&#22521;&#35757;&#25110;&#21453;&#39304;&#12290;&#20026;&#20102;&#25193;&#22823;&#36825;&#31181;&#26426;&#20250;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28436;&#32451;&#65288;Rehearsal&#65289;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#20801;&#35768;&#29992;&#25143;&#19982;&#21487;&#20449;&#30340;&#27169;&#25311;&#23545;&#35805;&#32773;&#19968;&#36215;&#25490;&#32451;&#20914;&#31361;&#65292;&#25506;&#32034;&#22914;&#26524;&#24773;&#20917;&#22914;&#20309;&#30340;&#8220;&#20551;&#35774;&#8221;&#22330;&#26223;&#20197;&#35782;&#21035;&#26367;&#20195;&#30340;&#23545;&#35805;&#36335;&#24452;&#65292;&#24182;&#36890;&#36807;&#21453;&#39304;&#23398;&#20064;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#24212;&#29992;&#29305;&#23450;&#30340;&#20914;&#31361;&#31574;&#30053;&#12290;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#28436;&#32451;&#26469;&#32451;&#20064;&#22788;&#29702;&#21508;&#31181;&#24050;&#23450;&#20041;&#30340;&#20914;&#31361;&#22330;&#26223;&#65292;&#20174;&#21150;&#20844;&#23460;&#20105;&#35758;&#21040;&#24773;&#24863;&#38382;&#39064;&#65292;&#25110;&#32773;&#20182;&#20204;&#20063;&#21487;&#20197;&#36873;&#25321;&#21019;&#24314;&#33258;&#24049;&#30340;&#20914;&#31361;&#22330;&#26223;&#12290;&#20026;&#20102;&#23454;&#29616;&#28436;&#32451;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;IRP&#25552;&#31034;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20914;&#31361;&#35299;&#20915;&#20013;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#21033;&#30410;-&#26435;&#21147;-&#33021;&#21147;&#65288;IRP&#65289;&#29702;&#35770;&#26469;&#35843;&#33410;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#28436;&#32451;&#20351;&#29992;IRP&#29983;&#25104;&#22522;&#20110;&#20914;&#31361;&#35299;&#20915;&#29702;&#35770;&#30340;&#35805;&#35821;&#65292;&#24341;&#23548;&#29992;&#25143;&#23454;&#36341;&#24212;&#29992;&#20914;&#31361;&#35299;&#20915;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpersonal conflict is an uncomfortable but unavoidable fact of life. Navigating conflict successfully is a skill -- one that can be learned through deliberate practice -- but few have access to effective training or feedback. To expand this access, we introduce Rehearsal, a system that allows users to rehearse conflicts with a believable simulated interlocutor, explore counterfactual "what if?" scenarios to identify alternative conversational paths, and learn through feedback on how and when to apply specific conflict strategies. Users can utilize Rehearsal to practice handling a variety of predefined conflict scenarios, from office disputes to relationship issues, or they can choose to create their own. To enable Rehearsal, we develop IRP prompting, a method of conditioning output of a large language model on the influential Interest-Rights-Power (IRP) theory from conflict resolution. Rehearsal uses IRP to generate utterances grounded in conflict resolution theory, guiding users t
&lt;/p&gt;</description></item><item><title>LongLoRA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#23427;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#23454;&#29616;&#19978;&#19979;&#25991;&#25193;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12307</link><description>&lt;p&gt;
LongLoRA: &#39640;&#25928;&#30340;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#32454;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. (arXiv:2309.12307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12307
&lt;/p&gt;
&lt;p&gt;
LongLoRA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#23427;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#23454;&#29616;&#19978;&#19979;&#25991;&#25193;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#8212;&#8212;LongLoRA&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#36890;&#24120;&#65292;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#22823;&#23567;&#35757;&#32451;LLM&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;GPU&#36164;&#28304;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#26041;&#38754;&#21152;&#24555;&#20102;LLM&#30340;&#19978;&#19979;&#25991;&#25193;&#23637;&#12290;&#19968;&#26041;&#38754;&#65292;&#23613;&#31649;&#25512;&#29702;&#36807;&#31243;&#20013;&#38656;&#35201;&#31264;&#23494;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#65292;&#20294;&#27169;&#22411;&#30340;&#31934;&#32454;&#35843;&#25972;&#21487;&#20197;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#23436;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#19978;&#19979;&#25991;&#30340;&#25193;&#23637;&#65292;&#22312;&#19982;&#20351;&#29992;&#20256;&#32479;&#27880;&#24847;&#21147;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#26102;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#35757;&#32451;&#20013;&#21482;&#29992;&#20004;&#34892;&#20195;&#30721;&#23454;&#29616;&#65292;&#22312;&#25512;&#29702;&#20013;&#26159;&#21487;&#36873;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#21442;&#25968;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shift short attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-effici
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#21644;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20174;&#36923;&#36753;&#24418;&#24335;&#20013;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#30340;&#36136;&#37327;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#20505;&#36873;&#36755;&#20986;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#20307;&#29616;&#36923;&#36753;&#24418;&#24335;&#30340;&#35821;&#20041;&#65292;&#24182;&#25552;&#21319;&#29983;&#25104;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.12294</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20174;&#36923;&#36753;&#24418;&#24335;&#20013;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#30340;&#37325;&#26032;&#25490;&#24207;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models. (arXiv:2309.12294v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#21644;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20174;&#36923;&#36753;&#24418;&#24335;&#20013;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#30340;&#36136;&#37327;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#20505;&#36873;&#36755;&#20986;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26356;&#22909;&#22320;&#20307;&#29616;&#36923;&#36753;&#24418;&#24335;&#30340;&#35821;&#20041;&#65292;&#24182;&#25552;&#21319;&#29983;&#25104;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#24341;&#20154;&#27880;&#30446;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#36755;&#20986;&#36136;&#37327;&#19981;&#19968;&#33268;&#65292;&#32473;&#20174;&#36923;&#36753;&#24418;&#24335;&#65288;LFs&#65289;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#36825;&#20010;&#20219;&#21153;&#35201;&#27714;&#29983;&#25104;&#30340;&#36755;&#20986;&#20307;&#29616;LFs&#30340;&#30830;&#20999;&#35821;&#20041;&#65292;&#19981;&#36951;&#28431;&#20219;&#20309;LF&#35821;&#20041;&#25110;&#20135;&#29983;&#20219;&#20309;&#24187;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#21644;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#36890;&#36807;&#25552;&#31034;LLM&#21021;&#22987;&#21270;&#29983;&#25104;&#19968;&#32452;&#20505;&#36873;&#36755;&#20986;&#65292;&#28982;&#21518;&#20351;&#29992;&#29305;&#23450;&#20219;&#21153;&#30340;&#37325;&#26032;&#25490;&#24207;&#27169;&#22411;&#23545;&#23427;&#20204;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#25163;&#21160;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#25490;&#24207;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25152;&#36873;&#25321;&#30340;&#25490;&#24207;&#25351;&#26631;&#34987;&#29992;&#20110;&#22686;&#24378;&#37325;&#26032;&#25490;&#24207;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#36890;&#36807;&#23545;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#37325;&#26032;&#25490;&#24207;&#27169;&#22411;&#36873;&#25321;&#30340;&#20505;&#36873;&#39033;&#20248;&#20110;&#37027;&#20123;&#36873;&#39033;.
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive capabilities in natural language generation. However, their output quality can be inconsistent, posing challenges for generating natural language from logical forms (LFs). This task requires the generated outputs to embody the exact semantics of LFs, without missing any LF semantics or creating any hallucinations. In this work, we tackle this issue by proposing a novel generate-and-rerank approach. Our approach involves initially generating a set of candidate outputs by prompting an LLM and subsequently reranking them using a task-specific reranker model. In addition, we curate a manually collected dataset to evaluate the alignment between different ranking metrics and human judgements. The chosen ranking metrics are utilized to enhance the training and evaluation of the reranker model. By conducting extensive experiments on three diverse datasets, we demonstrate that the candidates selected by our reranker outperform those sele
&lt;/p&gt;</description></item><item><title>LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12288</link><description>&lt;p&gt;
&#32763;&#36716;&#35781;&#21650;: &#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#30340;"A&#26159;B"&#26080;&#27861;&#23398;&#20064;"B&#26159;A"
&lt;/p&gt;
&lt;p&gt;
The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A". (arXiv:2309.12288v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12288
&lt;/p&gt;
&lt;p&gt;
LLMs&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#33021;&#23398;&#20064;&#21040;"A&#26159;B"&#30340;&#32467;&#26500;&#65292;&#26080;&#27861;&#33258;&#21160;&#25512;&#24191;&#21040;"B&#26159;A"&#12290;&#36825;&#34920;&#26126;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#21644;&#35757;&#32451;&#38598;&#20013;&#27169;&#24335;&#30340;&#25512;&#24191;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25581;&#31034;&#20102;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#27867;&#21270;&#19978;&#30340;&#20196;&#20154;&#24778;&#35766;&#30340;&#22833;&#36133;&#12290;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"A&#26159;B"&#24418;&#24335;&#30340;&#21477;&#23376;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#25512;&#24191;&#21040;&#30456;&#21453;&#30340;&#26041;&#21521;"B&#26159;A"&#12290;&#36825;&#23601;&#26159;&#32763;&#36716;&#35781;&#21650;&#12290;&#20363;&#22914;&#65292;&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#26159;&#22522;&#20110;"Olaf Scholz&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;"&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#23427;&#19981;&#20250;&#33258;&#21160;&#33021;&#22815;&#22238;&#31572;&#38382;&#39064;"&#35841;&#26159;&#24503;&#22269;&#31532;&#20061;&#20219;&#24635;&#29702;&#65311;"&#12290;&#27492;&#22806;&#65292;&#27491;&#30830;&#31572;&#26696;&#65288;"Olaf Scholz"&#65289;&#30340;&#21487;&#33021;&#24615;&#19981;&#20250;&#27604;&#38543;&#26426;&#21517;&#23383;&#26356;&#39640;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#26029;&#19978;&#23384;&#22312;&#22522;&#26412;&#22833;&#36133;&#65292;&#24182;&#19988;&#19981;&#20250;&#25512;&#24191;&#21040;&#23427;&#20204;&#35757;&#32451;&#38598;&#20013;&#30340;&#26222;&#36941;&#27169;&#24335;&#65288;&#21363;&#22914;&#26524;&#20986;&#29616;"A&#26159;B"&#65292;&#21017;"B&#26159;A"&#26356;&#21487;&#33021;&#20986;&#29616;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#34394;&#26500;&#30340;&#38472;&#36848;&#65288;&#22914;"Uriah Hawthorne&#26159;'Abyssal Melodies'&#30340;&#20316;&#26354;&#23478;"&#65289;&#19978;&#23545;GPT-3&#21644;Llama-1&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23637;&#31034;&#23427;&#20204;&#26080;&#27861;&#27491;&#30830;&#22238;&#31572;"&#35841;&#21019;&#20316;&#20102;'Abyssal Melodies'?"&#26469;&#25552;&#20379;&#32763;&#36716;&#35781;&#21650;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Cu
&lt;/p&gt;</description></item><item><title>MetaMath&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#29983;&#25104;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.12284</link><description>&lt;p&gt;
MetaMath&#65306;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#33258;&#24049;&#30340;&#25968;&#23398;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. (arXiv:2309.12284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12284
&lt;/p&gt;
&lt;p&gt;
MetaMath&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#32534;&#20889;&#38382;&#39064;&#26469;&#29983;&#25104;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#26497;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24320;&#28304;LLMs&#65288;&#20363;&#22914;LLaMA-2&#65289;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#20173;&#28982;&#36828;&#36828;&#19981;&#22815;&#20196;&#20154;&#28385;&#24847;&#65292;&#21407;&#22240;&#26159;&#22797;&#26434;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MetaMath&#65292;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;&#27809;&#26377;&#39069;&#22806;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#20197;&#22810;&#20010;&#35282;&#24230;&#37325;&#26032;&#20889;&#20837;&#38382;&#39064;&#26469;&#24341;&#23548;&#25968;&#23398;&#38382;&#39064;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#21517;&#20026;MetaMathQA&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;MetaMathQA&#19978;&#23545;LLaMA-2&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#23545;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#20004;&#20010;&#27969;&#34892;&#22522;&#20934;&#27979;&#35797;&#65288;&#21363;GSM8K&#21644;MATH&#65289;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MetaMath&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#19968;&#22871;&#24320;&#28304;LLMs&#12290;&#25105;&#20204;&#30340;MetaMath-7B&#27169;&#22411;&#22312;GSM8K&#19978;&#36798;&#21040;&#20102;66.4&#65285;&#65292;&#22312;MATH&#19978;&#36798;&#21040;&#20102;19.4&#65285;&#65292;&#36229;&#36807;&#20102;&#30456;&#21516;&#35268;&#27169;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (\eg, LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose \emph{MetaMath}, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called {MetaMathQA}. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (\ie, GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves $66.4\%$ on GSM8K and $19.4\%$ on MATH, exceeding the state-of-the-art models of the same size by 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22806;&#37096;&#30693;&#35782;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#35299;NER&#20219;&#21153;&#20026;&#23454;&#20307;&#36328;&#24230;&#25552;&#21462;&#21644;&#23454;&#20307;&#31867;&#22411;&#30830;&#23450;&#20004;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#26469;&#22686;&#24378;&#23454;&#20307;&#31867;&#21035;&#30830;&#23450;&#24615;&#33021;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.12278</link><description>&lt;p&gt;
&#36890;&#36807;&#22806;&#37096;&#30693;&#35782;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Inspire the Large Language Model by External Knowledge on BioMedical Named Entity Recognition. (arXiv:2309.12278v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22806;&#37096;&#30693;&#35782;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#35299;NER&#20219;&#21153;&#20026;&#23454;&#20307;&#36328;&#24230;&#25552;&#21462;&#21644;&#23454;&#20307;&#31867;&#22411;&#30830;&#23450;&#20004;&#27493;&#39588;&#65292;&#24182;&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#26469;&#22686;&#24378;&#23454;&#20307;&#31867;&#21035;&#30830;&#23450;&#24615;&#33021;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#29983;&#25104;&#24615;&#20219;&#21153;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20123;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#23427;&#20204;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20219;&#21153;&#65292;&#22914;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;&#26412;&#25991;&#21463;&#21040;&#24605;&#32500;&#38142;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#65292;&#36890;&#36807;&#20998;&#35299;NER&#20219;&#21153;&#20026;&#23454;&#20307;&#36328;&#24230;&#25552;&#21462;&#21644;&#23454;&#20307;&#31867;&#22411;&#30830;&#23450;&#20004;&#27493;&#39588;&#26469;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#26102;&#32570;&#20047;&#39046;&#22495;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23454;&#20307;&#30693;&#35782;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20043;&#21069;&#30340;&#23569;&#26679;&#26412;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#20004;&#27493;&#39588;BioNER&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#22806;&#37096;&#30693;&#35782;&#30340;&#25972;&#21512;&#26174;&#33879;&#25552;&#39640;&#20102;&#23454;&#20307;&#31867;&#22411;&#30830;&#23450;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated dominating performance in many NLP tasks, especially on generative tasks. However, they often fall short in some information extraction tasks, particularly those requiring domain-specific knowledge, such as Biomedical Named Entity Recognition (NER). In this paper, inspired by Chain-of-thought, we leverage the LLM to solve the Biomedical NER step-by-step: break down the NER task into entity span extraction and entity type determination. Additionally, for entity type determination, we inject entity knowledge to address the problem that LLM's lack of domain knowledge when predicting entity category. Experimental results show a significant improvement in our two-step BioNER approach compared to previous few-shot LLM baseline. Additionally, the incorporation of external knowledge significantly enhances entity category determination performance.
&lt;/p&gt;</description></item><item><title>LLMR&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#21019;&#24314;&#21644;&#20462;&#25913;&#20132;&#20114;&#24335;&#28151;&#21512;&#29616;&#23454;&#20307;&#39564;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#21644;&#35774;&#35745;&#30446;&#26631;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#26631;&#20934;&#30340;GPT-4&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMR&#30340;&#36328;&#24179;&#21488;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#21644;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#29983;&#25104;&#21644;&#32534;&#36753;&#21508;&#31181;&#23545;&#35937;&#12289;&#24037;&#20855;&#21644;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12276</link><description>&lt;p&gt;
LLMR&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#25552;&#31034;&#20132;&#20114;&#24335;&#19990;&#30028;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMR: Real-time Prompting of Interactive Worlds using Large Language Models. (arXiv:2309.12276v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12276
&lt;/p&gt;
&lt;p&gt;
LLMR&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#21019;&#24314;&#21644;&#20462;&#25913;&#20132;&#20114;&#24335;&#28151;&#21512;&#29616;&#23454;&#20307;&#39564;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#21644;&#35774;&#35745;&#30446;&#26631;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#26631;&#20934;&#30340;GPT-4&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMR&#30340;&#36328;&#24179;&#21488;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#21644;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#29983;&#25104;&#21644;&#32534;&#36753;&#21508;&#31181;&#23545;&#35937;&#12289;&#24037;&#20855;&#21644;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#28151;&#21512;&#29616;&#23454;&#22330;&#26223;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMR)&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#21019;&#24314;&#21644;&#20462;&#25913;&#20132;&#20114;&#24335;&#28151;&#21512;&#29616;&#23454;&#20307;&#39564;&#12290;LLMR&#21033;&#29992;&#20102;&#26032;&#39062;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#25110;&#35774;&#35745;&#30446;&#26631;&#38656;&#35201;&#21512;&#25104;&#20869;&#37096;&#21160;&#24577;&#12289;&#30452;&#35266;&#20998;&#26512;&#25110;&#39640;&#32423;&#20132;&#20114;&#30340;&#22256;&#38590;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20381;&#36182;&#20110;&#25991;&#26412;&#20132;&#20114;&#21644;Unity&#28216;&#25103;&#24341;&#25806;&#12290;&#36890;&#36807;&#34701;&#21512;&#22330;&#26223;&#29702;&#35299;&#12289;&#20219;&#21153;&#35268;&#21010;&#12289;&#33258;&#25105;&#35843;&#35797;&#21644;&#20869;&#23384;&#31649;&#29702;&#25216;&#26415;&#65292;LLMR&#22312;&#24179;&#22343;&#38169;&#35823;&#29575;&#19978;&#27604;&#26631;&#20934;&#30340;GPT-4&#25552;&#39640;&#20102;4&#20493;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMR&#19982;&#20960;&#20010;&#31034;&#20363;&#19990;&#30028;&#30340;&#36328;&#24179;&#21488;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#21019;&#24314;&#21644;&#20462;&#25913;&#20219;&#21153;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20197;&#23637;&#31034;&#23427;&#33021;&#22815;&#29983;&#25104;&#21644;&#32534;&#36753;&#21508;&#31181;&#23545;&#35937;&#12289;&#24037;&#20855;&#21644;&#22330;&#26223;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#26377;&#22810;&#26679;&#24615;&#30340;&#21487;&#29992;&#24615;&#30740;&#31350;&#65288;N=11&#65289;&#65292;&#25581;&#31034;&#20102;&#21442;&#19982;&#32773;&#23545;&#35813;&#31995;&#32479;&#26377;&#31215;&#26497;&#30340;&#20307;&#39564;&#65292;&#24182;&#24895;&#24847;&#20877;&#27425;&#20351;&#29992;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Large Language Model for Mixed Reality (LLMR), a framework for the real-time creation and modification of interactive Mixed Reality experiences using LLMs. LLMR leverages novel strategies to tackle difficult cases where ideal training data is scarce, or where the design goal requires the synthesis of internal dynamics, intuitive analysis, or advanced interactivity. Our framework relies on text interaction and the Unity game engine. By incorporating techniques for scene understanding, task planning, self-debugging, and memory management, LLMR outperforms the standard GPT-4 by 4x in average error rate. We demonstrate LLMR's cross-platform interoperability with several example worlds, and evaluate it on a variety of creation and modification tasks to show that it can produce and edit diverse objects, tools, and scenes. Finally, we conducted a usability study (N=11) with a diverse set that revealed participants had positive experiences with the system and would use it again.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#20020;&#24202;&#19987;&#23478;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;VTE&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#20934;&#30830;&#35782;&#21035;VTE&#20107;&#20214;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2309.12273</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#22522;&#20110;&#20020;&#24202;&#19987;&#23478;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#25913;&#36827;VTE&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports. (arXiv:2309.12273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12273
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#20020;&#24202;&#19987;&#23478;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#25913;&#36827;VTE&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#20934;&#30830;&#35782;&#21035;VTE&#20107;&#20214;&#30340;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#20934;&#30830;&#22320;&#35782;&#21035;&#38745;&#33033;&#34880;&#26643;&#26643;&#22622;&#65288;VTE&#65289;&#65292;&#21253;&#25324;&#28145;&#38745;&#33033;&#34880;&#26643;&#65288;DVT&#65289;&#21644;&#32954;&#26643;&#22622;&#65288;PE&#65289;&#65292;&#23545;&#20110;&#26377;&#25928;&#27835;&#30103;&#38750;&#24120;&#37325;&#35201;&#12290;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#65292;&#33258;&#21160;&#21270;&#26041;&#27861;&#24050;&#32463;&#22312;&#20174;&#22238;&#39038;&#24615;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;VTE&#20107;&#20214;&#25110;&#24110;&#21161;&#20020;&#24202;&#19987;&#23478;&#35782;&#21035;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#30340;VTE&#20107;&#20214;&#26041;&#38754;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26631;&#35760;&#26377;&#38480;&#30340;&#21307;&#23398;&#25991;&#26412;&#25968;&#25454;&#12289;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#22797;&#26434;&#24615;&#21644;&#24322;&#36136;&#24615;&#20197;&#21450;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#26377;&#25928;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#21644;NLP&#27169;&#22411;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DL&#26041;&#27861;&#30340;&#26032;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#32467;&#21512;&#25968;&#25454;&#22686;&#24378;&#12289;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;NLP&#27169;&#22411;&#36873;&#25321;&#21644;&#20020;&#24202;&#19987;&#23478;NLP&#22522;&#20110;&#35268;&#21017;&#30340;&#20998;&#31867;&#22120;&#65292;&#20197;&#25552;&#39640;&#38750;&#32467;&#26500;&#21270;&#65288;&#33258;&#30001;&#25991;&#26412;&#65289;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;VTE&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid and accurate identification of Venous thromboembolism (VTE), a severe cardiovascular condition including deep vein thrombosis (DVT) and pulmonary embolism (PE), is important for effective treatment. Leveraging Natural Language Processing (NLP) on radiology reports, automated methods have shown promising advancements in identifying VTE events from retrospective data cohorts or aiding clinical experts in identifying VTE events from radiology reports. However, effectively training Deep Learning (DL) and the NLP models is challenging due to limited labeled medical text data, the complexity and heterogeneity of radiology reports, and data imbalance. This study proposes novel method combinations of DL methods, along with data augmentation, adaptive pre-trained NLP model selection, and a clinical expert NLP rule-based classifier, to improve the accuracy of VTE identification in unstructured (free-text) radiology reports. Our experimental results demonstrate the model's efficacy, achievi
&lt;/p&gt;</description></item><item><title>&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#26159;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;&#33521;&#22269;&#30340;&#36229;&#36807;250,000&#20010;&#27861;&#24237;&#26696;&#20363;&#12290;&#22312;&#35813;&#35821;&#26009;&#24211;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26696;&#20363;&#32467;&#26524;&#30340;&#19987;&#23478;&#27880;&#35299;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#20102;&#26696;&#20363;&#32467;&#26524;&#25552;&#21462;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2309.12269</link><description>&lt;p&gt;
&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#65306;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
The Cambridge Law Corpus: A Corpus for Legal AI Research. (arXiv:2309.12269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12269
&lt;/p&gt;
&lt;p&gt;
&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#26159;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#26469;&#33258;&#33521;&#22269;&#30340;&#36229;&#36807;250,000&#20010;&#27861;&#24237;&#26696;&#20363;&#12290;&#22312;&#35813;&#35821;&#26009;&#24211;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26696;&#20363;&#32467;&#26524;&#30340;&#19987;&#23478;&#27880;&#35299;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#20102;&#26696;&#20363;&#32467;&#26524;&#25552;&#21462;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#21073;&#26725;&#27861;&#24459;&#35821;&#26009;&#24211;&#65288;CLC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;&#12290;&#23427;&#21253;&#21547;&#20102;&#26469;&#33258;&#33521;&#22269;&#30340;&#36229;&#36807;250,000&#20010;&#27861;&#24237;&#26696;&#20363;&#12290;&#22823;&#37096;&#20998;&#26696;&#20363;&#26469;&#33258;21&#19990;&#32426;&#65292;&#20294;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;&#20102;16&#19990;&#32426;&#20197;&#26469;&#30340;&#26696;&#20363;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#35813;&#35821;&#26009;&#24211;&#30340;&#39318;&#27425;&#21457;&#24067;&#65292;&#21253;&#25324;&#21407;&#22987;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#12290;&#22312;&#35821;&#26009;&#24211;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;638&#20010;&#26696;&#20363;&#30340;&#27861;&#24459;&#19987;&#23478;&#23545;&#26696;&#20363;&#32467;&#26524;&#30340;&#27880;&#35299;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#35757;&#32451;&#21644;&#35780;&#20272;&#20102;GPT-3&#12289;GPT-4&#21644;RoBERTa&#27169;&#22411;&#36827;&#34892;&#26696;&#20363;&#32467;&#26524;&#25552;&#21462;&#65292;&#20197;&#25552;&#20379;&#22522;&#20934;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27861;&#24459;&#21644;&#20262;&#29702;&#35752;&#35770;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#26448;&#26009;&#21487;&#33021;&#20855;&#26377;&#25935;&#24863;&#24615;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#35813;&#35821;&#26009;&#24211;&#21482;&#20250;&#22312;&#19968;&#23450;&#38480;&#21046;&#19979;&#29992;&#20110;&#30740;&#31350;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Cambridge Law Corpus (CLC), a corpus for legal AI research. It consists of over 250 000 court cases from the UK. Most cases are from the 21st century, but the corpus includes cases as old as the 16th century. This paper presents the first release of the corpus, containing the raw text and meta-data. Together with the corpus, we provide annotations on case outcomes for 638 cases, done by legal experts. Using our annotated data, we have trained and evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to provide benchmarks. We include an extensive legal and ethical discussion to address the potentially sensitive nature of this material. As a consequence, the corpus will only be released for research purposes under certain restrictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Prompt Tuning&#22312;&#19982;"&#25216;&#33021;&#31070;&#32463;&#20803;"&#30340;&#20851;&#31995;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#29305;&#23450;&#20219;&#21153;&#30340;&#35843;&#25972;&#25351;&#20196;&#22312;&#30456;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#19978;&#20855;&#26377;&#20256;&#36882;&#24615;&#65292;&#20294;&#23545;&#20110;&#23545;&#25239;&#24615;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#19981;&#39640;&#65292;&#20854;&#20013;T5&#30340;&#40065;&#26834;&#24615;&#27604;RoBERTa&#26356;&#39640;&#65292;&#24182;&#19988;&#21457;&#29616;T5&#21644;RoBERTa&#20013;&#37117;&#23384;&#22312;&#25216;&#33021;&#31070;&#32463;&#20803;&#12290;</title><link>http://arxiv.org/abs/2309.12263</link><description>&lt;p&gt;
&#20851;&#20110;&#25216;&#33021;&#31070;&#32463;&#20803;&#19982;Prompt Tuning&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;&#20851;&#31995;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Relationship between Skill Neurons and Robustness in Prompt Tuning. (arXiv:2309.12263v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Prompt Tuning&#22312;&#19982;"&#25216;&#33021;&#31070;&#32463;&#20803;"&#30340;&#20851;&#31995;&#20013;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#29305;&#23450;&#20219;&#21153;&#30340;&#35843;&#25972;&#25351;&#20196;&#22312;&#30456;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#19978;&#20855;&#26377;&#20256;&#36882;&#24615;&#65292;&#20294;&#23545;&#20110;&#23545;&#25239;&#24615;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#19981;&#39640;&#65292;&#20854;&#20013;T5&#30340;&#40065;&#26834;&#24615;&#27604;RoBERTa&#26356;&#39640;&#65292;&#24182;&#19988;&#21457;&#29616;T5&#21644;RoBERTa&#20013;&#37117;&#23384;&#22312;&#25216;&#33021;&#31070;&#32463;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt Tuning&#26159;&#19968;&#31181;&#29992;&#20110;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(PLMs)&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#23545;RoBERTa&#30340;&#23454;&#39564;&#65292;&#26377;&#20154;&#35748;&#20026;Prompt Tuning&#28608;&#27963;&#20102;Transformer&#21069;&#39304;&#32593;&#32476;&#20013;&#29305;&#23450;&#30340;&#31070;&#32463;&#20803;&#65292;&#36825;&#20123;&#31070;&#32463;&#20803;&#23545;&#32473;&#23450;&#20219;&#21153;&#20855;&#26377;&#39640;&#39044;&#27979;&#33021;&#21147;&#21644;&#36873;&#25321;&#24615;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;RoBERTa&#21644;T5&#26469;&#30740;&#31350;Prompt Tuning&#19982;&#36825;&#20123;&#8220;&#25216;&#33021;&#31070;&#32463;&#20803;&#8221;&#30340;&#40065;&#26834;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#29305;&#23450;&#20219;&#21153;&#30340;&#35843;&#25972;&#25351;&#20196;&#22312;&#30456;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#19978;&#20855;&#26377;&#20256;&#36882;&#24615;&#65292;&#20294;&#23545;&#20110;&#23545;&#25239;&#24615;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#19981;&#39640;&#65292;&#20854;&#20013;T5&#30340;&#40065;&#26834;&#24615;&#27604;RoBERTa&#26356;&#39640;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#37325;&#29616;&#20102;RoBERTa&#20013;&#30340;&#25216;&#33021;&#31070;&#32463;&#20803;&#23384;&#22312;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;T5&#20013;&#20063;&#23384;&#22312;&#25216;&#33021;&#31070;&#32463;&#20803;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;T5&#22312;&#38750;&#23545;&#25239;&#24615;&#25968;&#25454;&#19978;&#30830;&#23450;&#30340;&#25216;&#33021;&#31070;&#32463;&#20803;&#20063;&#26159;&#23545;&#25239;&#24615;&#25968;&#25454;&#19978;&#39044;&#27979;&#24615;&#26368;&#24378;&#30340;&#31070;&#32463;&#20803;&#65292;&#32780;&#36825;&#22312;RoBERTa&#20013;&#19981;&#26159;&#36825;&#31181;&#24773;&#20917;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#26356;&#39640;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#21487;&#33021;&#19982;&#25216;&#33021;&#31070;&#32463;&#20803;&#30340;&#23384;&#22312;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt Tuning is a popular parameter-efficient finetuning method for pre-trained large language models (PLMs). Recently, based on experiments with RoBERTa, it has been suggested that Prompt Tuning activates specific neurons in the transformer's feed-forward networks, that are highly predictive and selective for the given task. In this paper, we study the robustness of Prompt Tuning in relation to these "skill neurons", using RoBERTa and T5. We show that prompts tuned for a specific task are transferable to tasks of the same type but are not very robust to adversarial data, with higher robustness for T5 than RoBERTa. At the same time, we replicate the existence of skill neurons in RoBERTa and further show that skill neurons also seem to exist in T5. Interestingly, the skill neurons of T5 determined on non-adversarial data are also among the most predictive neurons on the adversarial data, which is not the case for RoBERTa. We conclude that higher adversarial robustness may be related to
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#31572;&#31995;&#32479;&#35780;&#20272;&#25351;&#26631;SQuArE&#65292;&#20351;&#29992;&#22810;&#20010;&#21442;&#32771;&#31572;&#26696;&#36827;&#34892;&#21477;&#23376;&#32423;&#38382;&#31572;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#39640;&#24230;&#30456;&#20851;&#24615;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.12250</link><description>&lt;p&gt;
SQUARE: &#20351;&#29992;&#22810;&#20010;&#27491;&#36127;&#21442;&#32771;&#31572;&#26696;&#33258;&#21160;&#35780;&#20272;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SQUARE: Automatic Question Answering Evaluation using Multiple Positive and Negative References. (arXiv:2309.12250v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12250
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#31572;&#31995;&#32479;&#35780;&#20272;&#25351;&#26631;SQuArE&#65292;&#20351;&#29992;&#22810;&#20010;&#21442;&#32771;&#31572;&#26696;&#36827;&#34892;&#21477;&#23376;&#32423;&#38382;&#31572;&#35780;&#20272;&#65292;&#23454;&#29616;&#20102;&#39640;&#24230;&#30456;&#20851;&#24615;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#31995;&#32479;&#30340;&#35780;&#20272;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#26114;&#36149;&#30340;&#65292;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#38382;&#39064;&#30340;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;Transformer LM&#32534;&#30721;&#22120;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#22312;&#38382;&#31572;&#35780;&#20272;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#36801;&#31227;&#24615;&#65292;&#20294;&#23427;&#20204;&#30340;&#20351;&#29992;&#21463;&#38480;&#20110;&#21333;&#20010;&#27491;&#30830;&#21442;&#32771;&#31572;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65306;SQuArE&#65288;&#21477;&#23376;&#32423;&#38382;&#31572;&#35780;&#20272;&#65289;&#65292;&#20351;&#29992;&#22810;&#20010;&#21442;&#32771;&#31572;&#26696;&#65288;&#32452;&#21512;&#22810;&#20010;&#27491;&#30830;&#21644;&#38169;&#35823;&#30340;&#21442;&#32771;&#31572;&#26696;&#65289;&#36827;&#34892;&#21477;&#23376;&#24418;&#24335;&#30340;&#38382;&#31572;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;&#21477;&#23376;&#32423;&#25552;&#21462;&#24335;&#65288;&#31572;&#26696;&#36873;&#25321;&#65289;&#21644;&#29983;&#25104;&#24335;&#65288;GenQA&#65289;&#38382;&#31572;&#31995;&#32479;&#19978;&#35780;&#20272;&#20102;SQuArE&#65292;&#22312;&#22810;&#20010;&#23398;&#26415;&#21644;&#24037;&#19994;&#25968;&#25454;&#38598;&#19978;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20248;&#20110;&#20808;&#21069;&#30340;&#22522;&#20934;&#65292;&#24182;&#19982;&#20154;&#24037;&#26631;&#27880;&#20855;&#26377;&#26368;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation of QA systems is very challenging and expensive, with the most reliable approach being human annotations of correctness of answers for questions. Recent works (AVA, BEM) have shown that transformer LM encoder based similarity metrics transfer well for QA evaluation, but they are limited by the usage of a single correct reference answer. We propose a new evaluation metric: SQuArE (Sentence-level QUestion AnsweRing Evaluation), using multiple reference answers (combining multiple correct and incorrect references) for sentence-form QA. We evaluate SQuArE on both sentence-level extractive (Answer Selection) and generative (GenQA) QA systems, across multiple academic and industrial datasets, and show that it outperforms previous baselines and obtains the highest correlation with human annotations.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#22797;&#26434;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25581;&#31034;&#20551;&#26032;&#38395;&#24182;&#25552;&#20379;&#22810;&#35282;&#24230;&#35299;&#37322;&#65292;&#20294;&#20173;&#19981;&#22914;&#32463;&#36807;fine-tuned&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#21462;&#20195;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#33391;&#22909;&#30340;&#36741;&#21161;&#39038;&#38382;&#12290;</title><link>http://arxiv.org/abs/2309.12247</link><description>&lt;p&gt;
&#22351;&#35282;&#33394;&#22909;&#39038;&#38382;&#65306;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection. (arXiv:2309.12247v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12247
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#22797;&#26434;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25581;&#31034;&#20551;&#26032;&#38395;&#24182;&#25552;&#20379;&#22810;&#35282;&#24230;&#35299;&#37322;&#65292;&#20294;&#20173;&#19981;&#22914;&#32463;&#36807;fine-tuned&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#21462;&#20195;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#33391;&#22909;&#30340;&#36741;&#21161;&#39038;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#20551;&#26032;&#38395;&#38656;&#35201;&#23545;&#22810;&#26679;&#32447;&#32034;&#26377;&#25935;&#38160;&#30340;&#24863;&#30693;&#21644;&#23545;&#29616;&#23454;&#19990;&#30028;&#32972;&#26223;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#23545;&#20110;&#22522;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#27979;&#22120;&#26469;&#35828;&#65292;&#30001;&#20110;&#20854;&#30693;&#35782;&#21644;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#36825;&#20173;&#28982;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#27493;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#20197;&#21450;&#22914;&#20309;&#24110;&#21161;&#20551;&#26032;&#38395;&#26816;&#27979;&#20173;&#28982;&#26410;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#20687;GPT 3.5&#36825;&#26679;&#30340;&#22797;&#26434;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#25581;&#31034;&#20551;&#26032;&#38395;&#24182;&#25552;&#20379;&#29702;&#24819;&#30340;&#22810;&#35282;&#24230;&#35299;&#37322;&#65292;&#20294;&#20173;&#28982;&#19981;&#22914;&#22522;&#30784;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;fine-tuned BERT&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#38543;&#21518;&#30340;&#20998;&#26512;&#23558;&#36825;&#31181;&#24046;&#36317;&#24402;&#22240;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#33021;&#27491;&#30830;&#36873;&#25321;&#24182;&#25972;&#21512;&#35777;&#25454;&#20197;&#24471;&#20986;&#32467;&#35770;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#21462;&#20195;&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#32463;&#36807;fine-tuned&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#33391;&#22909;&#30340;&#36741;&#21161;&#39038;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting fake news requires both a delicate sense of diverse clues and a profound understanding of the real-world background, which remains challenging for detectors based on small language models (SLMs) due to their knowledge and capability limitations. Recent advances in large language models (LLMs) have shown remarkable performance in various tasks, but whether and how LLMs could help with fake news detection remains underexplored. In this paper, we investigate the potential of LLMs in fake news detection. First, we conduct an empirical study and find that a sophisticated LLM such as GPT 3.5 could generally expose fake news and provide desirable multi-perspective rationales but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis attributes such a gap to the LLM's inability to select and integrate rationales properly to conclude. Based on these findings, we propose that current LLMs may not substitute fine-tuned SLMs in fake news detection but can be a good a
&lt;/p&gt;</description></item><item><title>ChaCha&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#40723;&#21169;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#12290;&#36890;&#36807;&#19968;&#20010;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#20799;&#31461;&#23558;ChaCha&#35270;&#20026;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#24895;&#24847;&#19982;&#20854;&#20998;&#20139;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#12290;</title><link>http://arxiv.org/abs/2309.12244</link><description>&lt;p&gt;
ChaCha&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#20799;&#31461;&#20998;&#20139;&#19982;&#20010;&#20154;&#20107;&#20214;&#30456;&#20851;&#30340;&#24773;&#32490;
&lt;/p&gt;
&lt;p&gt;
ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events. (arXiv:2309.12244v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12244
&lt;/p&gt;
&lt;p&gt;
ChaCha&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#40723;&#21169;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#12290;&#36890;&#36807;&#19968;&#20010;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;&#20799;&#31461;&#23558;ChaCha&#35270;&#20026;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#24895;&#24847;&#19982;&#20854;&#20998;&#20139;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20799;&#31461;&#36890;&#24120;&#36890;&#36807;&#19982;&#23478;&#20154;&#25110;&#20182;&#20154;&#20998;&#20139;&#25925;&#20107;&#21644;&#24863;&#21463;&#26469;&#23398;&#20064;&#36776;&#35782;&#21644;&#34920;&#36798;&#24773;&#32490;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#20799;&#31461;&#27491;&#22312;&#21457;&#23637;&#20182;&#20204;&#30340;&#20132;&#27969;&#25216;&#33021;&#65292;&#29238;&#27597;&#25110;&#20804;&#24351;&#22992;&#22969;&#24456;&#38590;&#19982;&#20182;&#20204;&#36827;&#34892;&#24773;&#24863;&#27807;&#36890;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChaCha&#65292;&#19968;&#20010;&#40723;&#21169;&#21644;&#24341;&#23548;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#21644;&#30456;&#20851;&#24773;&#32490;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;ChaCha&#32467;&#21512;&#20102;&#29366;&#24577;&#26426;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#36827;&#34892;&#33258;&#30001;&#23545;&#35805;&#30340;&#21516;&#26102;&#20445;&#25345;&#23545;&#35805;&#30340;&#26041;&#21521;&#24615;&#12290;&#36890;&#36807;&#19982;20&#21517;&#24180;&#40836;&#22312;8-12&#23681;&#30340;&#20799;&#31461;&#36827;&#34892;&#30340;&#25506;&#32034;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;ChaCha&#22914;&#20309;&#20419;&#20351;&#20799;&#31461;&#20998;&#20139;&#20010;&#20154;&#20107;&#20214;&#24182;&#24341;&#23548;&#20182;&#20204;&#25551;&#36848;&#30456;&#20851;&#24773;&#32490;&#12290;&#21442;&#19982;&#32773;&#35748;&#20026;ChaCha&#23601;&#20687;&#19968;&#20010;&#20146;&#23494;&#30340;&#26379;&#21451;&#65292;&#24182;&#20998;&#20139;&#20102;&#21508;&#31181;&#20027;&#39064;&#30340;&#25925;&#20107;&#65292;&#22914;&#23478;&#24237;&#26053;&#34892;&#21644;&#20010;&#20154;&#25104;&#23601;&#12290;&#22522;&#20110;&#23450;&#37327;&#21644;&#23450;&#24615;&#21457;&#29616;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21033;&#29992;LLMs&#35774;&#35745;&#36866;&#21512;&#20799;&#31461;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children typically learn to identify and express emotions through sharing their stories and feelings with others, particularly their family. However, it is challenging for parents or siblings to have emotional communication with children since children are still developing their communication skills. We present ChaCha, a chatbot that encourages and guides children to share personal events and associated emotions. ChaCha combines a state machine and large language models (LLMs) to keep the dialogue on track while carrying on free-form conversations. Through an exploratory study with 20 children (aged 8-12), we examine how ChaCha prompts children to share personal events and guides them to describe associated emotions. Participants perceived ChaCha as a close friend and shared their stories on various topics, such as family trips and personal achievements. Based on the quantitative and qualitative findings, we discuss opportunities for leveraging LLMs to design child-friendly chatbots to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#27493;&#21452;&#35821;CTC&#26694;&#26550;&#65292;&#29992;&#20110;&#26725;&#25509;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#27169;&#24577;&#21644;&#35821;&#35328;&#24046;&#36317;&#65292;&#24182;&#22312;&#36164;&#28304;&#26377;&#38480;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;state-of-the-art&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#22312;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.12234</link><description>&lt;p&gt;
&#26725;&#25509;&#27169;&#24577;&#21644;&#35821;&#35328;&#24046;&#36317;&#65306;&#21516;&#27493;&#21452;&#35821;CTC&#29992;&#20110;&#35821;&#38899;&#32763;&#35793;&#21644;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gaps of Both Modality and Language: Synchronous Bilingual CTC for Speech Translation and Speech Recognition. (arXiv:2309.12234v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#27493;&#21452;&#35821;CTC&#26694;&#26550;&#65292;&#29992;&#20110;&#26725;&#25509;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#27169;&#24577;&#21644;&#35821;&#35328;&#24046;&#36317;&#65292;&#24182;&#22312;&#36164;&#28304;&#26377;&#38480;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;state-of-the-art&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#22312;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21516;&#27493;&#21452;&#35821;CTC&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21452;&#37325;CTC&#26469;&#24357;&#21512;&#35821;&#38899;&#32763;&#35793;&#20219;&#21153;&#20013;&#27169;&#24577;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#23558;&#36716;&#20889;&#21644;&#32763;&#35793;&#20316;&#20026;CTC&#30340;&#24182;&#34892;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26725;&#25509;&#20102;&#38899;&#39057;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#21450;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;CTC&#24212;&#29992;&#30340;&#26368;&#26032;&#36827;&#23637;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#21464;&#20307;BiL-CTC+&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#22330;&#26223;&#19979;&#22312;MuST-C&#35821;&#38899;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#19978;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#65292;&#25581;&#31034;&#20102;&#36328;&#35821;&#35328;&#23398;&#20064;&#23545;&#36716;&#24405;&#30340;&#24433;&#21709;&#24182;&#23637;&#31034;&#20102;&#20854;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we present synchronous bilingual Connectionist Temporal Classification (CTC), an innovative framework that leverages dual CTC to bridge the gaps of both modality and language in the speech translation (ST) task. Utilizing transcript and translation as concurrent objectives for CTC, our model bridges the gap between audio and text as well as between source and target languages. Building upon the recent advances in CTC application, we develop an enhanced variant, BiL-CTC+, that establishes new state-of-the-art performances on the MuST-C ST benchmarks under resource-constrained scenarios. Intriguingly, our method also yields significant improvements in speech recognition performance, revealing the effect of cross-lingual learning on transcription and demonstrating its broad applicability. The source code is available at https://github.com/xuchennlp/S2T.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#21307;&#23398;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#31572;&#26696;&#26469;&#22238;&#31572;&#20844;&#20247;&#25552;&#20986;&#30340;&#20581;&#24247;&#30456;&#20851;&#38382;&#39064;&#65292;&#20854;&#20013;&#20851;&#38190;&#25361;&#25112;&#26159;&#21307;&#23398;&#39046;&#22495;&#32570;&#20047;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#20379;&#35270;&#35273;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.12224</link><description>&lt;p&gt;
&#20174;&#21307;&#23398;&#35270;&#39057;&#20013;&#22238;&#31572;&#20581;&#24247;&#30456;&#20851;&#38382;&#39064;&#65306;&#25968;&#25454;&#38598;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Answering Health-related Questions from Medical Videos: Datasets and Approaches. (arXiv:2309.12224v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#21307;&#23398;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#31572;&#26696;&#26469;&#22238;&#31572;&#20844;&#20247;&#25552;&#20986;&#30340;&#20581;&#24247;&#30456;&#20851;&#38382;&#39064;&#65292;&#20854;&#20013;&#20851;&#38190;&#25361;&#25112;&#26159;&#21307;&#23398;&#39046;&#22495;&#32570;&#20047;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#26377;&#25928;&#22320;&#25552;&#20379;&#35270;&#35273;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#35270;&#39057;&#30340;&#22686;&#21152;&#25913;&#21464;&#20102;&#25105;&#20204;&#33719;&#21462;&#20449;&#24687;&#21644;&#30693;&#35782;&#30340;&#26041;&#24335;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#29616;&#22312;&#21916;&#27426;&#20351;&#29992;&#25945;&#23398;&#35270;&#39057;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#36880;&#27493;&#25805;&#20316;&#30340;&#27493;&#39588;&#26469;&#23436;&#25104;&#29305;&#23450;&#20219;&#21153;&#12290;&#21307;&#23398;&#39046;&#22495;&#30340;&#25945;&#23398;&#35270;&#39057;&#21487;&#33021;&#25552;&#20379;&#20102;&#20851;&#20110;&#24613;&#25937;&#12289;&#21307;&#30103;&#24613;&#30151;&#21644;&#21307;&#23398;&#25945;&#32946;&#38382;&#39064;&#30340;&#26368;&#20339;&#35270;&#35273;&#31572;&#26696;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#21307;&#23398;&#35270;&#39057;&#20013;&#30340;&#35270;&#35273;&#31572;&#26696;&#26469;&#22238;&#31572;&#20844;&#20247;&#25552;&#20986;&#30340;&#20581;&#24247;&#30456;&#20851;&#38382;&#39064;&#12290;&#21307;&#23398;&#39046;&#22495;&#32570;&#20047;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26159;&#38459;&#30861;&#24320;&#21457;&#33021;&#22815;&#24110;&#21161;&#20844;&#20247;&#35299;&#31572;&#20581;&#24247;&#30456;&#20851;&#38382;&#39064;&#30340;&#24212;&#29992;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#27969;&#27700;&#32447;&#26041;&#27861;&#26469;&#21019;&#24314;&#20004;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65306;HealthVidQA-CRF&#21644;HealthVidQA-Prompt&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#21307;&#23398;&#35270;&#39057;&#20013;&#25552;&#20379;&#35270;&#35273;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increase in the availability of online videos has transformed the way we access information and knowledge. A growing number of individuals now prefer instructional videos as they offer a series of step-by-step procedures to accomplish particular tasks. The instructional videos from the medical domain may provide the best possible visual answers to first aid, medical emergency, and medical education questions. Toward this, this paper is focused on answering health-related questions asked by the public by providing visual answers from medical videos. The scarcity of large-scale datasets in the medical domain is a key challenge that hinders the development of applications that can help the public with their health-related questions. To address this issue, we first proposed a pipelined approach to create two large-scale datasets: HealthVidQA-CRF and HealthVidQA-Prompt. Later, we proposed monomodal and multimodal approaches that can effectively provide visual answers from medical videos
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#35745;&#31639;&#26041;&#38754;&#30340;&#38480;&#21046;&#24615;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#25311;&#23545;&#35805;&#65292;&#24182;&#22312;&#27599;&#20010;&#23398;&#29983;&#22238;&#31572;&#35302;&#21457;&#33258;&#35328;&#33258;&#35821;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12161</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20934;&#30830;&#35745;&#31639;&#30340;&#20195;&#30721;&#29420;&#30333;
&lt;/p&gt;
&lt;p&gt;
Code Soliloquies for Accurate Calculations in Large Language Models. (arXiv:2309.12161v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12161
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#35745;&#31639;&#26041;&#38754;&#30340;&#38480;&#21046;&#24615;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#27169;&#25311;&#23545;&#35805;&#65292;&#24182;&#22312;&#27599;&#20010;&#23398;&#29983;&#22238;&#31572;&#35302;&#21457;&#33258;&#35328;&#33258;&#35821;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#23545;&#20110;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21518;&#31471;&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITS&#65289;&#30340;&#25104;&#21151;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#36825;&#20123;&#25968;&#25454;&#38598;&#29992;&#20110;&#23545;LLM&#21518;&#31471;&#36827;&#34892;&#32454;&#35843;&#26102;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#23398;&#29983;&#21644;ITS&#20043;&#38388;&#30340;&#20114;&#21160;&#36136;&#37327;&#12290;&#24320;&#21457;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#24120;&#35265;&#31574;&#30053;&#28041;&#21450;&#20351;&#29992;&#20808;&#36827;&#30340;GPT-4&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#30340;&#23398;&#29983;-&#25945;&#24072;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#23545;&#35805;&#38656;&#35201;&#36827;&#34892;&#29289;&#29702;&#31561;&#31185;&#30446;&#20013;&#24120;&#35265;&#30340;&#22797;&#26434;&#35745;&#31639;&#26102;&#65292;&#23601;&#20250;&#20986;&#29616;&#25361;&#25112;&#12290;&#23613;&#31649;&#20854;&#20808;&#36827;&#30340;&#21151;&#33021;&#65292;GPT-4&#22312;&#21487;&#38752;&#22788;&#29702;&#29978;&#33267;&#31616;&#21333;&#30340;&#20056;&#27861;&#20219;&#21153;&#26041;&#38754;&#30340;&#34920;&#29616;&#36824;&#19981;&#22815;&#65292;&#36825;&#26159;&#20854;&#22312;&#36825;&#20123;&#31185;&#30446;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26377;&#29366;&#24577;&#25552;&#31034;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#19968;&#20010;&#30001;GPT-4&#27169;&#25311;&#30340;&#23398;&#29983;&#21644;&#23548;&#24072;&#26426;&#22120;&#20154;&#20043;&#38388;&#30340;&#27169;&#25311;&#23545;&#35805;&#12290;&#27599;&#20010;&#23398;&#29983;&#30340;&#22238;&#31572;&#37117;&#20250;&#35302;&#21457;&#19968;&#20010;&#33258;&#35328;&#33258;&#35821;&#65288;&#20869;&#24515;&#29420;&#30333;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality conversational datasets are integral to the successful development of Intelligent Tutoring Systems (ITS) that employ a Large Language Model (LLM) backend. These datasets, when used to fine-tune the LLM backend, significantly enhance the quality of interactions between students and ITS. A common strategy for developing these datasets involves generating synthetic student-teacher dialogues using advanced GPT-4 models. However, challenges arise when these dialogues demand complex calculations, common in subjects like physics. Despite its advanced capabilities, GPT-4's performance falls short in reliably handling even simple multiplication tasks, marking a significant limitation in its utility for these subjects. To address these challenges, this paper introduces an innovative stateful prompt design. Our approach generates a mock conversation between a student and a tutorbot, both roles simulated by GPT-4. Each student response triggers a soliloquy (an inner monologue) in the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#22810;&#26041;&#35328;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#23545;&#35805;&#30340;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#25351;&#20986;&#20102;&#38463;&#25289;&#20271;&#35821;&#36164;&#28304;&#19981;&#36275;&#30340;&#38382;&#39064;&#20197;&#21450;&#29616;&#26377;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#22788;&#29702;&#38463;&#25289;&#20271;&#26041;&#35328;&#26102;&#30340;&#22256;&#38590;&#12290;&#35813;&#30740;&#31350;&#30340;&#37325;&#28857;&#22312;&#20110;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#38463;&#25289;&#20271;&#21508;&#31181;&#26041;&#35328;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2309.12137</link><description>&lt;p&gt;
OSN-MDAD&#65306;&#29992;&#20110;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#19978;&#38463;&#25289;&#20271;&#22810;&#26041;&#35328;&#23545;&#35805;&#30340;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OSN-MDAD: Machine Translation Dataset for Arabic Multi-Dialectal Conversations on Online Social Media. (arXiv:2309.12137v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12137
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#38463;&#25289;&#20271;&#22810;&#26041;&#35328;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#23545;&#35805;&#30340;&#26426;&#22120;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#25351;&#20986;&#20102;&#38463;&#25289;&#20271;&#35821;&#36164;&#28304;&#19981;&#36275;&#30340;&#38382;&#39064;&#20197;&#21450;&#29616;&#26377;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#22788;&#29702;&#38463;&#25289;&#20271;&#26041;&#35328;&#26102;&#30340;&#22256;&#38590;&#12290;&#35813;&#30740;&#31350;&#30340;&#37325;&#28857;&#22312;&#20110;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#38463;&#25289;&#20271;&#21508;&#31181;&#26041;&#35328;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#33521;&#35821;&#36164;&#28304;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#29702;&#35299;&#20869;&#23481;&#30456;&#23545;&#20805;&#36275;&#65292;&#20294;&#38463;&#25289;&#20271;&#35821;&#30340;&#36164;&#28304;&#20173;&#19981;&#36275;&#22815;&#25104;&#29087;&#12290;&#38463;&#25289;&#20271;&#35821;&#36164;&#28304;&#19981;&#36275;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#65292;&#38463;&#25289;&#20271;&#35821;&#38500;&#20102;&#26631;&#20934;&#29256;&#26412;&#65288;MSA&#65289;&#22806;&#65292;&#36824;&#26377;&#35768;&#22810;&#26041;&#35328;&#12290;&#38463;&#25289;&#20271;&#20154;&#22312;&#26085;&#24120;&#20132;&#27969;&#20013;&#19981;&#20351;&#29992;MSA&#65292;&#32780;&#26159;&#20351;&#29992;&#26041;&#35328;&#29256;&#26412;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#20063;&#23558;&#36825;&#31181;&#29616;&#35937;&#24341;&#20837;&#21040;&#20182;&#20204;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30340;&#20351;&#29992;&#20013;&#65292;&#36825;&#36827;&#32780;&#24341;&#21457;&#20102;&#23545;&#20110;&#26500;&#24314;&#36866;&#29992;&#20110;&#35821;&#35328;&#30456;&#20851;&#24212;&#29992;&#30340;&#21512;&#36866;AI&#27169;&#22411;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;&#29616;&#26377;&#20026;MSA&#35774;&#35745;&#30340;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#31995;&#32479;&#26080;&#27861;&#24456;&#22909;&#22320;&#22788;&#29702;&#38463;&#25289;&#20271;&#26041;&#35328;&#12290;&#37492;&#20110;&#27492;&#65292;&#26377;&#24517;&#35201;&#36890;&#36807;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#38463;&#25289;&#20271;&#21508;&#31181;&#26041;&#35328;&#30340;MT&#31995;&#32479;&#26469;&#36866;&#24212;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#38750;&#27491;&#24335;&#20132;&#27969;&#26041;&#24335;&#12290;&#19982;&#22312;MT&#31995;&#32479;&#20013;&#23545;MSA&#26174;&#31034;&#20986;&#20808;&#36827;&#36827;&#23637;&#19981;&#21516;&#65292;&#21033;&#29992;&#38463;&#25289;&#20271;&#26041;&#35328;&#36827;&#34892;MT&#31995;&#32479;&#30340;&#21033;&#29992;&#25152;&#20184;&#20986;&#30340;&#21162;&#21147;&#19981;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
While resources for English language are fairly sufficient to understand content on social media, similar resources in Arabic are still immature. The main reason that the resources in Arabic are insufficient is that Arabic has many dialects in addition to the standard version (MSA). Arabs do not use MSA in their daily communications; rather, they use dialectal versions. Unfortunately, social users transfer this phenomenon into their use of social media platforms, which in turn has raised an urgent need for building suitable AI models for language-dependent applications. Existing machine translation (MT) systems designed for MSA fail to work well with Arabic dialects. In light of this, it is necessary to adapt to the informal nature of communication on social networks by developing MT systems that can effectively handle the various dialects of Arabic. Unlike for MSA that shows advanced progress in MT systems, little effort has been exerted to utilize Arabic dialects for MT systems. Whil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26469;&#33258;wikiHow&#24179;&#21488;&#30340;&#22914;&#20309;&#25351;&#21335;&#22312;&#23454;&#36341;&#20013;&#38754;&#21521;&#29305;&#23450;&#21463;&#20247;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#25351;&#21335;&#21463;&#21040;&#24494;&#22937;&#30340;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#26088;&#22312;&#24341;&#36215;&#20154;&#20204;&#23545;&#36825;&#20123;&#19981;&#24179;&#31561;&#38382;&#39064;&#30340;&#20851;&#27880;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2309.12117</link><description>&lt;p&gt;
&#38754;&#21521;&#29305;&#23450;&#21463;&#20247;&#30340;"&#22914;&#20309;&#25351;&#21335;"&#65306;&#19968;&#20010;&#35821;&#26009;&#24211;&#21644;&#21021;&#27493;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
How-to Guides for Specific Audiences: A Corpus and Initial Findings. (arXiv:2309.12117v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26469;&#33258;wikiHow&#24179;&#21488;&#30340;&#22914;&#20309;&#25351;&#21335;&#22312;&#23454;&#36341;&#20013;&#38754;&#21521;&#29305;&#23450;&#21463;&#20247;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#25351;&#21335;&#21463;&#21040;&#24494;&#22937;&#30340;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#26088;&#22312;&#24341;&#36215;&#20154;&#20204;&#23545;&#36825;&#20123;&#19981;&#24179;&#31561;&#38382;&#39064;&#30340;&#20851;&#27880;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#29305;&#23450;&#30446;&#26631;&#32676;&#20307;&#30340;&#25351;&#23548;&#24615;&#25991;&#26412;&#24212;&#35813;&#26681;&#25454;&#35835;&#32773;&#30340;&#20808;&#21069;&#30693;&#35782;&#21644;&#38656;&#27714;&#26469;&#26377;&#25928;&#22320;&#24341;&#23548;&#20182;&#20204;&#23454;&#29616;&#20854;&#26399;&#26395;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#29305;&#23450;&#32676;&#20307;&#20063;&#20250;&#23384;&#22312;&#21453;&#26144;&#19981;&#21516;&#31038;&#20250;&#35268;&#33539;&#21644;&#24494;&#22937;&#21051;&#26495;&#21360;&#35937;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26469;&#33258;wikiHow&#24179;&#21488;&#30340;&#29305;&#23450;&#21463;&#20247;&#30340;&#22914;&#20309;&#25351;&#21335;&#22312;&#23454;&#36341;&#20013;&#30340;&#24046;&#24322;&#31243;&#24230;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#38024;&#23545;&#29305;&#23450;&#21463;&#20247;&#25776;&#20889;&#30340;&#25991;&#26412;&#30340;&#23450;&#24615;&#29305;&#28857;&#12290;&#22312;&#19968;&#20010;&#27010;&#25324;&#24615;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#21738;&#20123;&#24046;&#24322;&#20063;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#26041;&#27861;&#31995;&#32479;&#22320;&#23637;&#31034;&#20986;&#26469;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#26469;&#33258;wikiHow&#30340;&#25351;&#21335;&#65292;&#20687;&#20854;&#20182;&#25991;&#26412;&#27969;&#27966;&#19968;&#26679;&#65292;&#20063;&#21463;&#21040;&#24494;&#22937;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#39640;&#23545;&#36825;&#20123;&#19981;&#24179;&#31561;&#38382;&#39064;&#30340;&#24847;&#35782;&#65292;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instructional texts for specific target groups should ideally take into account the prior knowledge and needs of the readers in order to guide them efficiently to their desired goals. However, targeting specific groups also carries the risk of reflecting disparate social norms and subtle stereotypes. In this paper, we investigate the extent to which how-to guides from one particular platform, wikiHow, differ in practice depending on the intended audience. We conduct two case studies in which we examine qualitative features of texts written for specific audiences. In a generalization study, we investigate which differences can also be systematically demonstrated using computational methods. The results of our studies show that guides from wikiHow, like other text genres, are subject to subtle biases. We aim to raise awareness of these inequalities as a first step to addressing them in future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#27169;&#22411;&#22914;&#34255;&#35821;&#20013;&#36827;&#34892;&#20102;&#39640;&#25928;&#24494;&#35843;&#31574;&#30053;&#30340;&#30740;&#31350;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#37325;&#35201;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2309.12109</link><description>&lt;p&gt;
PEFTT: &#22810;&#21442;&#25968;&#25928;&#29575;&#30340;&#20302;&#36164;&#28304;&#34255;&#35821;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan pre-trained language models. (arXiv:2309.12109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#27169;&#22411;&#22914;&#34255;&#35821;&#20013;&#36827;&#34892;&#20102;&#39640;&#25928;&#24494;&#35843;&#31574;&#30053;&#30340;&#30740;&#31350;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#37325;&#35201;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#20256;&#32479;&#27169;&#22411;&#35757;&#32451;&#23545;&#20110;&#26222;&#36890;&#29992;&#25143;&#21644;&#26426;&#26500;&#32780;&#35328;&#36234;&#26469;&#36234;&#38590;&#20197;&#24819;&#35937;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#19978;&#65292;&#23545;&#20110;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#39640;&#25928;&#24494;&#35843;&#30340;&#25506;&#32034;&#26159;&#19968;&#20010;&#19981;&#21487;&#21542;&#35748;&#30340;&#36235;&#21183;&#65292;&#32780;&#36825;&#31181;&#36235;&#21183;&#27491;&#36880;&#28176;&#27969;&#34892;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#22914;&#34255;&#35821;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#38750;&#24120;&#26377;&#38480;&#12290;&#34255;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#30740;&#31350;&#26412;&#23601;&#31232;&#32570;&#32780;&#21463;&#38480;&#12290;&#23613;&#31649;&#30446;&#21069;&#30001;&#20110;&#20854;&#20302;&#36164;&#28304;&#24615;&#36136;&#65292;&#36824;&#27809;&#26377;&#29616;&#26377;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#34255;&#35821;&#65292;&#20294;&#36825;&#19968;&#22825;&#27627;&#26080;&#30097;&#38382;&#20250;&#21040;&#26469;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#20687;&#34255;&#35821;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#24494;&#35843;&#30740;&#31350;&#38750;&#24120;&#24517;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21487;&#20197;&#20316;&#20026;&#22635;&#34917;&#36825;&#19968;&#37325;&#35201;&#31354;&#30333;&#30340;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this era of large language models (LLMs), the traditional training of models has become increasingly unimaginable for regular users and institutions. The exploration of efficient fine-tuning for high-resource languages on these models is an undeniable trend that is gradually gaining popularity. However, there has been very little exploration for various low-resource languages, such as Tibetan. Research in Tibetan NLP is inherently scarce and limited. While there is currently no existing large language model for Tibetan due to its low-resource nature, that day will undoubtedly arrive. Therefore, research on efficient fine-tuning for low-resource language models like Tibetan is highly necessary. Our research can serve as a reference to fill this crucial gap. Efficient fine-tuning strategies for pre-trained language models (PLMs) in Tibetan have seen minimal exploration. We conducted three types of efficient fine-tuning experiments on the publicly available TNCC-title dataset: "prompt-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20462;&#35746;&#21382;&#21490;&#26469;&#20998;&#26512;&#25945;&#23398;&#25991;&#26412;&#20462;&#35746;&#20013;&#30340;&#27169;&#31946;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#25104;&#23545;&#25490;&#21517;&#20219;&#21153;&#65292;&#31070;&#32463;&#27169;&#22411;&#22312;&#21306;&#20998;&#25351;&#20196;&#30340;&#19981;&#21516;&#29256;&#26412;&#19978;&#34920;&#29616;&#20986;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.12107</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#19968;&#39033;&#20851;&#20110;&#25945;&#23398;&#25991;&#26412;&#20462;&#35746;&#20013;&#27169;&#31946;&#24615;&#30340;&#35745;&#31639;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Computational Analysis of Vagueness in Revisions of Instructional Texts. (arXiv:2309.12107v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20462;&#35746;&#21382;&#21490;&#26469;&#20998;&#26512;&#25945;&#23398;&#25991;&#26412;&#20462;&#35746;&#20013;&#30340;&#27169;&#31946;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#25104;&#23545;&#25490;&#21517;&#20219;&#21153;&#65292;&#31070;&#32463;&#27169;&#22411;&#22312;&#21306;&#20998;&#25351;&#20196;&#30340;&#19981;&#21516;&#29256;&#26412;&#19978;&#34920;&#29616;&#20986;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
WikiHow&#26159;&#19968;&#20010;&#24320;&#25918;&#39046;&#22495;&#30340;&#25945;&#23398;&#25991;&#31456;&#24211;&#65292;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#29992;&#25143;&#21487;&#20197;&#23545;&#20854;&#36827;&#34892;&#20462;&#35746;&#12290;&#26412;&#25991;&#20174;&#20462;&#35746;&#21382;&#21490;&#30340;&#22122;&#22768;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20102;&#19968;&#23545;&#19968;&#30340;&#29256;&#26412;&#65292;&#21363;&#20462;&#35746;&#20043;&#21069;&#21644;&#20462;&#35746;&#20043;&#21518;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#29305;&#21035;&#25552;&#21462;&#24182;&#20998;&#26512;&#28041;&#21450;&#25351;&#20196;&#27169;&#31946;&#24615;&#30340;&#32534;&#36753;&#12290;&#36890;&#36807;&#37319;&#29992;&#20043;&#21069;&#24037;&#20316;&#20013;&#30340;&#19968;&#31181;&#25104;&#23545;&#25490;&#21517;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#23545;&#29616;&#26377;&#22522;&#32447;&#30340;&#25913;&#36827;&#65292;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#31070;&#32463;&#27169;&#22411;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#21306;&#20998;&#20004;&#20010;&#29256;&#26412;&#30340;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
WikiHow is an open-domain repository of instructional articles for a variety of tasks, which can be revised by users. In this paper, we extract pairwise versions of an instruction before and after a revision was made. Starting from a noisy dataset of revision histories, we specifically extract and analyze edits that involve cases of vagueness in instructions. We further investigate the ability of a neural model to distinguish between two versions of an instruction in our data by adopting a pairwise ranking task from previous work and showing improvements over existing baselines.
&lt;/p&gt;</description></item><item><title>SemEval-2022&#20219;&#21153;7&#26088;&#22312;&#35782;&#21035;&#25351;&#23548;&#25991;&#26412;&#20013;&#26263;&#21547;&#25110;&#19981;&#26126;&#30830;&#30701;&#35821;&#30340;&#21512;&#29702;&#35299;&#37322;&#12290;&#36890;&#36807;&#20154;&#31867;&#35780;&#21028;&#21644;&#21442;&#19982;&#31995;&#32479;&#30340;&#33258;&#21160;&#21028;&#26029;&#65292;&#26368;&#22909;&#30340;&#31995;&#32479;&#22312;&#27492;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;68.9%&#30340;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#36824;&#21457;&#29616;&#20102;&#26377;&#22810;&#20010;&#21512;&#29702;&#35299;&#37322;&#30340;&#19978;&#19979;&#25991;&#21487;&#20197;&#20197;75.2%&#30340;&#20934;&#30830;&#29575;&#34987;&#25490;&#21517;&#21069;&#30340;&#21442;&#19982;&#22242;&#38431;&#30340;&#39044;&#27979;&#25152;&#35782;&#21035;&#20986;&#26469;&#12290;</title><link>http://arxiv.org/abs/2309.12102</link><description>&lt;p&gt;
SemEval-2022&#20219;&#21153;7&#65306;&#35782;&#21035;&#25351;&#23548;&#25991;&#26412;&#20013;&#26263;&#21547;&#25110;&#19981;&#26126;&#30830;&#30701;&#35821;&#30340;&#21512;&#29702;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
SemEval-2022 Task 7: Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts. (arXiv:2309.12102v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12102
&lt;/p&gt;
&lt;p&gt;
SemEval-2022&#20219;&#21153;7&#26088;&#22312;&#35782;&#21035;&#25351;&#23548;&#25991;&#26412;&#20013;&#26263;&#21547;&#25110;&#19981;&#26126;&#30830;&#30701;&#35821;&#30340;&#21512;&#29702;&#35299;&#37322;&#12290;&#36890;&#36807;&#20154;&#31867;&#35780;&#21028;&#21644;&#21442;&#19982;&#31995;&#32479;&#30340;&#33258;&#21160;&#21028;&#26029;&#65292;&#26368;&#22909;&#30340;&#31995;&#32479;&#22312;&#27492;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;68.9%&#30340;&#20934;&#30830;&#29575;&#12290;&#21516;&#26102;&#65292;&#36824;&#21457;&#29616;&#20102;&#26377;&#22810;&#20010;&#21512;&#29702;&#35299;&#37322;&#30340;&#19978;&#19979;&#25991;&#21487;&#20197;&#20197;75.2%&#30340;&#20934;&#30830;&#29575;&#34987;&#25490;&#21517;&#21069;&#30340;&#21442;&#19982;&#22242;&#38431;&#30340;&#39044;&#27979;&#25152;&#35782;&#21035;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SemEval-2022&#20219;&#21153;7&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#35780;&#20272;&#25351;&#23548;&#25991;&#26412;&#20013;&#35299;&#37322;&#30340;&#21512;&#29702;&#24615;&#30340;&#20849;&#20139;&#20219;&#21153;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#25163;&#21160;&#28548;&#28165;&#30340;&#25805;&#20316;&#25351;&#21335;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#26367;&#20195;&#30340;&#35299;&#37322;&#24182;&#25910;&#38598;&#20102;&#20154;&#31867;&#30340;&#21512;&#29702;&#24615;&#21028;&#26029;&#12290;&#21442;&#19982;&#31995;&#32479;&#30340;&#20219;&#21153;&#26159;&#22312;&#30456;&#24212;&#30340;&#19978;&#19979;&#25991;&#20013;&#33258;&#21160;&#30830;&#23450;&#35299;&#37322;&#30340;&#21512;&#29702;&#24615;&#12290;&#24635;&#20849;&#26377;21&#20010;&#21442;&#19982;&#32773;&#21442;&#19982;&#20102;&#36825;&#20010;&#20219;&#21153;&#65292;&#26368;&#22909;&#30340;&#31995;&#32479;&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;68.9%&#12290;&#26412;&#25253;&#21578;&#24635;&#32467;&#20102;&#26469;&#33258;8&#20010;&#22242;&#38431;&#30340;&#32467;&#26524;&#21644;&#21457;&#29616;&#65292;&#20197;&#21450;&#20182;&#20204;&#30340;&#31995;&#32479;&#25551;&#36848;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#39069;&#22806;&#30340;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#25490;&#21517;&#21069;&#30340;&#21442;&#19982;&#22242;&#38431;&#30340;&#39044;&#27979;&#33021;&#20197;75.2%&#30340;&#20934;&#30830;&#29575;&#35782;&#21035;&#20986;&#26377;&#22810;&#20010;&#21512;&#29702;&#35299;&#37322;&#30340;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe SemEval-2022 Task 7, a shared task on rating the plausibility of clarifications in instructional texts. The dataset for this task consists of manually clarified how-to guides for which we generated alternative clarifications and collected human plausibility judgements. The task of participating systems was to automatically determine the plausibility of a clarification in the respective context. In total, 21 participants took part in this task, with the best system achieving an accuracy of 68.9%. This report summarizes the results and findings from 8 teams and their system descriptions. Finally, we show in an additional evaluation that predictions by the top participating team make it possible to identify contexts with multiple plausible clarifications with an accuracy of 75.2%.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;Benchmark&#27979;&#35797;&#65292;&#21457;&#29616;&#20351;&#29992;Prompt Tuning&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Trie&#25628;&#32034;&#26469;&#35299;&#20915;&#29983;&#25104;&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.12075</link><description>&lt;p&gt;
&#20351;&#29992;Prompt&#35843;&#20248;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21152;&#36895;&#20027;&#39064;&#25237;&#36164;
&lt;/p&gt;
&lt;p&gt;
Accelerating Thematic Investment with Prompt Tuned Pretrained Language Models. (arXiv:2309.12075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;Benchmark&#27979;&#35797;&#65292;&#21457;&#29616;&#20351;&#29992;Prompt Tuning&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Trie&#25628;&#32034;&#26469;&#35299;&#20915;&#29983;&#25104;&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt Tuning&#20316;&#20026;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#26041;&#27861;&#65292;&#27491;&#22312;&#25104;&#20026;&#32454;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#23545;Prompt Tuning&#21644;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23558;&#20854;&#24212;&#29992;&#20110;&#23558;&#20844;&#21496;&#20998;&#31867;&#20026;&#25237;&#36164;&#20844;&#21496;&#19987;&#26377;&#30340;&#34892;&#19994;&#20998;&#31867;&#27861;&#65292;&#20197;&#25903;&#25345;&#20854;&#20027;&#39064;&#25237;&#36164;&#31574;&#30053;&#12290;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;PLMs&#36827;&#34892;&#25991;&#26412;&#21040;&#25991;&#26412;&#20998;&#31867;&#32463;&#24120;&#34987;&#25253;&#21578;&#20026;&#20248;&#20110;&#20351;&#29992;&#20998;&#31867;&#22836;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#22312;&#27599;&#20010;&#26631;&#31614;&#30001;&#22810;&#20010;&#20196;&#29260;&#32452;&#25104;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#65288;a&#65289;&#29983;&#25104;&#30340;&#26631;&#31614;&#21487;&#33021;&#19981;&#21305;&#37197;&#34892;&#19994;&#20998;&#31867;&#27861;&#20013;&#30340;&#20219;&#20309;&#26631;&#31614;&#65307;&#65288;b&#65289;&#22312;&#32454;&#35843;&#38454;&#27573;&#65292;&#24517;&#39035;&#20197;&#20219;&#24847;&#39034;&#24207;&#25552;&#20379;&#22810;&#20010;&#26631;&#31614;&#65307;&#65288;c&#65289;&#27169;&#22411;&#20026;&#27599;&#20010;&#26631;&#31614;&#25552;&#20379;&#20108;&#36827;&#21046;&#20915;&#31574;&#65292;&#32780;&#19981;&#26159;&#36866;&#24403;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;&#36890;&#36807;&#24212;&#29992;Trie&#25628;&#32034;&#26469;&#35299;&#20915;&#38480;&#21046;&#65288;a&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt Tuning is emerging as a scalable and cost-effective method to fine-tune Pretrained Language Models (PLMs). This study benchmarks the performance and computational efficiency of Prompt Tuning and baseline methods on a multi-label text classification task. This is applied to the use case of classifying companies into an investment firm's proprietary industry taxonomy, supporting their thematic investment strategy. Text-to-text classification with PLMs is frequently reported to outperform classification with a classification head, but has several limitations when applied to a multi-label classification problem where each label consists of multiple tokens: (a) Generated labels may not match any label in the industry taxonomy; (b) During fine-tuning, multiple labels must be provided in an arbitrary order; (c) The model provides a binary decision for each label, rather than an appropriate confidence score. Limitation (a) is addressed by applying constrained decoding using Trie Search,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#22522;&#20110;&#37327;&#21270;LLaMa&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24052;&#35199;&#20013;&#23398;&#32771;&#35797;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#20339;&#34920;&#29616;&#30340;&#27169;&#22411;&#22312;&#21407;&#29256;&#33889;&#33796;&#29273;&#35821;&#38382;&#39064;&#21644;&#20854;&#33521;&#25991;&#32763;&#35793;&#19978;&#30340;&#20934;&#30830;&#29575;&#32422;&#20026;46%&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.12071</link><description>&lt;p&gt;
&#22312;&#24052;&#35199;&#20013;&#23398;&#32771;&#35797;&#19978;&#23545;&#22522;&#20110;&#37327;&#21270;LLaMa&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking quantized LLaMa-based models on the Brazilian Secondary School Exam. (arXiv:2309.12071v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#22522;&#20110;&#37327;&#21270;LLaMa&#27169;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24052;&#35199;&#20013;&#23398;&#32771;&#35797;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#20339;&#34920;&#29616;&#30340;&#27169;&#22411;&#22312;&#21407;&#29256;&#33889;&#33796;&#29273;&#35821;&#38382;&#39064;&#21644;&#20854;&#33521;&#25991;&#32763;&#35793;&#19978;&#30340;&#20934;&#30830;&#29575;&#32422;&#20026;46%&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#25105;&#20204;&#19982;&#35745;&#31639;&#26426;&#20132;&#20114;&#30340;&#26041;&#24335;&#19978;&#20195;&#34920;&#20102;&#19968;&#22330;&#38761;&#21629;&#65292;&#20801;&#35768;&#26500;&#24314;&#22797;&#26434;&#38382;&#39064;&#24182;&#33021;&#22815;&#23545;&#19968;&#31995;&#21015;&#38472;&#36848;&#36827;&#34892;&#25512;&#29702;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#19987;&#38376;&#30340;&#30828;&#20214;&#25191;&#34892;&#65292;&#23427;&#20204;&#30340;&#20351;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;70&#20159;&#21644;130&#20159;LLaMA&#27169;&#22411;&#30340;LLMs&#22312;&#37327;&#21270;&#22788;&#29702;&#21644;&#36816;&#34892;&#22312;&#23478;&#24237;&#30828;&#20214;&#19978;&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#21040;&#30340;&#27169;&#22411;&#26377;Alpaca&#12289;Koala&#21644;Vicuna&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;1006&#20010;&#38382;&#39064;&#30340;&#25968;&#25454;&#24211;&#65292;&#36825;&#20123;&#38382;&#39064;&#26469;&#33258;&#24052;&#35199;&#22269;&#23478;&#20013;&#23398;&#32771;&#35797;(ENEM)&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#22312;&#21407;&#29256;&#33889;&#33796;&#29273;&#35821;&#38382;&#39064;&#21644;&#20854;&#33521;&#25991;&#32763;&#35793;&#19978;&#30340;&#20934;&#30830;&#29575;&#32422;&#20026;46%&#65292;&#21478;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#25191;&#34892;&#25152;&#38656;&#30340;&#26102;&#38388;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#22312;&#24179;&#22343;&#24773;&#20917;&#19979;&#65292;70&#20159;&#21644;130&#20159;LLMs&#38656;&#35201;&#32422;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) represent a revolution in the way we interact with computers, allowing the construction of complex questions and the ability to reason over a sequence of statements, their use is restricted due to the need for dedicated hardware for execution. In this study, we evaluate the performance of LLMs based on the 7 and 13 billion LLaMA models, subjected to a quantization process and run on home hardware. The models considered were Alpaca, Koala, and Vicuna. To evaluate the effectiveness of these models, we developed a database containing 1,006 questions from the ENEM (Brazilian National Secondary School Exam). Our analysis revealed that the best performing models achieved an accuracy of approximately 46% for the original texts of the Portuguese questions and 49% on their English translations. In addition, we evaluated the computational efficiency of the models by measuring the time required for execution. On average, the 7 and 13 billion LLMs took approxi
&lt;/p&gt;</description></item><item><title>BELT&#26159;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#36827;&#34892;&#33041;&#30005;&#22270;&#21040;&#35821;&#35328;&#35299;&#30721;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#30340;&#24341;&#23548;&#24335;&#27169;&#22411;&#21644;&#23398;&#20064;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#29616;&#25104;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#36827;&#33041;&#30005;&#20449;&#21495;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#22823;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#24212;&#29992;&#21644;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.12056</link><description>&lt;p&gt;
BELT: &#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#36827;&#34892;&#33041;&#30005;&#22270;&#21040;&#35821;&#35328;&#35299;&#30721;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#30340;&#24341;&#23548;&#24335;&#27169;&#22411;&#21644;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BELT:Bootstrapping Electroencephalography-to-Language Decoding and Zero-Shot Sentiment Classification by Natural Language Supervision. (arXiv:2309.12056v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12056
&lt;/p&gt;
&lt;p&gt;
BELT&#26159;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#30417;&#30563;&#36827;&#34892;&#33041;&#30005;&#22270;&#21040;&#35821;&#35328;&#35299;&#30721;&#21644;&#38646;&#26679;&#26412;&#24773;&#24863;&#20998;&#31867;&#30340;&#24341;&#23548;&#24335;&#27169;&#22411;&#21644;&#23398;&#20064;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#29616;&#25104;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#25913;&#36827;&#33041;&#30005;&#20449;&#21495;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#25512;&#21160;&#20102;&#22823;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#24212;&#29992;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BELT&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#22823;&#33041;&#21040;&#35821;&#35328;&#32763;&#35793;&#30740;&#31350;&#30340;&#37325;&#35201;&#20027;&#39064;&#30340;&#26032;&#27169;&#22411;&#21644;&#23398;&#20064;&#26694;&#26550;&#12290;&#23558;&#38750;&#20405;&#20837;&#24615;&#33041;&#20449;&#21495;&#32763;&#35793;&#25104;&#21487;&#35835;&#30340;&#33258;&#28982;&#35821;&#35328;&#26377;&#28508;&#21147;&#25512;&#21160;&#22823;&#33041;-&#35745;&#31639;&#26426;&#30028;&#38754;&#65288;BCI&#65289;&#30340;&#24212;&#29992;&#22330;&#26223;&#21644;&#21457;&#23637;&#12290;&#33041;&#20449;&#21495;&#35299;&#30721;&#25110;&#33041;-&#35821;&#35328;&#32763;&#35793;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#26159;&#20174;&#26377;&#38480;&#35268;&#27169;&#21644;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#35821;&#20041;&#36866;&#24403;&#19988;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#33041;&#30005;&#22270;&#34920;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;BELT&#26041;&#27861;&#26159;&#19968;&#20010;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29616;&#25104;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24341;&#23548;&#33041;&#30005;&#22270;&#34920;&#31034;&#23398;&#20064;&#12290;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#35268;&#27169;LM&#29702;&#35299;&#35821;&#20041;&#20449;&#24687;&#21644;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#65292;BELT&#26497;&#22823;&#25913;&#36827;&#20102;&#23545;&#33041;&#30005;&#20449;&#21495;&#30340;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;BELT&#27169;&#22411;&#30001;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32452;&#25104;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#35299;&#30721;&#31070;&#32463;&#32593;&#32476;&#21644;&#19968;&#20010;&#39044;&#35757;&#32451;LM&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents BELT, a novel model and learning framework for the pivotal topic of brain-to-language translation research. The translation from noninvasive brain signals into readable natural language has the potential to promote the application scenario as well as the development of brain-computer interfaces (BCI) as a whole. The critical problem in brain signal decoding or brain-to-language translation is the acquisition of semantically appropriate and discriminative EEG representation from a dataset of limited scale and quality. The proposed BELT method is a generic and efficient framework that bootstraps EEG representation learning using off-the-shelf large-scale pretrained language models (LMs). With a large LM's capacity for understanding semantic information and zero-shot generalization, BELT utilizes large LMs trained on Internet-scale datasets to bring significant improvements to the understanding of EEG signals.  In particular, the BELT model is composed of a deep confor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#38463;&#25289;&#20271;&#25991;&#30340;&#26412;&#22320;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(AceGPT)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#26469;&#22521;&#20859;&#20855;&#22791;&#25991;&#21270;&#24847;&#35782;&#21644;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#65292;&#20197;&#28385;&#36275;&#38463;&#25289;&#20271;&#35821;&#31038;&#21306;&#29305;&#23450;&#24212;&#29992;&#38656;&#27714;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;AceGPT&#22312;&#21508;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#26159;&#26368;&#20808;&#36827;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.12053</link><description>&lt;p&gt;
AceGPT&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26412;&#22320;&#21270;&#20026;&#38463;&#25289;&#20271;&#25991;
&lt;/p&gt;
&lt;p&gt;
AceGPT, Localizing Large Language Models in Arabic. (arXiv:2309.12053v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#38463;&#25289;&#20271;&#25991;&#30340;&#26412;&#22320;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(AceGPT)&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#26469;&#22521;&#20859;&#20855;&#22791;&#25991;&#21270;&#24847;&#35782;&#21644;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#65292;&#20197;&#28385;&#36275;&#38463;&#25289;&#20271;&#35821;&#31038;&#21306;&#29305;&#23450;&#24212;&#29992;&#38656;&#27714;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;AceGPT&#22312;&#21508;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#37117;&#26159;&#26368;&#20808;&#36827;&#30340;&#38463;&#25289;&#20271;&#25991;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#21457;&#36866;&#29992;&#20110;&#38463;&#25289;&#20271;&#25991;&#30340;&#26412;&#22320;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#36843;&#20999;&#38656;&#27714;&#21644;&#26041;&#27861;&#35770;&#65292;&#38463;&#25289;&#20271;&#25991;&#20855;&#26377;&#29420;&#29305;&#30340;&#25991;&#21270;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#30446;&#21069;&#30340;&#20027;&#27969;&#27169;&#22411;&#22914;ChatGPT&#24182;&#26410;&#20805;&#20998;&#35299;&#20915;&#12290;&#22312;&#32771;&#34385;&#25991;&#21270;&#25935;&#24863;&#24615;&#21644;&#26412;&#22320;&#20215;&#20540;&#35266;&#26102;&#36824;&#23384;&#22312;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25171;&#21253;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#36827;&#19968;&#27493;&#20351;&#29992;&#38463;&#25289;&#20271;&#25991;&#26412;&#36827;&#34892;&#39044;&#35757;&#32451;&#12289;&#20351;&#29992;&#26412;&#22320;&#38463;&#25289;&#20271;&#25351;&#20196;&#21644;&#38463;&#25289;&#20271;&#35821;GPT-4&#22238;&#24212;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;(SFT)&#65292;&#20197;&#21450;&#20351;&#29992;&#23545;&#26412;&#22320;&#25991;&#21270;&#21644;&#20215;&#20540;&#35266;&#25935;&#24863;&#30340;&#22870;&#21169;&#27169;&#22411;&#36827;&#34892;&#22686;&#24378;&#23398;&#20064;&#19982;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;(RLAIF)&#12290;&#30446;&#26631;&#26159;&#35757;&#32451;&#20855;&#22791;&#25991;&#21270;&#24847;&#35782;&#21644;&#19982;&#20215;&#20540;&#35266;&#19968;&#33268;&#30340;&#38463;&#25289;&#20271;&#25991;LLM&#65292;&#20197;&#28385;&#36275;&#38463;&#25289;&#20271;&#35821;&#31038;&#21306;&#22810;&#26679;&#21270;&#30340;&#29305;&#23450;&#24212;&#29992;&#38656;&#27714;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#24471;&#21040;&#30340;&#21517;&#20026;AceGPT&#30340;&#38463;&#25289;&#20271;&#25991;LLM&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#26159;&#26368;&#20808;&#36827;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the imperative need and methodology for developing a localized Large Language Model (LLM) tailored for Arabic, a language with unique cultural characteristics that are not adequately addressed by current mainstream models like ChatGPT. Key concerns additionally arise when considering cultural sensitivity and local values. To this end, the paper outlines a packaged solution, including further pre-training with Arabic texts, supervised fine-tuning (SFT) using native Arabic instructions and GPT-4 responses in Arabic, and reinforcement learning with AI feedback (RLAIF) using a reward model that is sensitive to local culture and values. The objective is to train culturally aware and value-aligned Arabic LLMs that can serve the diverse application-specific needs of Arabic-speaking communities.  Extensive evaluations demonstrated that the resulting LLM called `\textbf{AceGPT}' is the SOTA open Arabic LLM in various benchmarks, including instruction-following benchmark (i.e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#37325;&#26032;&#35774;&#35745;&#30340;&#20219;&#21153;&#21644;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;CA Multimodal Evaluation for Ad Text GeneRAtion (CAMERA)&#25968;&#25454;&#38598;&#65292;&#25512;&#21160;&#20102;&#24191;&#21578;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#30340;&#21457;&#23637;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20934;&#20013;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2309.12030</link><description>&lt;p&gt;
CAMERA&#65306;&#24191;&#21578;&#25991;&#26412;&#29983;&#25104;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CAMERA: A Multimodal Dataset and Benchmark for Ad Text Generation. (arXiv:2309.12030v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#37325;&#26032;&#35774;&#35745;&#30340;&#20219;&#21153;&#21644;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#36890;&#36807;&#24341;&#20837;CA Multimodal Evaluation for Ad Text GeneRAtion (CAMERA)&#25968;&#25454;&#38598;&#65292;&#25512;&#21160;&#20102;&#24191;&#21578;&#25991;&#26412;&#29983;&#25104;&#39046;&#22495;&#30340;&#21457;&#23637;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20934;&#20013;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21709;&#24212;&#25163;&#21160;&#22312;&#32447;&#24191;&#21578;&#21046;&#20316;&#30340;&#38480;&#21046;&#26102;&#65292;&#24050;&#32463;&#22312;&#33258;&#21160;&#24191;&#21578;&#25991;&#26412;&#29983;&#25104;&#65288;ATG&#65289;&#39046;&#22495;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#28085;&#30422;&#25972;&#20010;&#39046;&#22495;&#30340;&#22522;&#20934;&#20197;&#21450;&#27809;&#26377;&#26126;&#30830;&#23450;&#20041;&#30340;&#38382;&#39064;&#38598;&#21644;&#28165;&#26224;&#30340;&#27169;&#22411;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#19968;&#30452;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#37325;&#26032;&#35774;&#35745;&#30340;&#20219;&#21153;&#21644;&#26500;&#24314;&#19968;&#20010;&#22522;&#20934;&#26469;&#25512;&#21160;ATG&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;ATG&#23450;&#20041;&#20026;&#19968;&#20010;&#36328;&#24212;&#29992;&#20219;&#21153;&#65292;&#28085;&#30422;&#20114;&#32852;&#32593;&#24191;&#21578;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#36129;&#29486;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31532;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21517;&#20026;CA Multimodal Evaluation for Ad Text GeneRAtion&#65288;CAMERA&#65289;&#65292;&#20026;ATG&#31934;&#24515;&#35774;&#35745;&#65292;&#21487;&#20197;&#21033;&#29992;&#22810;&#27169;&#24577;&#20449;&#24687;&#24182;&#36827;&#34892;&#34892;&#19994;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20934;&#30340;&#26377;&#29992;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26415;&#35821;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to the limitations of manual online ad production, significant research has been conducted in the field of automatic ad text generation (ATG). However, comparing different methods has been challenging because of the lack of benchmarks encompassing the entire field and the absence of well-defined problem sets with clear model inputs and outputs. To address these challenges, this paper aims to advance the field of ATG by introducing a redesigned task and constructing a benchmark. Specifically, we defined ATG as a cross-application task encompassing various aspects of the Internet advertising. As part of our contribution, we propose a first benchmark dataset, CA Multimodal Evaluation for Ad Text GeneRAtion (CAMERA), carefully designed for ATG to be able to leverage multi-modal information and conduct an industry-wise evaluation. Furthermore, we demonstrate the usefulness of our proposed benchmark through evaluation experiments using multiple baseline models, which vary in term
&lt;/p&gt;</description></item><item><title>LMSYS-Chat-1M&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20854;&#22810;&#26679;&#24615;&#21644;&#29992;&#20363;&#23637;&#31034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.11998</link><description>&lt;p&gt;
LMSYS-Chat-1M&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#23454;&#38469;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset. (arXiv:2309.11998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11998
&lt;/p&gt;
&lt;p&gt;
LMSYS-Chat-1M&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20854;&#22810;&#26679;&#24615;&#21644;&#29992;&#20363;&#23637;&#31034;&#20102;&#20854;&#22312;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#20351;&#29992;&#65292;&#30740;&#31350;&#20154;&#20204;&#22914;&#20309;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19982;&#20854;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LMSYS-Chat-1M&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#19968;&#30334;&#19975;&#20010;&#19982;25&#20010;&#26368;&#20808;&#36827;&#30340;LLM&#36827;&#34892;&#30340;&#23454;&#38469;&#23545;&#35805;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;&#25105;&#20204;&#30340;Vicuna&#28436;&#31034;&#21644;Chatbot Arena&#32593;&#31449;&#19978;&#30340;21&#19975;&#20010;&#29420;&#31435;IP&#22320;&#22336;&#20013;&#25910;&#38598;&#32780;&#26469;&#30340;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#25454;&#38598;&#20869;&#23481;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#20854;&#31574;&#21010;&#36807;&#31243;&#12289;&#22522;&#26412;&#32479;&#35745;&#25968;&#25454;&#21644;&#20027;&#39064;&#20998;&#24067;&#65292;&#24378;&#35843;&#20854;&#22810;&#26679;&#24615;&#12289;&#29420;&#29305;&#24615;&#21644;&#35268;&#27169;&#12290;&#25105;&#20204;&#36890;&#36807;&#22235;&#20010;&#29992;&#20363;&#23637;&#31034;&#20102;&#23427;&#30340;&#22810;&#26679;&#24615;&#65306;&#24320;&#21457;&#19982;GPT-4&#34920;&#29616;&#30456;&#20284;&#30340;&#20869;&#23481;&#36807;&#28388;&#27169;&#22411;&#12289;&#26500;&#24314;&#19968;&#20010;&#23433;&#20840;&#22522;&#20934;&#12289;&#35757;&#32451;&#19982;Vicuna&#34920;&#29616;&#30456;&#20284;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12289;&#21019;&#24314;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#38382;&#39064;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25105;&#20204;&#29702;&#35299;&#21644;&#25512;&#36827;LLM&#33021;&#21147;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is pub
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26680;&#24515;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26041;&#27861;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#24182;&#20511;&#37492;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.11981</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#35780;&#20272;&#26694;&#26550;&#65306;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26410;&#26469;&#24230;&#37327;&#30340;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics. (arXiv:2309.11981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11981
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26680;&#24515;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26041;&#27861;&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#24182;&#20511;&#37492;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#36827;&#23637;&#65292;&#36825;&#20026;&#37325;&#26032;&#23457;&#35270;&#20256;&#32479;&#30340;&#26426;&#22120;&#26234;&#33021;&#24230;&#37327;&#26041;&#27861;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20174;&#20256;&#32479;&#30340;&#22270;&#28789;&#27979;&#35797;&#36716;&#21521;&#20197;&#35821;&#35328;&#20064;&#24471;&#20026;&#26680;&#24515;&#30340;&#20840;&#38754;&#26694;&#26550;&#65292;&#24182;&#20511;&#37492;&#20102;&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#28145;&#21463;&#22810;&#20010;&#23398;&#31185;&#30340;&#21331;&#36234;&#24037;&#20316;&#30340;&#24433;&#21709;&#65292;&#25351;&#20986;&#20102;&#20445;&#25345;&#36328;&#23398;&#31185;&#26725;&#26753;&#24320;&#25918;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#21246;&#21202;&#20102;&#19968;&#20010;&#26356;&#21152;&#31283;&#20581;&#21644;&#21487;&#25345;&#32493;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the burgeoning field of artificial intelligence (AI), the unprecedented progress of large language models (LLMs) in natural language processing (NLP) offers an opportunity to revisit the entire approach of traditional metrics of machine intelligence, both in form and content. As the realm of machine cognitive evaluation has already reached Imitation, the next step is an efficient Language Acquisition and Understanding. Our paper proposes a paradigm shift from the established Turing Test towards an all-embracing framework that hinges on language acquisition, taking inspiration from the recent advancements in LLMs. The present contribution is deeply tributary of the excellent work from various disciplines, point out the need to keep interdisciplinary bridges open, and delineates a more robust and sustainable approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;BERT&#24182;&#23545;&#20854;&#36827;&#34892;fine-tune&#65292;&#23454;&#29616;&#20102;&#32929;&#24066;&#24773;&#32490;&#30340;&#20998;&#31867;&#21644;&#22522;&#20110;&#35813;&#27169;&#22411;&#30340;&#22238;&#27979;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;fine-tuned&#27169;&#22411;&#30456;&#27604;&#21407;&#22987;&#27169;&#22411;&#21644;&#22522;&#20934;&#27169;&#22411;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11979</link><description>&lt;p&gt;
&#32929;&#24066;&#24773;&#32490;&#20998;&#31867;&#19982;&#22522;&#20110;Fine-tuned BERT&#30340;&#22238;&#27979;
&lt;/p&gt;
&lt;p&gt;
Stock Market Sentiment Classification and Backtesting via Fine-tuned BERT. (arXiv:2309.11979v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;BERT&#24182;&#23545;&#20854;&#36827;&#34892;fine-tune&#65292;&#23454;&#29616;&#20102;&#32929;&#24066;&#24773;&#32490;&#30340;&#20998;&#31867;&#21644;&#22522;&#20110;&#35813;&#27169;&#22411;&#30340;&#22238;&#27979;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;fine-tuned&#27169;&#22411;&#30456;&#27604;&#21407;&#22987;&#27169;&#22411;&#21644;&#22522;&#20934;&#27169;&#22411;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#25968;&#25454;&#21644;&#35745;&#31639;&#35774;&#22791;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#22522;&#20110;&#23454;&#26102;&#20449;&#24687;&#33719;&#21462;&#30340;&#20302;&#24310;&#36831;&#33258;&#21160;&#20132;&#26131;&#24179;&#21488;&#25104;&#20026;&#32929;&#31080;&#20132;&#26131;&#24066;&#22330;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22240;&#27492;&#37327;&#21270;&#20132;&#26131;&#30340;&#20027;&#39064;&#24471;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23545;&#20110;&#38750;&#24378;&#26377;&#25928;&#30340;&#20132;&#26131;&#24066;&#22330;&#26469;&#35828;&#65292;&#20154;&#31867;&#24773;&#32490;&#21644;&#26399;&#26395;&#24635;&#26159;&#20027;&#23548;&#24066;&#22330;&#36235;&#21183;&#21644;&#20132;&#26131;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20174;&#24773;&#32490;&#29702;&#35770;&#20986;&#21457;&#65292;&#20197;&#19996;&#26041;&#36130;&#23500;&#20026;&#20363;&#65292;&#20174;&#20854;&#23545;&#24212;&#30340;&#32929;&#21543;&#29228;&#21462;&#29992;&#25143;&#35780;&#35770;&#26631;&#39064;&#25968;&#25454;&#24182;&#36827;&#34892;&#25968;&#25454;&#28165;&#27927;&#12290;&#38543;&#21518;&#65292;&#26500;&#24314;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;BERT&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#30340;&#24102;&#26377;&#26631;&#27880;&#25968;&#25454;&#38598;&#23545;BERT&#27169;&#22411;&#36827;&#34892;fine-tune&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#21407;&#22987;&#27169;&#22411;&#21644;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;fine-tuned&#27169;&#22411;&#26377;&#19981;&#21516;&#31243;&#24230;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#38543;&#21518;&#65292;&#22312;&#20197;&#19978;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#23545;&#29228;&#21462;&#30340;&#29992;&#25143;&#35780;&#35770;&#25968;&#25454;&#36827;&#34892;&#20102;&#24773;&#32490;&#26497;&#24615;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of big data and computing devices, low-latency automatic trading platforms based on real-time information acquisition have become the main components of the stock trading market, so the topic of quantitative trading has received widespread attention. And for non-strongly efficient trading markets, human emotions and expectations always dominate market trends and trading decisions. Therefore, this paper starts from the theory of emotion, taking East Money as an example, crawling user comment titles data from its corresponding stock bar and performing data cleaning. Subsequently, a natural language processing model BERT was constructed, and the BERT model was fine-tuned using existing annotated data sets. The experimental results show that the fine-tuned model has different degrees of performance improvement compared to the original model and the baseline model. Subsequently, based on the above model, the user comment data crawled is labeled with emotional pola
&lt;/p&gt;</description></item><item><title>Unbabel&#21644;Instituto Superior T&#233;cnico&#25552;&#20132;&#20102;Unbabel-IST 2023&#24180;&#21442;&#21152;WMT 2023&#24180;&#36136;&#37327;&#35780;&#20272;&#20849;&#20139;&#20219;&#21153;&#30340;&#25104;&#26524;&#65292;&#20182;&#20204;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#22312;&#36136;&#37327;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#21333;&#35789;&#12289;&#36328;&#24230;&#21644;&#21477;&#23376;&#32423;&#21035;&#30340;&#26368;&#26032;&#26368;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.11925</link><description>&lt;p&gt;
&#25193;&#23637;COMETKIWI&#65306;Unbabel-IST 2023&#24180;&#36136;&#37327;&#35780;&#20272;&#20849;&#20139;&#20219;&#21153;&#30340;&#25552;&#20132;
&lt;/p&gt;
&lt;p&gt;
Scaling up COMETKIWI: Unbabel-IST 2023 Submission for the Quality Estimation Shared Task. (arXiv:2309.11925v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11925
&lt;/p&gt;
&lt;p&gt;
Unbabel&#21644;Instituto Superior T&#233;cnico&#25552;&#20132;&#20102;Unbabel-IST 2023&#24180;&#21442;&#21152;WMT 2023&#24180;&#36136;&#37327;&#35780;&#20272;&#20849;&#20139;&#20219;&#21153;&#30340;&#25104;&#26524;&#65292;&#20182;&#20204;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#22312;&#36136;&#37327;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#21333;&#35789;&#12289;&#36328;&#24230;&#21644;&#21477;&#23376;&#32423;&#21035;&#30340;&#26368;&#26032;&#26368;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Unbabel&#21644;Instituto Superior T&#233;cnico&#20849;&#21516;&#20026;WMT 2023&#24180;&#36136;&#37327;&#35780;&#20272;&#65288;QE&#65289;&#20849;&#20139;&#20219;&#21153;&#20570;&#20986;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#21442;&#19982;&#20102;&#25152;&#26377;&#20219;&#21153;&#65306;&#21477;&#23376;&#21644;&#21333;&#35789;&#27700;&#24179;&#30340;&#36136;&#37327;&#39044;&#27979;&#20219;&#21153;&#65288;&#20219;&#21153;1&#65289;&#20197;&#21450;&#32454;&#31890;&#24230;&#38169;&#35823;&#36328;&#24230;&#26816;&#27979;&#20219;&#21153;&#65288;&#20219;&#21153;2&#65289;&#12290;&#23545;&#20110;&#25152;&#26377;&#20219;&#21153;&#65292;&#25105;&#20204;&#22522;&#20110;COMETKIWI-22&#27169;&#22411;&#65288;Rei&#31561;&#65292;2022b&#65289;&#36827;&#34892;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#22810;&#35821;&#31181;&#26041;&#27861;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#36798;&#21040;&#21333;&#35789;&#12289;&#36328;&#24230;&#21644;&#21477;&#23376;&#32423;&#31890;&#24230;&#30340;&#36136;&#37327;&#35780;&#20272;&#30340;&#26368;&#26032;&#34920;&#29616;&#27700;&#24179;&#12290;&#19982;&#20043;&#21069;&#30340;&#26368;&#26032;COMETKIWI-22&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#19978;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#65288;&#26368;&#39640;&#21487;&#36798;10&#20010;Spearman&#28857;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36229;&#36807;&#20102;&#31532;&#20108;&#21517;&#30340;&#22810;&#35821;&#31181;&#25552;&#20132;&#65292;&#22312;&#32477;&#23545;&#20998;&#25968;&#19978;&#25552;&#39640;&#20102;3.8&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the joint contribution of Unbabel and Instituto Superior T\'ecnico to the WMT 2023 Shared Task on Quality Estimation (QE). Our team participated on all tasks: sentence- and word-level quality prediction (task 1) and fine-grained error span detection (task 2). For all tasks, we build on the COMETKIWI-22 model (Rei et al., 2022b). Our multilingual approaches are ranked first for all tasks, reaching state-of-the-art performance for quality estimation at word-, span- and sentence-level granularity. Compared to the previous state-of-the-art COMETKIWI-22, we show large improvements in correlation with human judgements (up to 10 Spearman points). Moreover, we surpass the second-best multilingual submission to the shared-task with up to 3.8 absolute points.
&lt;/p&gt;</description></item><item><title>InstructERC&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29983;&#25104;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#27169;&#26495;&#27169;&#22359;&#21644;&#39069;&#22806;&#30340;&#24773;&#24863;&#23545;&#40784;&#20219;&#21153;&#65292;&#25913;&#38761;&#20102;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2309.11911</link><description>&lt;p&gt;
InstructERC&#65306;&#20511;&#21161;&#26816;&#32034;&#22810;&#20219;&#21153;LLMs&#26694;&#26550;&#25913;&#38761;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework. (arXiv:2309.11911v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11911
&lt;/p&gt;
&lt;p&gt;
InstructERC&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29983;&#25104;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#27169;&#26495;&#27169;&#22359;&#21644;&#39069;&#22806;&#30340;&#24773;&#24863;&#23545;&#40784;&#20219;&#21153;&#65292;&#25913;&#38761;&#20102;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24773;&#32490;&#35782;&#21035;(ERC)&#30340;&#21457;&#23637;&#19968;&#30452;&#21463;&#21040;&#31649;&#36947;&#35774;&#35745;&#22797;&#26434;&#24615;&#30340;&#38459;&#30861;&#65292;&#23548;&#33268;ERC&#27169;&#22411;&#24448;&#24448;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#23545;&#35805;&#27169;&#24335;&#36807;&#25311;&#21512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;InstructERC&#65292;&#23558;ERC&#20219;&#21153;&#20174;&#21028;&#21035;&#24335;&#26694;&#26550;&#36716;&#21270;&#20026;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#29983;&#25104;&#24335;&#26694;&#26550;&#12290;InstructERC&#26377;&#20004;&#20010;&#37325;&#35201;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;InstructERC&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26816;&#32034;&#27169;&#26495;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#21382;&#21490;&#23545;&#35805;&#20869;&#23481;&#12289;&#26631;&#31614;&#35821;&#21477;&#21644;&#24773;&#24863;&#39046;&#22495;&#28436;&#31034;&#19982;&#39640;&#35821;&#20041;&#30456;&#20284;&#24615;&#36827;&#34892;&#25340;&#25509;&#65292;&#24110;&#21161;&#27169;&#22411;&#26126;&#30830;&#22320;&#38598;&#25104;&#22810;&#31890;&#24230;&#23545;&#35805;&#30417;&#30563;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#39069;&#22806;&#30340;&#24773;&#24863;&#23545;&#40784;&#20219;&#21153;&#65292;&#21363;&#35828;&#35805;&#20154;&#35782;&#21035;&#21644;&#24773;&#24863;&#39044;&#27979;&#20219;&#21153;&#65292;&#20197;&#38544;&#24335;&#22320;&#24314;&#27169;&#23545;&#35805;&#35282;&#33394;&#20851;&#31995;&#21644;&#26410;&#26469;&#23545;&#35805;&#24773;&#32490;&#20542;&#21521;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;LLM&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The development of emotion recognition in dialogue (ERC) has been consistently hindered by the complexity of pipeline designs, leading to ERC models that often overfit to specific datasets and dialogue patterns. In this study, we propose a novel approach, namely  InstructERC, to reformulates the ERC task from a discriminative framework to a generative framework based on Large Language Models (LLMs) . InstructERC has two significant contributions: Firstly, InstructERC introduces a simple yet effective retrieval template module, which helps the model explicitly integrate multi-granularity dialogue supervision information by concatenating the historical dialog content, label statement, and emotional domain demonstrations with high semantic similarity. Furthermore, we introduce two additional emotion alignment tasks, namely speaker identification and emotion prediction tasks, to implicitly model the dialogue role relationships and future emotional tendencies in conversations. Our LLM-based
&lt;/p&gt;</description></item><item><title>FiADD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28966;&#28857;&#25512;&#29702;&#27880;&#20837;&#19982;&#26131;&#22788;&#29702;&#23494;&#24230;&#21306;&#20998;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#30340;&#34920;&#38754;&#24418;&#24335;&#19982;&#26263;&#31034;&#30340;&#24418;&#24335;&#26356;&#25509;&#36817;&#65292;&#21516;&#26102;&#22686;&#21152;&#19981;&#21516;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#38598;&#32676;&#38388;&#36317;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#38544;&#24615;&#20167;&#24680;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11896</link><description>&lt;p&gt;
&#38024;&#23545;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#30340;&#28966;&#28857;&#25512;&#29702;&#27880;&#20837;&#19982;&#26131;&#22788;&#29702;&#23494;&#24230;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;
Focal Inferential Infusion Coupled with Tractable Density Discrimination for Implicit Hate Speech Detection. (arXiv:2309.11896v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11896
&lt;/p&gt;
&lt;p&gt;
FiADD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28966;&#28857;&#25512;&#29702;&#27880;&#20837;&#19982;&#26131;&#22788;&#29702;&#23494;&#24230;&#21306;&#20998;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#30340;&#34920;&#38754;&#24418;&#24335;&#19982;&#26263;&#31034;&#30340;&#24418;&#24335;&#26356;&#25509;&#36817;&#65292;&#21516;&#26102;&#22686;&#21152;&#19981;&#21516;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#38598;&#32676;&#38388;&#36317;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#38544;&#24615;&#20167;&#24680;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#35768;&#22810;NLP&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#23545;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#24494;&#22937;&#34920;&#36798;&#30340;&#29702;&#35299;&#12290;&#36825;&#26679;&#24494;&#22937;&#32780;&#38544;&#24615;&#30340;&#20167;&#24680;&#32463;&#24120;&#34987;&#38169;&#35823;&#22320;&#20998;&#31867;&#20026;&#38750;&#20167;&#24680;&#12290;&#36890;&#36807;&#22686;&#21152;&#22806;&#37096;&#30340;&#19978;&#19979;&#25991;&#25110;&#36890;&#36807;&#22522;&#20110;&#36317;&#31163;&#30340;&#24230;&#37327;&#24378;&#21046;&#26631;&#31614;&#20998;&#31163;&#65292;&#24050;&#32463;&#23581;&#35797;&#36807;&#21508;&#31181;&#26041;&#27861;&#26469;&#22686;&#24378;&#65288;&#38544;&#24615;&#65289;&#20167;&#24680;&#20869;&#23481;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28966;&#28857;&#25512;&#29702;&#36866;&#24212;&#23494;&#24230;&#21306;&#20998;&#26694;&#26550;&#65288;FiADD&#65289;&#12290;FiADD&#36890;&#36807;&#23558;&#38544;&#24615;&#20167;&#24680;&#35328;&#35770;&#30340;&#34920;&#38754;&#24418;&#24335;&#19982;&#26263;&#31034;&#30340;&#24418;&#24335;&#26356;&#25509;&#36817;&#65292;&#21516;&#26102;&#22686;&#21152;&#19981;&#21516;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#38598;&#32676;&#38388;&#36317;&#65292;&#26469;&#22686;&#24378;PLM&#24494;&#35843;&#31649;&#36947;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#38544;&#24615;&#20167;&#24680;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;FiADD&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#20004;&#31867;&#21644;&#19977;&#31867;&#20167;&#24680;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;FiADD&#22312;&#19977;&#20010;&#20854;&#20182;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#24615;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21363;&#26816;&#27979;&#35773;&#21050;&#12289;&#35773;&#21050;&#21644;&#31435;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although pre-trained large language models (PLMs) have achieved state-of-the-art on many NLP tasks, they lack understanding of subtle expressions of implicit hate speech. Such nuanced and implicit hate is often misclassified as non-hate. Various attempts have been made to enhance the detection of (implicit) hate content by augmenting external context or enforcing label separation via distance-based metrics. We combine these two approaches and introduce FiADD, a novel Focused Inferential Adaptive Density Discrimination framework. FiADD enhances the PLM finetuning pipeline by bringing the surface form of an implicit hate speech closer to its implied form while increasing the inter-cluster distance among various class labels. We test FiADD on three implicit hate datasets and observe significant improvement in the two-way and three-way hate classification tasks. We further experiment on the generalizability of FiADD on three other tasks, namely detecting sarcasm, irony, and stance, in whic
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36890;&#36807;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11895</link><description>&lt;p&gt;
&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Audio Contrastive based Fine-tuning. (arXiv:2309.11895v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36890;&#36807;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#20998;&#31867;&#22312;&#35821;&#38899;&#21644;&#22768;&#38899;&#22788;&#29702;&#20219;&#21153;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#22312;&#23558;&#27169;&#22411;&#25311;&#21512;&#21040;&#35757;&#32451;&#25968;&#25454;&#65288;&#36991;&#20813;&#36807;&#25311;&#21512;&#65289;&#24182;&#20351;&#20854;&#33021;&#22815;&#33391;&#22909;&#22320;&#27867;&#21270;&#21040;&#26032;&#39046;&#22495;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#30528;&#24179;&#34913;&#30340;&#25361;&#25112;&#12290;&#20511;&#21161;&#23545;&#27604;&#23398;&#20064;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#38899;&#39057;&#23545;&#27604;&#30340;&#24494;&#35843;&#26041;&#27861;&#65288;AudioConFit&#65289;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#21508;&#31181;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio classification plays a crucial role in speech and sound processing tasks with a wide range of applications. There still remains a challenge of striking the right balance between fitting the model to the training data (avoiding overfitting) and enabling it to generalise well to a new domain. Leveraging the transferability of contrastive learning, we introduce Audio Contrastive-based Fine-tuning (AudioConFit), an efficient approach characterised by robust generalisability. Empirical experiments on a variety of audio classification tasks demonstrate the effectiveness and robustness of our approach, which achieves state-of-the-art results in various settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#21516;&#26102;&#35299;&#26512;&#30701;&#35821;&#32467;&#26500;&#26641;&#21644;&#20381;&#23384;&#26641;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#26356;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#12289;&#22312;&#35757;&#32451;&#38454;&#27573;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#12289;&#25552;&#20986;&#39640;&#38454;&#35780;&#20998;&#32452;&#20214;&#20197;&#21450;&#36827;&#34892;&#28145;&#20837;&#23454;&#39564;&#21644;&#20998;&#26512;&#31561;&#22235;&#20010;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#28508;&#21147;&#21644;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2309.11888</link><description>&lt;p&gt;
&#21516;&#26102;&#35299;&#26512;&#30701;&#35821;&#32467;&#26500;&#26641;&#21644;&#20381;&#23384;&#26641;&#30495;&#30340;&#26377;&#29992;&#21527;&#65311;&#37325;&#26032;&#23457;&#35270;
&lt;/p&gt;
&lt;p&gt;
Is It Really Useful to Jointly Parse Constituency and Dependency Trees? A Revisit. (arXiv:2309.11888v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#21516;&#26102;&#35299;&#26512;&#30701;&#35821;&#32467;&#26500;&#26641;&#21644;&#20381;&#23384;&#26641;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#26356;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#12289;&#22312;&#35757;&#32451;&#38454;&#27573;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#12289;&#25552;&#20986;&#39640;&#38454;&#35780;&#20998;&#32452;&#20214;&#20197;&#21450;&#36827;&#34892;&#28145;&#20837;&#23454;&#39564;&#21644;&#20998;&#26512;&#31561;&#22235;&#20010;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#28508;&#21147;&#21644;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#21516;&#26102;&#35299;&#26512;&#30701;&#35821;&#32467;&#26500;&#26641;&#21644;&#20381;&#23384;&#26641;&#36825;&#19968;&#35805;&#39064;&#65292;&#21363;&#20026;&#36755;&#20837;&#21477;&#23376;&#21516;&#26102;&#29983;&#25104;&#20860;&#23481;&#30340;&#30701;&#35821;&#32467;&#26500;&#26641;&#21644;&#20381;&#23384;&#26641;&#65292;&#32771;&#34385;&#21040;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#26641;&#22312;&#34920;&#31034;&#35821;&#27861;&#26041;&#38754;&#26159;&#20114;&#34917;&#30340;&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65306;&#65288;1&#65289;&#37319;&#29992;&#26356;&#39640;&#25928;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#65288;2&#65289;&#22312;&#35757;&#32451;&#38454;&#27573;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;&#65288;3&#65289;&#20026;&#30701;&#35821;&#32467;&#26500;&#21644;&#20381;&#23384;&#20043;&#38388;&#30340;&#20132;&#20114;&#25552;&#20986;&#20102;&#39640;&#38454;&#35780;&#20998;&#32452;&#20214;&#65292;&#65288;4&#65289;&#36890;&#36807;&#28145;&#20837;&#23454;&#39564;&#21644;&#20998;&#26512;&#33719;&#24471;&#20102;&#26356;&#22810;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work visits the topic of jointly parsing constituency and dependency trees, i.e., to produce compatible constituency and dependency trees simultaneously for input sentences, which is attractive considering that the two types of trees are complementary in representing syntax. Compared with previous works, we make progress in four aspects: (1) adopting a much more efficient decoding algorithm, (2) exploring joint modeling at the training phase, instead of only at the inference phase, (3) proposing high-order scoring components for constituent-dependency interaction, (4) gaining more insights via in-depth experiments and analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#19968;&#20010;&#22797;&#26434;&#33258;&#36866;&#24212;&#31995;&#32479;-&#35821;&#27861;&#20043;&#38388;&#30340;&#21477;&#27861;&#21464;&#24322;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#21477;&#27861;&#21464;&#24322;&#19981;&#20165;&#28041;&#21450;&#20010;&#21035;&#33410;&#28857;&#65292;&#36824;&#28041;&#21450;&#25972;&#20010;&#35821;&#27861;&#32467;&#26500;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2309.11869</link><description>&lt;p&gt;
&#35821;&#27861;&#20043;&#38388;&#30340;&#21477;&#27861;&#21464;&#24322;&#65306;&#23545;&#19968;&#20010;&#22797;&#26434;&#33258;&#36866;&#24212;&#31995;&#32479;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Syntactic Variation Across the Grammar: Modelling a Complex Adaptive System. (arXiv:2309.11869v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#19968;&#20010;&#22797;&#26434;&#33258;&#36866;&#24212;&#31995;&#32479;-&#35821;&#27861;&#20043;&#38388;&#30340;&#21477;&#27861;&#21464;&#24322;&#36827;&#34892;&#20102;&#24314;&#27169;&#21644;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#21477;&#27861;&#21464;&#24322;&#19981;&#20165;&#28041;&#21450;&#20010;&#21035;&#33410;&#28857;&#65292;&#36824;&#28041;&#21450;&#25972;&#20010;&#35821;&#27861;&#32467;&#26500;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#35821;&#35328;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#33258;&#36866;&#24212;&#31995;&#32479;&#65292;&#20294;&#22823;&#37096;&#20998;&#20851;&#20110;&#21477;&#27861;&#21464;&#24322;&#30340;&#30740;&#31350;&#37117;&#26159;&#29420;&#31435;&#22320;&#35266;&#23519;&#20960;&#20010;&#21333;&#20010;&#30340;&#32467;&#26500;&#65292;&#32780;&#24573;&#35270;&#20102;&#25972;&#20010;&#35821;&#27861;&#30340;&#36830;&#25509;&#32593;&#32476;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#22320;&#27169;&#25311;&#33521;&#35821;&#20351;&#29992;&#32773;16&#20010;&#22269;&#23478;&#30340;49&#20010;&#24403;&#22320;&#32676;&#20307;&#30340;&#26041;&#35328;&#21464;&#24322;&#65292;&#37327;&#21270;&#20102;&#36825;&#31181;&#31616;&#21270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#25972;&#20010;&#35821;&#27861;&#20197;&#21450;&#35821;&#27861;&#20869;&#37096;&#30340;&#29420;&#31435;&#33410;&#28857;&#36827;&#34892;&#26041;&#35328;&#20998;&#31867;&#65292;&#20197;&#25551;&#36848;&#36825;&#20123;&#26041;&#35328;&#20043;&#38388;&#30340;&#21477;&#27861;&#24046;&#24322;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#39318;&#20808;&#65292;&#35821;&#27861;&#20869;&#30340;&#35768;&#22810;&#29420;&#31435;&#33410;&#28857;&#37117;&#21463;&#21040;&#21464;&#24322;&#30340;&#24433;&#21709;&#65292;&#20294;&#29420;&#31435;&#22320;&#26469;&#30475;&#65292;&#27809;&#26377;&#19968;&#20010;&#33410;&#28857;&#30340;&#34920;&#29616;&#33021;&#22815;&#19982;&#25972;&#20010;&#35821;&#27861;&#30456;&#27604;&#12290;&#36825;&#34920;&#26126;&#21477;&#27861;&#21464;&#24322;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#26159;&#35821;&#27861;&#19981;&#21516;&#37096;&#20998;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20854;&#27425;&#65292;&#32467;&#26524;&#34920;&#26126;&#30456;&#20284;&#30340;&#35821;&#35328;&#20043;&#38388;&#30340;&#35821;&#27861;&#24046;&#24322;&#21644;&#21306;&#21035;&#19981;&#21482;&#26159;&#20010;&#21035;&#33410;&#28857;&#30340;&#24046;&#24322;&#65292;&#32780;&#26159;&#25972;&#20010;&#35821;&#27861;&#32467;&#26500;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
While language is a complex adaptive system, most work on syntactic variation observes a few individual constructions in isolation from the rest of the grammar. This means that the grammar, a network which connects thousands of structures at different levels of abstraction, is reduced to a few disconnected variables. This paper quantifies the impact of such reductions by systematically modelling dialectal variation across 49 local populations of English speakers in 16 countries. We perform dialect classification with both an entire grammar as well as with isolated nodes within the grammar in order to characterize the syntactic differences between these dialects. The results show, first, that many individual nodes within the grammar are subject to variation but, in isolation, none perform as well as the grammar as a whole. This indicates that an important part of syntactic variation consists of interactions between different parts of the grammar. Second, the results show that the simila
&lt;/p&gt;</description></item><item><title>BitCoin&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#21452;&#21521;&#26631;&#35760;&#21644;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#32852;&#21512;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#20013;&#30340;&#38382;&#39064;&#21644;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#27599;&#20010;&#38170;&#28857;&#30340;&#22810;&#20010;&#27491;&#20363;&#65292;&#25552;&#39640;&#20102;&#25552;&#21462;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11853</link><description>&lt;p&gt;
BitCoin: &#22522;&#20110;&#21452;&#21521;&#26631;&#35760;&#21644;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#32852;&#21512;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BitCoin: Bidirectional Tagging and Supervised Contrastive Learning based Joint Relational Triple Extraction Framework. (arXiv:2309.11853v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11853
&lt;/p&gt;
&lt;p&gt;
BitCoin&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#21452;&#21521;&#26631;&#35760;&#21644;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#32852;&#21512;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#20013;&#30340;&#38382;&#39064;&#21644;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#32771;&#34385;&#20102;&#27599;&#20010;&#38170;&#28857;&#30340;&#22810;&#20010;&#27491;&#20363;&#65292;&#25552;&#39640;&#20102;&#25552;&#21462;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#26159;&#20449;&#24687;&#25552;&#21462;&#21644;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20013;&#30340;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#20173;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#12290;&#23427;&#20204;&#20165;&#20351;&#29992;&#27867;&#21270;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#26410;&#32771;&#34385;RTE&#20219;&#21153;&#30340;&#29305;&#27530;&#24615;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;RTE&#20219;&#21153;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#39318;&#20808;&#35782;&#21035;&#20027;&#20307;&#65292;&#28982;&#21518;&#35782;&#21035;&#23458;&#20307;&#21644;&#20851;&#31995;&#12290;&#23427;&#20204;&#20165;&#20851;&#27880;&#20174;&#20027;&#20307;&#21040;&#23458;&#20307;&#30340;&#20851;&#31995;&#19977;&#20803;&#32452;&#30340;&#25552;&#21462;&#65292;&#24573;&#35270;&#20102;&#19968;&#26086;&#20027;&#20307;&#25552;&#21462;&#22833;&#36133;&#65292;&#23601;&#26080;&#27861;&#25552;&#21462;&#19982;&#35813;&#20027;&#20307;&#30456;&#20851;&#30340;&#25152;&#26377;&#19977;&#20803;&#32452;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BitCoin&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#21452;&#21521;&#26631;&#35760;&#21644;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#32852;&#21512;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#27599;&#20010;&#38170;&#28857;&#30340;&#22810;&#20010;&#27491;&#20363;&#65292;&#32780;&#19981;&#20165;&#20165;&#38480;&#20110;&#19968;&#20010;&#27491;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation triple extraction (RTE) is an essential task in information extraction and knowledge graph construction. Despite recent advancements, existing methods still exhibit certain limitations. They just employ generalized pre-trained models and do not consider the specificity of RTE tasks. Moreover, existing tagging-based approaches typically decompose the RTE task into two subtasks, initially identifying subjects and subsequently identifying objects and relations. They solely focus on extracting relational triples from subject to object, neglecting that once the extraction of a subject fails, it fails in extracting all triples associated with that subject. To address these issues, we propose BitCoin, an innovative Bidirectional tagging and supervised Contrastive learning based joint relational triple extraction framework. Specifically, we design a supervised contrastive learning method that considers multiple positives per anchor rather than restricting it to just one positive. Furt
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38544;&#31169;&#38382;&#39064;&#30340;&#30693;&#35782;&#20928;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#65292;&#20351;&#20854;&#22312;&#34987;&#35810;&#38382;&#25935;&#24863;&#20449;&#24687;&#26102;&#29983;&#25104;&#26080;&#23475;&#22238;&#31572;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;&#30693;&#35782;&#27844;&#28431;&#24182;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11852</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#20928;&#21270;
&lt;/p&gt;
&lt;p&gt;
Knowledge Sanitization of Large Language Models. (arXiv:2309.11852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11852
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38544;&#31169;&#38382;&#39064;&#30340;&#30693;&#35782;&#20928;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#65292;&#20351;&#20854;&#22312;&#34987;&#35810;&#38382;&#25935;&#24863;&#20449;&#24687;&#26102;&#29983;&#25104;&#26080;&#23475;&#22238;&#31572;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#23569;&#30693;&#35782;&#27844;&#28431;&#24182;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#30693;&#35782;&#20928;&#21270;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#22312;&#22823;&#35268;&#27169;Web&#25968;&#25454;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;LLMs&#21487;&#20197;&#35760;&#20303;&#24182;&#28508;&#22312;&#22320;&#36879;&#38706;&#25935;&#24863;&#25110;&#26426;&#23494;&#20449;&#24687;&#65292;&#24341;&#21457;&#20851;&#38190;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#36890;&#36807;&#24494;&#35843;&#36825;&#20123;&#27169;&#22411;&#65292;&#20419;&#20351;&#23427;&#20204;&#22312;&#34987;&#35810;&#38382;&#29305;&#23450;&#20449;&#24687;&#26102;&#29983;&#25104;&#26080;&#23475;&#30340;&#22238;&#31572;&#65292;&#20363;&#22914;&#8220;&#25105;&#19981;&#30693;&#36947;&#8221;&#12290;&#22312;&#23553;&#38381;&#24335;&#38382;&#31572;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#31616;&#21333;&#30340;&#26041;&#27861;&#19981;&#20165;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#20102;&#29305;&#23450;&#30693;&#35782;&#27844;&#28431;&#65292;&#36824;&#20445;&#30041;&#20102;LLM&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;&#36825;&#20004;&#20010;&#20248;&#28857;&#21152;&#24378;&#20102;&#23545;&#25552;&#21462;&#25915;&#20987;&#30340;&#38450;&#24481;&#65292;&#24182;&#20943;&#23569;&#20102;&#20135;&#29983;&#24187;&#35273;&#31561;&#26377;&#23475;&#20869;&#23481;&#30340;&#21457;&#36865;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore a knowledge sanitization approach to mitigate the privacy concerns associated with large language models (LLMs). LLMs trained on a large corpus of Web data can memorize and potentially reveal sensitive or confidential information, raising critical security concerns. Our technique fine-tunes these models, prompting them to generate harmless responses such as ``I don't know'' when queried about specific information. Experimental results in a closed-book question-answering task show that our straightforward method not only minimizes particular knowledge leakage but also preserves the overall performance of LLM. These two advantages strengthen the defense against extraction attacks and reduces the emission of harmful content such as hallucinations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35805;&#35821;&#32423;&#22810;&#23610;&#24230;&#38901;&#24459;&#27169;&#22411;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#39044;&#27979;&#36866;&#24403;&#30340;&#38901;&#24459;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25351;&#23548;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#29983;&#25104;&#26356;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#35821;&#38899;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22810;&#23610;&#24230;&#25991;&#26412;&#20449;&#24687;&#23545;&#20110;&#39044;&#27979;&#24773;&#24863;&#38901;&#24459;&#29305;&#24449;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11849</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#30340;&#35805;&#35821;&#32423;&#22810;&#23610;&#24230;&#38901;&#24459;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Discourse-level Multi-scale Prosodic Model for Fine-grained Emotion Analysis. (arXiv:2309.11849v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35805;&#35821;&#32423;&#22810;&#23610;&#24230;&#38901;&#24459;&#27169;&#22411;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#39044;&#27979;&#36866;&#24403;&#30340;&#38901;&#24459;&#29305;&#24449;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25351;&#23548;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#29983;&#25104;&#26356;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#35821;&#38899;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22810;&#23610;&#24230;&#25991;&#26412;&#20449;&#24687;&#23545;&#20110;&#39044;&#27979;&#24773;&#24863;&#38901;&#24459;&#29305;&#24449;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20174;&#35805;&#35821;&#32423;&#25991;&#26412;&#39044;&#27979;&#36866;&#24403;&#30340;&#38901;&#24459;&#29305;&#24449;&#29992;&#20110;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#39118;&#26684;&#36716;&#31227;&#27169;&#22411;&#65292;&#25105;&#20204;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#20102;&#38899;&#32032;&#32423;&#30340;&#26412;&#22320;&#38901;&#24459;&#23884;&#20837;&#24207;&#21015; (LPEs) &#21644;&#20840;&#23616;&#39118;&#26684;&#23884;&#20837;&#20316;&#20026;&#38901;&#24459;&#35821;&#38899;&#29305;&#24449;&#65292;&#20197;&#20316;&#20026;&#25105;&#20204;&#27169;&#22411;&#30340;&#39044;&#27979;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35805;&#35821;&#32423;&#22810;&#23610;&#24230;&#25991;&#26412;&#38901;&#24459;&#27169;&#22411; (D-MPM)&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#23610;&#24230;&#25991;&#26412;&#26469;&#39044;&#27979;&#36825;&#20004;&#20010;&#38901;&#24459;&#29305;&#24449;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#26356;&#22909;&#30340;&#24773;&#24863;&#38901;&#24459;&#29305;&#24449;&#65292;&#24182;&#25351;&#23548;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#21512;&#25104;&#26356;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#35821;&#38899;&#12290;&#20026;&#20102;&#23450;&#37327;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#26032;&#19988;&#22823;&#35268;&#27169;&#30340;&#35805;&#35821;&#32423;&#20013;&#25991;&#26377;&#22768;&#20070; (DCA) &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;13,000&#20010;&#26631;&#27880;&#24207;&#21015;&#29992;&#20110;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;&#22312;DCA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22810;&#23610;&#24230;&#25991;&#26412;&#20449;&#24687;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24773;&#24863;&#38901;&#24459;&#29305;&#24449;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores predicting suitable prosodic features for fine-grained emotion analysis from the discourse-level text. To obtain fine-grained emotional prosodic features as predictive values for our model, we extract a phoneme-level Local Prosody Embedding sequence (LPEs) and a Global Style Embedding as prosodic speech features from the speech with the help of a style transfer model. We propose a Discourse-level Multi-scale text Prosodic Model (D-MPM) that exploits multi-scale text to predict these two prosodic features. The proposed model can be used to analyze better emotional prosodic features and thus guide the speech synthesis model to synthesize more expressive speech. To quantitatively evaluate the proposed model, we contribute a new and large-scale Discourse-level Chinese Audiobook (DCA) dataset with more than 13,000 utterances annotated sequences to evaluate the proposed model. Experimental results on the DCA dataset show that the multi-scale text information effectively h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20449;&#24687;&#25628;&#32034;&#23545;&#35805;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#25991;&#26723;&#30340;&#21709;&#24212;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11838</link><description>&lt;p&gt;
&#22312;&#20449;&#24687;&#25628;&#32034;&#23545;&#35805;&#20013;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#22522;&#20110;&#25991;&#26723;&#30340;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues. (arXiv:2309.11838v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20449;&#24687;&#25628;&#32034;&#23545;&#35805;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#25991;&#26723;&#30340;&#21709;&#24212;&#29983;&#25104;&#65292;&#24182;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20449;&#24687;&#25628;&#32034;&#23545;&#35805;&#30340;&#32972;&#26223;&#19979;&#65292;&#20351;&#29992;&#31867;&#20284;ChatGPT&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#22522;&#20110;&#25991;&#26723;&#30340;&#21709;&#24212;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20808;&#21069;&#22312;DialDoc 2022&#20849;&#20139;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#22235;&#20010;&#31038;&#20250;&#26381;&#21153;&#39046;&#22495;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;MultiDoc2Dial&#35821;&#26009;&#24211;&#36827;&#34892;&#35780;&#20272;&#12290;&#20449;&#24687;&#25628;&#32034;&#23545;&#35805;&#30340;&#36716;&#25442;&#20197;&#22810;&#20010;&#25552;&#20379;&#30456;&#20851;&#20449;&#24687;&#30340;&#25991;&#26723;&#20026;&#22522;&#30784;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20004;&#31181;&#26041;&#27861;Chat-Completion&#21644;LlamaIndex&#65292;&#36890;&#36807;&#28608;&#21457;ChatGPT&#27169;&#22411;&#26469;&#29983;&#25104;&#23545;&#35805;&#23436;&#25104;&#21709;&#24212;&#12290;ChatCompletion&#20351;&#29992;ChatGPT&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#65292;&#32780;LlamaIndex&#36824;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36890;&#36807;LLM&#36827;&#34892;&#22522;&#20110;&#25991;&#26723;&#30340;&#21709;&#24212;&#29983;&#25104;&#19981;&#33021;&#36890;&#36807;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#20805;&#20998;&#35780;&#20272;&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#21152;&#20887;&#38271;&#65292;&#25152;&#20197;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#20854;&#20013;&#35780;&#20272;&#21592;&#23545;&#20849;&#20139;&#20219;&#21153;&#30340;&#33719;&#22870;&#31995;&#32479;&#12289;&#20004;&#20010;Chat-GPT&#21464;&#20307;&#30340;&#36755;&#20986;&#20197;&#21450;&#20154;&#31867;&#21709;&#24212;&#36827;&#34892;&#35780;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate the use of large language models (LLMs) like ChatGPT for document-grounded response generation in the context of information-seeking dialogues. For evaluation, we use the MultiDoc2Dial corpus of task-oriented dialogues in four social service domains previously used in the DialDoc 2022 Shared Task. Information-seeking dialogue turns are grounded in multiple documents providing relevant information. We generate dialogue completion responses by prompting a ChatGPT model, using two methods: Chat-Completion and LlamaIndex. ChatCompletion uses knowledge from ChatGPT model pretraining while LlamaIndex also extracts relevant information from documents. Observing that document-grounded response generation via LLMs cannot be adequately assessed by automatic evaluation metrics as they are significantly more verbose, we perform a human evaluation where annotators rate the output of the shared task winning system, the two Chat-GPT variants outputs, and human responses.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;LLMs&#30340;&#20013;&#25991;Prompt Attack&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#35780;&#20272;&#38450;&#24481;&#25552;&#31034;&#25915;&#20987;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.11830</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#20855;&#26377;&#24694;&#24847;&#20869;&#23481;&#30340;LLMs&#30340;&#20013;&#25991;&#25552;&#31034;&#25915;&#20987;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Chinese Prompt Attack Dataset for LLMs with Evil Content. (arXiv:2309.11830v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11830
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;LLMs&#30340;&#20013;&#25991;Prompt Attack&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#35780;&#20272;&#38450;&#24481;&#25552;&#31034;&#25915;&#20987;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#24212;&#29992;&#20013;&#23384;&#22312;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#39118;&#38505;&#65292;&#23588;&#20854;&#26159;&#22312;&#20351;&#29992;Prompt Attack&#31561;&#40657;&#30418;&#25915;&#20987;&#26041;&#27861;&#26102;&#12290;&#30740;&#31350;&#20154;&#21592;&#23545;LLMs&#30340;Prompt Attack&#21644;Defense&#24456;&#24863;&#20852;&#36259;&#65292;&#20294;&#30446;&#21069;&#27809;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#38450;&#24481;Prompt Attack&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;LLMs&#30340;&#20013;&#25991;Prompt Attack&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;CPAD&#12290;&#25105;&#20204;&#30340;&#25552;&#31034;&#26088;&#22312;&#24341;&#23548;LLMs&#29983;&#25104;&#20855;&#26377;&#22810;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#25915;&#20987;&#26041;&#27861;&#21644;&#24191;&#27867;&#20851;&#27880;&#30340;&#25915;&#20987;&#20869;&#23481;&#30340;&#24847;&#22806;&#36755;&#20986;&#12290;&#19982;&#20197;&#21069;&#28041;&#21450;&#23433;&#20840;&#20272;&#35745;&#30340;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#26500;&#24314;&#30340;&#25552;&#31034;&#32771;&#34385;&#20102;&#19977;&#20010;&#32500;&#24230;&#65306;&#20869;&#23481;&#12289;&#25915;&#20987;&#26041;&#27861;&#21644;&#30446;&#26631;&#65292;&#22240;&#27492;&#21487;&#20197;&#36731;&#26494;&#35780;&#20272;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) present significant priority in text understanding and generation. However, LLMs suffer from the risk of generating harmful contents especially while being employed to applications. There are several black-box attack methods, such as Prompt Attack, which can change the behaviour of LLMs and induce LLMs to generate unexpected answers with harmful contents. Researchers are interested in Prompt Attack and Defense with LLMs, while there is no publicly available dataset to evaluate the abilities of defending prompt attack. In this paper, we introduce a Chinese Prompt Attack Dataset for LLMs, called CPAD. Our prompts aim to induce LLMs to generate unexpected outputs with several carefully designed prompt attack approaches and widely concerned attacking contents. Different from previous datasets involving safety estimation, We construct the prompts considering three dimensions: contents, attacking methods and goals, thus the responses can be easily evaluated and a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#19982;&#35789;&#23884;&#20837;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#30340;&#31070;&#32463;&#27010;&#29575;&#20808;&#39564;&#65292;&#33021;&#22815;&#22686;&#24378;&#23884;&#20837;&#21521;&#37327;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#35789;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.11824</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#27010;&#29575;&#20808;&#39564;&#30340;&#35789;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Word Embedding with Neural Probabilistic Prior. (arXiv:2309.11824v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11824
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#19982;&#35789;&#23884;&#20837;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#30340;&#31070;&#32463;&#27010;&#29575;&#20808;&#39564;&#65292;&#33021;&#22815;&#22686;&#24378;&#23884;&#20837;&#21521;&#37327;&#30340;&#34920;&#31034;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#35789;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25913;&#36827;&#35789;&#34920;&#31034;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#19982;&#35789;&#23884;&#20837;&#27169;&#22411;&#26080;&#32541;&#38598;&#25104;&#30340;&#27010;&#29575;&#20808;&#39564;&#12290;&#19981;&#21516;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#65292;&#35789;&#23884;&#20837;&#34987;&#35270;&#20026;&#19968;&#20010;&#27010;&#29575;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23545;&#35789;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#20808;&#39564;&#27491;&#21017;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#20808;&#39564;&#19981;&#20165;&#22686;&#24378;&#20102;&#23884;&#20837;&#21521;&#37327;&#30340;&#34920;&#31034;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#20808;&#39564;&#32467;&#26500;&#31616;&#21333;&#26377;&#25928;&#65292;&#21487;&#20197;&#36731;&#26494;&#23454;&#29616;&#24182;&#28789;&#27963;&#22320;&#25554;&#20837;&#21040;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35789;&#23884;&#20837;&#27169;&#22411;&#20013;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#35789;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve word representation learning, we propose a probabilistic prior which can be seamlessly integrated with word embedding models. Different from previous methods, word embedding is taken as a probabilistic generative model, and it enables us to impose a prior regularizing word representation learning. The proposed prior not only enhances the representation of embedding vectors but also improves the model's robustness and stability. The structure of the proposed prior is simple and effective, and it can be easily implemented and flexibly plugged in most existing word embedding models. Extensive experiments show the proposed method improves word representation on various tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#21644;&#26412;&#20307;&#31867;&#21517;&#30340;&#35821;&#20041;&#21644;&#35789;&#27719;&#29305;&#24449;&#65292;&#23558;Wikipedia&#30340;&#20998;&#31867;&#21644;&#21015;&#34920;&#26144;&#23556;&#21040;DBpedia&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#23436;&#21892;&#21644;&#32454;&#31890;&#24230;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;</title><link>http://arxiv.org/abs/2309.11791</link><description>&lt;p&gt;
SLHCat: &#21033;&#29992;&#35821;&#20041;&#12289;&#35789;&#27719;&#21644;&#23618;&#27425;&#29305;&#24449;&#23558;Wikipedia&#30340;&#20998;&#31867;&#21644;&#21015;&#34920;&#26144;&#23556;&#21040;DBpedia
&lt;/p&gt;
&lt;p&gt;
SLHCat: Mapping Wikipedia Categories and Lists to DBpedia by Leveraging Semantic, Lexical, and Hierarchical Features. (arXiv:2309.11791v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11791
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#21644;&#26412;&#20307;&#31867;&#21517;&#30340;&#35821;&#20041;&#21644;&#35789;&#27719;&#29305;&#24449;&#65292;&#23558;Wikipedia&#30340;&#20998;&#31867;&#21644;&#21015;&#34920;&#26144;&#23556;&#21040;DBpedia&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#23436;&#21892;&#21644;&#32454;&#31890;&#24230;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wikipedia&#30340;&#25991;&#31456;&#36890;&#36807;&#20998;&#31867;&#21644;&#21015;&#34920;&#36827;&#34892;&#23618;&#27425;&#21270;&#32452;&#32455;&#65292;&#25552;&#20379;&#20102;&#20854;&#20013;&#19968;&#20010;&#26368;&#20840;&#38754;&#21644;&#26222;&#36941;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#20294;&#20854;&#24320;&#25918;&#24615;&#23548;&#33268;&#20102;&#37325;&#22797;&#21644;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;&#23558;DBpedia&#30340;&#31867;&#21035;&#20998;&#37197;&#32473;Wikipedia&#30340;&#20998;&#31867;&#21644;&#21015;&#34920;&#21487;&#20197;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#23454;&#29616;&#19968;&#20010;&#23545;&#23454;&#20307;&#38142;&#25509;&#21644;&#31867;&#22411;&#20998;&#31867;&#25968;&#23383;&#20869;&#23481;&#33267;&#20851;&#37325;&#35201;&#30340;&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CaLiGraph&#26041;&#27861;&#20135;&#29983;&#20102;&#19981;&#23436;&#25972;&#21644;&#38750;&#32454;&#31890;&#24230;&#30340;&#26144;&#23556;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#30475;&#20316;&#26412;&#20307;&#23545;&#40784;&#65292;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#20449;&#24687;&#21644;&#26412;&#20307;&#31867;&#21517;&#30340;&#35789;&#27719;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#21457;&#29616;&#33258;&#20449;&#26144;&#23556;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#26144;&#23556;&#20197;&#36828;&#31243;&#30417;&#30563;&#26041;&#24335;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;SLHCat&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;1&#65289;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#12289;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#21629;&#21517;&#23454;&#20307;&#33258;&#21160;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Wikipedia articles are hierarchically organized through categories and lists, providing one of the most comprehensive and universal taxonomy, but its open creation is causing redundancies and inconsistencies. Assigning DBPedia classes to Wikipedia categories and lists can alleviate the problem, realizing a large knowledge graph which is essential for categorizing digital contents through entity linking and typing. However, the existing approach of CaLiGraph is producing incomplete and non-fine grained mappings. In this paper, we tackle the problem as ontology alignment, where structural information of knowledge graphs and lexical and semantic features of ontology class names are utilized to discover confident mappings, which are in turn utilized for finetuing pretrained language models in a distant supervision fashion. Our method SLHCat consists of two main parts: 1) Automatically generating training data by leveraging knowledge graph structure, semantic similarities, and named entity 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ContextRef&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#25551;&#36848;&#29983;&#25104;&#30340;&#26080;&#21442;&#32771;&#24230;&#37327;&#12290;ContextRef&#21253;&#25324;&#20154;&#31867;&#35780;&#20998;&#21644;&#40065;&#26834;&#24615;&#26816;&#26597;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#19978;&#19979;&#25991;&#65292;&#36890;&#36807;&#31934;&#24515;&#24494;&#35843;&#21487;&#20197;&#21462;&#24471;&#23454;&#36136;&#24615;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2309.11710</link><description>&lt;p&gt;
ContextRef: &#35780;&#20272;&#22270;&#20687;&#25551;&#36848;&#29983;&#25104;&#30340;&#26080;&#21442;&#32771;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
ContextRef: Evaluating Referenceless Metrics For Image Description Generation. (arXiv:2309.11710v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ContextRef&#65292;&#29992;&#20110;&#35780;&#20272;&#22270;&#20687;&#25551;&#36848;&#29983;&#25104;&#30340;&#26080;&#21442;&#32771;&#24230;&#37327;&#12290;ContextRef&#21253;&#25324;&#20154;&#31867;&#35780;&#20998;&#21644;&#40065;&#26834;&#24615;&#26816;&#26597;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#19978;&#19979;&#25991;&#65292;&#36890;&#36807;&#31934;&#24515;&#24494;&#35843;&#21487;&#20197;&#21462;&#24471;&#23454;&#36136;&#24615;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#21442;&#32771;&#24230;&#37327;&#65288;&#20363;&#22914;CLIPScore&#65289;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30452;&#25509;&#35780;&#20272;&#22270;&#20687;&#25551;&#36848;&#65292;&#32780;&#26080;&#38656;&#26114;&#36149;&#30340;&#30495;&#23454;&#21442;&#32771;&#25991;&#26412;&#12290;&#36825;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#20419;&#36827;&#24555;&#36895;&#36827;&#27493;&#65292;&#20294;&#21069;&#25552;&#26159;&#23427;&#20204;&#30495;&#27491;&#19982;&#20154;&#30340;&#20559;&#22909;&#21028;&#26029;&#30456;&#21563;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ContextRef&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#26080;&#21442;&#32771;&#24230;&#37327;&#20197;&#23454;&#29616;&#27492;&#23545;&#40784;&#30340;&#22522;&#20934;&#12290;ContextRef&#26377;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#22522;&#20110;&#21508;&#31181;&#24050;&#24314;&#31435;&#30340;&#36136;&#37327;&#32500;&#24230;&#30340;&#20154;&#31867;&#35780;&#20998;&#65292;&#20197;&#21450;&#35774;&#35745;&#20102;&#21313;&#31181;&#19981;&#21516;&#30340;&#40065;&#26834;&#24615;&#26816;&#26597;&#65292;&#26088;&#22312;&#21457;&#29616;&#22522;&#26412;&#24369;&#28857;&#12290;ContextRef&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#22270;&#20687;&#21644;&#25551;&#36848;&#20197;&#19978;&#19979;&#25991;&#26041;&#24335;&#21576;&#29616;&#65292;&#21453;&#26144;&#20102;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#19978;&#19979;&#25991;&#23545;&#25551;&#36848;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#12290;&#20351;&#29992;ContextRef&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#35780;&#20998;&#20989;&#25968;&#21644;&#25972;&#21512;&#19978;&#19979;&#25991;&#30340;&#25216;&#26415;&#12290;&#20294;&#26159;&#65292;&#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#22312;ContextRef&#19978;&#33719;&#24471;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#32463;&#36807;&#31934;&#24515;&#24494;&#35843;&#21487;&#20197;&#21462;&#24471;&#23454;&#36136;&#24615;&#30340;&#25913;&#21892;&#12290;ContextRef&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Referenceless metrics (e.g., CLIPScore) use pretrained vision--language models to assess image descriptions directly without costly ground-truth reference texts. Such methods can facilitate rapid progress, but only if they truly align with human preference judgments. In this paper, we introduce ContextRef, a benchmark for assessing referenceless metrics for such alignment. ContextRef has two components: human ratings along a variety of established quality dimensions, and ten diverse robustness checks designed to uncover fundamental weaknesses. A crucial aspect of ContextRef is that images and descriptions are presented in context, reflecting prior work showing that context is important for description quality. Using ContextRef, we assess a variety of pretrained models, scoring functions, and techniques for incorporating context. None of the methods is successful with ContextRef, but we show that careful fine-tuning yields substantial improvements. ContextRef remains a challenging bench
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#20223;&#29983;&#35760;&#24518;&#26426;&#21046;&#65292;&#37197;&#22791;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#27169;&#24335;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;LLMs&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11696</link><description>&lt;p&gt;
&#24102;&#26377;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#21327;&#20316;&#30340;&#22686;&#24378;&#35760;&#24518;&#22411;LLM&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Memory-Augmented LLM Personalization with Short- and Long-Term Memory Coordination. (arXiv:2309.11696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#20223;&#29983;&#35760;&#24518;&#26426;&#21046;&#65292;&#37197;&#22791;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#27169;&#24335;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;LLMs&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT3.5&#65292;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#38750;&#20010;&#24615;&#21270;&#29983;&#25104;&#26041;&#24335;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#29305;&#23450;&#32467;&#26524;&#30340;&#20122;&#20248;&#21270;&#12290;&#36890;&#24120;&#65292;&#29992;&#25143;&#26681;&#25454;&#33258;&#24049;&#30340;&#30693;&#35782;&#21644;&#20559;&#22909;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#23545;&#35805;&#12290;&#36825;&#23601;&#38656;&#35201;&#22686;&#24378;&#38754;&#21521;&#29992;&#25143;&#30340;LLM&#30340;&#20219;&#21153;&#65292;&#20294;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#26410;&#28145;&#20837;&#25506;&#32034;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#26469;&#23384;&#20648;&#21644;&#26816;&#32034;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;&#29983;&#25104;&#32780;&#26080;&#38656;&#20026;&#26032;&#30340;&#26597;&#35810;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#20165;&#20165;&#20351;&#29992;&#35760;&#24518;&#27169;&#22359;&#26080;&#27861;&#29702;&#35299;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#24182;&#19988;&#23436;&#20840;&#35757;&#32451;&#19968;&#20010;LLM&#21487;&#33021;&#25104;&#26412;&#36807;&#39640;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#20223;&#29983;&#35760;&#24518;&#26426;&#21046;&#65292;&#37197;&#22791;&#20102;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#27169;&#24335;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;LLMs&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable proficiency in comprehending and generating natural language. However, their unpersonalized generation paradigm may result in suboptimal user-specific outcomes. Typically, users converse differently based on their knowledge and preferences. This necessitates the task of enhancing user-oriented LLM which remains unexplored. While one can fully train an LLM for this objective, the resource consumption is unaffordable. Prior research has explored memory-based methods to store and retrieve knowledge to enhance generation without retraining for new queries. However, we contend that a mere memory module is inadequate to comprehend a user's preference, and fully training an LLM can be excessively costly. In this study, we propose a novel computational bionic memory mechanism, equipped with a parameter-efficient fine-tuning schema, to personalize LLMs. Our extensive experimental results demonstrate the effectiveness and su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#26032;&#38395;&#35805;&#35821;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26032;&#38395;&#25991;&#31456;&#30340;&#32467;&#26500;&#29305;&#28857;&#26469;&#35299;&#20915;&#27880;&#37322;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11692</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#26032;&#38395;&#35805;&#35821;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised News Discourse Profiling with Contrastive Learning. (arXiv:2309.11692v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11692
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#26032;&#38395;&#35805;&#35821;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26032;&#38395;&#25991;&#31456;&#30340;&#32467;&#26500;&#29305;&#28857;&#26469;&#35299;&#20915;&#27880;&#37322;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#35780;&#20272;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#35805;&#35821;&#29305;&#24449;&#25552;&#21462;&#26088;&#22312;&#23457;&#26597;&#26032;&#38395;&#25991;&#31456;&#20013;&#27599;&#20010;&#21477;&#23376;&#30340;&#20107;&#20214;&#30456;&#20851;&#35282;&#33394;&#65292;&#24182;&#24050;&#22312;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#20013;&#35777;&#26126;&#20854;&#26377;&#29992;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#32473;&#23450;&#26032;&#38395;&#35805;&#35821;&#30340;&#24773;&#22659;&#20013;&#65292;&#27599;&#20010;&#21477;&#23376;&#37117;&#34987;&#20998;&#37197;&#21040;&#19968;&#20010;&#39044;&#23450;&#20041;&#30340;&#31867;&#21035;&#65292;&#21462;&#20915;&#20110;&#20854;&#23545;&#26032;&#38395;&#20107;&#20214;&#32467;&#26500;&#30340;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29983;&#25104;&#35805;&#35821;&#32423;&#27880;&#37322;&#30340;&#32321;&#29712;&#21644;&#32791;&#26102;&#24615;&#36136;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#21487;&#29992;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#30340;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#65292;&#31216;&#20026;&#20869;&#37096;&#25991;&#26723;&#23545;&#27604;&#23398;&#20064;&#19982;&#33976;&#39311;&#65288;ICLD&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#26032;&#38395;&#35805;&#35821;&#29305;&#24449;&#25552;&#21462;&#20219;&#21153;&#65292;&#21033;&#29992;&#20854;&#29420;&#29305;&#30340;&#32467;&#26500;&#29305;&#28857;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#22312;&#36825;&#20010;&#20219;&#21153;&#33539;&#24335;&#20013;&#24212;&#29992;&#21322;&#30417;&#30563;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#24182;&#19988;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
News Discourse Profiling seeks to scrutinize the event-related role of each sentence in a news article and has been proven useful across various downstream applications. Specifically, within the context of a given news discourse, each sentence is assigned to a pre-defined category contingent upon its depiction of the news event structure. However, existing approaches suffer from an inadequacy of available human-annotated data, due to the laborious and time-intensive nature of generating discourse-level annotations. In this paper, we present a novel approach, denoted as Intra-document Contrastive Learning with Distillation (ICLD), for addressing the news discourse profiling task, capitalizing on its unique structural characteristics. Notably, we are the first to apply a semi-supervised methodology within this task paradigm, and evaluation demonstrates the effectiveness of the presented approach.
&lt;/p&gt;</description></item><item><title>LLM&#24341;&#23548;&#30340;&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;REBEL&#36890;&#36807;&#36882;&#24402;&#38382;&#39064;&#20998;&#35299;&#21644;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#36827;&#34892;&#25512;&#29702;&#65292;&#33021;&#22815;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#21644;&#23545;&#35805;&#29615;&#22659;&#20013;&#30340;&#28145;&#24230;&#25512;&#29702;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.11688</link><description>&lt;p&gt;
LLM&#24341;&#23548;&#30340;&#24402;&#32435;&#25512;&#29702;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
LLM Guided Inductive Inference for Solving Compositional Problems. (arXiv:2309.11688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11688
&lt;/p&gt;
&lt;p&gt;
LLM&#24341;&#23548;&#30340;&#24402;&#32435;&#25512;&#29702;&#26041;&#27861;REBEL&#36890;&#36807;&#36882;&#24402;&#38382;&#39064;&#20998;&#35299;&#21644;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#36827;&#34892;&#25512;&#29702;&#65292;&#33021;&#22815;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#21644;&#23545;&#35805;&#29615;&#22659;&#20013;&#30340;&#28145;&#24230;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#24403;&#38382;&#39064;&#38656;&#35201;&#36890;&#36807;&#30452;&#25509;&#35266;&#23519;&#25110;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#20132;&#20114;&#26469;&#33719;&#21462;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#21253;&#25324;&#30340;&#30693;&#35782;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#39034;&#24207;&#35843;&#29992;&#27169;&#22359;&#26469;&#23545;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#20998;&#35299;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22238;&#31572;&#28145;&#24230;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22522;&#20110;&#36882;&#24402;&#30340;&#21487;&#25193;&#23637;LLM&#65288;REBEL&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#21160;&#24577;&#35268;&#21010;&#21644;&#21069;&#21521;&#38142;&#25509;&#31574;&#30053;&#31561;&#33258;&#21160;&#25512;&#29702;&#25216;&#26415;&#22788;&#29702;&#24320;&#25918;&#19990;&#30028;&#30340;&#28145;&#24230;&#25512;&#29702;&#20219;&#21153;&#12290;REBEL&#36890;&#36807;&#36882;&#24402;&#38382;&#39064;&#20998;&#35299;&#21644;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#36827;&#34892;&#25512;&#29702;&#12290;REBEL&#20351;&#29992;&#30340;&#24037;&#20855;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#25351;&#23450;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22312;&#32452;&#21512;&#21644;&#23545;&#35805;&#29615;&#22659;&#20013;&#30340;&#19968;&#32452;&#38656;&#35201;&#28145;&#24230;&#23884;&#22871;&#30340;&#22806;&#37096;&#24037;&#20855;&#20351;&#29992;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;REBEL&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have demonstrated impressive performance in question-answering tasks, their performance is limited when the questions require knowledge that is not included in the model's training data and can only be acquired through direct observation or interaction with the real world. Existing methods decompose reasoning tasks through the use of modules invoked sequentially, limiting their ability to answer deep reasoning tasks. We introduce a method, Recursion based extensible LLM (REBEL), which handles open-world, deep reasoning tasks by employing automated reasoning techniques like dynamic planning and forward-chaining strategies. REBEL allows LLMs to reason via recursive problem decomposition and utilization of external tools. The tools that REBEL uses are specified only by natural language description. We further demonstrate REBEL capabilities on a set of problems that require a deeply nested use of external tools in a compositional and conversational settin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#22120;(ALMA)&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#28040;&#38500;&#20102;&#23545;&#22823;&#37327;&#24179;&#34892;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2309.11674</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#33539;&#24335;&#36716;&#21464;&#65306;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models. (arXiv:2309.11674v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#22120;(ALMA)&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#28040;&#38500;&#20102;&#23545;&#22823;&#37327;&#24179;&#34892;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#65292;&#23588;&#20854;&#26159;&#37027;&#20123;&#20855;&#26377;&#36866;&#24230;&#27169;&#22411;&#22823;&#23567;&#65288;&#21363;7B&#25110;13B&#21442;&#25968;&#65289;&#30340;&#20219;&#21153;&#65292;&#36825;&#20123;&#36827;&#23637;&#23578;&#26410;&#24471;&#21040;&#21453;&#26144;&#65292;&#20173;&#28982;&#33853;&#21518;&#20110;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#25913;&#21892;&#36825;&#20123;&#36866;&#24230;LLMs&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#20294;&#20854;&#22686;&#30410;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLMs&#24494;&#35843;&#26041;&#27861;&#65292;&#19987;&#20026;&#32763;&#35793;&#20219;&#21153;&#32780;&#35774;&#35745;&#65292;&#28040;&#38500;&#20102;&#20256;&#32479;&#32763;&#35793;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#22823;&#37327;&#24179;&#34892;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#24494;&#35843;&#38454;&#27573;&#65306;&#22312;&#21333;&#35821;&#25968;&#25454;&#19978;&#36827;&#34892;&#21021;&#22987;&#24494;&#35843;&#65292;&#28982;&#21518;&#22312;&#19968;&#23567;&#37096;&#20998;&#39640;&#36136;&#37327;&#24179;&#34892;&#25968;&#25454;&#19978;&#36827;&#34892;&#21518;&#32493;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#24320;&#21457;&#30340;LLM&#34987;&#31216;&#20026;&#22522;&#20110;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#22120;(ALMA)&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. However, these advances have not been reflected in the translation task, especially those with moderate model sizes (i.e., 7B or 13B parameters), which still lag behind conventional supervised encoder-decoder translation models. Previous studies have attempted to improve the translation capabilities of these moderate LLMs, but their gains have been limited. In this study, we propose a novel fine-tuning approach for LLMs that is specifically designed for the translation task, eliminating the need for the abundant parallel data that traditional translation models usually depend on. Our approach consists of two fine-tuning stages: initial fine-tuning on monolingual data followed by subsequent fine-tuning on a small set of high-quality parallel data. We introduce the LLM developed through this strategy as Advanced Language Model-based trAnslator (ALMA). Based on LLaMA-2 as our underlying mod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24490;&#29615;&#35780;&#20272;&#39564;&#35777;&#20102;&#35757;&#32451;&#22312;&#31561;&#20215;&#24615;&#36739;&#24046;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#21487;&#33021;&#23548;&#33268;&#26356;&#22810;&#34394;&#26500;&#21644;&#26356;&#24046;&#21484;&#22238;&#30340;&#38382;&#39064;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#25968;&#25454;&#38598;LAGRANGE&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#25552;&#39640;KG&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11669</link><description>&lt;p&gt;
&#22522;&#20110;&#24490;&#29615;&#35780;&#20272;&#30340;&#37197;&#23545;&#30693;&#35782;&#22270;&#35889;-&#25991;&#26412;&#25968;&#25454;&#38598;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic Evaluation. (arXiv:2309.11669v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24490;&#29615;&#35780;&#20272;&#39564;&#35777;&#20102;&#35757;&#32451;&#22312;&#31561;&#20215;&#24615;&#36739;&#24046;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#21487;&#33021;&#23548;&#33268;&#26356;&#22810;&#34394;&#26500;&#21644;&#26356;&#24046;&#21484;&#22238;&#30340;&#38382;&#39064;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#25968;&#25454;&#38598;LAGRANGE&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#25552;&#39640;KG&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#21644;&#25991;&#26412;&#37197;&#23545;&#30340;&#25968;&#25454;&#38598;&#65288;KG-T&#65289;&#21487;&#29992;&#20110;&#35757;&#32451;&#29983;&#25104;KG&#25991;&#26412;&#21644;&#30456;&#21453;&#30340;&#31070;&#32463;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;KG&#21644;&#25991;&#26412;&#37197;&#23545;&#19981;&#31561;&#25928;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#23548;&#33268;&#26356;&#22810;&#30340;&#34394;&#26500;&#21644;&#26356;&#24046;&#30340;&#21484;&#22238;&#12290;&#26412;&#25991;&#36890;&#36807;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#22122;&#22768;&#32423;&#21035;&#30340;&#25968;&#25454;&#38598;&#26469;&#32463;&#39564;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#65292;&#24182;&#21457;&#29616;&#26356;&#22024;&#26434;&#30340;&#25968;&#25454;&#38598;&#30830;&#23454;&#20250;&#23548;&#33268;&#26356;&#22810;&#30340;&#34394;&#26500;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#35757;&#32451;&#22312;&#25968;&#25454;&#38598;&#19978;&#33021;&#22815;&#24490;&#29615;&#29983;&#25104;&#28304;KG&#25110;&#25991;&#26412;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#27169;&#22411;&#30340;&#33021;&#21147;&#26159;&#34913;&#37327;KG&#21644;&#25991;&#26412;&#20043;&#38388;&#31561;&#20215;&#24615;&#30340;&#20195;&#29702;&#12290;&#36890;&#36807;&#24490;&#29615;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#25163;&#21160;&#21019;&#24314;&#30340;WebNLG&#35201;&#27604;&#33258;&#21160;&#21019;&#24314;&#30340;TeKGen&#21644;T-REx&#22909;&#24471;&#22810;&#12290;&#22312;&#36825;&#20123;&#35266;&#23519;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#26088;&#22312;&#25552;&#39640;KG&#21644;&#25991;&#26412;&#31561;&#20215;&#24615;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#25913;&#36827;&#30340;&#25968;&#25454;&#38598;LAGRANGE&#65292;&#24182;&#23637;&#31034;&#20102;&#27599;&#20010;&#21551;&#21457;&#24335;&#26041;&#27861;&#23545;&#24490;&#29615;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used to train forward and reverse neural models that generate text from KG and vice versa. However models trained on datasets where KG and text pairs are not equivalent can suffer from more hallucination and poorer recall. In this paper, we verify this empirically by generating datasets with different levels of noise and find that noisier datasets do indeed lead to more hallucination. We argue that the ability of forward and reverse models trained on a dataset to cyclically regenerate source KG or text is a proxy for the equivalence between the KG and the text in the dataset. Using cyclic evaluation we find that manually created WebNLG is much better than automatically created TeKGen and T-REx. Guided by these observations, we construct a new, improved dataset called LAGRANGE using heuristics meant to improve equivalence between KG and text and show the impact of each of the heuristics on cyclic evaluation. We als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#32763;&#35793;&#27495;&#20041;&#21477;&#23376;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#27495;&#20041;&#25968;&#25454;&#38598;&#24494;&#35843;&#25552;&#20986;&#20102;&#25913;&#36827;&#22788;&#29702;&#27495;&#20041;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22810;&#20010;&#35821;&#35328;&#26041;&#21521;&#19978;&#26377;&#30528;&#19982;&#26368;&#20808;&#36827;&#31995;&#32479;&#30456;&#24403;&#29978;&#33267;&#36229;&#36234;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#30740;&#31350;&#20026;&#26426;&#22120;&#32763;&#35793;&#30340;&#26377;&#25928;&#28040;&#27495;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.11668</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#32763;&#35793;&#28040;&#27495;&#25928;&#26524;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Effective Disambiguation for Machine Translation with Large Language Models. (arXiv:2309.11668v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#32763;&#35793;&#27495;&#20041;&#21477;&#23376;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#27495;&#20041;&#25968;&#25454;&#38598;&#24494;&#35843;&#25552;&#20986;&#20102;&#25913;&#36827;&#22788;&#29702;&#27495;&#20041;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22810;&#20010;&#35821;&#35328;&#26041;&#21521;&#19978;&#26377;&#30528;&#19982;&#26368;&#20808;&#36827;&#31995;&#32479;&#30456;&#24403;&#29978;&#33267;&#36229;&#36234;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#30740;&#31350;&#20026;&#26426;&#22120;&#32763;&#35793;&#30340;&#26377;&#25928;&#28040;&#27495;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#65292;&#35299;&#20915;&#35821;&#20041;&#27495;&#20041;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#26368;&#36817;&#22312;&#27495;&#20041;&#21477;&#23376;&#30340;&#32763;&#35793;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20256;&#32479;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#26292;&#38706;&#20986;&#26469;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#20854;&#20013;&#35768;&#22810;&#24773;&#20917;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#34920;&#29616;&#20986;&#19982;&#20256;&#32479;NMT&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#20102;&#25511;&#21046;&#30446;&#26631;&#36755;&#20986;&#30340;&#26032;&#33539;&#24335;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#32763;&#35793;&#21253;&#21547;&#22810;&#20041;&#35789;&#21644;&#31232;&#26377;&#35789;&#20041;&#30340;&#27495;&#20041;&#21477;&#23376;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#31934;&#24515;&#31574;&#21010;&#30340;&#27495;&#20041;&#25968;&#25454;&#38598;&#24494;&#35843;&#26469;&#25913;&#36827;&#22788;&#29702;&#27492;&#31867;&#27495;&#20041;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20116;&#20010;&#35821;&#35328;&#26041;&#21521;&#20013;&#26377;&#22235;&#20010;&#26041;&#21521;&#33021;&#22815;&#19982;DeepL&#21644;NLLB&#31561;&#26368;&#20808;&#36827;&#31995;&#32479;&#21305;&#25932;&#29978;&#33267;&#36229;&#36234;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026; effective disambiguation for machine translation &#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resolving semantic ambiguity has long been recognised as a central challenge in the field of machine translation. Recent work on benchmarking translation performance on ambiguous sentences has exposed the limitations of conventional Neural Machine Translation (NMT) systems, which fail to capture many of these cases. Large language models (LLMs) have emerged as a promising alternative, demonstrating comparable performance to traditional NMT models while introducing new paradigms for controlling the target outputs. In this paper, we study the capabilities of LLMs to translate ambiguous sentences containing polysemous words and rare word senses. We also propose two ways to improve the handling of such ambiguity through in-context learning and fine-tuning on carefully curated ambiguous datasets. Experiments show that our methods can match or outperform state-of-the-art systems such as DeepL and NLLB in four out of five language directions. Our research provides valuable insights into effec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26469;&#26816;&#27979;&#22312;&#32447;&#38463;&#23572;&#21450;&#21033;&#20122;&#20449;&#24687;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#36890;&#36807;&#23545;&#38463;&#23572;&#21450;&#21033;&#20122;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.11611</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#22312;&#38463;&#23572;&#21450;&#21033;&#20122;&#26041;&#35328;&#20013;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;
&lt;/p&gt;
&lt;p&gt;
Hate speech detection in algerian dialect using deep learning. (arXiv:2309.11611v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26469;&#26816;&#27979;&#22312;&#32447;&#38463;&#23572;&#21450;&#21033;&#20122;&#20449;&#24687;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#36890;&#36807;&#23545;&#38463;&#23572;&#21450;&#21033;&#20122;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#32593;&#32476;&#19978;&#20167;&#24680;&#35328;&#35770;&#20197;&#19981;&#21516;&#30340;&#24418;&#24335;&#34067;&#24310;&#65292;&#22914;&#36785;&#39554;&#35821;&#35328;&#12289;&#32593;&#32476;&#27450;&#20940;&#21644;&#26292;&#21147;&#31561;&#65292;&#20154;&#20204;&#22312;&#26292;&#21147;&#26041;&#38754;&#32463;&#21382;&#20102;&#26174;&#33879;&#22686;&#21152;&#65292;&#20351;&#20182;&#20204;&#22788;&#20110;&#19981;&#36866;&#21644;&#23041;&#32961;&#30340;&#22659;&#22320;&#12290;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20154;&#20204;&#24050;&#32463;&#25237;&#20837;&#22823;&#37327;&#30340;&#21162;&#21147;&#26469;&#20811;&#26381;&#36825;&#19968;&#29616;&#35937;&#65292;&#20197;&#26816;&#27979;&#19981;&#21516;&#32467;&#26500;&#35821;&#35328;&#65288;&#22914;&#33521;&#35821;&#12289;&#27861;&#35821;&#12289;&#38463;&#25289;&#20271;&#35821;&#31561;&#65289;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#65292;&#24182;&#20026;&#38463;&#25289;&#20271;&#26041;&#35328;&#65288;&#22914;&#31361;&#23612;&#26031;&#12289;&#22467;&#21450;&#21644;&#28023;&#28286;&#65289;&#36827;&#34892;&#20102;&#36739;&#23569;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#38463;&#23572;&#21450;&#21033;&#20122;&#20449;&#24687;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#26159;&#20174;&#19968;&#20123;&#38463;&#23572;&#21450;&#21033;&#20122;&#31038;&#20132;&#32593;&#32476;&#65288;Facebook&#12289;YouTube&#21644;Twitter&#65289;&#20013;&#21019;&#24314;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#30340;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;13.5K&#22810;&#31687;&#38463;&#25289;&#20271;&#35821;&#30340;&#38463;&#23572;&#21450;&#21033;&#20122;&#26041;&#35328;&#25991;&#26723;&#65292;&#34987;&#26631;&#35760;&#20026;&#20167;&#24680;&#25110;&#38750;&#20167;&#24680;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#26174;&#31034;&#20986;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the proliferation of hate speech on social networks under different formats, such as abusive language, cyberbullying, and violence, etc., people have experienced a significant increase in violence, putting them in uncomfortable situations and threats. Plenty of efforts have been dedicated in the last few years to overcome this phenomenon to detect hate speech in different structured languages like English, French, Arabic, and others. However, a reduced number of works deal with Arabic dialects like Tunisian, Egyptian, and Gulf, mainly the Algerian ones. To fill in the gap, we propose in this work a complete approach for detecting hate speech on online Algerian messages. Many deep learning architectures have been evaluated on the corpus we created from some Algerian social networks (Facebook, YouTube, and Twitter). This corpus contains more than 13.5K documents in Algerian dialect written in Arabic, labeled as hateful or non-hateful. Promising results are obtained, which show the e
&lt;/p&gt;</description></item><item><title>SpeechAlign&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#27169;&#22411;&#20013;&#28304;&#30446;&#26631;&#23545;&#40784;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#25351;&#26631;&#65292;&#20026;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35775;&#38382;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#24320;&#28304;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.11585</link><description>&lt;p&gt;
SpeechAlign:&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#32763;&#35793;&#23545;&#40784;&#35780;&#20272;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SpeechAlign: a Framework for Speech Translation Alignment Evaluation. (arXiv:2309.11585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11585
&lt;/p&gt;
&lt;p&gt;
SpeechAlign&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#27169;&#22411;&#20013;&#28304;&#30446;&#26631;&#23545;&#40784;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#25351;&#26631;&#65292;&#20026;&#27169;&#22411;&#35780;&#20272;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35775;&#38382;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#24320;&#28304;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#21040;&#35821;&#38899;&#21644;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#30446;&#21069;&#26159;&#30740;&#31350;&#30340;&#21160;&#24577;&#39046;&#22495;&#12290;&#20026;&#20102;&#23545;&#36825;&#20123;&#39046;&#22495;&#20570;&#20986;&#36129;&#29486;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SpeechAlign&#65292;&#19968;&#31181;&#35780;&#20272;&#35821;&#38899;&#27169;&#22411;&#20013;&#28304;&#30446;&#26631;&#23545;&#40784;&#36825;&#19968;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#39046;&#22495;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#20004;&#20010;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#35299;&#20915;&#32570;&#20047;&#21512;&#36866;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Speech Gold Alignment&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#22312;&#33521;&#24503;&#25991;&#26412;&#32763;&#35793;&#30340;&#40644;&#37329;&#23545;&#40784;&#25968;&#25454;&#38598;&#22522;&#30784;&#19978;&#26500;&#24314;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#25351;&#26631;&#65292;&#35821;&#38899;&#23545;&#40784;&#38169;&#35823;&#29575;&#65288;SAER&#65289;&#21644;&#26102;&#38388;&#21152;&#26435;&#35821;&#38899;&#23545;&#40784;&#38169;&#35823;&#29575;&#65288;TW-SAER&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#38899;&#27169;&#22411;&#20013;&#30340;&#23545;&#40784;&#36136;&#37327;&#12290;&#36890;&#36807;&#21457;&#24067;SpeechAlign&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35775;&#38382;&#30340;&#35780;&#20272;&#26694;&#26550;&#29992;&#20110;&#27169;&#22411;&#35780;&#20272;&#65292;&#24182;&#19988;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#23545;&#24320;&#28304;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech-to-Speech and Speech-to-Text translation are currently dynamic areas of research. To contribute to these fields, we present SpeechAlign, a framework to evaluate the underexplored field of source-target alignment in speech models. Our framework has two core components. First, to tackle the absence of suitable evaluation datasets, we introduce the Speech Gold Alignment dataset, built upon a English-German text translation gold alignment dataset. Secondly, we introduce two novel metrics, Speech Alignment Error Rate (SAER) and Time-weighted Speech Alignment Error Rate (TW-SAER), to evaluate alignment quality in speech models. By publishing SpeechAlign we provide an accessible evaluation framework for model assessment, and we employ it to benchmark open-source Speech Translation models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23558;&#21333;&#20363;&#21644;&#22522;&#20110;&#25552;&#21450;&#30340;&#29305;&#24449;&#32435;&#20837;&#20849;&#25351;&#28040;&#35299;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#27169;&#22411;&#22312;OntoGUM&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20998;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#19978;&#22686;&#24378;&#20102;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11582</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#23558;&#21333;&#20363;&#21644;&#22522;&#20110;&#25552;&#21450;&#30340;&#29305;&#24449;&#32435;&#20837;&#20849;&#25351;&#28040;&#35299;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Incorporating Singletons and Mention-based Features in Coreference Resolution via Multi-task Learning for Better Generalization. (arXiv:2309.11582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23558;&#21333;&#20363;&#21644;&#22522;&#20110;&#25552;&#21450;&#30340;&#29305;&#24449;&#32435;&#20837;&#20849;&#25351;&#28040;&#35299;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#27169;&#22411;&#22312;OntoGUM&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20998;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#19978;&#22686;&#24378;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#22312;&#33521;&#35821;&#31471;&#21040;&#31471;&#31070;&#32463;&#20849;&#25351;&#28040;&#35299;&#20013;&#23558;&#25552;&#21450;&#26816;&#27979;&#27493;&#39588;&#32435;&#20837;&#20854;&#20013;&#30340;&#23581;&#35797;&#30001;&#20110;&#32570;&#20047;&#21333;&#20363;&#25552;&#21450;&#27573;&#25968;&#25454;&#20197;&#21450;&#20854;&#20182;&#23454;&#20307;&#20449;&#24687;&#32780;&#21463;&#21040;&#38459;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#25351;&#27169;&#22411;&#65292;&#36890;&#36807;&#22522;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#23398;&#20064;&#21333;&#20363;&#20197;&#21450;&#29305;&#24449;&#65292;&#20363;&#22914;&#23454;&#20307;&#31867;&#22411;&#21644;&#20449;&#24687;&#29366;&#24577;&#12290;&#19982;&#20165;&#36827;&#34892;&#21517;&#35789;&#23545;&#21305;&#37197;&#30456;&#27604;&#65292;&#27492;&#26041;&#27861;&#22312;OntoGUM&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#39640;&#20998;&#65288;+2.7&#20998;&#65289;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#22806;&#25968;&#25454;&#38598;&#19978;&#22686;&#24378;&#20102;&#40065;&#26834;&#24615;&#65288;&#24179;&#22343;&#22686;&#21152;&#20102;2.3&#20998;&#65289;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#25552;&#21450;&#26816;&#27979;&#30340;&#26356;&#22909;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#26356;&#22810;&#21333;&#20363;&#25968;&#25454;&#30340;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous attempts to incorporate a mention detection step into end-to-end neural coreference resolution for English have been hampered by the lack of singleton mention span data as well as other entity information. This paper presents a coreference model that learns singletons as well as features such as entity type and information status via a multi-task learning-based approach. This approach achieves new state-of-the-art scores on the OntoGUM benchmark (+2.7 points) and increases robustness on multiple out-of-domain datasets (+2.3 points on average), likely due to greater generalizability for mention detection and utilization of more data from singletons when compared to only coreferent mention pair matching.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#28145;&#20837;&#35780;&#20272;&#22522;&#20110;&#20869;&#23481;&#21644;&#22522;&#20110;&#19978;&#19979;&#25991;&#27169;&#22411;&#22312;&#26816;&#27979;&#26032;&#30340;&#26410;&#30693;&#35875;&#35328;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21457;&#29616;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#26469;&#28304;&#24086;&#23376;&#20449;&#24687;&#24182;&#24573;&#30053;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#20063;&#25552;&#20986;&#20102;&#20943;&#23567;&#26102;&#38388;&#27010;&#24565;&#24433;&#21709;&#30340;&#23454;&#38469;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2309.11576</link><description>&lt;p&gt;
&#32771;&#23519;&#22522;&#20110;&#38745;&#24577;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#35745;&#31639;&#21270;&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Examining the Limitations of Computational Rumor Detection Models Trained on Static Datasets. (arXiv:2309.11576v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#28145;&#20837;&#35780;&#20272;&#22522;&#20110;&#20869;&#23481;&#21644;&#22522;&#20110;&#19978;&#19979;&#25991;&#27169;&#22411;&#22312;&#26816;&#27979;&#26032;&#30340;&#26410;&#30693;&#35875;&#35328;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21457;&#29616;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#27169;&#22411;&#36807;&#24230;&#20381;&#36182;&#26469;&#28304;&#24086;&#23376;&#20449;&#24687;&#24182;&#24573;&#30053;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#20063;&#25552;&#20986;&#20102;&#20943;&#23567;&#26102;&#38388;&#27010;&#24565;&#24433;&#21709;&#30340;&#23454;&#38469;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#20854;&#27867;&#21270;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#20854;&#33021;&#22815;&#26816;&#27979;&#20986;&#26032;&#20986;&#29616;&#30340;&#12289;&#20197;&#21069;&#26410;&#30693;&#30340;&#35875;&#35328;&#30340;&#33021;&#21147;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#22522;&#20110;&#20869;&#23481;&#65288;&#21363;&#20165;&#20351;&#29992;&#26469;&#28304;&#24086;&#23376;&#20316;&#20026;&#36755;&#20837;&#65289;&#30340;&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#22312;&#26410;&#30693;&#35875;&#35328;&#19978;&#30340;&#34920;&#29616;&#25928;&#26524;&#36739;&#24046;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#27169;&#22411;&#30340;&#28508;&#21147;&#23578;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#28145;&#20837;&#35780;&#20272;&#22522;&#20110;&#20869;&#23481;&#21644;&#22522;&#20110;&#19978;&#19979;&#25991;&#27169;&#22411;&#22312;&#29305;&#21035;&#26159;&#26816;&#27979;&#26032;&#30340;&#26410;&#30693;&#35875;&#35328;&#19978;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#27169;&#22411;&#20173;&#28982;&#36807;&#24230;&#20381;&#36182;&#26469;&#33258;&#35875;&#35328;&#26469;&#28304;&#24086;&#23376;&#30340;&#20449;&#24687;&#65292;&#24182;&#20542;&#21521;&#20110;&#24573;&#30053;&#19978;&#19979;&#25991;&#20449;&#24687;&#21487;&#33021;&#21457;&#25381;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#25968;&#25454;&#25286;&#20998;&#31574;&#30053;&#23545;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#22914;&#20309;&#20943;&#23567;&#26102;&#38388;&#27010;&#24565;&#24433;&#21709;&#30340;&#23454;&#38469;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
A crucial aspect of a rumor detection model is its ability to generalize, particularly its ability to detect emerging, previously unknown rumors. Past research has indicated that content-based (i.e., using solely source posts as input) rumor detection models tend to perform less effectively on unseen rumors. At the same time, the potential of context-based models remains largely untapped. The main contribution of this paper is in the in-depth evaluation of the performance gap between content and context-based models specifically on detecting new, unseen rumors. Our empirical findings demonstrate that context-based models are still overly dependent on the information derived from the rumors' source post and tend to overlook the significant role that contextual information can play. We also study the effect of data split strategies on classifier performance. Based on our experimental results, the paper also offers practical suggestions on how to minimize the effects of temporal concept d
&lt;/p&gt;</description></item><item><title>BTLM-3B-8K&#26159;&#19968;&#20010;30&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;30&#20159;&#21644;70&#20159;&#21442;&#25968;&#27169;&#22411;&#65292;&#23427;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;2-5.5%&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#21516;&#26102;&#22312;&#38271;&#25991;&#26412;&#20219;&#21153;&#19978;&#20063;&#20855;&#26377;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#36825;&#31181;&#23558;70&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#21387;&#32553;&#21040;30&#20159;&#21442;&#25968;&#65292;&#24182;&#19988;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#21463;&#21040;&#24433;&#21709;&#30340;&#26041;&#27861;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.11568</link><description>&lt;p&gt;
BTLM-3B-8K: &#19968;&#20010;3B&#21442;&#25968;&#27169;&#22411;&#20013;&#20351;&#29992;7B&#21442;&#25968;&#24615;&#33021;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model. (arXiv:2309.11568v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11568
&lt;/p&gt;
&lt;p&gt;
BTLM-3B-8K&#26159;&#19968;&#20010;30&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;30&#20159;&#21644;70&#20159;&#21442;&#25968;&#27169;&#22411;&#65292;&#23427;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;2-5.5%&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#21516;&#26102;&#22312;&#38271;&#25991;&#26412;&#20219;&#21153;&#19978;&#20063;&#20855;&#26377;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#36825;&#31181;&#23558;70&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#21387;&#32553;&#21040;30&#20159;&#21442;&#25968;&#65292;&#24182;&#19988;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#21463;&#21040;&#24433;&#21709;&#30340;&#26041;&#27861;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Bittensor&#35821;&#35328;&#27169;&#22411;, &#21517;&#20026;"BTLM-3B-8K", &#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#12289;&#25317;&#26377;30&#20159;&#21442;&#25968;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;. BTLM-3B-8K&#22312;SlimPajama&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#35757;&#32451;&#25968;&#25454;&#20026;627B&#20010;token&#65292;&#37319;&#29992;&#20102;2048&#21644;8192&#30340;&#28151;&#21512;&#19978;&#19979;&#25991;&#38271;&#24230;. BTLM-3B-8K&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#27604;&#25152;&#26377;&#29616;&#26377;&#30340;30&#20159;&#21442;&#25968;&#27169;&#22411;&#25552;&#39640;&#20102;2-5.5% &#65292;&#29978;&#33267;&#19982;&#19968;&#20123;70&#20159;&#21442;&#25968;&#27169;&#22411;&#30456;&#23218;&#32654;. &#21478;&#22806;&#65292;BTLM-3B-8K&#22312;&#38271;&#25991;&#26412;&#19978;&#30340;&#34920;&#29616;&#20063;&#24456;&#22909;&#65292;&#22312;&#38271;&#24230;&#20026;8192&#30340;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;MPT-7B-8K&#21644;XGen-7B-8K. &#25105;&#20204;&#22312;&#28165;&#29702;&#21644;&#21435;&#37325;&#30340;SlimPajama&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#27169;&#22411;&#65292;&#23545;&#181;P&#36229;&#21442;&#25968;&#21644;&#35843;&#24230;&#36827;&#34892;&#20102;&#35843;&#20248;&#65292;&#20351;&#29992;&#20102;ALiBi&#20301;&#32622;&#23884;&#20837;&#21644;SwiGLU&#38750;&#32447;&#24615;. &#22312;Hugging Face&#19978;&#65292;&#26368;&#21463;&#27426;&#36814;&#30340;&#27169;&#22411;&#26159;70&#20159;&#21442;&#25968;&#65292;&#36825;&#34920;&#26126;&#29992;&#25143;&#26356;&#20542;&#21521;&#20110;&#36136;&#37327;&#22823;&#23567;&#27604;&#20026;70&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;. &#23558;70&#20159;&#21442;&#25968;&#27169;&#22411;&#21387;&#32553;&#20026;30&#20159;&#21442;&#25968;&#65292;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#65292;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#37324;&#31243;&#30865;.
&lt;/p&gt;
&lt;p&gt;
We introduce the Bittensor Language Model, called "BTLM-3B-8K", a new state-of-the-art 3 billion parameter open-source language model. BTLM-3B-8K was trained on 627B tokens from the SlimPajama dataset with a mixture of 2,048 and 8,192 context lengths. BTLM-3B-8K outperforms all existing 3B parameter models by 2-5.5% across downstream tasks. BTLM-3B-8K is even competitive with some 7B parameter models. Additionally, BTLM-3B-8K provides excellent long context performance, outperforming MPT-7B-8K and XGen-7B-8K on tasks up to 8,192 context length. We trained the model on a cleaned and deduplicated SlimPajama dataset; aggressively tuned the \textmu P hyperparameters and schedule; used ALiBi position embeddings; and adopted the SwiGLU nonlinearity.  On Hugging Face, the most popular models have 7B parameters, indicating that users prefer the quality-size ratio of 7B models. Compacting the 7B parameter model to one with 3B parameters, with little performance impact, is an important milestone
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SignBank+&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21270;&#25991;&#26412;&#23545;&#25991;&#26412;&#32763;&#35793;&#26041;&#27861;&#65292;&#25552;&#21319;&#20102;&#25163;&#35821;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2309.11566</link><description>&lt;p&gt;
SignBank +&#65306;&#22810;&#35821;&#31181;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SignBank+: Multilingual Sign Language Translation Dataset. (arXiv:2309.11566v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11566
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SignBank+&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#31616;&#21270;&#25991;&#26412;&#23545;&#25991;&#26412;&#32763;&#35793;&#26041;&#27861;&#65292;&#25552;&#21319;&#20102;&#25163;&#35821;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#20851;&#27880;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#31616;&#21270;&#32763;&#35793;&#31995;&#32479;&#65292;&#25512;&#36827;&#25163;&#35821;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SignBank+&#65292;&#36825;&#26159;SignBank&#25968;&#25454;&#38598;&#30340;&#20248;&#21270;&#29256;&#26412;&#65292;&#32463;&#36807;&#28165;&#29702;&#20197;&#36866;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#12290;&#19982;&#20197;&#24448;&#37319;&#29992;&#22797;&#26434;&#30340;&#20998;&#35299;&#25216;&#26415;&#36827;&#34892;&#32763;&#35793;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#20027;&#24352;&#37319;&#29992;&#31616;&#21270;&#30340;&#25991;&#26412;&#23545;&#25991;&#26412;&#32763;&#35793;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22312;SignBank+&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36229;&#36807;&#20102;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work advances the field of sign language machine translation by focusing on dataset quality and simplification of the translation system. We introduce SignBank+, a clean version of the SignBank dataset, optimized for machine translation. Contrary to previous works that employ complex factorization techniques for translation, we advocate for a simplified text-to-text translation approach. Our evaluation shows that models trained on SignBank+ surpass those on the original dataset, establishing a new benchmark and providing an open resource for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#26469;&#33258;&#20154;&#31867;&#35299;&#20915;&#20219;&#21153;&#30340;&#25968;&#25454;&#26469;&#30417;&#30563;&#19968;&#32452;&#38271;&#31243;&#20219;&#21153;&#30340;&#30446;&#26631;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26469;&#25551;&#36848;&#36825;&#20010;&#31354;&#38388;&#65292;&#35813;&#26041;&#27861;&#22312;&#20811;&#38534;&#19987;&#23478;&#34892;&#20026;&#30340;&#20195;&#29702;&#21644;&#26080;&#30417;&#30563;&#23376;&#30446;&#26631;&#31354;&#38388;&#30340;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2309.11564</link><description>&lt;p&gt;
&#24102;&#26377;&#33258;&#28982;&#35821;&#35328;&#23376;&#30446;&#26631;&#30340;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hierarchical reinforcement learning with natural language subgoals. (arXiv:2309.11564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#26469;&#33258;&#20154;&#31867;&#35299;&#20915;&#20219;&#21153;&#30340;&#25968;&#25454;&#26469;&#30417;&#30563;&#19968;&#32452;&#38271;&#31243;&#20219;&#21153;&#30340;&#30446;&#26631;&#31354;&#38388;&#65292;&#24182;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26469;&#25551;&#36848;&#36825;&#20010;&#31354;&#38388;&#65292;&#35813;&#26041;&#27861;&#22312;&#20811;&#38534;&#19987;&#23478;&#34892;&#20026;&#30340;&#20195;&#29702;&#21644;&#26080;&#30417;&#30563;&#23376;&#30446;&#26631;&#31354;&#38388;&#30340;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#19968;&#30452;&#26159;&#19968;&#31181;&#23454;&#29616;&#38271;&#24207;&#21015;&#21160;&#20316;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#25110;&#24320;&#25918;&#29615;&#22659;&#20013;&#23454;&#29616;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20027;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#25214;&#21040;&#36866;&#21512;&#23454;&#20363;&#21270;&#23618;&#27425;&#30340;&#23376;&#30446;&#26631;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#31867;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#30340;&#25968;&#25454;&#65292;&#23545;3D&#36527;&#20307;&#29615;&#22659;&#20013;&#19968;&#32452;&#38271;&#31243;&#20219;&#21153;&#30340;&#30446;&#26631;&#31354;&#38388;&#36827;&#34892;&#36719;&#30417;&#30563;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20351;&#29992;&#38750;&#32422;&#26463;&#30340;&#33258;&#28982;&#35821;&#35328;&#26469;&#21442;&#25968;&#21270;&#36825;&#20010;&#31354;&#38388;&#12290;&#36825;&#26377;&#20004;&#20010;&#20248;&#28857;&#65306;&#39318;&#20808;&#65292;&#21487;&#20197;&#20174;&#22825;&#30495;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#37027;&#37324;&#36731;&#26494;&#29983;&#25104;&#36825;&#20123;&#25968;&#25454;&#65307;&#20854;&#27425;&#65292;&#23427;&#36275;&#22815;&#28789;&#27963;&#65292;&#33021;&#22815;&#34920;&#31034;&#20154;&#31867;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#19968;&#22823;&#33539;&#22260;&#23376;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#20248;&#20110;&#20811;&#38534;&#19987;&#23478;&#34892;&#20026;&#30340;&#20195;&#29702;&#21644;&#27809;&#26377;&#36825;&#31181;&#21463;&#30417;&#30563;&#23376;&#30446;&#26631;&#31354;&#38388;&#30340;&#20174;&#22836;&#24320;&#22987;&#30340;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#19987;&#23478;&#30417;&#30563;&#19982;&#36825;&#31181;&#21463;&#30410;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical reinforcement learning has been a compelling approach for achieving goal directed behavior over long sequences of actions. However, it has been challenging to implement in realistic or open-ended environments. A main challenge has been to find the right space of sub-goals over which to instantiate a hierarchy. We present a novel approach where we use data from humans solving these tasks to softly supervise the goal space for a set of long range tasks in a 3D embodied environment. In particular, we use unconstrained natural language to parameterize this space. This has two advantages: first, it is easy to generate this data from naive human participants; second, it is flexible enough to represent a vast range of sub-goals in human-relevant tasks. Our approach outperforms agents that clone expert behavior on these tasks, as well as HRL from scratch without this supervised sub-goal space. Our work presents a novel approach to combining human expert supervision with the benefi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#35780;&#20998;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22914;&#20309;&#25903;&#25345;&#25945;&#32946;&#24037;&#20316;&#32773;&#39564;&#35777;&#35780;&#20998;&#31243;&#24207;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;LLMs&#20316;&#20026;&#34917;&#20805;&#35270;&#35282;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#20294;&#20173;&#38656;&#36827;&#19968;&#27493;&#20248;&#21270;&#20854;&#21487;&#29992;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.11508</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#30701;&#25991;&#26412;&#31572;&#26696;&#33258;&#21160;&#35780;&#20998;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards LLM-based Autograding for Short Textual Answers. (arXiv:2309.11508v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11508
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#35780;&#20998;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24378;&#35843;&#20102;&#23427;&#20204;&#22914;&#20309;&#25903;&#25345;&#25945;&#32946;&#24037;&#20316;&#32773;&#39564;&#35777;&#35780;&#20998;&#31243;&#24207;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;LLMs&#20316;&#20026;&#34917;&#20805;&#35270;&#35282;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#20294;&#20173;&#38656;&#36827;&#19968;&#27493;&#20248;&#21270;&#20854;&#21487;&#29992;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#35797;&#30340;&#35780;&#20998;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#12289;&#21171;&#21160;&#23494;&#38598;&#30340;&#12289;&#20027;&#35266;&#30340;&#12289;&#37325;&#22797;&#30340;&#19988;&#24120;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#21487;&#29992;&#24615;&#21644;&#25968;&#23383;&#21270;&#24102;&#26469;&#30340;&#22823;&#37327;&#25968;&#25454;&#30340;&#28044;&#20837;&#65292; greatly increased autograding textual responses&#30340;&#21487;&#34892;&#24615;&#12290;&#28982;&#32780;&#65292;&#23558;&#20915;&#31574;&#35282;&#33394;&#20132;&#32473;AI&#27169;&#22411;&#24341;&#36215;&#20102;&#20262;&#29702;&#32771;&#34385;&#65292;&#20027;&#35201;&#28304;&#20110;&#28508;&#22312;&#20559;&#35265;&#21644;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#33258;&#21160;&#35780;&#20998;&#65292;&#21516;&#26102;&#24378;&#35843;&#20102;LLMs&#22914;&#20309;&#25903;&#25345;&#25945;&#32946;&#24037;&#20316;&#32773;&#39564;&#35777;&#20854;&#35780;&#20998;&#31243;&#24207;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#38024;&#23545;&#33258;&#21160;&#30701;&#25991;&#26412;&#31572;&#26696;&#35780;&#20998;&#65288;ASAG&#65289;&#65292;&#28085;&#30422;&#20102;&#20004;&#20010;&#19981;&#21516;&#35838;&#31243;&#30340;&#21508;&#31181;&#35821;&#35328;&#21644;&#32771;&#35797;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#8220;&#24320;&#31665;&#21363;&#29992;&#8221;&#30340;LLMs&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#20379;&#34917;&#20805;&#30340;&#35270;&#35282;&#65292;&#20294;&#23427;&#20204;&#30340;&#21487;&#29992;&#24615;&#21644;&#24615;&#33021;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36824;&#38656;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grading of exams is an important, labor intensive, subjective, repetitive and frequently challenging task. The feasibility of autograding textual responses has greatly increased thanks to the availability of large language models (LLMs) such as ChatGPT and because of the substantial influx of data brought about by digitalization. However, entrusting AI models with decision-making roles raises ethical considerations, mainly stemming from potential biases and issues related to generating false information. Thus, in this manuscript we provide an evaluation of a large language model for the purpose of autograding, while also highlighting how LLMs can support educators in validating their grading procedures. Our evaluation is targeted towards automatic short textual answers grading (ASAG), spanning various languages and examinations from two distinct courses. Our findings suggest that while "out-of-the-box" LLMs provide a valuable tool to provide a complementary perspective, their readiness
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#34920;&#20803;&#25968;&#25454;&#19982;&#19994;&#21153;&#35789;&#27719;&#36827;&#34892;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21305;&#37197;&#21487;&#20197;&#22312;&#19981;&#35831;&#27714;&#35775;&#38382;&#25968;&#25454;&#20869;&#23481;&#20043;&#21069;&#25110;&#20043;&#21518;&#65292;&#26377;&#25928;&#21033;&#29992;&#21487;&#29992;&#30340;&#19994;&#21153;&#35789;&#27719;&#36827;&#34892;&#26816;&#32034;&#21644;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2309.11506</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#34920;&#20803;&#25968;&#25454;&#19982;&#19994;&#21153;&#35789;&#27719;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Matching Table Metadata with Business Glossaries Using Large Language Models. (arXiv:2309.11506v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#34920;&#20803;&#25968;&#25454;&#19982;&#19994;&#21153;&#35789;&#27719;&#36827;&#34892;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#21305;&#37197;&#21487;&#20197;&#22312;&#19981;&#35831;&#27714;&#35775;&#38382;&#25968;&#25454;&#20869;&#23481;&#20043;&#21069;&#25110;&#20043;&#21518;&#65292;&#26377;&#25928;&#21033;&#29992;&#21487;&#29992;&#30340;&#19994;&#21153;&#35789;&#27719;&#36827;&#34892;&#26816;&#32034;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20225;&#19994;&#36890;&#24120;&#25317;&#26377;&#22823;&#37327;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20197;&#22823;&#22411;&#25968;&#25454;&#24211;&#25110;&#20225;&#19994;&#25968;&#25454;&#28246;&#30340;&#24418;&#24335;&#23384;&#22312;&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#24448;&#24448;&#20855;&#26377;&#26377;&#38480;&#30340;&#20803;&#25968;&#25454;&#21644;&#20005;&#26684;&#30340;&#35775;&#38382;&#31574;&#30053;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#23545;&#25968;&#25454;&#20869;&#23481;&#30340;&#35775;&#38382;&#65292;&#24182;&#22240;&#27492;&#38480;&#21046;&#20102;&#32463;&#20856;&#30340;&#26816;&#32034;&#21644;&#20998;&#26512;&#35299;&#20915;&#26041;&#26696;&#30340;&#24212;&#29992;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#21487;&#29992;&#20803;&#25968;&#25454;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#34920;&#20803;&#25968;&#25454;&#19982;&#21253;&#21547;&#25968;&#25454;&#26631;&#31614;&#21644;&#25551;&#36848;&#30340;&#19994;&#21153;&#35789;&#27719;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21305;&#37197;&#65292;&#21487;&#20197;&#22312;&#19981;&#35831;&#27714;&#35775;&#38382;&#25968;&#25454;&#20869;&#23481;&#20043;&#21069;&#25110;&#20043;&#21518;&#65292;&#21033;&#29992;&#21487;&#29992;&#25110;&#31574;&#21010;&#30340;&#19994;&#21153;&#35789;&#27719;&#36827;&#34892;&#26816;&#32034;&#21644;&#20998;&#26512;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#25163;&#21160;&#23450;&#20041;&#30340;&#35268;&#21017;&#25110;&#30456;&#20284;&#24230;&#24230;&#37327;&#22312;&#21015;&#21517;&#21644;&#35789;&#27719;&#25551;&#36848;&#65288;&#25110;&#23427;&#20204;&#30340;&#21521;&#37327;&#23884;&#20837;&#65289;&#20043;&#38388;&#25214;&#21040;&#26368;&#21305;&#37197;&#30340;&#39033;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#36890;&#36807;&#25163;&#21160;&#26631;&#27880;&#36827;&#34892;&#35843;&#25972;&#65292;&#24182;&#19988;&#19981;&#33021;&#22788;&#29702;&#35768;&#22810;&#19994;&#21153;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enterprises often own large collections of structured data in the form of large databases or an enterprise data lake. Such data collections come with limited metadata and strict access policies that could limit access to the data contents and, therefore, limit the application of classic retrieval and analysis solutions. As a result, there is a need for solutions that can effectively utilize the available metadata. In this paper, we study the problem of matching table metadata to a business glossary containing data labels and descriptions. The resulting matching enables the use of an available or curated business glossary for retrieval and analysis without or before requesting access to the data contents. One solution to this problem is to use manually-defined rules or similarity measures on column names and glossary descriptions (or their vector embeddings) to find the closest match. However, such approaches need to be tuned through manual labeling and cannot handle many business gloss
&lt;/p&gt;</description></item><item><title>Text2Reward&#26159;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2309.11489</link><description>&lt;p&gt;
Text2Reward&#65306;&#38024;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11489
&lt;/p&gt;
&lt;p&gt;
Text2Reward&#26159;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#20801;&#35768;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#38271;&#26399;&#20197;&#26469;&#30340;&#25361;&#25112;&#65307;&#23427;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#25110;&#39046;&#22495;&#25968;&#25454;&#65292;&#23548;&#33268;&#24320;&#21457;&#25104;&#26412;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Text2Reward&#65292;&#19968;&#20010;&#26080;&#38656;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#21487;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#29983;&#25104;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#12290;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#30446;&#26631;&#65292;Text2Reward&#29983;&#25104;&#20316;&#20026;&#29615;&#22659;&#32039;&#20945;&#34920;&#31034;&#30340;&#21487;&#25191;&#34892;&#31243;&#24207;&#30340;&#23494;&#38598;&#22870;&#21169;&#20989;&#25968;&#12290;&#19982;&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#26368;&#36817;&#20351;&#29992;LLM&#32534;&#20889;&#31232;&#30095;&#22870;&#21169;&#20195;&#30721;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;Text2Reward&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#12289;&#33258;&#30001;&#24418;&#24335;&#30340;&#23494;&#38598;&#22870;&#21169;&#20195;&#30721;&#65292;&#21487;&#28085;&#30422;&#21508;&#31181;&#20219;&#21153;&#65292;&#21033;&#29992;&#29616;&#26377;&#36719;&#20214;&#21253;&#65292;&#24182;&#20801;&#35768;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26426;&#22120;&#20154;&#25805;&#20316;&#22522;&#20934;&#65288;ManiSkill2&#65292;MetaWorld&#65289;&#21644;&#20004;&#20010;MuJoCo&#30340;&#36816;&#21160;&#29615;&#22659;&#19978;&#35780;&#20272;&#20102;Text2Reward&#12290;&#22312;17&#20010;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;13&#20010;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#29983;&#25104;&#30340;&#22870;&#21169;&#20195;&#30721;&#35757;&#32451;&#30340;&#25919;&#31574;&#23454;&#29616;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing reward functions is a longstanding challenge in reinforcement learning (RL); it requires specialized knowledge or domain data, leading to high costs for development. To address this, we introduce Text2Reward, a data-free framework that automates the generation of dense reward functions based on large language models (LLMs). Given a goal described in natural language, Text2Reward generates dense reward functions as an executable program grounded in a compact representation of the environment. Unlike inverse RL and recent work that uses LLMs to write sparse reward codes, Text2Reward produces interpretable, free-form dense reward codes that cover a wide range of tasks, utilize existing packages, and allow iterative refinement with human feedback. We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2, MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17 manipulation tasks, policies trained with generated reward codes achieve similar or better
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auto-UI&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;API&#30340;&#38656;&#35201;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#20316;&#38142;&#25216;&#26415;&#26469;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2309.11436</link><description>&lt;p&gt;
&#20320;&#20165;&#20851;&#27880;&#23631;&#24149;&#65306;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
You Only Look at Screens: Multimodal Chain-of-Action Agents. (arXiv:2309.11436v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Auto-UI&#30340;&#22810;&#27169;&#24577;&#21160;&#20316;&#38142;&#26426;&#22120;&#20154;&#65292;&#36890;&#36807;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;API&#30340;&#38656;&#35201;&#65292;&#24182;&#24341;&#20837;&#20102;&#21160;&#20316;&#38142;&#25216;&#26415;&#26469;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#26426;&#22120;&#20154;&#26088;&#22312;&#36890;&#36807;&#19982;&#29992;&#25143;&#30028;&#38754;&#36827;&#34892;&#20132;&#20114;&#65292;&#23454;&#29616;&#20219;&#21153;&#33258;&#21160;&#21270;&#65292;&#26080;&#38656;&#25163;&#21160;&#24178;&#39044;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#20197;&#22312;&#22810;&#26679;&#29615;&#22659;&#20013;&#26377;&#25928;&#21442;&#19982;&#12290;&#20026;&#20102;&#31526;&#21512;LLM&#30340;&#36755;&#20837;-&#36755;&#20986;&#35201;&#27714;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#27801;&#30418;&#29615;&#22659;&#20013;&#24320;&#21457;&#65292;&#20381;&#36182;&#20110;&#22806;&#37096;&#24037;&#20855;&#21644;&#24212;&#29992;&#31243;&#24207;&#29305;&#23450;&#30340;API&#23558;&#29615;&#22659;&#35299;&#26512;&#20026;&#25991;&#26412;&#20803;&#32032;&#65292;&#24182;&#35299;&#37322;&#39044;&#27979;&#30340;&#21160;&#20316;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#21463;&#21040;&#25512;&#29702;&#25928;&#29575;&#20302;&#21644;&#38169;&#35823;&#20256;&#25773;&#39118;&#38505;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Auto-UI&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#30452;&#25509;&#19982;&#30028;&#38754;&#20132;&#20114;&#65292;&#36991;&#20813;&#20102;&#23545;&#29615;&#22659;&#35299;&#26512;&#25110;&#20381;&#36182;&#20110;&#24212;&#29992;&#31243;&#24207;&#30456;&#20851;&#30340;API&#30340;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#20316;&#38142;&#25216;&#26415;&#65292;&#21033;&#29992;&#19968;&#31995;&#21015;&#20013;&#38388;&#20808;&#21069;&#21160;&#20316;&#21382;&#21490;&#21644;&#26410;&#26469;&#21160;&#20316;&#35745;&#21010;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous user interface (UI) agents aim to facilitate task automation by interacting with the user interface without manual intervention. Recent studies have investigated eliciting the capabilities of large language models (LLMs) for effective engagement in diverse environments. To align with the input-output requirement of LLMs, existing approaches are developed under a sandbox setting where they rely on external tools and application-specific APIs to parse the environment into textual elements and interpret the predicted actions. Consequently, those approaches often grapple with inference inefficiency and error propagation risks. To mitigate the challenges, we introduce Auto-UI, a multimodal solution that directly interacts with the interface, bypassing the need for environment parsing or reliance on application-dependent APIs. Moreover, we propose a chain-of-action technique -leveraging a series of intermediate previous action histories and future action plans -- to help the age
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20219;&#21153;&#24615;&#33021;&#30340;&#22686;&#24378;&#22411;LLMs&#26694;&#26550;&#65292;&#36890;&#36807;&#36716;&#21270;KG&#30693;&#35782;&#20026;&#25991;&#26412;&#21270;&#38472;&#36848;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#31572;&#26696;&#25935;&#24863;&#30340;KG-to-Text&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.11206</link><description>&lt;p&gt;
&#25552;&#21462;-&#25913;&#20889;-&#22238;&#31572;&#65306;&#19968;&#31181;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#30340;&#22686;&#24378;&#22411;LLMs&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering. (arXiv:2309.11206v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11206
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#20219;&#21153;&#24615;&#33021;&#30340;&#22686;&#24378;&#22411;LLMs&#26694;&#26550;&#65292;&#36890;&#36807;&#36716;&#21270;KG&#30693;&#35782;&#20026;&#25991;&#26412;&#21270;&#38472;&#36848;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#20102;&#23545;&#31572;&#26696;&#25935;&#24863;&#30340;KG-to-Text&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#22312;&#35760;&#24518;&#25152;&#26377;&#19990;&#30028;&#30693;&#35782;&#65292;&#23588;&#20854;&#26159;&#38271;&#23614;&#30693;&#35782;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38656;&#35201;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#30340;&#30693;&#35782;&#22270;&#35889;&#38382;&#31572;&#65288;KGQA&#65289;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26816;&#32034;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20197;&#22686;&#24378;LLMs&#25552;&#31034;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;KGQA&#20013;LLMs&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#26041;&#27861;&#32570;&#20047;&#22522;&#20110;&#25991;&#26412;&#30340;&#21512;&#29702;&#34920;&#36848;KG&#30693;&#35782;&#65292;&#21363;&#24573;&#30053;&#20102;KG&#34920;&#31034;&#21644;&#25991;&#26412;&#34920;&#31034;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#31572;&#26696;&#25935;&#24863;&#30340;KG-to-Text&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;KG&#30693;&#35782;&#36716;&#21270;&#20026;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25991;&#26412;&#21270;&#38472;&#36848;&#65292;&#29992;&#20110;KGQA&#12290;&#22522;&#20110;&#35813;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;KGQA&#20219;&#21153;&#30340;&#22686;&#24378;&#22411;KG-to-Text LLMS&#26694;&#26550;&#12290;&#22312;&#20960;&#20010;KGQA&#22522;&#20934;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;KG-to-Text&#22686;&#24378;LLMs&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their competitive performance on knowledge-intensive tasks, large language models (LLMs) still have limitations in memorizing all world knowledge especially long tail knowledge. In this paper, we study the KG-augmented language model approach for solving the knowledge graph question answering (KGQA) task that requires rich world knowledge. Existing work has shown that retrieving KG knowledge to enhance LLMs prompting can significantly improve LLMs performance in KGQA. However, their approaches lack a well-formed verbalization of KG knowledge, i.e., they ignore the gap between KG representations and textual representations. To this end, we propose an answer-sensitive KG-to-Text approach that can transform KG knowledge into well-textualized statements most informative for KGQA. Based on this approach, we propose a KG-to-Text enhanced LLMs framework for solving the KGQA task. Experiments on several KGQA benchmarks show that the proposed KG-to-Text augmented LLMs approach outperfor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#24179;&#21488;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#35782;&#21035;&#20551;&#26032;&#38395;&#65292;&#21516;&#26102;&#25552;&#20379;&#23454;&#26102;&#20998;&#26512;&#21644;&#39564;&#35777;&#26032;&#38395;&#25991;&#31456;&#30495;&#23454;&#24615;&#30340;&#29992;&#25143;&#21451;&#22909;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2309.11052</link><description>&lt;p&gt;
Fake News BR: &#19968;&#31181;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Fake News BR: A Fake News Detection Platform for Brazilian Portuguese. (arXiv:2309.11052v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#24179;&#21488;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#35782;&#21035;&#20551;&#26032;&#38395;&#65292;&#21516;&#26102;&#25552;&#20379;&#23454;&#26102;&#20998;&#26512;&#21644;&#39564;&#35777;&#26032;&#38395;&#25991;&#31456;&#30495;&#23454;&#24615;&#30340;&#29992;&#25143;&#21451;&#22909;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20551;&#26032;&#38395;&#20256;&#25773;&#35823;&#23548;&#20844;&#20247;&#33286;&#35770;&#30340;&#28508;&#21147;&#65292;&#20854;&#20256;&#25773;&#24050;&#25104;&#20026;&#36817;&#26399;&#20851;&#27880;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#23545;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#20013;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#26032;&#38395;&#31867;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#21253;&#25324;TF-IDF&#21644;Word2Vec&#65292;&#20174;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#20998;&#31867;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#22914;&#36923;&#36753;&#22238;&#24402;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;AdaBoost&#21644;LightGBM&#65292;&#20351;&#29992;&#21253;&#21547;&#30495;&#23454;&#21644;&#20551;&#26032;&#38395;&#25991;&#31456;&#30340;&#25968;&#25454;&#38598;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#29575;&#21644;F1&#24471;&#20998;&#19978;&#37117;&#21462;&#24471;&#20102;&#39640;&#27700;&#24179;&#65292;&#35777;&#26126;&#20102;&#20854;&#35782;&#21035;&#20551;&#26032;&#38395;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#32593;&#31449;&#24179;&#21488;FAKENEWSBR.COM&#65292;&#20197;&#20415;&#39564;&#35777;&#26032;&#38395;&#25991;&#31456;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#30340;&#24179;&#21488;&#25552;&#20379;&#23454;&#26102;&#20998;&#26512;&#65292;&#20801;&#35768;&#29992;&#25143;&#26816;&#26597;&#26032;&#38395;&#25991;&#31456;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of fake news has become a significant concern in recent times due to its potential to spread misinformation and manipulate public opinion. In this paper, we present a comprehensive study on the detection of fake news in Brazilian Portuguese, focusing on journalistic-type news. We propose a machine learning-based approach that leverages natural language processing techniques, including TF-IDF and Word2Vec, to extract features from textual data. We evaluate the performance of various classification algorithms, such as logistic regression, support vector machine, random forest, AdaBoost, and LightGBM, on a dataset containing both true and fake news articles. The proposed approach achieves a high level of accuracy and F1-Score, demonstrating its effectiveness in identifying fake news. Additionally, we develop a user-friendly web platform, FAKENEWSBR.COM, to facilitate the verification of news articles' veracity. Our platform provides real-time analysis, allowing users to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#19978;&#30340;&#33258;&#30001;&#24418;&#24335;&#38382;&#31572;&#65292;&#22312;&#36873;&#21462;&#30456;&#20851;&#30340;&#34920;&#26684;&#21333;&#20803;&#12289;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#12289;&#20197;&#21450;&#25512;&#29702;&#38598;&#25104;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2309.11049</link><description>&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#34920;&#26684;&#36827;&#34892;&#33258;&#30001;&#24418;&#24335;&#38382;&#31572;&#65306;&#23450;&#20301;&#12289;&#26816;&#32034;&#21644;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over Tables. (arXiv:2309.11049v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#34920;&#26684;&#19978;&#30340;&#33258;&#30001;&#24418;&#24335;&#38382;&#31572;&#65292;&#22312;&#36873;&#21462;&#30456;&#20851;&#30340;&#34920;&#26684;&#21333;&#20803;&#12289;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#12289;&#20197;&#21450;&#25512;&#29702;&#38598;&#25104;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#38382;&#31572;&#65288;TableQA&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20542;&#21521;&#20110;&#20174;&#19968;&#20010;&#25110;&#20960;&#20010;&#34920;&#26684;&#21333;&#20803;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#29983;&#25104;&#20107;&#23454;&#24615;&#30340;&#31616;&#30701;&#31572;&#26696;&#65292;&#32780;&#32570;&#20047;&#23545;&#36873;&#23450;&#34920;&#26684;&#21333;&#20803;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#33258;&#30001;&#24418;&#24335;&#30340;TableQA&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#30456;&#20851;&#34920;&#26684;&#21333;&#20803;&#36873;&#25321;&#31574;&#30053;&#21644;&#29420;&#31435;&#20449;&#24687;&#30340;&#22797;&#26434;&#38598;&#25104;&#21644;&#25512;&#29702;&#65292;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#36824;&#19981;&#20805;&#20998;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24191;&#20041;&#30340;&#19977;&#38454;&#27573;&#26041;&#27861;&#65306;&#34920;&#26684;&#21040;&#22270;&#30340;&#36716;&#25442;&#21644;&#21333;&#20803;&#23450;&#20301;&#12289;&#22806;&#37096;&#30693;&#35782;&#26816;&#32034;&#21644;&#34920;&#26684;-&#25991;&#26412;&#34701;&#21512;&#65288;&#31216;&#20026;TAG-QA&#65289;&#65292;&#20197;&#35299;&#20915;&#29983;&#25104;&#24335;TableQA&#20013;&#38024;&#23545;&#38271;&#33258;&#30001;&#24418;&#24335;&#31572;&#26696;&#30340;&#25512;&#29702;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TAG-QA (1) &#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23450;&#20301;&#30456;&#20851;&#34920;&#26684;&#21333;&#20803;&#65292;&#20197;&#25910;&#38598;&#30456;&#20851;&#34892;&#21644;&#21015;&#20043;&#38388;&#30340;&#20132;&#21449;&#21333;&#20803;&#65307;(2) &#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#30340;&#22806;&#37096;&#30693;&#35782;&#65307;(3)...
&lt;/p&gt;
&lt;p&gt;
Question answering on tabular data (TableQA), which aims at generating answers to questions grounded on a given table, has attracted increasing attention in recent years. Existing work tends to generate factual short-form answers by extracting information from one or a few table cells without reasoning over selected table cells. However, the free-form TableQA, requiring a more complex relevant table cell selection strategy and the complex integration and inference of separate pieces of information, has been under-explored. To this end, this paper proposes a generalized three-stage approach: Table-to-Graph conversion and cell localizing, external knowledge retrieval and table-text fusion (called TAG-QA), addressing the challenge of inferring long free-form answer for generative TableQA. In particular, TAG-QA (1) locates relevant table cells using a graph neural network to gather intersecting cells between relevant rows and columns; (2) leverages external knowledge from Wikipedia and (3)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#23545;&#25239;&#23376;&#31354;&#38388;&#25216;&#26415;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#36817;&#37051;&#21644;&#24433;&#21709;&#20989;&#25968;&#30340;&#26816;&#27979;&#22120;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#23376;&#31354;&#38388;&#19982;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#23376;&#31354;&#38388;&#30340;&#20851;&#31995;&#21644;&#20219;&#21153;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.10916</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#34920;&#31034;&#21644;&#24433;&#21709;&#20989;&#25968;&#65292;&#25105;&#20204;&#33021;&#20174;&#23545;&#25239;&#26679;&#26412;&#20013;&#33719;&#24471;&#20160;&#20040;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples. (arXiv:2309.10916v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#23545;&#25239;&#23376;&#31354;&#38388;&#25216;&#26415;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#36817;&#37051;&#21644;&#24433;&#21709;&#20989;&#25968;&#30340;&#26816;&#27979;&#22120;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24433;&#21709;&#20989;&#25968;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#23376;&#31354;&#38388;&#19982;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#23376;&#31354;&#38388;&#30340;&#20851;&#31995;&#21644;&#20219;&#21153;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#26159;&#36890;&#36807;&#24494;&#23567;&#25200;&#21160;&#26469;&#27450;&#39575;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#65292;&#36215;&#21021;&#22312;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#30740;&#31350;&#65292;&#26368;&#36817;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20063;&#24320;&#22987;&#20851;&#27880;&#12290;&#23613;&#31649;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26816;&#27979;&#23545;&#25239;&#26679;&#26412;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#36755;&#20837;&#25200;&#21160;&#30340;&#25628;&#32034;&#65292;&#20294;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#24050;&#32463;&#21457;&#23637;&#20986;&#19968;&#31995;&#21015;&#25216;&#26415;&#26469;&#34920;&#24449;&#23398;&#20064;&#34920;&#31034;&#20013;&#30340;&#23545;&#25239;&#23376;&#31354;&#38388;&#12290;&#26412;&#25991;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#21644;&#24433;&#21709;&#20989;&#25968;&#65292;&#19968;&#31181;&#22522;&#20110;&#39532;&#27663;&#36317;&#31163;&#12290;&#29305;&#21035;&#26159;&#21069;&#32773;&#30456;&#27604;&#20960;&#20010;&#24378;&#22522;&#20934;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#22120;&#65307;&#27492;&#22806;&#65292;&#23545;&#24433;&#21709;&#20989;&#25968;&#30340;&#26032;&#39062;&#20351;&#29992;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#23545;&#25239;&#26679;&#26412;&#23376;&#31354;&#38388;&#19982;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#23376;&#31354;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#26681;&#25454;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples, deliberately crafted using small perturbations to fool deep neural networks, were first studied in image processing and more recently in NLP. While approaches to detecting adversarial examples in NLP have largely relied on search over input perturbations, image processing has seen a range of techniques that aim to characterise adversarial subspaces over the learned representations.  In this paper, we adapt two such approaches to NLP, one based on nearest neighbors and influence functions and one on Mahalanobis distances. The former in particular produces a state-of-the-art detector when compared against several strong baselines; moreover, the novel use of influence functions provides insight into how the nature of adversarial example subspaces in NLP relate to those in image processing, and also how they differ depending on the kind of NLP task.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;L1&#24863;&#30693;&#30340;&#22810;&#35821;&#35328;&#21457;&#38899;&#38169;&#35823;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#40784;&#36755;&#20837;&#38899;&#39057;&#21644;&#21442;&#32771;&#38899;&#32032;&#24207;&#21015;&#65292;&#24182;&#23558;&#39044;&#35757;&#32451;&#30340;&#36741;&#21161;&#27169;&#22411;&#25552;&#21462;&#30340;L1-L2&#35821;&#38899;&#23884;&#20837;&#19982;&#20027;&#35201;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#12290;&#35813;&#26694;&#26550;&#22312;&#33521;&#25991;&#12289;&#38463;&#25289;&#20271;&#35821;&#21644;&#26222;&#36890;&#35805;&#19978;&#30340;&#32479;&#19968;&#22810;&#35821;&#35328;&#38899;&#32032;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.07719</link><description>&lt;p&gt;
L1&#24863;&#30693;&#22810;&#35821;&#35328;&#21457;&#38899;&#38169;&#35823;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
L1-aware Multilingual Mispronunciation Detection Framework. (arXiv:2309.07719v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;L1&#24863;&#30693;&#30340;&#22810;&#35821;&#35328;&#21457;&#38899;&#38169;&#35823;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#40784;&#36755;&#20837;&#38899;&#39057;&#21644;&#21442;&#32771;&#38899;&#32032;&#24207;&#21015;&#65292;&#24182;&#23558;&#39044;&#35757;&#32451;&#30340;&#36741;&#21161;&#27169;&#22411;&#25552;&#21462;&#30340;L1-L2&#35821;&#38899;&#23884;&#20837;&#19982;&#20027;&#35201;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#12290;&#35813;&#26694;&#26550;&#22312;&#33521;&#25991;&#12289;&#38463;&#25289;&#20271;&#35821;&#21644;&#26222;&#36890;&#35805;&#19978;&#30340;&#32479;&#19968;&#22810;&#35821;&#35328;&#38899;&#32032;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#35805;&#32773;&#30340;&#27597;&#35821;(L1)&#21644;&#38750;&#27597;&#35821;(L2)&#20043;&#38388;&#30340;&#35821;&#38899;&#24046;&#24322;&#26159;&#21457;&#38899;&#38169;&#35823;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#35821;&#35328;MDD&#26550;&#26500;&#8212;&#8212;L1-MultiMDD&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;L1&#24863;&#30693;&#30340;&#35821;&#38899;&#34920;&#31034;&#36827;&#34892;&#22686;&#24378;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#36755;&#20837;&#38899;&#39057;&#19982;&#21442;&#32771;&#38899;&#32032;&#24207;&#21015;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#21518;&#65292;&#20174;&#22312;&#22810;&#20219;&#21153;&#35774;&#32622;&#20013;&#39044;&#20808;&#35757;&#32451;&#30340;&#36741;&#21161;&#27169;&#22411;&#20013;&#25552;&#21462;L1-L2&#35821;&#38899;&#23884;&#20837;&#65292;&#24182;&#23558;&#20854;&#19982;&#20027;&#35201;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#36830;&#25509;&#26102;&#24207;&#20998;&#31867;(CTC)&#25439;&#22833;&#20248;&#21270;L1-MultiMDD&#26694;&#26550;&#65292;&#29992;&#20110;&#30446;&#26631;&#35821;&#35328;&#33521;&#25991;&#12289;&#38463;&#25289;&#20271;&#35821;&#21644;&#26222;&#36890;&#35805;&#30340;&#32479;&#19968;&#22810;&#35821;&#35328;&#38899;&#32032;&#35782;&#21035;&#20219;&#21153;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;L1-MultiMDD&#26694;&#26550;&#22312;&#24050;&#35265;&#25968;&#25454;&#38598;L2-ARTIC&#12289;LATIC&#21644;AraVoiceL2&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The phonological discrepancies between a speaker's native (L1) and the non-native language (L2) serves as a major factor for mispronunciation. This paper introduces a novel multilingual MDD architecture, L1-MultiMDD, enriched with L1-aware speech representation. An end-to-end speech encoder is trained on the input signal and its corresponding reference phoneme sequence. First, an attention mechanism is deployed to align the input audio with the reference phoneme sequence. Afterwards, the L1-L2-speech embedding are extracted from an auxiliary model, pretrained in a multi-task setup identifying L1 and L2 language, and are infused with the primary network. Finally, the L1-MultiMDD is then optimized for a unified multilingual phoneme recognition task using connectionist temporal classification (CTC) loss for the target languages: English, Arabic, and Mandarin. Our experiments demonstrate the effectiveness of the proposed L1-MultiMDD framework on both seen -- L2-ARTIC, LATIC, and AraVoiceL2
&lt;/p&gt;</description></item><item><title>CPPF&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#21644;&#21518;&#22788;&#29702;&#26080;&#20851;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#23427;&#25972;&#21512;&#20102;&#22810;&#20010;&#19982;&#35821;&#38899;&#35782;&#21035;&#30456;&#20851;&#30340;ASR&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#27969;&#31243;&#12289;&#36991;&#20813;&#32423;&#32852;&#38169;&#35823;&#20256;&#25773;&#19988;&#35782;&#21035;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.07413</link><description>&lt;p&gt;
CPPF: &#19968;&#31181;&#19978;&#19979;&#25991;&#21644;&#21518;&#22788;&#29702;&#26080;&#20851;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CPPF: A contextual and post-processing-free model for automatic speech recognition. (arXiv:2309.07413v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07413
&lt;/p&gt;
&lt;p&gt;
CPPF&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#21644;&#21518;&#22788;&#29702;&#26080;&#20851;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#23427;&#25972;&#21512;&#20102;&#22810;&#20010;&#19982;&#35821;&#38899;&#35782;&#21035;&#30456;&#20851;&#30340;ASR&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#21270;&#27969;&#31243;&#12289;&#36991;&#20813;&#32423;&#32852;&#38169;&#35823;&#20256;&#25773;&#19988;&#35782;&#21035;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;ASR&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25991;&#26412;&#36755;&#20986;&#36890;&#24120;&#38656;&#35201;&#32463;&#36807;&#21518;&#22788;&#29702;&#20219;&#21153;&#25165;&#33021;&#23454;&#38469;&#21033;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20174;LLMs&#21644;Whisper&#30340;&#22810;&#26041;&#38754;&#33021;&#21147;&#20013;&#33719;&#24471;&#21551;&#21457;&#65292;&#19987;&#27880;&#20110;&#23558;&#19982;&#35821;&#38899;&#35782;&#21035;&#30456;&#20851;&#30340;&#22810;&#20010;ASR&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#25972;&#21512;&#21040;ASR&#27169;&#22411;&#20013;&#12290;&#36825;&#31181;&#25972;&#21512;&#19981;&#20165;&#32553;&#30701;&#20102;&#22810;&#38454;&#27573;&#30340;&#27969;&#31243;&#65292;&#36824;&#38450;&#27490;&#20102;&#32423;&#32852;&#38169;&#35823;&#30340;&#20256;&#25773;&#65292;&#20174;&#32780;&#30452;&#25509;&#29983;&#25104;&#21518;&#22788;&#29702;&#36807;&#30340;&#25991;&#26412;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#19982;ASR&#30456;&#20851;&#30340;&#22788;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;&#19978;&#19979;&#25991;ASR&#21644;&#22810;&#20010;ASR&#21518;&#22788;&#29702;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CPPF&#27169;&#22411;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#21644;&#39640;&#25928;&#30340;ASR&#22788;&#29702;&#26367;&#20195;&#26041;&#26696;&#12290;CPPF&#26080;&#32541;&#22320;&#25972;&#21512;&#20102;&#36825;&#20123;&#20219;&#21153;&#65292;&#32780;&#19988;&#35782;&#21035;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#26126;&#26174;&#30340;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
ASR systems have become increasingly widespread in recent years. However, their textual outputs often require post-processing tasks before they can be practically utilized. To address this issue, we draw inspiration from the multifaceted capabilities of LLMs and Whisper, and focus on integrating multiple ASR text processing tasks related to speech recognition into the ASR model. This integration not only shortens the multi-stage pipeline, but also prevents the propagation of cascading errors, resulting in direct generation of post-processed text. In this study, we focus on ASR-related processing tasks, including Contextual ASR and multiple ASR post processing tasks. To achieve this objective, we introduce the CPPF model, which offers a versatile and highly effective alternative to ASR processing. CPPF seamlessly integrates these tasks without any significant loss in recognition performance.
&lt;/p&gt;</description></item><item><title>&#21019;&#26032;&#28857;&#65306;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#22495;&#36866;&#24212;&#30340;&#26694;&#26550; ConDA&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#26032;&#38395;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#33719;&#21462;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2309.03992</link><description>&lt;p&gt;
ConDA: &#22522;&#20110;&#23545;&#27604;&#22495;&#36866;&#24212;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ConDA: Contrastive Domain Adaptation for AI-generated Text Detection. (arXiv:2309.03992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03992
&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#28857;&#65306;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#22495;&#36866;&#24212;&#30340;&#26694;&#26550; ConDA&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#26032;&#38395;&#25991;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#33719;&#21462;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#22256;&#38590;&#65292;&#36890;&#36807;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#21508;&#31181;&#29992;&#36884;&#30340;&#25991;&#26412;&#29983;&#25104;&#65292;&#21253;&#25324;&#26032;&#38395;&#25253;&#36947;&#12290;&#37492;&#20110;&#36825;&#20123;LLMs&#21487;&#33021;&#34987;&#24694;&#24847;&#20351;&#29992;&#26469;&#22823;&#35268;&#27169;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#24037;&#20855;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#30001;&#20110;&#26032;&#30340;LLMs&#19981;&#26029;&#34987;&#24320;&#21457;&#65292;&#33719;&#21462;&#29992;&#20110;&#30417;&#30563;&#24335;&#26816;&#27979;&#22120;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#25104;&#20026;&#19968;&#20010;&#29942;&#39048;&#12290;&#28982;&#32780;&#65292;&#21487;&#33021;&#23384;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#27809;&#26377;&#20851;&#20110;&#20854;&#29983;&#25104;&#22120;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#27492;&#25968;&#25454;&#38382;&#39064;&#65292;&#21363;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#26032;&#38395;&#25991;&#26412;&#65292;&#24182;&#23558;&#38382;&#39064;&#26694;&#26550;&#21270;&#20026;&#26080;&#30417;&#30563;&#22495;&#36866;&#24212;&#20219;&#21153;&#12290;&#36825;&#37324;&#30340;&#22495;&#26159;&#19981;&#21516;&#30340;&#25991;&#26412;&#29983;&#25104;&#22120;&#65292;&#21363;LLMs&#65292;&#25105;&#20204;&#20551;&#35774;&#21482;&#33021;&#35775;&#38382;&#26631;&#35760;&#30340;&#28304;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;ConDA&#30340;&#23545;&#27604;&#22495;&#36866;&#24212;&#26694;&#26550;&#65292;&#23558;&#26631;&#20934;&#30340;&#22495;&#36866;&#24212;&#25216;&#26415;&#19982;&#34920;&#31034;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly being used for generating text in a variety of use cases, including journalistic news articles. Given the potential malicious nature in which these LLMs can be used to generate disinformation at scale, it is important to build effective detectors for such AI-generated text. Given the surge in development of new LLMs, acquiring labeled training data for supervised detectors is a bottleneck. However, there might be plenty of unlabeled text data available, without information on which generator it came from. In this work we tackle this data problem, in detecting AI-generated news text, and frame the problem as an unsupervised domain adaptation task. Here the domains are the different text generators, i.e. LLMs, and we assume we have access to only the labeled source data and unlabeled target data. We develop a Contrastive Domain Adaptation framework, called ConDA, that blends standard domain adaptation techniques with the representation power 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#25552;&#39640;&#38590;&#27665;&#27861;&#20915;&#31574;&#30340;&#26234;&#33021;&#21270;&#21644;&#36879;&#26126;&#24230;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#20915;&#31574;&#32467;&#26524;&#12290;&#30740;&#31350;&#28041;&#21450;&#26816;&#32034;&#36807;&#21435;&#30340;&#26696;&#20363;&#21644;&#20998;&#26512;&#21152;&#25343;&#22823;&#26696;&#20363;&#25968;&#25454;&#38598;&#30340;&#27861;&#24459;&#20915;&#31574;&#27969;&#31243;&#12290;&#36890;&#36807;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#26032;&#30340;&#22522;&#20934;&#65292;&#30740;&#31350;&#24076;&#26395;&#20026;&#25152;&#26377;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#24102;&#26469;&#21253;&#23481;&#24615;&#21644;&#39044;&#26399;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2308.11531</link><description>&lt;p&gt;
&#25480;&#26435;&#38590;&#27665;&#30003;&#35831;&#32773;&#21450;&#20854;&#24459;&#24072;&#65306;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#38590;&#27665;&#27861;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Empowering Refugee Claimants and their Lawyers: Using Machine Learning to Examine Decision-Making in Refugee Law. (arXiv:2308.11531v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11531
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#25552;&#39640;&#38590;&#27665;&#27861;&#20915;&#31574;&#30340;&#26234;&#33021;&#21270;&#21644;&#36879;&#26126;&#24230;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#20915;&#31574;&#32467;&#26524;&#12290;&#30740;&#31350;&#28041;&#21450;&#26816;&#32034;&#36807;&#21435;&#30340;&#26696;&#20363;&#21644;&#20998;&#26512;&#21152;&#25343;&#22823;&#26696;&#20363;&#25968;&#25454;&#38598;&#30340;&#27861;&#24459;&#20915;&#31574;&#27969;&#31243;&#12290;&#36890;&#36807;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#26032;&#30340;&#22522;&#20934;&#65292;&#30740;&#31350;&#24076;&#26395;&#20026;&#25152;&#26377;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#24102;&#26469;&#21253;&#23481;&#24615;&#21644;&#39044;&#26399;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26234;&#33021;&#26469;&#24110;&#21161;&#21644;&#25903;&#25345;&#38590;&#27665;&#22320;&#20301;&#23457;&#29702;&#30340;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#22914;&#24459;&#24072;&#12289;&#27861;&#23448;&#12289;&#31649;&#29702;&#26426;&#26500;&#21644;&#30003;&#35831;&#20154;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#20570;&#20986;&#20915;&#31574;&#65292;&#24182;&#22686;&#21152;&#38590;&#27665;&#30003;&#35831;&#27969;&#31243;&#30340;&#29702;&#35299;&#21644;&#36879;&#26126;&#24230;&#12290;&#36825;&#20010;&#21338;&#22763;&#39033;&#30446;&#26377;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;&#65306;&#65288;1&#65289;&#26816;&#32034;&#36807;&#21435;&#30340;&#26696;&#20363;&#65292;&#65288;2&#65289;&#22312;&#21152;&#25343;&#22823;&#26696;&#20363;&#25968;&#25454;&#38598;&#19978;&#20998;&#26512;&#27861;&#24459;&#20915;&#31574;&#27969;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#24037;&#20316;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#21253;&#25324;&#23545;&#65288;1&#65289;&#37096;&#20998;&#30340;&#23436;&#25104;&#35797;&#39564;&#21644;&#19982;&#65288;2&#65289;&#37096;&#20998;&#30456;&#20851;&#30340;&#25345;&#32493;&#21162;&#21147;&#12290;&#25105;&#20204;&#30456;&#20449;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#24456;&#36866;&#21512;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#30740;&#31350;&#20102;&#33258;&#21160;&#21270;&#25152;&#28041;&#21450;&#30340;&#25152;&#26377;&#27493;&#39588;&#30340;&#21487;&#34892;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26410;&#26469;&#38590;&#27665;&#27861;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#26032;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#21253;&#23481;&#25152;&#26377;&#26368;&#32456;&#29992;&#25143;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#24182;&#24102;&#26469;&#39044;&#26399;&#30340;&#22909;&#22788;&#65292;&#21253;&#25324;&#20943;&#23569;&#20915;&#31574;&#26102;&#38388;&#65292;&#26356;&#20844;&#24179;&#22320;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our project aims at helping and supporting stakeholders in refugee status adjudications, such as lawyers, judges, governing bodies, and claimants, in order to make better decisions through data-driven intelligence and increase the understanding and transparency of the refugee application process for all involved parties. This PhD project has two primary objectives: (1) to retrieve past cases, and (2) to analyze legal decision-making processes on a dataset of Canadian cases. In this paper, we present the current state of our work, which includes a completed experiment on part (1) and ongoing efforts related to part (2). We believe that NLP-based solutions are well-suited to address these challenges, and we investigate the feasibility of automating all steps involved. In addition, we introduce a novel benchmark for future NLP research in refugee law. Our methodology aims to be inclusive to all end-users and stakeholders, with expected benefits including reduced time-to-decision, fairer a
&lt;/p&gt;</description></item><item><title>LatEval&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;LLMs&#35780;&#20272;&#22522;&#20934;&#65292;&#36890;&#36807;&#20391;&#24605;&#32500;&#35868;&#39064;&#25361;&#25112;&#27169;&#22411;&#30340;&#27178;&#21521;&#24605;&#32771;&#33021;&#21147;&#65292;&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#32771;&#39564;&#27169;&#22411;&#25552;&#20986;&#38382;&#39064;&#30340;&#36136;&#37327;&#21644;&#25972;&#21512;&#20449;&#24687;&#35299;&#20915;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20960;&#20046;&#25152;&#26377;LLMs&#22312;&#27178;&#21521;&#24605;&#32771;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;GPT-4&#27169;&#22411;&#30456;&#27604;&#20154;&#31867;&#20063;&#26377;&#26126;&#26174;&#24046;&#36317;&#12290;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#23545;&#20110;&#26377;&#25928;&#30340;AI&#21161;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.10855</link><description>&lt;p&gt;
LatEval: &#19968;&#20010;&#24102;&#26377;&#26469;&#33258;&#20391;&#24605;&#32500;&#35868;&#39064;&#30340;&#19981;&#23436;&#25972;&#20449;&#24687;&#30340;&#20132;&#20114;&#24335;LLMs&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles. (arXiv:2308.10855v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10855
&lt;/p&gt;
&lt;p&gt;
LatEval&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;LLMs&#35780;&#20272;&#22522;&#20934;&#65292;&#36890;&#36807;&#20391;&#24605;&#32500;&#35868;&#39064;&#25361;&#25112;&#27169;&#22411;&#30340;&#27178;&#21521;&#24605;&#32771;&#33021;&#21147;&#65292;&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#32771;&#39564;&#27169;&#22411;&#25552;&#20986;&#38382;&#39064;&#30340;&#36136;&#37327;&#21644;&#25972;&#21512;&#20449;&#24687;&#35299;&#20915;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20960;&#20046;&#25152;&#26377;LLMs&#22312;&#27178;&#21521;&#24605;&#32771;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;GPT-4&#27169;&#22411;&#30456;&#27604;&#20154;&#31867;&#20063;&#26377;&#26126;&#26174;&#24046;&#36317;&#12290;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#23545;&#20110;&#26377;&#25928;&#30340;AI&#21161;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;LLMs&#30340;&#19981;&#26029;&#21457;&#23637;&#21644;&#25913;&#36827;&#65292;&#23427;&#20204;&#34987;&#36171;&#20104;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36923;&#36753;&#25512;&#29702;&#25110;&#32437;&#21521;&#24605;&#32500;&#33021;&#21147;&#12290;&#20294;&#23427;&#20204;&#33021;&#21542;&#36339;&#20986;&#22266;&#23450;&#27169;&#24335;&#24605;&#32771;&#65311;&#23427;&#20204;&#26159;&#21542;&#20855;&#22791;&#39640;&#36229;&#30340;&#27178;&#21521;&#24605;&#32771;&#33021;&#21147;&#65311;&#22312;&#20391;&#24605;&#32500;&#35868;&#39064;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#31216;&#20026;LatEval&#65292;&#23427;&#22312;&#19968;&#20010;&#20132;&#20114;&#24335;&#26694;&#26550;&#20013;&#35780;&#20272;&#27169;&#22411;&#30340;&#27178;&#21521;&#24605;&#32771;&#33021;&#21147;&#12290;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#21521;LLMs&#25552;&#20986;&#20102;&#20004;&#20010;&#26041;&#38754;&#30340;&#25361;&#25112;&#65306;&#27169;&#22411;&#25552;&#20986;&#30340;&#38382;&#39064;&#30340;&#36136;&#37327;&#21644;&#27169;&#22411;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#25972;&#21512;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#30340;LLMs&#22312;&#20132;&#20114;&#36807;&#31243;&#20013;&#37117;&#24456;&#38590;&#36816;&#29992;&#27178;&#21521;&#24605;&#32771;&#12290;&#20363;&#22914;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;GPT-4&#65292;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#20063;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#19982;&#20154;&#31867;&#30456;&#27604;&#20173;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#36825;&#20010;&#35780;&#20272;&#22522;&#20934;&#20026;LLMs&#25552;&#20379;&#20102;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#21644;&#29420;&#29305;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#21161;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the continuous evolution and refinement of LLMs, they are endowed with impressive logical reasoning or vertical thinking capabilities. But can they think out of the box? Do they possess proficient lateral thinking abilities? Following the setup of Lateral Thinking Puzzles, we propose a novel evaluation benchmark, LatEval, which assesses the model's lateral thinking within an interactive framework. In our benchmark, we challenge LLMs with 2 aspects: the quality of questions posed by the model and the model's capability to integrate information for problem-solving. We find that nearly all LLMs struggle with employing lateral thinking during interactions. For example, even the most advanced model, GPT-4, exhibits the advantage to some extent, yet still maintain a noticeable gap when compared to human. This evaluation benchmark provides LLMs with a highly challenging and distinctive task that is crucial to an effective AI assistant.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2308.10792</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#35843;&#20248;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10792
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#36825;&#19968;&#20851;&#38190;&#25216;&#26415;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#26500;&#24314;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#24212;&#29992;&#65292;&#20197;&#21450;&#23545;&#32467;&#26524;&#24433;&#21709;&#30340;&#20998;&#26512;&#12290;&#21516;&#26102;&#22238;&#39038;&#20102;&#21487;&#33021;&#30340;&#38382;&#39064;&#21644;&#25209;&#35780;&#65292;&#24182;&#25351;&#20986;&#20102;&#30446;&#21069;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#25351;&#20196;&#35843;&#20248;&#65288;IT&#65289;&#36825;&#19968;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#31181;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#21147;&#21644;&#21487;&#25511;&#24615;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#25351;&#20196;&#35843;&#20248;&#26159;&#25351;&#20197;&#30417;&#30563;&#26041;&#24335;&#22312;&#21253;&#21547;&#8220;&#25351;&#20196;-&#36755;&#20986;&#8221;&#23545;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#19968;&#27493;&#35757;&#32451;LLM&#65292;&#36825;&#23558;LLM&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#30446;&#26631;&#19982;&#29992;&#25143;&#24076;&#26395;LLM&#36981;&#23432;&#20154;&#31867;&#25351;&#20196;&#30340;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#23545;IT&#30340;&#24120;&#35268;&#26041;&#27861;&#12289;IT&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#12289;IT&#27169;&#22411;&#30340;&#35757;&#32451;&#20197;&#21450;&#24212;&#29992;&#20110;&#19981;&#21516;&#27169;&#24577;&#12289;&#39046;&#22495;&#21644;&#24212;&#29992;&#30340;&#24773;&#20917;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#23545;&#24433;&#21709;IT&#32467;&#26524;&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#20998;&#26512;&#65288;&#20363;&#22914;&#65292;&#25351;&#20196;&#36755;&#20986;&#30340;&#29983;&#25104;&#12289;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#31561;&#65289;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;IT&#30340;&#28508;&#22312;&#38382;&#39064;&#20197;&#21450;&#38024;&#23545;&#20854;&#30340;&#25209;&#35780;&#65292;&#20197;&#21450;&#25351;&#20986;&#24403;&#21069;&#19981;&#36275;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36136;&#30097;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#23567;&#22411;LLMs&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;HPC&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;Tokompiler&#30340;&#26032;&#22411;&#26631;&#35760;&#22120;&#65292;&#29992;&#20110;&#20195;&#30721;&#39044;&#22788;&#29702;&#21644;&#32534;&#35793;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.09440</link><description>&lt;p&gt;
Scope is all you need: Transforming LLMs for HPC Code (arXiv:2308.09440v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Scope is all you need: Transforming LLMs for HPC Code. (arXiv:2308.09440v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36136;&#30097;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#23567;&#22411;LLMs&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20026;HPC&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;Tokompiler&#30340;&#26032;&#22411;&#26631;&#35760;&#22120;&#65292;&#29992;&#20110;&#20195;&#30721;&#39044;&#22788;&#29702;&#21644;&#32534;&#35793;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24378;&#22823;&#35745;&#31639;&#36164;&#28304;&#30340;&#26356;&#23481;&#26131;&#33719;&#21462;&#65292;AI&#39046;&#22495;&#30340;&#36719;&#20214;&#24320;&#21457;&#36234;&#26469;&#36234;&#20542;&#21521;&#20110;&#24320;&#21457;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#35299;&#20915;&#21508;&#31181;&#32534;&#31243;&#20219;&#21153;&#12290;&#21363;&#20351;&#24212;&#29992;&#20110;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#39046;&#22495;&#30340;LLMs&#20063;&#38750;&#24120;&#24222;&#22823;&#65288;&#20363;&#22914;&#65292;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#65289;&#65292;&#24182;&#19988;&#38656;&#35201;&#26114;&#36149;&#30340;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#20196;&#20154;&#22256;&#24785;-&#20026;&#20160;&#20040;&#25105;&#20204;&#38656;&#35201;&#22312;&#19982;HPC&#19981;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;LLMs&#26469;&#22788;&#29702;HPC&#29305;&#23450;&#20219;&#21153;&#65311;&#22312;&#36825;&#19968;&#31995;&#21015;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36136;&#30097;&#29616;&#26377;LLMs&#25152;&#20570;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#36890;&#36807;&#20026;&#29305;&#23450;&#39046;&#22495;&#24320;&#21457;&#26356;&#23567;&#30340;LLMs-&#25105;&#20204;&#31216;&#20043;&#20026;&#39046;&#22495;&#29305;&#23450;LLMs&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20197;HPC&#20026;&#39046;&#22495;&#24320;&#22987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Tokompiler&#30340;&#26032;&#22411;&#26631;&#35760;&#22120;&#65292;&#19987;&#38376;&#29992;&#20110;HPC&#20013;&#30340;&#20195;&#30721;&#39044;&#22788;&#29702;&#21644;&#32534;&#35793;&#20013;&#24515;&#20219;&#21153;&#12290;Tokompiler&#21033;&#29992;&#35821;&#35328;&#21407;&#35821;&#30340;&#30693;&#35782;&#29983;&#25104;&#38754;&#21521;&#35821;&#35328;&#30340;&#26631;&#35760;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#22788;&#29702;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With easier access to powerful compute resources, there is a growing trend in the field of AI for software development to develop larger and larger language models (LLMs) to address a variety of programming tasks. Even LLMs applied to tasks from the high-performance computing (HPC) domain are huge in size (e.g., billions of parameters) and demand expensive compute resources for training. We found this design choice confusing - why do we need large LLMs trained on natural languages and programming languages unrelated to HPC for HPC-specific tasks? In this line of work, we aim to question design choices made by existing LLMs by developing smaller LLMs for specific domains - we call them domain-specific LLMs. Specifically, we start off with HPC as a domain and propose a novel tokenizer named Tokompiler, designed specifically for preprocessing code in HPC and compilation-centric tasks. Tokompiler leverages knowledge of language primitives to generate language-oriented tokens, providing a c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Whisper&#30340;&#23454;&#26102;&#35821;&#38899;&#36716;&#24405;&#21644;&#32763;&#35793;&#31995;&#32479;Whisper-Streaming&#65292;&#36890;&#36807;&#20351;&#29992;&#26412;&#22320;&#21327;&#35758;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#24310;&#36831;&#65292;&#23454;&#29616;&#20102;&#27969;&#24335;&#36716;&#24405;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;Whisper-Streaming&#22312;&#26410;&#20998;&#21106;&#30340;&#38271;&#31687;&#28436;&#35762;&#36716;&#24405;&#27979;&#35797;&#38598;&#19978;&#20855;&#26377;&#39640;&#36136;&#37327;&#21644;&#20302;&#24310;&#36831;&#65292;&#24182;&#19988;&#22312;&#23454;&#38469;&#22810;&#35821;&#31181;&#20250;&#35758;&#20013;&#30340;&#24212;&#29992;&#20063;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.14743</link><description>&lt;p&gt;
&#23558;Whisper&#36716;&#21270;&#20026;&#23454;&#26102;&#36716;&#24405;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Turning Whisper into Real-Time Transcription System. (arXiv:2307.14743v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;Whisper&#30340;&#23454;&#26102;&#35821;&#38899;&#36716;&#24405;&#21644;&#32763;&#35793;&#31995;&#32479;Whisper-Streaming&#65292;&#36890;&#36807;&#20351;&#29992;&#26412;&#22320;&#21327;&#35758;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#24310;&#36831;&#65292;&#23454;&#29616;&#20102;&#27969;&#24335;&#36716;&#24405;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;Whisper-Streaming&#22312;&#26410;&#20998;&#21106;&#30340;&#38271;&#31687;&#28436;&#35762;&#36716;&#24405;&#27979;&#35797;&#38598;&#19978;&#20855;&#26377;&#39640;&#36136;&#37327;&#21644;&#20302;&#24310;&#36831;&#65292;&#24182;&#19988;&#22312;&#23454;&#38469;&#22810;&#35821;&#31181;&#20250;&#35758;&#20013;&#30340;&#24212;&#29992;&#20063;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Whisper&#26159;&#26368;&#36817;&#30340;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#31181;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#27169;&#22411;&#65292;&#28982;&#32780;&#65292;&#23427;&#24182;&#19981;&#36866;&#29992;&#20110;&#23454;&#26102;&#36716;&#24405;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;Whisper&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;Whisper-Streaming&#65292;&#36825;&#26159;&#19968;&#20010;&#23454;&#26102;&#35821;&#38899;&#36716;&#24405;&#21644;&#32763;&#35793;&#23454;&#29616;Whisper&#31867;&#27169;&#22411;&#30340;&#31995;&#32479;&#12290;Whisper-Streaming&#20351;&#29992;&#26412;&#22320;&#21327;&#35758;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#24310;&#36831;&#65292;&#23454;&#29616;&#20102;&#27969;&#24335;&#36716;&#24405;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Whisper-Streaming&#22312;&#26410;&#20998;&#21106;&#30340;&#38271;&#31687;&#28436;&#35762;&#36716;&#24405;&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;&#39640;&#36136;&#37327;&#21644;3.3&#31186;&#30340;&#24310;&#36831;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20316;&#20026;&#19968;&#20010;&#22810;&#35821;&#31181;&#20250;&#35758;&#23454;&#26102;&#36716;&#24405;&#26381;&#21153;&#32452;&#20214;&#30340;&#40065;&#26834;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whisper is one of the recent state-of-the-art multilingual speech recognition and translation models, however, it is not designed for real time transcription. In this paper, we build on top of Whisper and create Whisper-Streaming, an implementation of real-time speech transcription and translation of Whisper-like models. Whisper-Streaming uses local agreement policy with self-adaptive latency to enable streaming transcription. We show that Whisper-Streaming achieves high quality and 3.3 seconds latency on unsegmented long-form speech transcription test set, and we demonstrate its robustness and practical usability as a component in live transcription service at a multilingual conference.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30495;&#23454;&#23454;&#20307;&#30340;&#26174;&#33879;&#36127;&#38754;&#38472;&#36848;&#30340;&#33021;&#21147;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21542;&#23450;&#38472;&#36848;&#20013;&#30340;&#20107;&#23454;&#27010;&#24565;&#19978;&#20173;&#26377;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.16755</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#26174;&#33879;&#30340;&#36127;&#38754;&#22768;&#26126;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can large language models generate salient negative statements?. (arXiv:2305.16755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30495;&#23454;&#23454;&#20307;&#30340;&#26174;&#33879;&#36127;&#38754;&#38472;&#36848;&#30340;&#33021;&#21147;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21542;&#23450;&#38472;&#36848;&#20013;&#30340;&#20107;&#23454;&#27010;&#24565;&#19978;&#20173;&#26377;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#20851;&#20110;&#29616;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#26174;&#33879;&#65288;&#26377;&#36259;&#30340;&#65289;&#36127;&#38754;&#38472;&#36848;&#30340;&#33021;&#21147;; &#36825;&#26159;&#36807;&#21435;&#20960;&#24180;&#20013;&#28044;&#29616;&#20986;&#30340;&#19968;&#20010;&#30740;&#31350;&#35838;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#38646;&#28857;&#21644;k&#27425;&#26080;&#32422;&#26463;&#25506;&#38024;&#26469;&#25506;&#27979;LLMs&#65292;&#24182;&#19982;&#20256;&#32479;&#30340;&#21542;&#23450;&#29983;&#25104;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#27169;&#24335;&#30340;&#25991;&#26412;&#25552;&#21462;&#21644;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#25512;&#29702;&#20197;&#21450;&#20247;&#21253;&#37329;&#26631;&#35821;&#21477;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#20027;&#39064;&#29983;&#25104;&#21015;&#34920;&#30340;&#27491;&#30830;&#24615;&#21644;&#26174;&#30528;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#26377;&#25351;&#23548;&#30340;&#25506;&#38024;&#30830;&#23454;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#36127;&#38754;&#38472;&#36848;&#30340;&#36136;&#37327;&#65292;&#19982;&#26080;&#25351;&#23548;&#30340;&#21464;&#20307;&#30456;&#27604;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20004;&#20010;&#25552;&#31034;&#65292;LLMs&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#36127;&#38754;&#20107;&#23454;&#30340;&#27010;&#24565;&#65292;&#24120;&#24120;&#29983;&#25104;&#35768;&#22810;&#21547;&#31946;&#19981;&#28165;&#30340;&#38472;&#36848;&#65292;&#25110;&#32773;&#24102;&#26377;&#36127;&#38754;&#20851;&#38190;&#35789;&#20294;&#20855;&#26377;&#31215;&#26497;&#24847;&#20041;&#30340;&#38472;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the ability of large language models (LLMs) to generate salient (interesting) negative statements about real-world entities; an emerging research topic of the last few years. We probe the LLMs using zero- and k-shot unconstrained probes, and compare with traditional methods for negation generation, i.e., pattern-based textual extractions and knowledge-graph-based inferences, as well as crowdsourced gold statements. We measure the correctness and salience of the generated lists about subjects from different domains. Our evaluation shows that guided probes do in fact improve the quality of generated negatives, compared to the zero-shot variant. Nevertheless, using both prompts, LLMs still struggle with the notion of factuality of negatives, frequently generating many ambiguous statements, or statements with negative keywords but a positive meaning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20845;&#20010;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#26080;&#27861;&#19982;&#36739;&#23567;&#30340;&#24494;&#35843;&#22522;&#32447;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2305.14310</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#20998;&#31867;&#20013;&#30340;&#25552;&#31034;&#22797;&#26434;&#24615;&#23548;&#33322;&#65306;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Navigating Prompt Complexity for Zero-Shot Classification: A Study of Large Language Models in Computational Social Science. (arXiv:2305.14310v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20845;&#20010;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#24182;&#30740;&#31350;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#26080;&#27861;&#19982;&#36739;&#23567;&#30340;&#24494;&#35843;&#22522;&#32447;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#19988;&#20855;&#26377;&#26681;&#25454;&#29305;&#23450;&#25552;&#31034;&#29983;&#25104;&#21709;&#24212;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#38656;&#27714;&#65292;&#23427;&#20204;&#30340;&#24212;&#29992;&#36890;&#24120;&#37319;&#29992;&#38646;&#26679;&#26412;&#35774;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;LLM&#65292;ChatGPT&#21644;OpenAssistant&#22312;&#20845;&#20010;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#20998;&#31867;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35843;&#26597;&#20102;&#25552;&#31034;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#22312;&#25552;&#31034;&#20013;&#21152;&#20837;&#26631;&#31614;&#23450;&#20041;&#30340;&#25928;&#26524;&#65307;&#20351;&#29992;&#26631;&#31614;&#21517;&#31216;&#30340;&#21516;&#20041;&#35789;&#65307;&#20197;&#21450;&#22312;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#25972;&#21512;&#36807;&#21435;&#35760;&#24518;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#30446;&#21069;&#30340;LLMs&#26080;&#27861;&#36798;&#21040;&#36739;&#23567;&#30340;&#24494;&#35843;&#22522;&#32447;&#36716;&#25442;&#27169;&#22411;&#65288;&#22914;BERT-large&#65289;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned Large Language Models (LLMs) have exhibited impressive language understanding and the capacity to generate responses that follow specific prompts. However, due to the computational demands associated with training these models, their applications often adopt a zero-shot setting. In this paper, we evaluate the zero-shot performance of two publicly accessible LLMs, ChatGPT and OpenAssistant, in the context of six Computational Social Science classification tasks, while also investigating the effects of various prompting strategies. Our experiments investigate the impact of prompt complexity, including the effect of incorporating label definitions into the prompt; use of synonyms for label names; and the influence of integrating past memories during foundation model training. The findings indicate that in a zero-shot setting, current LLMs are unable to match the performance of smaller, fine-tuned baseline transformer models (such as BERT-large). Additionally, we find tha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#33521;&#22269;&#20445;&#23432;&#20826;&#21644;&#24037;&#20826;&#22823;&#36873;&#23459;&#35328;&#20013;&#30340;&#24773;&#24863;&#35821;&#35328;&#65292;&#24182;&#21457;&#29616;&#29616;&#20219;&#20826;&#27966;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#31215;&#26497;&#24773;&#24863;&#35789;&#27719;&#65292;&#32780;&#21453;&#23545;&#27966;&#20826;&#27966;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#36127;&#38754;&#24773;&#24863;&#35789;&#27719;&#12290;&#21516;&#26102;&#65292;&#24847;&#35782;&#24418;&#24577;&#30456;&#20284;&#30340;&#20826;&#27966;&#20063;&#26356;&#22810;&#22320;&#20351;&#29992;&#31215;&#26497;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2305.08383</link><description>&lt;p&gt;
&#22312;&#25919;&#27835;&#23459;&#35328;&#20013;&#65292;&#29616;&#20219;/&#21453;&#23545;&#27966;&#22320;&#20301;&#21644;&#24847;&#35782;&#24418;&#24577;&#30456;&#20284;&#24230;&#23545;&#24773;&#32490;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Incumbent/Opposition Status and Ideological Similitude on Emotions in Political Manifestos. (arXiv:2305.08383v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#33521;&#22269;&#20445;&#23432;&#20826;&#21644;&#24037;&#20826;&#22823;&#36873;&#23459;&#35328;&#20013;&#30340;&#24773;&#24863;&#35821;&#35328;&#65292;&#24182;&#21457;&#29616;&#29616;&#20219;&#20826;&#27966;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#31215;&#26497;&#24773;&#24863;&#35789;&#27719;&#65292;&#32780;&#21453;&#23545;&#27966;&#20826;&#27966;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#36127;&#38754;&#24773;&#24863;&#35789;&#27719;&#12290;&#21516;&#26102;&#65292;&#24847;&#35782;&#24418;&#24577;&#30456;&#20284;&#30340;&#20826;&#27966;&#20063;&#26356;&#22810;&#22320;&#20351;&#29992;&#31215;&#26497;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#33521;&#22269;&#20445;&#23432;&#20826;&#21644;&#24037;&#20826;2000&#24180;&#33267;2019&#24180;&#30340;&#22823;&#36873;&#23459;&#35328;&#20013;&#19982;&#24773;&#32490;&#30456;&#20851;&#30340;&#35821;&#35328;&#12290;&#34429;&#28982;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#24847;&#35782;&#24418;&#24577;&#23450;&#20301;&#21644;&#20844;&#20849;&#25919;&#31574;&#30340;&#37325;&#21472;&#20043;&#38388;&#23384;&#22312;&#19968;&#33324;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#22312;&#36825;&#31867;&#23459;&#35328;&#20013;&#28041;&#21450;&#24773;&#24863;&#26041;&#38754;&#20173;&#23384;&#22312;&#30683;&#30462;&#30340;&#32467;&#26524;&#12290;&#21033;&#29992;&#26032;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20826;&#27966;&#22320;&#20301;&#23545;&#24773;&#24863;&#20215;&#20540;&#27700;&#24179;&#30340;&#24433;&#21709;&#65292;&#29616;&#20219;&#20826;&#27966;&#21576;&#29616;&#26356;&#22810;&#19982;&#31215;&#26497;&#24773;&#32490;&#30456;&#20851;&#30340;&#35789;&#35821;&#65292;&#32780;&#21453;&#23545;&#27966;&#20826;&#27966;&#20013;&#26356;&#22810;&#36127;&#38754;&#24773;&#32490;&#30456;&#20851;&#30340;&#35789;&#35821;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#24847;&#35782;&#24418;&#24577;&#30456;&#20284;&#30340;&#20826;&#27966;&#26356;&#22810;&#22320;&#20351;&#29992;&#31215;&#26497;&#35821;&#35328;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#24773;&#32490;&#19982;&#20826;&#27966;&#22320;&#20301;&#20043;&#38388;&#20851;&#31995;&#30340;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The study involved the analysis of emotion-associated language in the UK Conservative and Labour party general election manifestos between 2000 to 2019. While previous research have shown a general correlation between ideological positioning and overlap of public policies, there are still conflicting results in matters of sentiments in such manifestos. Using new data, we present how valence level can be swayed by party status within government with incumbent parties presenting a higher frequency in positive emotion-associated words while negative emotion-associated words are more prevalent in opposition parties. We also demonstrate that parties with ideological similitude use positive language prominently further adding to the literature on the relationship between sentiments and party status.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ERNIE-Music&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#27874;&#24418;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#21019;&#26032;&#22320;&#21033;&#29992;&#33258;&#30001;&#24418;&#24335;&#30340;&#25991;&#26412;&#25552;&#31034;&#20316;&#20026;&#26465;&#20214;&#22240;&#32032;&#65292;&#25105;&#20204;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#25991;&#26412;&#21040;&#38899;&#20048;&#27874;&#24418;&#30340;&#29983;&#25104;&#12290;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#36164;&#28304;&#26500;&#24314;&#25968;&#25454;&#38598;&#24182;&#37319;&#29992;&#24369;&#30417;&#30563;&#25216;&#26415;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#26377;&#38480;&#30340;&#25991;&#26412;-&#38899;&#20048;&#24179;&#34892;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#23545;&#27604;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25991;&#26412;&#26465;&#20214;&#26684;&#24335;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#35777;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04456</link><description>&lt;p&gt;
ERNIE-Music: &#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#27874;&#24418;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models. (arXiv:2302.04456v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ERNIE-Music&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#27874;&#24418;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#12290;&#36890;&#36807;&#21019;&#26032;&#22320;&#21033;&#29992;&#33258;&#30001;&#24418;&#24335;&#30340;&#25991;&#26412;&#25552;&#31034;&#20316;&#20026;&#26465;&#20214;&#22240;&#32032;&#65292;&#25105;&#20204;&#25104;&#21151;&#23454;&#29616;&#20102;&#20174;&#25991;&#26412;&#21040;&#38899;&#20048;&#27874;&#24418;&#30340;&#29983;&#25104;&#12290;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#36164;&#28304;&#26500;&#24314;&#25968;&#25454;&#38598;&#24182;&#37319;&#29992;&#24369;&#30417;&#30563;&#25216;&#26415;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#26377;&#38480;&#30340;&#25991;&#26412;-&#38899;&#20048;&#24179;&#34892;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#23545;&#27604;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25991;&#26412;&#26465;&#20214;&#26684;&#24335;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#25193;&#25955;&#27169;&#22411;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#65292;&#36825;&#23548;&#33268;&#20102;&#22270;&#20687;&#21644;&#35821;&#38899;&#29983;&#25104;&#26041;&#38754;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20174;&#26080;&#38480;&#21046;&#30340;&#25991;&#26412;&#25552;&#31034;&#30452;&#25509;&#21512;&#25104;&#38899;&#20048;&#27874;&#24418;&#20173;&#28982;&#26159;&#19968;&#20010;&#30456;&#23545;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#24615;&#36129;&#29486;&#65292;&#21363;&#20197;&#25193;&#25955;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#25991;&#26412;&#21040;&#27874;&#24418;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#23558;&#33258;&#30001;&#24418;&#24335;&#30340;&#25991;&#26412;&#25552;&#31034;&#20316;&#20026;&#26377;&#26465;&#20214;&#30340;&#22240;&#32032;&#65292;&#20197;&#25351;&#23548;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#20869;&#30340;&#27874;&#24418;&#29983;&#25104;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#26377;&#38480;&#30340;&#25991;&#26412;-&#38899;&#20048;&#24179;&#34892;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#36164;&#28304;&#26469;&#21019;&#24314;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#36825;&#19968;&#20219;&#21153;&#24471;&#21040;&#20102;&#24369;&#30417;&#30563;&#25216;&#26415;&#30340;&#24110;&#21161;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#23545;&#27604;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25991;&#26412;&#26465;&#20214;&#26684;&#24335;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#38899;&#20048;&#26631;&#31614;&#21644;&#26080;&#32422;&#26463;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the burgeoning interest in diffusion models has led to significant advances in image and speech generation. Nevertheless, the direct synthesis of music waveforms from unrestricted textual prompts remains a relatively underexplored domain. In response to this lacuna, this paper introduces a pioneering contribution in the form of a text-to-waveform music generation model, underpinned by the utilization of diffusion models. Our methodology hinges on the innovative incorporation of free-form textual prompts as conditional factors to guide the waveform generation process within the diffusion model framework. Addressing the challenge of limited text-music parallel data, we undertake the creation of a dataset by harnessing web resources, a task facilitated by weak supervision techniques. Furthermore, a rigorous empirical inquiry is undertaken to contrast the efficacy of two distinct prompt formats for text conditioning, namely, music tags and unconstrained textual description
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24544;&#23454;&#30340;&#25512;&#29702;&#38142;&#24605;&#32771;&#26694;&#26550;&#65292;&#36890;&#36807;&#32763;&#35793;&#21644;&#38382;&#39064;&#27714;&#35299;&#20004;&#20010;&#38454;&#27573;&#65292;&#30830;&#20445;&#25512;&#29702;&#38142;&#33021;&#24544;&#23454;&#35299;&#37322;&#26368;&#32456;&#31572;&#26696;&#12290;&#23427;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#21319;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36229;&#36807;&#20102;&#26631;&#20934;&#30340;&#25512;&#29702;&#38142;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.13379</link><description>&lt;p&gt;
&#24544;&#23454;&#30340;&#25512;&#29702;&#38142;&#24605;&#32771;
&lt;/p&gt;
&lt;p&gt;
Faithful Chain-of-Thought Reasoning. (arXiv:2301.13379v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13379
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24544;&#23454;&#30340;&#25512;&#29702;&#38142;&#24605;&#32771;&#26694;&#26550;&#65292;&#36890;&#36807;&#32763;&#35793;&#21644;&#38382;&#39064;&#27714;&#35299;&#20004;&#20010;&#38454;&#27573;&#65292;&#30830;&#20445;&#25512;&#29702;&#38142;&#33021;&#24544;&#23454;&#35299;&#37322;&#26368;&#32456;&#31572;&#26696;&#12290;&#23427;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#25552;&#21319;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36229;&#36807;&#20102;&#26631;&#20934;&#30340;&#25512;&#29702;&#38142;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24605;&#32771;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#21487;&#20197;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22312;&#21508;&#31181;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20294;&#29983;&#25104;&#30340;&#25512;&#29702;&#38142;&#19981;&#19968;&#23450;&#21453;&#26144;&#27169;&#22411;&#22914;&#20309;&#24471;&#20986;&#31572;&#26696;&#65288;&#21363;&#24544;&#23454;&#24615;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24544;&#23454;&#30340;CoT&#65292;&#36825;&#26159;&#19968;&#20010;&#28041;&#21450;&#20004;&#20010;&#38454;&#27573;&#30340;&#25512;&#29702;&#26694;&#26550;&#65306;&#32763;&#35793;&#65288;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;$\rightarrow$&#31526;&#21495;&#25512;&#29702;&#38142;&#65289;&#21644;&#38382;&#39064;&#27714;&#35299;&#65288;&#25512;&#29702;&#38142;$\rightarrow$&#31572;&#26696;&#65289;&#65292;&#20998;&#21035;&#20351;&#29992;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#21644;&#19968;&#20010;&#30830;&#23450;&#24615;&#27714;&#35299;&#22120;&#12290;&#36825;&#20445;&#35777;&#20102;&#25512;&#29702;&#38142;&#25552;&#20379;&#20102;&#23545;&#26368;&#32456;&#31572;&#26696;&#30340;&#24544;&#23454;&#35299;&#37322;&#12290;&#38500;&#20102;&#21487;&#35299;&#37322;&#24615;&#22806;&#65292;&#24544;&#23454;&#30340;CoT&#36824;&#25552;&#39640;&#20102;&#32463;&#39564;&#24615;&#33021;&#65306;&#23427;&#22312;&#26469;&#33258;4&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;10&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20248;&#20110;&#26631;&#20934;&#30340;CoT&#65292;&#25968;&#23398;&#38382;&#39064;&#65288;MWP&#65289;&#30340;&#30456;&#23545;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;6.3&#65285;&#65292;&#35268;&#21010;&#25552;&#39640;&#20102;3.4&#65285;&#65292;&#22810;&#36339;&#38382;&#31572;&#65288;QA&#65289;&#25552;&#39640;&#20102;5.5&#65285;&#65292;&#20851;&#31995;&#25512;&#29702;&#25552;&#39640;&#20102;21.4&#65285;&#12290;&#27492;&#22806;&#65292;&#20511;&#21161;GPT-4&#21644;Codex&#65292;&#22312;7&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#26032;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;.
&lt;/p&gt;
&lt;p&gt;
While Chain-of-Thought (CoT) prompting boosts Language Models' (LM) performance on a gamut of complex reasoning tasks, the generated reasoning chain does not necessarily reflect how the model arrives at the answer (aka. faithfulness). We propose Faithful CoT, a reasoning framework involving two stages: Translation (Natural Language query $\rightarrow$ symbolic reasoning chain) and Problem Solving (reasoning chain $\rightarrow$ answer), using an LM and a deterministic solver respectively. This guarantees that the reasoning chain provides a faithful explanation of the final answer. Aside from interpretability, Faithful CoT also improves empirical performance: it outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning, 5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference. Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot performance on 7 dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27719;&#24635;&#20102;65&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ABSA&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;45&#20010;&#33521;&#25991;&#25968;&#25454;&#38598;&#21644;20&#20010;&#20854;&#20182;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#33258;&#20027;ABSA&#31995;&#32479;&#30340;&#25968;&#25454;&#24211;&#12290;</title><link>http://arxiv.org/abs/2204.05232</link><description>&lt;p&gt;
&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey of Aspect-based Sentiment Analysis Datasets. (arXiv:2204.05232v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27719;&#24635;&#20102;65&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ABSA&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;45&#20010;&#33521;&#25991;&#25968;&#25454;&#38598;&#21644;20&#20010;&#20854;&#20182;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#33258;&#20027;ABSA&#31995;&#32479;&#30340;&#25968;&#25454;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;(ABSA)&#26159;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#65292;&#38656;&#35201;&#20998;&#26512;&#29992;&#25143;&#29983;&#25104;&#30340;&#35780;&#35770;&#20197;&#30830;&#23450;&#65306;a)&#27491;&#22312;&#23457;&#26597;&#30340;&#30446;&#26631;&#23454;&#20307;&#65292;b)&#23646;&#20110;&#21738;&#20010;&#39640;&#32423;&#26041;&#38754;&#65292;c)&#23545;&#30446;&#26631;&#21644;&#26041;&#38754;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;ABSA&#30340;&#20247;&#22810;&#20294;&#20998;&#25955;&#30340;&#35821;&#26009;&#24211;&#20351;&#30740;&#31350;&#20154;&#21592;&#24456;&#38590;&#24555;&#36895;&#30830;&#23450;&#26368;&#36866;&#21512;&#29305;&#23450;ABSA&#23376;&#20219;&#21153;&#30340;&#35821;&#26009;&#24211;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#33258;&#20027;ABSA&#31995;&#32479;&#30340;&#25968;&#25454;&#24211;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;ABSA&#21644;&#20854;&#23376;&#20219;&#21153;&#30340;&#20027;&#35201;&#35821;&#26009;&#24211;&#27010;&#36848;&#65292;&#24182;&#24378;&#35843;&#30740;&#31350;&#20154;&#21592;&#22312;&#36873;&#25321;&#35821;&#26009;&#24211;&#26102;&#24212;&#32771;&#34385;&#30340;&#20960;&#20010;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#25910;&#38598;&#26041;&#27861;&#30340;&#20248;&#32570;&#28857;&#24182;&#20026;&#26410;&#26469;&#35821;&#26009;&#24211;&#21019;&#24314;&#25552;&#20986;&#24314;&#35758;&#12290;&#26412;&#35843;&#26597;&#23457;&#26680;&#20102;65&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;ABSA&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;25&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;45&#20010;&#33521;&#35821;&#21644;20&#20010;&#20854;&#20182;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment analysis (ABSA) is a natural language processing problem that requires analyzing user-generated reviews to determine: a) The target entity being reviewed, b) The high-level aspect to which it belongs, and c) The sentiment expressed toward the targets and the aspects. Numerous yet scattered corpora for ABSA make it difficult for researchers to identify corpora best suited for a specific ABSA subtask quickly. This study aims to present a database of corpora that can be used to train and assess autonomous ABSA systems. Additionally, we provide an overview of the major corpora for ABSA and its subtasks and highlight several features that researchers should consider when selecting a corpus. Finally, we discuss the advantages and disadvantages of current collection approaches and make recommendations for future corpora creation. This survey examines 65 publicly available ABSA datasets covering over 25 domains, including 45 English and 20 other languages datasets.
&lt;/p&gt;</description></item><item><title>&#20511;&#21161;&#35821;&#27861;&#32447;&#32034;&#26469;&#30830;&#23450;&#21477;&#23376;&#30340;&#20027;&#35821;&#22312;&#22823;&#22810;&#25968;&#31616;&#21333;&#20174;&#21477;&#20013;&#26159;&#22810;&#20313;&#30340;&#65292;&#36825;&#20010;&#20887;&#20313;&#24615;&#22312;&#36328;&#35821;&#35328;&#20013;&#20063;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#34892;&#20026;&#23454;&#39564;&#21644;&#35745;&#31639;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#20887;&#20313;&#24615;&#30340;&#21457;&#29983;&#39057;&#29575;&#24456;&#39640;&#65292;&#36825;&#26377;&#21161;&#20110;&#25581;&#31034;&#35821;&#27861;&#30340;&#21151;&#33021;&#21644;&#28436;&#21270;&#12290;</title><link>http://arxiv.org/abs/2201.12911</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#24773;&#20917;&#19979;&#65292;&#22823;&#22810;&#25968;&#31616;&#21333;&#20174;&#21477;&#20013;&#30340;&#35821;&#27861;&#32447;&#32034;&#26159;&#22810;&#20313;&#30340;
&lt;/p&gt;
&lt;p&gt;
Grammatical cues to subjecthood are redundant in a majority of simple clauses across languages. (arXiv:2201.12911v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12911
&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#35821;&#27861;&#32447;&#32034;&#26469;&#30830;&#23450;&#21477;&#23376;&#30340;&#20027;&#35821;&#22312;&#22823;&#22810;&#25968;&#31616;&#21333;&#20174;&#21477;&#20013;&#26159;&#22810;&#20313;&#30340;&#65292;&#36825;&#20010;&#20887;&#20313;&#24615;&#22312;&#36328;&#35821;&#35328;&#20013;&#20063;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#34892;&#20026;&#23454;&#39564;&#21644;&#35745;&#31639;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#20887;&#20313;&#24615;&#30340;&#21457;&#29983;&#39057;&#29575;&#24456;&#39640;&#65292;&#36825;&#26377;&#21161;&#20110;&#25581;&#31034;&#35821;&#27861;&#30340;&#21151;&#33021;&#21644;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#65292;&#35821;&#27861;&#32447;&#32034;&#26377;&#26102;&#19982;&#35789;&#20041;&#37325;&#22797;&#12290;&#20363;&#22914;&#65292;&#33521;&#35821;&#30340;&#35789;&#24207;&#35268;&#21017;&#38480;&#21046;&#20102;&#21477;&#23376;&#30340;&#35789;&#24207;&#65292;&#21363;&#20351;&#21487;&#20197;&#20174;&#19990;&#30028;&#30693;&#35782;&#21644;&#21487;&#20449;&#24615;&#25512;&#26029;&#20986;"dog"&#20316;&#20026;&#20027;&#35821;&#65292;"bone"&#20316;&#20026;&#23486;&#35821;&#30340;&#29366;&#24577;&#12290;&#37327;&#21270;&#36825;&#31181;&#20887;&#20313;&#30340;&#21457;&#29983;&#39057;&#29575;&#65292;&#20197;&#21450;&#20887;&#20313;&#27700;&#24179;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#35821;&#35328;&#20013;&#22914;&#20309;&#21464;&#21270;&#65292;&#21487;&#20197;&#25581;&#31034;&#35821;&#27861;&#30340;&#21151;&#33021;&#21644;&#28436;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#33521;&#35821;&#21644;&#20420;&#35821;&#20013;&#36827;&#34892;&#20102;&#19968;&#39033;&#34892;&#20026;&#23454;&#39564;&#65292;&#24182;&#36827;&#34892;&#20102;&#36328;&#35821;&#35328;&#30340;&#35745;&#31639;&#20998;&#26512;&#65292;&#27979;&#37327;&#20174;&#35821;&#26009;&#24211;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#21450;&#29289;&#20174;&#21477;&#20013;&#35821;&#27861;&#32447;&#32034;&#30340;&#20887;&#20313;&#31243;&#24230;&#12290;&#33521;&#35821;&#21644;&#20420;&#35821;&#30340;&#21442;&#19982;&#32773;&#65288;n = 484&#65289;&#34987;&#21576;&#29616;&#30001;&#33258;&#28982;&#21457;&#29983;&#30340;&#21477;&#23376;&#20013;&#25552;&#21462;&#30340;&#20027;&#35821;&#12289;&#21160;&#35789;&#21644;&#23486;&#35821;&#65288;&#20197;&#38543;&#26426;&#39034;&#24207;&#21644;&#21024;&#38500;&#24418;&#24577;&#26631;&#35760;&#65289;&#65292;&#24182;&#34987;&#35201;&#27714;&#30830;&#23450;&#21738;&#20010;&#21517;&#35789;&#26159;&#34892;&#20026;&#30340;&#20027;&#35821;&#12290;&#20004;&#31181;&#35821;&#35328;&#30340;&#20934;&#30830;&#29575;&#37117;&#24456;&#39640;&#65288;&#12316;
&lt;/p&gt;
&lt;p&gt;
Grammatical cues are sometimes redundant with word meanings in natural language. For instance, English word order rules constrain the word order of a sentence like "The dog chewed the bone" even though the status of "dog" as subject and "bone" as object can be inferred from world knowledge and plausibility. Quantifying how often this redundancy occurs, and how the level of redundancy varies across typologically diverse languages, can shed light on the function and evolution of grammar. To that end, we performed a behavioral experiment in English and Russian and a cross-linguistic computational analysis measuring the redundancy of grammatical cues in transitive clauses extracted from corpus text. English and Russian speakers (n=484) were presented with subjects, verbs, and objects (in random order and with morphological markings removed) extracted from naturally occurring sentences and were asked to identify which noun is the subject of the action. Accuracy was high in both languages (~
&lt;/p&gt;</description></item></channel></rss>