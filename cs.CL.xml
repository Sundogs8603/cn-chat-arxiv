<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>Qwen-VL&#26159;&#19968;&#31181;&#20855;&#26377;&#22810;&#21151;&#33021;&#33021;&#21147;&#30340;&#21069;&#27839;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12289;&#38382;&#39064;&#22238;&#31572;&#12289;&#35270;&#35273;&#23450;&#20301;&#21644;&#28789;&#27963;&#20132;&#20114;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#22312;&#25512;&#21160;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2308.12966</link><description>&lt;p&gt;
Qwen-VL: &#19968;&#31181;&#20855;&#26377;&#22810;&#21151;&#33021;&#33021;&#21147;&#30340;&#21069;&#27839;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities. (arXiv:2308.12966v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12966
&lt;/p&gt;
&lt;p&gt;
Qwen-VL&#26159;&#19968;&#31181;&#20855;&#26377;&#22810;&#21151;&#33021;&#33021;&#21147;&#30340;&#21069;&#27839;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12289;&#38382;&#39064;&#22238;&#31572;&#12289;&#35270;&#35273;&#23450;&#20301;&#21644;&#28789;&#27963;&#20132;&#20114;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#22312;&#25512;&#21160;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#21517;&#20026;Qwen-VL&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#24863;&#30693;&#21644;&#29702;&#35299;&#25991;&#26412;&#21644;&#22270;&#20687;&#12290;&#21253;&#25324;Qwen-VL&#21644;Qwen-VL-Chat&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12289;&#38382;&#39064;&#22238;&#31572;&#12289;&#35270;&#35273;&#23450;&#20301;&#21644;&#28789;&#27963;&#20132;&#20114;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#35780;&#20272;&#33539;&#22260;&#28085;&#30422;&#20102;&#38646;&#26679;&#26412;&#23383;&#24149;&#29983;&#25104;&#12289;&#35270;&#35273;&#25110;&#25991;&#26723;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#21644; grounding &#31561;&#21508;&#31181;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Qwen-VL&#27604;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#34920;&#29616;&#26356;&#20248;&#24322;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#35757;&#32451;&#26041;&#27861;&#12289;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#24182;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#25512;&#21160;&#22810;&#27169;&#24577;&#20154;&#24037;&#26234;&#33021;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#20195;&#30721;&#12289;&#28436;&#31034;&#21644;&#27169;&#22411;&#21487;&#20197;&#22312;https://github.com/QwenLM/Qwen-VL&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Qwen-VL series, a set of large-scale vision-language models designed to perceive and understand both text and images. Comprising Qwen-VL and Qwen-VL-Chat, these models exhibit remarkable performance in tasks like image captioning, question answering, visual localization, and flexible interaction. The evaluation covers a wide range of tasks including zero-shot captioning, visual or document visual question answering, and grounding. We demonstrate the Qwen-VL outperforms existing Large Vision Language Models (LVLMs). We present their architecture, training, capabilities, and performance, highlighting their contributions to advancing multimodal artificial intelligence. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.
&lt;/p&gt;</description></item><item><title>Code Llama&#26159;&#19968;&#31995;&#21015;&#29992;&#20110;&#20195;&#30721;&#30340;&#24320;&#25918;&#22522;&#30784;&#27169;&#22411;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#22635;&#20805;&#21151;&#33021;&#65292;&#25903;&#25345;&#22823;&#22411;&#36755;&#20837;&#19978;&#19979;&#25991;&#21644;&#38646;-shot&#25351;&#20196;&#36319;&#36394;&#33021;&#21147;&#12290;&#22312;&#22810;&#20010;&#20195;&#30721;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;Code Llama&#36798;&#21040;&#24320;&#25918;&#27169;&#22411;&#20013;&#26368;&#39640;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;Python&#19987;&#38376;&#21270;&#27169;&#22411;&#22312;&#26576;&#20123;&#27979;&#35797;&#19978;&#36229;&#36234;&#20102;Llama 2&#30340;70B&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.12950</link><description>&lt;p&gt;
Code Llama: &#29992;&#20110;&#20195;&#30721;&#30340;&#24320;&#25918;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Code Llama: Open Foundation Models for Code. (arXiv:2308.12950v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12950
&lt;/p&gt;
&lt;p&gt;
Code Llama&#26159;&#19968;&#31995;&#21015;&#29992;&#20110;&#20195;&#30721;&#30340;&#24320;&#25918;&#22522;&#30784;&#27169;&#22411;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#22635;&#20805;&#21151;&#33021;&#65292;&#25903;&#25345;&#22823;&#22411;&#36755;&#20837;&#19978;&#19979;&#25991;&#21644;&#38646;-shot&#25351;&#20196;&#36319;&#36394;&#33021;&#21147;&#12290;&#22312;&#22810;&#20010;&#20195;&#30721;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;Code Llama&#36798;&#21040;&#24320;&#25918;&#27169;&#22411;&#20013;&#26368;&#39640;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;Python&#19987;&#38376;&#21270;&#27169;&#22411;&#22312;&#26576;&#20123;&#27979;&#35797;&#19978;&#36229;&#36234;&#20102;Llama 2&#30340;70B&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#24067;&#20102;Code Llama&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#22522;&#20110;Llama 2&#30340;&#29992;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24320;&#25918;&#27169;&#22411;&#20013;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22635;&#20805;&#21151;&#33021;&#65292;&#25903;&#25345;&#22823;&#22411;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#33021;&#22815;&#36827;&#34892;&#38646;-shot&#25351;&#20196;&#36319;&#36394;&#32534;&#31243;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20379;&#22810;&#31181;&#29256;&#26412;&#20197;&#35206;&#30422;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#65306;&#22522;&#30784;&#27169;&#22411;&#65288;Code Llama&#65289;&#65292;Python&#19987;&#38376;&#21270;&#27169;&#22411;&#65288;Code Llama-Python&#65289;&#65292;&#20197;&#21450;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;&#65288;Code Llama-Instruct&#65289;&#65292;&#27599;&#20010;&#27169;&#22411;&#21442;&#25968;&#20998;&#21035;&#20026;7B&#12289;13B&#21644;34B&#12290;&#25152;&#26377;&#27169;&#22411;&#37117;&#26159;&#22312;16k&#26631;&#35760;&#24207;&#21015;&#19978;&#35757;&#32451;&#30340;&#65292;&#21487;&#20197;&#25913;&#21892;&#38271;&#24230;&#19981;&#36229;&#36807;100k&#26631;&#35760;&#30340;&#36755;&#20837;&#12290;7B&#21644;13B&#30340;Code Llama&#21644;Code Llama-Instruct&#21464;&#31181;&#20250;&#26681;&#25454;&#21608;&#22260;&#20869;&#23481;&#36827;&#34892;&#22635;&#20805;&#12290;Code Llama&#22312;&#20960;&#20010;&#20195;&#30721;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#24320;&#25918;&#27169;&#22411;&#20013;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;HumanEval&#21644;MBPP&#20998;&#21035;&#36798;&#21040;&#20102;53%&#21644;55%&#30340;&#20998;&#25968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Code Llama-Python 7B&#22312;HumanEval&#21644;MBPP&#19978;&#20248;&#20110;Llama 2 70B&#65292;&#32780;&#25105;&#20204;&#30340;&#25152;&#26377;&#27169;&#22411;&#37117;&#20248;&#20110;&#20854;&#20182;&#20219;&#20309;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every othe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OptiChat&#65292;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#35786;&#26029;&#19981;&#21487;&#34892;&#30340;&#20248;&#21270;&#27169;&#22411;&#12290;&#23427;&#21487;&#20197;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#29702;&#35299;&#21644;&#35299;&#37322;&#19981;&#21487;&#34892;&#30340;&#20248;&#21270;&#27169;&#22411;&#65292;&#26080;&#38656;&#20855;&#22791;&#28145;&#21402;&#30340;&#20248;&#21270;&#32972;&#26223;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2308.12923</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35786;&#26029;&#19981;&#21487;&#34892;&#30340;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Diagnosing Infeasible Optimization Problems Using Large Language Models. (arXiv:2308.12923v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OptiChat&#65292;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#35786;&#26029;&#19981;&#21487;&#34892;&#30340;&#20248;&#21270;&#27169;&#22411;&#12290;&#23427;&#21487;&#20197;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#29702;&#35299;&#21644;&#35299;&#37322;&#19981;&#21487;&#34892;&#30340;&#20248;&#21270;&#27169;&#22411;&#65292;&#26080;&#38656;&#20855;&#22791;&#28145;&#21402;&#30340;&#20248;&#21270;&#32972;&#26223;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#38382;&#39064;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#25968;&#23398;&#20248;&#21270;&#27169;&#22411;&#65292;&#22312;&#32463;&#27982;&#23398;&#12289;&#24037;&#31243;&#23398;&#19982;&#21046;&#36896;&#19994;&#12289;&#20132;&#36890;&#36816;&#36755;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#20248;&#21270;&#27169;&#22411;&#26159;&#22312;&#28385;&#36275;&#19968;&#32452;&#35201;&#27714;&#25110;&#32422;&#26463;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#20570;&#20986;&#26368;&#20339;&#20915;&#31574;&#30340;&#25968;&#23398;&#25277;&#35937;&#12290;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#30340;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#26159;&#24110;&#21161;&#20174;&#19994;&#20154;&#21592;&#29702;&#35299;&#21644;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#27809;&#26377;&#20915;&#31574;&#28385;&#36275;&#25152;&#26377;&#30340;&#32422;&#26463;&#26465;&#20214;&#12290;&#29616;&#26377;&#30340;&#35786;&#26029;&#19981;&#21487;&#34892;&#20248;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#19987;&#23478;&#31995;&#32479;&#65292;&#38656;&#35201;&#22312;&#20248;&#21270;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#30340;&#32972;&#26223;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OptiChat&#65292;&#36825;&#26159;&#19968;&#31181;&#39318;&#21019;&#30340;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#31995;&#32479;&#65292;&#37197;&#22791;&#20102;&#19968;&#20010;&#19982;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#20132;&#20114;&#24335;&#23545;&#35805;&#30340;GUI&#65292;&#29992;&#20110;&#35752;&#35770;&#19981;&#21487;&#34892;&#30340;&#20248;&#21270;&#27169;&#22411;&#12290;OptiChat&#21487;&#20197;&#25552;&#20379;&#23545;&#20248;&#21270;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-making problems can be represented as mathematical optimization models, finding wide applications in fields such as economics, engineering and manufacturing, transportation, and health care. Optimization models are mathematical abstractions of the problem of making the best decision while satisfying a set of requirements or constraints. One of the primary barriers to deploying these models in practice is the challenge of helping practitioners understand and interpret such models, particularly when they are infeasible, meaning no decision satisfies all the constraints. Existing methods for diagnosing infeasible optimization models often rely on expert systems, necessitating significant background knowledge in optimization. In this paper, we introduce OptiChat, a first-of-its-kind natural language-based system equipped with a chatbot GUI for engaging in interactive conversations about infeasible optimization models. OptiChat can provide natural language descriptions of the optim
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#30693;&#35782;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#30340;&#20316;&#29992;&#65292;&#35774;&#35745;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23545;&#40784;&#25506;&#27979;&#22522;&#20934;&#26469;&#26816;&#27979;&#20851;&#38190;&#30340;&#35821;&#35328;&#32452;&#25104;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.12898</link><description>&lt;p&gt;
&#35821;&#35328;&#30693;&#35782;&#33021;&#25913;&#36827;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?. (arXiv:2308.12898v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#30693;&#35782;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#30340;&#20316;&#29992;&#65292;&#35774;&#35745;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23545;&#40784;&#25506;&#27979;&#22522;&#20934;&#26469;&#26816;&#27979;&#20851;&#38190;&#30340;&#35821;&#35328;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23186;&#20307;&#39046;&#22495;&#23545;&#36890;&#36807;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24863;&#30693;&#21644;&#34920;&#36798;&#29289;&#29702;&#19990;&#30028;&#23637;&#29616;&#20986;&#20102;&#24378;&#28872;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#35270;&#35273;-&#35821;&#35328;&#30456;&#20851;&#30340;&#30740;&#31350;&#26159;&#24403;&#21069;&#26368;&#21560;&#24341;&#20154;&#30340;&#35805;&#39064;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20197;&#19979;&#20004;&#20010;&#38382;&#39064;&#30340;&#25506;&#32034;&#38750;&#24120;&#26377;&#38480;&#65306;1&#65289;&#22312;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#26159;&#21542;&#21487;&#20197;&#25552;&#21462;&#20851;&#38190;&#30340;&#35821;&#35328;&#30693;&#35782;&#65288;&#22914;&#35821;&#20041;&#21644;&#21477;&#27861;&#65289;&#65292;2&#65289;&#36825;&#31181;&#35821;&#35328;&#30693;&#35782;&#22914;&#20309;&#24433;&#21709;&#25110;&#22686;&#24378;&#22810;&#27169;&#24577;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#38416;&#26126;&#20840;&#38754;&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#21253;&#25324;&#35821;&#20041;&#34920;&#36798;&#21644;&#21477;&#27861;&#32467;&#26500;&#65292;&#23545;&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#21457;&#24067;&#20102;SNARE&#65292;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#25506;&#27979;&#22522;&#20934;&#65292;&#26469;&#26816;&#27979;&#20851;&#38190;&#30340;&#35821;&#35328;&#32452;&#25104;&#37096;&#20998;&#65292;&#22914;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#21477;&#27861;&#30693;&#35782;&#65292;&#21253;&#21547;&#20102;&#22235;&#20010;&#20219;&#21153;&#65306;&#35821;&#20041;&#32467;&#26500;&#12289;&#21542;&#23450;&#36923;&#36753;&#12289;&#23646;&#24615;&#24402;&#23646;&#21644;&#20851;&#31995;&#32452;&#21512;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;.....
&lt;/p&gt;
&lt;p&gt;
The multimedia community has shown a significant interest in perceiving and representing the physical world with multimodal pretrained neural network models, and among them, the visual-language pertaining (VLP) is, currently, the most captivating topic. However, there have been few endeavors dedicated to the exploration of 1) whether essential linguistic knowledge (e.g., semantics and syntax) can be extracted during VLP, and 2) how such linguistic knowledge impact or enhance the multimodal alignment. In response, here we aim to elucidate the impact of comprehensive linguistic knowledge, including semantic expression and syntactic structure, on multimodal alignment. Specifically, we design and release the SNARE, the first large-scale multimodal alignment probing benchmark, to detect the vital linguistic components, e.g., lexical, semantic, and syntax knowledge, containing four tasks: Semantic structure, Negation logic, Attribute ownership, and Relationship composition. Based on our prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24378;&#35843;&#20102;&#23558;&#25991;&#26723;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#26356;&#25509;&#36817;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#25552;&#20986;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#21450;&#39640;&#25928;&#30340;&#22810;&#39029;&#25991;&#26723;&#34920;&#31034;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#23436;&#25972;&#25991;&#26723;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12896</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26723;&#39029;&#20998;&#31867;&#65306;&#35774;&#35745;&#12289;&#25968;&#25454;&#38598;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Beyond Document Page Classification: Design, Datasets, and Challenges. (arXiv:2308.12896v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#20102;&#23558;&#25991;&#26723;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#26356;&#25509;&#36817;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#25552;&#20986;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#21644;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#21450;&#39640;&#25928;&#30340;&#22810;&#39029;&#25991;&#26723;&#34920;&#31034;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#19981;&#36866;&#29992;&#20110;&#23454;&#38469;&#23436;&#25972;&#25991;&#26723;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#20102;&#23558;&#25991;&#26723;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#26356;&#25509;&#36817;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#30340;&#38656;&#27714;&#65292;&#21363;&#22312;&#27979;&#35797;&#25968;&#25454;&#30340;&#24615;&#36136;&#19978;&#65288;$X$&#65306;&#22810;&#36890;&#36947;&#12289;&#22810;&#39029;&#12289;&#22810;&#34892;&#19994;&#65307;$Y$&#65306;&#31867;&#21035;&#20998;&#24067;&#21644;&#26631;&#31614;&#38598;&#30340;&#22810;&#26679;&#24615;&#65289;&#21644;&#32771;&#34385;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#65288;$f$&#65306;&#22810;&#39029;&#25991;&#26723;&#12289;&#39029;&#38754;&#27969;&#21644;&#25991;&#26723;&#25414;&#32465;&#20998;&#31867;&#65292;...&#65289;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20844;&#20849;&#30340;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#32570;&#20047;&#65292;&#24182;&#35268;&#33539;&#20102;&#24212;&#29992;&#22330;&#26223;&#20013;&#20135;&#29983;&#30340;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#28608;&#21457;&#20102;&#20197;&#39640;&#25928;&#30340;&#22810;&#39029;&#25991;&#26723;&#34920;&#31034;&#20026;&#30446;&#26631;&#30340;&#20215;&#20540;&#12290;&#23545;&#25552;&#20986;&#30340;&#22810;&#39029;&#25991;&#26723;&#20998;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#24050;&#32463;&#21464;&#24471;&#26080;&#20851;&#32039;&#35201;&#65292;&#24182;&#38656;&#35201;&#26356;&#26032;&#20197;&#35780;&#20272;&#23454;&#38469;&#20013;&#33258;&#28982;&#21457;&#29983;&#30340;&#23436;&#25972;&#25991;&#26723;&#12290;&#36825;&#20010;&#29616;&#23454;&#24773;&#20917;&#26816;&#26597;&#20063;&#21628;&#21505;&#26356;&#25104;&#29087;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#28085;&#30422;&#26657;&#20934;&#35780;&#20272;&#12289;&#25512;&#29702;&#22797;&#26434;&#24615;&#65288;&#26102;&#38388;-&#20869;&#23384;&#65289;&#21644;&#19968;&#31995;&#21015;&#29616;&#23454;&#20998;&#25955;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper highlights the need to bring document classification benchmarking closer to real-world applications, both in the nature of data tested ($X$: multi-channel, multi-paged, multi-industry; $Y$: class distributions and label set variety) and in classification tasks considered ($f$: multi-page document, page stream, and document bundle classification, ...). We identify the lack of public multi-page document classification datasets, formalize different classification tasks arising in application scenarios, and motivate the value of targeting efficient multi-page document representations. An experimental study on proposed multi-page document classification datasets demonstrates that current benchmarks have become irrelevant and need to be updated to evaluate complete documents, as they naturally occur in practice. This reality check also calls for more mature evaluation methodologies, covering calibration evaluation, inference complexity (time-memory), and a range of realistic distr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#25237;&#31080;&#25552;&#31034;(MVP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;(FSL)&#29615;&#22659;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;MVP&#36890;&#36807;&#25552;&#31034;&#22810;&#20010;LLMs&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#36755;&#20986;&#36827;&#34892;&#22810;&#25968;&#25237;&#31080;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#32597;&#35265;&#30149;&#30340;&#35782;&#21035;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.12890</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25237;&#31080;&#65306;&#29992;&#20110;&#32597;&#35265;&#30149;&#35782;&#21035;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Vote: Prompting for Rare Disease Identification. (arXiv:2308.12890v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#27169;&#22411;&#25237;&#31080;&#25552;&#31034;(MVP)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#21892;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;(FSL)&#29615;&#22659;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26597;&#35810;&#24615;&#33021;&#12290;MVP&#36890;&#36807;&#25552;&#31034;&#22810;&#20010;LLMs&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#36755;&#20986;&#36827;&#34892;&#22810;&#25968;&#25237;&#31080;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#32597;&#35265;&#30149;&#30340;&#35782;&#21035;&#21644;&#20998;&#31867;&#20219;&#21153;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#24378;&#35843;&#20102;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#30340;&#38656;&#27714;&#12290;LLMs&#32463;&#24120;&#24212;&#29992;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;(FSL)&#30340;&#24773;&#22659;&#20013;&#65292;&#36825;&#37324;&#20219;&#21153;&#21482;&#20351;&#29992;&#24456;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#25191;&#34892;&#12290;FSL&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;(AI)&#23376;&#39046;&#22495;&#20013;&#21464;&#24471;&#27969;&#34892;&#65292;&#21253;&#25324;&#29992;&#20110;&#20581;&#24247;&#30340;AI&#12290;&#32597;&#35265;&#30149;&#24433;&#21709;&#20154;&#21475;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#22312;&#25968;&#25454;&#21487;&#29992;&#24615;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979; inherently &#38656;&#35201;FSL&#25216;&#26415;&#65292;&#23613;&#31649;&#20154;&#24037;&#25968;&#25454;&#25910;&#38598;&#21644;&#26631;&#27880;&#36153;&#26102;&#36153;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#22411;&#25237;&#31080;&#25552;&#31034;(MVP)&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;FSL&#29615;&#22659;&#20013;LLM&#26597;&#35810;&#24615;&#33021;&#30340;&#28789;&#27963;&#25552;&#31034;&#26041;&#27861;&#12290;MVP&#36890;&#36807;&#25552;&#31034;&#22810;&#20010;LLMs&#25191;&#34892;&#30456;&#21516;&#30340;&#20219;&#21153;&#65292;&#28982;&#21518;&#23545;&#29983;&#25104;&#30340;&#36755;&#20986;&#36827;&#34892;&#22810;&#25968;&#25237;&#31080;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#22312;&#21333;&#27425;&#32597;&#35265;&#30149;&#35782;&#21035;&#21644;&#20998;&#31867;&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#20219;&#20309;&#21333;&#20010;&#27169;&#22411;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32597;&#35265;&#30149;&#25968;&#25454;&#38598;&#29992;&#20110;FSL&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of generative Large Language Models (LLMs) emphasizes the need for accurate and efficient prompting approaches. LLMs are often applied in Few-Shot Learning (FSL) contexts, where tasks are executed with minimal training data. FSL has become popular in many Artificial Intelligence (AI) subdomains, including AI for health. Rare diseases, affecting a small fraction of the population, inherently require FSL techniques due to limited data availability, though manual data collection and annotation is costly and time-consuming. In this paper, we propose Models-Vote Prompting (MVP), a flexible prompting approach for improving the performance of LLM queries in FSL settings. MVP works by prompting numerous LLMs to perform the same tasks and then conducting a majority vote on the resulting outputs. This method achieves improved results to any one model in the ensemble on one-shot rare disease identification and classification tasks. We also release a novel rare disease dataset for FS
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#26469;&#35825;&#23548;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#22240;&#26524;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#20851;&#31995;&#65292;&#21487;&#20197;&#32531;&#35299;&#35757;&#32451;&#25968;&#25454;&#20013;&#35821;&#35328;&#20808;&#39564;&#30340;&#24178;&#25200;&#65292;&#36827;&#32780;&#25552;&#39640;&#25688;&#35201;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#32773;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#22240;&#26524;&#21551;&#21457;&#24335;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#26469;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#65292;&#20197;&#25351;&#23548;&#25688;&#35201;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.12888</link><description>&lt;p&gt;
&#35825;&#23548;&#22240;&#26524;&#32467;&#26500;&#36827;&#34892;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Inducing Causal Structure for Abstractive Text Summarization. (arXiv:2308.12888v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#26469;&#35825;&#23548;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#22240;&#26524;&#32467;&#26500;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#24341;&#20837;&#22240;&#26524;&#20851;&#31995;&#65292;&#21487;&#20197;&#32531;&#35299;&#35757;&#32451;&#25968;&#25454;&#20013;&#35821;&#35328;&#20808;&#39564;&#30340;&#24178;&#25200;&#65292;&#36827;&#32780;&#25552;&#39640;&#25688;&#35201;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#32773;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#22240;&#26524;&#21551;&#21457;&#24335;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#26469;&#23398;&#20064;&#22240;&#26524;&#34920;&#31034;&#65292;&#20197;&#25351;&#23548;&#25688;&#35201;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#20027;&#35201;&#25506;&#32034;&#30456;&#20851;&#24615;&#32780;&#38750;&#22240;&#26524;&#20851;&#31995;&#12290;&#22312;&#36825;&#20123;&#30456;&#20851;&#24615;&#20013;&#65292;&#21487;&#33021;&#23384;&#22312;&#21463;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30340;&#35821;&#35328;&#20808;&#39564;&#24178;&#25200;&#30340;&#20266;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#21066;&#24369;&#20102;&#23398;&#20064;&#27169;&#22411;&#30340;&#25972;&#20307;&#25928;&#26524;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32467;&#26500;&#24615;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#26469;&#35825;&#23548;&#25688;&#35201;&#25968;&#25454;&#30340;&#28508;&#22312;&#22240;&#26524;&#32467;&#26500;&#12290;&#25105;&#20204;&#20551;&#35774;&#23384;&#22312;&#20960;&#20010;&#28508;&#22312;&#30340;&#22240;&#26524;&#22240;&#32032;&#21644;&#38750;&#22240;&#26524;&#22240;&#32032;&#65292;&#20998;&#21035;&#34920;&#31034;&#25991;&#26723;&#21644;&#25688;&#35201;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;SCM&#20013;&#30340;&#28508;&#22312;&#22240;&#32032;&#21487;&#20197;&#36890;&#36807;&#25311;&#21512;&#35266;&#23519;&#21040;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#35782;&#21035;&#20986;&#26469;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22240;&#26524;&#21551;&#21457;&#24335;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65288;CI-Seq2Seq&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#33021;&#22815;&#27169;&#25311;&#22240;&#26524;&#22240;&#32032;&#30340;&#22240;&#26524;&#34920;&#31034;&#65292;&#25351;&#23548;&#25105;&#20204;&#33719;&#21462;&#29992;&#20110;&#29983;&#25104;&#25688;&#35201;&#30340;&#22240;&#26524;&#20449;&#24687;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#37325;&#26032;&#26500;&#36896;Va&#27169;&#22411;&#65292;&#20197;&#24341;&#20837;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mainstream of data-driven abstractive summarization models tends to explore the correlations rather than the causal relationships. Among such correlations, there can be spurious ones which suffer from the language prior learned from the training corpus and therefore undermine the overall effectiveness of the learned model. To tackle this issue, we introduce a Structural Causal Model (SCM) to induce the underlying causal structure of the summarization data. We assume several latent causal factors and non-causal factors, representing the content and style of the document and summary. Theoretically, we prove that the latent factors in our SCM can be identified by fitting the observed training data under certain conditions. On the basis of this, we propose a Causality Inspired Sequence-to-Sequence model (CI-Seq2Seq) to learn the causal representations that can mimic the causal factors, guiding us to pursue causal information for summary generation. The key idea is to reformulate the Va
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DS4DH&#22312;#SMM4H 2023&#20013;&#24320;&#21457;&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;&#31995;&#32479;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#21477;&#23376;&#36716;&#25442;&#21644;&#20498;&#25968;&#25490;&#21517;&#34701;&#21512;&#36827;&#34892;&#38646;&#26679;&#26412;&#35268;&#33539;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20849;&#20139;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#26377;&#25928;&#24212;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25366;&#25496;&#20013;&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.12877</link><description>&lt;p&gt;
DS4DH&#22312;#SMM4H 2023&#19978;&#65306;&#20351;&#29992;&#21477;&#23376;&#36716;&#25442;&#21644;&#20498;&#25968;&#25490;&#21517;&#34701;&#21512;&#36827;&#34892;&#38646;&#26679;&#26412;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;
&lt;/p&gt;
&lt;p&gt;
DS4DH at #SMM4H 2023: Zero-Shot Adverse Drug Events Normalization using Sentence Transformers and Reciprocal-Rank Fusion. (arXiv:2308.12877v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DS4DH&#22312;#SMM4H 2023&#20013;&#24320;&#21457;&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;&#31995;&#32479;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#35813;&#31995;&#32479;&#21033;&#29992;&#21477;&#23376;&#36716;&#25442;&#21644;&#20498;&#25968;&#25490;&#21517;&#34701;&#21512;&#36827;&#34892;&#38646;&#26679;&#26412;&#35268;&#33539;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20849;&#20139;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#26377;&#25928;&#24212;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25366;&#25496;&#20013;&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#30001;&#25968;&#25454;&#31185;&#23398;&#19982;&#25968;&#23383;&#20581;&#24247;&#22242;&#38431;&#24320;&#21457;&#30340;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#25366;&#25496;&#20581;&#24247;&#24212;&#29992;2023&#20849;&#20139;&#20219;&#21153;5&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;&#31995;&#32479;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;&#20849;&#20139;&#20219;&#21153;5&#26088;&#22312;&#23558;Twitter&#20013;&#30340;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#25552;&#21450;&#26631;&#20934;&#21270;&#20026;&#21307;&#30103;&#27861;&#35268;&#27963;&#21160;&#26415;&#35821;&#23383;&#20856;&#20013;&#30340;&#26631;&#20934;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#37319;&#29992;&#20004;&#38454;&#27573;&#26041;&#27861;&#65306;BERT&#24494;&#35843;&#23454;&#20307;&#35782;&#21035;&#65292;&#28982;&#21518;&#20351;&#29992;&#21477;&#23376;&#36716;&#25442;&#21644;&#20498;&#25968;&#25490;&#21517;&#34701;&#21512;&#36827;&#34892;&#38646;&#26679;&#26412;&#35268;&#33539;&#21270;&#12290;&#35813;&#26041;&#27861;&#30340;&#31934;&#30830;&#24230;&#20026;44.9%&#65292;&#21484;&#22238;&#29575;&#20026;40.5%&#65292;F1&#20998;&#25968;&#20026;42.6%&#12290;&#23427;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20849;&#20139;&#20219;&#21153;5&#20013;&#20301;&#25968;&#34920;&#29616;10%&#65292;&#24182;&#22312;&#25152;&#26377;&#21442;&#19982;&#32773;&#20013;&#23637;&#31034;&#20102;&#26368;&#39640;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#22312;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25366;&#25496;&#39046;&#22495;&#20013;&#36827;&#34892;&#19981;&#33391;&#33647;&#29289;&#20107;&#20214;&#35268;&#33539;&#21270;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper outlines the performance evaluation of a system for adverse drug event normalization, developed by the Data Science for Digital Health group for the Social Media Mining for Health Applications 2023 shared task 5. Shared task 5 targeted the normalization of adverse drug event mentions in Twitter to standard concepts from the Medical Dictionary for Regulatory Activities terminology. Our system hinges on a two-stage approach: BERT fine-tuning for entity recognition, followed by zero-shot normalization using sentence transformers and reciprocal-rank fusion. The approach yielded a precision of 44.9%, recall of 40.5%, and an F1-score of 42.6%. It outperformed the median performance in shared task 5 by 10% and demonstrated the highest performance among all participants. These results substantiate the effectiveness of our approach and its potential application for adverse drug event normalization in the realm of social media text mining.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#32479;&#35745;&#21644;&#35821;&#20041;&#20998;&#26512;&#25216;&#26415;&#20174;&#22270;&#20687;&#20869;&#23481;&#20013;&#36827;&#34892;&#25991;&#26412;&#30456;&#20284;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#25220;&#34989;&#38382;&#39064;&#12290;&#36825;&#31181;&#25216;&#26415;&#19981;&#20165;&#35206;&#30422;&#20102;&#35821;&#20041;&#12289;&#21629;&#21517;&#23454;&#20307;&#12289;&#37322;&#20041;&#31561;NLP&#26041;&#27861;&#65292;&#36824;&#21033;&#29992;&#22270;&#20687;&#20869;&#23481;&#22788;&#29702;&#25216;&#26415;&#26469;&#35782;&#21035;&#22270;&#20687;&#20869;&#23481;&#30340;&#25220;&#34989;&#23454;&#20363;&#65292;&#30830;&#20445;&#22270;&#20687;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12842</link><description>&lt;p&gt;
&#20351;&#29992;&#32479;&#35745;&#21644;&#35821;&#20041;&#20998;&#26512;&#25216;&#26415;&#20174;&#22270;&#20687;&#20869;&#23481;&#20013;&#36827;&#34892;&#25991;&#26412;&#30456;&#20284;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Text Similarity from Image Contents using Statistical and Semantic Analysis Techniques. (arXiv:2308.12842v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12842
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#32479;&#35745;&#21644;&#35821;&#20041;&#20998;&#26512;&#25216;&#26415;&#20174;&#22270;&#20687;&#20869;&#23481;&#20013;&#36827;&#34892;&#25991;&#26412;&#30456;&#20284;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#25220;&#34989;&#38382;&#39064;&#12290;&#36825;&#31181;&#25216;&#26415;&#19981;&#20165;&#35206;&#30422;&#20102;&#35821;&#20041;&#12289;&#21629;&#21517;&#23454;&#20307;&#12289;&#37322;&#20041;&#31561;NLP&#26041;&#27861;&#65292;&#36824;&#21033;&#29992;&#22270;&#20687;&#20869;&#23481;&#22788;&#29702;&#25216;&#26415;&#26469;&#35782;&#21035;&#22270;&#20687;&#20869;&#23481;&#30340;&#25220;&#34989;&#23454;&#20363;&#65292;&#30830;&#20445;&#22270;&#20687;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25220;&#34989;&#26816;&#27979;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#20013;&#26368;&#30740;&#31350;&#30340;&#39046;&#22495;&#20043;&#19968;&#12290;&#33391;&#22909;&#30340;&#25220;&#34989;&#26816;&#27979;&#28085;&#30422;&#20102;&#25152;&#26377;&#30340;NLP&#26041;&#27861;&#65292;&#21253;&#25324;&#35821;&#20041;&#12289;&#21629;&#21517;&#23454;&#20307;&#12289;&#37322;&#20041;&#31561;&#65292;&#24182;&#29983;&#25104;&#35814;&#32454;&#30340;&#25220;&#34989;&#25253;&#21578;&#12290;&#36328;&#35821;&#35328;&#25220;&#34989;&#30340;&#26816;&#27979;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#21508;&#31181;&#39640;&#32423;&#26041;&#27861;&#21644;&#31639;&#27861;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#25991;&#26412;&#30456;&#20284;&#24615;&#26816;&#26597;&#12290;&#29616;&#20170;&#30340;&#25220;&#34989;&#32773;&#20063;&#22312;&#25552;&#21319;&#33258;&#24049;&#65292;&#20197;&#36991;&#20813;&#34987;&#21457;&#29616;&#12290;&#20182;&#20204;&#20351;&#29992;&#25913;&#20889;&#12289;&#21516;&#20041;&#35789;&#26367;&#25442;&#12289;&#24341;&#25991;&#19981;&#21305;&#37197;&#12289;&#19981;&#21516;&#35821;&#35328;&#30340;&#32763;&#35793;&#31561;&#25216;&#26415;&#26469;&#35268;&#36991;&#26816;&#27979;&#12290;&#22270;&#20687;&#20869;&#23481;&#25220;&#34989;&#26816;&#27979;&#65288;ICPD&#65289;&#24471;&#21040;&#20102;&#37325;&#35270;&#65292;&#21033;&#29992;&#20808;&#36827;&#30340;&#22270;&#20687;&#20869;&#23481;&#22788;&#29702;&#25216;&#26415;&#26469;&#35782;&#21035;&#25220;&#34989;&#23454;&#20363;&#65292;&#20197;&#30830;&#20445;&#22270;&#20687;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#12290;&#25220;&#34989;&#38382;&#39064;&#19981;&#20165;&#38480;&#20110;&#25991;&#26412;&#20869;&#23481;&#65292;&#22270;&#29255;&#22914;&#22270;&#34920;&#12289;&#22270;&#24418;&#31561;&#20063;&#20855;&#26377;&#28508;&#22312;&#30340;&#25220;&#34989;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Plagiarism detection is one of the most researched areas among the Natural Language Processing(NLP) community. A good plagiarism detection covers all the NLP methods including semantics, named entities, paraphrases etc. and produces detailed plagiarism reports. Detection of Cross Lingual Plagiarism requires deep knowledge of various advanced methods and algorithms to perform effective text similarity checking. Nowadays the plagiarists are also advancing themselves from hiding the identity from being catch in such offense. The plagiarists are bypassed from being detected with techniques like paraphrasing, synonym replacement, mismatching citations, translating one language to another. Image Content Plagiarism Detection (ICPD) has gained importance, utilizing advanced image content processing to identify instances of plagiarism to ensure the integrity of image content. The issue of plagiarism extends beyond textual content, as images such as figures, graphs, and tables also have the pote
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;&#26377;&#20851;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23041;&#32961;&#21644;&#28431;&#27934;&#30340;&#29616;&#26377;&#31185;&#23398;&#21162;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#25551;&#36848;&#36825;&#20123;&#23041;&#32961;&#12289;&#39044;&#38450;&#25514;&#26045;&#21644;&#30001;&#20110;&#39044;&#38450;&#25514;&#26045;&#19981;&#23436;&#21892;&#32780;&#20135;&#29983;&#30340;&#28431;&#27934;&#20043;&#38388;&#20851;&#31995;&#30340;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2308.12833</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#36827;&#34892;&#38750;&#27861;&#30446;&#30340;&#65306;&#23041;&#32961;&#12289;&#39044;&#38450;&#25514;&#26045;&#21644;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities. (arXiv:2308.12833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;&#26377;&#20851;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23041;&#32961;&#21644;&#28431;&#27934;&#30340;&#29616;&#26377;&#31185;&#23398;&#21162;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#25551;&#36848;&#36825;&#20123;&#23041;&#32961;&#12289;&#39044;&#38450;&#25514;&#26045;&#21644;&#30001;&#20110;&#39044;&#38450;&#25514;&#26045;&#19981;&#23436;&#21892;&#32780;&#20135;&#29983;&#30340;&#28431;&#27934;&#20043;&#38388;&#20851;&#31995;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24037;&#19994;&#21644;&#23398;&#26415;&#30028;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#20998;&#21457;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#37117;&#20851;&#27880;&#20102;LLMs&#30340;&#23433;&#20840;&#21644;&#23433;&#20840;&#30456;&#20851;&#23041;&#32961;&#21644;&#28431;&#27934;&#65292;&#21253;&#25324;&#22312;&#28508;&#22312;&#30340;&#29359;&#32618;&#27963;&#21160;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24050;&#32463;&#34920;&#26126;LLMs&#21487;&#20197;&#34987;&#28389;&#29992;&#29992;&#20110;&#27450;&#35784;&#12289;&#20882;&#20805;&#21644;&#29983;&#25104;&#24694;&#24847;&#36719;&#20214;&#65307;&#32780;&#20854;&#20182;&#20316;&#32773;&#21017;&#32771;&#34385;&#20102;AI&#23545;&#40784;&#30340;&#26356;&#19968;&#33324;&#30340;&#38382;&#39064;&#12290;&#24320;&#21457;&#20154;&#21592;&#21644;&#20174;&#19994;&#20154;&#21592;&#37117;&#38656;&#35201;&#24847;&#35782;&#21040;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;&#30340;&#23384;&#22312;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#29616;&#26377;&#30340;&#20027;&#35201;&#26159;&#31185;&#23398;&#21162;&#21147;&#65292;&#20197;&#30830;&#23450;&#21644;&#20943;&#36731;&#30001;LLMs&#24341;&#36215;&#30340;&#23041;&#32961;&#21644;&#28431;&#27934;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25551;&#36848;&#30001;LLMs&#30340;&#29983;&#25104;&#33021;&#21147;&#24341;&#36215;&#30340;&#23041;&#32961;&#12289;&#39044;&#38450;&#25514;&#26045;&#20197;&#21450;&#30001;&#20110;&#39044;&#38450;&#25514;&#26045;&#19981;&#23436;&#21892;&#32780;&#20135;&#29983;&#30340;&#28431;&#27934;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spurred by the recent rapid increase in the development and distribution of large language models (LLMs) across industry and academia, much recent work has drawn attention to safety- and security-related threats and vulnerabilities of LLMs, including in the context of potentially criminal activities. Specifically, it has been shown that LLMs can be misused for fraud, impersonation, and the generation of malware; while other authors have considered the more general problem of AI alignment. It is important that developers and practitioners alike are aware of security-related problems with such models. In this paper, we provide an overview of existing - predominantly scientific - efforts on identifying and mitigating threats and vulnerabilities arising from LLMs. We present a taxonomy describing the relationship between threats caused by the generative capabilities of LLMs, prevention measures intended to address such threats, and vulnerabilities arising from imperfect prevention measures
&lt;/p&gt;</description></item><item><title>WavMark&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#38899;&#39057;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#30701;&#30701;1&#31186;&#30340;&#38899;&#39057;&#29255;&#27573;&#20013;&#32534;&#30721;&#22810;&#36798;32&#20301;&#30340;&#27700;&#21360;&#65292;&#23545;&#20154;&#31867;&#24863;&#23448;&#26080;&#24863;&#30693;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#38887;&#24615;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#21512;&#25104;&#22768;&#38899;&#30340;&#26377;&#25928;&#35782;&#21035;&#21644;&#38899;&#39057;&#29256;&#26435;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2308.12770</link><description>&lt;p&gt;
WavMark&#65306;&#29992;&#20110;&#38899;&#39057;&#29983;&#25104;&#30340;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
WavMark: Watermarking for Audio Generation. (arXiv:2308.12770v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12770
&lt;/p&gt;
&lt;p&gt;
WavMark&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#38899;&#39057;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#30701;&#30701;1&#31186;&#30340;&#38899;&#39057;&#29255;&#27573;&#20013;&#32534;&#30721;&#22810;&#36798;32&#20301;&#30340;&#27700;&#21360;&#65292;&#23545;&#20154;&#31867;&#24863;&#23448;&#26080;&#24863;&#30693;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#38887;&#24615;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#21512;&#25104;&#22768;&#38899;&#30340;&#26377;&#25928;&#35782;&#21035;&#21644;&#38899;&#39057;&#29256;&#26435;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#38646;-shot&#35821;&#38899;&#21512;&#25104;&#26041;&#38754;&#30340;&#31361;&#30772;&#20351;&#24471;&#21482;&#29992;&#20960;&#31186;&#38047;&#30340;&#24405;&#38899;&#23601;&#33021;&#27169;&#20223;&#35828;&#35805;&#32773;&#30340;&#22768;&#38899;&#65292;&#24182;&#19988;&#20445;&#25345;&#39640;&#24230;&#30340;&#30495;&#23454;&#24863;&#12290;&#38500;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#20043;&#22806;&#65292;&#36825;&#39033;&#24378;&#22823;&#30340;&#25216;&#26415;&#36824;&#24102;&#26469;&#20102;&#26126;&#26174;&#30340;&#39118;&#38505;&#65292;&#21253;&#25324;&#35821;&#38899;&#27450;&#35784;&#21644;&#20882;&#20805;&#35828;&#35805;&#32773;&#12290;&#19982;&#20165;&#20381;&#36182;&#34987;&#21160;&#26041;&#27861;&#26469;&#26816;&#27979;&#21512;&#25104;&#25968;&#25454;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#27700;&#21360;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#31215;&#26497;&#19988;&#24378;&#22823;&#30340;&#38450;&#24481;&#26426;&#21046;&#26469;&#24212;&#23545;&#36825;&#20123;&#28508;&#22312;&#39118;&#38505;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38899;&#39057;&#27700;&#21360;&#25216;&#26415;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#20165;1&#31186;&#30340;&#38899;&#39057;&#29255;&#27573;&#20013;&#32534;&#30721;&#22810;&#36798;32&#20301;&#30340;&#27700;&#21360;&#12290;&#27700;&#21360;&#23545;&#20154;&#31867;&#24863;&#23448;&#26469;&#35828;&#26159;&#26080;&#27861;&#23519;&#35273;&#30340;&#65292;&#24182;&#19988;&#23545;&#21508;&#31181;&#25915;&#20987;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#38887;&#24615;&#12290;&#23427;&#21487;&#20197;&#20316;&#20026;&#21512;&#25104;&#22768;&#38899;&#30340;&#26377;&#25928;&#26631;&#35782;&#31526;&#65292;&#24182;&#22312;&#38899;&#39057;&#29256;&#26435;&#20445;&#25252;&#30340;&#26356;&#24191;&#27867;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#26694;&#26550;&#20855;&#26377;&#24456;&#39640;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#23558;&#22810;&#20010;&#27700;&#21360;&#29255;&#27573;&#36827;&#34892;&#32452;&#21512;&#20197;&#23454;&#29616;&#26356;&#21152;&#20016;&#23500;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in zero-shot voice synthesis have enabled imitating a speaker's voice using just a few seconds of recording while maintaining a high level of realism. Alongside its potential benefits, this powerful technology introduces notable risks, including voice fraud and speaker impersonation. Unlike the conventional approach of solely relying on passive methods for detecting synthetic data, watermarking presents a proactive and robust defence mechanism against these looming risks. This paper introduces an innovative audio watermarking framework that encodes up to 32 bits of watermark within a mere 1-second audio snippet. The watermark is imperceptible to human senses and exhibits strong resilience against various attacks. It can serve as an effective identifier for synthesized voices and holds potential for broader applications in audio copyright protection. Moreover, this framework boasts high flexibility, allowing for the combination of multiple watermark segments to achi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21019;&#24314;&#20102;DEEP-VOICE&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#23454;&#26102;&#26816;&#27979;AI&#29983;&#25104;&#35821;&#38899;&#30340;&#30446;&#26631;&#65292;&#20197;&#24212;&#23545;DeepFake&#35821;&#38899;&#36716;&#25442;&#24102;&#26469;&#30340;&#36947;&#24503;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12734</link><description>&lt;p&gt;
&#23454;&#26102;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#35821;&#38899;&#29992;&#20110;DeepFake&#35821;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion. (arXiv:2308.12734v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21019;&#24314;&#20102;DEEP-VOICE&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#23454;&#26102;&#26816;&#27979;AI&#29983;&#25104;&#35821;&#38899;&#30340;&#30446;&#26631;&#65292;&#20197;&#24212;&#23545;DeepFake&#35821;&#38899;&#36716;&#25442;&#24102;&#26469;&#30340;&#36947;&#24503;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#39046;&#22495;&#20013;&#65292;&#29983;&#25104;&#22411;AI&#25216;&#26415;&#20351;&#24471;&#35821;&#38899;&#20811;&#38534;&#21644;&#23454;&#26102;&#35821;&#38899;&#36716;&#25442;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#28508;&#22312;&#30340;&#36947;&#24503;&#38382;&#39064;&#65292;&#21253;&#25324;&#38544;&#31169;&#20405;&#29359;&#21644;&#34394;&#20551;&#38472;&#36848;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20127;&#38656;&#19968;&#31181;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;AI&#29983;&#25104;&#35821;&#38899;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;DeepFake&#35821;&#38899;&#36716;&#25442;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;DEEP-VOICE&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20843;&#20301;&#30693;&#21517;&#20154;&#29289;&#30340;&#30495;&#23454;&#35821;&#38899;&#21644;&#20182;&#20204;&#20043;&#38388;&#30456;&#20114;&#36716;&#25442;&#21518;&#30340;&#35821;&#38899;&#12290;&#36890;&#36807;&#23545;&#35821;&#38899;&#30495;&#23454;&#24615;&#36827;&#34892;&#20108;&#20998;&#31867;&#65292;&#36890;&#36807;t&#26816;&#39564;&#23545;&#26102;&#38388;&#38899;&#39057;&#29305;&#24449;&#36827;&#34892;&#20102;&#32479;&#35745;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#20998;&#24067;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#20351;&#29992;&#36229;&#21442;&#25968;&#20248;&#21270;&#26469;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#35782;&#21035;&#35821;&#38899;&#30340;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are growing implications surrounding generative AI in the speech domain that enable voice cloning and real-time voice conversion from one individual to another. This technology poses a significant ethical threat and could lead to breaches of privacy and misrepresentation, thus there is an urgent need for real-time detection of AI-generated speech for DeepFake Voice Conversion. To address the above emerging issues, the DEEP-VOICE dataset is generated in this study, comprised of real human speech from eight well-known figures and their speech converted to one another using Retrieval-based Voice Conversion. Presenting as a binary classification problem of whether the speech is real or AI-generated, statistical analysis of temporal audio features through t-testing reveals that there are significantly different distributions. Hyperparameter optimisation is implemented for machine learning models to identify the source of speech. Following the training of 208 individual machine learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#20381;&#36182;&#38381;&#28304;&#27169;&#22411;&#30340;&#26041;&#27861;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#20197;&#24212;&#23545;&#20351;&#29992;&#38381;&#28304;&#27169;&#22411;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#25506;&#32034;&#20013;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#25351;&#20196;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#23558;&#25928;&#29575;&#26368;&#39640;&#30340;&#26041;&#27861;&#19982;&#20004;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2308.12711</link><description>&lt;p&gt;
&#21457;&#25381;&#22823;&#21355;&#23545;&#25112;&#27468;&#21033;&#20122;&#30340;&#21147;&#37327;&#65306;&#25506;&#32034;&#19981;&#20351;&#29992;&#38381;&#28304;&#27169;&#22411;&#30340;&#25351;&#20196;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of David against Goliath: Exploring Instruction Data Generation without Using Closed-Source Models. (arXiv:2308.12711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#20381;&#36182;&#38381;&#28304;&#27169;&#22411;&#30340;&#26041;&#27861;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#20197;&#24212;&#23545;&#20351;&#29992;&#38381;&#28304;&#27169;&#22411;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#25506;&#32034;&#20013;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#25351;&#20196;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#23558;&#25928;&#29575;&#26368;&#39640;&#30340;&#26041;&#27861;&#19982;&#20004;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#23545;&#20110;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#25353;&#29031;&#29992;&#25143;&#30340;&#25351;&#20196;&#23436;&#25104;&#21508;&#31181;&#24320;&#25918;&#39046;&#22495;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#25351;&#20196;&#35843;&#25972;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#30001;&#20110;&#20154;&#24037;&#27880;&#37322;&#30340;&#39640;&#26114;&#25104;&#26412;&#21644;&#36136;&#37327;&#19981;&#20339;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#19968;&#30452;&#22312;&#28145;&#20837;&#25506;&#32034;&#21033;&#29992;&#24378;&#22823;&#30340;&#38381;&#28304;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#25351;&#20196;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#28508;&#22312;&#30340;&#39118;&#38505;&#65292;&#22240;&#20026;&#24378;&#22823;&#30340;&#38381;&#28304;&#27169;&#22411;&#30340;&#20351;&#29992;&#35201;&#27714;&#20005;&#31105;&#21033;&#29992;&#23427;&#20204;&#30340;&#36755;&#20986;&#26469;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#20381;&#36182;&#38381;&#28304;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#27861;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#21253;&#25324;&#23545;&#21508;&#31181;&#29616;&#26377;&#30340;&#25351;&#20196;&#29983;&#25104;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#26368;&#32456;&#23558;&#25928;&#29575;&#26368;&#39640;&#30340;&#21464;&#20307;&#19982;&#20004;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#36827;&#34892;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning is instrumental in enabling Large Language Models~(LLMs) to follow user instructions to complete various open-domain tasks. The success of instruction tuning depends on the availability of high-quality instruction data. Owing to the exorbitant cost and substandard quality of human annotation, recent works have been deeply engaged in the exploration of the utilization of powerful closed-source models to generate instruction data automatically. However, these methods carry potential risks arising from the usage requirements of powerful closed-source models, which strictly forbid the utilization of their outputs to develop machine learning models. To deal with this problem, in this work, we explore alternative approaches to generate high-quality instruction data that do not rely on closed-source models. Our exploration includes an investigation of various existing instruction generation methods, culminating in the integration of the most efficient variant with two novel
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#24378;&#25351;&#20196;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#30495;&#23454;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SWIE&#65288;&#20998;&#27573;&#21152;&#26435;&#25351;&#20196;&#23884;&#20837;&#65289;&#21644;&#19968;&#20010;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;OVERMISS&#65292;&#29992;&#20110;&#25913;&#21892;&#27169;&#22411;&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#21644;&#25552;&#39640;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.12674</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#25351;&#20196;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Translation Faithfulness of Large Language Models via Augmenting Instructions. (arXiv:2308.12674v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12674
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#25351;&#20196;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#30495;&#23454;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SWIE&#65288;&#20998;&#27573;&#21152;&#26435;&#25351;&#20196;&#23884;&#20837;&#65289;&#21644;&#19968;&#20010;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;OVERMISS&#65292;&#29992;&#20110;&#25913;&#21892;&#27169;&#22411;&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#21644;&#25552;&#39640;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20855;&#26377;&#24456;&#24378;&#30340;&#36890;&#29992;&#33021;&#21147;&#65292;&#30446;&#21069;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#25361;&#25112;&#26159;&#36890;&#36807;&#20302;&#25104;&#26412;&#30340;&#25351;&#20196;&#35843;&#25972;&#26469;&#28608;&#21457;&#23427;&#20204;&#30340;&#19987;&#38376;&#33021;&#21147;&#65292;&#22914;&#26426;&#22120;&#32763;&#35793;&#12290;&#26631;&#20934;&#30340;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#26159;&#25353;&#39034;&#24207;&#32452;&#32455;&#30340;&#65292;&#21253;&#25324;&#25351;&#20196;&#12289;&#36755;&#20837;&#21644;&#21709;&#24212;&#30340;&#36830;&#25509;&#12290;&#30001;&#20110;LLMs&#30340;&#27880;&#24847;&#26426;&#21046;&#22312;&#23616;&#37096;&#20851;&#27880;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;LLMs&#20542;&#21521;&#20110;&#22312;&#27599;&#20010;&#20301;&#32622;&#26356;&#22810;&#22320;&#20851;&#27880;&#38468;&#36817;&#30340;&#21333;&#35789;&#25110;&#21477;&#23376;&#12290;&#36825;&#23548;&#33268;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#36951;&#24536;&#25351;&#20196;&#30340;&#39118;&#38505;&#24456;&#39640;&#12290;&#20026;&#20102;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SWIE&#65288;&#20998;&#27573;&#21152;&#26435;&#25351;&#20196;&#23884;&#20837;&#65289;&#21644;&#19968;&#20010;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;OVERMISS&#12290;SWIE&#36890;&#36807;&#22312;&#21518;&#32493;&#30340;&#36755;&#20837;&#21644;&#21709;&#24212;&#34920;&#31034;&#19978;&#28155;&#21152;&#20840;&#23616;&#25351;&#20196;&#34920;&#31034;&#26469;&#25913;&#21892;&#27169;&#22411;&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#12290;OVERMISS&#36890;&#36807;&#23558;&#36807;&#24230;&#32763;&#35793;&#21644;&#36951;&#28431;&#32763;&#35793;&#32467;&#26524;&#19982;&#27491;&#30830;&#32763;&#35793;&#36827;&#34892;&#27604;&#36739;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#21040;&#20004;&#20010;&#20027;&#35201;&#30340;&#32763;&#35793;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) present strong general capabilities, and a current compelling challenge is stimulating their specialized capabilities, such as machine translation, through low-cost instruction tuning. The standard instruction-following data is sequentially organized as the concatenation of an instruction, an input, and a response. As the attention mechanism of LLMs has limitations on local focus, LLMs tend to focus more on the words or sentences nearby at each position. This leads to a high risk of instruction forgetting during decoding. To alleviate the above issues, We propose SWIE (Segment-Weighted Instruction Embedding) and an instruction-following dataset OVERMISS. SWIE improves the model instruction understanding by adding a global instruction representation on the following input and response representations. OVERMISS improves model faithfulness by comparing over-translation and miss-translation results with the correct translation. We apply our methods to two main-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#38386;&#32842;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#36716;&#21464;&#20026;&#20219;&#21153;&#23548;&#21521;&#22411;&#65292;&#24182;&#22312;&#25968;&#25454;&#12289;&#29305;&#24449;&#21644;&#30446;&#26631;&#19977;&#26041;&#38754;&#36827;&#34892;&#20102;&#20851;&#38190;&#25913;&#36827;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12648</link><description>&lt;p&gt;
&#20174;&#32842;&#22825;&#21040;&#23454;&#36136;&#65306;&#35299;&#20915;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#24773;&#24863;&#35782;&#21035;&#23398;&#20064;&#30340;&#20851;&#38190;&#27493;&#39588;
&lt;/p&gt;
&lt;p&gt;
From Chatter to Matter: Addressing Critical Steps of Emotion Recognition Learning in Task-oriented Dialogue. (arXiv:2308.12648v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#38386;&#32842;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#36716;&#21464;&#20026;&#20219;&#21153;&#23548;&#21521;&#22411;&#65292;&#24182;&#22312;&#25968;&#25454;&#12289;&#29305;&#24449;&#21644;&#30446;&#26631;&#19977;&#26041;&#38754;&#36827;&#34892;&#20102;&#20851;&#38190;&#25913;&#36827;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#65288;ERC&#65289;&#26159;&#26500;&#24314;&#31867;&#20284;&#20154;&#31867;&#23545;&#35805;&#20195;&#29702;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#23613;&#31649;&#22312;&#38386;&#32842;&#23545;&#35805;&#26041;&#38754;&#24050;&#32463;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#20013;&#30340;ERC&#20173;&#28982;&#20960;&#20046;&#27809;&#26377;&#24471;&#21040;&#20851;&#27880;&#12290;&#30452;&#25509;&#23558;&#38386;&#32842;ERC&#27169;&#22411;&#24212;&#29992;&#20110;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#65288;ToDs&#65289;&#20250;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#24573;&#35270;&#20102;ToDs&#20013;&#24773;&#24863;&#19982;&#20219;&#21153;&#23436;&#25104;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#38386;&#32842;ERC&#27169;&#22411;&#36716;&#21464;&#20026;&#20219;&#21153;&#23548;&#21521;&#22411;&#65292;&#35299;&#20915;&#20102;&#19977;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#25968;&#25454;&#12289;&#29305;&#24449;&#21644;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#22686;&#21152;&#32597;&#35265;&#24773;&#24863;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;ERC&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#35805;&#29366;&#24577;&#20316;&#20026;&#36741;&#21161;&#29305;&#24449;&#65292;&#23558;&#29992;&#25143;&#30446;&#26631;&#30340;&#20851;&#38190;&#20449;&#24687;&#25972;&#21512;&#36827;&#26469;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;ToDs&#20013;&#30340;&#22810;&#26041;&#38754;&#24773;&#24863;&#23450;&#20041;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#30446;&#26631;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#24773;&#24863;&#36317;&#31163;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition in conversations (ERC) is a crucial task for building human-like conversational agents. While substantial efforts have been devoted to ERC for chit-chat dialogues, the task-oriented counterpart is largely left unattended. Directly applying chit-chat ERC models to task-oriented dialogues (ToDs) results in suboptimal performance as these models overlook key features such as the correlation between emotions and task completion in ToDs. In this paper, we propose a framework that turns a chit-chat ERC model into a task-oriented one, addressing three critical aspects: data, features and objective. First, we devise two ways of augmenting rare emotions to improve ERC performance. Second, we use dialogue states as auxiliary features to incorporate key information from the goal of the user. Lastly, we leverage a multi-aspect emotion definition in ToDs to devise a multi-task learning objective and a novel emotion-distance weighted loss function. Our framework yields significan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#32452;&#21512;&#35789;&#32512;&#21644;&#35789;&#26681;&#26469;&#34913;&#37327;&#35821;&#35328;&#29983;&#20135;&#21147;&#65292;&#36991;&#20813;&#20102;&#23545;&#35789;&#39057;&#30340;&#30452;&#25509;&#20381;&#36182;&#12290;&#35813;&#31639;&#27861;&#22312;&#33521;&#35821;&#21644;&#20420;&#35821;&#25968;&#25454;&#19978;&#35780;&#20272;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#35789;&#32512;&#21644;&#35789;&#26681;&#20851;&#31995;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.12643</link><description>&lt;p&gt;
&#38543;&#26426;&#26041;&#27861;&#35780;&#20272;&#35821;&#35328;&#29983;&#20135;&#21147;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Method of Measuring Linguistic Productivity. (arXiv:2308.12643v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#32452;&#21512;&#35789;&#32512;&#21644;&#35789;&#26681;&#26469;&#34913;&#37327;&#35821;&#35328;&#29983;&#20135;&#21147;&#65292;&#36991;&#20813;&#20102;&#23545;&#35789;&#39057;&#30340;&#30452;&#25509;&#20381;&#36182;&#12290;&#35813;&#31639;&#27861;&#22312;&#33521;&#35821;&#21644;&#20420;&#35821;&#25968;&#25454;&#19978;&#35780;&#20272;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#35789;&#32512;&#21644;&#35789;&#26681;&#20851;&#31995;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34913;&#37327;&#35821;&#35328;&#29983;&#20135;&#21147;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23458;&#35266;&#35780;&#20272;&#20102;&#19968;&#20010;&#35789;&#32512;&#29992;&#20110;&#26500;&#24314;&#26032;&#22797;&#26434;&#35789;&#30340;&#33021;&#21147;&#65292;&#19982;&#20854;&#20182;&#24120;&#29992;&#30340;&#34913;&#37327;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#19981;&#30452;&#25509;&#20381;&#36182;&#20110;&#35789;&#39057;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20316;&#32773;&#24314;&#35758;&#23558;&#35821;&#35328;&#29983;&#20135;&#21147;&#35270;&#20026;&#19968;&#20010;&#35789;&#32512;&#19982;&#38543;&#26426;&#35789;&#26681;&#32452;&#21512;&#30340;&#27010;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#21253;&#25324;&#65306;&#39318;&#20808;&#65292;&#35789;&#39057;&#19981;&#20250;&#20027;&#23548;&#29983;&#20135;&#21147;&#30340;&#34913;&#37327;&#65292;&#32780;&#26159;&#33258;&#28982;&#22320;&#24433;&#21709;&#22522;&#30784;&#26679;&#26412;&#30340;&#25277;&#26679;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#19981;&#20165;&#20165;&#26159;&#35745;&#31639;&#24102;&#26377;&#35789;&#32512;&#30340;&#24050;&#39564;&#35777;&#35789;&#27719;&#31867;&#22411;&#65292;&#32780;&#26159;&#27169;&#25311;&#26500;&#36896;&#36825;&#20123;&#31867;&#22411;&#65292;&#28982;&#21518;&#26816;&#26597;&#23427;&#20204;&#26159;&#21542;&#20986;&#29616;&#22312;&#35821;&#26009;&#24211;&#20013;&#12290;&#31532;&#19977;&#65292;&#20197;&#35821;&#26009;&#24211;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#21644;&#38543;&#26426;&#35774;&#35745;&#30830;&#20445;&#30495;&#27491;&#30340;&#26032;&#35789;&#21644;&#26089;&#26399;&#26500;&#35789;&#20855;&#26377;&#30456;&#31561;&#30340;&#36873;&#25321;&#26426;&#20250;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#33521;&#35821;&#21644;&#20420;&#35821;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25152;&#24471;&#32467;&#26524;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#28041;&#21450;&#35789;&#32512;&#19982;&#35789;&#26681;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper I propose a new way of measuring linguistic productivity that objectively assesses the ability of an affix to be used to coin new complex words and, unlike other popular measures, is not directly dependent upon token frequency. Specifically, I suggest that linguistic productivity may be viewed as the probability of an affix to combine with a random base. The advantages of this approach include the following. First, token frequency does not dominate the productivity measure but naturally influences the sampling of bases. Second, we are not just counting attested word types with an affix but rather simulating the construction of these types and then checking whether they are attested in the corpus. Third, a corpus-based approach and randomised design assure that true neologisms and words coined long ago have equal chances to be selected. The proposed algorithm is evaluated both on English and Russian data. The obtained results provide some valuable insights into the relatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#24037;&#19994;&#32423;&#21256;&#29273;&#21033;&#25991;&#26412;&#22788;&#29702;&#27169;&#22411;&#65292;&#21033;&#29992;HuSpaCy&#26694;&#26550;&#23454;&#29616;&#65292;&#36890;&#36807;&#22810;&#39033;&#25913;&#36827;&#22312;&#36164;&#28304;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#22791;&#39640;&#20934;&#30830;&#24615;&#21644;&#21534;&#21520;&#37327;&#65292;&#24182;&#22312;&#25152;&#26377;&#22522;&#26412;&#25991;&#26412;&#22788;&#29702;&#27493;&#39588;&#20013;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12635</link><description>&lt;p&gt;
&#20351;&#29992;HuSpaCy&#25512;&#36827;&#21256;&#29273;&#21033;&#25991;&#26412;&#22788;&#29702;&#65306;&#39640;&#25928;&#20934;&#30830;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
Advancing Hungarian Text Processing with HuSpaCy: Efficient and Accurate NLP Pipelines. (arXiv:2308.12635v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#24037;&#19994;&#32423;&#21256;&#29273;&#21033;&#25991;&#26412;&#22788;&#29702;&#27169;&#22411;&#65292;&#21033;&#29992;HuSpaCy&#26694;&#26550;&#23454;&#29616;&#65292;&#36890;&#36807;&#22810;&#39033;&#25913;&#36827;&#22312;&#36164;&#28304;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#20855;&#22791;&#39640;&#20934;&#30830;&#24615;&#21644;&#21534;&#21520;&#37327;&#65292;&#24182;&#22312;&#25152;&#26377;&#22522;&#26412;&#25991;&#26412;&#22788;&#29702;&#27493;&#39588;&#20013;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#22871;&#29992;&#20110;&#21256;&#29273;&#21033;&#25991;&#26412;&#22788;&#29702;&#30340;&#24037;&#19994;&#32423;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36164;&#28304;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#26159;&#22522;&#20110;spaCy&#26694;&#26550;&#23454;&#29616;&#30340;&#65292;&#22312;HuSpaCy&#24037;&#20855;&#21253;&#30340;&#26550;&#26500;&#19978;&#36827;&#34892;&#20102;&#22810;&#20010;&#25913;&#36827;&#12290;&#19982;&#29616;&#26377;&#30340;&#21256;&#29273;&#21033;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#30456;&#27604;&#65292;&#25105;&#20204;&#25152;&#26377;&#30340;&#31649;&#36947;&#37117;&#20855;&#22791;&#21253;&#25324;&#26631;&#35760;&#21270;&#12289;&#21477;&#23376;&#36793;&#30028;&#26816;&#27979;&#12289;&#35789;&#24615;&#26631;&#27880;&#12289;&#35789;&#24418;&#29305;&#24449;&#26631;&#27880;&#12289;&#35789;&#24418;&#36824;&#21407;&#12289;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#22312;&#20869;&#30340;&#25152;&#26377;&#22522;&#26412;&#25991;&#26412;&#22788;&#29702;&#27493;&#39588;&#65292;&#24182;&#19988;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#39640;&#21534;&#21520;&#37327;&#12290;&#25105;&#20204;&#23545;&#25552;&#20986;&#30340;&#25913;&#36827;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#23558;&#31649;&#36947;&#19982;&#26368;&#20808;&#36827;&#30340;&#24037;&#20855;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#22312;&#25152;&#26377;&#25991;&#26412;&#39044;&#22788;&#29702;&#27493;&#39588;&#20013;&#23637;&#31034;&#20102;&#26032;&#27169;&#22411;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;&#25152;&#26377;&#23454;&#39564;&#37117;&#21487;&#20197;&#37325;&#29616;&#65292;&#24182;&#19988;&#36825;&#20123;&#31649;&#36947;&#21487;&#20197;&#20813;&#36153;&#20351;&#29992;&#24182;&#37319;&#29992;&#23485;&#26494;&#30340;&#35768;&#21487;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a set of industrial-grade text processing models for Hungarian that achieve near state-of-the-art performance while balancing resource efficiency and accuracy. Models have been implemented in the spaCy framework, extending the HuSpaCy toolkit with several improvements to its architecture. Compared to existing NLP tools for Hungarian, all of our pipelines feature all basic text processing steps including tokenization, sentence-boundary detection, part-of-speech tagging, morphological feature tagging, lemmatization, dependency parsing and named entity recognition with high accuracy and throughput. We thoroughly evaluated the proposed enhancements, compared the pipelines with state-of-the-art tools and demonstrated the competitive performance of the new models in all text preprocessing steps. All experiments are reproducible and the pipelines are freely available under a permissive license.
&lt;/p&gt;</description></item><item><title>PromptMRG&#26159;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#35786;&#26029;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#35786;&#26029;&#24863;&#30693;&#30340;&#25552;&#31034;&#26469;&#25552;&#39640;MRG&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#20855;&#26377;&#39069;&#22806;&#30340;&#30142;&#30149;&#20998;&#31867;&#20998;&#25903;&#12290;</title><link>http://arxiv.org/abs/2308.12604</link><description>&lt;p&gt;
PromptMRG: &#35786;&#26029;&#39537;&#21160;&#30340;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation. (arXiv:2308.12604v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12604
&lt;/p&gt;
&lt;p&gt;
PromptMRG&#26159;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#35786;&#26029;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#35786;&#26029;&#24863;&#30693;&#30340;&#25552;&#31034;&#26469;&#25552;&#39640;MRG&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#20855;&#26377;&#39069;&#22806;&#30340;&#30142;&#30149;&#20998;&#31867;&#20998;&#25903;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;(MRG)&#20855;&#26377;&#24456;&#22823;&#30340;&#30740;&#31350;&#20215;&#20540;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#20943;&#36731;&#25918;&#23556;&#31185;&#21307;&#29983;&#25253;&#21578;&#25776;&#20889;&#30340;&#36127;&#25285;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20934;&#30830;&#30340;MRG&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#31934;&#30830;&#30340;&#20020;&#24202;&#29702;&#35299;&#21644;&#20020;&#24202;&#32467;&#26524;&#30340;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#30142;&#30149;&#30340;&#19981;&#24179;&#34913;&#20998;&#24067;&#20351;&#36825;&#19968;&#25361;&#25112;&#26356;&#21152;&#31361;&#20986;&#65292;&#22240;&#20026;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32597;&#35265;&#30142;&#30149;&#30340;&#27604;&#20363;&#36739;&#23569;&#65292;&#20351;&#24471;&#23427;&#20204;&#30340;&#35786;&#26029;&#24615;&#33021;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35786;&#26029;&#39537;&#21160;&#30340;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#26041;&#27861;(PromptMRG)&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#35786;&#26029;&#24863;&#30693;&#30340;&#25552;&#31034;&#26469;&#25552;&#39640;MRG&#35786;&#26029;&#20934;&#30830;&#24615;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PromptMRG&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#26377;&#19968;&#20010;&#39069;&#22806;&#30340;&#30142;&#30149;&#20998;&#31867;&#20998;&#25903;&#12290;&#22312;&#29983;&#25104;&#25253;&#21578;&#26102;&#65292;&#20174;&#20998;&#31867;&#20998;&#25903;&#24471;&#21040;&#30340;&#35786;&#26029;&#32467;&#26524;&#34987;&#36716;&#21270;&#20026;&#26631;&#35760;&#25552;&#31034;&#65292;&#20197;&#26126;&#30830;&#22320;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic medical report generation (MRG) is of great research value as it has the potential to relieve radiologists from the heavy burden of report writing. Despite recent advancements, accurate MRG remains challenging due to the need for precise clinical understanding and the identification of clinical findings. Moreover, the imbalanced distribution of diseases makes the challenge even more pronounced, as rare diseases are underrepresented in training data, making their diagnostic performance unreliable. To address these challenges, we propose diagnosis-driven prompts for medical report generation (PromptMRG), a novel framework that aims to improve the diagnostic accuracy of MRG with the guidance of diagnosis-aware prompts. Specifically, PromptMRG is based on encoder-decoder architecture with an extra disease classification branch. When generating reports, the diagnostic results from the classification branch are converted into token prompts to explicitly guide the generation process
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#37325;&#26032;&#21028;&#26029;&#19981;&#19968;&#33268;&#24615;&#8221;&#30340;&#29616;&#35937;&#65292;&#21363;&#22312;&#23436;&#25104;&#38472;&#36848;&#21644;&#37325;&#26032;&#35780;&#21028;&#36807;&#31243;&#20013;&#23384;&#22312;&#30683;&#30462;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#21644;&#38544;&#21547;&#20559;&#35265;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.12578</link><description>&lt;p&gt;
&#29702;&#26234;&#23545;&#35805;&#22768;&#38899;&#65306;&#20851;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#37327;&#31038;&#20250;&#20559;&#35265;&#30340;&#37325;&#26032;&#21028;&#26029;&#19981;&#19968;&#33268;&#24615;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models. (arXiv:2308.12578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#37325;&#26032;&#21028;&#26029;&#19981;&#19968;&#33268;&#24615;&#8221;&#30340;&#29616;&#35937;&#65292;&#21363;&#22312;&#23436;&#25104;&#38472;&#36848;&#21644;&#37325;&#26032;&#35780;&#21028;&#36807;&#31243;&#20013;&#23384;&#22312;&#30683;&#30462;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#21644;&#38544;&#21547;&#20559;&#35265;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20855;&#26377;&#19982;&#20154;&#31867;&#35266;&#23519;&#21040;&#30340;&#35748;&#30693;&#32467;&#26500;&#30456;&#20284;&#30340;&#29305;&#28857;&#65292;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#35843;&#26597;LLMs&#30340;&#35748;&#30693;&#26041;&#38754;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#20102;&#26126;&#30830;&#21644;&#38544;&#21547;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#36825;&#26159;&#24515;&#29702;&#23398;&#20013;&#19968;&#31181;&#29420;&#29305;&#30340;&#20004;&#32423;&#35748;&#30693;&#32467;&#26500;&#12290;&#25991;&#20013;&#25552;&#20986;&#65292;&#20010;&#20307;&#30340;&#26126;&#30830;&#31038;&#20250;&#20559;&#35265;&#65292;&#21363;&#20854;&#22312;&#38472;&#36848;&#20013;&#34920;&#36798;&#30340;&#26377;&#24847;&#35782;&#20559;&#35265;&#65292;&#21487;&#33021;&#19982;&#20854;&#38544;&#21547;&#30340;&#31038;&#20250;&#20559;&#35265;&#19981;&#21516;&#65292;&#21518;&#32773;&#20195;&#34920;&#20854;&#26080;&#24847;&#35782;&#20559;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;LLMs&#20013;&#30340;&#19968;&#31181;&#24182;&#34892;&#29616;&#35937;&#65292;&#21363;&#31038;&#20250;&#20559;&#35265;&#20013;&#30340;&#8220;&#37325;&#26032;&#21028;&#26029;&#19981;&#19968;&#33268;&#24615;&#8221;&#12290;&#22312;&#21021;&#22987;&#38454;&#27573;&#65292;LLM&#36127;&#36131;&#33258;&#21160;&#23436;&#25104;&#38472;&#36848;&#65292;&#21487;&#33021;&#20250;&#21253;&#21547;&#38544;&#21547;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#22312;&#38543;&#21518;&#30340;&#38454;&#27573;&#65292;&#21516;&#26679;&#30340;LLM&#37325;&#26032;&#35780;&#21028;&#20102;&#20854;&#33258;&#21160;&#29983;&#25104;&#30340;&#26377;&#20559;&#35265;&#30340;&#38472;&#36848;&#65292;&#20294;&#21364;&#19982;&#20043;&#30456;&#30683;&#30462;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#36825;&#31181;&#37325;&#26032;&#21028;&#26029;&#30340;&#19981;&#19968;&#33268;&#24615;&#21487;&#33021;&#31867;&#20284;&#20110;&#20154;&#31867;&#19981;&#30693;&#36947;&#20854;&#20559;&#35265;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent researches indicate that Pre-trained Large Language Models (LLMs) possess cognitive constructs similar to those observed in humans, prompting researchers to investigate the cognitive aspects of LLMs. This paper focuses on explicit and implicit social bias, a distinctive two-level cognitive construct in psychology. It posits that individuals' explicit social bias, which is their conscious expression of bias in the statements, may differ from their implicit social bias, which represents their unconscious bias. We propose a two-stage approach and discover a parallel phenomenon in LLMs known as "re-judge inconsistency" in social bias. In the initial stage, the LLM is tasked with automatically completing statements, potentially incorporating implicit social bias. However, in the subsequent stage, the same LLM re-judges the biased statement generated by itself but contradicts it. We propose that this re-judge inconsistency can be similar to the inconsistency between human's unaware im
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#21307;&#23398;&#26631;&#28857;&#20462;&#22797;&#30340;&#24555;&#36895;&#23567;&#22411;BERT&#27169;&#22411;&#12290;&#36890;&#36807;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#36741;&#21161;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#22312;&#20855;&#26377;&#36739;&#23567;&#27169;&#22411;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#20013;&#25991;RoBERTa&#27169;&#22411;&#30456;&#24403;&#30340;95%&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12568</link><description>&lt;p&gt;
&#29992;&#20110;&#20013;&#25991;&#21307;&#23398;&#26631;&#28857;&#20462;&#22797;&#30340;&#23567;&#22411;&#24555;&#36895;BERT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Small and Fast BERT for Chinese Medical Punctuation Restoration. (arXiv:2308.12568v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12568
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20013;&#25991;&#21307;&#23398;&#26631;&#28857;&#20462;&#22797;&#30340;&#24555;&#36895;&#23567;&#22411;BERT&#27169;&#22411;&#12290;&#36890;&#36807;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#36741;&#21161;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#22312;&#20855;&#26377;&#36739;&#23567;&#27169;&#22411;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#20013;&#25991;RoBERTa&#27169;&#22411;&#30456;&#24403;&#30340;95%&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#21548;&#20889;&#20013;&#65292;&#27809;&#26377;&#26126;&#30830;&#26631;&#28857;&#31526;&#21495;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#23548;&#33268;&#20102;&#23545;&#21548;&#20889;&#25253;&#21578;&#30340;&#35823;&#35299;&#12290;&#20026;&#20102;&#20351;&#29992;ASR&#25552;&#20379;&#31934;&#30830;&#21644;&#26131;&#25026;&#30340;&#20020;&#24202;&#25253;&#21578;&#65292;&#38656;&#35201;&#36827;&#34892;&#33258;&#21160;&#26631;&#28857;&#20462;&#22797;&#12290;&#32771;&#34385;&#21040;&#23454;&#38469;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#30340;&#24555;&#36895;&#36731;&#37327;&#32423;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#20013;&#25991;&#21307;&#23398;&#26631;&#28857;&#20462;&#22797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#39044;&#35757;&#32451;&#20219;&#21153;&#65288;&#26631;&#28857;&#31526;&#21495;&#39044;&#27979;&#65289;&#26469;&#25552;&#28860;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#26631;&#28857;&#20462;&#22797;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#25552;&#28860;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#20013;&#25991;RoBERTa&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;10%&#30340;&#27169;&#22411;&#22823;&#23567;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;95%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In clinical dictation, utterances after automatic speech recognition (ASR) without explicit punctuation marks may lead to the misunderstanding of dictated reports. To give a precise and understandable clinical report with ASR, automatic punctuation restoration is required. Considering a practical scenario, we propose a fast and light pre-trained model for Chinese medical punctuation restoration based on 'pretraining and fine-tuning' paradigm. In this work, we distill pre-trained models by incorporating supervised contrastive learning and a novel auxiliary pre-training task (Punctuation Mark Prediction) to make it well-suited for punctuation restoration. Our experiments on various distilled models reveal that our model can achieve 95% performance while 10% model size relative to state-of-the-art Chinese RoBERTa.
&lt;/p&gt;</description></item><item><title>CALM&#26159;&#19968;&#20010;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30456;&#27604;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.12539</link><description>&lt;p&gt;
CALM: &#19968;&#31181;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CALM : A Multi-task Benchmark for Comprehensive Assessment of Language Model Bias. (arXiv:2308.12539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12539
&lt;/p&gt;
&lt;p&gt;
CALM&#26159;&#19968;&#20010;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#30456;&#27604;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#19981;&#26029;&#22686;&#24378;&#65292;&#37327;&#21270;&#21644;&#27604;&#36739;&#23427;&#20204;&#22312;&#31038;&#20250;&#21644;&#20154;&#21475;&#23398;&#20559;&#35265;&#26041;&#38754;&#30340;&#33021;&#21147;&#20197;&#21450;&#28508;&#22312;&#30340;&#21361;&#23475;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#20559;&#35265;&#27979;&#37327;&#25968;&#25454;&#38598;&#23545;&#20110;&#20154;&#24037;&#35774;&#35745;&#27169;&#26495;&#30340;&#25200;&#21160;&#25935;&#24863;&#65292;&#22240;&#27492;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#20445;&#35777;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20840;&#38754;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#65288;CALM&#65289;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#37327;&#21270;LMs&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#25972;&#21512;&#20102;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#65288;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#26032;&#38395;&#25991;&#31456;&#65289;&#30340;16&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#36807;&#28388;&#20986;224&#20010;&#27169;&#26495;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;78,400&#20010;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#24179;&#22343;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#27169;&#26495;&#38271;&#24230;&#30340;&#21464;&#24322;&#31243;&#24230;&#31561;&#25351;&#26631;&#65292;&#27604;&#36739;CALM&#19982;&#20808;&#21069;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#27979;&#35797;&#20854;&#23545;&#32454;&#24494;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30456;&#23545;&#20110;&#20808;&#21069;&#25968;&#25454;&#38598;&#26356;&#21152;&#22810;&#26679;&#21644;&#21487;&#38752;&#65292;&#22240;&#27492;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#35780;&#20272;&#27169;&#22411;&#20559;&#35265;&#25152;&#38656;&#30340;&#35821;&#35328;&#21464;&#21270;&#30340;&#24191;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;20&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models (LMs) become increasingly powerful, it is important to quantify and compare them for sociodemographic bias with potential for harm. Prior bias measurement datasets are sensitive to perturbations in their manually designed templates, therefore unreliable. To achieve reliability, we introduce the Comprehensive Assessment of Language Model bias (CALM), a benchmark dataset to quantify bias in LMs across three tasks. We integrate 16 existing datasets across different domains, such as Wikipedia and news articles, to filter 224 templates from which we construct a dataset of 78,400 examples. We compare the diversity of CALM with prior datasets on metrics such as average semantic similarity, and variation in template length, and test the sensitivity to small perturbations. We show that our dataset is more diverse and reliable than previous datasets, thus better capture the breadth of linguistic variation required to reliably evaluate model bias. We evaluate 20 large language 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#20849;&#21516;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;CARE&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#21644;&#21452;&#21521;&#20132;&#20114;&#26469;&#35299;&#20915;&#29305;&#24449;&#28151;&#28102;&#21644;&#23376;&#20219;&#21153;&#20132;&#20114;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12531</link><description>&lt;p&gt;
CARE: &#29992;&#20110;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#20849;&#21516;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CARE: Co-Attention Network for Joint Entity and Relation Extraction. (arXiv:2308.12531v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#20849;&#21516;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;CARE&#65289;&#65292;&#36890;&#36807;&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#21644;&#21452;&#21521;&#20132;&#20114;&#26469;&#35299;&#20915;&#29305;&#24449;&#28151;&#28102;&#21644;&#23376;&#20219;&#21153;&#20132;&#20114;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#26159;&#20449;&#24687;&#25277;&#21462;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#21253;&#25324;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#32852;&#21512;&#25277;&#21462;&#26041;&#27861;&#37117;&#23384;&#22312;&#29305;&#24449;&#28151;&#28102;&#25110;&#20004;&#20010;&#23376;&#20219;&#21153;&#20043;&#38388;&#20132;&#20114;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#20849;&#21516;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;CARE&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23398;&#20064;&#27599;&#20010;&#23376;&#20219;&#21153;&#30340;&#20998;&#31163;&#34920;&#31034;&#65292;&#26088;&#22312;&#36991;&#20813;&#29305;&#24449;&#37325;&#21472;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20849;&#21516;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#23427;&#25429;&#25417;&#20004;&#20010;&#23376;&#20219;&#21153;&#20043;&#38388;&#30340;&#21452;&#21521;&#20132;&#20114;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#23454;&#20307;&#20449;&#24687;&#36827;&#34892;&#20851;&#31995;&#39044;&#27979;&#65292;&#21453;&#20043;&#20134;&#28982;&#65292;&#20174;&#32780;&#20419;&#36827;&#30456;&#20114;&#22686;&#24378;&#12290;&#22312;&#19977;&#20010;&#32852;&#21512;&#23454;&#20307;-&#20851;&#31995;&#25277;&#21462;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;NYT&#12289;WebNLG&#21644;SciERC&#65289;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Joint entity and relation extraction is the fundamental task of information extraction, consisting of two subtasks: named entity recognition and relation extraction. Most existing joint extraction methods suffer from issues of feature confusion or inadequate interaction between two subtasks. In this work, we propose a Co-Attention network for joint entity and Relation Extraction (CARE). Our approach involves learning separate representations for each subtask, aiming to avoid feature overlap. At the core of our approach is the co-attention module that captures two-way interaction between two subtasks, allowing the model to leverage entity information for relation prediction and vice versa, thus promoting mutual enhancement. Extensive experiments on three joint entity-relation extraction benchmark datasets (NYT, WebNLG and SciERC) show that our proposed model achieves superior performance, surpassing existing baseline models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;JuDec&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#20102;&#33258;&#25105;&#21028;&#26029;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#20316;&#20026;&#33258;&#20027;&#20915;&#31574;&#32773;&#23454;&#29616;&#33258;&#20027;&#21028;&#26029;&#21644;&#20915;&#31574;&#25506;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;JuDec&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#25552;&#39640;&#20102;&#36890;&#36807;&#29575;&#24182;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.12519</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#33258;&#20027;&#20915;&#31574;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as Autonomous Decision Maker. (arXiv:2308.12519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;JuDec&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#20102;&#33258;&#25105;&#21028;&#26029;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#20316;&#20026;&#33258;&#20027;&#20915;&#31574;&#32773;&#23454;&#29616;&#33258;&#20027;&#21028;&#26029;&#21644;&#20915;&#31574;&#25506;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;JuDec&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#25552;&#39640;&#20102;&#36890;&#36807;&#29575;&#24182;&#38477;&#20302;&#20102;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#26102;&#20173;&#20005;&#37325;&#20381;&#36182;&#20110;&#19987;&#23478;&#30693;&#35782;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#21457;&#25381;LLMs&#20316;&#20026;&#33258;&#20027;&#20915;&#31574;&#32773;&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;JuDec&#30340;&#26041;&#27861;&#65292;&#36171;&#20104;LLMs&#33258;&#25105;&#21028;&#26029;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#23454;&#29616;&#33258;&#20027;&#21028;&#26029;&#21644;&#20915;&#31574;&#25506;&#32034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;JuDec&#20013;&#65292;&#35774;&#35745;&#20102;&#22522;&#20110;Elo&#30340;&#33258;&#25105;&#21028;&#26029;&#26426;&#21046;&#65292;&#36890;&#36807;&#23545;&#20004;&#20010;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#37197;&#23545;&#27604;&#36739;&#65292;&#20026;&#20915;&#31574;&#27493;&#39588;&#20998;&#37197;Elo&#20998;&#25968;&#65292;&#20197;&#21028;&#26029;&#23427;&#20204;&#30340;&#20215;&#20540;&#21644;&#25928;&#29992;&#65292;&#24182;&#30456;&#24212;&#22320;&#24341;&#23548;&#20915;&#31574;&#25628;&#32034;&#36807;&#31243;&#26397;&#21521;&#26368;&#20248;&#35299;&#12290;&#22312;ToolBench&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;JuDec&#30456;&#23545;&#20110;&#22522;&#20934;&#27169;&#22411;&#20855;&#26377;&#20248;&#21183;&#65292;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#36890;&#36807;&#29575;&#25552;&#39640;&#20102;10%&#20197;&#19978;&#12290;&#23427;&#25552;&#20379;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#38477;&#20302;&#20102;&#25104;&#26412;(ChatGPT API&#35843;&#29992;)&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) exhibit impressive language understanding and in-context learning abilities, their decision-making ability still heavily relies on the guidance of task-specific expert knowledge when solving real-world tasks. To unleash the potential of LLMs as autonomous decision makers, this paper presents an approach JuDec to endow LLMs with the self-judgment ability, enabling LLMs to achieve autonomous judgment and exploration for decision making. Specifically, in JuDec, Elo-based Self-Judgment Mechanism is designed to assign Elo scores to decision steps to judge their values and utilities via pairwise comparisons between two solutions and then guide the decision-searching process toward the optimal solution accordingly. Experimental results on the ToolBench dataset demonstrate JuDec's superiority over baselines, achieving over 10% improvement in Pass Rate on diverse tasks. It offers higher-quality solutions and reduces costs (ChatGPT API calls), highlighting its 
&lt;/p&gt;</description></item><item><title>MultiPA&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#38381;&#21512;&#21644;&#24320;&#25918;&#21709;&#24212;&#22330;&#26223;&#30340;&#22810;&#20219;&#21153;&#35821;&#38899;&#21457;&#38899;&#35780;&#20272;&#31995;&#32479;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#31995;&#32479;&#65292;&#23427;&#20855;&#26377;&#26356;&#31616;&#21333;&#30340;&#26684;&#24335;&#35201;&#27714;&#21644;&#26356;&#22909;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#35780;&#20272;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2308.12490</link><description>&lt;p&gt;
MultiPA:&#19968;&#31181;&#36866;&#29992;&#20110;&#38381;&#21512;&#21644;&#24320;&#25918;&#21709;&#24212;&#22330;&#26223;&#30340;&#22810;&#20219;&#21153;&#35821;&#38899;&#21457;&#38899;&#35780;&#20272;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MultiPA: a multi-task speech pronunciation assessment system for a closed and open response scenario. (arXiv:2308.12490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12490
&lt;/p&gt;
&lt;p&gt;
MultiPA&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#38381;&#21512;&#21644;&#24320;&#25918;&#21709;&#24212;&#22330;&#26223;&#30340;&#22810;&#20219;&#21153;&#35821;&#38899;&#21457;&#38899;&#35780;&#20272;&#31995;&#32479;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#31995;&#32479;&#65292;&#23427;&#20855;&#26377;&#26356;&#31616;&#21333;&#30340;&#26684;&#24335;&#35201;&#27714;&#21644;&#26356;&#22909;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#35780;&#20272;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#21457;&#38899;&#35780;&#20272;&#31995;&#32479;&#30340;&#35774;&#35745;&#21487;&#20197;&#20998;&#20026;&#38381;&#21512;&#21644;&#24320;&#25918;&#21709;&#24212;&#22330;&#26223;&#65292;&#27599;&#31181;&#22330;&#26223;&#37117;&#26377;&#20854;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#20855;&#22791;&#22312;&#20004;&#31181;&#22330;&#26223;&#19979;&#37117;&#33021;&#21457;&#25381;&#20316;&#29992;&#30340;&#31995;&#32479;&#33021;&#22815;&#28385;&#36275;&#22810;&#26679;&#21270;&#30340;&#23398;&#20064;&#38656;&#27714;&#65292;&#24182;&#25552;&#20379;&#26356;&#31934;&#30830;&#12289;&#20840;&#38754;&#30340;&#21457;&#38899;&#25216;&#33021;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MultiPA&#30340;&#22810;&#20219;&#21153;&#21457;&#38899;&#35780;&#20272;&#27169;&#22411;&#12290;MultiPA&#19982;&#22522;&#20110;Kaldi&#30340;&#31995;&#32479;&#30456;&#27604;&#65292;&#20855;&#26377;&#26356;&#31616;&#21333;&#30340;&#26684;&#24335;&#35201;&#27714;&#65292;&#26356;&#22909;&#22320;&#20860;&#23481;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#19982;&#20808;&#21069;&#30340;&#24320;&#25918;&#21709;&#24212;&#31995;&#32479;&#30456;&#27604;&#65292;MultiPA&#25552;&#20379;&#20102;&#26356;&#24191;&#27867;&#30340;&#35780;&#20272;&#33539;&#22260;&#65292;&#21253;&#25324;&#21477;&#23376;&#21644;&#21333;&#35789;&#32423;&#21035;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MultiPA&#22312;&#38381;&#21512;&#21709;&#24212;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#19982;&#20808;&#21069;&#31995;&#32479;&#30456;&#24403;&#65292;&#24182;&#22312;&#30452;&#25509;&#29992;&#20110;&#24320;&#25918;&#21709;&#24212;&#26102;&#20445;&#25345;&#26356;&#31283;&#20581;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The design of automatic speech pronunciation assessment can be categorized into closed and open response scenarios, each with strengths and limitations. A system with the ability to function in both scenarios can cater to diverse learning needs and provide a more precise and holistic assessment of pronunciation skills. In this study, we propose a Multi-task Pronunciation Assessment model called MultiPA. MultiPA provides an alternative to Kaldi-based systems in that it has simpler format requirements and better compatibility with other neural network models. Compared with previous open response systems, MultiPA provides a wider range of evaluations, encompassing assessments at both the sentence and word-level. Our experimental results show that MultiPA achieves comparable performance when working in closed response scenarios and maintains more robust performance when directly used for open responses.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;ChatGPT&#21644;GPT-4&#30340;&#20808;&#21069;&#35780;&#20272;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#20851;&#27880;&#20854;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#12289;&#31185;&#23398;&#30693;&#35782;&#21644;&#20262;&#29702;&#32771;&#34385;&#65292;&#25552;&#20986;&#20102;&#20960;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.12488</link><description>&lt;p&gt;
GPTEval: &#23545;ChatGPT&#21644;GPT-4&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
GPTEval: A Survey on Assessments of ChatGPT and GPT-4. (arXiv:2308.12488v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ChatGPT&#21644;GPT-4&#30340;&#20808;&#21069;&#35780;&#20272;&#36827;&#34892;&#20102;&#32508;&#21512;&#20998;&#26512;&#65292;&#20851;&#27880;&#20854;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#12289;&#31185;&#23398;&#30693;&#35782;&#21644;&#20262;&#29702;&#32771;&#34385;&#65292;&#25552;&#20986;&#20102;&#20960;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#23186;&#20307;&#23545;&#20854;&#25200;&#20081;&#31038;&#20250;&#21644;&#32463;&#27982;&#31995;&#32479;&#28508;&#21147;&#30340;&#35768;&#22810;&#29468;&#27979;&#12290;&#20854;&#24778;&#20154;&#30340;&#35821;&#35328;&#33021;&#21147;&#28608;&#36215;&#23398;&#32773;&#20204;&#23545;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#34920;&#29616;&#30340;&#27987;&#21402;&#20852;&#36259;&#12290;&#24050;&#32463;&#26377;&#35768;&#22810;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#21644;GPT-4&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#23398;&#31185;&#20013;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#39033;&#32508;&#21512;&#24615;&#30340;&#32508;&#36848;&#24635;&#32467;&#38598;&#20307;&#35780;&#20272;&#32467;&#26524;&#12290;&#26412;&#35843;&#26597;&#30340;&#30446;&#26631;&#26159;&#23545;ChatGPT&#21644;GPT-4&#30340;&#20808;&#21069;&#35780;&#20272;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#12289;&#31185;&#23398;&#30693;&#35782;&#21644;&#20262;&#29702;&#32771;&#34385;&#12290;&#27492;&#22806;&#65292;&#23545;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#20102;&#26816;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of ChatGPT has generated much speculation in the press about its potential to disrupt social and economic systems. Its astonishing language ability has aroused strong curiosity among scholars about its performance in different domains. There have been many studies evaluating the ability of ChatGPT and GPT-4 in different tasks and disciplines. However, a comprehensive review summarizing the collective assessment findings is lacking. The objective of this survey is to thoroughly analyze prior assessments of ChatGPT and GPT-4, focusing on its language and reasoning abilities, scientific knowledge, and ethical considerations. Furthermore, an examination of the existing evaluation methods is conducted, offering several recommendations for future research in evaluating large language models.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#20174;&#21382;&#21490;&#32654;&#22269;&#25253;&#32440;&#22270;&#20687;&#20013;&#25552;&#21462;&#23436;&#25972;&#30340;&#25991;&#31456;&#25991;&#26412;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#24067;&#23616;&#35782;&#21035;&#21644;OCR&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#39640;&#25928;&#30340;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#25193;&#23637;&#24615;&#65292;&#24182;&#21019;&#24314;&#20102;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#21319;&#23545;&#21382;&#21490;&#33521;&#35821;&#21644;&#21382;&#21490;&#19990;&#30028;&#30693;&#35782;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2308.12477</link><description>&lt;p&gt;
&#32654;&#22269;&#25925;&#20107;&#65306;&#19968;&#31181;&#22522;&#20110;&#21382;&#21490;&#32654;&#22269;&#25253;&#32440;&#30340;&#22823;&#35268;&#27169;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
American Stories: A Large-Scale Structured Text Dataset of Historical U.S. Newspapers. (arXiv:2308.12477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12477
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#20174;&#21382;&#21490;&#32654;&#22269;&#25253;&#32440;&#22270;&#20687;&#20013;&#25552;&#21462;&#23436;&#25972;&#30340;&#25991;&#31456;&#25991;&#26412;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#24067;&#23616;&#35782;&#21035;&#21644;OCR&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#26500;&#24314;&#39640;&#25928;&#30340;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#39640;&#25193;&#23637;&#24615;&#65292;&#24182;&#21019;&#24314;&#20102;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#21319;&#23545;&#21382;&#21490;&#33521;&#35821;&#21644;&#21382;&#21490;&#19990;&#30028;&#30693;&#35782;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#32654;&#22269;&#20844;&#20849;&#39046;&#22495;&#25253;&#32440;&#20840;&#25991;&#25968;&#25454;&#38598;&#27809;&#26377;&#35782;&#21035;&#25253;&#32440;&#25195;&#25551;&#30340;&#22797;&#26434;&#24067;&#23616;&#65292;&#32467;&#26524;&#23548;&#33268;&#25968;&#23383;&#21270;&#20869;&#23481;&#23545;&#25991;&#31456;&#12289;&#26631;&#39064;&#12289;&#23383;&#24149;&#12289;&#24191;&#21578;&#31561;&#24067;&#23616;&#21306;&#22495;&#30340;&#25991;&#26412;&#36827;&#34892;&#20102;&#28151;&#21512;&#12290;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#30340;&#36136;&#37327;&#20063;&#21487;&#33021;&#36739;&#20302;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#20174;&#25253;&#32440;&#22270;&#20687;&#20013;&#25552;&#21462;&#23436;&#25972;&#30340;&#25991;&#31456;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#32654;&#22269;&#22269;&#20250;&#22270;&#20070;&#39302;&#20844;&#20849;&#39046;&#22495;&#12298;&#24930;&#24615;&#32654;&#22269;&#12299;&#38598;&#21512;&#20013;&#30340;&#36817;2000&#19975;&#20221;&#25195;&#25551;&#12290;&#35813;&#27969;&#27700;&#32447;&#21253;&#25324;&#24067;&#23616;&#26816;&#27979;&#12289;&#21487;&#35835;&#24615;&#20998;&#31867;&#12289;&#33258;&#23450;&#20041;OCR&#21644;&#36328;&#22810;&#20010;&#36793;&#30028;&#26694;&#20851;&#32852;&#25991;&#31456;&#25991;&#26412;&#31561;&#27493;&#39588;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25193;&#23637;&#24615;&#65292;&#23427;&#37319;&#29992;&#20102;&#19987;&#20026;&#31227;&#21160;&#30005;&#35805;&#35774;&#35745;&#30340;&#39640;&#25928;&#26550;&#26500;&#12290;&#32467;&#26524;&#20135;&#29983;&#30340;&#32654;&#22269;&#25925;&#20107;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#29992;&#20110;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#23545;&#21382;&#21490;&#33521;&#35821;&#21644;&#21382;&#21490;&#19990;&#30028;&#30693;&#35782;&#30340;&#26356;&#22909;&#29702;&#35299;&#12290;&#35813;&#25968;&#25454;&#38598;&#36824;&#21487;&#20197;&#28155;&#21152;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Existing full text datasets of U.S. public domain newspapers do not recognize the often complex layouts of newspaper scans, and as a result the digitized content scrambles texts from articles, headlines, captions, advertisements, and other layout regions. OCR quality can also be low. This study develops a novel, deep learning pipeline for extracting full article texts from newspaper images and applies it to the nearly 20 million scans in Library of Congress's public domain Chronicling America collection. The pipeline includes layout detection, legibility classification, custom OCR, and association of article texts spanning multiple bounding boxes. To achieve high scalability, it is built with efficient architectures designed for mobile phones. The resulting American Stories dataset provides high quality data that could be used for pre-training a large language model to achieve better understanding of historical English and historical world knowledge. The dataset could also be added to 
&lt;/p&gt;</description></item><item><title>ChatGPT&#21644;GPT-4&#22312;&#25169;&#20811;&#20013;&#26174;&#31034;&#20986;&#39640;&#32423;&#29702;&#35299;&#65292;&#20294;&#19981;&#26159;&#28216;&#25103;&#35770;&#29702;&#26368;&#20248;&#30340;&#25169;&#20811;&#29609;&#23478;&#12290;&#23545;&#27169;&#22411;&#21442;&#25968;&#21644;&#25552;&#31034;&#30340;&#20248;&#21270;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#22312;&#25169;&#20811;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.12466</link><description>&lt;p&gt;
ChatGPT&#21644;GPT-4&#26159;&#20248;&#31168;&#30340;&#25169;&#20811;&#29609;&#23478;&#21527;&#65311;&#8212;&#8212;&#19968;&#39033;Pre-Flop&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are ChatGPT and GPT-4 Good Poker Players? -- A Pre-Flop Analysis. (arXiv:2308.12466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12466
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#21644;GPT-4&#22312;&#25169;&#20811;&#20013;&#26174;&#31034;&#20986;&#39640;&#32423;&#29702;&#35299;&#65292;&#20294;&#19981;&#26159;&#28216;&#25103;&#35770;&#29702;&#26368;&#20248;&#30340;&#25169;&#20811;&#29609;&#23478;&#12290;&#23545;&#27169;&#22411;&#21442;&#25968;&#21644;&#25552;&#31034;&#30340;&#20248;&#21270;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#22312;&#25169;&#20811;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;ChatGPT&#21644;GPT-4&#38382;&#19990;&#20197;&#26469;&#65292;&#36825;&#20123;&#27169;&#22411;&#24050;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#29087;&#32451;&#31243;&#24230;&#26159;&#26174;&#32780;&#26131;&#35265;&#30340;&#65292;&#20294;&#23427;&#20204;&#22312;&#28216;&#25103;&#20013;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#25169;&#20811;&#39046;&#22495;&#30340;&#33021;&#21147;&#65292;&#36824;&#26410;&#34987;&#25506;&#32034;&#12290;&#25169;&#20811;&#26159;&#19968;&#31181;&#38656;&#35201;&#22312;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#23436;&#20840;&#20449;&#24687;&#19979;&#20570;&#20986;&#20915;&#31574;&#30340;&#28216;&#25103;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;ChatGPT&#21644;GPT-4&#36827;&#34892;&#20102;&#25169;&#20811;&#27979;&#35797;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#30340;&#25169;&#20811;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;&#36825;&#20004;&#20010;&#27169;&#22411;&#37117;&#23637;&#31034;&#20102;&#23545;&#25169;&#20811;&#30340;&#39640;&#32423;&#29702;&#35299;&#65292;&#21253;&#25324;&#36215;&#22987;&#25163;&#29260;&#30340;&#20272;&#20540;&#12289;&#25171;&#29260;&#20301;&#32622;&#20197;&#21450;&#28216;&#25103;&#35770;&#29702;&#26368;&#20248;(GTO)&#25169;&#20811;&#30340;&#20854;&#20182;&#22797;&#26434;&#24615;&#65292;&#20294;ChatGPT&#21644;GPT-4&#24182;&#19981;&#26159;&#28216;&#25103;&#35770;&#29702;&#26368;&#20248;&#30340;&#25169;&#20811;&#29609;&#23478;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#20102;&#19982;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#29609;&#25169;&#20811;&#30456;&#20851;&#30340;&#26368;&#20339;&#25552;&#31034;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#29305;&#24449;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#36825;&#20004;&#20010;&#27169;&#22411;&#20855;&#26377;&#19981;&#21516;&#30340;&#25171;&#29260;&#39118;&#26684;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65306;GPT-4&#26159;
&lt;/p&gt;
&lt;p&gt;
Since the introduction of ChatGPT and GPT-4, these models have been tested across a large number of tasks. Their adeptness across domains is evident, but their aptitude in playing games and specifically their aptitude in the realm of poker has remained unexplored. Poker is a game that requires decision making under uncertainty and incomplete information. In this paper, we put ChatGPT and GPT-4 through the poker test and evaluate their poker skills. Our findings reveal that while both models display an advanced understanding of poker, encompassing concepts like the valuation of starting hands, playing positions and other intricacies of game theory optimal (GTO) poker, both ChatGPT and GPT-4 are NOT game theory optimal poker players.  Through a series of experiments, we first discover the characteristics of optimal prompts and model parameters for playing poker with these models. Our observations then unveil the distinct playing personas of the two models. We first conclude that GPT-4 is
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;NLP&#20998;&#26512;&#20102;ESG&#20027;&#23548;&#30340;DLT&#30740;&#31350;&#30340;&#28436;&#21270;&#65292;&#36890;&#36807;&#26500;&#24314;&#24341;&#29992;&#32593;&#32476;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#23545;DLT&#22312;ESG&#32972;&#26223;&#19979;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.12420</link><description>&lt;p&gt;
ESG&#20027;&#23548;&#30340;DLT&#30740;&#31350;&#30340;&#28436;&#21270;&#65306;&#23545;&#25991;&#29486;&#36827;&#34892;NLP&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature. (arXiv:2308.12420v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;NLP&#20998;&#26512;&#20102;ESG&#20027;&#23548;&#30340;DLT&#30740;&#31350;&#30340;&#28436;&#21270;&#65292;&#36890;&#36807;&#26500;&#24314;&#24341;&#29992;&#32593;&#32476;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#23545;DLT&#22312;ESG&#32972;&#26223;&#19979;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#36134;&#26412;&#25216;&#26415;(DLT)&#36805;&#36895;&#21457;&#23637;&#65292;&#38656;&#35201;&#20840;&#38754;&#20102;&#35299;&#20854;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;DLT&#30340;&#29615;&#22659;&#12289;&#21487;&#25345;&#32493;&#24615;&#21644;&#27835;&#29702;(ESG)&#32452;&#25104;&#37096;&#20998;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#36824;&#19981;&#36275;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;107&#31687;&#31181;&#23376;&#25991;&#29486;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;63,083&#20010;&#21442;&#32771;&#25991;&#29486;&#30340;&#24341;&#29992;&#32593;&#32476;&#65292;&#24182;&#23558;&#20854;&#31934;&#28860;&#20026;24,539&#31687;&#25991;&#29486;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#19968;&#20010;&#24050;&#24314;&#31435;&#30340;&#25216;&#26415;&#20998;&#31867;&#27861;&#20174;46&#31687;&#35770;&#25991;&#20013;&#26631;&#35760;&#20102;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#36890;&#36807;&#25214;&#20986;DLT&#30340;ESG&#35201;&#32032;&#26469;&#23436;&#21892;&#36825;&#20010;&#20998;&#31867;&#27861;&#12290;&#21033;&#29992;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#32454;&#21270;&#35843;&#25972;&#65292;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#20351;&#29992;&#25105;&#20204;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#35843;&#25972;&#21518;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#31934;&#31616;&#65292;&#24471;&#21040;&#20102;505&#31687;&#20851;&#38190;&#35770;&#25991;&#65292;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#21644;&#26102;&#38388;&#22270;&#20998;&#26512;&#65292;&#20419;&#36827;&#20102;&#23545;DLT&#22312;ESG&#32972;&#26223;&#19979;&#30340;&#28436;&#21270;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating comprehensive insights into their diverse components. However, a systematic literature review that emphasizes the Environmental, Sustainability, and Governance (ESG) components of DLT remains lacking. To bridge this gap, we selected 107 seed papers to build a citation network of 63,083 references and refined it to a corpus of 24,539 publications for analysis. Then, we labeled the named entities in 46 papers according to twelve top-level categories derived from an established technology taxonomy and enhanced the taxonomy by pinpointing DLT's ESG elements. Leveraging transformer-based language models, we fine-tuned a pre-trained language model for a Named Entity Recognition (NER) task using our labeled dataset. We used our fine-tuned language model to distill the corpus to 505 key papers, facilitating a literature review via named entities and temporal graph analysis on DLT evolution in the context of ESG. Our con
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#21160;&#25163;&#35821;&#22788;&#29702;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#25163;&#25351;&#25340;&#20889;&#35782;&#21035;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.12419</link><description>&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#32654;&#22269;&#25163;&#35821;&#22788;&#29702;:&#25968;&#25454;&#12289;&#20219;&#21153;&#21644;&#26041;&#27861;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Toward American Sign Language Processing in the Real World: Data, Tasks, and Methods. (arXiv:2308.12419v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#21160;&#25163;&#35821;&#22788;&#29702;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#25163;&#25351;&#25340;&#20889;&#35782;&#21035;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#26159;&#32843;&#20154;&#20043;&#38388;&#36890;&#36807;&#25163;&#21183;&#20256;&#36798;&#24847;&#24605;&#30340;&#20027;&#35201;&#20132;&#27969;&#26041;&#24335;&#12290;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#35782;&#21035;&#25163;&#35821;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#65292;&#22914;&#20809;&#29031;&#12289;&#32972;&#26223;&#26434;&#20081;&#21644;&#25163;&#21183;&#32773;&#29305;&#24449;&#30340;&#21464;&#21270;&#31561;&#12290;&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#30340;&#33258;&#21160;&#25163;&#35821;&#22788;&#29702;&#65292;&#20351;&#29992;&#20102;&#20174;&#20114;&#32852;&#32593;&#25910;&#38598;&#30340;&#25163;&#35821;&#35270;&#39057;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#12289;&#20219;&#21153;&#21644;&#26041;&#27861;&#12290;&#22823;&#37096;&#20998;&#31456;&#33410;&#28041;&#21450;&#25163;&#25351;&#25340;&#20889;&#20219;&#21153;&#65292;&#36825;&#26159;&#25163;&#35821;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#20043;&#21069;&#30340;&#30740;&#31350;&#24182;&#27809;&#26377;&#24191;&#27867;&#30740;&#31350;&#12290;&#25105;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#25163;&#35821;&#25968;&#25454;&#38598;&#65306;ChicagoFSWild&#12289;ChicagoFSWild+&#21644;OpenASL&#12290;&#20351;&#29992;ChicagoFSWild&#21644;ChicagoFSWild+&#65292;&#25105;&#35299;&#20915;&#20102;&#25163;&#25351;&#25340;&#20889;&#35782;&#21035;&#38382;&#39064;&#65292;&#21363;&#23558;&#25163;&#25351;&#25340;&#20889;&#24207;&#21015;&#36716;&#24405;&#20026;&#25991;&#26412;&#12290;&#25105;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#27880;&#24847;&#21147;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#21407;&#22987;&#35270;&#39057;&#20013;&#35782;&#21035;&#25163;&#25351;&#25340;&#20889;&#65292;&#32780;&#19981;&#38656;&#35201;&#26174;&#24335;&#25163;&#37096;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sign language, which conveys meaning through gestures, is the chief means of communication among deaf people. Recognizing sign language in natural settings presents significant challenges due to factors such as lighting, background clutter, and variations in signer characteristics. In this thesis, I study automatic sign language processing in the wild, using signing videos collected from the Internet. This thesis contributes new datasets, tasks, and methods. Most chapters of this thesis address tasks related to fingerspelling, an important component of sign language and yet has not been studied widely by prior work. I present three new large-scale ASL datasets in the wild: ChicagoFSWild, ChicagoFSWild+, and OpenASL. Using ChicagoFSWild and ChicagoFSWild+, I address fingerspelling recognition, which consists of transcribing fingerspelling sequences into text. I propose an end-to-end approach based on iterative attention that allows recognition from a raw video without explicit hand dete
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20856;&#22411;&#35760;&#24518;&#32593;&#32476;&#65292;&#29992;&#20110;&#25552;&#21462;&#22270;&#20687;&#30340;&#35821;&#20041;&#24182;&#29983;&#25104;&#35821;&#20041;&#36830;&#36143;&#30340;&#25551;&#36848;&#12290;&#36890;&#36807;&#22312;&#22788;&#29702;&#20854;&#20182;&#35757;&#32451;&#26679;&#26412;&#26102;&#25191;&#34892;&#27880;&#24847;&#21147;&#25805;&#20316;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#21033;&#29992;&#20808;&#21069;&#28608;&#27963;&#30340;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#27169;&#22411;&#21644;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.12383</link><description>&lt;p&gt;
&#36890;&#36807;&#36807;&#21435;&#30340;&#35760;&#24518;&#65306;&#29992;&#20110;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#20856;&#22411;&#35760;&#24518;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning. (arXiv:2308.12383v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20856;&#22411;&#35760;&#24518;&#32593;&#32476;&#65292;&#29992;&#20110;&#25552;&#21462;&#22270;&#20687;&#30340;&#35821;&#20041;&#24182;&#29983;&#25104;&#35821;&#20041;&#36830;&#36143;&#30340;&#25551;&#36848;&#12290;&#36890;&#36807;&#22312;&#22788;&#29702;&#20854;&#20182;&#35757;&#32451;&#26679;&#26412;&#26102;&#25191;&#34892;&#27880;&#24847;&#21147;&#25805;&#20316;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#21033;&#29992;&#20808;&#21069;&#28608;&#27963;&#30340;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;COCO&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#27169;&#22411;&#21644;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26159;&#19968;&#20010;&#28041;&#21450;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#20219;&#21153;&#65292;&#30446;&#21069;&#20381;&#36182;&#20110;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#26469;&#25552;&#21462;&#22270;&#20687;&#30340;&#35821;&#20041;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#35821;&#20041;&#36830;&#36143;&#30340;&#25551;&#36848;&#12290;&#23613;&#31649;&#25104;&#21151;&#65292;&#20294;&#27880;&#24847;&#21147;&#26426;&#21046;&#21482;&#32771;&#34385;&#24403;&#21069;&#36755;&#20837;&#26679;&#26412;&#30340;&#25237;&#24433;&#21152;&#26435;&#27714;&#21644;&#65292;&#22240;&#27492;&#24573;&#30053;&#20102;&#26469;&#33258;&#20854;&#20182;&#26679;&#26412;&#30340;&#30456;&#20851;&#35821;&#20041;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#32593;&#32476;&#65292;&#35813;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#20856;&#22411;&#35760;&#24518;&#27169;&#22411;&#22312;&#22788;&#29702;&#20854;&#20182;&#35757;&#32451;&#26679;&#26412;&#26102;&#25191;&#34892;&#27880;&#24847;&#21147;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#35760;&#24518;&#36890;&#36807;&#21407;&#22411;&#21521;&#37327;&#30340;&#23450;&#20041;&#26469;&#24314;&#27169;&#36807;&#21435;&#30340;&#38190;&#21644;&#20540;&#30340;&#20998;&#24067;&#65292;&#36825;&#20123;&#21407;&#22411;&#21521;&#37327;&#26082;&#20855;&#26377;&#21306;&#20998;&#24615;&#21448;&#32039;&#20945;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#27169;&#22411;&#19982;&#31934;&#24515;&#35774;&#35745;&#30340;&#22522;&#20934;&#27169;&#22411;&#21644;&#26368;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#30740;&#31350;&#27599;&#20010;&#25552;&#20986;&#30340;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image captioning, like many tasks involving vision and language, currently relies on Transformer-based architectures for extracting the semantics in an image and translating it into linguistically coherent descriptions. Although successful, the attention operator only considers a weighted summation of projections of the current input sample, therefore ignoring the relevant semantic information which can come from the joint observation of other samples. In this paper, we devise a network which can perform attention over activations obtained while processing other training samples, through a prototypical memory model. Our memory models the distribution of past keys and values through the definition of prototype vectors which are both discriminative and compact. Experimentally, we assess the performance of the proposed model on the COCO dataset, in comparison with carefully designed baselines and state-of-the-art approaches, and by investigating the role of each of the proposed components
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20174;&#22995;&#21517;&#20013;&#25512;&#26029;&#24615;&#21035;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#24615;&#21035;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26159;&#19968;&#31181;&#21487;&#34892;&#19988;&#24191;&#27867;&#24212;&#29992;&#30340;&#26041;&#27861;&#12290;&#20854;&#37325;&#35201;&#24615;&#22312;&#20110;&#30740;&#31350;&#21508;&#31181;&#31185;&#23398;&#23398;&#31185;&#20013;&#23545;&#24615;&#21035;&#24046;&#24322;&#30340;&#27169;&#24335;&#21644;&#20915;&#23450;&#22240;&#32032;&#36827;&#34892;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.12381</link><description>&lt;p&gt;
&#20174;&#22995;&#21517;&#20013;&#25512;&#26029;&#24615;&#21035;&#65306;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24615;&#33021;&#35780;&#20272;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Inferring gender from name: a large scale performance evaluation study. (arXiv:2308.12381v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20174;&#22995;&#21517;&#20013;&#25512;&#26029;&#24615;&#21035;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#22312;&#27809;&#26377;&#24615;&#21035;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#26159;&#19968;&#31181;&#21487;&#34892;&#19988;&#24191;&#27867;&#24212;&#29992;&#30340;&#26041;&#27861;&#12290;&#20854;&#37325;&#35201;&#24615;&#22312;&#20110;&#30740;&#31350;&#21508;&#31181;&#31185;&#23398;&#23398;&#31185;&#20013;&#23545;&#24615;&#21035;&#24046;&#24322;&#30340;&#27169;&#24335;&#21644;&#20915;&#23450;&#22240;&#32032;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#20154;&#30340;&#24615;&#21035;&#26159;&#22312;&#36827;&#34892;&#21307;&#23398;&#12289;&#31038;&#20250;&#23398;&#12289;&#25919;&#27835;&#23398;&#21644;&#32463;&#27982;&#23398;&#31561;&#21508;&#31181;&#31185;&#23398;&#23398;&#31185;&#30340;&#30740;&#31350;&#26102;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#25968;&#25454;&#30340;&#28608;&#22686;&#65292;&#24615;&#21035;&#20449;&#24687;&#30340;&#33719;&#21462;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#30740;&#31350;&#20154;&#21592;&#38656;&#35201;&#20174;&#21487;&#33719;&#24471;&#30340;&#20449;&#24687;&#20013;&#65292;&#20027;&#35201;&#26159;&#20174;&#20154;&#21517;&#20013;&#25512;&#26029;&#24615;&#21035;&#12290;&#23613;&#31649;&#36890;&#36807;&#22995;&#21517;&#26469;&#25512;&#26029;&#24615;&#21035;&#21487;&#33021;&#24341;&#21457;&#19968;&#20123;&#20262;&#29702;&#38382;&#39064;&#65292;&#20294;&#32570;&#20047;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#27861;&#24847;&#21619;&#30528;&#30740;&#31350;&#20154;&#21592;&#19981;&#24471;&#19981;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#24403;&#30446;&#26631;&#20351;&#25163;&#27573;&#21512;&#29702;&#26102;-&#22312;&#22823;&#22810;&#25968;&#36825;&#31867;&#30740;&#31350;&#20013;&#65292;&#30446;&#26631;&#26159;&#30740;&#31350;&#24615;&#21035;&#24046;&#24322;&#30340;&#27169;&#24335;&#21644;&#20915;&#23450;&#22240;&#32032;&#12290;&#22995;&#21517;&#21040;&#24615;&#21035;&#25512;&#26029;&#30340;&#24517;&#35201;&#24615;&#20135;&#29983;&#20102;&#19968;&#20010;&#36234;&#26469;&#36234;&#22810;&#30340;&#31639;&#27861;&#26041;&#27861;&#21644;&#36719;&#20214;&#20135;&#21697;&#30340;&#39046;&#22495;&#12290;&#36825;&#20123;&#26041;&#27861;&#24050;&#32463;&#22312;&#19990;&#30028;&#21508;&#22320;&#30340;&#23398;&#26415;&#30028;&#12289;&#24037;&#19994;&#30028;&#12289;&#25919;&#24220;&#21644;&#38750;&#25919;&#24220;&#32452;&#32455;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A person's gender is a crucial piece of information when performing research across a wide range of scientific disciplines, such as medicine, sociology, political science, and economics, to name a few. However, in increasing instances, especially given the proliferation of big data, gender information is not readily available. In such cases researchers need to infer gender from readily available information, primarily from persons' names. While inferring gender from name may raise some ethical questions, the lack of viable alternatives means that researchers have to resort to such approaches when the goal justifies the means - in the majority of such studies the goal is to examine patterns and determinants of gender disparities. The necessity of name-to-gender inference has generated an ever-growing domain of algorithmic approaches and software products. These approaches have been used throughout the world in academia, industry, governmental and non-governmental organizations. Neverthe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36866;&#37197;&#22120;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#21487;&#27867;&#21270;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#22810;&#20010;&#23494;&#38598;&#35270;&#35273;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#20219;&#21153;&#36801;&#31227;&#12289;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#21644;&#27867;&#21270;&#21040;&#26032;&#39046;&#22495;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12372</link><description>&lt;p&gt;
Vision Transformer&#36866;&#37197;&#22120;&#29992;&#20110;&#21487;&#27867;&#21270;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer Adapters for Generalizable Multitask Learning. (arXiv:2308.12372v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12372
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36866;&#37197;&#22120;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#21487;&#27867;&#21270;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#22810;&#20010;&#23494;&#38598;&#35270;&#35273;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#38646;&#26679;&#26412;&#20219;&#21153;&#36801;&#31227;&#12289;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#21644;&#27867;&#21270;&#21040;&#26032;&#39046;&#22495;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#22810;&#20219;&#21153;&#35270;&#35273;&#21464;&#25442;&#22120;&#36866;&#37197;&#22120;&#65292;&#23398;&#20064;&#21487;&#20197;&#24212;&#29992;&#20110;&#26032;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#36890;&#29992;&#20219;&#21153;&#20851;&#32852;&#24615;&#12290;&#23558;&#20854;&#38598;&#25104;&#21040;&#29616;&#25104;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#39592;&#24178;&#20013;&#65292;&#25105;&#20204;&#30340;&#36866;&#37197;&#22120;&#21487;&#20197;&#20197;&#21442;&#25968;&#26377;&#25928;&#30340;&#26041;&#24335;&#21516;&#26102;&#35299;&#20915;&#22810;&#20010;&#23494;&#38598;&#35270;&#35273;&#20219;&#21153;&#65292;&#32780;&#19981;&#20687;&#29616;&#26377;&#30340;&#22810;&#20219;&#21153;&#21464;&#25442;&#22120;&#37027;&#26679;&#20855;&#26377;&#21442;&#25968;&#30340;&#39640;&#26114;&#25104;&#26412;&#12290;&#19982;&#24182;&#34892;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#22312;&#28155;&#21152;&#26032;&#20219;&#21153;&#25110;&#39046;&#22495;&#26102;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#36866;&#37197;&#22120;&#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#20219;&#21153;&#33258;&#36866;&#24212;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#23558;&#22522;&#20110;&#26799;&#24230;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#19982;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#30456;&#20284;&#24615;&#32467;&#21512;&#36215;&#26469;&#12290;&#23398;&#20064;&#30340;&#20219;&#21153;&#20851;&#32852;&#24615;&#22312;&#20197;&#19979;&#24773;&#20917;&#19979;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65306;&#38646;&#26679;&#26412;&#20219;&#21153;&#36801;&#31227;&#65292;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#20197;&#21450;&#19981;&#38656;&#35201;&#23545;&#26032;&#39046;&#22495;&#36827;&#34892;&#24494;&#35843;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#32988;&#36807;&#29616;&#26377;&#30340;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20219;&#21153;&#26041;&#27861;&#65292;&#36824;&#32988;&#36807;&#35270;&#35273;&#21464;&#25442;&#22120;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the first multitasking vision transformer adapters that learn generalizable task affinities which can be applied to novel tasks and domains. Integrated into an off-the-shelf vision transformer backbone, our adapters can simultaneously solve multiple dense vision tasks in a parameter-efficient manner, unlike existing multitasking transformers that are parametrically expensive. In contrast to concurrent methods, we do not require retraining or fine-tuning whenever a new task or domain is added. We introduce a task-adapted attention mechanism within our adapter framework that combines gradient-based task similarities with attention-based ones. The learned task affinities generalize to the following settings: zero-shot task transfer, unsupervised domain adaptation, and generalization without fine-tuning to novel domains. We demonstrate that our approach outperforms not only the existing convolutional neural network-based multitasking methods but also the vision transformer-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#26469;&#20943;&#36731;&#20302;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#24471;&#21040;&#20102;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2308.11764</link><description>&lt;p&gt;
Halo&#65306;&#35780;&#20272;&#21644;&#38477;&#20302;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models. (arXiv:2308.11764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11764
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#23569;&#24320;&#28304;&#24369;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#26469;&#20943;&#36731;&#20302;&#21442;&#25968;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#24471;&#21040;&#20102;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#12290;&#34429;&#28982;&#23545;&#20110;&#30740;&#31350;&#21644;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#26041;&#20415;&#65292;&#20294;&#26159;&#19982;&#20854;&#26356;&#22823;&#35268;&#27169;&#30340;&#23545;&#24212;&#27169;&#22411;&#30456;&#27604;&#65292;&#24320;&#28304;&#30340;&#21442;&#25968;&#36739;&#23569;&#30340;LLMs&#32463;&#24120;&#20986;&#29616;&#20005;&#37325;&#24187;&#35273;&#38382;&#39064;&#12290;&#26412;&#25991;&#30528;&#37325;&#20110;&#27979;&#37327;&#21644;&#20943;&#23569;BLOOM 7B&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#26159;&#20844;&#24320;&#25552;&#20379;&#32473;&#30740;&#31350;&#21644;&#21830;&#19994;&#24212;&#29992;&#30340;&#24369;&#24320;&#28304;LLMs&#30340;&#20195;&#34920;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;HaloCheck&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26080;&#38656;&#30693;&#35782;&#30340;&#40657;&#30418;&#23376;&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;LLMs&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#30693;&#35782;&#27880;&#20837;&#21644;&#24072;&#29983;&#26041;&#27861;&#31561;&#25216;&#26415;&#65292;&#20197;&#20943;&#36731;&#20302;&#21442;&#25968;LLMs&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;LLMs&#30340;&#25361;&#25112;&#24615;&#39046;&#22495;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP). Although convenient for research and practical applications, open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts. This paper focuses on measuring and reducing hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs that are publicly available for research and commercial applications. We introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed to quantify the severity of hallucinations in LLMs. Additionally, we explore techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter LLMs. Our experiments effectively demonstrate the reduction of hallucinations in challenging domains for these LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SeamlessM4T&#65292;&#36825;&#26159;&#19968;&#20010;&#25903;&#25345;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#35821;&#38899;&#25968;&#25454;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#32763;&#35793;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#32763;&#35793;&#65292;&#20197;&#21450;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11596</link><description>&lt;p&gt;
SeamlessM4T-&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
SeamlessM4T-Massively Multilingual &amp; Multimodal Machine Translation. (arXiv:2308.11596v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SeamlessM4T&#65292;&#36825;&#26159;&#19968;&#20010;&#25903;&#25345;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#35821;&#38899;&#25968;&#25454;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#32763;&#35793;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#32763;&#35793;&#65292;&#20197;&#21450;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#36896;&#19968;&#31181;&#31867;&#20284;&#20110;&#24052;&#21035;&#40060;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#20154;&#22312;&#20219;&#24847;&#20004;&#31181;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#35821;&#38899;&#32763;&#35793;&#65292;&#38656;&#35201;&#20184;&#20986;&#20160;&#20040;&#26679;&#30340;&#21162;&#21147;&#65311;&#34429;&#28982;&#26368;&#36817;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#20351;&#26426;&#22120;&#32763;&#35793;&#30340;&#35206;&#30422;&#33539;&#22260;&#36229;&#36807;&#20102;200&#31181;&#35821;&#35328;&#65292;&#20294;&#32479;&#19968;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#36824;&#27809;&#26377;&#21462;&#24471;&#31867;&#20284;&#30340;&#36827;&#23637;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#20256;&#32479;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#20381;&#36182;&#20110;&#28176;&#36827;&#24335;&#30340;&#32423;&#32852;&#31995;&#32479;&#36827;&#34892;&#32763;&#35793;&#65292;&#20351;&#39640;&#24615;&#33021;&#30340;&#32479;&#19968;&#31995;&#32479;&#38590;&#20197;&#23454;&#29616;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SeamlessM4T&#65292;&#19968;&#31181;&#25903;&#25345;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#25991;&#26412;&#32763;&#35793;&#20197;&#21450;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#25903;&#25345;&#22810;&#36798;100&#31181;&#35821;&#35328;&#12290;&#20026;&#20102;&#26500;&#24314;&#36825;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;100&#19975;&#23567;&#26102;&#30340;&#24320;&#25918;&#24335;&#35821;&#38899;&#38899;&#39057;&#25968;&#25454;&#65292;&#20351;&#29992;&#20102;w2v-BERT 2.0&#26469;&#23398;&#20064;&#33258;&#30417;&#30563;&#30340;&#35821;&#38899;&#34920;&#31034;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#33258;&#21160;&#23545;&#40784;&#35821;&#38899;&#32763;&#35793;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
What does it take to create the Babel Fish, a tool that can help individuals translate speech between any two languages? While recent breakthroughs in text-based models have pushed machine translation coverage beyond 200 languages, unified speech-to-speech translation models have yet to achieve similar strides. More specifically, conventional speech-to-speech translation systems rely on cascaded systems that perform translation progressively, putting high-performing unified systems out of reach. To address these gaps, we introduce SeamlessM4T, a single model that supports speech-to-speech translation, speech-to-text translation, text-to-speech translation, text-to-text translation, and automatic speech recognition for up to 100 languages. To build this, we used 1 million hours of open speech audio data to learn self-supervised speech representations with w2v-BERT 2.0. Subsequently, we created a multimodal corpus of automatically aligned speech translations. Filtered and combined with h
&lt;/p&gt;</description></item><item><title>DiagGPT&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25193;&#23637;&#21040;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#35786;&#26029;&#22330;&#26223;&#20013;&#20027;&#21160;&#25552;&#38382;&#21644;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08043</link><description>&lt;p&gt;
DiagGPT:&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue. (arXiv:2308.08043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08043
&lt;/p&gt;
&lt;p&gt;
DiagGPT&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25193;&#23637;&#21040;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#35786;&#26029;&#22330;&#26223;&#20013;&#20027;&#21160;&#25552;&#38382;&#21644;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;ChatGPT&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23637;&#31034;&#20986;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;AI&#27169;&#22411;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#36741;&#21161;&#20154;&#31867;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;AI&#20316;&#20026;&#32842;&#22825;&#20195;&#29702;&#20154;&#30340;&#37325;&#35201;&#24212;&#29992;&#26159;&#22238;&#31572;&#20154;&#31867;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;LLMs&#22312;&#22238;&#31572;&#19968;&#33324;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#29087;&#32451;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#35786;&#26029;&#22330;&#26223;(&#22914;&#27861;&#24459;&#25110;&#21307;&#30103;&#21672;&#35810;)&#20013;&#65292;&#22522;&#26412;&#30340;&#38382;&#31572;&#23545;&#35805;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#38656;&#35201;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;(TOD)&#65292;&#20854;&#20013;AI&#32842;&#22825;&#20195;&#29702;&#38656;&#35201;&#20027;&#21160;&#25552;&#38382;&#24182;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#29305;&#23450;&#20219;&#21153;&#12290;&#20197;&#21069;&#30340;&#24494;&#35843;&#27169;&#22411;&#22312;TOD&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#24403;&#21069;&#30340;LLMs&#24182;&#26410;&#22266;&#26377;&#36825;&#31181;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DiagGPT (Diagnosis GPT)&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;LLMs&#25512;&#24191;&#21040;TOD&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT, are becoming increasingly sophisticated, demonstrating capabilities that closely resemble those of humans. These AI models are playing an essential role in assisting humans with a wide array of tasks in daily life. A significant application of AI is its use as a chat agent, responding to human inquiries across various domains. Current LLMs have shown proficiency in answering general questions. However, basic question-answering dialogue often falls short in complex diagnostic scenarios, such as legal or medical consultations. These scenarios typically necessitate Task-Oriented Dialogue (TOD), wherein an AI chat agent needs to proactively pose questions and guide users towards specific task completion. Previous fine-tuning models have underperformed in TOD, and current LLMs do not inherently possess this capability. In this paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovative method that extends LLMs to TOD scenarios. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.07134</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#26159;&#22270;&#34920;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36880;&#28176;&#21462;&#20195;&#20102;CNN&#21644;RNN&#65292;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#32479;&#19968;&#36215;&#26469;&#12290;&#19982;&#30456;&#23545;&#29420;&#31435;&#23384;&#22312;&#30340;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#65289;&#30456;&#27604;&#65292;&#22270;&#34920;&#26159;&#19968;&#31181;&#21253;&#21547;&#20016;&#23500;&#32467;&#26500;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#21516;&#26102;&#65292;&#20316;&#20026;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#23186;&#20171;&#20043;&#19968;&#65292;&#33258;&#28982;&#35821;&#35328;&#22312;&#25551;&#36848;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#32435;&#20837;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#30340;&#29616;&#26377;&#24037;&#20316;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#38271;&#65292;&#25506;&#32034;LLMs&#26159;&#21542;&#20063;&#21487;&#20197;&#26367;&#20195;GNNs&#25104;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructGLM&#65288;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#65289;&#31639;&#27861;&#65292;&#31995;&#32479;&#22320;&#35774;&#35745;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#37325;&#26032;&#25968;&#23383;&#21270;&#30340;&#26080;&#29256;&#26435;&#32654;&#22269;&#26412;&#22320;&#25253;&#32440;&#25991;&#31456;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#36328;&#36234;&#20102;70&#24180;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;&#65292;&#24182;&#21253;&#21547;&#36817;4&#20159;&#20010;&#27491;&#21521;&#35821;&#20041;&#30456;&#20284;&#24615;&#23545;&#12290;</title><link>http://arxiv.org/abs/2306.17810</link><description>&lt;p&gt;
&#19968;&#20010;&#21382;&#21490;&#33521;&#35821;&#30340;&#22823;&#35268;&#27169;&#35821;&#20041;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Massive Scale Semantic Similarity Dataset of Historical English. (arXiv:2306.17810v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#37325;&#26032;&#25968;&#23383;&#21270;&#30340;&#26080;&#29256;&#26435;&#32654;&#22269;&#26412;&#22320;&#25253;&#32440;&#25991;&#31456;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#36328;&#36234;&#20102;70&#24180;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;&#65292;&#24182;&#21253;&#21547;&#36817;4&#20159;&#20010;&#27491;&#21521;&#35821;&#20041;&#30456;&#20284;&#24615;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21508;&#31181;&#20219;&#21153;&#20351;&#29992;&#22312;&#35821;&#20041;&#30456;&#20284;&#24615;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#34429;&#28982;&#26377;&#22810;&#31181;&#25968;&#25454;&#38598;&#21487;&#25429;&#25417;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#26159;&#20174;&#29616;&#20195;&#32593;&#32476;&#25968;&#25454;&#26500;&#24314;&#30340;&#65292;&#35201;&#20040;&#26159;&#30001;&#20154;&#24037;&#26631;&#27880;&#21592;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#21019;&#24314;&#30340;&#30456;&#23545;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#26469;&#28304;&#65292;&#21363;&#37325;&#26032;&#25968;&#23383;&#21270;&#30340;&#26080;&#29256;&#26435;&#32654;&#22269;&#26412;&#22320;&#25253;&#32440;&#25991;&#31456;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#25968;&#25454;&#38598;&#65292;&#36328;&#36234;&#20102;1920&#24180;&#21040;1989&#24180;&#30340;70&#24180;&#65292;&#24182;&#21253;&#21547;&#36817;4&#20159;&#20010;&#27491;&#21521;&#35821;&#20041;&#30456;&#20284;&#24615;&#23545;&#12290;&#22312;&#32654;&#22269;&#26412;&#22320;&#25253;&#32440;&#20013;&#65292;&#22823;&#32422;&#19968;&#21322;&#30340;&#25991;&#31456;&#26469;&#33258;&#26032;&#38395;&#26426;&#26500;&#30340;&#26032;&#38395;&#31295;&#65292;&#32780;&#26412;&#22320;&#25253;&#32440;&#22797;&#21046;&#20102;&#26032;&#38395;&#31295;&#30340;&#25991;&#31456;&#65292;&#24182;&#25776;&#20889;&#20102;&#33258;&#24049;&#30340;&#26631;&#39064;&#65292;&#36825;&#20123;&#26631;&#39064;&#24418;&#25104;&#20102;&#19982;&#25991;&#31456;&#30456;&#20851;&#30340;&#25552;&#21462;&#24615;&#25688;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#25991;&#26723;&#24067;&#23616;&#21644;&#35821;&#35328;&#29702;&#35299;&#23558;&#25991;&#31456;&#21644;&#26631;&#39064;&#20851;&#32852;&#36215;&#26469;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#26041;&#27861;&#26469;&#26816;&#27979;&#21738;&#20123;&#25991;&#31456;&#26469;&#33258;&#30456;&#21516;&#30340;&#22522;&#30784;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
A diversity of tasks use language models trained on semantic similarity data. While there are a variety of datasets that capture semantic similarity, they are either constructed from modern web data or are relatively small datasets created in the past decade by human annotators. This study utilizes a novel source, newly digitized articles from off-copyright, local U.S. newspapers, to assemble a massive-scale semantic similarity dataset spanning 70 years from 1920 to 1989 and containing nearly 400M positive semantic similarity pairs. Historically, around half of articles in U.S. local newspapers came from newswires like the Associated Press. While local papers reproduced articles from the newswire, they wrote their own headlines, which form abstractive summaries of the associated articles. We associate articles and their headlines by exploiting document layouts and language understanding. We then use deep neural methods to detect which articles are from the same underlying source, in th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#35757;&#32451;&#26679;&#26412;&#36739;&#23567;&#26102;&#65292;&#38646;&#26679;&#20363;ChatGPT&#29978;&#33267;&#20248;&#20110;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#30001;&#27492;&#34920;&#26126;ChatGPT&#20855;&#26377;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#25104;&#20026;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04504</link><description>&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#35780;&#20272;ChatGPT&#65306;&#19982;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#30340;&#38646;&#26679;&#20363;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. (arXiv:2306.04504v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#35757;&#32451;&#26679;&#26412;&#36739;&#23567;&#26102;&#65292;&#38646;&#26679;&#20363;ChatGPT&#29978;&#33267;&#20248;&#20110;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#30001;&#27492;&#34920;&#26126;ChatGPT&#20855;&#26377;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#25104;&#20026;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;OpenAI&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23613;&#31649;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20808;&#21069;&#30340;&#24037;&#20316;&#23578;&#26410;&#30740;&#31350;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#21508;&#31181;&#22522;&#20934;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#22914;&#20851;&#31995;&#25552;&#21462;&#12289;&#25991;&#26723;&#20998;&#31867;&#12289;&#38382;&#31572;&#21644;&#25688;&#35201;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23545;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#24037;&#20316;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#35757;&#32451;&#38598;&#36739;&#23567;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#38646;&#26679;&#20363;ChatGPT&#29978;&#33267;&#20248;&#20110;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#22914;BioGPT&#21644;BioBART&#12290;&#36825;&#34920;&#26126;ChatGPT&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#30340;&#39044;&#35757;&#32451;&#20351;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#26377;&#30456;&#24403;&#30340;&#19987;&#19994;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#26377;&#25104;&#20026;&#21508;&#31181;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a large language model developed by OpenAI. Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such as relation extraction, document classification, question answering, and summarization. To the best of our knowledge, this is the first work that conducts an extensive evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative transformer models, such as BioGPT and BioBART. This suggests that ChatGPT's pre-training on large text corpora makes it quite specialized even in the biomedical domain. Our findings demonstrate that ChatGPT has the potential to be a valuable tool for various tasks in the biomedical domain that la
&lt;/p&gt;</description></item><item><title>Structure-CLIP&#20351;&#29992;&#25991;&#26412;&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20351;&#29992;&#22330;&#26223;&#22270;&#24378;&#21270;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06152</link><description>&lt;p&gt;
Structure-CLIP: &#32467;&#21512;&#32467;&#26500;&#30693;&#35782;&#20248;&#21270;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Structure-CLIP: Enhance Multi-modal Language Representations with Structure Knowledge. (arXiv:2305.06152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06152
&lt;/p&gt;
&lt;p&gt;
Structure-CLIP&#20351;&#29992;&#25991;&#26412;&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20351;&#29992;&#22330;&#26223;&#22270;&#24378;&#21270;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#38656;&#35201;&#23545;&#25991;&#26412;&#36827;&#34892;&#35814;&#32454;&#35821;&#20041;&#29702;&#35299;&#30340;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#19978;&#36890;&#24120;&#34920;&#29616;&#36739;&#24046;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#19968;&#20123;&#30740;&#31350;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#21477;&#23376;&#20013;&#23384;&#22312;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#26469;&#22686;&#24378;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26694;&#26550;Structure-CLIP&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#38544;&#24335;&#35814;&#32454;&#35821;&#20041;&#65292;&#20197;&#22686;&#24378;&#31934;&#32454;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;(1)&#25105;&#20204;&#20351;&#29992;&#22330;&#26223;&#22270;&#26469;&#26356;&#21152;&#20851;&#27880;&#25991;&#26412;&#20013;&#30340;&#35814;&#32454;&#35821;&#20041;&#23398;&#20064;&#65292;&#24182;&#20805;&#20998;&#25506;&#32034;&#32454;&#31890;&#24230;&#35821;&#20041;&#20043;&#38388;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;(2)&#25105;&#20204;&#32467;&#21512;&#22330;&#26223;&#22270;&#30340;&#30693;&#35782;&#24378;&#21270;&#26694;&#26550;&#26469;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale vision-language pre-training has shown promising advances on various downstream tasks and achieved significant performance in multi-modal understanding and generation tasks. However, existing methods often perform poorly on image-text matching tasks that require a detailed semantics understanding of the text. Although there have been some works on this problem, they do not sufficiently exploit the structural knowledge present in sentences to enhance multi-modal language representations, which leads to poor performance. In this paper, we present an end-to-end framework Structure-CLIP, which integrates latent detailed semantics from the text to enhance fine-grained semantic representations. Specifically, (1) we use scene graphs in order to pay more attention to the detailed semantic learning in the text and fully explore structured knowledge between fine-grained semantics, and (2) we utilize the knowledge-enhanced framework with the help of the scene graph to make full use of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#22686;&#24378;&#30340;&#33258;&#36866;&#24212;&#34701;&#21512;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#23558;&#20381;&#36182;&#20449;&#24687;&#19982;&#21407;&#22987;&#35821;&#20041;&#20449;&#21495;&#33258;&#36866;&#24212;&#34701;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#35821;&#20041;&#21305;&#37197;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2210.08471</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#33258;&#36866;&#24212;&#34701;&#21512;&#25552;&#39640;&#35821;&#20041;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion. (arXiv:2210.08471v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#22686;&#24378;&#30340;&#33258;&#36866;&#24212;&#34701;&#21512;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#23558;&#20381;&#36182;&#20449;&#24687;&#19982;&#21407;&#22987;&#35821;&#20041;&#20449;&#21495;&#33258;&#36866;&#24212;&#34701;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#35821;&#20041;&#21305;&#37197;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22914;BERT&#65292;&#22312;&#35821;&#20041;&#21477;&#23376;&#21305;&#37197;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#12290;&#21516;&#26102;&#65292;&#20381;&#36182;&#24615;&#20808;&#39564;&#30693;&#35782;&#22312;&#22810;&#20010;NLP&#20219;&#21153;&#20013;&#20063;&#26174;&#31034;&#20986;&#26222;&#36941;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#20381;&#36182;&#24615;&#20808;&#39564;&#32467;&#26500;&#26377;&#25928;&#22320;&#38598;&#25104;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#35821;&#20041;&#21305;&#37197;&#20851;&#31995;&#65292;&#20173;&#26410;&#30830;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DAFA&#30340;&#20381;&#36182;&#22686;&#24378;&#33258;&#36866;&#24212;&#34701;&#21512;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#36825;&#23558;&#20381;&#36182;&#32467;&#26500;&#26126;&#30830;&#22320;&#24341;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#21040;&#35821;&#20041;&#20449;&#24687;&#20013;&#12290;&#20855;&#20307;&#22320;&#65292;DAFA&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#25935;&#24863;&#33539;&#24335;&#26469;&#26500;&#24314;&#19968;&#20010;&#20381;&#36182;&#30697;&#38453;&#65292;&#20197;&#26657;&#20934;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#23427;&#37319;&#29992;&#33258;&#36866;&#24212;&#34701;&#21512;&#27169;&#22359;&#26469;&#38598;&#25104;&#33719;&#21462;&#30340;&#20381;&#36182;&#20449;&#24687;&#21644;&#21407;&#22987;&#35821;&#20041;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;DAFA&#37325;&#26500;&#20102;&#27880;&#24847;&#21147;&#35745;&#31639;&#27969;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pre-trained models like BERT have achieved great progress on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also shown general benefits in multiple NLP tasks. However, how to efficiently integrate dependency prior structure into pre-trained models to better model complex semantic matching relations is still unsettled. In this paper, we propose the \textbf{D}ependency-Enhanced \textbf{A}daptive \textbf{F}usion \textbf{A}ttention (\textbf{DAFA}), which explicitly introduces dependency structure into pre-trained models and adaptively fuses it with semantic information. Specifically, \textbf{\emph{(i)}} DAFA first proposes a structure-sensitive paradigm to construct a dependency matrix for calibrating attention weights. It adopts an adaptive fusion module to integrate the obtained dependency information and the original semantic signals. Moreover, DAFA reconstructs the attention calculation flow and provides better interpretability. By applying it o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#28155;&#21152;&#38543;&#26426;&#39057;&#29575;&#21464;&#35843;&#30446;&#26631;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#23545;&#27874;&#26031;&#35821;&#23398;&#21069;&#20799;&#31461;&#35328;&#35821;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.12886</link><description>&lt;p&gt;
&#29992;&#20110;&#35780;&#20272;&#27874;&#26031;&#35821;&#23398;&#21069;&#20799;&#31461;&#35328;&#35821;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition for Speech Assessment of Persian Preschool Children. (arXiv:2203.12886v10 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.12886
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20013;&#28155;&#21152;&#38543;&#26426;&#39057;&#29575;&#21464;&#35843;&#30446;&#26631;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#23545;&#27874;&#26031;&#35821;&#23398;&#21069;&#20799;&#31461;&#35328;&#35821;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#21069;&#35780;&#20272;&#23545;&#25945;&#24072;&#21644;&#23478;&#38271;&#20102;&#35299;&#20799;&#31461;&#30340;&#25104;&#38271;&#21644;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;COVID-19&#30123;&#24773;&#31361;&#26174;&#20102;&#22312;&#32447;&#35780;&#20272;&#23398;&#21069;&#20799;&#31461;&#30340;&#24517;&#35201;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#38656;&#35201;&#27979;&#35797;&#30340;&#39046;&#22495;&#26159;&#20182;&#20204;&#30340;&#21475;&#35821;&#33021;&#21147;&#12290;&#30001;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65288;ASR&#65289;&#26159;&#22312;&#19982;&#20799;&#31461;&#19981;&#21516;&#30340;&#39057;&#29575;&#21644;&#25391;&#24133;&#29305;&#24449;&#30340;&#22768;&#38899;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#65292;&#25152;&#20197;&#26080;&#27861;&#36215;&#21040;&#24110;&#21161;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;Wav2Vec 2.0&#27169;&#22411;&#30340;&#25513;&#27169;&#30446;&#26631;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#31216;&#20026;&#38543;&#26426;&#39057;&#29575;&#21464;&#35843;&#65288;RFP&#65289;&#30340;&#26032;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#26032;&#20171;&#32461;&#30340;&#25968;&#25454;&#38598;&#26469;&#23545;&#8220;&#26080;&#24847;&#20041;&#35789;&#8221;&#65288;MW&#65289;&#21644;&#8220;&#36895;&#26597;&#33258;&#21160;&#21629;&#21517;&#8221;&#65288;RAN&#65289;&#27979;&#35797;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#12290;&#20351;&#29992;&#25513;&#27169;&#19982;RFP&#30340;&#32452;&#21512;&#20248;&#20110;Wav2Vec 2.0&#30340;&#25513;&#27169;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preschool evaluation is crucial because it gives teachers and parents influential knowledge about children's growth and development. The COVID-19 pandemic has highlighted the necessity of online assessment for preschool children. One of the areas that should be tested is their ability to speak. Employing an Automatic Speech Recognition (ASR) system would not help since they are pre-trained on voices that differ from children's in terms of frequency and amplitude. Because most of these are pre-trained with data in a specific range of amplitude, their objectives do not make them ready for voices in different amplitudes. To overcome this issue, we added a new objective to the masking objective of the Wav2Vec 2.0 model called Random Frequency Pitch (RFP). In addition, we used our newly introduced dataset to fine-tune our model for Meaningless Words (MW) and Rapid Automatic Naming (RAN) tests. Using masking in concatenation with RFP outperforms the masking objective of Wav2Vec 2.0 by reachi
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#12289;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#20294;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#36739;&#20302;&#65292;&#38656;&#35201;&#20445;&#35777;&#20854;&#21487;&#25511;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.05337</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models. (arXiv:2201.05337v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05337
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#12289;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#20294;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#36739;&#20302;&#65292;&#38656;&#35201;&#20445;&#35777;&#20854;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#20013;&#26032;&#20852;&#30340;&#26041;&#21521;&#65292;&#34987;&#35748;&#20026;&#23545;&#20110;&#24320;&#21457;&#26356;&#33258;&#28982;&#12289;&#26356;&#31526;&#21512;&#29305;&#23450;&#24212;&#29992;&#22330;&#26223;&#30340;&#20808;&#36827;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#23588;&#20854;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;Transformer&#30340;PLMs&#65292;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26032;&#33539;&#24335;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#12289;&#26356;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#36739;&#20302;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#25511;&#24615;&#38656;&#35201;&#24471;&#21040;&#20445;&#35777;&#12290;&#20026;&#27492;&#65292;&#22522;&#20110;Transformer&#30340;PLMs&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#24050;&#25104;&#20026;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26032;&#30740;&#31350;&#28909;&#28857;&#12290;&#26368;&#36817;3-4&#24180;&#20986;&#29616;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#38024;&#23545;&#21487;&#33021;&#38656;&#35201;&#19981;&#21516;&#31867;&#22411;&#25511;&#21046;&#32422;&#26463;&#30340;&#19981;&#21516;CTG&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24403;&#21069;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controllable Text Generation (CTG) is emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that are more natural and better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the lower level of interpretability of deep neural networks, the controllability of these methods need to be guaranteed. To this end, controllable text generation using transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the recent 3-4 years, targeting different CTG tasks which may require different types of controlled constraints. In this paper, we present a systematic critical review on the com
&lt;/p&gt;</description></item></channel></rss>