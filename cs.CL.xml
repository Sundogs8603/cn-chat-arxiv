<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#36827;&#32780;&#25552;&#39640;&#38646;&#26679;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01313</link><description>&lt;p&gt;
&#26356;&#22810;&#19978;&#19979;&#25991;&#65292;&#26356;&#23569;&#24178;&#25200;&#65306;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#36827;&#34892;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes. (arXiv:2308.01313v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20511;&#37492;&#20102;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25512;&#26029;&#21644;&#35843;&#33410;&#19978;&#19979;&#25991;&#23646;&#24615;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#32473;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#65292;&#21487;&#20197;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#65292;&#36827;&#32780;&#25552;&#39640;&#38646;&#26679;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#20316;&#20026;&#19968;&#31181;&#22522;&#30784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#30001;&#20110;&#20854;&#29702;&#35299;&#21508;&#31181;&#35270;&#35273;&#27010;&#24565;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#33021;&#21147;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20805;&#20998;&#21033;&#29992;CLIP&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#20154;&#31867;&#33324;&#29702;&#35299;&#33021;&#21147;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#26412;&#25991;&#20174;&#20154;&#31867;&#30340;&#35270;&#35273;&#24863;&#30693;&#36807;&#31243;&#20013;&#24471;&#21040;&#21551;&#21457;&#65306;&#29616;&#20195;&#31070;&#32463;&#31185;&#23398;&#35266;&#28857;&#35748;&#20026;&#65292;&#22312;&#23545;&#29289;&#20307;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;&#20154;&#31867;&#39318;&#20808;&#25512;&#26029;&#20854;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#23646;&#24615;&#65288;&#22914;&#32972;&#26223;&#21644;&#26041;&#21521;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#23558;&#21069;&#26223;&#23545;&#35937;&#19982;&#32972;&#26223;&#21306;&#20998;&#24320;&#26469;&#65292;&#28982;&#21518;&#20197;&#27492;&#20449;&#24687;&#20026;&#22522;&#30784;&#36827;&#34892;&#20915;&#31574;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20026;CLIP&#25552;&#20379;&#19978;&#19979;&#25991;&#23646;&#24615;&#21487;&#20197;&#25913;&#21892;&#38646;&#26679;&#26412;&#20998;&#31867;&#24182;&#20943;&#36731;&#23545;&#34394;&#20551;&#29305;&#24449;&#30340;&#20381;&#36182;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;CLIP&#26412;&#36523;&#21487;&#20197;&#21512;&#29702;&#22320;&#20174;&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#36825;&#20123;&#23646;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#35757;&#32451;&#12289;&#20004;&#27493;&#39588;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
CLIP, as a foundational vision language model, is widely used in zero-shot image classification due to its ability to understand various visual concepts and natural language descriptions. However, how to fully leverage CLIP's unprecedented human-like understanding capabilities to achieve better zero-shot classification is still an open question. This paper draws inspiration from the human visual perception process: a modern neuroscience view suggests that in classifying an object, humans first infer its class-independent attributes (e.g., background and orientation) which help separate the foreground object from the background, and then make decisions based on this information. Inspired by this, we observe that providing CLIP with contextual attributes improves zero-shot classification and mitigates reliance on spurious features. We also observe that CLIP itself can reasonably infer the attributes from an image. With these observations, we propose a training-free, two-step zero-shot cl
&lt;/p&gt;</description></item><item><title>ChatGPT&#20316;&#20026;&#26816;&#27979;&#22120;&#33021;&#21542;&#26377;&#25928;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#20854;&#22312;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#20043;&#38388;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;ChatGPT&#22312;&#33258;&#21160;&#21270;&#26816;&#27979;&#27969;&#31243;&#20013;&#30340;&#24212;&#29992;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2308.01284</link><description>&lt;p&gt;
&#29992;&#28779;&#25915;&#28779;&#65306;ChatGPT&#33021;&#22815;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fighting Fire with Fire: Can ChatGPT Detect AI-generated Text?. (arXiv:2308.01284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01284
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#26816;&#27979;&#22120;&#33021;&#21542;&#26377;&#25928;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#25105;&#20204;&#36890;&#36807;&#35780;&#20272;&#20854;&#22312;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#20043;&#38388;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;ChatGPT&#22312;&#33258;&#21160;&#21270;&#26816;&#27979;&#27969;&#31243;&#20013;&#30340;&#24212;&#29992;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#34987;&#29992;&#20110;&#21508;&#31181;&#29992;&#20363;&#65292;&#21253;&#25324;&#35268;&#27169;&#21270;&#30340;&#25991;&#26412;&#20869;&#23481;&#29983;&#25104;&#12290;&#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#38024;&#23545;&#36825;&#31181;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#30740;&#31350;&#20102;ChatGPT&#22312;&#36825;&#31181;AI&#29983;&#25104;&#25991;&#26412;&#19978;&#30340;&#26816;&#27979;&#24615;&#33021;&#65292;&#21463;&#21040;&#23558;ChatGPT&#29992;&#20316;&#25968;&#25454;&#26631;&#27880;&#22120;&#25110;&#27880;&#37322;&#22120;&#30340;&#30740;&#31350;&#21551;&#21457;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#38646;-shot&#24615;&#33021;&#65292;&#24182;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;ChatGPT&#22312;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#25110;&#20154;&#24037;&#32534;&#20889;&#25991;&#26412;&#26041;&#38754;&#26159;&#21542;&#20855;&#26377;&#23545;&#31216;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#36890;&#36807;&#31616;&#21333;&#20851;&#27880;&#38382;&#39064;&#30340;&#29305;&#23450;&#26041;&#38754;&#24182;&#20174;&#35813;&#35299;&#20915;&#26041;&#26696;&#20013;&#25512;&#23548;&#20986;&#20854;&#20313;&#37096;&#20998;&#65292;&#22914;&#20309;&#21033;&#29992;ChatGPT&#21644;&#31867;&#20284;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#21160;&#21270;&#26816;&#27979;&#27969;&#31243;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312; \url{https://github.com/AmritaBh/ChatGPT-as-Detector} &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT are increasingly being used for various use cases, including text content generation at scale. Although detection methods for such AI-generated text exist already, we investigate ChatGPT's performance as a detector on such AI-generated text, inspired by works that use ChatGPT as a data labeler or annotator. We evaluate the zero-shot performance of ChatGPT in the task of human-written vs. AI-generated text detection, and perform experiments on publicly available datasets. We empirically investigate if ChatGPT is symmetrically effective in detecting AI-generated or human-written text. Our findings provide insight on how ChatGPT and similar LLMs may be leveraged in automated detection pipelines by simply focusing on solving a specific aspect of the problem and deriving the rest from that solution. All code and data is available at \url{https://github.com/AmritaBh/ChatGPT-as-Detector}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;GPT-4&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#25512;&#29702;&#65292;&#21457;&#29616;&#20854;&#19982;&#20154;&#31867;&#20043;&#38388;&#22312;&#24847;&#22270;&#24402;&#22240;&#12289;&#22240;&#26524;&#21028;&#26029;&#12289;&#27450;&#39575;&#30340;&#36947;&#24503;&#24615;&#12289;&#36947;&#24503;&#22522;&#30784;&#12289;&#36947;&#24503;&#36816;&#27668;&#23545;&#27861;&#24459;&#21028;&#26029;&#30340;&#24433;&#21709;&#12289;&#21516;&#24847;&#30340;&#27010;&#24565;&#20197;&#21450;&#35268;&#21017;&#36829;&#21453;&#21028;&#26029;&#26041;&#38754;&#23384;&#22312;&#39640;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01264</link><description>&lt;p&gt;
&#25506;&#32034;GPT-4&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#25512;&#29702;&#30340;&#24515;&#29702;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring the psychology of GPT-4's Moral and Legal Reasoning. (arXiv:2308.01264v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;GPT-4&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#25512;&#29702;&#65292;&#21457;&#29616;&#20854;&#19982;&#20154;&#31867;&#20043;&#38388;&#22312;&#24847;&#22270;&#24402;&#22240;&#12289;&#22240;&#26524;&#21028;&#26029;&#12289;&#27450;&#39575;&#30340;&#36947;&#24503;&#24615;&#12289;&#36947;&#24503;&#22522;&#30784;&#12289;&#36947;&#24503;&#36816;&#27668;&#23545;&#27861;&#24459;&#21028;&#26029;&#30340;&#24433;&#21709;&#12289;&#21516;&#24847;&#30340;&#27010;&#24565;&#20197;&#21450;&#35268;&#21017;&#36829;&#21453;&#21028;&#26029;&#26041;&#38754;&#23384;&#22312;&#39640;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#29992;&#20316;&#39640;&#24230;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#30784;&#65292;&#33021;&#22815;&#23545;&#27861;&#24459;&#21644;&#36947;&#24503;&#38382;&#39064;&#20316;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#33258;&#36523;&#20869;&#37096;&#24037;&#20316;&#30340;&#25351;&#23548;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#21363;&#20351;&#26159;&#23427;&#20204;&#30340;&#21019;&#24314;&#24037;&#31243;&#22242;&#38431;&#20063;&#26080;&#27861;&#35299;&#37322;&#23427;&#20204;&#22914;&#20309;&#33719;&#24471;&#24403;&#21069;&#25152;&#26377;&#33021;&#21147;&#30340;&#20855;&#20307;&#36807;&#31243;&#12290;&#26426;&#22120;&#24515;&#29702;&#23398;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#26088;&#22312;&#28145;&#20837;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#25317;&#26377;&#30340;&#36807;&#31243;&#21644;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36816;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#26469;&#25506;&#31350;GPT-4&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;GPT-4&#19982;&#20154;&#31867;&#22312;&#24847;&#22270;&#24402;&#22240;&#12289;&#22240;&#26524;&#21028;&#26029;&#12289;&#27450;&#39575;&#30340;&#36947;&#24503;&#24615;&#12289;&#36947;&#24503;&#22522;&#30784;&#12289;&#36947;&#24503;&#36816;&#27668;&#23545;&#27861;&#24459;&#21028;&#26029;&#30340;&#24433;&#21709;&#12289;&#21516;&#24847;&#30340;&#27010;&#24565;&#20197;&#21450;&#35268;&#21017;&#36829;&#21453;&#21028;&#26029;&#26041;&#38754;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#22238;&#31572;&#20043;&#38388;&#23384;&#22312;&#36739;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have been used as the foundation of highly sophisticated artificial intelligences, capable of delivering human-like responses to probes about legal and moral issues. However, these models are unreliable guides to their own inner workings, and even the engineering teams behind their creation are unable to explain exactly how they came to develop all of the capabilities they currently have. The emerging field of machine psychology seeks to gain insight into the processes and concepts that these models possess. In this paper, we employ the methods of psychology to probe into GPT-4's moral and legal reasoning. More specifically, we investigate the similarities and differences between GPT-4 and humans when it comes to intentionality ascriptions, judgments about causation, the morality of deception, moral foundations, the impact of moral luck on legal judgments, the concept of consent, and rule violation judgments. We find high correlations between human and AI response
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;XSTest&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#12290;&#35813;&#22871;&#20214;&#30001;200&#20010;&#23433;&#20840;&#25552;&#31034;&#32452;&#25104;&#65292;&#28085;&#30422;&#21313;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#26088;&#22312;&#24341;&#20986;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01263</link><description>&lt;p&gt;
XSTest: &#29992;&#20110;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22840;&#22823;&#23433;&#20840;&#34892;&#20026;&#30340;&#27979;&#35797;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models. (arXiv:2308.01263v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;XSTest&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35782;&#21035;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#12290;&#35813;&#22871;&#20214;&#30001;200&#20010;&#23433;&#20840;&#25552;&#31034;&#32452;&#25104;&#65292;&#28085;&#30422;&#21313;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#26088;&#22312;&#24341;&#20986;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27809;&#26377;&#36866;&#24403;&#30340;&#20445;&#25252;&#25514;&#26045;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24456;&#23481;&#26131;&#36981;&#24490;&#24694;&#24847;&#25351;&#20196;&#24182;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#36825;&#28608;&#21457;&#20102;&#23433;&#20840;&#24037;&#20316;&#65292;&#22914;&#32418;&#38431;&#27979;&#35797;&#21644;&#22823;&#35268;&#27169;&#21453;&#39304;&#23398;&#20064;&#65292;&#26088;&#22312;&#20351;&#27169;&#22411;&#26082;&#26377;&#29992;&#21448;&#26080;&#23475;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#32039;&#24352;&#20851;&#31995;&#65292;&#22240;&#20026;&#26080;&#23475;&#24615;&#35201;&#27714;&#27169;&#22411;&#25298;&#32477;&#36981;&#20174;&#19981;&#23433;&#20840;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26080;&#27861;&#25552;&#20379;&#24110;&#21161;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#35777;&#25454;&#34920;&#26126;&#65292;&#19968;&#20123;&#27169;&#22411;&#21487;&#33021;&#22312;&#24179;&#34913;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#20197;&#33267;&#20110;&#21363;&#20351;&#20351;&#29992;&#31867;&#20284;&#19981;&#23433;&#20840;&#25552;&#31034;&#30340;&#35821;&#35328;&#25110;&#25552;&#21450;&#25935;&#24863;&#20027;&#39064;&#30340;&#26126;&#26174;&#23433;&#20840;&#25552;&#31034;&#20063;&#20250;&#34987;&#25298;&#32477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;XSTest&#30340;&#26032;&#27979;&#35797;&#22871;&#20214;&#65292;&#20197;&#31995;&#32479;&#21270;&#21644;&#32467;&#26500;&#21270;&#30340;&#26041;&#24335;&#35782;&#21035;&#36825;&#31181;&#22840;&#24352;&#30340;&#23433;&#20840;&#34892;&#20026;&#12290;&#30446;&#21069;&#65292;XSTest&#21253;&#25324;200&#20010;&#23433;&#20840;&#25552;&#31034;&#65292;&#28085;&#30422;&#21313;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#33391;&#22909;&#26657;&#20934;&#30340;&#27169;&#22411;&#19981;&#24212;&#35813;&#25298;&#32477;&#36981;&#24490;&#36825;&#20123;&#25552;&#31034;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;XSTest&#30340;&#21019;&#24314;&#21644;&#32452;&#25104;&#65292;&#24182;&#20351;&#29992;&#27979;&#35797;&#22871;&#20214;&#31361;&#26174;&#31995;&#32479;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse complying with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a structured and systematic way. In its current form, XSTest comprises 200 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with. We describe XSTest's creation and composition, and use the test suite to highlight systematic f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22235;&#20010;&#20195;&#34920;&#24615;&#30340;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;10&#31181;&#24320;&#28304;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#22312;&#38646;&#30693;&#35782;&#36801;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#22312;&#23569;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#28155;&#21152;&#31034;&#33539;&#26679;&#20363;&#21487;&#20197;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#24494;&#35843;&#35774;&#32622;&#19979;&#65292;&#24494;&#35843;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01240</link><description>&lt;p&gt;
&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#35780;&#20272;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Instruction-Tuned Large Language Models on Code Comprehension and Generation. (arXiv:2308.01240v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22235;&#20010;&#20195;&#34920;&#24615;&#30340;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;10&#31181;&#24320;&#28304;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#22312;&#38646;&#30693;&#35782;&#36801;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#22312;&#23569;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#28155;&#21152;&#31034;&#33539;&#26679;&#20363;&#21487;&#20197;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#22312;&#24494;&#35843;&#35774;&#32622;&#19979;&#65292;&#24494;&#35843;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;10&#31181;&#24320;&#28304;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22235;&#20010;&#20195;&#34920;&#24615;&#30340;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#20197;&#19979;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#22312;&#38646;&#30693;&#35782;&#36801;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#65292;&#26377;&#26102;&#29978;&#33267;&#20248;&#20110;&#19987;&#38376;&#38024;&#23545;&#27599;&#20010;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#23567;&#22411;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#26356;&#22823;&#30340;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24182;&#19981;&#24635;&#26159;&#22312;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#20219;&#21153;&#19978;&#26356;&#22909;&#12290;&#20854;&#27425;&#65292;&#22312;&#23569;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21457;&#29616;&#28155;&#21152;&#31034;&#33539;&#26679;&#20363;&#21487;&#20197;&#22823;&#22823;&#24110;&#21161;&#25351;&#23548;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#31034;&#33539;&#26679;&#20363;&#26377;&#26102;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#29978;&#33267;&#26356;&#24046;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;BM25&#30340;&#26679;&#26412;&#36873;&#25321;&#31574;&#30053;&#22312;&#29983;&#25104;&#38382;&#39064;&#19978;&#26126;&#26174;&#20248;&#20110;&#22522;&#30784;&#30340;&#38543;&#26426;&#36873;&#25321;&#25110;&#22266;&#23450;&#36873;&#25321;&#12290;&#31532;&#19977;&#65292;&#23545;&#20110;&#24494;&#35843;&#35774;&#32622;&#65292;&#25105;&#20204;&#21457;&#29616;&#24494;&#35843;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we evaluate 10 open-source instructed LLMs on four representative code comprehension and generation tasks. We have the following main findings. First, for the zero-shot setting, instructed LLMs are very competitive on code comprehension and generation tasks and sometimes even better than small SOTA models specifically fine-tuned on each downstream task. We also find that larger instructed LLMs are not always better on code-related tasks. Second, for the few-shot setting, we find that adding demonstration examples substantially helps instructed LLMs perform better on most code comprehension and generation tasks; however, the examples would sometimes induce unstable or even worse performance. Furthermore, we find widely-used BM25-based shot selection strategy significantly outperforms the basic random selection or fixed selection only on generation problems. Third, for the fine-tuning setting, we find that fine-tuning could further improve the model performance on downstrea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#65292;&#35201;&#27714;&#27169;&#22411;&#33021;&#22815;&#30830;&#23450;&#22270;&#20687;&#19982;&#25991;&#26412;&#30340;&#20851;&#31995;&#65292;&#24182;&#23450;&#20301;&#25152;&#25351;&#30340;&#23545;&#35937;&#25110;&#32773;&#23545;&#19981;&#21305;&#37197;&#30340;&#37096;&#20998;&#36827;&#34892; grounding&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#19978;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#38271;&#24230;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#25935;&#24863;&#30340;&#23545;&#24212;&#25512;&#29702;&#32593;&#32476;&#65288;RCRN&#65289;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#21452;&#21521;&#20449;&#24687;&#20256;&#36882;&#21644;&#35821;&#35328;&#32467;&#26500;&#24341;&#23548;&#30340;&#20851;&#31995;&#24863;&#30693;&#25512;&#29702;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.01236</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21305;&#37197;&#20851;&#31995;&#25512;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Grounded Image Text Matching with Mismatched Relation Reasoning. (arXiv:2308.01236v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#65292;&#35201;&#27714;&#27169;&#22411;&#33021;&#22815;&#30830;&#23450;&#22270;&#20687;&#19982;&#25991;&#26412;&#30340;&#20851;&#31995;&#65292;&#24182;&#23450;&#20301;&#25152;&#25351;&#30340;&#23545;&#35937;&#25110;&#32773;&#23545;&#19981;&#21305;&#37197;&#30340;&#37096;&#20998;&#36827;&#34892; grounding&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#19978;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#38271;&#24230;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#25935;&#24863;&#30340;&#23545;&#24212;&#25512;&#29702;&#32593;&#32476;&#65288;RCRN&#65289;&#65292;&#35813;&#32593;&#32476;&#36890;&#36807;&#21452;&#21521;&#20449;&#24687;&#20256;&#36882;&#21644;&#35821;&#35328;&#32467;&#26500;&#24341;&#23548;&#30340;&#20851;&#31995;&#24863;&#30693;&#25512;&#29702;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20351;&#29992;&#19981;&#21305;&#37197;&#20851;&#31995;&#25512;&#29702;&#30340;&#22522;&#20110;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#65288;GITM-MR&#65289;&#8221;&#30340;&#26032;&#22411;&#35270;&#35273;-&#35821;&#35328;&#32852;&#21512;&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20851;&#31995;&#29702;&#35299;&#33021;&#21147;&#12290;GITM-MR&#38656;&#35201;&#27169;&#22411;&#39318;&#20808;&#30830;&#23450;&#19968;&#20010;&#34920;&#36798;&#26159;&#21542;&#25551;&#36848;&#20102;&#19968;&#24352;&#22270;&#20687;&#65292;&#28982;&#21518;&#23450;&#20301;&#25152;&#25351;&#30340;&#23545;&#35937;&#25110;&#32773;&#23545;&#25991;&#26412;&#20013;&#30340;&#19981;&#21305;&#37197;&#37096;&#20998;&#36827;&#34892; grounding&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#30340;&#22522;&#20934;&#65292;&#37325;&#28857;&#20851;&#27880;&#26377;&#38480;&#25968;&#25454;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#21477;&#23376;&#38271;&#24230;&#30340;&#25361;&#25112;&#24615;&#35774;&#32622;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#32570;&#20047;&#25968;&#25454;&#25928;&#29575;&#21644;&#38271;&#24230;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20851;&#31995;&#25935;&#24863;&#30340;&#23545;&#24212;&#25512;&#29702;&#32593;&#32476;&#65288;RCRN&#65289;&#65292;&#36890;&#36807;&#21452;&#21521;&#20449;&#24687;&#20256;&#36882;&#21644;&#35821;&#35328;&#32467;&#26500;&#24341;&#23548;&#30340;&#20851;&#31995;&#24863;&#30693;&#25512;&#29702;&#65292;&#23558;RCRN&#35299;&#37322;&#20026;&#19968;&#20010;&#27169;&#22359;&#21270;&#31243;&#24207;&#65292;&#24182;&#22312;&#38271;&#24230;&#27867;&#21270;&#21644;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24378;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Grounded Image Text Matching with Mismatched Relation (GITM-MR), a novel visual-linguistic joint task that evaluates the relation understanding capabilities of transformer-based pre-trained models. GITM-MR requires a model to first determine if an expression describes an image, then localize referred objects or ground the mismatched parts of the text. We provide a benchmark for evaluating pre-trained models on this task, with a focus on the challenging settings of limited data and out-of-distribution sentence lengths. Our evaluation demonstrates that pre-trained models lack data efficiency and length generalization ability. To address this, we propose the Relation-sensitive Correspondence Reasoning Network (RCRN), which incorporates relation-aware reasoning via bi-directional message propagation guided by language structure. RCRN can be interpreted as a modular program and delivers strong performance in both length generalization and data efficiency.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#32763;&#35793;&#33021;&#21147;&#65292;&#20811;&#26381;&#20102;&#23545;&#22806;&#37096;&#32763;&#35793;&#31995;&#32479;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#33258;&#25105;&#32763;&#35793;&#30456;&#23545;&#20110;&#30452;&#25509;&#25512;&#29702;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.01223</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#20013;&#24605;&#32771;&#26159;&#21542;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Multilingual Language Models Think Better in English?. (arXiv:2308.01223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01223
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#25105;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#32763;&#35793;&#33021;&#21147;&#65292;&#20811;&#26381;&#20102;&#23545;&#22806;&#37096;&#32763;&#35793;&#31995;&#32479;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#33258;&#25105;&#32763;&#35793;&#30456;&#23545;&#20110;&#30452;&#25509;&#25512;&#29702;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#27979;&#35797;&#26159;&#25552;&#39640;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#19968;&#31181;&#24120;&#29992;&#25216;&#26415;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#22806;&#37096;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#23558;&#36755;&#20837;&#32763;&#35793;&#25104;&#33521;&#35821;&#65292;&#24182;&#23545;&#32763;&#35793;&#21518;&#30340;&#36755;&#20837;&#36827;&#34892;&#25512;&#29702;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25913;&#36827;&#21487;&#20197;&#24402;&#22240;&#20110;&#20351;&#29992;&#19968;&#20010;&#21333;&#29420;&#30340;&#32763;&#35793;&#31995;&#32479;&#65292;&#36825;&#20010;&#31995;&#32479;&#36890;&#24120;&#26159;&#22312;&#22823;&#37327;&#30340;&#24179;&#34892;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#26159;&#30475;&#19981;&#21040;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#33258;&#25105;&#32763;&#35793;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#32763;&#35793;&#33021;&#21147;&#26469;&#20811;&#26381;&#23545;&#22806;&#37096;&#32763;&#35793;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#23545;5&#20010;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#33258;&#25105;&#32763;&#35793;&#22987;&#32456;&#20248;&#20110;&#30452;&#25509;&#25512;&#29702;&#65292;&#35777;&#26126;&#20102;&#24403;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#36827;&#34892;&#25552;&#31034;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#21457;&#25381;&#20854;&#22810;&#35821;&#35328;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; https://github.com/juletx/self-translate &#20013;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translate-test is a popular technique to improve the performance of multilingual language models. This approach works by translating the input into English using an external machine translation system, and running inference over the translated input. However, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by the language model. In this work, we introduce a new approach called self-translate, which overcomes the need of an external translation system by leveraging the few-shot translation capabilities of multilingual language models. Experiments over 5 tasks show that self-translate consistently outperforms direct inference, demonstrating that language models are unable to leverage their full multilingual potential when prompted in non-English languages. Our code is available at https://github.com/juletx/self-translate.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23618;&#27425;&#21270;softmax&#30340;&#20840;&#23616;&#23618;&#27425;&#21270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#22235;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#24120;&#35268;softmax&#21644;&#24179;&#38754;&#20998;&#31867;&#22120;&#65292;&#23618;&#27425;&#21270;softmax&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01210</link><description>&lt;p&gt;
&#20351;&#29992;&#23618;&#27425;&#21270;softmax&#30340;&#20840;&#23616;&#23618;&#27425;&#21270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Global Hierarchical Neural Networks using Hierarchical Softmax. (arXiv:2308.01210v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23618;&#27425;&#21270;softmax&#30340;&#20840;&#23616;&#23618;&#27425;&#21270;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#22235;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#24120;&#35268;softmax&#21644;&#24179;&#38754;&#20998;&#31867;&#22120;&#65292;&#23618;&#27425;&#21270;softmax&#33021;&#22815;&#21462;&#24471;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#20351;&#29992;&#23618;&#27425;&#21270;softmax&#26469;&#21019;&#24314;&#19968;&#20010;&#20840;&#23616;&#23618;&#27425;&#21270;&#20998;&#31867;&#22120;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#20855;&#26377;&#33258;&#28982;&#23618;&#27425;&#32467;&#26500;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#23454;&#35777;&#32467;&#26524;&#12290;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#20013;&#65292;&#30456;&#23545;&#20110;&#20351;&#29992;&#24179;&#38754;&#20998;&#31867;&#22120;&#30340;&#24120;&#35268;softmax&#65292;&#23618;&#27425;&#21270;softmax&#22312;&#23439;F1&#21644;&#23439;&#21484;&#22238;&#29575;&#26041;&#38754;&#37117;&#26377;&#25152;&#25552;&#21319;&#12290;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#23618;&#27425;&#21270;softmax&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24494;&#20934;&#30830;&#29575;&#21644;&#23439;&#31934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a framework in which hierarchical softmax is used to create a global hierarchical classifier. The approach is applicable for any classification task where there is a natural hierarchy among classes. We show empirical results on four text classification datasets. In all datasets the hierarchical softmax improved on the regular softmax used in a flat classifier in terms of macro-F1 and macro-recall. In three out of four datasets hierarchical softmax achieved a higher micro-accuracy and macro-precision.
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#27573;&#33853;&#32423;&#21035;&#20449;&#24687;&#22312;&#25429;&#25417;&#21028;&#20363;&#30456;&#20284;&#24615;&#20197;&#25552;&#39640;&#20808;&#20363;&#26816;&#32034;&#24615;&#33021;&#26041;&#38754;&#30340;&#36164;&#28304;&#21033;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;&#27573;&#33853;&#32423;&#21035;&#26041;&#27861;&#22312;&#19982;&#22522;&#32447;&#25991;&#26723;&#32423;&#21035;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#24378;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;&#23545;&#21360;&#24230;&#26368;&#39640;&#27861;&#38498;&#21028;&#20915;&#20219;&#21153;&#30340;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#32467;&#26524;&#26174;&#31034;&#65292;&#27573;&#33853;&#32423;&#21035;&#26041;&#27861;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.01203</link><description>&lt;p&gt;
&#20998;&#26512;&#27573;&#33853;&#30340;&#36164;&#28304;&#21033;&#29992;&#24615;&#20197;&#29992;&#20110;&#20808;&#20363;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Analysing the Resourcefulness of the Paragraph for Precedence Retrieval. (arXiv:2308.01203v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01203
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#27573;&#33853;&#32423;&#21035;&#20449;&#24687;&#22312;&#25429;&#25417;&#21028;&#20363;&#30456;&#20284;&#24615;&#20197;&#25552;&#39640;&#20808;&#20363;&#26816;&#32034;&#24615;&#33021;&#26041;&#38754;&#30340;&#36164;&#28304;&#21033;&#29992;&#24615;&#65292;&#24182;&#21457;&#29616;&#27573;&#33853;&#32423;&#21035;&#26041;&#27861;&#22312;&#19982;&#22522;&#32447;&#25991;&#26723;&#32423;&#21035;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#24378;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;&#23545;&#21360;&#24230;&#26368;&#39640;&#27861;&#38498;&#21028;&#20915;&#20219;&#21153;&#30340;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#27604;&#36739;&#32467;&#26524;&#26174;&#31034;&#65292;&#27573;&#33853;&#32423;&#21035;&#26041;&#27861;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#25552;&#21462;&#30456;&#20851;&#27861;&#24459;&#20449;&#24687;&#20197;&#24110;&#21161;&#27861;&#24459;&#20174;&#19994;&#32773;&#30340;&#26041;&#27861;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#30740;&#31350;&#24037;&#20316;&#36890;&#36807;&#21033;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#20449;&#24687;&#65292;&#22914;&#20803;&#25968;&#25454;&#12289;&#24341;&#29992;&#25991;&#29486;&#12289;&#20851;&#38190;&#35789;&#12289;&#21477;&#23376;&#12289;&#27573;&#33853;&#31561;&#65292;&#27491;&#22312;&#36827;&#34892;&#12290;&#19982;&#20219;&#20309;&#25991;&#26412;&#25991;&#26723;&#19968;&#26679;&#65292;&#27861;&#24459;&#25991;&#20214;&#30001;&#27573;&#33853;&#32452;&#25104;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#27573;&#33853;&#32423;&#21035;&#20449;&#24687;&#22312;&#25429;&#25417;&#21028;&#20363;&#30456;&#20284;&#24615;&#20197;&#25552;&#39640;&#20808;&#20363;&#26816;&#32034;&#24615;&#33021;&#26041;&#38754;&#30340;&#36164;&#28304;&#21033;&#29992;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27573;&#33853;&#32423;&#21035;&#26041;&#27861;&#21482;&#38656;&#23569;&#37327;&#27573;&#33853;&#20132;&#20114;&#21363;&#21487;&#25429;&#25417;&#21028;&#20363;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#19988;&#22312;&#22522;&#32447;&#25991;&#26723;&#32423;&#21035;&#26041;&#27861;&#19978;&#20855;&#26377;&#26356;&#24378;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23545;&#21360;&#24230;&#26368;&#39640;&#27861;&#38498;&#21028;&#20915;&#20219;&#21153;&#30340;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#27604;&#36739;&#32467;&#26524;&#34920;&#26126;&#65292;&#27573;&#33853;&#32423;&#21035;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing methods for extracting relevant legal information to aid legal practitioners is an active research area. In this regard, research efforts are being made by leveraging different kinds of information, such as meta-data, citations, keywords, sentences, paragraphs, etc. Similar to any text document, legal documents are composed of paragraphs. In this paper, we have analyzed the resourcefulness of paragraph-level information in capturing similarity among judgments for improving the performance of precedence retrieval. We found that the paragraph-level methods could capture the similarity among the judgments with only a few paragraph interactions and exhibit more discriminating power over the baseline document-level method. Moreover, the comparison results on two benchmark datasets for the precedence retrieval on the Indian supreme court judgments task show that the paragraph-level methods exhibit comparable performance with the state-of-the-art methods
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31639;&#26415;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20869;&#37096;&#30340;&#20540;&#31354;&#38388;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.01154</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31639;&#26415;&#36816;&#31639;&#65306;&#20174;&#35760;&#24518;&#21040;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Arithmetic with Language Models: from Memorization to Computation. (arXiv:2308.01154v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31639;&#26415;&#35745;&#31639;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20869;&#37096;&#30340;&#20540;&#31354;&#38388;&#36827;&#34892;&#35745;&#31639;&#65292;&#24182;&#21462;&#24471;&#20102;&#25104;&#21151;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26356;&#22909;&#22320;&#29702;&#35299;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#24615;&#35745;&#31639;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#23545;&#20110;&#36827;&#19968;&#27493;&#25913;&#36827;&#23427;&#20204;&#24182;&#25299;&#23485;&#20854;&#36866;&#29992;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#20010;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#22312;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#25191;&#34892;&#31639;&#26415;&#35745;&#31639;&#12290;&#20108;&#36827;&#21046;&#21152;&#27861;&#21644;&#20056;&#27861;&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#27979;&#35797;&#22522;&#30784;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#19968;&#20010;&#38750;&#24120;&#23567;&#30340;&#35789;&#27719;&#34920;&#65292;&#24182;&#19988;&#22312;&#36755;&#20837;/&#36755;&#20986;&#19978;&#23637;&#31034;&#20102;&#30456;&#20851;&#30340;&#19981;&#36830;&#32493;&#24615;&#65292;&#20351;&#24471;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#24179;&#28369;&#30340;&#36755;&#20837;&#25554;&#20540;&#26080;&#25928;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#35757;&#32451;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#23398;&#20064;&#36825;&#20123;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20854;&#22806;&#25512;&#33021;&#21147;&#21644;&#20869;&#37096;&#20449;&#24687;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25903;&#25345;&#36825;&#26679;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#19968;&#20010;&#32534;&#30721;-&#22238;&#24402;-&#35299;&#30721;&#26426;&#22120;&#65292;&#19968;&#26086;&#23558;&#36755;&#20837;&#26631;&#35760;&#34920;&#31034;&#26144;&#23556;&#21040;&#21512;&#36866;&#30340;&#20869;&#37096;&#20540;&#31354;&#38388;&#65292;&#35745;&#31639;&#23601;&#22312;&#20540;&#31354;&#38388;&#20013;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
A better understanding of the emergent computation and problem-solving capabilities of recent large language models is of paramount importance to further improve them and broaden their applicability. This work investigates how a language model, trained to predict the next token, can perform arithmetic computations generalizing beyond training data. Binary addition and multiplication constitute a good testbed for this purpose, since they require a very small vocabulary and exhibit relevant input/output discontinuities making smooth input interpolation ineffective for novel data. We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing. Our findings support the hypotheses that the language model works as an Encoding-Regression-Decoding machine where the computation takes place in the value space once the input token representation is mapped to an appropriate intern
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#37197;&#23545;&#39118;&#26684;&#35821;&#26009;&#24211;&#30340;ADS-Cap&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#22810;&#26679;&#21270;&#22320;&#29983;&#25104;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#39118;&#26684;&#21270;&#23383;&#24149;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#27169;&#22359;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#35760;&#24518;&#22810;&#26679;&#30340;&#39118;&#26684;&#27169;&#24335;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ADS-Cap&#22312;&#19982;&#22270;&#20687;&#30340;&#19968;&#33268;&#24615;&#12289;&#39118;&#26684;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2308.01143</link><description>&lt;p&gt;
ADS-Cap&#65306;&#19968;&#31181;&#22522;&#20110;&#26080;&#37197;&#23545;&#39118;&#26684;&#35821;&#26009;&#24211;&#30340;&#20934;&#30830;&#22810;&#26679;&#21270;&#30340;&#39118;&#26684;&#21270;&#23383;&#24149;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ADS-Cap: A Framework for Accurate and Diverse Stylized Captioning with Unpaired Stylistic Corpora. (arXiv:2308.01143v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#37197;&#23545;&#39118;&#26684;&#35821;&#26009;&#24211;&#30340;ADS-Cap&#26694;&#26550;&#65292;&#29992;&#20110;&#20934;&#30830;&#22810;&#26679;&#21270;&#22320;&#29983;&#25104;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#39118;&#26684;&#21270;&#23383;&#24149;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#27169;&#22359;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#35760;&#24518;&#22810;&#26679;&#30340;&#39118;&#26684;&#27169;&#24335;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ADS-Cap&#22312;&#19982;&#22270;&#20687;&#30340;&#19968;&#33268;&#24615;&#12289;&#39118;&#26684;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26080;&#37197;&#23545;&#39118;&#26684;&#35821;&#26009;&#24211;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#35821;&#35328;&#39118;&#26684;&#30340;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#23383;&#24149;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#25105;&#20204;&#26399;&#26395;&#20855;&#26377;&#21508;&#31181;&#39118;&#26684;&#27169;&#24335;&#30340;&#39118;&#26684;&#21270;&#23383;&#24149;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#26469;&#29983;&#25104;&#20934;&#30830;&#22810;&#26679;&#21270;&#30340;&#39118;&#26684;&#21270;&#23383;&#24149; (ADS-Cap)&#12290;&#25105;&#20204;&#30340;ADS-Cap&#39318;&#20808;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#27169;&#22359;&#26469;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#36825;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32479;&#19968;&#20102;&#37197;&#23545;&#30340;&#20107;&#23454;&#21644;&#26080;&#37197;&#23545;&#30340;&#39118;&#26684;&#35821;&#26009;&#24211;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26377;&#26465;&#20214;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#33258;&#21160;&#35760;&#24518;&#22810;&#26679;&#21270;&#30340;&#39118;&#26684;&#27169;&#24335;&#65292;&#24182;&#36890;&#36807;&#37319;&#26679;&#22686;&#24378;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22797;&#26680;&#27169;&#22359;&#65292;&#36890;&#36807;&#36807;&#28388;&#29305;&#23450;&#39118;&#26684;&#30340;&#23383;&#24149;&#26469;&#25552;&#39640;&#39118;&#26684;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#39118;&#26684;&#21270;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#21508;&#31181;&#22522;&#20934;&#26041;&#27861;&#65292;ADS-Cap&#22312;&#19982;&#22270;&#20687;&#30340;&#19968;&#33268;&#24615;&#12289;&#39118;&#26684;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating visually grounded image captions with specific linguistic styles using unpaired stylistic corpora is a challenging task, especially since we expect stylized captions with a wide variety of stylistic patterns. In this paper, we propose a novel framework to generate Accurate and Diverse Stylized Captions (ADS-Cap). Our ADS-Cap first uses a contrastive learning module to align the image and text features, which unifies paired factual and unpaired stylistic corpora during the training process. A conditional variational auto-encoder is then used to automatically memorize diverse stylistic patterns in latent space and enhance diversity through sampling. We also design a simple but effective recheck module to boost style accuracy by filtering style-specific captions. Experimental results on two widely used stylized image captioning datasets show that regarding consistency with the image, style accuracy and diversity, ADS-Cap achieves outstanding performances compared to various bas
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30693;&#35782;&#24341;&#23548;&#30340;&#22238;&#25918;(K-Replay)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24494;&#35843;&#26399;&#38388;&#20445;&#25345;&#39044;&#35757;&#32451;&#30693;&#35782;&#65292;&#23558;&#30495;&#23454;&#19990;&#30028;&#30693;&#35782;&#34701;&#20837;&#22270;&#20687;&#25551;&#36848;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22270;&#20687;&#25551;&#36848;&#26041;&#27861;&#30340;&#36890;&#29992;&#24615;&#21644;&#30495;&#23454;&#19990;&#30028;&#30693;&#35782;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.01126</link><description>&lt;p&gt;
&#36229;&#36234;&#36890;&#29992;&#65306;&#22312;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#21033;&#29992;&#30495;&#23454;&#19990;&#30028;&#30693;&#35782;&#22686;&#24378;&#22270;&#20687;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Beyond Generic: Enhancing Image Captioning with Real-World Knowledge using Vision-Language Pre-Training Model. (arXiv:2308.01126v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#30693;&#35782;&#24341;&#23548;&#30340;&#22238;&#25918;(K-Replay)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#24494;&#35843;&#26399;&#38388;&#20445;&#25345;&#39044;&#35757;&#32451;&#30693;&#35782;&#65292;&#23558;&#30495;&#23454;&#19990;&#30028;&#30693;&#35782;&#34701;&#20837;&#22270;&#20687;&#25551;&#36848;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#22270;&#20687;&#25551;&#36848;&#26041;&#27861;&#30340;&#36890;&#29992;&#24615;&#21644;&#30495;&#23454;&#19990;&#30028;&#30693;&#35782;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22270;&#20687;&#25551;&#36848;&#26041;&#27861;&#24448;&#24448;&#29983;&#25104;&#27491;&#30830;&#20294;&#8220;&#36890;&#29992;&#8221;&#30340;&#25551;&#36848;&#65292;&#32570;&#20047;&#30495;&#23454;&#19990;&#30028;&#30693;&#35782;&#65292;&#20363;&#22914;&#21629;&#21517;&#23454;&#20307;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#32771;&#34385;&#21040;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;(VLP)&#21487;&#20197;&#20174;&#22823;&#35268;&#27169;&#30340;&#32593;&#32476;&#25968;&#25454;&#20013;&#25484;&#25569;&#22823;&#37327;&#36825;&#26679;&#30340;&#30693;&#35782;&#65292;&#21033;&#29992;VLP&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#23558;&#30693;&#35782;&#34701;&#20837;&#22270;&#20687;&#25551;&#36848;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;VLP&#27169;&#22411;&#38754;&#20020;&#25361;&#25112;&#65306;&#38646;&#26679;&#26412;&#25512;&#29702;&#20250;&#23548;&#33268;&#30693;&#35782;&#24187;&#35273;&#65292;&#20174;&#32780;&#20135;&#29983;&#20302;&#36136;&#37327;&#30340;&#25551;&#36848;&#65292;&#32780;&#19979;&#28216;&#20219;&#21153;&#24494;&#35843;&#20013;&#30340;&#36890;&#29992;&#20559;&#24046;&#38459;&#30861;&#20102;VLP&#27169;&#22411;&#34920;&#36798;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#30693;&#35782;&#24341;&#23548;&#30340;&#22238;&#25918;(K-Replay)&#65292;&#22312;&#24494;&#35843;&#26399;&#38388;&#20445;&#25345;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#37096;&#20998;&#65306;(1)&#22312;&#33258;&#21160;&#25910;&#38598;&#30340;&#22238;&#25918;&#31034;&#20363;&#19978;&#36827;&#34892;&#30693;&#35782;&#39044;&#27979;&#20219;&#21153;&#65292;&#36830;&#32493;&#21796;&#37266;VLP&#27169;&#22411;&#23545;&#30693;&#35782;&#30340;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current captioning approaches tend to generate correct but "generic" descriptions that lack real-world knowledge, e.g., named entities and contextual information. Considering that Vision-Language Pre-Training (VLP) models master massive such knowledge from large-scale web-harvested data, it is promising to utilize the generalizability of VLP models to incorporate knowledge into image descriptions. However, using VLP models faces challenges: zero-shot inference suffers from knowledge hallucination that leads to low-quality descriptions, but the generic bias in downstream task fine-tuning hinders the VLP model from expressing knowledge. To address these concerns, we propose a simple yet effective method called Knowledge-guided Replay (K-Replay), which enables the retention of pre-training knowledge during fine-tuning. Our approach consists of two parts: (1) a knowledge prediction task on automatically collected replay exemplars to continuously awaken the VLP model's memory about knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#23569;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#21644;&#28689;&#24067;&#25552;&#31034;&#30340;&#26041;&#24335;&#26469;&#36827;&#34892;&#22238;&#22797;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#20998;&#26512;&#30830;&#23450;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.01080</link><description>&lt;p&gt;
&#21033;&#29992;&#23569;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#21644;&#28689;&#24067;&#25552;&#31034;&#30340;&#26041;&#24335;&#26469;&#36827;&#34892;&#22238;&#22797;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Leveraging Few-Shot Data Augmentation and Waterfall Prompting for Response Generation. (arXiv:2308.01080v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#23569;&#26679;&#26412;&#25968;&#25454;&#22686;&#24378;&#21644;&#28689;&#24067;&#25552;&#31034;&#30340;&#26041;&#24335;&#26469;&#36827;&#34892;&#22238;&#22797;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#20998;&#26512;&#30830;&#23450;&#20851;&#38190;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#25105;&#20204;&#22312;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#24314;&#27169;&#20013;&#20351;&#29992;&#20027;&#35266;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#24378;&#35843;&#22238;&#22797;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#23545;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#65292;&#35780;&#20272;&#22238;&#22797;&#38271;&#24230;&#12289;&#24773;&#24863;&#21644;&#23545;&#35805;&#34892;&#20026;&#31561;&#20851;&#38190;&#22240;&#32032;&#26469;&#30830;&#23450;&#30340;&#12290;&#25105;&#20204;&#21033;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#20197;&#29983;&#25104;&#26032;&#30340;&#20027;&#35266;&#30693;&#35782;&#39033;&#30446;&#26469;&#22686;&#24378;&#25968;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;DSTC11&#30340;&#26041;&#27861;&#65306;&#65288;1&#65289;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#25506;&#32034;&#65292;&#65288;2&#65289;&#23558;&#26368;&#24120;&#35265;&#30340;&#38382;&#39064;&#24182;&#20837;&#25152;&#26377;&#29983;&#25104;&#30340;&#22238;&#22797;&#20013;&#65292;&#20197;&#21450;&#65288;3&#65289;&#20351;&#29992;GPT-3&#21644;ChatGPT&#30340;&#28689;&#24067;&#25552;&#31034;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses our approaches for task-oriented conversational modelling using subjective knowledge, with a particular emphasis on response generation. Our methodology was shaped by an extensive data analysis that evaluated key factors such as response length, sentiment, and dialogue acts present in the provided dataset. We used few-shot learning to augment the data with newly generated subjective knowledge items and present three approaches for DSTC11: (1) task-specific model exploration, (2) incorporation of the most frequent question into all generated responses, and (3) a waterfall prompting technique using a combination of both GPT-3 and ChatGPT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#20449;&#25903;&#25345;&#31995;&#32479;&#65292;&#33021;&#22815;&#26816;&#27979;&#38169;&#35823;&#30340;&#32763;&#35793;&#65292;&#20197;&#20419;&#36827;&#36328;&#35821;&#35328;&#20132;&#27969;&#12290;&#30740;&#31350;&#32773;&#24320;&#21457;&#20102;&#19968;&#20010;&#38169;&#35823;&#26816;&#27979;&#22120;&#20316;&#20026;&#31995;&#32479;&#30340;&#22522;&#32447;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#26085;&#33521;&#21452;&#35821;&#32842;&#22825;&#35821;&#26009;&#24211;&#65292;&#27492;&#20030;&#20026;&#26356;&#39640;&#32423;&#38169;&#35823;&#32763;&#35793;&#26816;&#27979;&#31995;&#32479;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2308.01044</link><description>&lt;p&gt;
&#36741;&#21161;&#36328;&#35821;&#35328;&#20132;&#27969;&#30340;&#32842;&#22825;&#32763;&#35793;&#38169;&#35823;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Chat Translation Error Detection for Assisting Cross-lingual Communications. (arXiv:2308.01044v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#36890;&#20449;&#25903;&#25345;&#31995;&#32479;&#65292;&#33021;&#22815;&#26816;&#27979;&#38169;&#35823;&#30340;&#32763;&#35793;&#65292;&#20197;&#20419;&#36827;&#36328;&#35821;&#35328;&#20132;&#27969;&#12290;&#30740;&#31350;&#32773;&#24320;&#21457;&#20102;&#19968;&#20010;&#38169;&#35823;&#26816;&#27979;&#22120;&#20316;&#20026;&#31995;&#32479;&#30340;&#22522;&#32447;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#26085;&#33521;&#21452;&#35821;&#32842;&#22825;&#35821;&#26009;&#24211;&#65292;&#27492;&#20030;&#20026;&#26356;&#39640;&#32423;&#38169;&#35823;&#32763;&#35793;&#26816;&#27979;&#31995;&#32479;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#36890;&#20449;&#25903;&#25345;&#31995;&#32479;&#30340;&#24320;&#21457;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26816;&#27979;&#38169;&#35823;&#30340;&#32763;&#35793;&#65292;&#20197;&#20419;&#36827;&#36328;&#35821;&#35328;&#20132;&#27969;&#65292;&#22240;&#20026;&#24403;&#21069;&#26426;&#22120;&#32842;&#22825;&#32763;&#35793;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#38169;&#35823;&#26816;&#27979;&#22120;&#20316;&#20026;&#31995;&#32479;&#30340;&#22522;&#32447;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#26085;&#33521;&#21452;&#35821;&#32842;&#22825;&#35821;&#26009;&#24211;&#65292;BPesona-chat&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#29992;&#20247;&#21253;&#36827;&#34892;&#36136;&#37327;&#35780;&#32423;&#30340;&#22810;&#36718;&#21475;&#35821;&#32842;&#22825;&#12290;&#38169;&#35823;&#26816;&#27979;&#22120;&#21487;&#20197;&#20026;&#26356;&#39640;&#32423;&#38169;&#35823;&#32763;&#35793;&#26816;&#27979;&#31995;&#32479;&#25171;&#19979;&#33391;&#22909;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we describe the development of a communication support system that detects erroneous translations to facilitate crosslingual communications due to the limitations of current machine chat translation methods. We trained an error detector as the baseline of the system and constructed a new Japanese-English bilingual chat corpus, BPersona-chat, which comprises multiturn colloquial chats augmented with crowdsourced quality ratings. The error detector can serve as an encouraging foundation for more advanced erroneous translation detection systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#25552;&#21319;&#25991;&#26412;&#36716;&#35821;&#38899;&#30340;&#21512;&#25104;&#36136;&#37327;&#65292;&#36890;&#36807;&#37325;&#26500;&#33258;&#30417;&#30563;&#23398;&#20064;&#34920;&#31034;&#65292;&#36741;&#21161;&#37325;&#26500;&#25439;&#22833;&#20197;&#21450;&#22686;&#21152;&#35299;&#30721;&#22120;&#30340;&#20256;&#36882;&#65292;&#23454;&#29616;&#20102;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#30340;&#35821;&#38899;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2308.01018</link><description>&lt;p&gt;
SALTTS&#65306;&#21033;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#25552;&#21319;&#25991;&#26412;&#36716;&#35821;&#38899;&#30340;&#21512;&#25104;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
SALTTS: Leveraging Self-Supervised Speech Representations for improved Text-to-Speech Synthesis. (arXiv:2308.01018v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#25552;&#21319;&#25991;&#26412;&#36716;&#35821;&#38899;&#30340;&#21512;&#25104;&#36136;&#37327;&#65292;&#36890;&#36807;&#37325;&#26500;&#33258;&#30417;&#30563;&#23398;&#20064;&#34920;&#31034;&#65292;&#36741;&#21161;&#37325;&#26500;&#25439;&#22833;&#20197;&#21450;&#22686;&#21152;&#35299;&#30721;&#22120;&#30340;&#20256;&#36882;&#65292;&#23454;&#29616;&#20102;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#30340;&#35821;&#38899;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
FastSpeech2&#35797;&#22270;&#23558;&#38899;&#35843;&#12289;&#33021;&#37327;&#21644;&#25345;&#32493;&#26102;&#38388;&#31561;&#35821;&#38899;&#26041;&#38754;&#30340;&#29305;&#24449;&#20316;&#20026;&#26465;&#20214;&#36755;&#20837;&#65292;&#20294;&#20173;&#26377;&#25552;&#21319;&#31354;&#38388;&#12290;&#26412;&#25991;&#30340;&#19968;&#37096;&#20998;&#24037;&#20316;&#26159;&#21033;&#29992;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#31034;&#26469;&#25552;&#39640;&#21512;&#25104;&#35821;&#38899;&#30340;&#36136;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;FastSpeech2&#32534;&#30721;&#22120;&#30340;&#38271;&#24230;&#35843;&#33410;&#36755;&#20986;&#36890;&#36807;&#19968;&#31995;&#21015;&#32534;&#30721;&#22120;&#23618;&#65292;&#29992;&#37325;&#26500;&#33258;&#30417;&#30563;&#23398;&#20064;&#34920;&#31034;&#30340;&#30446;&#26631;&#12290;&#22312;SALTTS-parallel&#23454;&#29616;&#20013;&#65292;&#26469;&#33258;&#31532;&#20108;&#20010;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#19982;&#33258;&#30417;&#30563;&#23398;&#20064;&#29305;&#24449;&#19968;&#36215;&#29992;&#20110;&#36741;&#21161;&#37325;&#26500;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#22312;SALTTS-cascade&#30340;&#23454;&#29616;&#20013;&#65292;&#36825;&#20123;&#34920;&#31034;&#19981;&#20165;&#36890;&#36807;&#35299;&#30721;&#22120;&#65292;&#36824;&#36890;&#36807;&#37325;&#26500;&#25439;&#22833;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#29305;&#24449;&#24102;&#26469;&#30340;&#35821;&#38899;&#29305;&#24449;&#30340;&#20016;&#23500;&#24615;&#20307;&#29616;&#22312;&#36755;&#20986;&#35821;&#38899;&#30340;&#36136;&#37327;&#19978;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#25351;&#26631;&#19978;&#20248;&#20110;&#22522;&#32447;&#30340;FastSpeech2&#12290;
&lt;/p&gt;
&lt;p&gt;
While FastSpeech2 aims to integrate aspects of speech such as pitch, energy, and duration as conditional inputs, it still leaves scope for richer representations. As a part of this work, we leverage representations from various Self-Supervised Learning (SSL) models to enhance the quality of the synthesized speech. In particular, we pass the FastSpeech2 encoder's length-regulated outputs through a series of encoder layers with the objective of reconstructing the SSL representations. In the SALTTS-parallel implementation, the representations from this second encoder are used for an auxiliary reconstruction loss with the SSL features. The SALTTS-cascade implementation, however, passes these representations through the decoder in addition to having the reconstruction loss. The richness of speech characteristics from the SSL features reflects in the output speech quality, with the objective and subjective evaluation measures of the proposed approach outperforming the baseline FastSpeech2.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#25945;&#25480;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#35299;&#20915;&#22810;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00946</link><description>&lt;p&gt;
&#25945;&#25480;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#32452;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Teaching Smaller Language Models To Generalise To Unseen Compositional Questions. (arXiv:2308.00946v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00946
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#25945;&#25480;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20219;&#21153;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#35299;&#20915;&#22810;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#19968;&#20010;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#25512;&#24191;&#21040;&#22238;&#31572;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#20986;&#29616;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#26368;&#22810;93&#20010;&#20219;&#21153;&#65292;&#26088;&#22312;&#22521;&#20859;&#22810;&#26679;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#32467;&#21512;&#20102;&#19968;&#20010;&#23494;&#38598;&#30340;&#26816;&#32034;&#31995;&#32479;&#65292;&#26088;&#22312;&#26816;&#32034;&#19968;&#32452;&#35777;&#25454;&#24615;&#30340;&#27573;&#33853;&#29255;&#27573;&#12290;&#22312;&#38382;&#31572;&#26041;&#38754;&#65292;&#26368;&#36817;&#30340;&#36827;&#23637;&#35201;&#20040;&#36890;&#36807;&#38024;&#23545;&#38750;&#24120;&#22823;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#26041;&#27861;&#23454;&#29616;&#38646;&#25110;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#35201;&#20040;&#36890;&#36807;&#24494;&#35843;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#26377;&#26102;&#32467;&#21512;&#20449;&#24687;&#26816;&#32034;&#36827;&#34892;&#12290;&#25105;&#20204;&#20851;&#27880;&#36739;&#23569;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#21363;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;&#23545;&#20110;&#19981;&#23384;&#22312;&#36275;&#22815;&#20449;&#24687;&#26469;&#22238;&#31572;&#29305;&#23450;&#38382;&#39064;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#26816;&#32034;&#26102;&#65292;&#33021;&#21542;&#23454;&#29616;&#38646;&#26679;&#26412;&#25512;&#24191;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#20026;&#22810;&#26679;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65288;StrategyQA&#65292;CommonsenseQA&#65292;IIRC&#65292;DROP&#65292;Musique&#21644;ARC-DA&#65289;&#24314;&#31435;&#20102;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We equip a smaller Language Model to generalise to answering challenging compositional questions that have not been seen in training. To do so we propose a combination of multitask supervised pretraining on up to 93 tasks designed to instill diverse reasoning abilities, and a dense retrieval system that aims to retrieve a set of evidential paragraph fragments. Recent progress in question-answering has been achieved either through prompting methods against very large pretrained Language Models in zero or few-shot fashion, or by fine-tuning smaller models, sometimes in conjunction with information retrieval. We focus on the less explored question of the extent to which zero-shot generalisation can be enabled in smaller models with retrieval against a corpus within which sufficient information to answer a particular question may not exist. We establish strong baselines in this setting for diverse evaluation datasets (StrategyQA, CommonsenseQA, IIRC, DROP, Musique and ARC-DA), and show tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#29305;&#24449;&#24863;&#30693;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;FA-GAN&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#25991;&#26412;GAN&#20013;&#30340;&#31163;&#25955;&#24615;&#12289;&#35757;&#32451;&#19981;&#31283;&#23450;&#12289;&#27169;&#24335;&#23849;&#28291;&#12289;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#21487;&#25511;&#24615;&#31561;&#38382;&#39064;&#12290;FA-GAN&#20351;&#29992;&#29305;&#24449;&#24863;&#30693;&#32534;&#30721;&#22120;&#21644;&#31867;&#21035;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#20197;&#21450;&#20851;&#31995;&#35760;&#24518;&#26680;&#30340;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#24207;&#21015;&#26469;&#25552;&#39640;&#21477;&#23376;&#22810;&#26679;&#24615;&#65292;&#24182;&#20855;&#26377;&#39069;&#22806;&#30340;&#31867;&#21035;&#20998;&#31867;&#22836;&#12290;</title><link>http://arxiv.org/abs/2308.00939</link><description>&lt;p&gt;
&#29305;&#24449;&#24863;&#30693;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29992;&#20110;&#31867;&#21035;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Feature-aware conditional GAN for category text generation. (arXiv:2308.00939v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#29305;&#24449;&#24863;&#30693;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;FA-GAN&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#25991;&#26412;GAN&#20013;&#30340;&#31163;&#25955;&#24615;&#12289;&#35757;&#32451;&#19981;&#31283;&#23450;&#12289;&#27169;&#24335;&#23849;&#28291;&#12289;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#21487;&#25511;&#24615;&#31561;&#38382;&#39064;&#12290;FA-GAN&#20351;&#29992;&#29305;&#24449;&#24863;&#30693;&#32534;&#30721;&#22120;&#21644;&#31867;&#21035;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#20197;&#21450;&#20851;&#31995;&#35760;&#24518;&#26680;&#30340;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#24207;&#21015;&#26469;&#25552;&#39640;&#21477;&#23376;&#22810;&#26679;&#24615;&#65292;&#24182;&#20855;&#26377;&#39069;&#22806;&#30340;&#31867;&#21035;&#20998;&#31867;&#22836;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#21035;&#25991;&#26412;&#29983;&#25104;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#23545;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#37117;&#26377;&#30410;&#22788;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#36825;&#24402;&#21151;&#20110;&#20854;&#23545;&#25239;&#35757;&#32451;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;GAN&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#31163;&#25955;&#24615;&#12289;&#35757;&#32451;&#19981;&#31283;&#23450;&#12289;&#27169;&#24335;&#23849;&#28291;&#12289;&#32570;&#20047;&#22810;&#26679;&#24615;&#21644;&#21487;&#25511;&#24615;&#31561;&#31561;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GAN&#26694;&#26550;&#65292;&#21363;&#29305;&#24449;&#24863;&#30693;&#30340;&#26465;&#20214;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;FA-GAN&#65289;&#65292;&#29992;&#20110;&#21487;&#25511;&#30340;&#31867;&#21035;&#25991;&#26412;&#29983;&#25104;&#12290;&#22312;FA-GAN&#20013;&#65292;&#29983;&#25104;&#22120;&#20855;&#26377;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#32467;&#26500;&#65292;&#29992;&#20110;&#25552;&#39640;&#21477;&#23376;&#22810;&#26679;&#24615;&#65292;&#23427;&#21253;&#25324;&#19977;&#20010;&#32534;&#30721;&#22120;&#65292;&#21253;&#25324;&#19968;&#20010;&#29305;&#24449;&#24863;&#30693;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#31867;&#21035;&#24863;&#30693;&#32534;&#30721;&#22120;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#20851;&#31995;&#35760;&#24518;&#26680;&#30340;&#35299;&#30721;&#22120;&#65292;&#20351;&#29992;Gumbel SoftMax&#28608;&#27963;&#20989;&#25968;&#12290;&#37492;&#21035;&#22120;&#36824;&#20855;&#26377;&#39069;&#22806;&#30340;&#31867;&#21035;&#20998;&#31867;&#22836;&#12290;&#20026;&#20102;&#29983;&#25104;&#25351;&#23450;&#31867;&#21035;&#30340;&#21477;&#23376;&#65292;
&lt;/p&gt;
&lt;p&gt;
Category text generation receives considerable attentions since it is beneficial for various natural language processing tasks. Recently, the generative adversarial network (GAN) has attained promising performance in text generation, attributed to its adversarial training process. However, there are several issues in text GANs, including discreteness, training instability, mode collapse, lack of diversity and controllability etc. To address these issues, this paper proposes a novel GAN framework, the feature-aware conditional GAN (FA-GAN), for controllable category text generation. In FA-GAN, the generator has a sequence-to-sequence structure for improving sentence diversity, which consists of three encoders including a special feature-aware encoder and a category-aware encoder, and one relational-memory-core-based decoder with the Gumbel SoftMax activation function. The discriminator has an additional category classification head. To generate sentences with specified categories, the m
&lt;/p&gt;</description></item><item><title>DiactTOD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#28508;&#22312;&#23545;&#35805;&#34892;&#20026;&#27169;&#22411;&#65292;&#22312;&#27809;&#26377;&#26174;&#24335;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#39044;&#27979;&#21644;&#25511;&#21046;&#23545;&#35805;&#34892;&#20026;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#29983;&#25104;&#21487;&#25511;&#30340;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#21709;&#24212;&#29983;&#25104;&#20013;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00878</link><description>&lt;p&gt;
DiactTOD&#65306;&#23398;&#20064;&#36890;&#29992;&#30340;&#28508;&#22312;&#23545;&#35805;&#34892;&#20026;&#20197;&#23454;&#29616;&#21487;&#25511;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DiactTOD: Learning Generalizable Latent Dialogue Acts for Controllable Task-Oriented Dialogue Systems. (arXiv:2308.00878v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00878
&lt;/p&gt;
&lt;p&gt;
DiactTOD&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#28508;&#22312;&#23545;&#35805;&#34892;&#20026;&#27169;&#22411;&#65292;&#22312;&#27809;&#26377;&#26174;&#24335;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#39044;&#27979;&#21644;&#25511;&#21046;&#23545;&#35805;&#34892;&#20026;&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#29983;&#25104;&#21487;&#25511;&#30340;&#21709;&#24212;&#12290;&#35813;&#26041;&#27861;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#21709;&#24212;&#29983;&#25104;&#20013;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#34892;&#20026;&#27880;&#37322;&#23545;&#20110;&#25913;&#21892;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#21709;&#24212;&#29983;&#25104;&#36136;&#37327;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#21487;&#33021;&#20855;&#26377;&#19981;&#20860;&#23481;&#30340;&#27880;&#37322;&#65292;&#20351;&#29992;&#23545;&#35805;&#34892;&#20026;&#20197;&#36890;&#29992;&#30340;&#26041;&#24335;&#25511;&#21046;&#21709;&#24212;&#29983;&#25104;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#21033;&#29992;&#28508;&#22312;&#21160;&#20316;&#31354;&#38388;&#25110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26367;&#20195;&#26041;&#27861;&#19981;&#38656;&#35201;&#26174;&#24335;&#27880;&#37322;&#65292;&#20294;&#21487;&#33021;&#32570;&#20047;&#35299;&#37322;&#24615;&#25110;&#36935;&#21040;&#23450;&#20041;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#30340;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#28508;&#22312;&#23545;&#35805;&#34892;&#20026;&#27169;&#22411;&#65288;DiactTOD&#65289;&#65292;&#35813;&#27169;&#22411;&#23558;&#23545;&#35805;&#34892;&#20026;&#34920;&#31034;&#20026;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#21521;&#37327;&#12290;DiactTOD&#22312;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#21518;&#65292;&#33021;&#22815;&#20351;&#29992;&#36825;&#20123;&#28508;&#22312;&#34920;&#31034;&#39044;&#27979;&#21644;&#25511;&#21046;&#23545;&#35805;&#34892;&#20026;&#65292;&#20197;&#38646;-shot&#30340;&#26041;&#24335;&#29983;&#25104;&#21487;&#25511;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MultiWOZ&#25968;&#25454;&#38598;&#30340;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38646;-shot&#12289;&#23569;-shot&#21644;&#23436;&#25972;&#25968;&#25454;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue act annotations are important to improve response generation quality in task-oriented dialogue systems. However, it can be challenging to use dialogue acts to control response generation in a generalizable way because different datasets and tasks may have incompatible annotations. While alternative methods that utilize latent action spaces or reinforcement learning do not require explicit annotations, they may lack interpretability or face difficulties defining task-specific rewards. In this work, we present a novel end-to-end latent dialogue act model (DiactTOD) that represents dialogue acts in a latent space. DiactTOD, when pre-trained on a large corpus, is able to predict and control dialogue acts to generate controllable responses using these latent representations in a zero-shot fashion. Our approach demonstrates state-of-the-art performance across a wide range of experimental settings on the MultiWOZ dataset, including zero-shot, few-shot, and full data fine-tuning with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#29616;&#20195;&#24076;&#33098;&#26041;&#35328;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;GRDD&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#26041;&#35328;&#35782;&#21035;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20063;&#33021;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.00802</link><description>&lt;p&gt;
GRDD: &#24076;&#33098;&#26041;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
GRDD: A Dataset for Greek Dialectal NLP. (arXiv:2308.00802v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#29616;&#20195;&#24076;&#33098;&#26041;&#35328;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;GRDD&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#26041;&#35328;&#35782;&#21035;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20063;&#33021;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#29616;&#20195;&#24076;&#33098;&#26041;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20811;&#37324;&#29305;&#12289;&#24222;&#25552;&#12289;&#21271;&#24076;&#33098;&#21644;&#22622;&#28006;&#36335;&#26031;&#24076;&#33098;&#22235;&#31181;&#26041;&#35328;&#30340;&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#12290;&#23613;&#31649;&#23384;&#22312;&#19981;&#24179;&#34913;&#65292;&#20294;&#35813;&#25968;&#25454;&#38598;&#26159;&#30456;&#24403;&#22823;&#30340;&#65292;&#24182;&#19988;&#26159;&#21019;&#24314;&#29616;&#20195;&#24076;&#33098;&#26041;&#35328;&#31867;&#20284;&#36164;&#28304;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#26041;&#35328;&#35782;&#21035;&#65292;&#24182;&#23581;&#35797;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#38750;&#24120;&#22909;&#65292;&#36825;&#21487;&#33021;&#34920;&#26126;&#25152;&#30740;&#31350;&#30340;&#26041;&#35328;&#20855;&#26377;&#36275;&#22815;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20063;&#33021;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#38024;&#23545;&#34920;&#29616;&#26368;&#20339;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#38169;&#35823;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#38169;&#35823;&#26159;&#30001;&#20110;&#25968;&#25454;&#38598;&#28165;&#29702;&#19981;&#36275;&#36896;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a dataset for the computational study of a number of Modern Greek dialects. It consists of raw text data from four dialects of Modern Greek, Cretan, Pontic, Northern Greek and Cypriot Greek. The dataset is of considerable size, albeit imbalanced, and presents the first attempt to create large scale dialectal resources of this type for Modern Greek dialects. We then use the dataset to perform dialect idefntification. We experiment with traditional ML algorithms, as well as simple DL architectures. The results show very good performance on the task, potentially revealing that the dialects in question have distinct enough characteristics allowing even simple ML models to perform well on the task. Error analysis is performed for the top performing algorithms showing that in a number of cases the errors are due to insufficient dataset cleaning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#23637;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#21040;&#35780;&#35770;&#39033;&#26816;&#32034;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;BERT&#23884;&#20837;&#65292;&#20197;&#34701;&#21512;&#26597;&#35810;&#21644;&#35780;&#35770;&#24471;&#20998;&#24182;&#36827;&#34892;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.00762</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;BERT&#24494;&#35843;&#29992;&#20110;&#22522;&#20110;&#34701;&#21512;&#30340;&#35780;&#35770;&#39033;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Contrastive BERT Fine-tuning for Fusion-based Reviewed-Item Retrieval. (arXiv:2308.00762v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25193;&#23637;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#21040;&#35780;&#35770;&#39033;&#26816;&#32034;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;BERT&#23884;&#20837;&#65292;&#20197;&#34701;&#21512;&#26597;&#35810;&#21644;&#35780;&#35770;&#24471;&#20998;&#24182;&#36827;&#34892;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#20351;&#29992;&#25143;&#33021;&#22815;&#34920;&#36798;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#29992;&#25143;&#35780;&#35770;&#20869;&#23481;&#20063;&#21576;&#29190;&#28856;&#24335;&#22686;&#38271;&#65292;&#36825;&#21487;&#20197;&#20351;&#29992;&#25143;&#26356;&#22909;&#22320;&#25214;&#21040;&#19982;&#36825;&#20123;&#34920;&#36798;&#24615;&#26597;&#35810;&#21305;&#37197;&#30340;&#39184;&#21381;&#12289;&#20070;&#31821;&#25110;&#30005;&#24433;&#31561;&#29289;&#21697;&#12290;&#34429;&#28982;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;(IR)&#26041;&#27861;&#20026;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#21305;&#37197;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#25193;&#23637;&#21040;&#35780;&#20272;&#39033;&#26816;&#32034;(RIR)&#20219;&#21153;&#65292;&#20854;&#20013;&#26597;&#35810;-&#35780;&#35770;&#24471;&#20998;&#24517;&#39035;&#32858;&#21512;(&#25110;&#34701;&#21512;)&#25104;&#29289;&#21697;&#32423;&#24471;&#20998;&#36827;&#34892;&#25490;&#21517;&#12290;&#22312;&#27809;&#26377;&#26631;&#35760;&#30340;RIR&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#33258;&#30417;&#30563;&#26041;&#27861;&#23545;BERT&#23884;&#20837;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#26469;&#23558;&#31070;&#32463;IR&#26041;&#27861;&#25193;&#23637;&#21040;RIR&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#27604;&#23398;&#20064;&#38656;&#35201;&#36873;&#25321;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#65292;&#32780;&#25105;&#20204;&#30340;&#39033;-&#35780;&#35770;&#25968;&#25454;&#30340;&#29420;&#29305;&#20108;&#32423;&#32467;&#26500;&#32467;&#21512;&#20803;&#25968;&#25454;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#26679;&#26412;&#36873;&#25321;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
As natural language interfaces enable users to express increasingly complex natural language queries, there is a parallel explosion of user review content that can allow users to better find items such as restaurants, books, or movies that match these expressive queries. While Neural Information Retrieval (IR) methods have provided state-of-the-art results for matching queries to documents, they have not been extended to the task of Reviewed-Item Retrieval (RIR), where query-review scores must be aggregated (or fused) into item-level scores for ranking. In the absence of labeled RIR datasets, we extend Neural IR methodology to RIR by leveraging self-supervised methods for contrastive learning of BERT embeddings for both queries and reviews. Specifically, contrastive learning requires a choice of positive and negative samples, where the unique two-level structure of our item-review data combined with meta-data affords us a rich structure for the selection of these samples. For contrasti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20854;&#20027;&#35201;&#21407;&#22240;&#26159;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#25552;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#19968;&#26086;&#32771;&#34385;&#21040;&#21508;&#31181;&#20998;&#24067;&#24046;&#24322;&#65292;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#26174;&#33879;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2308.00755</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#25918;&#22823;&#24726;&#35770;
&lt;/p&gt;
&lt;p&gt;
The Bias Amplification Paradox in Text-to-Image Generation. (arXiv:2308.00755v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20854;&#20027;&#35201;&#21407;&#22240;&#26159;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#25552;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#19968;&#26086;&#32771;&#34385;&#21040;&#21508;&#31181;&#20998;&#24067;&#24046;&#24322;&#65292;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#35265;&#25918;&#22823;&#26159;&#19968;&#31181;&#27169;&#22411;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#24179;&#34913;&#30340;&#29616;&#35937;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#26469;&#27604;&#36739;&#35757;&#32451;&#25968;&#25454;&#19982;&#29983;&#25104;&#22270;&#20687;&#20013;&#30340;&#24615;&#21035;&#27604;&#20363;&#65292;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#39046;&#22495;&#20013;&#30340;&#20559;&#35265;&#25918;&#22823;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#20284;&#20046;&#25918;&#22823;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#24615;&#21035;-&#32844;&#19994;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#25918;&#22823;&#24456;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#24402;&#22240;&#20110;&#35757;&#32451;&#25968;&#25454;&#21644;&#27169;&#22411;&#25552;&#31034;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20363;&#22914;&#65292;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#26631;&#39064;&#36890;&#24120;&#21253;&#21547;&#26126;&#30830;&#30340;&#24615;&#21035;&#20449;&#24687;&#65292;&#32780;&#25105;&#20204;&#20351;&#29992;&#30340;&#25552;&#31034;&#21017;&#19981;&#21253;&#21547;&#65292;&#36825;&#23548;&#33268;&#20102;&#20998;&#24067;&#30340;&#20559;&#31227;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#20559;&#35265;&#24230;&#37327;&#12290;&#19968;&#26086;&#25105;&#20204;&#32771;&#34385;&#21040;&#35757;&#32451;&#21644;&#29983;&#25104;&#26102;&#20351;&#29992;&#30340;&#25991;&#26412;&#20043;&#38388;&#30340;&#21508;&#31181;&#20998;&#24067;&#24046;&#24322;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25918;&#22823;&#29616;&#35937;&#22823;&#22823;&#20943;&#23569;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#35828;&#26126;&#20102;&#27604;&#36739;&#27169;&#22411;&#21644;&#23427;&#20204;&#25152;&#35757;&#32451;&#30340;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#24378;&#35843;&#20102;&#28151;&#28102;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias amplification is a phenomenon in which models increase imbalances present in the training data. In this paper, we study bias amplification in the text-to-image domain using Stable Diffusion by comparing gender ratios in training vs. generated images. We find that the model appears to amplify gender-occupation biases found in the training data (LAION). However, we discover that amplification can largely be attributed to discrepancies between training captions and model prompts. For example, an inherent difference is that captions from the training data often contain explicit gender information while the prompts we use do not, which leads to a distribution shift and consequently impacts bias measures. Once we account for various distributional differences between texts used for training and generation, we observe that amplification decreases considerably. Our findings illustrate the challenges of comparing biases in models and the data they are trained on, and highlight confounding 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#33258;&#26816;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#65292;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#24182;&#25552;&#39640;&#20102;&#38382;&#31572;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.00436</link><description>&lt;p&gt;
SelfCheck: &#20351;&#29992;LLMs&#33258;&#26816;&#20854;&#36880;&#27493;&#25512;&#29702;&#30340;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#33258;&#26816;&#36880;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#65292;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#24182;&#25552;&#39640;&#20102;&#38382;&#31572;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#30340;&#21457;&#26126;&#65292;&#20351;&#24471;&#35299;&#20915;&#25512;&#29702;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26368;&#24378;&#22823;&#30340;LLMs&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#38656;&#35201;&#38750;&#32447;&#24615;&#24605;&#32500;&#21644;&#22810;&#27493;&#25512;&#29702;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLMs&#26159;&#21542;&#20855;&#26377;&#35782;&#21035;&#33258;&#24049;&#38169;&#35823;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#36880;&#27493;&#25512;&#29702;&#20013;&#30340;&#20010;&#21035;&#38169;&#35823;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;-shot&#39564;&#35777;&#26041;&#26696;&#20197;&#35782;&#21035;&#27492;&#31867;&#38169;&#35823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#39564;&#35777;&#26041;&#26696;&#26469;&#25913;&#36827;&#38382;&#31572;&#24615;&#33021;&#65292;&#36890;&#36807;&#23545;&#19981;&#21516;&#29983;&#25104;&#30340;&#31572;&#26696;&#36827;&#34892;&#21152;&#26435;&#25237;&#31080;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#23398;&#25968;&#25454;&#38598;-GSM8K&#65292;MathQA&#21644;MATH&#19978;&#27979;&#35797;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#25104;&#21151;&#35782;&#21035;&#38169;&#35823;&#65292;&#24182;&#36827;&#32780;&#25552;&#39640;&#20102;&#26368;&#32456;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent progress in large language models (LLMs), especially the invention of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning problems. However, even the strongest LLMs are still struggling with more complicated problems that require non-linear thinking and multi-step reasoning. In this work, we explore whether LLMs have the ability to recognize their own errors, without resorting to external resources. In particular, we investigate whether they can be used to identify individual errors within a step-by-step reasoning. To this end, we propose a zero-shot verification scheme to recognize such errors. We then use this verification scheme to improve question-answering performance, by using it to perform weighted voting on different generated answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and find that it successfully recognizes errors and, in turn, increases final predictive performance.
&lt;/p&gt;</description></item><item><title>ZRIGF&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#36164;&#28304;&#24773;&#22659;&#19979;&#30340;&#22270;&#20687;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#12290;&#23427;&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#23454;&#29616;&#20102;&#35270;&#35273;&#29305;&#24449;&#30340;&#23545;&#40784;&#65292;&#29983;&#25104;&#26377;&#27934;&#23519;&#21147;&#30340;&#22238;&#24212;&#12290;</title><link>http://arxiv.org/abs/2308.00400</link><description>&lt;p&gt;
ZRIGF&#65306;&#19968;&#31181;&#29992;&#20110;&#26080;&#36164;&#28304;&#22270;&#20687;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#21019;&#26032;&#22810;&#27169;&#24577;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ZRIGF: An Innovative Multimodal Framework for Zero-Resource Image-Grounded Dialogue Generation. (arXiv:2308.00400v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00400
&lt;/p&gt;
&lt;p&gt;
ZRIGF&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#36164;&#28304;&#24773;&#22659;&#19979;&#30340;&#22270;&#20687;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#12290;&#23427;&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#23454;&#29616;&#20102;&#35270;&#35273;&#29305;&#24449;&#30340;&#23545;&#40784;&#65292;&#29983;&#25104;&#26377;&#27934;&#23519;&#21147;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#36890;&#36807;&#25972;&#21512;&#35270;&#35273;&#20449;&#24687;&#65292;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22238;&#24212;&#26041;&#38754;&#20855;&#26377;&#24456;&#22823;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#22312;&#26080;&#36164;&#28304;&#24773;&#22659;&#20013;&#38590;&#20197;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22810;&#27169;&#24577;&#26694;&#26550;&#65292;&#31216;&#20026;ZRIGF&#65292;&#23427;&#22312;&#26080;&#36164;&#28304;&#24773;&#22659;&#20013;&#34701;&#21512;&#20102;&#22270;&#20687;&#39537;&#21160;&#20449;&#24687;&#26469;&#29983;&#25104;&#23545;&#35805;&#12290;ZRIGF&#37319;&#29992;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#21253;&#25324;&#23545;&#27604;&#39044;&#35757;&#32451;&#21644;&#29983;&#25104;&#39044;&#35757;&#32451;&#12290;&#23545;&#27604;&#39044;&#35757;&#32451;&#21253;&#25324;&#19968;&#20010;&#25991;&#26412;-&#22270;&#20687;&#21305;&#37197;&#27169;&#22359;&#65292;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#26144;&#23556;&#21040;&#32479;&#19968;&#30340;&#32534;&#30721;&#21521;&#37327;&#31354;&#38388;&#20013;&#65292;&#20197;&#21450;&#19968;&#20010;&#25991;&#26412;&#36741;&#21161;&#30340;&#36974;&#34109;&#22270;&#20687;&#24314;&#27169;&#27169;&#22359;&#65292;&#29992;&#20110;&#20445;&#23384;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#29305;&#24449;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#22810;&#27169;&#24577;&#29305;&#24449;&#23545;&#40784;&#12290;&#29983;&#25104;&#39044;&#35757;&#32451;&#20351;&#29992;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;&#21644;&#20449;&#24687;&#20256;&#36882;&#27169;&#22359;&#26469;&#29983;&#25104;&#26377;&#27934;&#23519;&#21147;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-grounded dialogue systems benefit greatly from integrating visual information, resulting in high-quality response generation. However, current models struggle to effectively utilize such information in zero-resource scenarios, mainly due to the disparity between image and text modalities. To overcome this challenge, we propose an innovative multimodal framework, called ZRIGF, which assimilates image-grounded information for dialogue generation in zero-resource situations. ZRIGF implements a two-stage learning strategy, comprising contrastive pre-training and generative pre-training. Contrastive pre-training includes a text-image matching module that maps images and texts into a unified encoded vector space, along with a text-assisted masked image modeling module that preserves pre-training visual features and fosters further multimodal feature alignment. Generative pre-training employs a multimodal fusion module and an information transfer module to produce insightful responses b
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#20197;&#21450;&#21033;&#29992;&#23884;&#20837;&#27169;&#22411;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#20013;&#35821;&#20041;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#30693;&#35782;&#22270;&#35889;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#20114;&#21463;&#30410;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2308.00081</link><description>&lt;p&gt;
&#20026;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26500;&#24314;&#35821;&#20041;&#20016;&#23500;&#30340;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Towards Semantically Enriched Embeddings for Knowledge Graph Completion. (arXiv:2308.00081v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#20197;&#21450;&#21033;&#29992;&#23884;&#20837;&#27169;&#22411;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#20013;&#35821;&#20041;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#30693;&#35782;&#22270;&#35889;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#20114;&#21463;&#30410;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23884;&#20837;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#22823;&#22810;&#25968;&#31639;&#27861;&#23558;&#30693;&#35782;&#22270;&#35889;&#35270;&#20026;&#19968;&#20010;&#22810;&#21521;&#26631;&#35760;&#22270;&#65292;&#32570;&#20047;&#25429;&#25417;&#24213;&#23618;&#35821;&#20041;&#30340;&#33021;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25429;&#33719;&#20102;&#22823;&#37327;&#20449;&#24687;&#65292;&#36825;&#19968;&#25429;&#33719;&#23545;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#24433;&#21709;&#12290;&#30693;&#35782;&#22270;&#35889;&#21487;&#20197;&#20174;LLMs&#20013;&#21463;&#30410;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#19981;&#21516;&#29983;&#25104;&#23884;&#20837;&#27169;&#22411;&#21464;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#12290;&#39318;&#20808;&#35752;&#35770;&#20102;&#21508;&#31181;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#31639;&#27861;&#65292;&#22914;&#36716;&#23548;&#21644;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20197;&#21450;&#23454;&#20307;&#31867;&#22411;&#39044;&#27979;&#31639;&#27861;&#12290;&#28982;&#21518;&#65292;&#20171;&#32461;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#31867;&#22411;&#20449;&#24687;&#12289;LLMs&#20197;&#21450;&#25429;&#25417;&#19981;&#21516;&#25551;&#36848;&#36923;&#36753;&#20844;&#29702;&#20013;&#30340;&#35821;&#20041;&#30340;&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#23545;&#29616;&#26377;&#31639;&#27861;&#30340;&#20851;&#38190;&#21453;&#24605;&#23545;&#35770;&#25991;&#36827;&#34892;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
Embedding based Knowledge Graph (KG) Completion has gained much attention over the past few years. Most of the current algorithms consider a KG as a multidirectional labeled graph and lack the ability to capture the semantics underlying the schematic information. In a separate development, a vast amount of information has been captured within the Large Language Models (LLMs) which has revolutionized the field of Artificial Intelligence. KGs could benefit from these LLMs and vice versa. This vision paper discusses the existing algorithms for KG completion based on the variations for generating KG embeddings. It starts with discussing various KG completion algorithms such as transductive and inductive link prediction and entity type prediction algorithms. It then moves on to the algorithms utilizing type information within the KGs, LLMs, and finally to algorithms capturing the semantics represented in different description logic axioms. We conclude the paper with a critical reflection on
&lt;/p&gt;</description></item><item><title>AsdKB&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#26089;&#26399;&#31579;&#36873;&#21644;&#35786;&#26029;&#30340;&#20013;&#25991;&#30693;&#35782;&#24211;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#30142;&#30149;&#21644;&#35786;&#26029;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#12289;&#36741;&#21161;&#35786;&#26029;&#21644;&#19987;&#23478;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2307.16773</link><description>&lt;p&gt;
AsdKB: &#19968;&#20010;&#29992;&#20110;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#26089;&#26399;&#31579;&#36873;&#21644;&#35786;&#26029;&#30340;&#20013;&#25991;&#30693;&#35782;&#24211;
&lt;/p&gt;
&lt;p&gt;
AsdKB: A Chinese Knowledge Base for the Early Screening and Diagnosis of Autism Spectrum Disorder. (arXiv:2307.16773v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16773
&lt;/p&gt;
&lt;p&gt;
AsdKB&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#26089;&#26399;&#31579;&#36873;&#21644;&#35786;&#26029;&#30340;&#20013;&#25991;&#30693;&#35782;&#24211;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#30142;&#30149;&#21644;&#35786;&#26029;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#20197;&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#12289;&#36741;&#21161;&#35786;&#26029;&#21644;&#19987;&#23478;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20415;&#25463;&#22320;&#33719;&#21462;&#26377;&#20851;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#30340;&#30693;&#35782;&#24182;&#24110;&#21161;&#20854;&#26089;&#26399;&#31579;&#36873;&#21644;&#35786;&#26029;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;AsdKB&#65292;&#19968;&#20010;&#20851;&#20110;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#30340;&#20013;&#25991;&#30693;&#35782;&#24211;&#12290;&#35813;&#30693;&#35782;&#24211;&#24314;&#31435;&#22312;&#22810;&#31181;&#26469;&#28304;&#30340;&#22522;&#30784;&#19978;&#65292;&#21253;&#25324;1&#65289;&#20174;SNOMED CT&#21644;ICD-10&#30340;&#20020;&#24202;&#25551;&#36848;&#20013;&#33719;&#24471;&#30340;&#30142;&#30149;&#30693;&#35782;&#65292;2&#65289;&#20174;DSM-5&#21644;&#31038;&#20250;&#32452;&#32455;&#21644;&#21307;&#23398;&#30740;&#31350;&#26426;&#26500;&#25512;&#33616;&#30340;&#19981;&#21516;&#31579;&#36873;&#24037;&#20855;&#20013;&#33719;&#24471;&#30340;&#35786;&#26029;&#30693;&#35782;&#65292;&#20197;&#21450;3&#65289;&#26469;&#33258;&#32593;&#32476;&#19978;&#19987;&#19994;&#21307;&#29983;&#21644;&#21307;&#38498;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;AsdKB&#21253;&#21547;&#26412;&#20307;&#30693;&#35782;&#21644;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807; https://w3id.org/asdkb/ &#20316;&#20026;&#38142;&#25509;&#25968;&#25454;&#36827;&#34892;&#35775;&#38382;&#12290;AsdKB&#30340;&#28508;&#22312;&#24212;&#29992;&#21253;&#25324;&#38382;&#39064;&#22238;&#31572;&#12289;&#36741;&#21161;&#35786;&#26029;&#21644;&#19987;&#23478;&#25512;&#33616;&#65292;&#24182;&#19988;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#21407;&#22411;&#26469;&#36827;&#34892;&#28436;&#31034;&#65292;&#35813;&#21407;&#22411;&#21487;&#20197;&#36890;&#36807;&#27492;http URL&#36827;&#34892;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
To easily obtain the knowledge about autism spectrum disorder and help its early screening and diagnosis, we create AsdKB, a Chinese knowledge base on autism spectrum disorder. The knowledge base is built on top of various sources, including 1) the disease knowledge from SNOMED CT and ICD-10 clinical descriptions on mental and behavioural disorders, 2) the diagnostic knowledge from DSM-5 and different screening tools recommended by social organizations and medical institutes, and 3) the expert knowledge on professional physicians and hospitals from the Web. AsdKB contains both ontological and factual knowledge, and is accessible as Linked Data at https://w3id.org/asdkb/. The potential applications of AsdKB are question answering, auxiliary diagnosis, and expert recommendation, and we illustrate them with a prototype which can be accessed at this http URL
&lt;/p&gt;</description></item><item><title>LLMs4OL&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26412;&#20307;&#23398;&#20064;&#20013;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#33021;&#22815;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.16648</link><description>&lt;p&gt;
LLMs4OL: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26412;&#20307;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LLMs4OL: Large Language Models for Ontology Learning. (arXiv:2307.16648v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16648
&lt;/p&gt;
&lt;p&gt;
LLMs4OL&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26412;&#20307;&#23398;&#20064;&#20013;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#33021;&#22815;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;LLMs4OL&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26412;&#20307;&#23398;&#20064;&#65288;OL&#65289;&#12290;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#30693;&#35782;&#39046;&#22495;&#20013;&#25429;&#25417;&#22797;&#26434;&#35821;&#35328;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;LLMs4OL&#33539;&#24335;&#30740;&#31350;&#20102;&#20197;&#19979;&#20551;&#35774;&#65306;\textit{LLMs&#33021;&#21542;&#26377;&#25928;&#24212;&#29992;&#23427;&#20204;&#30340;&#35821;&#35328;&#27169;&#24335;&#25429;&#25417;&#33021;&#21147;&#21040;OL&#20013;&#65292;&#36825;&#28041;&#21450;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;?} &#20026;&#20102;&#27979;&#35797;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#20351;&#29992;&#38646;-shot&#25552;&#31034;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20061;&#20010;&#19981;&#21516;&#30340;LLM&#27169;&#22411;&#26063;&#32676;&#65292;&#38024;&#23545;&#19977;&#20010;&#20027;&#35201;&#30340;OL&#20219;&#21153;&#65306;&#26415;&#35821;&#31867;&#22411;&#21010;&#20998;&#12289;&#23618;&#32423;&#21457;&#29616;&#21644;&#38750;&#23618;&#32423;&#20851;&#31995;&#30340;&#25552;&#21462;&#12290;&#27492;&#22806;&#65292;&#35780;&#20272;&#36824;&#28085;&#30422;&#20102;&#26412;&#20307;&#30693;&#35782;&#30340;&#19981;&#21516;&#31867;&#22411;&#65292;&#21253;&#25324;WordNet&#20013;&#30340;&#35789;&#27719;&#35821;&#20041;&#30693;&#35782;&#12289;GeoNames&#20013;&#30340;&#22320;&#29702;&#30693;&#35782;&#21644;UMLS&#20013;&#30340;&#21307;&#23398;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs) for Ontology Learning (OL). LLMs have shown significant advancements in natural language processing, demonstrating their ability to capture complex language patterns in different knowledge domains. Our LLMs4OL paradigm investigates the following hypothesis: \textit{Can LLMs effectively apply their language pattern capturing capability to OL, which involves automatically extracting and structuring knowledge from natural language text?} To test this hypothesis, we conduct a comprehensive evaluation using the zero-shot prompting method. We evaluate nine different LLM model families for three main OL tasks: term typing, taxonomy discovery, and extraction of non-taxonomic relations. Additionally, the evaluations encompass diverse genres of ontological knowledge, including lexicosemantic knowledge in WordNet, geographical knowledge in GeoNames, and medical knowledge in UMLS.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31169;&#23494;&#27700;&#21360;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#65292;&#24182;&#20849;&#20139;&#37096;&#20998;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#39640;&#20934;&#30830;&#24615;&#30340;&#26816;&#27979;&#65292;&#21516;&#26102;&#23545;&#29983;&#25104;&#21644;&#26816;&#27979;&#36895;&#24230;&#24433;&#21709;&#26368;&#23567;&#12290;</title><link>http://arxiv.org/abs/2307.16230</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31169;&#23494;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
A Private Watermark for Large Language Models. (arXiv:2307.16230v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16230
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31169;&#23494;&#27700;&#21360;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#65292;&#24182;&#20849;&#20139;&#37096;&#20998;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#39640;&#20934;&#30830;&#24615;&#30340;&#26816;&#27979;&#65292;&#21516;&#26102;&#23545;&#29983;&#25104;&#21644;&#26816;&#27979;&#36895;&#24230;&#24433;&#21709;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#24050;&#32463;&#20943;&#36731;&#20102;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#24102;&#26469;&#30340;&#20266;&#26032;&#38395;&#21644;&#29256;&#26435;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#30340;&#27700;&#21360;&#26816;&#27979;&#38656;&#35201;&#29983;&#25104;&#36807;&#31243;&#30340;&#23494;&#38053;&#65292;&#20351;&#20854;&#23481;&#26131;&#21463;&#21040;&#36829;&#35268;&#21644;&#20266;&#36896;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31169;&#23494;&#27700;&#21360;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#38454;&#27573;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#32780;&#19981;&#26159;&#20351;&#29992;&#30456;&#21516;&#30340;&#23494;&#38053;&#26469;&#25193;&#23637;&#24403;&#21069;&#30340;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#12290;&#21516;&#26102;&#65292;&#27700;&#21360;&#29983;&#25104;&#21644;&#26816;&#27979;&#32593;&#32476;&#30340;&#37096;&#20998;&#21442;&#25968;&#26159;&#20849;&#20139;&#30340;&#65292;&#36825;&#20351;&#24471;&#26816;&#27979;&#32593;&#32476;&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#30001;&#20110;&#20004;&#20010;&#32593;&#32476;&#30340;&#21442;&#25968;&#35268;&#27169;&#36739;&#23567;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30830;&#20445;&#20102;&#39640;&#30340;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#23545;&#29983;&#25104;&#21644;&#26816;&#27979;&#36895;&#24230;&#30340;&#24433;&#21709;&#26368;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, text watermarking algorithms for large language models (LLMs) have been mitigating the potential harms of text generated by the LLMs, including fake news and copyright issues. However, the watermark detection of current text algorithms requires the key from the generation process, making them susceptible to breaches and counterfeiting. In this work, we propose the first private watermarking algorithm, which extends the current text watermarking algorithms by using two different neural networks respectively for watermark generation and detection, rather than using the same key at both stages. Meanwhile, part of the parameters of the watermark generation and detection networks are shared, which makes the detection network achieve a high accuracy very efficiently. Experiments show that our algorithm ensures high detection accuracy with minimal impact on generation and detection speed, due to the small parameter size of both networks. Additionally, our subsequent analysis demonst
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SEED-Bench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;SEED-Bench&#21253;&#25324;19K&#20010;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#28085;&#30422;&#20102;&#22270;&#20687;&#21644;&#35270;&#39057;&#27169;&#24577;&#31561;12&#20010;&#35780;&#20272;&#32500;&#24230;&#12290;&#36890;&#36807;&#20154;&#24037;&#27880;&#37322;&#25552;&#20379;&#30340;&#27491;&#30830;&#36873;&#39033;&#65292;&#33021;&#22815;&#23458;&#35266;&#39640;&#25928;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.16125</link><description>&lt;p&gt;
SEED-Bench: &#29992;&#29983;&#25104;&#24335;&#29702;&#35299;&#23545;&#22810;&#27169;&#24577;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension. (arXiv:2307.16125v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16125
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;SEED-Bench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;SEED-Bench&#21253;&#25324;19K&#20010;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#28085;&#30422;&#20102;&#22270;&#20687;&#21644;&#35270;&#39057;&#27169;&#24577;&#31561;12&#20010;&#35780;&#20272;&#32500;&#24230;&#12290;&#36890;&#36807;&#20154;&#24037;&#27880;&#37322;&#25552;&#20379;&#30340;&#27491;&#30830;&#36873;&#39033;&#65292;&#33021;&#22815;&#23458;&#35266;&#39640;&#25928;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22522;&#20110;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23637;&#31034;&#20986;&#20102;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;SEED-Bench&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35299;&#20915;&#20102;&#23545;MLLMs&#20013;&#29983;&#25104;&#24335;&#29702;&#35299;&#30340;&#35780;&#20272;&#38382;&#39064;&#65292;&#36825;&#26159;&#23545;&#29983;&#25104;&#24335;&#27169;&#22411;&#20840;&#38754;&#35780;&#20272;&#30340;&#19968;&#20010;&#21021;&#27493;&#27493;&#39588;&#12290;SEED-Bench&#21253;&#25324;19K&#20010;&#20934;&#30830;&#30340;&#20154;&#24037;&#27880;&#37322;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#65288;&#27604;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#22823;6&#20493;&#65289;&#65292;&#28085;&#30422;&#20102;&#21253;&#25324;&#22270;&#20687;&#21644;&#35270;&#39057;&#27169;&#24577;&#22312;&#20869;&#30340;12&#20010;&#35780;&#20272;&#32500;&#24230;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#27969;&#31243;&#26469;&#29983;&#25104;&#38024;&#23545;&#29305;&#23450;&#35780;&#20272;&#32500;&#24230;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#25972;&#21512;&#20102;&#33258;&#21160;&#31579;&#36873;&#21644;&#25163;&#21160;&#39564;&#35777;&#36807;&#31243;&#12290;&#36890;&#36807;&#20154;&#24037;&#27880;&#37322;&#33719;&#24471;&#22320;&#38754;&#23454;&#20917;&#36873;&#39033;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#33021;&#22815;&#23458;&#35266;&#39640;&#25928;&#22320;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;
&lt;/p&gt;
&lt;p&gt;
Based on powerful Large Language Models (LLMs), recent generative Multimodal Large Language Models (MLLMs) have gained prominence as a pivotal research area, exhibiting remarkable capability for both comprehension and generation. In this work, we address the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment of generative models, by introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple choice questions with accurate human annotations (x 6 larger than existing benchmarks), which spans 12 evaluation dimensions including the comprehension of both the image and video modality. We develop an advanced pipeline for generating multiple-choice questions that target specific evaluation dimensions, integrating both automatic filtering and manual verification processes. Multiple-choice questions with groundtruth options derived from human annotation enables an objective and efficient assessment of model performance, elim
&lt;/p&gt;</description></item><item><title>Okapi&#26159;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#20248;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#35299;&#20915;&#20102;&#30446;&#21069;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21482;&#38024;&#23545;&#33521;&#35821;&#21644;&#23569;&#25968;&#27969;&#34892;&#35821;&#35328;&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.16039</link><description>&lt;p&gt;
Okapi: &#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#20248;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback. (arXiv:2307.16039v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16039
&lt;/p&gt;
&lt;p&gt;
Okapi&#26159;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35843;&#20248;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#35299;&#20915;&#20102;&#30446;&#21069;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#21482;&#38024;&#23545;&#33521;&#35821;&#21644;&#23569;&#25968;&#27969;&#34892;&#35821;&#35328;&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#26159;&#25351;&#20196;&#35843;&#20248;&#65292;&#23427;&#26377;&#21161;&#20110;&#23558;&#27169;&#22411;&#30340;&#21709;&#24212;&#19982;&#20154;&#31867;&#39044;&#26399;&#23545;&#40784;&#65292;&#23454;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#20004;&#31181;&#20027;&#35201;&#30340;&#25351;&#20196;&#35843;&#20248;&#26041;&#27861;&#26159;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#30446;&#21069;&#24050;&#24212;&#29992;&#20110;&#29983;&#20135;&#26368;&#20339;&#30340;&#21830;&#19994;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;ChatGPT&#65289;&#12290;&#20026;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#30740;&#31350;&#21644;&#24320;&#21457;&#24037;&#20316;&#20013;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#26368;&#36817;&#36824;&#25512;&#20986;&#20102;&#21508;&#31181;&#32463;&#36807;&#25351;&#20196;&#35843;&#20248;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;Alpaca&#12289;Vicuna&#31561;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#20165;&#23545;&#33521;&#35821;&#21644;&#23569;&#25968;&#27969;&#34892;&#35821;&#35328;&#36827;&#34892;&#20102;&#25351;&#20196;&#35843;&#20248;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20840;&#29699;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#24433;&#21709;&#21147;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;&#26368;&#36817;&#26377;&#19968;&#20123;&#25506;&#32034;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#20248;&#30340;&#24037;&#20316;&#65292;&#20294;&#30446;&#21069;&#21482;&#20351;&#29992;&#20102;SFT&#20316;&#20026;&#25351;&#20196;&#35843;&#20248;&#30340;&#21807;&#19968;&#26041;&#27861;&#12290;&#36825;&#24050;&#32463;&#23384;&#22312;&#20102;&#19968;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A key technology for the development of large language models (LLMs) involves instruction tuning that helps align the models' responses with human expectations to realize impressive learning abilities. Two major approaches for instruction tuning characterize supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF), which are currently applied to produce the best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for research and development efforts, various instruction-tuned open-source LLMs have also been introduced recently, e.g., Alpaca, Vicuna, to name a few. However, existing open-source LLMs have only been instruction-tuned for English and a few popular languages, thus hindering their impacts and accessibility to many other languages in the world. Among a few very recent work to explore instruction tuning for LLMs in multiple languages, SFT has been used as the only approach to instruction-tune LLMs for multiple languages. This has lef
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#38598;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#23545;&#28385;&#24847;&#24230;&#35843;&#26597;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#35782;&#21035;&#37325;&#22797;&#30340;&#35789;&#35821;&#27169;&#24335;&#21644;&#21033;&#29992;&#24847;&#35265;&#25366;&#25496;&#26469;&#29702;&#35299;&#21442;&#19982;&#32773;&#30340;&#24847;&#35265;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#35789;&#35821;&#27169;&#24335;&#26469;&#33719;&#21462;&#26356;&#28145;&#20837;&#30340;&#24773;&#24863;&#12289;&#24847;&#35265;&#21644;&#20027;&#39064;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.11771</link><description>&lt;p&gt;
&#19968;&#31181;&#38598;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#29992;&#20110;&#28385;&#24847;&#24230;&#35843;&#26597;&#20013;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
an integrated npl approach to sentiment analysis in satisfaction surveys. (arXiv:2307.11771v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11771
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#38598;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#23545;&#28385;&#24847;&#24230;&#35843;&#26597;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#35782;&#21035;&#37325;&#22797;&#30340;&#35789;&#35821;&#27169;&#24335;&#21644;&#21033;&#29992;&#24847;&#35265;&#25366;&#25496;&#26469;&#29702;&#35299;&#21442;&#19982;&#32773;&#30340;&#24847;&#35265;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#35789;&#35821;&#27169;&#24335;&#26469;&#33719;&#21462;&#26356;&#28145;&#20837;&#30340;&#24773;&#24863;&#12289;&#24847;&#35265;&#21644;&#20027;&#39064;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39033;&#30446;&#26088;&#22312;&#23558;&#38598;&#25104;&#26041;&#27861;&#24212;&#29992;&#20110;&#28385;&#24847;&#24230;&#35843;&#26597;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#23427;&#23558;&#30528;&#37325;&#20110;&#29702;&#35299;&#21644;&#25552;&#21462;&#35843;&#26597;&#22238;&#31572;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#20998;&#26512;&#24773;&#24863;&#65292;&#35782;&#21035;&#37325;&#22797;&#30340;&#35789;&#35821;&#27169;&#24335;&#12290;&#23558;&#20351;&#29992;NLP&#25216;&#26415;&#26469;&#30830;&#23450;&#24773;&#24863;&#26497;&#24615;&#65292;&#23558;&#22238;&#31572;&#20998;&#31867;&#20026;&#31215;&#26497;&#12289;&#28040;&#26497;&#25110;&#20013;&#24615;&#31867;&#21035;&#65292;&#24182;&#21033;&#29992;&#24847;&#35265;&#25366;&#25496;&#26469;&#31361;&#20986;&#21442;&#19982;&#32773;&#30340;&#24847;&#35265;&#12290;&#35813;&#26041;&#27861;&#23558;&#26377;&#21161;&#20110;&#30830;&#23450;&#23545;&#21442;&#19982;&#32773;&#26368;&#30456;&#20851;&#30340;&#26041;&#38754;&#65292;&#24182;&#20102;&#35299;&#20182;&#20204;&#23545;&#36825;&#20123;&#29305;&#23450;&#26041;&#38754;&#30340;&#24847;&#35265;&#12290;&#35813;&#30740;&#31350;&#39033;&#30446;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#23558;&#26159;&#20351;&#29992;NPL&#23545;&#28385;&#24847;&#24230;&#35843;&#26597;&#22238;&#31572;&#20013;&#30340;&#35789;&#35821;&#27169;&#24335;&#36827;&#34892;&#20998;&#26512;&#12290;&#35813;&#20998;&#26512;&#23558;&#25552;&#20379;&#23545;&#22238;&#31572;&#32773;&#24773;&#24863;&#12289;&#24847;&#35265;&#20197;&#21450;&#20986;&#29616;&#30340;&#20027;&#39064;&#21644;&#36235;&#21183;&#30340;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#20174;&#35813;&#26041;&#27861;&#24471;&#21040;&#30340;&#32467;&#26524;&#21487;&#20197;&#29992;&#20110;&#30830;&#23450;&#25913;&#36827;&#30340;&#26041;&#21521;&#65292;&#20102;&#35299;&#22238;&#31572;&#32773;&#30340;&#20559;&#22909;&#65292;&#24182;&#20570;&#20986;&#25112;&#30053;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research project aims to apply an integrated approach to natural language processing NLP to satisfaction surveys. It will focus on understanding and extracting relevant information from survey responses, analyzing feelings, and identifying recurring word patterns. NLP techniques will be used to determine emotional polarity, classify responses into positive, negative, or neutral categories, and use opinion mining to highlight participants opinions. This approach will help identify the most relevant aspects for participants and understand their opinions in relation to those specific aspects. A key component of the research project will be the analysis of word patterns in satisfaction survey responses using NPL. This analysis will provide a deeper understanding of feelings, opinions, and themes and trends present in respondents responses. The results obtained from this approach can be used to identify areas for improvement, understand respondents preferences, and make strategic decisi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.03109</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#32780;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#38543;&#30528;LLMs&#22312;&#30740;&#31350;&#21644;&#26085;&#24120;&#20351;&#29992;&#20013;&#32487;&#32493;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#19981;&#20165;&#22312;&#20219;&#21153;&#27700;&#24179;&#19978;&#65292;&#32780;&#19988;&#22312;&#31038;&#20250;&#23618;&#38754;&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;LLMs&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#36825;&#20123;&#35780;&#20272;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35780;&#20272;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#19968;&#33324;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#21644;&#20854;&#20182;&#39046;&#22495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20934;&#31572;&#26696;&#26469;&#22238;&#31572;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#20505;&#36873;&#24230;&#37327;&#26469;&#20248;&#21270;&#38598;&#21512;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.00925</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Automatic Design of Semantic Similarity Ensembles Using Grammatical Evolution. (arXiv:2307.00925v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#65292;&#36890;&#36807;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#20505;&#36873;&#24230;&#37327;&#26469;&#20248;&#21270;&#38598;&#21512;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#31181;&#19982;&#35745;&#31639;&#26426;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#21333;&#19968;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#36866;&#29992;&#20110;&#25152;&#26377;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#20351;&#29992;&#38598;&#21512;&#31574;&#30053;&#26469;&#30830;&#20445;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35774;&#35745;&#35821;&#20041;&#30456;&#20284;&#24615;&#38598;&#21512;&#30340;&#26041;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#39318;&#27425;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#26469;&#33258;&#21160;&#36873;&#25321;&#21644;&#32858;&#21512;&#19968;&#32452;&#20505;&#36873;&#24230;&#37327;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#26368;&#22823;&#21270;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#30340;&#38598;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#38598;&#21512;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30456;&#20284;&#24230;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26082;&#23637;&#31034;&#20102;&#20351;&#29992;&#35821;&#27861;&#28436;&#21270;&#26469;&#33258;&#21160;&#27604;&#36739;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#20063;&#35777;&#26126;&#20102;&#20351;&#29992;&#38598;&#21512;&#23545;&#35821;&#20041;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity measures are widely used in natural language processing to catalyze various computer-related tasks. However, no single semantic similarity measure is the most appropriate for all tasks, and researchers often use ensemble strategies to ensure performance. This research work proposes a method for automatically designing semantic similarity ensembles. In fact, our proposed method uses grammatical evolution, for the first time, to automatically select and aggregate measures from a pool of candidates to create an ensemble that maximizes correlation to human judgment. The method is evaluated on several benchmark datasets and compared to state-of-the-art ensembles, showing that it can significantly improve similarity assessment accuracy and outperform existing methods in some cases. As a result, our research demonstrates the potential of using grammatical evolution to automatically compare text and prove the benefits of using ensembles for semantic similarity tasks. The so
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;CamemBERT-bio&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#36890;&#29992;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;2.54&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.15550</link><description>&lt;p&gt;
CamemBERT-bio&#65306;&#19968;&#31181;&#26356;&#20581;&#24247;&#30340;&#27861;&#35821;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CamemBERT-bio: a Tasty French Language Model Better for your Health. (arXiv:2306.15550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;CamemBERT-bio&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#36890;&#29992;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;2.54&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20020;&#24202;&#25968;&#25454;&#20179;&#24211;&#65292;&#21307;&#38498;&#20013;&#30340;&#20020;&#24202;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#23481;&#26131;&#29992;&#20110;&#30740;&#31350;&#65292;&#28982;&#32780;&#36825;&#20123;&#25991;&#20214;&#37117;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#20174;&#21307;&#30103;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#20197;&#36827;&#34892;&#20020;&#24202;&#30740;&#31350;&#12290;&#20351;&#29992;CamemBERT&#31561;BERT-like&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#20026;&#36890;&#29992;&#35821;&#35328;&#35757;&#32451;&#30340;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#25928;&#26524;&#36739;&#24369;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27861;&#35821;&#20844;&#20849;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#23545;CamemBERT&#36827;&#34892;&#20102;&#32487;&#32493;&#39044;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CamemBERT-bio&#30340;&#31532;&#19968;&#20010;&#29256;&#26412;&#65292;&#23427;&#26159;&#19968;&#31181;&#20026;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#20844;&#20849;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#24179;&#22343;F1&#20998;&#25968;&#25552;&#39640;&#20102;2.54&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical data in hospitals are increasingly accessible for research through clinical data warehouses, however these documents are unstructured. It is therefore necessary to extract information from medical reports to conduct clinical studies. Transfer learning with BERT-like models such as CamemBERT has allowed major advances, especially for named entity recognition. However, these models are trained for plain language and are less efficient on biomedical data. This is why we propose a new French public biomedical dataset on which we have continued the pre-training of CamemBERT. Thus, we introduce a first version of CamemBERT-bio, a specialized public model for the French biomedical domain that shows 2.54 points of F1 score improvement on average on different biomedical named entity recognition tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Vistaar&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;59&#20010;&#22522;&#20934;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#21360;&#24230;&#35821;&#38899;ASR&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24494;&#35843;&#20844;&#24320;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;IndicWhisper&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;39&#20010;&#22522;&#20934;&#20013;&#20855;&#26377;&#26368;&#20302;&#30340;WER&#65292;&#24179;&#22343;&#38477;&#20302;&#20102;4.1&#30340;WER&#12290;</title><link>http://arxiv.org/abs/2305.15386</link><description>&lt;p&gt;
Vistaar: &#20026;&#21360;&#24230;&#35821;&#38899;ASR&#25552;&#20379;&#22810;&#26679;&#21270;&#30340;&#22522;&#20934;&#21644;&#35757;&#32451;&#38598;
&lt;/p&gt;
&lt;p&gt;
Vistaar: Diverse Benchmarks and Training Sets for Indian Language ASR. (arXiv:2305.15386v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Vistaar&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;59&#20010;&#22522;&#20934;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#21360;&#24230;&#35821;&#38899;ASR&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24494;&#35843;&#20844;&#24320;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;IndicWhisper&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;39&#20010;&#22522;&#20934;&#20013;&#20855;&#26377;&#26368;&#20302;&#30340;WER&#65292;&#24179;&#22343;&#38477;&#20302;&#20102;4.1&#30340;WER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#36827;ASR&#31995;&#32479;&#26159;&#35753;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#26032;LLM&#22522;&#20110;&#29992;&#20363;&#21487;&#29992;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#21360;&#24230;&#35821;&#35328;&#65292;&#24182;&#19988;&#25552;&#20986;&#22810;&#26679;&#21270;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#25913;&#36827;&#21360;&#24230;&#35821;&#38899;ASR&#31995;&#32479;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;Vistaar&#20316;&#20026;&#30001;59&#20010;&#22522;&#20934;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#35821;&#35328;&#21644;&#39046;&#22495;&#30340;&#32452;&#21512;&#65292;&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#25105;&#20204;&#35780;&#20272;&#20102;3&#20010;&#20844;&#20849;ASR&#31995;&#32479;&#21644;2&#20010;&#21830;&#19994;&#31995;&#32479;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#22312;12&#31181;&#21360;&#24230;&#35821;&#35328;&#19978;&#23545;&#20844;&#24320;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#35757;&#32451;&#20102;IndicWhisper&#27169;&#22411;&#65292;&#24635;&#20849;&#36798;&#21040;&#20102;10.7K&#23567;&#26102;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;IndicWhisper&#22312;Vistaar&#22522;&#20934;&#19978;&#26174;&#33879;&#25913;&#36827;&#20102;&#34987;&#32771;&#34385;&#30340;ASR&#31995;&#32479;&#12290;&#23454;&#38469;&#19978;&#65292;IndicWhisper&#22312;59&#20010;&#22522;&#20934;&#20013;&#26377;39&#20010;&#22522;&#20934;&#20855;&#26377;&#26368;&#20302;&#30340;WER&#65292;&#24179;&#22343;&#38477;&#20302;&#20102;4.1&#30340;WER&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#25152;&#26377;&#30340;&#25968;&#25454;&#38598;&#65292;&#20195;&#30721;&#21644;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving ASR systems is necessary to make new LLM-based use-cases accessible to people across the globe. In this paper, we focus on Indian languages, and make the case that diverse benchmarks are required to evaluate and improve ASR systems for Indian languages. To address this, we collate Vistaar as a set of 59 benchmarks across various language and domain combinations, on which we evaluate 3 publicly available ASR systems and 2 commercial systems. We also train IndicWhisper models by fine-tuning the Whisper models on publicly available training datasets across 12 Indian languages totalling to 10.7K hours. We show that IndicWhisper significantly improves on considered ASR systems on the Vistaar benchmark. Indeed, IndicWhisper has the lowest WER in 39 out of the 59 benchmarks, with an average reduction of 4.1 WER. We open-source all datasets, code and models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#24494;&#35843;&#21518;&#20351;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.07804</link><description>&lt;p&gt;
Dr. LLaMA&#65306;&#36890;&#36807;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#25913;&#21892;&#29305;&#23450;&#39046;&#22495;QA&#20013;&#30340;&#23567;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation. (arXiv:2305.07804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#24494;&#35843;&#21518;&#20351;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#38543;&#30528;&#20854;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#20063;&#38754;&#20020;&#30528;&#35745;&#31639;&#24320;&#38144;&#21644;&#25928;&#29575;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30001;&#20110;&#23481;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#32858;&#28966;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#21644;PubMedQA&#25968;&#25454;&#38598;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LLM&#26377;&#25928;&#22320;&#32454;&#21270;&#21644;&#25193;&#23637;&#29616;&#26377;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#22312;&#24494;&#35843;&#21518;&#65292;&#20351;&#24471;&#23567;&#22411;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;QA&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#25552;&#39640;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26368;&#32456;&#26088;&#22312;&#20026;&#19987;&#19994;&#24212;&#29992;&#21019;&#24314;&#26356;&#39640;&#25928;&#21644;&#33021;&#21147;&#26356;&#24378;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made significant strides in natural language processing but face challenges in terms of computational expense and inefficiency as they grow in size, especially in domain-specific tasks. Small Language Models (SLMs), on the other hand, often struggle in these tasks due to limited capacity and training data. In this paper, we introduce Dr. LLaMA, a method for improving SLMs through generative data augmentation using LLMs, focusing on medical question-answering tasks and the PubMedQA dataset. Our findings indicate that LLMs effectively refine and diversify existing question-answer pairs, resulting in improved performance of a much smaller model on domain-specific QA datasets after fine-tuning. This study highlights the challenges of using LLMs for domain-specific question answering and suggests potential research directions to address these limitations, ultimately aiming to create more efficient and capable models for specialized applications. We have als
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#22823;&#35268;&#27169;&#26816;&#32034;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#26597;&#35810;&#21644;&#26597;&#35810;&#30340;&#20505;&#36873;&#31572;&#26696;&#30340;&#32452;&#21512;&#20316;&#20026;&#25552;&#31034;&#65292;&#20351;LLM&#29983;&#25104;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#30001;&#20110;&#33258;&#30417;&#30563;&#26816;&#32034;&#22120;&#22312;&#38646;-shot&#22330;&#26223;&#20013;&#24615;&#33021;&#36739;&#24046;&#65292;&#22240;&#27492;LameR&#20248;&#20110;&#33258;&#30417;&#30563;&#26816;&#32034;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.14233</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#26816;&#32034;&#20013;&#20855;&#26377;&#36739;&#24378;&#30340;&#34920;&#29616;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Strong Zero-Shot Retriever. (arXiv:2304.14233v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#22823;&#35268;&#27169;&#26816;&#32034;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#26597;&#35810;&#21644;&#26597;&#35810;&#30340;&#20505;&#36873;&#31572;&#26696;&#30340;&#32452;&#21512;&#20316;&#20026;&#25552;&#31034;&#65292;&#20351;LLM&#29983;&#25104;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#30001;&#20110;&#33258;&#30417;&#30563;&#26816;&#32034;&#22120;&#22312;&#38646;-shot&#22330;&#26223;&#20013;&#24615;&#33021;&#36739;&#24046;&#65292;&#22240;&#27492;LameR&#20248;&#20110;&#33258;&#30417;&#30563;&#26816;&#32034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#22823;&#35268;&#27169;&#26816;&#32034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Language Model&#20316;&#20026;&#26816;&#32034;&#22120;&#65288;LameR&#65289;&#20165;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#32780;&#19981;&#26159;&#20854;&#20182;&#31070;&#32463;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;LLM&#19982;&#26816;&#32034;&#22120;&#30340;&#26292;&#21147;&#32452;&#21512;&#36827;&#34892;&#20998;&#35299;&#65292;&#23558;&#38646;-shot&#26816;&#32034;&#30340;&#24615;&#33021;&#25552;&#39640;&#21040;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#24456;&#24378;&#30340;&#31454;&#20105;&#21147;&#12290;&#26412;&#25991;&#20027;&#35201;&#25552;&#20986;&#36890;&#36807;&#20351;&#29992;&#26597;&#35810;&#21644;&#26597;&#35810;&#30340;&#20505;&#36873;&#31572;&#26696;&#30340;&#32452;&#21512;&#20316;&#20026;&#25552;&#31034;&#65292;&#20351;LLM&#29983;&#25104;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#26080;&#35770;&#20505;&#36873;&#31572;&#26696;&#26159;&#21542;&#27491;&#30830;&#65292;&#37117;&#21487;&#20197;&#36890;&#36807;&#27169;&#24335;&#27169;&#20223;&#25110;&#20505;&#36873;&#25688;&#35201;&#26469;&#24110;&#21161;LLM&#20135;&#29983;&#26356;&#31934;&#30830;&#30340;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#33258;&#30417;&#30563;&#26816;&#32034;&#22120;&#22312;&#38646;-shot&#22330;&#26223;&#20013;&#24615;&#33021;&#36739;&#24046;&#65292;&#22240;&#27492;&#36890;&#36807;&#21033;&#29992;LLM&#23545;&#25991;&#26412;&#27169;&#24335;&#30340;&#24378;&#22823;&#34920;&#29616;&#33021;&#21147;&#65292;LameR&#21487;&#20197;&#20248;&#20110;&#33258;&#30417;&#30563;&#26816;&#32034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a simple method that applies a large language model (LLM) to large-scale retrieval in zero-shot scenarios. Our method, Language language model as Retriever (LameR) is built upon no other neural models but an LLM, while breaking up brute-force combinations of retrievers with LLMs and lifting the performance of zero-shot retrieval to be very competitive on benchmark datasets. Essentially, we propose to augment a query with its potential answers by prompting LLMs with a composition of the query and the query's in-domain candidates. The candidates, regardless of correct or wrong, are obtained by a vanilla retrieval procedure on the target collection. Such candidates, as a part of prompts, are likely to help LLM generate more precise answers by pattern imitation or candidate summarization. Even if all the candidates are wrong, the prompts at least make LLM aware of in-collection patterns and genres. Moreover, due to the low performance of a self-supervised retriever
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20195;&#25968;&#35821;&#35328;&#29255;&#27573;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#19978;&#32852;&#31995;&#30340;&#26694;&#26550;&#65292;&#24182;&#20174;MATLANG&#23450;&#20041;&#20102;&#19968;&#20010;&#31526;&#21512;3-WL&#27979;&#35797;&#30340;&#35821;&#27861;&#65292;&#36827;&#32780;&#24471;&#20986;&#19968;&#20010;&#31526;&#21512;3-WL GNN&#27169;&#22411;&#30340;G$^2$N$^2$&#12290;&#27492;&#22806;&#65292;&#35821;&#27861;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#35745;&#31639;&#38271;&#24230;&#20026;&#20845;&#21450;&#20197;&#19979;&#30340;&#29615;&#21644;&#24358;&#29615;&#30340;&#20195;&#25968;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.01590</link><description>&lt;p&gt;
&#25216;&#26415;&#25253;&#21578;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#20063;&#21487;&#20197;&#21464;&#24471;&#35821;&#27861;&#21270;
&lt;/p&gt;
&lt;p&gt;
Technical report: Graph Neural Networks go Grammatical. (arXiv:2303.01590v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#20195;&#25968;&#35821;&#35328;&#29255;&#27573;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#19978;&#32852;&#31995;&#30340;&#26694;&#26550;&#65292;&#24182;&#20174;MATLANG&#23450;&#20041;&#20102;&#19968;&#20010;&#31526;&#21512;3-WL&#27979;&#35797;&#30340;&#35821;&#27861;&#65292;&#36827;&#32780;&#24471;&#20986;&#19968;&#20010;&#31526;&#21512;3-WL GNN&#27169;&#22411;&#30340;G$^2$N$^2$&#12290;&#27492;&#22806;&#65292;&#35821;&#27861;&#26041;&#27861;&#36824;&#25552;&#20379;&#20102;&#35745;&#31639;&#38271;&#24230;&#20026;&#20845;&#21450;&#20197;&#19979;&#30340;&#29615;&#21644;&#24358;&#29615;&#30340;&#20195;&#25968;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#19968;&#20010;&#20195;&#25968;&#35821;&#35328;&#30340;&#19968;&#20010;&#29255;&#27573;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24418;&#24335;&#19978;&#32852;&#31995;&#36215;&#26469;&#12290;&#23427;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#65288;CFG&#65289;&#65292;&#23558;&#20195;&#25968;&#25805;&#20316;&#32452;&#32455;&#25104;&#21487;&#20197;&#32763;&#35793;&#20026;GNN&#23618;&#27169;&#22411;&#30340;&#29983;&#25104;&#35268;&#21017;&#12290;&#30001;&#20110;&#30452;&#25509;&#20174;&#35821;&#35328;&#27966;&#29983;&#20986;&#30340;CFG&#30340;&#35268;&#21017;&#21644;&#21464;&#37327;&#21253;&#21547;&#20887;&#20313;&#65292;&#22240;&#27492;&#20171;&#32461;&#20102;&#19968;&#31181;&#35821;&#27861;&#31616;&#21270;&#26041;&#26696;&#65292;&#20351;&#24471;&#23558;&#20854;&#32763;&#35793;&#20026;GNN&#23618;&#25104;&#20026;&#21487;&#33021;&#12290;&#24212;&#29992;&#36825;&#31181;&#31574;&#30053;&#65292;&#20174;MATLANG&#23450;&#20041;&#20102;&#19968;&#20010;&#31526;&#21512;&#31532;&#19977;&#38454;Weisfeiler-Lehman&#65288;3-WL&#65289;&#27979;&#35797;&#35201;&#27714;&#30340;&#35821;&#27861;&#12290;&#20174;&#36825;&#20010;3-WL CFG&#20013;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#35777;&#26126;&#31526;&#21512;3-WL GNN&#27169;&#22411;&#30340;G$^2$N$^2$&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#35821;&#27861;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20379;&#35745;&#31639;&#38271;&#24230;&#20026;&#20845;&#21450;&#20197;&#19979;&#30340;&#29615;&#21644;&#24358;&#29615;&#30340;&#20195;&#25968;&#20844;&#24335;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;3-WL&#30340;&#35745;&#25968;&#33021;&#21147;&#12290;&#22810;&#20010;&#23454;&#39564;&#35777;&#26126;&#65292;G$^2$N$^2$&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#35201;&#27604;&#20854;&#20182;3-WL GNN&#26356;&#20026;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a framework to formally link a fragment of an algebraic language to a Graph Neural Network (GNN). It relies on Context Free Grammars (CFG) to organise algebraic operations into generative rules that can be translated into a GNN layer model. Since the rules and variables of a CFG directly derived from a language contain redundancies, a grammar reduction scheme is presented making tractable the translation into a GNN layer. Applying this strategy, a grammar compliant with the third-order Weisfeiler-Lehman (3-WL) test is defined from MATLANG. From this 3-WL CFG, we derive a provably 3-WL GNN model called G$^2$N$^2$. Moreover, this grammatical approach allows us to provide algebraic formulas to count the cycles of length up to six and chordal cycles at the edge level, which enlightens the counting power of 3-WL. Several experiments illustrate that G$^2$N$^2$ efficiently outperforms other 3-WL GNNs on many downstream tasks.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;AdaPTGen&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#25552;&#31034;&#27169;&#26495;&#35843;&#25972;&#20026;&#27169;&#22411;&#25152;&#38656;&#65292;&#26469;&#35299;&#20915;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#35813;&#26694;&#26550;&#27880;&#20837;&#20102;&#24120;&#35268;&#34920;&#26684;&#30456;&#20851;&#25551;&#36848;&#30340;&#34920;&#31034;&#65292;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#24182;&#20801;&#35768;&#35774;&#35745;&#21508;&#31181;&#20219;&#21153;&#26469;&#25506;&#32034;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2302.12468</link><description>&lt;p&gt;
&#20026;&#23569;&#25968;&#25454;&#26679;&#26412;&#30340;&#34920;&#26684;&#29983;&#25104;&#33258;&#36866;&#24212;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Adapting Prompt for Few-shot Table-to-Text Generation. (arXiv:2302.12468v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12468
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;AdaPTGen&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#25552;&#31034;&#27169;&#26495;&#35843;&#25972;&#20026;&#27169;&#22411;&#25152;&#38656;&#65292;&#26469;&#35299;&#20915;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#35813;&#26694;&#26550;&#27880;&#20837;&#20102;&#24120;&#35268;&#34920;&#26684;&#30456;&#20851;&#25551;&#36848;&#30340;&#34920;&#31034;&#65292;&#20805;&#20998;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#24182;&#20801;&#35768;&#35774;&#35745;&#21508;&#31181;&#20219;&#21153;&#26469;&#25506;&#32034;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#34920;&#26684;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#24456;&#38590;&#24357;&#21512;&#34920;&#26684;&#25968;&#25454;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#36317;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#26377;&#38480;&#36164;&#28304;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65306;&#33258;&#36866;&#24212;&#29983;&#25104;&#25552;&#31034;&#65288;AdaPTGen&#65289;&#12290;AdaPTGen&#30340;&#26680;&#24515;&#26159;&#23558;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#25552;&#31034;&#27169;&#26495;&#35843;&#25972;&#20026;&#27169;&#22411;&#25152;&#38656;&#65292;&#24102;&#26469;&#20102;&#33267;&#23569;&#19977;&#20010;&#22909;&#22788;&#65306;&#65288;1&#65289;&#23427;&#27880;&#20837;&#20102;&#24120;&#35268;&#34920;&#26684;&#30456;&#20851;&#25551;&#36848;&#30340;&#34920;&#31034;&#65292;&#20197;&#24357;&#21512;&#34920;&#26684;&#25968;&#25454;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#36317;&#65307;&#65288;2&#65289;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;PLMs&#32570;&#20047;&#39046;&#22495;&#30693;&#35782;&#30340;&#22266;&#26377;&#32570;&#28857;&#65307;&#65288;3&#65289;&#23427;&#20801;&#35768;&#25105;&#20204;&#35774;&#35745;&#21508;&#31181;&#20219;&#21153;&#26469;&#25506;&#32034;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#22312;&#19977;&#20010;&#24320;&#25918;&#39046;&#22495;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (PLMs) have made remarkable progress in table-to-text generation tasks. However, the lack of domain-specific knowledge makes it challenging to bridge the topological gap between tabular data and text, especially in real-world applications with limited resources. To mitigate the limitation of insufficient labeled data, we propose a novel framework: Adapt-Prompt-to-Generate (AdaPTGen). The core insight of AdaPTGen is to adapt prompt templates of domain-specific knowledge into the model, which brings at least three benefits: (1) it injects representation of normal table-related descriptions to bridge the topological gap between tabular data and texts; (2) it enables us to use large amounts of unlabeled domain-specific knowledge fully, which can alleviate the PLMs' inherent shortcomings of lacking domain knowledge; (3) it allows us to design various tasks to explore the domain-specific knowledge. Extensive experiments and analyses are conducted on three open-doma
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#26032;&#38395;&#25991;&#31456;&#20013;&#26816;&#27979;&#26377;&#23475;&#35758;&#31243;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#38395;&#25991;&#31456;&#27880;&#37322;&#25968;&#25454;&#38598;&#20197;&#20379;&#30740;&#31350;&#20351;&#29992;&#12290;&#30740;&#31350;&#32773;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#31995;&#32479;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#21644;&#40657;&#30418;&#27169;&#22411;&#26377;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.00102</link><description>&lt;p&gt;
&#22312;&#26032;&#38395;&#25991;&#31456;&#20013;&#26816;&#27979;&#26377;&#23475;&#35758;&#31243;
&lt;/p&gt;
&lt;p&gt;
Detecting Harmful Agendas in News Articles. (arXiv:2302.00102v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00102
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#26032;&#38395;&#25991;&#31456;&#20013;&#26816;&#27979;&#26377;&#23475;&#35758;&#31243;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#38395;&#25991;&#31456;&#27880;&#37322;&#25968;&#25454;&#38598;&#20197;&#20379;&#30740;&#31350;&#20351;&#29992;&#12290;&#30740;&#31350;&#32773;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#31995;&#32479;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#21644;&#40657;&#30418;&#27169;&#22411;&#26377;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#19978;&#25805;&#32437;&#26032;&#38395;&#26159;&#19968;&#20010;&#26085;&#30410;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#20351;&#29992;&#33258;&#21160;&#21270;&#31995;&#32479;&#26469;&#36943;&#21046;&#20854;&#20256;&#25773;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#34429;&#28982;&#35823;&#23548;&#20449;&#24687;&#21644;&#34394;&#20551;&#20449;&#24687;&#30340;&#26816;&#27979;&#24050;&#32463;&#24471;&#21040;&#30740;&#31350;&#65292;&#20294;&#22312;&#26816;&#27979;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#26377;&#23475;&#35758;&#31243;&#36825;&#19968;&#37325;&#35201;&#25361;&#25112;&#26041;&#38754;&#32570;&#20047;&#25237;&#36164;&#65307;&#35782;&#21035;&#26377;&#23475;&#35758;&#31243;&#23545;&#20110;&#35782;&#21035;&#20855;&#26377;&#26368;&#22823;&#28508;&#22312;&#29616;&#23454;&#21361;&#23475;&#30340;&#26032;&#38395;&#36816;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23545;&#23457;&#26597;&#21046;&#24230;&#23384;&#22312;&#30495;&#23454;&#30340;&#25285;&#24551;&#65292;&#26377;&#23475;&#35758;&#31243;&#26816;&#27979;&#22120;&#24517;&#39035;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#25165;&#33021;&#21457;&#25381;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#19968;&#20840;&#26032;&#30340;&#20219;&#21153;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;NewsAgendas&#30340;&#26032;&#38395;&#25991;&#31456;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35758;&#31243;&#35782;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#31995;&#32479;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#19982;&#40657;&#30418;&#27169;&#22411;&#20855;&#26377;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manipulated news online is a growing problem which necessitates the use of automated systems to curtail its spread. We argue that while misinformation and disinformation detection have been studied, there has been a lack of investment in the important open challenge of detecting harmful agendas in news articles; identifying harmful agendas is critical to flag news campaigns with the greatest potential for real world harm. Moreover, due to real concerns around censorship, harmful agenda detectors must be interpretable to be effective. In this work, we propose this new task and release a dataset, NewsAgendas, of annotated news articles for agenda identification. We show how interpretable systems can be effective on this task and demonstrate that they can perform comparably to black-box models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#22312;&#34892;&#20026;&#19978;&#19982;&#20154;&#31867;&#30452;&#35273;&#30456;&#20284;&#65292;&#20294;&#21487;&#33021;&#24102;&#26377;&#35748;&#30693;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26356;&#39640;&#35748;&#30693;&#33021;&#21147;&#30340;LLMs&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#23398;&#20250;&#20102;&#36991;&#20813;&#36825;&#20123;&#38169;&#35823;&#65292;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#22312;&#24515;&#29702;&#23398;&#26041;&#27861;&#30340;&#24110;&#21161;&#19979;&#30740;&#31350;LLMs&#65292;&#25105;&#20204;&#21487;&#20197;&#25581;&#31034;&#20986;&#20854;&#23427;&#26410;&#30693;&#30340;&#26032;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2212.05206</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#24555;&#36895;&#21644;&#24930;&#36895;&#24605;&#32771;
&lt;/p&gt;
&lt;p&gt;
Thinking Fast and Slow in Large Language Models. (arXiv:2212.05206v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#22312;&#34892;&#20026;&#19978;&#19982;&#20154;&#31867;&#30452;&#35273;&#30456;&#20284;&#65292;&#20294;&#21487;&#33021;&#24102;&#26377;&#35748;&#30693;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26356;&#39640;&#35748;&#30693;&#33021;&#21147;&#30340;LLMs&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#23398;&#20250;&#20102;&#36991;&#20813;&#36825;&#20123;&#38169;&#35823;&#65292;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#22312;&#24515;&#29702;&#23398;&#26041;&#27861;&#30340;&#24110;&#21161;&#19979;&#30740;&#31350;LLMs&#65292;&#25105;&#20204;&#21487;&#20197;&#25581;&#31034;&#20986;&#20854;&#23427;&#26410;&#30693;&#30340;&#26032;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30446;&#21069;&#22788;&#20110;&#23558;AI&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#32467;&#21512;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;&#23427;&#20204;&#26032;&#20852;&#30340;&#33021;&#21147;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20687;GPT-3&#36825;&#26679;&#30340;LLMs&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#30452;&#35273;&#24778;&#20154;&#30456;&#20284;&#30340;&#34892;&#20026;&#65292;&#20197;&#21450;&#30001;&#27492;&#24102;&#26469;&#30340;&#35748;&#30693;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26356;&#39640;&#35748;&#30693;&#33021;&#21147;&#30340;LLMs&#65292;&#29305;&#21035;&#26159;ChatGPT&#21644;GPT-4&#65292;&#23398;&#20250;&#20102;&#36991;&#20813;&#38519;&#20837;&#36825;&#20123;&#38169;&#35823;&#65292;&#34920;&#29616;&#20986;&#36229;&#29702;&#24615;&#30340;&#26041;&#24335;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#35748;&#30693;&#21453;&#24605;&#27979;&#35797;&#65288;CRT&#65289;&#20197;&#21450;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#30452;&#35273;&#20915;&#31574;&#30340;&#35821;&#20041;&#38169;&#35273;&#26469;&#25506;&#32034;LLMs&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#26041;&#27861;&#30740;&#31350;LLMs&#26377;&#21161;&#20110;&#25581;&#31034;&#20854;&#20182;&#26410;&#30693;&#30340;&#26032;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Therefore, it is of great importance to evaluate their emerging abilities. In this study, we show that LLMs like GPT-3 exhibit behavior that strikingly resembles human-like intuition - and the cognitive errors that come with it. However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits.
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#22312;&#38405;&#35835;&#21477;&#27861;&#27495;&#20041;&#21477;&#23376;&#26102;&#20250;&#25918;&#24930;&#38405;&#35835;&#36895;&#24230;&#65292;Surprisal&#29702;&#35770;&#35748;&#20026;&#36825;&#26159;&#30001;&#20110;&#21477;&#23376;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#20294;&#36825;&#31181;&#29702;&#35770;&#20302;&#20272;&#20102;&#20154;&#31867;&#30340;&#25928;&#24212;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#29420;&#31435;&#32771;&#34385;&#21477;&#27861;&#39044;&#27979;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20934;&#30830;&#30340;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.12187</link><description>&lt;p&gt;
&#20174;&#31070;&#32463;&#27169;&#22411;&#20013;&#30340;&#21477;&#27861;&#24847;&#22806;&#24863;&#30693;&#39044;&#27979;&#20154;&#31867;&#22312;&#21477;&#27861;&#27495;&#20041;&#20013;&#30340;&#22788;&#29702;&#22256;&#38590;&#65292;&#20294;&#20302;&#20272;&#20102;&#20854;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Syntactic Surprisal From Neural Models Predicts, But Underestimates, Human Processing Difficulty From Syntactic Ambiguities. (arXiv:2210.12187v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12187
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#38405;&#35835;&#21477;&#27861;&#27495;&#20041;&#21477;&#23376;&#26102;&#20250;&#25918;&#24930;&#38405;&#35835;&#36895;&#24230;&#65292;Surprisal&#29702;&#35770;&#35748;&#20026;&#36825;&#26159;&#30001;&#20110;&#21477;&#23376;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#65292;&#20294;&#36825;&#31181;&#29702;&#35770;&#20302;&#20272;&#20102;&#20154;&#31867;&#30340;&#25928;&#24212;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#29420;&#31435;&#32771;&#34385;&#21477;&#27861;&#39044;&#27979;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20934;&#30830;&#30340;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#38405;&#35835;&#26242;&#26102;&#20855;&#26377;&#32467;&#26500;&#27495;&#20041;&#30340;&#21477;&#23376;&#26102;&#34920;&#29616;&#20986;&#33457;&#22253;&#23567;&#24452;&#25928;&#24212;&#65292;&#24403;&#32467;&#26500;&#34987;&#28040;&#38500;&#27495;&#20041;&#20026;&#19981;&#22826;&#21463;&#27426;&#36814;&#30340;&#36873;&#25321;&#26102;&#65292;&#20182;&#20204;&#20250;&#25918;&#24930;&#38405;&#35835;&#36895;&#24230;&#12290;Surprisal&#29702;&#35770;&#65288;Hale, 2001; Levy, 2008&#65289;&#20316;&#20026;&#23545;&#27492;&#29616;&#35937;&#30340;&#19968;&#20010;&#37325;&#35201;&#35299;&#37322;&#65292;&#35748;&#20026;&#36825;&#31181;&#25918;&#24930;&#26159;&#30001;&#36825;&#20123;&#21477;&#23376;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#24341;&#36215;&#30340;&#12290;van Schijndel&#21644;Linzen&#65288;2021&#65289;&#23545;&#27492;&#20551;&#35774;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#20182;&#20204;&#21457;&#29616;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#21462;&#30340;&#21333;&#35789;&#21487;&#39044;&#27979;&#24615;&#20195;&#20215;&#20272;&#35745;&#20005;&#37325;&#20302;&#20272;&#20102;&#20154;&#31867;&#33457;&#22253;&#23567;&#24452;&#25928;&#24212;&#30340;&#31243;&#24230;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#36825;&#31181;&#20302;&#20272;&#26159;&#21542;&#26159;&#22240;&#20026;&#20154;&#31867;&#22312;&#39044;&#27979;&#20013;&#26356;&#39640;&#22320;&#26435;&#34913;&#20102;&#21477;&#27861;&#22240;&#32032;&#65292;&#32780;&#35821;&#35328;&#27169;&#22411;&#24182;&#26410;&#22914;&#27492;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#20272;&#35745;&#21477;&#27861;&#39044;&#27979;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#29420;&#31435;&#35780;&#20272;&#35789;&#27719;&#21644;&#21477;&#27861;&#39044;&#27979;&#24615;&#30340;&#20195;&#20215;&#12290;&#25105;&#20204;&#21457;&#29616;&#29420;&#31435;&#32771;&#34385;&#21477;&#27861;&#39044;&#27979;&#24615;&#20250;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans exhibit garden path effects: When reading sentences that are temporarily structurally ambiguous, they slow down when the structure is disambiguated in favor of the less preferred alternative. Surprisal theory (Hale, 2001; Levy, 2008), a prominent explanation of this finding, proposes that these slowdowns are due to the unpredictability of each of the words that occur in these sentences. Challenging this hypothesis, van Schijndel &amp; Linzen (2021) find that estimates of the cost of word predictability derived from language models severely underestimate the magnitude of human garden path effects. In this work, we consider whether this underestimation is due to the fact that humans weight syntactic factors in their predictions more highly than language models do. We propose a method for estimating syntactic predictability from a language model, allowing us to weigh the cost of lexical and syntactic predictability independently. We find that treating syntactic predictability independe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Transformer&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#32534;&#36753;&#20869;&#23384;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#20851;&#32852;&#25968;&#37327;&#19978;&#20855;&#26377;&#25968;&#37327;&#32423;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.07229</link><description>&lt;p&gt;
&#22312;Transformer&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#32534;&#36753;&#20869;&#23384;
&lt;/p&gt;
&lt;p&gt;
Mass-Editing Memory in a Transformer. (arXiv:2210.07229v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07229
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Transformer&#20013;&#36827;&#34892;&#22823;&#35268;&#27169;&#32534;&#36753;&#20869;&#23384;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#20851;&#32852;&#25968;&#37327;&#19978;&#20855;&#26377;&#25968;&#37327;&#32423;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20351;&#29992;&#26032;&#30340;&#35760;&#24518;&#30340;&#28608;&#21160;&#20154;&#24515;&#30340;&#21069;&#26223;&#65292;&#20197;&#26367;&#25442;&#36807;&#26102;&#30340;&#20449;&#24687;&#25110;&#28155;&#21152;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#24037;&#20316;&#20027;&#35201;&#20165;&#38480;&#20110;&#26356;&#26032;&#21333;&#20010;&#20851;&#32852;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;MEMIT&#65292;&#19968;&#31181;&#30452;&#25509;&#26356;&#26032;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#23427;&#21487;&#20197;&#25193;&#23637;&#21040;&#25968;&#21315;&#20010;&#20851;&#32852;&#65292;&#23545;&#20110;GPT-J(6B)&#21644;GPT-NeoX(20B)&#65292;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#24037;&#20316;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#20197;&#22312;https://memit.baulab.info&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by orders of magnitude. Our code and data are at https://memit.baulab.info.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35757;&#32451;&#19982;&#26799;&#24230;&#24341;&#23548;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#20351;&#29992;Abstract Meaning Representation&#65288;AMR&#65289;&#22270;&#20316;&#20026;&#21453;&#39304;&#65292;&#20197;&#25913;&#21892;&#20107;&#20214;&#25277;&#21462;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.12490</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#35757;&#32451;&#19982;&#26799;&#24230;&#24341;&#23548;&#25552;&#39640;&#20107;&#20214;&#25277;&#21462;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Improve Event Extraction via Self-Training with Gradient Guidance. (arXiv:2205.12490v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35757;&#32451;&#19982;&#26799;&#24230;&#24341;&#23548;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#20351;&#29992;Abstract Meaning Representation&#65288;AMR&#65289;&#22270;&#20316;&#20026;&#21453;&#39304;&#65292;&#20197;&#25913;&#21892;&#20107;&#20214;&#25277;&#21462;&#20013;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#32570;&#19968;&#30452;&#26159;&#38480;&#21046;&#20107;&#20214;&#25277;&#21462;&#36827;&#23637;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#35757;&#32451;&#19982;&#21453;&#39304;&#65288;STF&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#23558;&#20854;&#19982;&#30456;&#21516;&#21477;&#23376;&#30340;Abstract Meaning Representation&#65288;AMR&#65289;&#22270;&#36827;&#34892;&#27604;&#36739;&#65292;&#20026;&#27599;&#20010;&#26032;&#20107;&#20214;&#39044;&#27979;&#33719;&#21462;&#21453;&#39304;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;STF&#21253;&#25324;&#65288;1&#65289;&#22312;&#29616;&#26377;&#20107;&#20214;&#27880;&#37322;&#19978;&#35757;&#32451;&#30340;&#22522;&#30784;&#20107;&#20214;&#25277;&#21462;&#27169;&#22411;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#35821;&#26009;&#24211;&#20197;&#39044;&#27979;&#26032;&#30340;&#20107;&#20214;&#25552;&#21450;&#20316;&#20026;&#20266;&#35757;&#32451;&#26679;&#26412;&#65292;&#21644;&#65288;2&#65289;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23545;&#20110;&#27599;&#20010;&#26032;&#39044;&#27979;&#30340;&#20107;&#20214;&#35302;&#21457;&#22120;&#12289;&#19968;&#20010;&#21442;&#25968;&#12289;&#23427;&#30340;&#21442;&#25968;&#35282;&#33394;&#20197;&#21450;&#23427;&#20204;&#22312;AMR&#22270;&#20013;&#30340;&#36335;&#24452;&#36827;&#34892;&#20272;&#35745;&#65292;&#20197;&#34920;&#31034;&#20266;&#26631;&#31614;&#30340;&#27491;&#30830;&#24615;&#12290;&#36825;&#20123;&#20860;&#23481;&#24615;&#20998;&#25968;&#36827;&#19968;&#27493;&#20316;&#20026;&#21453;&#39304;&#65292;&#40723;&#21169;&#25110;&#38459;&#27490;&#27169;&#22411;&#22312;&#33258;&#35757;&#32451;&#36807;&#31243;&#20013;&#23398;&#20064;&#20266;&#26631;&#31614;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data scarcity has been the main factor that hinders the progress of event extraction. To overcome this issue, we propose a Self-Training with Feedback (STF) framework that leverages the large-scale unlabeled data and acquires feedback for each new event prediction from the unlabeled data by comparing it to the Abstract Meaning Representation (AMR) graph of the same sentence. Specifically, STF consists of (1) a base event extraction model trained on existing event annotations and then applied to large-scale unlabeled corpora to predict new event mentions as pseudo training samples, and (2) a novel scoring model that takes in each new predicted event trigger, an argument, its argument role, as well as their paths in the AMR graph to estimate a compatibility score indicating the correctness of the pseudo label. The compatibility scores further act as feedback to encourage or discourage the model learning on the pseudo labels during self-training. Experimental results on three benchmark da
&lt;/p&gt;</description></item><item><title>DePA&#26159;&#19968;&#31181;&#20381;&#36182;&#24863;&#30693;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#39044;&#35757;&#32451;&#21644;&#20851;&#27880;&#36716;&#25442;&#20004;&#20010;&#27493;&#39588;&#26469;&#25913;&#36827;&#38750;&#33258;&#22238;&#24402;&#26426;&#22120;&#32763;&#35793;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DePA&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2203.16266</link><description>&lt;p&gt;
DePA: &#20351;&#29992;&#20381;&#36182;&#24863;&#30693;&#35299;&#30721;&#22120;&#25913;&#36827;&#38750;&#33258;&#22238;&#24402;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
DePA: Improving Non-autoregressive Machine Translation with Dependency-Aware Decoder. (arXiv:2203.16266v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16266
&lt;/p&gt;
&lt;p&gt;
DePA&#26159;&#19968;&#31181;&#20381;&#36182;&#24863;&#30693;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#33258;&#22238;&#24402;&#39044;&#35757;&#32451;&#21644;&#20851;&#27880;&#36716;&#25442;&#20004;&#20010;&#27493;&#39588;&#26469;&#25913;&#36827;&#38750;&#33258;&#22238;&#24402;&#26426;&#22120;&#32763;&#35793;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DePA&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#26426;&#22120;&#32763;&#35793;&#65288;NAT&#65289;&#27169;&#22411;&#19982;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;AT&#65289;&#27169;&#22411;&#30456;&#27604;&#65292;&#32763;&#35793;&#36136;&#37327;&#36739;&#20302;&#65292;&#22240;&#20026;NAT&#35299;&#30721;&#22120;&#22312;&#35299;&#30721;&#22120;&#36755;&#20837;&#20013;&#19981;&#20381;&#36182;&#20110;&#20043;&#21069;&#30340;&#30446;&#26631;&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#20381;&#36182;&#24863;&#30693;&#35299;&#30721;&#22120;&#65288;DePA&#65289;&#65292;&#20174;&#35299;&#30721;&#22120;&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;&#35299;&#30721;&#22120;&#36755;&#20837;&#20004;&#20010;&#26041;&#38754;&#26469;&#22686;&#24378;&#23436;&#20840;NAT&#27169;&#22411;&#20013;&#30340;&#30446;&#26631;&#20381;&#36182;&#24314;&#27169;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#22238;&#24402;&#30340;&#21069;&#21521;-&#21518;&#21521;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#22312;NAT&#35757;&#32451;&#20043;&#21069;&#65292;&#20351;NAT&#35299;&#30721;&#22120;&#33021;&#22815;&#36880;&#28176;&#23398;&#20064;&#21452;&#21521;&#30446;&#26631;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#29992;&#20110;&#26368;&#32456;&#30340;NAT&#35757;&#32451;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#20851;&#27880;&#36716;&#25442;&#36807;&#31243;&#65292;&#23558;&#35299;&#30721;&#22120;&#36755;&#20837;&#20174;&#28304;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#36716;&#25442;&#21040;&#30446;&#26631;&#35821;&#35328;&#34920;&#31034;&#31354;&#38388;&#65292;&#20174;&#32780;&#20351;&#35299;&#30721;&#22120;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#30446;&#26631;&#20381;&#36182;&#20851;&#31995;&#12290;DePA&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#20840;NAT&#27169;&#22411;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;DePA&#22312;&#31454;&#20105;&#28608;&#28872;&#19988;&#20855;&#26377;&#39046;&#20808;&#27700;&#24179;&#30340;&#27169;&#22411;&#19978;&#37117;&#26377;&#31283;&#23450;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive machine translation (NAT) models have lower translation quality than autoregressive translation (AT) models because NAT decoders do not depend on previous target tokens in the decoder input. We propose a novel and general Dependency-Aware Decoder (DePA) to enhance target dependency modeling in the decoder of fully NAT models from two perspectives: decoder self-attention and decoder input. First, we propose an autoregressive forward-backward pre-training phase before NAT training, which enables the NAT decoder to gradually learn bidirectional target dependencies for the final NAT training. Second, we transform the decoder input from the source language representation space to the target language representation space through a novel attentive transformation process, which enables the decoder to better capture target dependencies. DePA can be applied to any fully NAT models. Extensive experiments show that DePA consistently improves highly competitive and state-of-the-a
&lt;/p&gt;</description></item><item><title>Transformer&#26159;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#32593;&#32476;&#26550;&#26500;&#65292;&#23436;&#20840;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21462;&#20195;&#20102;&#22797;&#26434;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#23454;&#39564;&#35777;&#26126;Transformer&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#36136;&#37327;&#26356;&#22909;&#12289;&#24182;&#34892;&#21270;&#25928;&#26524;&#26356;&#20339;&#65292;&#19988;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#12290;&#23427;&#22312;&#33521;&#35793;&#24503;&#21644;&#33521;&#35793;&#27861;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/1706.03762</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#23601;&#26159;&#19968;&#20999;&#65288;arXiv:1706.03762v6 [cs.CL]&#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Attention Is All You Need. (arXiv:1706.03762v6 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1706.03762
&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#32593;&#32476;&#26550;&#26500;&#65292;&#23436;&#20840;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21462;&#20195;&#20102;&#22797;&#26434;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#23454;&#39564;&#35777;&#26126;Transformer&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#36136;&#37327;&#26356;&#22909;&#12289;&#24182;&#34892;&#21270;&#25928;&#26524;&#26356;&#20339;&#65292;&#19988;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#12290;&#23427;&#22312;&#33521;&#35793;&#24503;&#21644;&#33521;&#35793;&#27861;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20027;&#35201;&#30340;&#24207;&#21015;&#36716;&#25442;&#27169;&#22411;&#22522;&#20110;&#22797;&#26434;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#37197;&#32622;&#12290;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#36824;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#36830;&#25509;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#32593;&#32476;&#26550;&#26500;&#65292;Transformer&#65292;&#23436;&#20840;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#19981;&#20877;&#20351;&#29992;&#24490;&#29615;&#21644;&#21367;&#31215;&#12290;&#22312;&#20004;&#20010;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#21516;&#26102;&#26356;&#26131;&#20110;&#24182;&#34892;&#21270;&#65292;&#35757;&#32451;&#26102;&#38388;&#26174;&#33879;&#20943;&#23569;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;WMT 2014&#33521;&#35793;&#24503;&#20219;&#21153;&#19978;&#36798;&#21040;28.4&#30340;BLEU&#20998;&#25968;&#65292;&#27604;&#29616;&#26377;&#26368;&#22909;&#32467;&#26524;&#65288;&#21253;&#25324;&#38598;&#25104;&#27169;&#22411;&#65289;&#25552;&#39640;&#20102;2&#20010;BLEU&#20998;&#12290;&#22312;WMT 2014&#33521;&#35793;&#27861;&#20219;&#21153;&#19978;&#65292;&#22312;8&#20010;GPU&#19978;&#35757;&#32451;&#20102;3.5&#22825;&#21518;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33719;&#24471;&#20102;41.8&#30340;&#21333;&#27169;&#22411;&#26368;&#26032;BLEU&#20998;&#25968;&#65292;&#35757;&#32451;&#25104;&#26412;&#20165;&#20026;&#25991;&#29486;&#20013;&#26368;&#22909;&#27169;&#22411;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Transformer&#26550;&#26500;&#30340;&#20248;&#21183;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transforme
&lt;/p&gt;</description></item></channel></rss>