<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>CB2&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#24179;&#21488;&#65292;&#22312;3D&#28216;&#25103;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#21518;&#31471;&#26381;&#21153;&#22120;&#21644;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#12290;&#23427;&#22312;&#21487;&#25193;&#23637;&#30340;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.08127</link><description>&lt;p&gt;
CB2&#65306;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30740;&#31350;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
CB2: Collaborative Natural Language Interaction Research Platform. (arXiv:2303.08127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08127
&lt;/p&gt;
&lt;p&gt;
CB2&#26159;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#30340;&#24179;&#21488;&#65292;&#22312;3D&#28216;&#25103;&#29615;&#22659;&#20013;&#25552;&#20379;&#20102;&#21518;&#31471;&#26381;&#21153;&#22120;&#21644;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#12290;&#23427;&#22312;&#21487;&#25193;&#23637;&#30340;&#30740;&#31350;&#20013;&#23637;&#31034;&#20102;&#23398;&#20064;&#30340;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CB2 &#26159;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#24179;&#21488;&#65292;&#29992;&#20110;&#30740;&#31350;&#22522;&#20110;&#20219;&#21153;&#30340;&#24773;&#22659;&#19979;&#30340;&#21512;&#20316;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#12290;&#23427;&#21253;&#25324;&#19968;&#20010; 3D &#28216;&#25103;&#29615;&#22659;&#12289;&#19968;&#20010;&#21518;&#31471;&#26381;&#21153;&#22120;&#65292;&#21487;&#20026;&#20154;&#31867;&#26234;&#33021;&#20307;&#25552;&#20379;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#21450;&#21508;&#31181;&#24037;&#20855;&#21644;&#27969;&#31243;&#65292;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#24615;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312; https://cb2.ai &#19978;&#23637;&#31034;&#20102;&#19968;&#20010;&#20855;&#26377;&#23398;&#20064;&#25351;&#20196;&#36319;&#38543;&#27169;&#22411;&#30340;&#31995;&#32479;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23454;&#38469;&#19978;&#36827;&#34892;&#35299;&#26512;&#20197;&#21450;&#20026;&#20160;&#20040;&#33021;&#25429;&#25417;&#35299;&#26512;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#31867;&#20284;&#20110;BERT&#25110;RoBERTa&#36825;&#26679;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36817;&#20284;&#25191;&#34892;&#33521;&#35821;PCFG&#30340;Inside-Outside&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.08117</link><description>&lt;p&gt;
&#36716;&#25442;&#22120;&#22312;&#39044;&#27979;&#25513;&#30721;&#21333;&#35789;&#26102;&#26159;&#21542;&#35299;&#26512;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Transformers Parse while Predicting the Masked Word?. (arXiv:2303.08117v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23454;&#38469;&#19978;&#36827;&#34892;&#35299;&#26512;&#20197;&#21450;&#20026;&#20160;&#20040;&#33021;&#25429;&#25417;&#35299;&#26512;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#31867;&#20284;&#20110;BERT&#25110;RoBERTa&#36825;&#26679;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36817;&#20284;&#25191;&#34892;&#33521;&#35821;PCFG&#30340;Inside-Outside&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#31867;&#20284;&#20110;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#36825;&#26679;&#30340;&#26080;&#30417;&#30563;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#21487;&#20197;&#23545;&#35821;&#35328;&#32467;&#26500;&#36827;&#34892;&#32534;&#30721;&#65292;&#20363;&#22914;&#20381;&#36182;&#20851;&#31995;&#21644;&#32452;&#25104;&#25104;&#20998;&#20998;&#26512;&#26641;&#12290;&#20294;&#26159;&#20154;&#20204;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#23454;&#38469;&#19978;&#36827;&#34892;&#35299;&#26512;&#25110;&#20165;&#36827;&#34892;&#19982;&#35299;&#26512;&#24369;&#30456;&#20851;&#30340;&#19968;&#20123;&#35745;&#31639;&#23384;&#22312;&#30097;&#38382;&#12290;&#26412;&#25991;&#22312;&#29983;&#25104;&#24314;&#27169;&#30340;&#19978;&#19979;&#25991;&#20013;&#19968;&#27493;&#27493;&#22238;&#31572;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;(a)&#26159;&#21542;&#26377;&#21487;&#33021;&#26126;&#30830;&#25551;&#36848;&#20855;&#26377;&#29616;&#23454;&#23884;&#20837;&#32500;&#24230;&#65292;&#22836;&#25968;&#31561;&#30340;&#36716;&#25442;&#22120;&#65292;&#33021;&#22815;&#36827;&#34892;&#35299;&#26512;&#29978;&#33267;&#36817;&#20284;&#35299;&#26512;&#65307;(b)&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#20160;&#20040;&#33021;&#22815;&#25429;&#25417;&#35299;&#26512;&#32467;&#26500;&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;&#31867;&#20284;&#20110;BERT&#25110;RoBERTa&#36825;&#26679;&#30340;&#20013;&#31561;&#22823;&#23567;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36817;&#20284;&#25191;&#34892;&#33521;&#35821;PCFG&#65288;Marcus&#31561;&#65292;1993&#65289;&#30340;Inside-Outside&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#65292;&#22312;PCFG&#29983;&#25104;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#19978;&#65292;Inside-Outside&#31639;&#27861;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have been shown to encode linguistic structures, e.g. dependency and constituency parse trees, in their embeddings while being trained on unsupervised loss functions like masked language modeling. Some doubts have been raised whether the models actually are doing parsing or only some computation weakly correlated with it. We study questions: (a) Is it possible to explicitly describe transformers with realistic embedding dimension, number of heads, etc. that are capable of doing parsing -- or even approximate parsing? (b) Why do pre-trained models capture parsing structure? This paper takes a step toward answering these questions in the context of generative modeling with PCFGs. We show that masked language models like BERT or RoBERTa of moderate sizes can approximately execute the Inside-Outside algorithm for the English PCFG [Marcus et al, 1993]. We also show that the Inside-Outside algorithm is optimal for masked language modeling loss on the PCFG-generate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;Simfluence&#65292;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#35757;&#32451;&#36807;&#31243;&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#36861;&#28335;&#27169;&#22411;&#22312;&#20219;&#20309;&#32473;&#23450;&#26679;&#20363;&#19978;&#30340;&#39044;&#27979;&#32467;&#26524;&#21040;&#29305;&#23450;&#30340;&#26377;&#24433;&#21709;&#21147;&#30340;&#35757;&#32451;&#26679;&#20363;&#65292;&#20174;&#32780;&#30740;&#31350;&#35757;&#32451;&#26679;&#20363;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.08114</link><description>&lt;p&gt;
Simfluence&#65306;&#36890;&#36807;&#27169;&#25311;&#35757;&#32451;&#36807;&#31243;&#24314;&#27169;&#21333;&#20010;&#35757;&#32451;&#26679;&#20363;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs. (arXiv:2303.08114v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;Simfluence&#65292;&#36890;&#36807;&#24314;&#31435;&#19968;&#20010;&#35757;&#32451;&#36807;&#31243;&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#36861;&#28335;&#27169;&#22411;&#22312;&#20219;&#20309;&#32473;&#23450;&#26679;&#20363;&#19978;&#30340;&#39044;&#27979;&#32467;&#26524;&#21040;&#29305;&#23450;&#30340;&#26377;&#24433;&#21709;&#21147;&#30340;&#35757;&#32451;&#26679;&#20363;&#65292;&#20174;&#32780;&#30740;&#31350;&#35757;&#32451;&#26679;&#20363;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#25968;&#25454;&#24402;&#22240;&#65288;TDA&#65289;&#26041;&#27861;&#21487;&#20197;&#36861;&#28335;&#27169;&#22411;&#22312;&#20219;&#20309;&#32473;&#23450;&#31034;&#20363;&#19978;&#30340;&#39044;&#27979;&#32467;&#26524;&#21040;&#29305;&#23450;&#30340;&#26377;&#24433;&#21709;&#21147;&#30340;&#35757;&#32451;&#31034;&#20363;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20026;&#27599;&#20010;&#35757;&#32451;&#31034;&#20363;&#20998;&#37197;&#19968;&#20010;&#26631;&#37327;&#24433;&#21709;&#20998;&#25968;&#26469;&#23454;&#29616;&#65292;&#20551;&#35774;&#24433;&#21709;&#26159;&#21487;&#21152;&#30340;&#12290;&#20294;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#30001;&#20110;&#35832;&#22914;&#31034;&#20363;&#20043;&#38388;&#30340;&#20887;&#20313;&#12289;&#35757;&#32451;&#39034;&#24207;&#21644;&#35838;&#31243;&#23398;&#20064;&#25928;&#24212;&#31561;&#22240;&#32032;&#65292;&#35757;&#32451;&#31034;&#20363;&#20197;&#39640;&#24230;&#38750;&#21487;&#21152;&#30340;&#26041;&#24335;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#20132;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Simfluence&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;TDA&#33539;&#24335;&#65292;&#30446;&#26631;&#19981;&#26159;&#20026;&#27599;&#20010;&#31034;&#20363;&#29983;&#25104;&#21333;&#20010;&#30340;&#24433;&#21709;&#20998;&#25968;&#65292;&#32780;&#26159;&#19968;&#20010;&#35757;&#32451;&#36807;&#31243;&#27169;&#25311;&#22120;&#65306;&#29992;&#25143;&#21487;&#20197;&#35810;&#38382;&#8220;&#22914;&#26524;&#25105;&#30340;&#27169;&#22411;&#35757;&#32451;&#20102;&#31034;&#20363; z1&#12289;z2&#12289;&#8230;&#8230;&#12289;zn&#65292;&#23427;&#22312;&#31034;&#20363;ztest&#19978;&#30340;&#34920;&#29616;&#20250;&#22914;&#20309;&#65311;&#8221;&#65307;&#28982;&#21518;&#27169;&#25311;&#22120;&#24212;&#35813;&#36755;&#20986;&#19968;&#20010;&#27169;&#25311;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#23427;&#26159;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#65292;&#22312;&#27169;&#25311;&#30340;&#27599;&#20010;&#27493;&#39588;&#37117;&#39044;&#27979;&#20102;&#22312;ztest&#19978;&#30340;&#25439;&#22833;&#12290;&#36825;&#20351;&#29992;&#25143;&#33021;&#22815;&#22238;&#31572;&#21453;&#20107;&#23454;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training data attribution (TDA) methods offer to trace a model's prediction on any given example back to specific influential training examples. Existing approaches do so by assigning a scalar influence score to each training example, under a simplifying assumption that influence is additive. But in reality, we observe that training examples interact in highly non-additive ways due to factors such as inter-example redundancy, training order, and curriculum learning effects.  To study such interactions, we propose Simfluence, a new paradigm for TDA where the goal is not to produce a single influence score per example, but instead a training run simulator: the user asks, ``If my model had trained on example $z_1$, then $z_2$, ..., then $z_n$, how would it behave on $z_{test}$?''; the simulator should then output a simulated training run, which is a time series predicting the loss on $z_{test}$ at every step of the simulated run. This enables users to answer counterfactual questions about
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#30340;&#35748;&#30693;&#19981;&#33021;&#20165;&#20165;&#23616;&#38480;&#20110;&#35821;&#27861;&#32467;&#26500;&#30740;&#31350;&#65292;&#20854;&#27169;&#24335;&#21270;&#12289;&#21160;&#24577;&#30340;&#12289;&#22810;&#27169;&#24577;&#30340;&#12289;&#20855;&#26377;&#30446;&#30340;&#24615;&#21450;&#20419;&#36827;&#20182;&#20154;&#21644;&#33258;&#36523;&#29702;&#24819;&#34892;&#20026;&#65288;&#25110;&#24605;&#32771;&#65289;&#30340;&#29305;&#36136;&#21487;&#33021;&#26159;&#19968;&#20010;&#26032;&#30340;&#12289;&#21487;&#34892;&#30340;&#35821;&#35328;&#29702;&#35770;&#12290;</title><link>http://arxiv.org/abs/2303.08080</link><description>&lt;p&gt;
&#35821;&#27861;&#32467;&#26500;&#22806;&#30340;&#35328;&#35821;&#34892;&#20026;&#65306;&#36229;&#36234;&#26031;&#37329;&#32435;&#21644;&#20052;&#22982;&#26031;&#22522;
&lt;/p&gt;
&lt;p&gt;
Verbal behavior without syntactic structures: beyond Skinner and Chomsky. (arXiv:2303.08080v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08080
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#30340;&#35748;&#30693;&#19981;&#33021;&#20165;&#20165;&#23616;&#38480;&#20110;&#35821;&#27861;&#32467;&#26500;&#30740;&#31350;&#65292;&#20854;&#27169;&#24335;&#21270;&#12289;&#21160;&#24577;&#30340;&#12289;&#22810;&#27169;&#24577;&#30340;&#12289;&#20855;&#26377;&#30446;&#30340;&#24615;&#21450;&#20419;&#36827;&#20182;&#20154;&#21644;&#33258;&#36523;&#29702;&#24819;&#34892;&#20026;&#65288;&#25110;&#24605;&#32771;&#65289;&#30340;&#29305;&#36136;&#21487;&#33021;&#26159;&#19968;&#20010;&#26032;&#30340;&#12289;&#21487;&#34892;&#30340;&#35821;&#35328;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20160;&#20040;&#26159;&#35821;&#35328;&#35748;&#30693;&#65311;&#33258;&#20052;&#22982;&#26031;&#22522;&#38761;&#21629;&#20197;&#26469;&#65292;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#26222;&#36941;&#22238;&#31572;&#26159;&#65306;&#25317;&#26377;&#20135;&#29983;&#35821;&#27861;&#32467;&#26500;&#30340;&#29983;&#25104;&#24335;&#35821;&#27861;&#12290;&#25968;&#21313;&#24180;&#36807;&#21435;&#20102;&#65292;&#20219;&#20309;&#35821;&#35328;&#30340;&#36817;&#20284;&#35821;&#27861;&#32467;&#26500;&#37117;&#26410;&#34987;&#21046;&#23450;&#20986;&#26469;&#12290;&#36890;&#29992;&#35821;&#27861;&#26159;&#22825;&#29983;&#20855;&#26377;&#30340;&#65292;&#36825;&#20010;&#29702;&#24565;&#24050;&#32463;&#35777;&#26126;&#26159;&#27627;&#26080;&#25104;&#26524;&#30340;&#12290;&#35797;&#22270;&#23637;&#31034;&#35821;&#27861;&#22914;&#20309;&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;&#20063;&#23649;&#23649;&#22833;&#36133;&#12290;&#20026;&#20102;&#25670;&#33073;&#36825;&#20010;&#20725;&#23616;&#65292;&#25105;&#20204;&#24517;&#39035;&#37325;&#26032;&#21457;&#29616;&#35821;&#35328;&#19982;&#25152;&#26377;&#20854;&#20182;&#20154;&#31867;&#34892;&#20026;&#30340;&#30456;&#20284;&#24615;&#65306;&#21160;&#24577;&#30340;&#12289;&#31038;&#20132;&#30340;&#12289;&#22810;&#27169;&#24577;&#30340;&#12289;&#26377;&#27169;&#24335;&#30340;&#12289;&#26377;&#30446;&#30340;&#24615;&#30340;&#65292;&#20854;&#30446;&#30340;&#26159;&#20419;&#36827;&#20182;&#20154;&#21644;&#33258;&#24049;&#30340;&#29702;&#24819;&#34892;&#20026;&#65288;&#25110;&#24605;&#32771;&#65289;&#12290;&#26368;&#36817;&#23545;&#34892;&#20026;&#22609;&#36896;&#21644;&#32467;&#26500;&#30340;&#24515;&#29702;&#12289;&#35745;&#31639;&#12289;&#31070;&#32463;&#29983;&#29289;&#23398;&#21644;&#36827;&#21270;&#26041;&#38754;&#30340;&#20102;&#35299;&#65292;&#21487;&#33021;&#20250;&#25351;&#24341;&#25105;&#20204;&#36208;&#21521;&#19968;&#31181;&#26032;&#30340;&#12289;&#21487;&#34892;&#30340;&#35821;&#35328;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
What does it mean to know language? Since the Chomskian revolution, one popular answer to this question has been: to possess a generative grammar that exclusively licenses certain syntactic structures. Decades later, not even an approximation to such a grammar, for any language, has been formulated; the idea that grammar is universal and innately specified has proved barren; and attempts to show how it could be learned from experience invariably come up short. To move on from this impasse, we must rediscover the extent to which language is like any other human behavior: dynamic, social, multimodal, patterned, and purposive, its purpose being to promote desirable actions (or thoughts) in others and self. Recent psychological, computational, neurobiological, and evolutionary insights into the shaping and structure of behavior may then point us toward a new, viable account of language.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; Happy-GLL &#35299;&#26512;&#22120;&#29983;&#25104;&#22120;&#30340;&#21518;&#31471;&#65292;&#35813;&#21518;&#31471;&#25903;&#25345;&#21442;&#25968;&#21270;&#38750;&#32456;&#32467;&#31526;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#37325;&#29992;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22359;&#21270;&#12289;&#21487;&#37325;&#29992;&#21644;&#23436;&#25972;&#35299;&#26512;&#22120;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2303.08044</link><description>&lt;p&gt;
Happy-GLL:&#27169;&#22359;&#21270;&#12289;&#21487;&#37325;&#29992;&#21644;&#23436;&#25972;&#30340;&#25903;&#25345;&#21442;&#25968;&#21270;&#38750;&#32456;&#32467;&#31526;&#30340;&#33258;&#39030;&#21521;&#19979;&#35299;&#26512;&#22120;
&lt;/p&gt;
&lt;p&gt;
Happy-GLL: modular, reusable and complete top-down parsers for parameterized nonterminals. (arXiv:2303.08044v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; Happy-GLL &#35299;&#26512;&#22120;&#29983;&#25104;&#22120;&#30340;&#21518;&#31471;&#65292;&#35813;&#21518;&#31471;&#25903;&#25345;&#21442;&#25968;&#21270;&#38750;&#32456;&#32467;&#31526;&#30340;&#27169;&#22359;&#21270;&#21644;&#21487;&#37325;&#29992;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#29983;&#25104;&#27169;&#22359;&#21270;&#12289;&#21487;&#37325;&#29992;&#21644;&#23436;&#25972;&#35299;&#26512;&#22120;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#26512;&#22120;&#29983;&#25104;&#22120;&#21644;&#35299;&#26512;&#22120;&#32452;&#21512;&#24211;&#26159;&#29983;&#25104;&#35299;&#26512;&#22120;&#30340;&#26368;&#27969;&#34892;&#30340;&#24037;&#20855;&#12290;&#35299;&#26512;&#22120;&#32452;&#21512;&#22120;&#20351;&#29992;&#20027;&#26426;&#35821;&#35328;&#20197;&#39640;&#38454;&#20989;&#25968;&#30340;&#24418;&#24335;&#25552;&#20379;&#21487;&#37325;&#29992;&#30340;&#32452;&#20214;&#65292;&#24182;&#23558;&#35299;&#26512;&#22120;&#20316;&#20026;&#21442;&#25968;&#12290;&#24456;&#23569;&#26377;&#35299;&#26512;&#22120;&#29983;&#25104;&#22120;&#36890;&#36807;&#25277;&#35937;&#25903;&#25345;&#36825;&#31181;&#37325;&#29992;&#65292;&#32780;&#19988;&#29983;&#25104;&#30340;&#35299;&#26512;&#22120;&#20687;&#20135;&#29983;&#23427;&#20204;&#30340;&#35821;&#27861;&#37096;&#20998;&#19968;&#26679;&#27169;&#22359;&#21270;&#21644;&#21487;&#37325;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#22522;&#20110; GL&#31639;&#27861;&#30340; FUN-GLL&#21464;&#20307;&#65292;&#20174;&#20855;&#26377;&#21442;&#25968;&#21270;&#38750;&#32456;&#32467;&#31526;&#30340;&#35821;&#27861;&#25551;&#36848;&#29983;&#25104;&#27169;&#22359;&#21270;&#12289;&#21487;&#37325;&#29992;&#21644;&#23436;&#25972;&#30340;&#33258;&#39030;&#21521;&#19979;&#35299;&#26512;&#22120;&#12290;&#35813;&#31574;&#30053;&#20316;&#20026; Happy &#35299;&#26512;&#22120;&#29983;&#25104;&#22120;&#30340;&#19968;&#31181;&#26032;&#30340;&#21518;&#31471;&#26469;&#23637;&#31034;&#21644;&#35752;&#35770;&#12290;Happy &#35821;&#27861;&#21487;&#20197;&#21253;&#21547;&#21442;&#25968;&#21270;&#38750;&#32456;&#32467;&#31526;&#65292;&#22312;&#36825;&#20123;&#38750;&#32456;&#32467;&#31526;&#20013;&#65292;&#21442;&#25968;&#25277;&#35937;&#20102;&#35821;&#27861;&#31526;&#21495;&#65292;&#36890;&#36807;&#25480;&#26435;&#25277;&#35937;&#26426;&#21046;&#23450;&#20041;&#21487;&#37325;&#29992;&#30340;&#35821;&#27861;&#36816;&#31639;&#31526;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340; Happy &#21518;&#31471;&#19981;&#33021;&#23436;&#20840;&#21457;&#25381;&#21442;&#25968;&#21270;&#38750;&#32456;&#32467;&#31526;&#30340;&#28508;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340; Happy-GLL &#21518;&#31471;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20801;&#35768;&#22312; Happy &#35821;&#27861;&#20013;&#20351;&#29992;&#27169;&#22359;&#21270;&#21644;&#21487;&#37325;&#29992;&#30340;&#21442;&#25968;&#21270;&#38750;&#32456;&#32467;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parser generators and parser combinator libraries are the most popular tools for producing parsers. Parser combinators use the host language to provide reusable components in the form of higher-order functions with parsers as parameters. Very few parser generators support this kind of reuse through abstraction and even fewer generate parsers that are as modular and reusable as the parts of the grammar for which they are produced. This paper presents a strategy for generating modular, reusable and complete top-down parsers from syntax descriptions with parameterized nonterminals, based on the FUN-GLL variant of the GLL algorithm.  The strategy is discussed and demonstrated as a novel back-end for the Happy parser generator. Happy grammars can contain `parameterized nonterminals' in which parameters abstract over grammar symbols, granting an abstraction mechanism to define reusable grammar operators. However, the existing Happy back-ends do not deliver on the full potential of parameteri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#28151;&#21512;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65288;MCL2&#65289;&#29992;&#20110;&#24322;&#26500;&#27979;&#35797;&#38382;&#39064;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#19982;&#29616;&#26377;&#30340;&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.08039</link><description>&lt;p&gt;
TQ-Net&#65306;&#28151;&#21512;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#24322;&#26500;&#27979;&#35797;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
TQ-Net: Mixed Contrastive Representation Learning For Heterogeneous Test Questions. (arXiv:2303.08039v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08039
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#28151;&#21512;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65288;MCL2&#65289;&#29992;&#20110;&#24322;&#26500;&#27979;&#35797;&#38382;&#39064;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#19982;&#29616;&#26377;&#30340;&#25991;&#26412;&#34920;&#31034;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#36890;&#36807;&#32593;&#32476;&#23398;&#20064;&#20197;&#20415;&#33719;&#21462;&#28023;&#37327;&#23398;&#20064;&#26448;&#26009;&#65288;&#20363;&#22914;&#27979;&#35797;&#38382;&#39064;/&#31508;&#35760;&#65289;&#65292;&#22240;&#27492;&#20934;&#30830;&#29702;&#35299;&#23398;&#20064;&#26448;&#26009;&#25104;&#20026;&#37325;&#35201;&#38382;&#39064;&#65292;&#23545;&#35768;&#22810;&#25945;&#32946;&#24212;&#29992;&#31243;&#24207;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#34920;&#31034;&#38382;&#39064;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#27979;&#35797;&#38382;&#39064;&#65288;TQ&#65289;&#36890;&#24120;&#26159;&#24322;&#26500;&#30340;&#21644;&#22810;&#27169;&#24335;&#30340;&#65292;&#20363;&#22914;&#65292;&#19968;&#20123;&#38382;&#39064;&#20165;&#21253;&#21547;&#25991;&#26412;&#65292;&#32780;&#21478;&#19968;&#20123;&#38382;&#39064;&#21253;&#21547;&#36229;&#20986;&#20854;&#25991;&#23383;&#25551;&#36848;&#30340;&#22270;&#20687;&#20449;&#24687;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#37117;&#38590;&#20197;&#23398;&#20064;&#38382;&#39064;&#30340;&#34701;&#21512;&#34920;&#31034;&#12290;&#21516;&#26102;&#65292;&#20256;&#32479;&#26041;&#27861;&#22914;&#22270;&#20687;&#35828;&#26126;&#20063;&#26080;&#27861;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#22270;&#20687;&#21487;&#33021;&#21253;&#21547;&#20114;&#34917;&#32780;&#19981;&#26159;&#22797;&#21046;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#26080;&#30417;&#30563;&#23454;&#20363;&#32423;&#23545;&#27604;&#35757;&#32451;&#26041;&#27861;&#65288;MCL:&#28151;&#21512;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65289;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#21333;&#29420;&#25991;&#26412;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Mixed Contrastive Learning&#65288;MCL2&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#34701;&#21512;&#20102;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#65292;&#37319;&#29992;&#22810;&#23454;&#20363;&#21644;&#22810;&#32423;&#29305;&#24449;&#34701;&#21512;&#29992;&#20110;TQ&#34920;&#31034;&#23398;&#20064;&#12290;&#22312;&#20004;&#20010;TQ&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#65292;&#24182;&#22312;TQ&#20998;&#31867;&#21644;&#38382;&#39064;&#31867;&#22411;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, more and more people study online for the convenience of access to massive learning materials (e.g. test questions/notes), thus accurately understanding learning materials became a crucial issue, which is essential for many educational applications. Previous studies focus on using language models to represent the question data. However, test questions (TQ) are usually heterogeneous and multi-modal, e.g., some of them may only contain text, while others half contain images with information beyond their literal description. In this context, both supervised and unsupervised methods are difficult to learn a fused representation of questions. Meanwhile, this problem cannot be solved by conventional methods such as image caption, as the images may contain information complementary rather than duplicate to the text. In this paper, we first improve previous text-only representation with a two-stage unsupervised instance level contrastive based pre-training method (MCL: Mixture Unsupe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;2022 N2C2&#20020;&#24202;&#25361;&#25112;&#36187;&#20013;&#20851;&#20110;&#36827;&#23637;&#31508;&#35760;&#29702;&#35299;-&#35780;&#20272;&#21644;&#35745;&#21010;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#24320;&#21457;&#24182;&#35780;&#20272;&#33258;&#21160;&#39044;&#27979;&#22240;&#26524;&#30456;&#20851;&#27010;&#24565;&#30340;NLP&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.08038</link><description>&lt;p&gt;
&#36827;&#23637;&#31508;&#35760;&#29702;&#35299;-&#35780;&#20272;&#21644;&#35745;&#21010;&#25512;&#29702;&#65306;2022 N2C2Track 3&#20849;&#20139;&#20219;&#21153;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Progress Note Understanding -- Assessment and Plan Reasoning: Overview of the 2022 N2C2 Track 3 Shared Task. (arXiv:2303.08038v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08038
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;2022 N2C2&#20020;&#24202;&#25361;&#25112;&#36187;&#20013;&#20851;&#20110;&#36827;&#23637;&#31508;&#35760;&#29702;&#35299;-&#35780;&#20272;&#21644;&#35745;&#21010;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#24320;&#21457;&#24182;&#35780;&#20272;&#33258;&#21160;&#39044;&#27979;&#22240;&#26524;&#30456;&#20851;&#27010;&#24565;&#30340;NLP&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#26085;&#36827;&#23637;&#31508;&#35760;&#26159;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#24120;&#35265;&#30340;&#31867;&#22411;&#65292;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#22312;&#20854;&#20013;&#35760;&#24405;&#24739;&#32773;&#30340;&#27599;&#26085;&#36827;&#23637;&#21644;&#27835;&#30103;&#35745;&#21010;&#12290; EHR&#26088;&#22312;&#35760;&#24405;&#20026;&#24739;&#32773;&#25552;&#20379;&#30340;&#25152;&#26377;&#25252;&#29702;&#65292;&#20294;&#23427;&#20063;&#20250;&#20351;&#31508;&#35760;&#33192;&#32960;&#24182;&#21253;&#21547;&#20998;&#25955;&#35786;&#26029;&#21644;&#27835;&#30103;&#35745;&#21010;&#30340;&#22810;&#20313;&#20449;&#24687;&#12290; &#22312;EHR&#20013;&#24212;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26159;&#19968;&#20010;&#19981;&#26029;&#22686;&#38271;&#30340;&#39046;&#22495;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#29992;&#20110;&#20449;&#24687;&#25552;&#21462;&#12290;&#24456;&#23569;&#26377;&#20219;&#21153;&#20351;&#29992;NLP&#26041;&#27861;&#36827;&#34892;&#19979;&#28216;&#35786;&#26029;&#20915;&#31574;&#25903;&#25345;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;2022&#24180;&#22269;&#23478;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20020;&#24202;&#25361;&#25112;&#36187;&#65288;N2C2&#65289;Track 3&#65306;&#36827;&#23637;&#31508;&#35760;&#29702;&#35299;-&#35780;&#20272;&#21644;&#35745;&#21010;&#25512;&#29702;&#65292;&#20316;&#20026;&#26032;&#19968;&#22871;&#20219;&#21153;&#30340;&#19968;&#27493;&#12290; &#35780;&#20272;&#21644;&#35745;&#21010;&#25512;&#29702;&#20219;&#21153;&#20391;&#37325;&#20110;&#36827;&#23637;&#31508;&#35760;&#30340;&#26368;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#21253;&#21547;&#20581;&#24247;&#38382;&#39064;&#21644;&#35786;&#26029;&#30340;&#35780;&#20272;&#21644;&#35745;&#21010;&#23376;&#37096;&#20998;&#12290; &#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#24182;&#35780;&#20272;&#33258;&#21160;&#39044;&#27979;&#35780;&#20272;&#21644;&#35745;&#21010;&#25512;&#29702;&#20013;&#22240;&#26524;&#30456;&#20851;&#27010;&#24565;&#30340;NLP&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Daily progress notes are common types in the electronic health record (EHR) where healthcare providers document the patient's daily progress and treatment plans. The EHR is designed to document all the care provided to patients, but it also enables note bloat with extraneous information that distracts from the diagnoses and treatment plans. Applications of natural language processing (NLP) in the EHR is a growing field with the majority of methods in information extraction. Few tasks use NLP methods for downstream diagnostic decision support. We introduced the 2022 National NLP Clinical Challenge (N2C2) Track 3: Progress Note Understanding - Assessment and Plan Reasoning as one step towards a new suite of tasks. The Assessment and Plan Reasoning task focuses on the most critical components of progress notes, Assessment and Plan subsections where health problems and diagnoses are contained. The goal of the task was to develop and evaluate NLP systems that automatically predict causal re
&lt;/p&gt;</description></item><item><title>BODEGA&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#30340;&#20869;&#23481;&#31649;&#29702;&#22330;&#26223;&#65292;&#22312;&#22235;&#20010;&#35823;&#20256;&#26816;&#27979;&#20219;&#21153;&#19978;&#27979;&#35797;&#21463;&#23475;&#27169;&#22411;&#21644;&#25915;&#20987;&#26041;&#27861;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#36827;&#34892;&#24494;&#23567;&#30340;&#25991;&#26412;&#20462;&#25913;&#65292;&#20063;&#21487;&#20197;&#27450;&#39575;&#26368;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2303.08032</link><description>&lt;p&gt;
BODEGA: &#38024;&#23545;&#21487;&#20449;&#24230;&#35780;&#20272;&#20013;&#23545;&#25239;&#24615;&#26679;&#26412;&#29983;&#25104;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BODEGA: Benchmark for Adversarial Example Generation in Credibility Assessment. (arXiv:2303.08032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08032
&lt;/p&gt;
&lt;p&gt;
BODEGA&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#27169;&#25311;&#30495;&#23454;&#30340;&#20869;&#23481;&#31649;&#29702;&#22330;&#26223;&#65292;&#22312;&#22235;&#20010;&#35823;&#20256;&#26816;&#27979;&#20219;&#21153;&#19978;&#27979;&#35797;&#21463;&#23475;&#27169;&#22411;&#21644;&#25915;&#20987;&#26041;&#27861;&#12290;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#36827;&#34892;&#24494;&#23567;&#30340;&#25991;&#26412;&#20462;&#25913;&#65292;&#20063;&#21487;&#20197;&#27450;&#39575;&#26368;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26816;&#27979;&#19981;&#21487;&#20449;&#20869;&#23481;&#65292;&#22914;&#20551;&#26032;&#38395;&#12289;&#31038;&#20132;&#23186;&#20307;&#26426;&#22120;&#20154;&#12289;&#23459;&#20256;&#31561;&#12290;&#36739;&#20026;&#20934;&#30830;&#30340;&#27169;&#22411;&#65288;&#21487;&#33021;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#26377;&#21161;&#20110;&#31649;&#29702;&#20844;&#20849;&#30005;&#23376;&#24179;&#21488;&#65292;&#24182;&#32463;&#24120;&#23548;&#33268;&#20869;&#23481;&#21019;&#24314;&#32773;&#38754;&#20020;&#25552;&#20132;&#25298;&#32477;&#25110;&#24050;&#21457;&#24067;&#25991;&#26412;&#30340;&#25764;&#19979;&#12290;&#20026;&#20102;&#36991;&#20813;&#36827;&#19968;&#27493;&#34987;&#26816;&#27979;&#65292;&#20869;&#23481;&#21019;&#24314;&#32773;&#23581;&#35797;&#20135;&#29983;&#19968;&#20010;&#31245;&#24494;&#20462;&#25913;&#36807;&#30340;&#25991;&#26412;&#29256;&#26412;&#65288;&#21363;&#25915;&#20987;&#23545;&#25239;&#24615;&#26679;&#26412;&#65289;&#65292;&#21033;&#29992;&#20998;&#31867;&#22120;&#30340;&#24369;&#28857;&#23548;&#33268;&#19981;&#21516;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BODEGA&#65306;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#22312;&#27169;&#25311;&#20869;&#23481;&#31649;&#29702;&#30340;&#30495;&#23454;&#29992;&#20363;&#20013;&#27979;&#35797;&#21463;&#23475;&#27169;&#22411;&#21644;&#25915;&#20987;&#26041;&#27861;&#22312;&#22235;&#20010;&#35823;&#20256;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#31995;&#32479;&#22320;&#27979;&#35797;&#20102;&#21463;&#27426;&#36814;&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#23545;&#21487;&#29992;&#25915;&#20987;&#25216;&#26415;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#22312;&#25991;&#26412;&#20013;&#36827;&#34892;&#24494;&#23567;&#30340;&#20462;&#25913;&#20063;&#21487;&#20197;&#27450;&#39575;&#26368;&#20934;&#30830;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification methods have been widely investigated as a way to detect content of low credibility: fake news, social media bots, propaganda, etc. Quite accurate models (likely based on deep neural networks) help in moderating public electronic platforms and often cause content creators to face rejection of their submissions or removal of already published texts. Having the incentive to evade further detection, content creators try to come up with a slightly modified version of the text (known as an attack with an adversarial example) that exploit the weaknesses of classifiers and result in a different output. Here we introduce BODEGA: a benchmark for testing both victim models and attack methods on four misinformation detection tasks in an evaluation framework designed to simulate real use-cases of content moderation. We also systematically test the robustness of popular text classifiers against available attacking techniques and discover that, indeed, in some cases barely signif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#34588;&#34562;&#31639;&#27861;&#20248;&#21270;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#21307;&#23398;&#25991;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#39640;&#20934;&#30830;&#29575;&#22312;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;99.63%&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;88%&#12290;</title><link>http://arxiv.org/abs/2303.08021</link><description>&lt;p&gt;
&#29992;&#34588;&#34562;&#31639;&#27861;&#20248;&#21270;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#25552;&#39640;&#21307;&#23398;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Optimizing Deep Learning Model Parameters with the Bees Algorithm for Improved Medical Text Classification. (arXiv:2303.08021v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#34588;&#34562;&#31639;&#27861;&#20248;&#21270;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#21307;&#23398;&#25991;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#39640;&#20934;&#30830;&#29575;&#22312;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;99.63%&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;88%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#34588;&#34562;&#31639;&#27861;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21442;&#25968;&#20248;&#21270;&#30340;&#26032;&#26426;&#21046;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#36817;&#24456;&#26377;&#21069;&#36884;&#30340;&#32676;&#26234;&#33021;&#31639;&#27861;&#12290;&#20248;&#21270;&#38382;&#39064;&#26159;&#22312;&#32473;&#23450;&#21021;&#22987;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#30830;&#23450;&#30340;&#36845;&#20195;&#27425;&#25968;&#26469;&#26368;&#22823;&#21270;&#22522;&#20110;&#21307;&#23398;&#25991;&#26412;&#20998;&#31867;&#30142;&#30149;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#21253;&#25324;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65306;&#33521;&#35821;&#21644;&#38463;&#25289;&#20271;&#35821;&#12290;&#20351;&#29992;&#38271;&#30701;&#26399;&#35760;&#24518; (LSTM) &#21644;&#34588;&#34562;&#31639;&#27861;&#65292;&#22312;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;99.63%&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#65292;&#22312;&#38463;&#25289;&#20271;&#35821;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;AraBERT&#33719;&#24471;&#20102;88%&#30340;&#26368;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel mechanism to obtain the optimal parameters of a deep learning model using the Bees Algorithm, which is a recent promising swarm intelligence algorithm. The optimization problem is to maximize the accuracy of classifying ailments based on medical text given the initial hyper-parameters to be adjusted throughout a definite number of iterations. Experiments included two different datasets: English and Arabic. The highest accuracy achieved is 99.63% on the English dataset using Long Short-Term Memory (LSTM) along with the Bees Algorithm, and 88% on the Arabic dataset using AraBERT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38134;&#34892;&#26381;&#21153;&#20013;&#30340;&#25216;&#26415;&#36741;&#21161;&#34384;&#24453;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#21644;&#35780;&#20998;&#20132;&#26131;&#20197;&#35782;&#21035;&#34384;&#24453;&#34892;&#20026;&#30340;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.08016</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26816;&#27979;&#37329;&#34701;&#20132;&#26131;&#25551;&#36848;&#20013;&#30340;&#34384;&#24453;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Detection of Abuse in Financial Transaction Descriptions Using Machine Learning. (arXiv:2303.08016v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38134;&#34892;&#26381;&#21153;&#20013;&#30340;&#25216;&#26415;&#36741;&#21161;&#34384;&#24453;&#38382;&#39064;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#21644;&#35780;&#20998;&#20132;&#26131;&#20197;&#35782;&#21035;&#34384;&#24453;&#34892;&#20026;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#26032;&#25903;&#20184;&#24179;&#21488;&#65288;NPP&#65289;&#24341;&#20837;&#20102;&#23558;&#28040;&#24687;&#20316;&#20026;&#20184;&#27454;&#25551;&#36848;&#30340;&#36739;&#38271;&#26684;&#24335;&#21518;&#65292;&#20154;&#20204;&#29616;&#22312;&#21457;&#29616;&#23427;&#34987;&#29992;&#20110;&#27807;&#36890;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#35813;&#31995;&#32479;&#34987;&#29992;&#20316;&#23450;&#21521;&#30340;&#23478;&#24237;&#26292;&#21147;&#24418;&#24335;&#12290;&#36825;&#31181;&#21033;&#29992;&#25216;&#26415;&#23454;&#29616;&#30340;&#34384;&#24453;&#34892;&#20026;&#22312;&#35782;&#21035;&#12289;&#37319;&#21462;&#25514;&#26045;&#21644;&#32416;&#27491;&#36825;&#31181;&#34892;&#20026;&#26041;&#38754;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#28595;&#22823;&#21033;&#20122;&#32852;&#37030;&#38134;&#34892;&#30340;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#65288;CBA AI Labs&#65289;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36827;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#31995;&#32479;&#65292;&#23450;&#26399;&#35780;&#20998;&#25152;&#26377;&#20132;&#26131;&#65292;&#24182;&#22312;&#25968;&#30334;&#19975;&#26465;&#35760;&#24405;&#20013;&#35782;&#21035;&#39640;&#39118;&#38505;&#34384;&#24453;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since introducing changes to the New Payments Platform (NPP) to include longer messages as payment descriptions, it has been identified that people are now using it for communication, and in some cases, the system was being used as a targeted form of domestic and family violence. This type of tech-assisted abuse poses new challenges in terms of identification, actions and approaches to rectify this behaviour. Commonwealth Bank of Australia's Artificial Intelligence Labs team (CBA AI Labs) has developed a new system using advances in deep learning models for natural language processing (NLP) to create a powerful abuse detector that periodically scores all the transactions, and identifies cases of high-risk abuse in millions of records. In this paper, we describe the problem of tech-assisted abuse in the context of banking services, outline the developed model and its performance, and the operating framework more broadly.
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#22823;&#37096;&#20998;&#35821;&#35328;&#22788;&#29702;&#23454;&#39564;&#20013;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#20284;&#65292;&#33021;&#22815;&#20135;&#29983;&#20154;&#31867;&#19968;&#26679;&#30340;&#35821;&#35328;&#20351;&#29992;&#29305;&#24449;&#12290;&#20294;&#22312;&#20004;&#20010;&#23454;&#39564;&#20013;&#23384;&#22312;&#20559;&#24046;&#65292;&#35828;&#26126;&#20154;&#31867;&#21644;&#26426;&#22120;&#35821;&#35328;&#22788;&#29702;&#20043;&#38388;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.08014</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#21644;&#20154;&#31867;&#22312;&#35821;&#35328;&#20351;&#29992;&#19978;&#30456;&#20284;?
&lt;/p&gt;
&lt;p&gt;
Does ChatGPT resemble humans in language use?. (arXiv:2303.08014v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08014
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#22823;&#37096;&#20998;&#35821;&#35328;&#22788;&#29702;&#23454;&#39564;&#20013;&#19982;&#20154;&#31867;&#34920;&#29616;&#30456;&#20284;&#65292;&#33021;&#22815;&#20135;&#29983;&#20154;&#31867;&#19968;&#26679;&#30340;&#35821;&#35328;&#20351;&#29992;&#29305;&#24449;&#12290;&#20294;&#22312;&#20004;&#20010;&#23454;&#39564;&#20013;&#23384;&#22312;&#20559;&#24046;&#65292;&#35828;&#26126;&#20154;&#31867;&#21644;&#26426;&#22120;&#35821;&#35328;&#22788;&#29702;&#20043;&#38388;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21644;&#20197;LLM&#20026;&#39537;&#21160;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;(&#22914;ChatGPT)&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#22312;&#35748;&#30693;&#23618;&#38754;&#19978;&#65292;&#23427;&#20204;&#30340;&#20869;&#37096;&#26426;&#21046;&#20173;&#28982;&#26159;&#40657;&#21283;&#23376;&#65292;&#19981;&#28165;&#26970;LLM&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#21542;&#33021;&#22815;&#21457;&#23637;&#20986;&#20154;&#31867;&#30340;&#35821;&#35328;&#20351;&#29992;&#29305;&#24449;&#12290;&#25105;&#20204;&#23545;ChatGPT&#36827;&#34892;&#20102;12&#20010;&#23454;&#39564;&#65292;&#27599;&#20010;&#23454;&#39564;&#27880;&#20876;&#21069;&#36827;&#34892;&#20102;1000&#27425;&#36816;&#34892;&#12290;&#22312;&#20854;&#20013;&#30340;10&#20010;&#23454;&#39564;&#20013;&#65292;ChatGPT&#22797;&#21046;&#20102;&#20154;&#31867;&#35821;&#35328;&#20351;&#29992;&#30340;&#27169;&#24335;&#12290;&#23427;&#23558;&#19981;&#29087;&#24713;&#30340;&#21333;&#35789;&#19982;&#19981;&#21516;&#30340;&#21547;&#20041;&#36827;&#34892;&#20851;&#32852;&#65292;&#26681;&#25454;&#21333;&#35789;&#24418;&#24335;&#32487;&#32493;&#35775;&#38382;&#26368;&#36817;&#36935;&#21040;&#30340;&#27495;&#20041;&#35789;&#27719;&#30340;&#21547;&#20041;&#65292;&#37325;&#29992;&#26368;&#36817;&#30340;&#35821;&#21477;&#32467;&#26500;&#65292;&#37325;&#26032;&#35299;&#37322;&#21487;&#33021;&#34987;&#22122;&#22768;&#24178;&#25200;&#30340;&#19981;&#21512;&#29702;&#35821;&#21477;&#65292;&#24573;&#30053;&#38169;&#35823;&#65292;&#36827;&#34892;&#21512;&#29702;&#25512;&#26029;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#39034;&#24207;&#21644;&#25509;&#36817;&#31243;&#24230;&#23558;&#22240;&#26524;&#20851;&#31995;&#19982;&#19981;&#21516;&#30340;&#35805;&#35821;&#23454;&#20307;&#30456;&#20851;&#32852;&#65292;&#24182;&#23454;&#26102;&#26356;&#27491;&#19968;&#33268;&#24615;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#22312;&#20004;&#20010;&#23454;&#39564;&#20013;&#65292;ChatGPT&#26174;&#31034;&#20986;&#19982;&#20154;&#31867;&#34920;&#29616;&#30340;&#20559;&#24046;&#65292;&#36825;&#34920;&#26126;&#20154;&#31867;&#21644;&#26426;&#22120;&#35821;&#35328;&#22788;&#29702;&#20043;&#38388;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) and LLM-driven chatbots such as ChatGPT have shown remarkable capacities in comprehending and producing language. However, their internal workings remain a black box in cognitive terms, and it is unclear whether LLMs and chatbots can develop humanlike characteristics in language use. Cognitive scientists have devised many experiments that probe, and have made great progress in explaining, how people process language. We subjected ChatGPT to 12 of these experiments, pre-registered and with 1,000 runs per experiment. In 10 of them, ChatGPT replicated the human pattern of language use. It associated unfamiliar words with different meanings depending on their forms, continued to access recently encountered meanings of ambiguous words, reused recent sentence structures, reinterpreted implausible sentences that were likely to have been corrupted by noise, glossed over errors, drew reasonable inferences, associated causality with different discourse entities accor
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31639;&#27861;&#29983;&#25104; LTL &#20844;&#24335;&#65292;&#36716;&#25442;&#25104;&#32467;&#26500;&#21270;&#33521;&#35821;&#26469;&#32508;&#21512;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#30340; LTL &#35268;&#26684;&#32763;&#35793;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.08006</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#26684;&#33258;&#28982;&#35821;&#35328;&#21040;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#32763;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Learning of Natural Language to Linear Temporal Logic Translators for Robot Task Specification. (arXiv:2303.08006v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31639;&#27861;&#29983;&#25104; LTL &#20844;&#24335;&#65292;&#36716;&#25442;&#25104;&#32467;&#26500;&#21270;&#33521;&#35821;&#26469;&#32508;&#21512;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#30340; LTL &#35268;&#26684;&#32763;&#35793;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26381;&#21153;&#20110;&#26356;&#24191;&#27867;&#30340;&#21463;&#20247;&#65292;&#36171;&#20104;&#20854;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#24182;&#29992;&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#65288;LTL&#65289;&#31561;&#24418;&#24335;&#35821;&#35328;&#23450;&#20041;&#20855;&#20307;&#20219;&#21153;&#35268;&#26684;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#32763;&#35793;&#25104; LTL &#35268;&#26684;&#65292;&#32780;&#19988;&#21482;&#38656;&#35201;&#38750;&#24120;&#26377;&#38480;&#30340;&#21463;&#35797;&#32773;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#12290;&#19982;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;LTL&#32763;&#35793;&#22120;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#32780;&#26159;&#36890;&#36807;&#31639;&#27861;&#29983;&#25104;LTL&#20844;&#24335;&#65292;&#36716;&#25442;&#25104;&#32467;&#26500;&#21270;&#33521;&#35821;&#65292;&#28982;&#21518;&#21033;&#29992;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25913;&#20889;&#33021;&#21147;&#26469;&#32508;&#21512;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#33258;&#28982;&#35821;&#35328;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
To make robots accessible to a broad audience, it is critical to endow them with the ability to take universal modes of communication, like commands given in natural language, and extract a concrete desired task specification, defined using a formal language like linear temporal logic (LTL). In this paper, we present a learning-based approach for translating from natural language commands to LTL specifications with very limited human-labeled training data. This is in stark contrast to existing natural-language to LTL translators, which require large human-labeled datasets, often in the form of labeled pairs of LTL formulas and natural language commands, to train the translator. To reduce reliance on human data, our approach generates a large synthetic training dataset through algorithmic generation of LTL formulas, conversion to structured English, and then exploiting the paraphrasing capabilities of modern large language models (LLMs) to synthesize a diverse corpus of natural language
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;ChatGPT&#27169;&#22411;&#30340;&#38382;&#31572;&#31995;&#32479;&#22312;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#20010;&#20998;&#31867;&#26694;&#26550;&#23545;&#28508;&#22312;&#30340;&#38382;&#39064;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#40657;&#30418;&#27979;&#35797;&#35268;&#33539;CheckList&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07992</link><description>&lt;p&gt;
&#35780;&#20272; ChatGPT &#20316;&#20026;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Evaluation of ChatGPT as a Question Answering System for Answering Complex Questions. (arXiv:2303.07992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;ChatGPT&#27169;&#22411;&#30340;&#38382;&#31572;&#31995;&#32479;&#22312;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#19968;&#20010;&#20998;&#31867;&#26694;&#26550;&#23545;&#28508;&#22312;&#30340;&#38382;&#39064;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#65292;&#36890;&#36807;&#40657;&#30418;&#27979;&#35797;&#35268;&#33539;CheckList&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT &#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#23616;&#38480;&#24615;&#20173;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#12290;&#30001;&#20110; ChatGPT &#35206;&#30422;&#32500;&#22522;&#30334;&#31185;&#31561;&#36164;&#28304;&#24182;&#25903;&#25345;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#22240;&#27492;&#23427;&#24341;&#36215;&#20102;&#20316;&#20026;&#20256;&#32479;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#27169;&#22411;&#26367;&#20195;&#21697;&#30340;&#20851;&#27880;&#12290;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#26159; KBQA &#30340;&#19968;&#39033;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#20840;&#38754;&#27979;&#35797;&#20102;&#27169;&#22411;&#22312;&#35821;&#20041;&#35299;&#26512;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35780;&#20272; ChatGPT &#20316;&#20026;&#19968;&#20010;&#20351;&#29992;&#33258;&#24049;&#30693;&#35782;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#30340;&#38382;&#31572;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#20854;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#23545;&#22797;&#26434;&#38382;&#39064;&#30340;&#28508;&#22312;&#29305;&#24449;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#26631;&#31614;&#25551;&#36848;&#27599;&#20010;&#27979;&#35797;&#38382;&#39064;&#65292;&#20197;&#35782;&#21035;&#32452;&#21512;&#25512;&#29702;&#12290;&#26681;&#25454; Ribeir &#25552;&#20986;&#30340; CheckList &#30340;&#40657;&#30418;&#27979;&#35797;&#35268;&#33539;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a powerful large language model (LLM) that has made remarkable progress in natural language understanding. Nevertheless, the performance and limitations of the model still need to be extensively evaluated. As ChatGPT covers resources such as Wikipedia and supports natural language question answering, it has garnered attention as a potential replacement for traditional knowledge based question answering (KBQA) models. Complex question answering is a challenge task of KBQA, which comprehensively tests the ability of models in semantic parsing and reasoning. To assess the performance of ChatGPT as a question answering system (QAS) using its own knowledge, we present a framework that evaluates its ability to answer complex questions. Our approach involves categorizing the potential features of complex questions and describing each test question with multiple labels to identify combinatorial reasoning. Following the black-box testing specifications of CheckList proposed by Ribeir
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#29702;&#30001;&#25552;&#21462;&#30340;&#25991;&#26723;&#20998;&#31867;&#32972;&#26223;&#19979;&#65292;&#38271;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;Longformer&#39537;&#21160;&#30340;&#22522;&#32447;&#26126;&#26174;&#26356;&#22909;&#30340;RoBERTa&#21477;&#23376;&#32423;&#32452;&#21512;&#36719;&#27880;&#24847;&#21147;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2303.07991</link><description>&lt;p&gt;
&#22312;&#38271;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#26080;&#30417;&#30563;&#22320;&#25552;&#21462;&#29702;&#30001;
&lt;/p&gt;
&lt;p&gt;
Finding the Needle in a Haystack: Unsupervised Rationale Extraction from Long Text Classifiers. (arXiv:2303.07991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07991
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#29702;&#30001;&#25552;&#21462;&#30340;&#25991;&#26723;&#20998;&#31867;&#32972;&#26223;&#19979;&#65292;&#38271;&#25991;&#26412;&#20998;&#31867;&#22120;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27604;Longformer&#39537;&#21160;&#30340;&#22522;&#32447;&#26126;&#26174;&#26356;&#22909;&#30340;RoBERTa&#21477;&#23376;&#32423;&#32452;&#21512;&#36719;&#27880;&#24847;&#21147;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24207;&#21015;&#36716;&#25442;&#22120;&#26088;&#22312;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#36739;&#38271;&#25991;&#26412;&#30340;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#23427;&#20204;&#22312;&#19979;&#28216;&#25991;&#26723;&#32423;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#38271;&#26684;&#24335;&#27169;&#22411;&#20013;&#20196;&#29260;&#32423;&#21035;&#39044;&#27979;&#30340;&#36136;&#37327;&#23578;&#19981;&#22826;&#20102;&#35299;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26550;&#26500;&#22312;&#26080;&#30417;&#30563;&#29702;&#30001;&#25552;&#21462;&#30340;&#25991;&#26723;&#20998;&#31867;&#32972;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#24403;&#19982;Longformer&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#26631;&#20934;&#36719;&#37327;&#27880;&#24847;&#26041;&#27861;&#34920;&#29616;&#26174;&#30528;&#36739;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32452;&#21512;&#36719;&#37327;&#20851;&#27880;&#26550;&#26500;&#65292;&#23427;&#23558;RoBERTa&#24212;&#29992;&#20110;&#21477;&#23376;&#65292;&#20197;&#22312;&#20196;&#29260;&#32423;&#21035;&#25552;&#21462;&#21487;&#20449;&#29702;&#30001;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#22312;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;Longformer&#34893;&#29983;&#22522;&#32447;&#65292;&#24182;&#19988;&#23637;&#29616;&#20986;&#26126;&#26174;&#26356;&#20302;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-sequence transformers are designed to improve the representation of longer texts by language models and their performance on downstream document-level tasks. However, not much is understood about the quality of token-level predictions in long-form models. We investigate the performance of such architectures in the context of document classification with unsupervised rationale extraction. We find standard soft attention methods to perform significantly worse when combined with the Longformer language model. We propose a compositional soft attention architecture that applies RoBERTa sentence-wise to extract plausible rationales at the token-level. We find this method to significantly outperform Longformer-driven baselines on sentiment classification datasets, while also exhibiting significantly lower runtimes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25512;&#23548;&#20102;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#65292;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#20855;&#26377;&#36275;&#22815;&#30340;&#32452;&#25104;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#19968;&#33324;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#33719;&#24471;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#20026;&#39564;&#35777;&#29702;&#35770;&#39044;&#27979;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21463;&#25511;&#21046;&#30340;&#35774;&#32622;&#26469;&#35825;&#23548;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#35777;&#26126;&#20102;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#21487;&#20197;&#20026;&#19968;&#31995;&#21015;&#20219;&#21153;&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.07971</link><description>&lt;p&gt;
&#19968;&#31181;&#20851;&#20110;&#38544;&#21547;&#32467;&#26500;&#24402;&#32435;&#30340;&#19978;&#19979;&#25991;&#20013;&#28044;&#29616;&#23398;&#20064;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Theory of Emergent In-Context Learning as Implicit Structure Induction. (arXiv:2303.07971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25512;&#23548;&#20102;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#65292;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#20855;&#26377;&#36275;&#22815;&#30340;&#32452;&#25104;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#19968;&#33324;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#33719;&#24471;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#20026;&#39564;&#35777;&#29702;&#35770;&#39044;&#27979;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21463;&#25511;&#21046;&#30340;&#35774;&#32622;&#26469;&#35825;&#23548;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#35777;&#26126;&#20102;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#21487;&#20197;&#20026;&#19968;&#31995;&#21015;&#20219;&#21153;&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#24341;&#21457;&#20102;&#28044;&#29616;&#24335;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#21363;&#22522;&#20110;&#31034;&#20363;&#28436;&#31034;&#36827;&#34892;&#23398;&#20064;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#23545;&#36825;&#31181;&#29616;&#35937;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20381;&#36182;&#20110;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#32452;&#21512;&#24615;&#25805;&#20316;&#30340;&#37325;&#26032;&#32452;&#21512;&#12290;&#22312;&#22522;&#20110;&#35821;&#35328;&#23398;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#65292;&#23637;&#31034;&#20102;&#24403;&#39044;&#35757;&#32451;&#20998;&#24067;&#20855;&#26377;&#36275;&#22815;&#30340;&#32452;&#25104;&#32467;&#26500;&#26102;&#65292;&#22914;&#20309;&#20174;&#19968;&#33324;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#33719;&#24471;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#31532;&#20108;&#20010;&#30028;&#38480;&#20026;&#25552;&#31034;LLM&#36755;&#20986;&#26397;&#30528;&#31572;&#26696;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#23454;&#35777;&#25104;&#21151;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290;&#20026;&#20102;&#39564;&#35777;&#29702;&#35770;&#39044;&#27979;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21463;&#25511;&#21046;&#30340;&#35774;&#32622;&#26469;&#35825;&#23548;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#19982;&#20197;&#24448;&#26041;&#27861;&#19981;&#21516;&#65292;&#23427;&#32771;&#34385;&#21040;&#35821;&#35328;&#30340;&#32452;&#21512;&#26412;&#36136;&#12290;&#32463;&#36807;&#35757;&#32451;&#30340;Transformer&#21487;&#20197;&#20026;&#19968;&#31995;&#21015;&#20219;&#21153;&#25191;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#36825;&#19982;&#22312;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340; Transformer &#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling large language models (LLMs) leads to an emergent capacity to learn in-context from example demonstrations. Despite progress, theoretical understanding of this phenomenon remains limited. We argue that in-context learning relies on recombination of compositional operations found in natural language data. We derive an information-theoretic bound showing how in-context learning abilities arise from generic next-token prediction when the pretraining distribution has sufficient amounts of compositional structure, under linguistically motivated assumptions. A second bound provides a theoretical justification for the empirical success of prompting LLMs to output intermediate steps towards an answer. To validate theoretical predictions, we introduce a controlled setup for inducing in-context learning; unlike previous approaches, it accounts for the compositional nature of language. Trained transformers can perform in-context learning for a range of tasks, in a manner consistent with t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22235;&#31181;&#19981;&#21516;&#27861;&#35821;&#21475;&#38899;&#26469;&#21019;&#24314;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;ASR&#27169;&#22411;&#23545;&#21475;&#38899;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#24182;&#25104;&#21151;&#23558;&#38169;&#35823;&#29575;&#22312;&#38750;&#27954;&#21644;&#27604;&#21033;&#26102;&#21475;&#38899;&#19978;&#38477;&#20302;&#39640;&#36798;25%&#12290;</title><link>http://arxiv.org/abs/2303.07924</link><description>&lt;p&gt;
&#22810;&#39046;&#22495;&#35757;&#32451;&#22312;&#25552;&#39640;&#21475;&#38899;&#35821;&#38899;&#35782;&#21035;&#26041;&#38754;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Improving Accented Speech Recognition with Multi-Domain Training. (arXiv:2303.07924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22235;&#31181;&#19981;&#21516;&#27861;&#35821;&#21475;&#38899;&#26469;&#21019;&#24314;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;ASR&#27169;&#22411;&#23545;&#21475;&#38899;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#24182;&#25104;&#21151;&#23558;&#38169;&#35823;&#29575;&#22312;&#38750;&#27954;&#21644;&#27604;&#21033;&#26102;&#21475;&#38899;&#19978;&#38477;&#20302;&#39640;&#36798;25%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#22686;&#38271;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#24191;&#27867;&#25968;&#25454;&#38598;&#19978;&#24050;&#32463;&#23454;&#29616;&#20102;&#25509;&#36817;&#20154;&#31867;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#19981;&#20855;&#22791;&#23545;&#21475;&#38899;&#21464;&#21270;&#31561;&#39046;&#22495;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20195;&#34920;&#22235;&#31181;&#19981;&#21516;&#27861;&#35821;&#21475;&#38899;&#30340;&#35821;&#38899;&#38899;&#39057;&#26469;&#21019;&#24314;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;ASR&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#38598;&#20013;&#21253;&#21547;&#19981;&#21516;&#30340;&#21475;&#38899;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#25913;&#36827;&#12290;&#25968;&#23383;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#19982;&#21333;&#39046;&#22495;&#35757;&#32451;&#30456;&#27604;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#38169;&#35823;&#29575;&#22312;&#38750;&#27954;&#21644;&#27604;&#21033;&#26102;&#21475;&#38899;&#19978;&#38477;&#20302;&#39640;&#36798;25%&#65288;&#30456;&#23545;&#20540;&#65289;&#65292;&#21516;&#26102;&#22312;&#26631;&#20934;&#27861;&#35821;&#19978;&#20445;&#25345;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Thanks to the rise of self-supervised learning, automatic speech recognition (ASR) systems now achieve near-human performance on a wide variety of datasets. However, they still lack generalization capability and are not robust to domain shifts like accent variations. In this work, we use speech audio representing four different French accents to create fine-tuning datasets that improve the robustness of pre-trained ASR models. By incorporating various accents in the training set, we obtain both in-domain and out-of-domain improvements. Our numerical experiments show that we can reduce error rates by up to 25% (relative) on African and Belgian accents compared to single-domain training while keeping a good performance on standard French.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#22522;&#20110;PAC&#30340;&#19978;&#19979;&#25991;&#21487;&#23398;&#20064;&#24615;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#20854;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#35774;&#32622;&#25552;&#20379;&#20102;&#39318;&#20010;&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24230;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.07895</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#21487;&#23398;&#20064;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Learnability of In-Context Learning. (arXiv:2303.07895v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#22522;&#20110;PAC&#30340;&#19978;&#19979;&#25991;&#21487;&#23398;&#20064;&#24615;&#26694;&#26550;&#65292;&#24182;&#20351;&#29992;&#20854;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#35774;&#32622;&#25552;&#20379;&#20102;&#39318;&#20010;&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24230;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21442;&#25968;&#25193;&#23637;&#21040;&#25968;&#21313;&#20159;&#20010;&#30340;&#24773;&#20917;&#19979;&#20986;&#29616;&#20102;&#20196;&#20154;&#24778;&#35766;&#19988;&#37325;&#35201;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#29616;&#35937;&#12290;&#22312;&#19981;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#65292;&#21482;&#38656;&#23558;&#36825;&#20123;&#20219;&#21153;&#30340;&#35757;&#32451;&#26679;&#20363;&#19982;&#20854;&#36755;&#20837;&#36830;&#25509;&#21363;&#21487;&#23558;&#20854;&#35843;&#25972;&#20026;&#25191;&#34892;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;&#34429;&#28982;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20855;&#26377;&#30772;&#22351;&#24615;&#65292;&#20294;&#36825;&#31181;&#26032;&#20852;&#30340;&#23398;&#20064;&#33539;&#24335;&#20174;&#29702;&#35770;&#35282;&#24230;&#23578;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39318;&#27425;&#22522;&#20110;PAC&#30340;&#19978;&#19979;&#25991;&#21487;&#23398;&#20064;&#24615;&#26694;&#26550;&#65292;&#24182;&#21033;&#29992;&#23427;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#35774;&#32622;&#25552;&#20379;&#20102;&#39318;&#20010;&#26377;&#38480;&#26679;&#26412;&#22797;&#26434;&#24230;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#20010;&#21021;&#22987;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#23427;&#23558;&#19968;&#20010;&#20989;&#25968;&#25311;&#21512;&#21040;&#39044;&#35757;&#32451;&#20998;&#24067;&#20013;&#65292;&#28982;&#21518;&#26159;&#31532;&#20108;&#20010;&#19978;&#19979;&#25991;&#23398;&#20064;&#38454;&#27573;&#65292;&#23427;&#20445;&#25345;&#35813;&#20989;&#25968;&#19981;&#21464;&#65292;&#24182;&#23558;&#19979;&#28216;&#20219;&#21153;&#30340;&#35757;&#32451;&#26679;&#20363;&#36830;&#25509;&#21040;&#20854;&#36755;&#20837;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#26469;
&lt;/p&gt;
&lt;p&gt;
In-context learning is a surprising and important phenomenon that emerged when modern language models were scaled to billions of learned parameters. Without modifying a large language model's weights, it can be tuned to perform various downstream natural language tasks simply by including concatenated training examples of these tasks in its input. Though disruptive for many practical applications of large language models, this emergent learning paradigm is not well understood from a theoretical perspective. In this paper, we propose a first-of-its-kind PAC based framework for in-context learnability, and use it to provide the first finite sample complexity results for the in-context learning setup. Our framework includes an initial pretraining phase, which fits a function to the pretraining distribution, and then a second in-context learning phase, which keeps this function constant and concatenates training examples of the downstream task in its input. We use our framework in order to
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.07865</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Geolocation Predicting of Tweets Using BERT-Based Models. (arXiv:2303.07865v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07865
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#25512;&#25991;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#20840;&#29699;&#21644;&#32654;&#22269;&#19978;&#30340;&#20013;&#20301;&#35823;&#24046;&#20998;&#21035;&#23567;&#20110;30&#20844;&#37324;&#21644;15&#20844;&#37324;&#30340;&#23450;&#20301;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#25512;&#25991;/&#29992;&#25143;&#22320;&#29702;&#20301;&#32622;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#22788;&#29702;&#25991;&#26412;&#22823;&#25968;&#25454;&#22320;&#29702;&#26631;&#35760;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#20272;&#35745;&#22352;&#26631;&#23545;&#65288;&#32463;&#24230;&#65292;&#32428;&#24230;&#65289;&#21644;&#20108;&#32500;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#30340;&#33539;&#22260;&#24050;&#32463;&#22312;Twitter&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#36827;&#34892;&#35843;&#25972;&#12290;&#24615;&#33021;&#25351;&#26631;&#34920;&#26126;&#65292;&#23545;&#20110;&#22312;&#25512;&#25991;&#20869;&#23481;&#21644;&#20803;&#25968;&#25454;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#27169;&#22411;&#65292;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;30&#20844;&#37324;&#65292;&#32654;&#22269;&#33539;&#22260;&#20869;&#30340;&#20013;&#20301;&#35823;&#24046;&#23567;&#20110;15&#20844;&#37324;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research is aimed to solve the tweet/user geolocation prediction task and provide a flexible methodology for the geotagging of textual big data. The suggested approach implements neural networks for natural language processing (NLP) to estimate the location as coordinate pairs (longitude, latitude) and two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder Representations from Transformers (BERT) as base models. Performance metrics show a median error of fewer than 30 km on a worldwide-level, and fewer than 15 km on the US-level datasets for the models trained and evaluated on text features of tweets' content and metadata context.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#27169;&#22411; X-ReCoSa&#65292;&#23454;&#29616;&#20102;&#22810;&#23618;&#27425;&#19978;&#19979;&#25991;&#32858;&#21512;&#65292;&#23558;&#19978;&#19979;&#25991;&#34920;&#31034;&#21644;&#21477;&#23376;&#34920;&#31034;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20998;&#23618;&#23545;&#35805;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.07833</link><description>&lt;p&gt;
X-ReCoSa: &#22810;&#23618;&#27425;&#19978;&#19979;&#25991;&#32858;&#21512;&#30340;&#22810;&#36718;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
X-ReCoSa: Multi-Scale Context Aggregation For Multi-Turn Dialogue Generation. (arXiv:2303.07833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07833
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#27169;&#22411; X-ReCoSa&#65292;&#23454;&#29616;&#20102;&#22810;&#23618;&#27425;&#19978;&#19979;&#25991;&#32858;&#21512;&#65292;&#23558;&#19978;&#19979;&#25991;&#34920;&#31034;&#21644;&#21477;&#23376;&#34920;&#31034;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#20998;&#23618;&#23545;&#35805;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#36718;&#23545;&#35805;&#29983;&#25104;&#20013;&#65292;&#22238;&#22797;&#19981;&#20165;&#19982;&#19978;&#19979;&#25991;&#30340;&#20027;&#39064;&#21644;&#32972;&#26223;&#26377;&#20851;&#65292;&#36824;&#19982;&#19978;&#19979;&#25991;&#20013;&#21477;&#23376;&#20013;&#30340;&#21333;&#35789;&#21644;&#30701;&#35821;&#26377;&#20851;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#20998;&#23618;&#23545;&#35805;&#27169;&#22411;&#20165;&#20381;&#38752;&#35805;&#35821;&#32423;&#21035;&#32534;&#30721;&#22120;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#24573;&#30053;&#20102;&#21333;&#35789;&#32423;&#21035;&#32534;&#30721;&#22120;&#30340;&#21477;&#23376;&#34920;&#31034;&#12290;&#36825;&#24517;&#28982;&#20250;&#22312;&#35299;&#30721;&#21644;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#20002;&#22833;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#35805;&#27169;&#22411;X-ReCoSa&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#32858;&#21512;&#20102;&#22810;&#23618;&#27425;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#36866;&#29992;&#20110;&#20998;&#23618;&#23545;&#35805;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#29983;&#25104;&#35299;&#30721;&#22120;&#20998;&#20026;&#19978;&#37096;&#21644;&#19979;&#37096;&#65292;&#21363;&#24847;&#22270;&#37096;&#20998;&#21644;&#29983;&#25104;&#37096;&#20998;&#12290;&#39318;&#20808;&#65292;&#24847;&#22270;&#37096;&#20998;&#23558;&#19978;&#19979;&#25991;&#34920;&#31034;&#20316;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#22238;&#22797;&#30340;&#24847;&#22270;&#12290;&#28982;&#21518;&#65292;&#29983;&#25104;&#37096;&#20998;&#26681;&#25454;&#21477;&#23376;&#34920;&#31034;&#29983;&#25104;&#21333;&#35789;&#12290;&#22240;&#27492;&#65292;&#20998;&#23618;&#20449;&#24687;&#24050;&#34987;&#34701;&#21512;&#21040;&#32467;&#26524;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In multi-turn dialogue generation, responses are not only related to the topic and background of the context but also related to words and phrases in the sentences of the context. However, currently widely used hierarchical dialog models solely rely on context representations from the utterance-level encoder, ignoring the sentence representations output by the word-level encoder. This inevitably results in a loss of information while decoding and generating. In this paper, we propose a new dialog model X-ReCoSa to tackle this problem which aggregates multi-scale context information for hierarchical dialog models. Specifically, we divide the generation decoder into upper and lower parts, namely the intention part and the generation part. Firstly, the intention part takes context representations as input to generate the intention of the response. Then the generation part generates words depending on sentence representations. Therefore, the hierarchical information has been fused into res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#35789;&#24341;&#23548;&#30340;&#39044;&#22788;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#36827;&#34892;&#24555;&#36895;&#39640;&#25928;&#30340;&#20851;&#38190;&#35789;&#21305;&#37197;&#65292;&#20197;&#25490;&#38500;&#22823;&#37327;&#30340;&#26080;&#20851;&#26679;&#26412;&#12290;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#26696;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#20851;&#38190;&#35789;&#39044;&#27979;&#65292;&#24182;&#22312;&#26816;&#32034;&#20013;&#24341;&#20837;&#20498;&#25490;&#32034;&#24341;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#20851;&#38190;&#35789;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2303.07740</link><description>&lt;p&gt;
&#36890;&#36807;&#20851;&#38190;&#35789;&#24341;&#23548;&#30340;&#39044;&#22788;&#29702;&#23454;&#29616;&#39640;&#25928;&#30340;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient Image-Text Retrieval via Keyword-Guided Pre-Screening. (arXiv:2303.07740v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#38190;&#35789;&#24341;&#23548;&#30340;&#39044;&#22788;&#29702;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#36827;&#34892;&#24555;&#36895;&#39640;&#25928;&#30340;&#20851;&#38190;&#35789;&#21305;&#37197;&#65292;&#20197;&#25490;&#38500;&#22823;&#37327;&#30340;&#26080;&#20851;&#26679;&#26412;&#12290;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#26696;&#20197;&#23454;&#29616;&#39640;&#24615;&#33021;&#20851;&#38190;&#35789;&#39044;&#27979;&#65292;&#24182;&#22312;&#26816;&#32034;&#20013;&#24341;&#20837;&#20498;&#25490;&#32034;&#24341;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#20851;&#38190;&#35789;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#30446;&#21069;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#34028;&#21187;&#21457;&#23637;&#65292;&#20294;&#23384;&#22312;$N$&#30456;&#20851;&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#38382;&#39064;&#65292;&#24433;&#21709;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#22522;&#20110;&#20851;&#38190;&#35789;&#24341;&#23548;&#30340;&#39044;&#22788;&#29702;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#36716;&#25442;&#20026;&#20851;&#38190;&#35789;&#65292;&#24182;&#22312;&#27169;&#24577;&#20043;&#38388;&#25191;&#34892;&#20851;&#38190;&#35789;&#21305;&#37197;&#20197;&#25490;&#38500;&#22823;&#37327;&#26080;&#20851;&#30340;&#26679;&#26412;&#65292;&#28982;&#21518;&#20877;&#36755;&#20837;&#21040;&#26816;&#32034;&#32593;&#32476;&#20013;&#12290;&#23545;&#20110;&#20851;&#38190;&#35789;&#39044;&#27979;&#65292;&#25105;&#20204;&#23558;&#20854;&#36716;&#25442;&#20026;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#38468;&#21152;&#21040;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#32593;&#32476;&#20013;&#20197;&#23454;&#29616;&#36731;&#37327;&#32423;&#21644;&#39640;&#24615;&#33021;&#30340;&#20851;&#38190;&#35789;&#39044;&#27979;&#12290;&#23545;&#20110;&#20851;&#38190;&#35789;&#21305;&#37197;&#65292;&#25105;&#20204;&#22312;&#25628;&#32034;&#24341;&#25806;&#20013;&#24341;&#20837;&#20498;&#25490;&#32034;&#24341;&#65292;&#23454;&#29616;&#20102;&#23545;&#39044;&#26816;&#32034;&#20013;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#21452;&#36194;&#30340;&#24773;&#20917;&#12290;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Under the flourishing development in performance, current image-text retrieval methods suffer from $N$-related time complexity, which hinders their application in practice. Targeting at efficiency improvement, this paper presents a simple and effective keyword-guided pre-screening framework for the image-text retrieval. Specifically, we convert the image and text data into the keywords and perform the keyword matching across modalities to exclude a large number of irrelevant gallery samples prior to the retrieval network. For the keyword prediction, we transfer it into a multi-label classification problem and propose a multi-task learning scheme by appending the multi-label classifiers to the image-text retrieval network to achieve a lightweight and high-performance keyword prediction. For the keyword matching, we introduce the inverted index in the search engine and create a win-win situation on both time and space complexities for the pre-screening. Extensive experiments on two widel
&lt;/p&gt;</description></item><item><title>Reinforcer&#27169;&#22411;&#20351;&#29992;&#37051;&#36817;&#23383;&#31526;&#20851;&#31995;&#24378;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20013;&#25991;G2P&#20013;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#21477;&#23376;&#36807;&#20110;&#26222;&#36941;&#21644;&#35789;&#36793;&#30028;&#20998;&#21106;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07726</link><description>&lt;p&gt;
&#20165;&#38656;&#22909;&#30340;&#37051;&#23621;&#65306;&#22522;&#20110;&#37051;&#36817;&#23383;&#31526;&#20851;&#31995;&#24378;&#21270;&#30340;&#20013;&#25991;&#23383;&#32032;&#21040;&#38899;&#32032;&#36716;&#25442;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Good Neighbors Are All You Need for Chinese Grapheme-to-Phoneme Conversion. (arXiv:2303.07726v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07726
&lt;/p&gt;
&lt;p&gt;
Reinforcer&#27169;&#22411;&#20351;&#29992;&#37051;&#36817;&#23383;&#31526;&#20851;&#31995;&#24378;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#20013;&#25991;G2P&#20013;&#35821;&#35328;&#27169;&#22411;&#32534;&#30721;&#21477;&#23376;&#36807;&#20110;&#26222;&#36941;&#21644;&#35789;&#36793;&#30028;&#20998;&#21106;&#19981;&#19968;&#33268;&#24615;&#30340;&#38382;&#39064;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20013;&#25991;&#23383;&#32032;&#21040;&#38899;&#32032;&#65288;G2P&#65289;&#31995;&#32479;&#37319;&#29992;&#19977;&#38454;&#27573;&#26550;&#26500;&#65292;&#39318;&#20808;&#23558;&#36755;&#20837;&#24207;&#21015;&#36716;&#21270;&#20026;&#23383;&#31526;&#23884;&#20837;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#33719;&#21462;&#35821;&#35328;&#20449;&#24687;&#65292;&#28982;&#21518;&#22522;&#20110;&#25972;&#20010;&#36755;&#20837;&#24207;&#21015;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#39044;&#27979;&#38899;&#32032;&#12290;&#28982;&#32780;&#65292;&#20165;&#20973;&#35821;&#35328;&#30693;&#35782;&#24448;&#24448;&#26159;&#19981;&#20805;&#20998;&#30340;&#12290;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#32534;&#30721;&#21477;&#23376;&#30340;&#36807;&#20110;&#26222;&#36941;&#30340;&#32467;&#26500;&#65292;&#24182;&#26410;&#28085;&#30422;&#20351;&#29992;&#35821;&#38899;&#30693;&#35782;&#25152;&#38656;&#30340;&#29305;&#23450;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#38656;&#35201;&#19968;&#20010;&#25163;&#24037;&#21518;&#22788;&#29702;&#31995;&#32479;&#26469;&#35299;&#20915;&#19982;&#23383;&#31526;&#38899;&#35843;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#31995;&#32479;&#22312;&#35789;&#36793;&#30028;&#20998;&#21106;&#19978;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;G2P&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Reinforcer&#27169;&#22411;&#65292;&#36890;&#36807;&#24378;&#35843;&#37051;&#36817;&#23383;&#31526;&#20043;&#38388;&#30340;&#35821;&#38899;&#20449;&#24687;&#26469;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#24378;&#22823;&#30340;&#24341;&#23548;&#20559;&#24046;&#65292;&#20197;&#24110;&#21161;&#28040;&#38500;&#21457;&#38899;&#30340;&#27495;&#20041;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#20013;&#25991;G2P&#31995;&#32479;&#30456;&#27604;&#65292;Reinforcer&#27169;&#22411;&#22312;&#21508;&#31181;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most Chinese Grapheme-to-Phoneme (G2P) systems employ a three-stage framework that first transforms input sequences into character embeddings, obtains linguistic information using language models, and then predicts the phonemes based on global context about the entire input sequence. However, linguistic knowledge alone is often inadequate. Language models frequently encode overly general structures of a sentence and fail to cover specific cases needed to use phonetic knowledge. Also, a handcrafted post-processing system is needed to address the problems relevant to the tone of the characters. However, the system exhibits inconsistency in the segmentation of word boundaries which consequently degrades the performance of the G2P system. To address these issues, we propose the Reinforcer that provides strong inductive bias for language models by emphasizing the phonological information between neighboring characters to help disambiguate pronunciations. Experimental results show that the R
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24378;&#24230;&#21463;&#25511;&#30340;&#21322;&#30417;&#30563;&#39118;&#26684;&#25552;&#21462;&#22120;&#20197;&#21450;&#20998;&#23618;&#38901;&#24459;&#39044;&#27979;&#22120;&#26469;&#25913;&#21892;&#38899;&#39057;&#36328;&#21457;&#35328;&#20154;&#39118;&#26684;&#36716;&#31227;&#20013;&#30340;&#38901;&#24459;&#38382;&#39064;&#65292;&#35299;&#38500;&#20102;&#19968;&#23545;&#22810;&#26144;&#23556;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.07711</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#39118;&#26684;&#25552;&#21462;&#22120;&#21644;&#20998;&#23618;&#24314;&#27169;&#25913;&#36827;&#38899;&#39057;&#36328;&#21457;&#35328;&#20154;&#39118;&#26684;&#36716;&#31227;&#30340;&#38901;&#24459;
&lt;/p&gt;
&lt;p&gt;
Improving Prosody for Cross-Speaker Style Transfer by Semi-Supervised Style Extractor and Hierarchical Modeling in Speech Synthesis. (arXiv:2303.07711v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07711
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#24378;&#24230;&#21463;&#25511;&#30340;&#21322;&#30417;&#30563;&#39118;&#26684;&#25552;&#21462;&#22120;&#20197;&#21450;&#20998;&#23618;&#38901;&#24459;&#39044;&#27979;&#22120;&#26469;&#25913;&#21892;&#38899;&#39057;&#36328;&#21457;&#35328;&#20154;&#39118;&#26684;&#36716;&#31227;&#20013;&#30340;&#38901;&#24459;&#38382;&#39064;&#65292;&#35299;&#38500;&#20102;&#19968;&#23545;&#22810;&#26144;&#23556;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#21512;&#25104;&#20013;&#65292;&#36328;&#21457;&#35328;&#20154;&#39118;&#26684;&#36716;&#31227;&#26088;&#22312;&#23558;&#28304;&#21457;&#35328;&#20154;&#30340;&#39118;&#26684;&#36716;&#31227;&#21040;&#30446;&#26631;&#21457;&#35328;&#20154;&#38899;&#33394;&#30340;&#21512;&#25104;&#35821;&#38899;&#20013;&#12290;&#22312;&#22823;&#22810;&#25968;&#26041;&#27861;&#20013;&#65292;&#21512;&#25104;&#30340;&#32454;&#31890;&#24230;&#38901;&#24459;&#29305;&#24449;&#36890;&#24120;&#34920;&#31034;&#28304;&#21457;&#35328;&#20154;&#30340;&#24179;&#22343;&#39118;&#26684;&#65292;&#31867;&#20284;&#20110;&#19968;&#23545;&#22810;&#38382;&#39064;&#65288;&#21363;&#65292;&#22810;&#20010;&#38901;&#24459;&#21464;&#21270;&#23545;&#24212;&#20110;&#21516;&#19968;&#25991;&#26412;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#24230;&#21463;&#25511;&#30340;&#21322;&#30417;&#30563;&#39118;&#26684;&#25552;&#21462;&#22120;&#65292;&#20197;&#35299;&#24320;&#39118;&#26684;&#19982;&#20869;&#23481;&#21644;&#38899;&#33394;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#25913;&#21892;&#20840;&#23616;&#39118;&#26684;&#23884;&#20837;&#30340;&#34920;&#31034;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#21487;&#20197;&#32531;&#35299;&#38901;&#24459;&#39044;&#27979;&#20013;&#30340;&#19968;&#23545;&#22810;&#26144;&#23556;&#21644;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#38901;&#24459;&#39044;&#27979;&#22120;&#26469;&#25913;&#21892;&#38901;&#24459;&#24314;&#27169;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#26131;&#20110;&#39044;&#27979;&#30340;&#28304;&#21457;&#35328;&#20154;&#38901;&#24459;&#29305;&#24449;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#39118;&#26684;&#36716;&#31227;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35762;&#35805;&#20154;&#38388;&#24490;&#29615;&#19968;&#33268;&#24615;&#25439;&#22833;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#23398;&#20064;&#26410;&#35266;&#23519;&#21040;&#30340;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-speaker style transfer in speech synthesis aims at transferring a style from source speaker to synthesized speech of a target speaker's timbre. In most previous methods, the synthesized fine-grained prosody features often represent the source speaker's average style, similar to the one-to-many problem(i.e., multiple prosody variations correspond to the same text). In response to this problem, a strength-controlled semi-supervised style extractor is proposed to disentangle the style from content and timbre, improving the representation and interpretability of the global style embedding, which can alleviate the one-to-many mapping and data imbalance problems in prosody prediction. A hierarchical prosody predictor is proposed to improve prosody modeling. We find that better style transfer can be achieved by using the source speaker's prosody features that are easily predicted. Additionally, a speaker-transfer-wise cycle consistency loss is proposed to assist the model in learning un
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#31867;&#30340;&#21452;&#37325;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20351;&#29992;&#20381;&#23384;&#26631;&#31614;&#20026;&#27880;&#24847;&#21147;&#26426;&#21046;&#25191;&#34892;&#20219;&#21153;&#65292;&#23545;&#19977;&#20010;&#25968;&#25454;&#38598;&#22343;&#26377;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.07689</link><description>&lt;p&gt;
&#38754;&#21521;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#31867;&#30340;&#21452;&#37325;&#27880;&#24847;&#21147;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dual-Attention Model for Aspect-Level Sentiment Classification. (arXiv:2303.07689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#31867;&#30340;&#21452;&#37325;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20351;&#29992;&#20381;&#23384;&#26631;&#31614;&#20026;&#27880;&#24847;&#21147;&#26426;&#21046;&#25191;&#34892;&#20219;&#21153;&#65292;&#23545;&#19977;&#20010;&#25968;&#25454;&#38598;&#22343;&#26377;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#26041;&#38754;&#32423;&#24773;&#24863;&#20998;&#31867;&#30340;&#21452;&#37325;&#27880;&#24847;&#21147;&#27169;&#22411;(DAM)&#12290;&#23613;&#31649;&#35768;&#22810;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#29992;&#20110;&#20154;&#24037;&#35774;&#35745;&#29305;&#24449;&#12289;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#21644;&#22522;&#20110;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#37117;&#26377;&#19981;&#38169;&#30340;&#24615;&#33021;&#65292;&#20294;&#25105;&#35748;&#20026;&#23427;&#20204;&#37117;&#32570;&#23569;&#19968;&#20010;&#37325;&#35201;&#30340;&#21477;&#27861;&#20449;&#24687;&#65306;&#20381;&#23384;&#26631;&#31614;&#12290;&#22522;&#20110;&#36825;&#20010;&#24819;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20381;&#23384;&#26631;&#31614;&#20026;&#27880;&#24847;&#21147;&#26426;&#21046;&#25191;&#34892;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65306;&#31508;&#35760;&#26412;&#30005;&#33041;&#21644;&#39184;&#21381;&#25968;&#25454;&#38598;&#26469;&#33258;SemEval 2014&#65292;&#26368;&#21518;&#19968;&#20010;&#26159;Twitter&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21452;&#37325;&#27880;&#24847;&#21147;&#27169;&#22411;&#22312;&#25152;&#26377;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
I propose a novel dual-attention model(DAM) for aspect-level sentiment classification. Many methods have been proposed, such as support vector machines for artificial design features, long short-term memory networks based on attention mechanisms, and graph neural networks based on dependency parsing. While these methods all have decent performance, I think they all miss one important piece of syntactic information: dependency labels. Based on this idea, this paper proposes a model using dependency labels for the attention mechanism to do this task. We evaluate the proposed approach on three datasets: laptop and restaurant are from SemEval 2014, and the last one is a twitter dataset. Experimental results show that the dual attention model has good performance on all three datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#23545;&#40784;&#36974;&#32617;CTC&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#21160;&#24577;&#35268;&#21010;&#30340;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#21516;&#26102;&#20351;&#29992;&#21160;&#24577;&#30699;&#27491;&#26041;&#27861;&#21019;&#24314;&#26032;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#20351;&#24471;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07687</link><description>&lt;p&gt;
&#21160;&#24577;&#23545;&#40784;&#36974;&#32617;CTC: &#36890;&#36807;&#23545;&#40784;&#20132;&#21449;&#29109;&#36827;&#34892;&#25913;&#36827;&#30340;&#36974;&#32617;CTC
&lt;/p&gt;
&lt;p&gt;
Dynamic Alignment Mask CTC: Improved Mask-CTC with Aligned Cross Entropy. (arXiv:2303.07687v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#23545;&#40784;&#36974;&#32617;CTC&#31639;&#27861;&#65292;&#37319;&#29992;&#20102;&#21160;&#24577;&#35268;&#21010;&#30340;&#26041;&#27861;&#26469;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#21516;&#26102;&#20351;&#29992;&#21160;&#24577;&#30699;&#27491;&#26041;&#27861;&#21019;&#24314;&#26032;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#20351;&#24471;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30001;&#20110;&#21487;&#20197;&#21516;&#26102;&#39044;&#27979;&#25152;&#26377;&#30446;&#26631;&#20196;&#29260;&#65292;&#22240;&#27492;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#35821;&#38899;&#35782;&#21035;&#30340;&#35299;&#30721;&#25928;&#29575;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#23545;&#40784;&#36974;&#32617;CTC&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#65288;1&#65289;&#23545;&#40784;&#30340;&#20132;&#21449;&#29109;&#65288;AXE&#65289;&#65292;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#25214;&#21040;&#26368;&#23567;&#21270;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#21333;&#35843;&#23545;&#40784;&#65307;&#65288;2&#65289;&#21160;&#24577;&#30699;&#27491;&#65292;&#36890;&#36807;&#23558;&#19968;&#20123;&#36974;&#32617;&#26367;&#25442;&#20026;&#27169;&#22411;&#39044;&#27979;&#30340;&#20196;&#29260;&#65292;&#21019;&#24314;&#26032;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;AXE&#24573;&#30053;&#20102;&#39044;&#27979;&#21644;&#23454;&#38469;&#21477;&#23376;&#20043;&#38388;&#30340;&#32477;&#23545;&#20301;&#32622;&#23545;&#40784;&#65292;&#32780;&#26159;&#20851;&#27880;&#20197;&#30456;&#23545;&#39034;&#24207;&#21305;&#37197;&#30340;&#20196;&#29260;&#12290;&#21160;&#24577;&#30699;&#27491;&#26041;&#27861;&#20351;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#38750;&#36974;&#32617;&#20294;&#21487;&#33021;&#38169;&#35823;&#30340;&#20196;&#29260;&#65292;&#21363;&#20351;&#23427;&#20204;&#20855;&#26377;&#36739;&#39640;&#30340;&#32622;&#20449;&#24230;&#12290;&#25105;&#20204;&#22312;WSJ&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19981;&#20165;AXE&#25439;&#22833;&#65292;&#32780;&#19988;&#30699;&#27491;&#26041;&#27861;&#37117;&#21487;&#20197;&#25552;&#39640;&#36974;&#32617;CTC&#30340;WER&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Because of predicting all the target tokens in parallel, the non-autoregressive models greatly improve the decoding efficiency of speech recognition compared with traditional autoregressive models. In this work, we present dynamic alignment Mask CTC, introducing two methods: (1) Aligned Cross Entropy (AXE), finding the monotonic alignment that minimizes the cross-entropy loss through dynamic programming, (2) Dynamic Rectification, creating new training samples by replacing some masks with model predicted tokens. The AXE ignores the absolute position alignment between prediction and ground truth sentence and focuses on tokens matching in relative order. The dynamic rectification method makes the model capable of simulating the non-mask but possible wrong tokens, even if they have high confidence. Our experiments on WSJ dataset demonstrated that not only AXE loss but also the rectification method could improve the WER performance of Mask CTC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; QI-TTS&#65292;&#19968;&#31181;&#29992;&#20110;&#24773;&#24863;&#35821;&#38899;&#21512;&#25104;&#30340;&#30097;&#38382;&#35821;&#35843;&#25511;&#21046;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22810;&#39118;&#26684;&#25552;&#21462;&#22120;&#21644;&#30456;&#23545;&#23646;&#24615;&#25511;&#21046;&#38899;&#33410;&#23618;&#38754;&#30340;&#35821;&#35843;&#24378;&#24230;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#36798;&#21457;&#35328;&#32773;&#30340;&#30097;&#38382;&#24847;&#22270;&#20197;&#21450;&#24773;&#24863;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2303.07682</link><description>&lt;p&gt;
QI-TTS: &#29992;&#20110;&#24773;&#24863;&#35821;&#38899;&#21512;&#25104;&#30340;&#30097;&#38382;&#35821;&#35843;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
QI-TTS: Questioning Intonation Control for Emotional Speech Synthesis. (arXiv:2303.07682v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07682
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; QI-TTS&#65292;&#19968;&#31181;&#29992;&#20110;&#24773;&#24863;&#35821;&#38899;&#21512;&#25104;&#30340;&#30097;&#38382;&#35821;&#35843;&#25511;&#21046;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#22810;&#39118;&#26684;&#25552;&#21462;&#22120;&#21644;&#30456;&#23545;&#23646;&#24615;&#25511;&#21046;&#38899;&#33410;&#23618;&#38754;&#30340;&#35821;&#35843;&#24378;&#24230;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#36798;&#21457;&#35328;&#32773;&#30340;&#30097;&#38382;&#24847;&#22270;&#20197;&#21450;&#24773;&#24863;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#24773;&#24863;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#21512;&#25104;&#34920;&#36798;&#24773;&#24863;&#30340;&#35821;&#38899;&#65292;&#20294;&#19968;&#20123;&#32454;&#31890;&#24230;&#30340;&#39118;&#26684;&#65292;&#20363;&#22914;&#35821;&#35843;&#65292;&#34987;&#24573;&#30053;&#20102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; QI-TTS&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#36716;&#31227;&#21644;&#25511;&#21046;&#35821;&#35843;&#65292;&#22312;&#36716;&#31227;&#24773;&#24863;&#30340;&#21516;&#26102;&#20256;&#36882;&#21457;&#35328;&#32773;&#30340;&#30097;&#38382;&#24847;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39118;&#26684;&#25552;&#21462;&#22120;&#65292;&#20174;&#20004;&#20010;&#19981;&#21516;&#23618;&#27425;&#25552;&#21462;&#39118;&#26684;&#20449;&#24687;&#12290;&#34429;&#28982;&#21477;&#23376;&#23618;&#27425;&#34920;&#31034;&#24773;&#24863;&#65292;&#20294;&#26368;&#32456;&#30340;&#38899;&#33410;&#23618;&#27425;&#21017;&#34920;&#31034;&#35821;&#35843;&#12290;&#20026;&#20102;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#35821;&#35843;&#25511;&#21046;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#23545;&#23646;&#24615;&#26469;&#34920;&#31034;&#38899;&#33410;&#23618;&#27425;&#30340;&#35821;&#35843;&#24378;&#24230;&#12290;&#23454;&#39564;&#39564;&#35777;&#20102; QI-TTS &#22312;&#24773;&#24863;&#35821;&#38899;&#21512;&#25104;&#20013;&#25552;&#39640;&#35821;&#35843;&#34920;&#29616;&#21147;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent expressive text to speech (TTS) models focus on synthesizing emotional speech, but some fine-grained styles such as intonation are neglected. In this paper, we propose QI-TTS which aims to better transfer and control intonation to further deliver the speaker's questioning intention while transferring emotion from reference speech. We propose a multi-style extractor to extract style embedding from two different levels. While the sentence level represents emotion, the final syllable level represents intonation. For fine-grained intonation control, we use relative attributes to represent intonation intensity at the syllable level.Experiments have validated the effectiveness of QI-TTS for improving intonation expressiveness in emotional speech synthesis.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;query2doc&#30340;&#26597;&#35810;&#25193;&#23637;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20266;&#25991;&#26723;&#26469;&#25913;&#21892;&#31232;&#30095;&#21644;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640; BM25 &#24615;&#33021;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.07678</link><description>&lt;p&gt;
Query2doc: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#25193;&#23637;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Query2doc: Query Expansion with Large Language Models. (arXiv:2303.07678v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;query2doc&#30340;&#26597;&#35810;&#25193;&#23637;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20266;&#25991;&#26723;&#26469;&#25913;&#21892;&#31232;&#30095;&#21644;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640; BM25 &#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26597;&#35810;&#25193;&#23637;&#26041;&#27861;&#65292;&#31216;&#20026;query2doc&#65292;&#21487;&#25913;&#21892;&#31232;&#30095;&#21644;&#23494;&#38598;&#26816;&#32034;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#21033;&#29992;&#23567;&#25209;&#37327;&#25552;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20266;&#25991;&#26723;&#65292;&#28982;&#21518;&#20351;&#29992;&#29983;&#25104;&#30340;&#20266;&#25991;&#26723;&#25193;&#23637;&#26597;&#35810;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32463;&#36807;&#35757;&#32451;&#65292;&#33021;&#22815;&#35760;&#24518;&#30693;&#35782;&#65292;&#20174;&#32780;&#29983;&#25104;&#30340;&#20266;&#25991;&#26723;&#36890;&#24120;&#21253;&#21547;&#39640;&#24230;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#26377;&#21161;&#20110;&#26597;&#35810;&#28040;&#23696;&#21644;&#25351;&#23548;&#26816;&#32034;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#36827;&#34892;&#20219;&#20309;&#27169;&#22411;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;query2doc &#22312; MS-MARCO &#21644; TREC DL &#31561; ad-hoc IR &#25968;&#25454;&#38598;&#19978;&#23558; BM25 &#30340;&#24615;&#33021;&#25552;&#39640;&#20102; 3% &#21040; 15%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#30340;&#32467;&#26524;&#26041;&#38754;&#21463;&#30410;&#20110;&#26368;&#20808;&#36827;&#30340;&#23494;&#38598;&#26816;&#32034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo-documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#8212;&#8212;RenewNAT&#65292;&#23427;&#32467;&#21512;&#20102;&#23436;&#20840;&#21644;&#36845;&#20195;NAT&#27169;&#22411;&#30340;&#20248;&#28857;&#65292;&#36890;&#36807;&#19968;&#27425;&#20256;&#36882;&#20197;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#26041;&#24335;&#29983;&#25104;&#28508;&#22312;&#32763;&#35793;&#32467;&#26524;&#24182;&#26356;&#26032;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#30456;&#23545;&#20110;&#20256;&#32479;NAT&#27169;&#22411;&#30340;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.07665</link><description>&lt;p&gt;
RenewNAT: &#38750;&#33258;&#22238;&#24402;Transformer&#30340;&#26356;&#26032;&#28508;&#22312;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
RenewNAT: Renewing Potential Translation for Non-Autoregressive Transformer. (arXiv:2303.07665v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07665
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#33258;&#22238;&#24402;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#8212;&#8212;RenewNAT&#65292;&#23427;&#32467;&#21512;&#20102;&#23436;&#20840;&#21644;&#36845;&#20195;NAT&#27169;&#22411;&#30340;&#20248;&#28857;&#65292;&#36890;&#36807;&#19968;&#27425;&#20256;&#36882;&#20197;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#26041;&#24335;&#29983;&#25104;&#28508;&#22312;&#32763;&#35793;&#32467;&#26524;&#24182;&#26356;&#26032;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#30456;&#23545;&#20110;&#20256;&#32479;NAT&#27169;&#22411;&#30340;&#26174;&#33879;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NAT)&#27169;&#22411;&#34987;&#25552;&#20986;&#26469;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#21516;&#26102;&#20445;&#25345;&#30456;&#23545;&#39640;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NAT&#27169;&#22411;&#24456;&#38590;&#23454;&#29616;&#25152;&#26399;&#26395;&#30340;&#25928;&#29575;-&#36136;&#37327;&#26435;&#34913;&#12290;&#19968;&#26041;&#38754;&#65292;&#23436;&#20840;NAT&#27169;&#22411;&#22312;&#39640;&#25928;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#19981;&#21450;&#20854;&#33258;&#22238;&#24402;&#23545;&#24212;&#29289;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36845;&#20195;NAT&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#21487;&#27604;&#30340;&#24615;&#33021;&#65292;&#20294;&#20943;&#24369;&#20102;&#36895;&#24230;&#20248;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RenewNAT&#65292;&#19968;&#20010;&#39640;&#25928;&#26377;&#25928;&#30340;&#28789;&#27963;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#23436;&#20840;&#21644;&#36845;&#20195;NAT&#27169;&#22411;&#30340;&#20248;&#28857;&#12290;RenewNAT&#39318;&#20808;&#29983;&#25104;&#28508;&#22312;&#30340;&#32763;&#35793;&#32467;&#26524;&#65292;&#28982;&#21518;&#22312;&#21333;&#27425;&#20256;&#36882;&#20013;&#26356;&#26032;&#23427;&#20204;&#12290;&#23427;&#21487;&#20197;&#22312;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#27169;&#22411;&#21442;&#25968;&#21644;&#35299;&#30721;&#24310;&#36831;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#22312;&#21508;&#31181;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#65288;&#20363;&#22914;&#65292;\textbf{4} WMT&#65289;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive neural machine translation (NAT) models are proposed to accelerate the inference process while maintaining relatively high performance. However, existing NAT models are difficult to achieve the desired efficiency-quality trade-off. For one thing, fully NAT models with efficient inference perform inferior to their autoregressive counterparts. For another, iterative NAT models can, though, achieve comparable performance while diminishing the advantage of speed. In this paper, we propose RenewNAT, a flexible framework with high efficiency and effectiveness, to incorporate the merits of fully and iterative NAT models. RenewNAT first generates the potential translation results and then renews them in a single pass. It can achieve significant performance improvements at the same expense as traditional NAT models (without introducing additional model parameters and decoding latency). Experimental results on various translation benchmarks (e.g., \textbf{4} WMT) show that our
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#65292;&#20351;&#29992;&#24179;&#34892;&#35821;&#35328;&#21644;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22768;&#23398;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;69.6%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;4.788&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2303.07650</link><description>&lt;p&gt;
&#22522;&#20110;&#24179;&#34892;&#35821;&#35328;&#21644;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#36328;&#35821;&#35328;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Alzheimer's Disease detection based on paralinguistic and pre-trained features. (arXiv:2303.07650v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07650
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26816;&#27979;&#65292;&#20351;&#29992;&#24179;&#34892;&#35821;&#35328;&#21644;&#39044;&#35757;&#32451;&#29305;&#24449;&#25552;&#21462;&#22768;&#23398;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;69.6%&#30340;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;4.788&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#25105;&#20204;&#22312;ICASSP-SPGC-2023 ADReSS-M&#25361;&#25112;&#20219;&#21153;&#20013;&#30340;&#25552;&#20132;&#65292;&#26088;&#22312;&#30740;&#31350;&#21738;&#20123;&#22768;&#23398;&#29305;&#24449;&#21487;&#20197;&#36328;&#35821;&#35328;&#24212;&#29992;&#20110;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#39044;&#27979;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;openSmile&#24037;&#20855;&#21253;&#25552;&#21462;&#24179;&#34892;&#35821;&#35328;&#29305;&#24449;&#21644;XLSR-53&#25552;&#21462;&#22768;&#23398;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#35821;&#38899;&#36716;&#24405;&#20026;&#25991;&#26412;&#21518;&#25552;&#21462;&#35821;&#35328;&#29305;&#24449;&#12290;&#36825;&#20123;&#29305;&#24449;&#34987;&#29992;&#20316;&#25105;&#20204;&#26041;&#27861;&#20013;&#30340;AD&#26816;&#27979;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;69.6&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#22238;&#24402;&#20219;&#21153;&#19978;&#30340;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#20026;4.788&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#26395;&#22312;&#36328;&#35821;&#35328;AD&#26816;&#27979;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present our submission to the ICASSP-SPGC-2023 ADReSS-M Challenge Task, which aims to investigate which acoustic features can be generalized and transferred across languages for Alzheimer's Disease (AD) prediction. The challenge consists of two tasks: one is to classify the speech of AD patients and healthy individuals, and the other is to infer Mini Mental State Examination (MMSE) score based on speech only. The difficulty is mainly embodied in the mismatch of the dataset, in which the training set is in English while the test set is in Greek. We extract paralinguistic features using openSmile toolkit and acoustic features using XLSR-53. In addition, we extract linguistic features after transcribing the speech into text. These features are used as indicators for AD detection in our method. Our method achieves an accuracy of 69.6% on the classification task and a root mean squared error (RMSE) of 4.788 on the regression task. The results show that our proposed method is expected to 
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36755;&#20837;&#20381;&#36182;&#24615;&#21160;&#24577;&#28145;&#24230;&#30340;Transformer&#32534;&#30721;&#22120;&#65292;&#21363;I3D&#65292;&#22312;&#25512;&#29702;&#26102;&#37319;&#29992;&#31867;&#20284;&#25968;&#37327;&#30340;&#23618;&#27425;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#20110;&#26222;&#36890;&#30340;Transformer&#21644;&#36890;&#36807;&#36845;&#20195;&#23618;&#32423;&#20462;&#21098;&#24471;&#21040;&#30340;&#38745;&#24577;&#20462;&#21098;&#27169;&#22411;&#65292;&#26377;&#26395;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#23454;&#29616;&#24615;&#33021;&#25928;&#29575;&#26368;&#22823;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.07624</link><description>&lt;p&gt;
I3D: &#22522;&#20110;&#36755;&#20837;&#20381;&#36182;&#24615;&#21160;&#24577;&#28145;&#24230;&#30340;Transformer&#32467;&#26500;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
I3D: Transformer architectures with input-dependent dynamic depth for speech recognition. (arXiv:2303.07624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36755;&#20837;&#20381;&#36182;&#24615;&#21160;&#24577;&#28145;&#24230;&#30340;Transformer&#32534;&#30721;&#22120;&#65292;&#21363;I3D&#65292;&#22312;&#25512;&#29702;&#26102;&#37319;&#29992;&#31867;&#20284;&#25968;&#37327;&#30340;&#23618;&#27425;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#20110;&#26222;&#36890;&#30340;Transformer&#21644;&#36890;&#36807;&#36845;&#20195;&#23618;&#32423;&#20462;&#21098;&#24471;&#21040;&#30340;&#38745;&#24577;&#20462;&#21098;&#27169;&#22411;&#65292;&#26377;&#26395;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#23454;&#29616;&#24615;&#33021;&#25928;&#29575;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#24050;&#32463;&#21462;&#24471;&#20102;&#38750;&#24120;&#22823;&#30340;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#20854;&#22823;&#30340;&#23610;&#23544;&#21644;&#35745;&#31639;&#24320;&#38144;&#65292;&#20351;&#24471;&#22312;&#26576;&#20123;&#30495;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#38590;&#20197;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#12290;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#21487;&#20197;&#20943;&#23567;&#27169;&#22411;&#23610;&#23544;&#24182;&#21152;&#24555;&#25512;&#29702;&#36895;&#24230;&#65292;&#20294;&#21387;&#32553;&#27169;&#22411;&#20855;&#26377;&#22266;&#23450;&#30340;&#32467;&#26500;&#65292;&#36825;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Transformer&#32534;&#30721;&#22120;&#65292;&#31216;&#20026;&#36755;&#20837;&#20381;&#36182;&#24615;&#21160;&#24577;&#28145;&#24230;&#65288;I3D&#65289;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#24615;&#33021;&#25928;&#29575;&#25240;&#34935;&#12290;&#22312;&#25512;&#29702;&#26102;&#37319;&#29992;&#31867;&#20284;&#25968;&#37327;&#30340;&#23618;&#27425;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;I3D&#30340;&#27169;&#22411;&#20248;&#20110;&#26222;&#36890;&#30340;Transformer&#21644;&#36890;&#36807;&#36845;&#20195;&#23618;&#32423;&#20462;&#21098;&#24471;&#21040;&#30340;&#38745;&#24577;&#20462;&#21098;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23545;&#38376;&#27010;&#29575;&#21644;&#36755;&#20837;&#20381;&#36182;&#24615;&#36827;&#34892;&#20102;&#26377;&#36259;&#30340;&#20998;&#26512;&#65292;&#36825;&#26377;&#21161;&#20110;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#28145;&#24230;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it difficult to deploy these models in some real-world applications. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a fixed architecture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#30340;&#29983;&#21629;&#21608;&#26399;&#65292;&#25506;&#35752;&#20102;&#30693;&#35782;&#22914;&#20309;&#22312; PLMs &#20013;&#24490;&#29615;&#27969;&#21160;&#65292;&#24635;&#32467;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#38480;&#21046;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.07616</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#30340;&#29983;&#21629;&#21608;&#26399;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
The Life Cycle of Knowledge in Big Language Models: A Survey. (arXiv:2303.07616v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#30693;&#35782;&#30340;&#29983;&#21629;&#21608;&#26399;&#65292;&#25506;&#35752;&#20102;&#30693;&#35782;&#22914;&#20309;&#22312; PLMs &#20013;&#24490;&#29615;&#27969;&#21160;&#65292;&#24635;&#32467;&#20102;&#29616;&#26377;&#30740;&#31350;&#30340;&#38480;&#21046;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24191;&#27867;&#25104;&#21151;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#33719;&#21462;&#12289;&#32500;&#25252;&#12289;&#26356;&#26032;&#21644;&#20351;&#29992;&#30693;&#35782;&#30340;&#37325;&#35270;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#30456;&#20851;&#30740;&#31350;&#65292;&#20294;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#26469;&#20102;&#35299;&#30693;&#35782;&#22312;&#23398;&#20064;&#12289;&#35843;&#25972;&#21644;&#24212;&#29992;&#36807;&#31243;&#20013;&#22312;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#22914;&#20309;&#24490;&#29615;&#65292;&#36825;&#21487;&#33021;&#38459;&#30861;&#25105;&#20204;&#36827;&#19968;&#27493;&#20102;&#35299;&#24403;&#21069;&#36827;&#23637;&#30340;&#32852;&#31995;&#25110;&#23454;&#29616;&#29616;&#26377;&#38480;&#21046;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23558;PLMs&#37325;&#26032;&#35270;&#20026;&#22522;&#20110;&#30693;&#35782;&#30340;&#31995;&#32479;&#65292;&#23558;PLMs&#20013;&#30693;&#35782;&#30340;&#29983;&#21629;&#21608;&#26399;&#20998;&#20026;&#20116;&#20010;&#20851;&#38190;&#26102;&#26399;&#65292;&#24182;&#35843;&#26597;&#20102;&#30693;&#35782;&#22312;&#24314;&#31435;&#12289;&#32500;&#25252;&#21644;&#20351;&#29992;&#26102;&#30340;&#24490;&#29615;&#26041;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#23457;&#26597;&#20102;&#30693;&#35782;&#29983;&#21629;&#21608;&#26399;&#30340;&#27599;&#20010;&#26102;&#26399;&#30340;&#29616;&#26377;&#30740;&#31350;&#65292;&#24635;&#32467;&#20102;&#20027;&#35201;&#25361;&#25112;&#21644;&#24403;&#21069;&#38480;&#21046;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge plays a critical role in artificial intelligence. Recently, the extensive success of pre-trained language models (PLMs) has raised significant attention about how knowledge can be acquired, maintained, updated and used by language models. Despite the enormous amount of related studies, there still lacks a unified view of how knowledge circulates within language models throughout the learning, tuning, and application processes, which may prevent us from further understanding the connections between current progress or realizing existing limitations. In this survey, we revisit PLMs as knowledge-based systems by dividing the life circle of knowledge in PLMs into five critical periods, and investigating how knowledge circulates when it is built, maintained and used. To this end, we systematically review existing studies of each period of the knowledge life cycle, summarize the main challenges and current limitations, and discuss future directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;ChatGPT&#22312;&#20869;&#23481;&#25490;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25490;&#21517;&#27979;&#35797;&#38598;&#39564;&#35777;&#20854;&#25490;&#21517;&#20559;&#22909;&#19982;&#20154;&#31867;&#30456;&#20284;&#65292;&#36825;&#24847;&#21619;&#30528;ChatGPT&#30340;&#38646;&#26679;&#26412;&#25490;&#21517;&#33021;&#21147;&#21487;&#29992;&#20110;&#20943;&#36731;&#25490;&#24207;&#20219;&#21153;&#30340;&#27880;&#37322;&#21387;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.07610</link><description>&lt;p&gt;
&#25506;&#32034;ChatGPT&#22312;&#20869;&#23481;&#25490;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#65306;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#24615;&#30340;&#21021;&#27493;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences. (arXiv:2303.07610v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;ChatGPT&#22312;&#20869;&#23481;&#25490;&#24207;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25490;&#21517;&#27979;&#35797;&#38598;&#39564;&#35777;&#20854;&#25490;&#21517;&#20559;&#22909;&#19982;&#20154;&#31867;&#30456;&#20284;&#65292;&#36825;&#24847;&#21619;&#30528;ChatGPT&#30340;&#38646;&#26679;&#26412;&#25490;&#21517;&#33021;&#21147;&#21487;&#29992;&#20110;&#20943;&#36731;&#25490;&#24207;&#20219;&#21153;&#30340;&#27880;&#37322;&#21387;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#21161;&#25163;&#65292;ChatGPT&#33021;&#22815;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#25991;&#31456;&#29983;&#25104;&#12289;&#20195;&#30721;&#23436;&#25104;&#21644;&#25968;&#25454;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;ChatGPT&#22312;&#20869;&#23481;&#35780;&#20272;&#26041;&#38754;&#22987;&#32456;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#23637;&#31034;&#20986;&#27169;&#20223;&#20154;&#31867;&#20559;&#22909;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25506;&#32034;ChatGPT&#22312;&#36825;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#20197;&#35780;&#20272;&#20854;&#25490;&#24207;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#27979;&#35797;&#38598;&#65292;&#21253;&#25324;&#24191;&#27867;&#30340;&#29992;&#20363;&#65292;&#21033;&#29992;&#20116;&#20010;&#27169;&#22411;&#29983;&#25104;&#30456;&#24212;&#30340;&#22238;&#24212;&#12290;&#28982;&#21518;&#65292;&#25351;&#31034;ChatGPT&#23545;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#22238;&#24212;&#36827;&#34892;&#25490;&#24207;&#12290;&#27979;&#35797;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#30340;&#25490;&#24207;&#20559;&#22909;&#19982;&#20154;&#31867;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#19968;&#33268;&#12290;&#36825;&#20010;&#21021;&#27493;&#30340;&#23454;&#39564;&#21457;&#29616;&#24847;&#21619;&#30528;ChatGPT&#30340;&#38646;&#26679;&#26412;&#25490;&#24207;&#33021;&#21147;&#21487;&#29992;&#20110;&#20943;&#36731;&#35768;&#22810;&#25490;&#24207;&#20219;&#21153;&#20013;&#30340;&#27880;&#37322;&#21387;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a natural language assistant, ChatGPT is capable of performing various tasks, including but not limited to article generation, code completion, and data analysis. Furthermore, ChatGPT has consistently demonstrated a remarkable level of accuracy and reliability in terms of content evaluation, exhibiting the capability of mimicking human preferences. To further explore ChatGPT's potential in this regard, a study is conducted to assess its ability to rank content. In order to do so, a test set consisting of prompts is created, covering a wide range of use cases, and five models are utilized to generate corresponding responses. ChatGPT is then instructed to rank the responses generated by these models. The results on the test set show that ChatGPT's ranking preferences are consistent with human to a certain extent. This preliminary experimental finding implies that ChatGPT's zero-shot ranking capability could be used to reduce annotation pressure in a number of ranking tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;BERT&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#22914;&#20309;&#21033;&#29992;&#27880;&#24847;&#21147;&#32553;&#30701;&#36755;&#20837;&#38271;&#24230;&#21644;&#25511;&#21046;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#36827;&#34892;&#24212;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;BERT&#30340;&#26089;&#26399;&#23618;&#20026;&#25991;&#26412;&#20998;&#31867;&#20998;&#37197;&#20102;&#26356;&#20851;&#38190;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.07585</link><description>&lt;p&gt;
&#21033;&#29992;&#27880;&#24847;&#21147;&#20540;&#32553;&#30701;&#36755;&#20837;&#38271;&#24230;&#21644;&#29983;&#25104;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Input-length-shortening and text generation via attention values. (arXiv:2303.07585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;BERT&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#22914;&#20309;&#21033;&#29992;&#27880;&#24847;&#21147;&#32553;&#30701;&#36755;&#20837;&#38271;&#24230;&#21644;&#25511;&#21046;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#36827;&#34892;&#24212;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;BERT&#30340;&#26089;&#26399;&#23618;&#20026;&#25991;&#26412;&#20998;&#31867;&#20998;&#37197;&#20102;&#26356;&#20851;&#38190;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#35782;&#21035;&#24433;&#21709;&#20219;&#21153;&#24615;&#33021;&#30340;&#35789;&#35821;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;Transformer&#27169;&#22411;&#24050;&#32463;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#20026;&#19968;&#20123;&#35789;&#20998;&#37197;&#26356;&#39640;&#30340;&#27880;&#24847;&#21147;&#65288;&#21363;&#30456;&#20851;&#24615;&#65289;&#20998;&#25968;&#12290;&#30001;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;Transformer&#27169;&#22411;&#36890;&#24120;&#30001;&#20110;&#30828;&#20214;&#38480;&#21046;&#32780;&#26377;&#36755;&#20837;&#38271;&#24230;&#38480;&#21046;&#12290;&#36825;&#20010;&#38480;&#21046;&#36866;&#29992;&#20110;&#35768;&#22810;Transformer&#65292;&#21253;&#25324;&#30693;&#21517;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#30340;Transformer&#65288;BERT&#65289;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;BERT&#30340;&#27880;&#24847;&#21147;&#20998;&#37197;&#26426;&#21046;&#65292;&#20851;&#27880;&#20197;&#19979;&#20004;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#22914;&#20309;&#21033;&#29992;&#27880;&#24847;&#21147;&#32553;&#30701;&#36755;&#20837;&#38271;&#24230;&#65311;&#65288;2&#65289;&#22914;&#20309;&#21033;&#29992;&#27880;&#24847;&#21147;&#20316;&#20026;&#26465;&#20214;&#25991;&#26412;&#29983;&#25104;&#30340;&#25511;&#21046;&#26426;&#21046;&#65311;&#25105;&#20204;&#23558;&#36825;&#20123;&#38382;&#39064;&#24212;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;BERT&#30340;&#26089;&#26399;&#23618;&#20026;&#25991;&#26412;&#20998;&#31867;&#20998;&#37197;&#20102;&#26356;&#20851;&#38190;&#30340;&#27880;&#24847;&#21147;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying words that impact a task's performance more than others is a challenge in natural language processing. Transformers models have recently addressed this issue by incorporating an attention mechanism that assigns greater attention (i.e., relevance) scores to some words than others. Because of the attention mechanism's high computational cost, transformer models usually have an input-length limitation caused by hardware constraints. This limitation applies to many transformers, including the well-known bidirectional encoder representations of the transformer (BERT) model. In this paper, we examined BERT's attention assignment mechanism, focusing on two questions: (1) How can attention be employed to reduce input length? (2) How can attention be used as a control mechanism for conditional text generation? We investigated these questions in the context of a text classification task. We discovered that BERT's early layers assign more critical attention scores for text classificat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#39537;&#21160;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21019;&#32426;&#24405;&#24615;&#33021;&#65292;&#24182;&#28145;&#20837;&#20998;&#26512;&#24182;&#24635;&#32467;&#30456;&#20851;&#25991;&#29486;&#36164;&#26009;&#12290;</title><link>http://arxiv.org/abs/2303.07576</link><description>&lt;p&gt;
NLP &#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models in NLP: A Survey. (arXiv:2303.07576v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#21644;&#39537;&#21160;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#31561;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21019;&#32426;&#24405;&#24615;&#33021;&#65292;&#24182;&#28145;&#20837;&#20998;&#26512;&#24182;&#24635;&#32467;&#30456;&#20851;&#25991;&#29486;&#36164;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#26174;&#31034;&#20102;&#21019;&#35760;&#24405;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#39318;&#20808;&#27010;&#36848;&#21644;&#25512;&#23548;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#26412;&#29702;&#35770;&#65292;&#28982;&#21518;&#22238;&#39038;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#20174;&#25991;&#26412;&#29983;&#25104;&#12289;&#25991;&#26412;&#39537;&#21160;&#30340;&#22270;&#20687;&#29983;&#25104;&#31561;&#22235;&#20010;&#26041;&#38754;&#36827;&#34892;&#20998;&#26512;&#21644;&#24635;&#32467;&#20102;&#30456;&#20851;&#30340;&#25991;&#29486;&#36164;&#26009;&#65292;&#24182;&#26368;&#32456;&#35760;&#24405;&#20102;&#36825;&#20010;&#20027;&#39064;&#25991;&#29486;&#32508;&#36848;&#30740;&#31350;&#30340;&#32463;&#39564;&#21644;&#24863;&#21463;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have become a powerful family of deep generative models, with record-breaking performance in many applications. This paper first gives an overview and derivation of the basic theory of diffusion models, then reviews the research results of diffusion models in the field of natural language processing, from text generation, text-driven image generation and other four aspects, and analyzes and summarizes the relevant literature materials sorted out, and finally records the experience and feelings of this topic literature review research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;(AVLMaps)&#65292;&#29992;&#20110;&#23384;&#20648;&#36328;&#27169;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#26681;&#25454;&#22810;&#27169;&#24577;&#26597;&#35810;&#22312;&#22320;&#22270;&#20013;&#32034;&#24341;&#30446;&#26631;&#30340;&#23548;&#33322;&#26041;&#24335;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;AVLMaps&#23454;&#29616;&#20102;&#20174;&#22810;&#27169;&#24577;&#25552;&#31034;&#30340;&#38646;&#27425;&#23398;&#20064;&#24335;&#22810;&#27169;&#24577;&#30446;&#26631;&#23548;&#33322;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.07522</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Audio Visual Language Maps for Robot Navigation. (arXiv:2303.07522v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;(AVLMaps)&#65292;&#29992;&#20110;&#23384;&#20648;&#36328;&#27169;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#26681;&#25454;&#22810;&#27169;&#24577;&#26597;&#35810;&#22312;&#22320;&#22270;&#20013;&#32034;&#24341;&#30446;&#26631;&#30340;&#23548;&#33322;&#26041;&#24335;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;AVLMaps&#23454;&#29616;&#20102;&#20174;&#22810;&#27169;&#24577;&#25552;&#31034;&#30340;&#38646;&#27425;&#23398;&#20064;&#24335;&#22810;&#27169;&#24577;&#30446;&#26631;&#23548;&#33322;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#19990;&#30028;&#30340;&#20114;&#21160;&#26159;&#19968;&#31181;&#22810;&#24863;&#23448;&#30340;&#20307;&#39564;&#65292;&#20294;&#26159;&#35768;&#22810;&#26426;&#22120;&#20154;&#20173;&#28982;&#20027;&#35201;&#20381;&#36182;&#35270;&#35273;&#24863;&#30693;&#26469;&#32472;&#21046;&#21644;&#23548;&#33322;&#20182;&#20204;&#30340;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;(AVLMaps)&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;3D&#31354;&#38388;&#22320;&#22270;&#34920;&#31034;&#65292;&#29992;&#20110;&#23384;&#20648;&#26469;&#33258;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#35821;&#35328;&#32447;&#32034;&#30340;&#36328;&#27169;&#24577;&#20449;&#24687;&#12290;&#22312;&#23548;&#33322;&#30340;&#24773;&#22659;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AVLMaps&#33021;&#22815;&#20351;&#26426;&#22120;&#20154;&#31995;&#32479;&#26681;&#25454;&#22810;&#27169;&#24577;&#26597;&#35810;(&#20363;&#22914;&#65292;&#25991;&#26412;&#25551;&#36848;&#12289;&#22270;&#20687;&#25110;&#22320;&#26631;&#30340;&#38899;&#39057;&#29255;&#27573;)&#22312;&#22320;&#22270;&#20013;&#32034;&#24341;&#30446;&#26631;&#12290;&#29305;&#21035;&#26159;&#65292;&#28155;&#21152;&#38899;&#39057;&#20449;&#24687;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26356;&#21487;&#38752;&#22320;&#28040;&#38500;&#30446;&#26631;&#20301;&#32622;&#30340;&#27495;&#20041;&#24615;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AVLMaps&#33021;&#22815;&#23454;&#29616;&#20174;&#22810;&#27169;&#24577;&#25552;&#31034;&#36827;&#34892;&#38646;&#27425;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#23548;&#33322;&#65292;&#24182;&#22312;&#27169;&#31946;&#22330;&#26223;&#20013;&#25552;&#20379;50%&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While interacting in the world is a multi-sensory experience, many robots continue to predominantly rely on visual perception to map and navigate in their environments. In this work, we propose Audio-Visual-Language Maps (AVLMaps), a unified 3D spatial map representation for storing cross-modal information from audio, visual, and language cues. AVLMaps integrate the open-vocabulary capabilities of multimodal foundation models pre-trained on Internet-scale data by fusing their features into a centralized 3D voxel grid. In the context of navigation, we show that AVLMaps enable robot systems to index goals in the map based on multimodal queries, e.g., textual descriptions, images, or audio snippets of landmarks. In particular, the addition of audio information enables robots to more reliably disambiguate goal locations. Extensive experiments in simulation show that AVLMaps enable zero-shot multimodal goal navigation from multimodal prompts and provide 50% better recall in ambiguous scenar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615; Masking over Masking &#31574;&#30053;&#26469;&#22686;&#24378;&#26465;&#20214; Masked &#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#21270;&#33021;&#21147;&#21644;&#20248;&#21270;&#25928;&#29575;&#65292;&#36825;&#31181;&#31574;&#30053;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#12289;&#25688;&#35201;&#21644;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2303.07457</link><description>&lt;p&gt;
AMOM: &#36866;&#24212;&#24615; Masking over Masking &#29992;&#20110;&#26465;&#20214; Masked &#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AMOM: Adaptive Masking over Masking for Conditional Masked Language Model. (arXiv:2303.07457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615; Masking over Masking &#31574;&#30053;&#26469;&#22686;&#24378;&#26465;&#20214; Masked &#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#21270;&#33021;&#21147;&#21644;&#20248;&#21270;&#25928;&#29575;&#65292;&#36825;&#31181;&#31574;&#30053;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#12289;&#25688;&#35201;&#21644;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110; Transformer &#30340;&#33258;&#22238;&#24402;&#26041;&#27861;&#24050;&#32463;&#22312;&#21508;&#31181;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#12289;&#25688;&#35201;&#21644;&#20195;&#30721;&#29983;&#25104;&#65292;&#20294;&#26159;&#25512;&#29702;&#25928;&#29575;&#36739;&#20302;&#12290;&#20026;&#20102;&#21152;&#36895;&#25512;&#29702;&#38454;&#27573;&#65292;&#36807;&#21435;&#20960;&#24180;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#38750;&#33258;&#22238;&#24402;&#31574;&#30053;&#12290;&#20854;&#20013;&#65292;&#26465;&#20214; Masked &#35821;&#35328;&#27169;&#22411; (CMLM) &#26159;&#26368;&#36890;&#29992;&#30340;&#26694;&#26550;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25903;&#25345;&#35768;&#22810;&#19981;&#21516;&#30340;&#24207;&#21015;&#29983;&#25104;&#22330;&#26223;&#65292;&#24182;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#21462;&#24471;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36866;&#24212;&#24615; Masking over Masking &#31574;&#30053;&#26469;&#22686;&#24378;&#35299;&#30721;&#22120;&#30340;&#32454;&#21270;&#33021;&#21147;&#24182;&#20351;&#32534;&#30721;&#22120;&#30340;&#20248;&#21270;&#26356;&#21152;&#23481;&#26131;&#12290;&#22312;&#24635;&#20849; \textbf{15} &#20010;&#25968;&#25454;&#38598;&#19978;&#30340; \textbf{3} &#20010;&#19981;&#21516;&#20219;&#21153;&#65288;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#12289;&#25688;&#35201;&#21644;&#20195;&#30721;&#29983;&#25104;&#65289;&#30340;&#23454;&#39564;&#30830;&#35748;&#65306;&#25105;&#20204;&#25552;&#20986;&#30340;&#31616;&#21333;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based autoregressive (AR) methods have achieved appealing performance for varied sequence-to-sequence generation tasks, e.g., neural machine translation, summarization, and code generation, but suffer from low inference efficiency. To speed up the inference stage, many non-autoregressive (NAR) strategies have been proposed in the past few years. Among them, the conditional masked language model (CMLM) is one of the most versatile frameworks, as it can support many different sequence generation scenarios and achieve very competitive performance on these tasks. In this paper, we further introduce a simple yet effective adaptive masking over masking strategy to enhance the refinement capability of the decoder and make the encoder optimization easier. Experiments on \textbf{3} different tasks (neural machine translation, summarization, and code generation) with \textbf{15} datasets in total confirm that our proposed simple method achieves significant performance improvement ove
&lt;/p&gt;</description></item><item><title>MetaTroll&#26159;&#19968;&#31181;&#20351;&#29992;Transformer Adapter&#30340;&#23569;&#26679;&#26412;&#24040;&#39764;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#36866;&#24212;&#26032;&#30340;&#36816;&#21160;&#65292;&#24182;&#35299;&#20915;&#30001;&#20110;&#25345;&#32493;&#36866;&#24212;&#32780;&#23548;&#33268;&#27169;&#22411;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.07354</link><description>&lt;p&gt;
MetaTroll: &#20351;&#29992;Transformer Adapter&#36827;&#34892;&#23569;&#26679;&#26412;&#25552;&#21462;&#22269;&#23478;&#36190;&#21161;&#30340;&#24040;&#39764;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MetaTroll: Few-shot Detection of State-Sponsored Trolls with Transformer Adapters. (arXiv:2303.07354v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07354
&lt;/p&gt;
&lt;p&gt;
MetaTroll&#26159;&#19968;&#31181;&#20351;&#29992;Transformer Adapter&#30340;&#23569;&#26679;&#26412;&#24040;&#39764;&#26816;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#36866;&#24212;&#26032;&#30340;&#36816;&#21160;&#65292;&#24182;&#35299;&#20915;&#30001;&#20110;&#25345;&#32493;&#36866;&#24212;&#32780;&#23548;&#33268;&#27169;&#22411;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22269;&#23478;&#36190;&#21161;&#30340;&#24040;&#39764;&#26159;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#36816;&#21160;&#30340;&#20027;&#35201;&#21442;&#19982;&#32773;&#65292;&#33258;&#21160;&#24040;&#39764;&#26816;&#27979;&#23545;&#22823;&#35268;&#27169;&#25171;&#20987;&#38169;&#35823;&#20449;&#24687;&#24456;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#24040;&#39764;&#26816;&#27979;&#27169;&#22411;&#26159;&#22522;&#20110;&#24050;&#30693;&#36816;&#21160;&#30340;&#35757;&#32451;&#25968;&#25454;&#24320;&#21457;&#30340;&#65288;&#20363;&#22914;&#65306;&#20420;&#32599;&#26031;&#20114;&#32852;&#32593;&#30740;&#31350;&#26426;&#26500;&#23545;2016&#24180;&#32654;&#22269;&#22823;&#36873;&#30340;&#24433;&#21709;&#36816;&#21160;&#65289;&#65292;&#24403;&#22788;&#29702;&#23545;&#26032;&#30446;&#26631;&#30340;&#26032;&#22411;&#36816;&#21160;&#26102;&#65292;&#26080;&#27861;&#32988;&#20219;&#12290;&#22312;&#20803;&#23398;&#20064;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#24040;&#39764;&#26816;&#27979;&#27169;&#22411;MetaTroll&#65292;&#20854;&#21487;&#20197;&#20351;&#29992;&#26497;&#23569;&#37327;&#30340;&#26631;&#35760;&#26679;&#26412;&#36866;&#24212;&#24182;&#36866;&#29992;&#20110;&#26032;&#30340;&#36816;&#21160;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#24230;&#21487;&#31227;&#26893;&#24615;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;MetaTroll&#20013;&#24341;&#20837;&#20102;&#8220;&#29305;&#23450;&#20110;&#36816;&#21160;&#8221;&#30340;&#36716;&#25442;&#36866;&#37197;&#22120;&#65292;&#20197;&#8220;&#35760;&#24518;&#8221;&#36816;&#21160;&#29305;&#23450;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#25345;&#32493;&#36866;&#24212;&#32780;&#23548;&#33268;&#27169;&#22411;&#8220;&#36951;&#24536;&#8221;&#22914;&#20309;&#26816;&#27979;&#26087;&#36816;&#21160;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MetaTroll&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#21644;&#21516;&#31867;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-sponsored trolls are the main actors of influence campaigns on social media and automatic troll detection is important to combat misinformation at scale. Existing troll detection models are developed based on training data for known campaigns (e.g.\ the influence campaign by Russia's Internet Research Agency on the 2016 US Election), and they fall short when dealing with {\em novel} campaigns with new targets. We propose MetaTroll, a text-based troll detection model based on the meta-learning framework that enables high portability and parameter-efficient adaptation to new campaigns using only a handful of labelled samples for few-shot transfer. We introduce \textit{campaign-specific} transformer adapters to MetaTroll to ``memorise'' campaign-specific knowledge so as to tackle catastrophic forgetting, where a model ``forgets'' how to detect trolls from older campaigns due to continual adaptation. Our experiments demonstrate that MetaTroll substantially outperforms baselines and s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#21360;&#24230;&#30340;&#27861;&#24459;&#39046;&#22495;&#20986;&#21457;&#65292;&#36890;&#36807;&#23545;&#22312;&#21360;&#22320;&#35821;&#27861;&#24459;&#25991;&#26723;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20445;&#37322;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#31639;&#27861;&#20559;&#35265;&#20256;&#36882;&#36827;&#34892;&#20102;&#21021;&#27493;&#35843;&#26597;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20915;&#31574;&#26641;&#27169;&#22411;&#22312;&#19982;&#21360;&#24230;&#25945;&#24466;&#21644;&#31302;&#26031;&#26519;&#30456;&#20851;&#30340;&#36755;&#20837;&#29305;&#24449;&#19978;&#20855;&#26377;0.237&#30340;&#25972;&#20307;&#20844;&#24179;&#24615;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2303.07247</link><description>&lt;p&gt;
&#21360;&#24230;&#27861;&#24459;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#26159;&#21542;&#20844;&#24179;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Models Trained on Indian Legal Data Fair?. (arXiv:2303.07247v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#21360;&#24230;&#30340;&#27861;&#24459;&#39046;&#22495;&#20986;&#21457;&#65292;&#36890;&#36807;&#23545;&#22312;&#21360;&#22320;&#35821;&#27861;&#24459;&#25991;&#26723;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20445;&#37322;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#31639;&#27861;&#20559;&#35265;&#20256;&#36882;&#36827;&#34892;&#20102;&#21021;&#27493;&#35843;&#26597;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20915;&#31574;&#26641;&#27169;&#22411;&#22312;&#19982;&#21360;&#24230;&#25945;&#24466;&#21644;&#31302;&#26031;&#26519;&#30456;&#20851;&#30340;&#36755;&#20837;&#29305;&#24449;&#19978;&#20855;&#26377;0.237&#30340;&#25972;&#20307;&#20844;&#24179;&#24615;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#24212;&#29992;&#22312;&#22810;&#20010;&#39046;&#22495;&#65288;&#22914;&#27861;&#24459;&#12289;&#21307;&#30103;&#21644;&#24515;&#29702;&#20581;&#24247;&#65289;&#21462;&#24471;&#20102;&#24456;&#22823;&#25104;&#21151;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;&#21028;&#20915;&#39044;&#27979;&#65289;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#25429;&#25417;&#21040;&#20102;&#31038;&#20250;&#20559;&#35265;&#12290;&#34429;&#28982;NLP&#39046;&#22495;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#24050;&#32463;&#24471;&#21040;&#30740;&#31350;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#20027;&#35201;&#23450;&#20301;&#22312;&#35199;&#26041;&#32972;&#26223;&#19979;&#12290;&#26412;&#25991;&#20174;&#21360;&#24230;&#30340;&#27861;&#24459;&#39046;&#22495;&#20986;&#21457;&#65292;&#23545;&#20844;&#24179;&#24615;&#36827;&#34892;&#20102;&#21021;&#27493;&#35843;&#26597;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#20102;&#22312;&#21360;&#22320;&#35821;&#27861;&#24459;&#25991;&#26723;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20445;&#37322;&#39044;&#27979;&#20219;&#21153;&#20013;&#20256;&#36882;&#23398;&#20064;&#21040;&#30340;&#31639;&#27861;&#20559;&#35265;&#12290;&#25105;&#20204;&#20351;&#29992;&#32676;&#20307;&#24179;&#31561;&#35780;&#20272;&#20844;&#24179;&#24615;&#24046;&#36317;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#20915;&#31574;&#26641;&#27169;&#22411;&#22312;&#20445;&#37322;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#22312;&#19982;&#21360;&#24230;&#25945;&#24466;&#21644;&#31302;&#26031;&#26519;&#30456;&#20851;&#30340;&#36755;&#20837;&#29305;&#24449;&#19978;&#20855;&#26377;0.237&#30340;&#25972;&#20307;&#20844;&#24179;&#24615;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23545;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#30340;&#20844;&#24179;&#24615;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances and applications of language technology and artificial intelligence have enabled much success across multiple domains like law, medical and mental health. AI-based Language Models, like Judgement Prediction, have recently been proposed for the legal sector. However, these models are strife with encoded social biases picked up from the training data. While bias and fairness have been studied across NLP, most studies primarily locate themselves within a Western context. In this work, we present an initial investigation of fairness from the Indian perspective in the legal domain. We highlight the propagation of learnt algorithmic biases in the bail prediction task for models trained on Hindi legal documents. We evaluate the fairness gap using demographic parity and show that a decision tree model trained for the bail prediction task has an overall fairness disparity of 0.237 between input features associated with Hindus and Muslims. Additionally, we highlight the need for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#24037;&#20316;&#22330;&#25152;&#20013;&#30340;&#32844;&#19994;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#20219;&#21153;&#25552;&#31034;&#24037;&#31243;&#35774;&#35745;&#20102;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#35813;&#20219;&#21153;&#20013;&#65292;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.07142</link><description>&lt;p&gt;
&#24037;&#20316;&#22330;&#25152;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#20010;&#20851;&#20110;&#20219;&#21153;&#25552;&#31034;&#24037;&#31243;&#22312;&#32844;&#19994;&#31867;&#22411;&#20998;&#31867;&#20013;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification. (arXiv:2303.07142v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07142
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#24037;&#20316;&#22330;&#25152;&#20013;&#30340;&#32844;&#19994;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#20219;&#21153;&#25552;&#31034;&#24037;&#31243;&#35774;&#35745;&#20102;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#25104;&#21151;&#22320;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#35813;&#20219;&#21153;&#20013;&#65292;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#22810;&#31181;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#20256;&#32479;&#27169;&#22411;&#22914;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVMs&#65289;&#20197;&#21450;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;DeBERTa&#65292;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#65292;&#26469;&#30740;&#31350;&#23454;&#38469;&#24037;&#20316;&#22330;&#25152;&#20013;&#30340;&#32844;&#19994;&#20998;&#31867;&#20219;&#21153;&#12290;&#20026;&#20102;&#23436;&#25104;&#27492;&#20219;&#21153;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20219;&#21153;&#25552;&#31034;&#24037;&#31243;&#30340;&#25216;&#26415;&#65292;&#21363;&#35774;&#35745;&#25552;&#31034;&#20197;&#24341;&#23548;LLMs&#36798;&#21040;&#25152;&#38656;&#30340;&#36755;&#20986;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#21830;&#19994;&#21487;&#29992;&#30340;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;GPT-3.5&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;text-davinci-003&#21644;gpt-3.5-turbo&#12290;&#25105;&#20204;&#36824;&#23545;&#25552;&#31034;&#24037;&#31243;&#30340;&#19981;&#21516;&#26041;&#38754;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#33391;&#22909;&#35774;&#35745;&#30340;&#25552;&#31034;&#30340;&#24110;&#21161;&#19979;&#65292;LLMs&#22312;&#32844;&#19994;&#31867;&#22411;&#20998;&#31867;&#20219;&#21153;&#19978;&#21487;&#20197;&#36798;&#21040;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#22914;SVMs&#65292;&#29978;&#33267;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22914;DeBERTa&#12290;
&lt;/p&gt;
&lt;p&gt;
This case study investigates the task of job classification in a real-world setting, where the goal is to determine whether an English-language job posting is appropriate for a graduate or entry-level position. We explore multiple approaches to text classification, including supervised approaches such as traditional models like Support Vector Machines (SVMs) and state-of-the-art deep learning methods such as DeBERTa. We compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings. To accomplish this task, we employ prompt engineering, a technique that involves designing prompts to guide the LLMs towards the desired output. Specifically, we evaluate the performance of two commercially available state-of-the-art GPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also conduct a detailed analysis of the impact of different aspects of prompt engineering on the model's performance. Our results show that, with a well-designed pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21452;&#36335;&#24452;&#24314;&#27169;&#26694;&#26550;&#26469;&#22686;&#24378;&#27169;&#22411;&#24863;&#30693;&#21477;&#23376;&#23545;&#20013;&#24494;&#22937;&#24046;&#24322;&#30340;&#33021;&#21147;&#65292;&#24182;&#35774;&#35745;&#20102;DPM-Net&#26469;&#35782;&#21035;&#35821;&#20041;&#20851;&#31995;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2302.12530</link><description>&lt;p&gt;
&#24863;&#30693;&#24494;&#22937;&#20914;&#31361;&#30340;&#35821;&#20041;&#21305;&#37197;&#21452;&#36335;&#24452;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Dual Path Modeling for Semantic Matching by Perceiving Subtle Conflicts. (arXiv:2302.12530v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21452;&#36335;&#24452;&#24314;&#27169;&#26694;&#26550;&#26469;&#22686;&#24378;&#27169;&#22411;&#24863;&#30693;&#21477;&#23376;&#23545;&#20013;&#24494;&#22937;&#24046;&#24322;&#30340;&#33021;&#21147;&#65292;&#24182;&#35774;&#35745;&#20102;DPM-Net&#26469;&#35782;&#21035;&#35821;&#20041;&#20851;&#31995;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#35821;&#20041;&#21305;&#37197;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#20173;&#28982;&#32570;&#20047;&#25429;&#25417;&#24494;&#22937;&#24046;&#24322;&#30340;&#33021;&#21147;&#12290;&#22312;&#21477;&#23376;&#23545;&#20013;&#20462;&#25913;&#12289;&#28155;&#21152;&#21644;&#21024;&#38500;&#21333;&#35789;&#21487;&#33021;&#20351;&#24471;&#27169;&#22411;&#38590;&#20197;&#39044;&#27979;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21452;&#36335;&#24452;&#24314;&#27169;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#21035;&#24314;&#27169;&#20146;&#21644;&#21644;&#24046;&#24322;&#35821;&#20041;&#26469;&#22686;&#24378;&#27169;&#22411;&#24863;&#30693;&#21477;&#23376;&#23545;&#20013;&#24494;&#22937;&#24046;&#24322;&#30340;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#20010;&#21452;&#36335;&#24452;&#24314;&#27169;&#26694;&#26550;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#21452;&#36335;&#24452;&#24314;&#27169;&#32593;&#32476;&#65288;DPM-Net&#65289;&#26469;&#35782;&#21035;&#35821;&#20041;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;10&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#35821;&#20041;&#21305;&#37197;&#21644;&#40065;&#26834;&#24615;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#23454;&#29616;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pre-trained models have achieved great improvements in semantic matching. However, existing models still suffer from insufficient ability to capture subtle differences. The modification, addition and deletion of words in sentence pairs may make it difficult for the model to predict their relationship. To alleviate this problem, we propose a novel Dual Path Modeling Framework to enhance the model's ability to perceive subtle differences in sentence pairs by separately modeling affinity and difference semantics. Based on dual-path modeling framework we design the Dual Path Modeling Network (DPM-Net) to recognize semantic relations. And we conduct extensive experiments on 10 well-studied semantic matching and robustness test datasets, and the experimental results show that our proposed method achieves consistent improvements over baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#24863;&#30693;&#22810;&#36335;&#33258;&#36866;&#24212;&#34701;&#21512;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#24577;&#30693;&#35782;&#22270;&#38382;&#31572;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.12529</link><description>&lt;p&gt;
&#26102;&#38388;&#24863;&#30693;&#22810;&#36335;&#33258;&#36866;&#24212;&#34701;&#21512;&#32593;&#32476;&#29992;&#20110;&#26102;&#24577;&#30693;&#35782;&#22270;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Time-aware Multiway Adaptive Fusion Network for Temporal Knowledge Graph Question Answering. (arXiv:2302.12529v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#24863;&#30693;&#22810;&#36335;&#33258;&#36866;&#24212;&#34701;&#21512;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#26102;&#24577;&#30693;&#35782;&#22270;&#38382;&#31572;&#38382;&#39064;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22240;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#26102;&#24577;&#38382;&#31572;&#26041;&#38754;&#30340;&#20351;&#29992;&#26696;&#20363;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#25506;&#32034;&#12290;&#29616;&#26377;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24320;&#21457;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#23398;&#20064;&#26377;&#20851;&#20110;&#26102;&#24577;&#30693;&#35782;&#22270;QA&#20219;&#21153;&#26041;&#38754;&#30340;&#23454;&#20307;&#30340;&#26102;&#24577;&#19987;&#29992;&#34920;&#31034;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#24863;&#30693;&#22810;&#36335;&#33258;&#36866;&#24212;&#65288;TMA&#65289;&#34701;&#21512;&#32593;&#32476;&#12290;&#21463;&#20154;&#31867;&#36880;&#27493;&#25512;&#29702;&#34892;&#20026;&#30340;&#21551;&#21457;&#65292;&#23545;&#20110;&#27599;&#20010;&#32473;&#23450;&#30340;&#38382;&#39064;&#65292;TMA&#39318;&#20808;&#20174;KG&#20013;&#25552;&#21462;&#30456;&#20851;&#27010;&#24565;&#65292;&#28982;&#21518;&#23558;&#20854;&#39304;&#36865;&#21040;&#22810;&#36335;&#33258;&#36866;&#24212;&#27169;&#22359;&#20197;&#29983;&#25104;&#38382;&#39064;&#30340;&#26102;&#24577;&#29305;&#23450;&#34920;&#31034;&#12290;&#35813;&#34920;&#31034;&#21487;&#20197;&#19982;&#39044;&#20808;&#35757;&#32451;&#30340;KG&#23884;&#20837;&#30456;&#32467;&#21512;&#20197;&#29983;&#25104;&#26368;&#32456;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) have received increasing attention due to its wide applications on natural language processing. However, its use case on temporal question answering (QA) has not been well-explored. Most of existing methods are developed based on pre-trained language models, which might not be capable to learn \emph{temporal-specific} presentations of entities in terms of temporal KGQA task. To alleviate this problem, we propose a novel \textbf{T}ime-aware \textbf{M}ultiway \textbf{A}daptive (\textbf{TMA}) fusion network. Inspired by the step-by-step reasoning behavior of humans. For each given question, TMA first extracts the relevant concepts from the KG, and then feeds them into a multiway adaptive module to produce a \emph{temporal-specific} representation of the question. This representation can be incorporated with the pre-trained KG embedding to generate the final prediction. Empirical results verify that the proposed model achieves better performance than the state-of-the
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24494;&#23567;&#30340;&#29702;&#35770;&#20219;&#21153;&#25913;&#21160;&#19978;&#23481;&#26131;&#22833;&#36133;&#65292;&#34920;&#26126;&#22312;&#30452;&#35273;&#24515;&#29702;&#23398;&#27169;&#22411;&#35780;&#20272;&#20013;&#38656;&#35201;&#25345;&#24576;&#30097;&#24577;&#24230;&#65292;&#19988;&#22833;&#36133;&#26696;&#20363;&#24212;&#34987;&#37325;&#35270;&#12290;</title><link>http://arxiv.org/abs/2302.08399</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312; Theory-of-Mind &#20219;&#21153;&#30340;&#24494;&#23567;&#25913;&#21464;&#19978;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks. (arXiv:2302.08399v5 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08399
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24494;&#23567;&#30340;&#29702;&#35770;&#20219;&#21153;&#25913;&#21160;&#19978;&#23481;&#26131;&#22833;&#36133;&#65292;&#34920;&#26126;&#22312;&#30452;&#35273;&#24515;&#29702;&#23398;&#27169;&#22411;&#35780;&#20272;&#20013;&#38656;&#35201;&#25345;&#24576;&#30097;&#24577;&#24230;&#65292;&#19988;&#22833;&#36133;&#26696;&#20363;&#24212;&#34987;&#37325;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30452;&#35273;&#24515;&#29702;&#23398;&#26159;&#24120;&#35782;&#25512;&#29702;&#30340;&#25903;&#26609;&#12290;&#22312;&#26426;&#22120;&#26234;&#33021;&#20013;&#22797;&#21046;&#36825;&#31181;&#25512;&#29702;&#26159;&#36808;&#21521;&#31867;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#22522;&#30707;&#12290;&#26368;&#36817;&#65292;&#26377;&#20960;&#39033;&#20219;&#21153;&#21644;&#22522;&#20934;&#29992;&#20110;&#26816;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#25512;&#29702;&#65292;&#29305;&#21035;&#20851;&#27880;&#24515;&#28789;&#29702;&#35770;&#20219;&#21153;&#20013;&#30340;&#20449;&#24565;&#24402;&#23646;&#12290;&#36825;&#20123;&#20219;&#21153;&#26082;&#26377;&#25104;&#21151;&#26696;&#20363;&#20063;&#26377;&#22833;&#36133;&#26696;&#20363;&#12290;&#25105;&#20204;&#29305;&#21035;&#32771;&#34385;&#20102;&#19968;&#20010;&#26368;&#36817;&#22768;&#31216;&#30340;&#25104;&#21151;&#26696;&#20363;&#65292;&#24182;&#23637;&#31034;&#20102;&#32500;&#25345;ToM&#21407;&#21017;&#30340;&#23567;&#24133;&#21464;&#21270;&#20351;&#32467;&#26524;&#22823;&#30456;&#24452;&#24237;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19968;&#33324;&#26469;&#35828;&#65292;&#22312;&#30452;&#35273;&#24515;&#29702;&#23398;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#38646;&#20551;&#35774;&#24212;&#35813;&#25345;&#24576;&#30097;&#24577;&#24230;&#65292;&#24182;&#19988;&#31163;&#32676;&#25925;&#38556;&#26696;&#20363;&#24212;&#35813;&#36229;&#36807;&#24179;&#22343;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;&#26356;&#24378;&#22823;&#30340;LLM&#65288;Large-Large Models&#65289;&#22312;&#29702;&#35299;&#24515;&#29702;&#23398;&#20219;&#21153;&#19978;&#21487;&#33021;&#21462;&#24471;&#30340;&#26410;&#26469;&#25104;&#21151;&#23545;&#20154;&#31867;ToM&#20219;&#21153;&#24847;&#21619;&#30528;&#20160;&#20040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intuitive psychology is a pillar of common-sense reasoning. The replication of this reasoning in machine intelligence is an important stepping-stone on the way to human-like artificial intelligence. Several recent tasks and benchmarks for examining this reasoning in Large-Large Models have focused in particular on belief attribution in Theory-of-Mind tasks. These tasks have shown both successes and failures. We consider in particular a recent purported success case, and show that small variations that maintain the principles of ToM turn the results on their head. We argue that in general, the zero-hypothesis for model evaluation in intuitive psychology should be skeptical, and that outlying failure cases should outweigh average success rates. We also consider what possible future successes on Theory-of-Mind tasks by more powerful LLMs would mean for ToM tasks with people.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;GLADIS&#30340;&#36890;&#29992;&#22823;&#22411;&#32553;&#20889;&#28040;&#27495;&#22522;&#20934;&#65292;&#21253;&#25324;&#19968;&#20010;&#26356;&#22823;&#30340;&#32553;&#20889;&#35789;&#20856;&#12289;&#19968;&#20010;&#21253;&#21547;1.6&#20159;&#20010;&#21477;&#23376;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21644;&#35206;&#30422;&#36890;&#29992;&#12289;&#31185;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#39044;&#35757;&#32451;&#20102;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;AcroBERT&#65292;&#20197;&#29992;&#20110;&#36890;&#29992;&#32553;&#20889;&#28040;&#27495;&#12290;</title><link>http://arxiv.org/abs/2302.01860</link><description>&lt;p&gt;
GLADIS: &#19968;&#20010;&#36890;&#29992;&#30340;&#22823;&#22411;&#32553;&#20889;&#28040;&#27495;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
GLADIS: A General and Large Acronym Disambiguation Benchmark. (arXiv:2302.01860v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01860
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;GLADIS&#30340;&#36890;&#29992;&#22823;&#22411;&#32553;&#20889;&#28040;&#27495;&#22522;&#20934;&#65292;&#21253;&#25324;&#19968;&#20010;&#26356;&#22823;&#30340;&#32553;&#20889;&#35789;&#20856;&#12289;&#19968;&#20010;&#21253;&#21547;1.6&#20159;&#20010;&#21477;&#23376;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#21644;&#35206;&#30422;&#36890;&#29992;&#12289;&#31185;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#39044;&#35757;&#32451;&#20102;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;AcroBERT&#65292;&#20197;&#29992;&#20110;&#36890;&#29992;&#32553;&#20889;&#28040;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32553;&#20889;&#28040;&#27495;&#23545;&#20110;&#29702;&#35299;&#21508;&#31181;&#26469;&#28304;&#30340;&#33258;&#28982;&#35821;&#35328;&#37117;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#29983;&#29289;&#21307;&#23398;&#25253;&#21578;&#12289;&#31185;&#23398;&#35770;&#25991;&#21644;&#25628;&#32034;&#24341;&#25806;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#32553;&#20889;&#28040;&#27495;&#22522;&#20934;&#21644;&#24037;&#20855;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#65292;&#20808;&#21069;&#30340;&#22522;&#20934;&#30340;&#35268;&#27169;&#20063;&#30456;&#23545;&#36739;&#23567;&#12290;&#20026;&#20102;&#21152;&#36895;&#32553;&#20889;&#28040;&#27495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;GLADIS&#30340;&#26032;&#22522;&#20934;&#65292;&#21253;&#25324;&#19977;&#20010;&#37096;&#20998;&#65306;(1)&#19968;&#20010;&#21547;&#26377;1.5M&#32553;&#20889;&#21644;6.4M&#38271;&#26684;&#24335;&#30340;&#26356;&#22823;&#30340;&#32553;&#20889;&#35789;&#20856;; (2)&#19968;&#20010;&#21253;&#21547;1.6&#20159;&#20010;&#21477;&#23376;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;; (3)&#35206;&#30422;&#36890;&#29992;&#12289;&#31185;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#26500;&#24314;&#30340;&#35821;&#26009;&#24211;&#19978;&#39044;&#20808;&#35757;&#32451;&#20102;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;AcroBERT&#20197;&#36827;&#34892;&#36890;&#29992;&#32553;&#20889;&#28040;&#27495;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#26032;&#22522;&#20934;&#30340;&#25361;&#25112;&#21644;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acronym Disambiguation (AD) is crucial for natural language understanding on various sources, including biomedical reports, scientific papers, and search engine queries. However, existing acronym disambiguation benchmarks and tools are limited to specific domains, and the size of prior benchmarks is rather small. To accelerate the research on acronym disambiguation, we construct a new benchmark named GLADIS with three components: (1) a much larger acronym dictionary with 1.5M acronyms and 6.4M long forms; (2) a pre-training corpus with 160 million sentences; (3) three datasets that cover the general, scientific, and biomedical domains. We then pre-train a language model, \emph{AcroBERT}, on our constructed corpus for general acronym disambiguation, and show the challenges and values of our new benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;TriNet&#37319;&#29992;&#19977;&#20998;&#25903;&#32467;&#26500;&#65292;&#21487;&#38450;&#27490;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;ASR&#20013;&#30340;&#23849;&#28291;&#65292;&#24182;&#22312;&#19979;&#28216;ASR&#20219;&#21153;&#20013;&#27604;SOTA&#26041;&#27861;Data2vec&#23454;&#29616;&#20102;6.06%&#30340;&#30456;&#23545;&#21333;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#65288;WERR&#65289;&#12290;</title><link>http://arxiv.org/abs/2301.00656</link><description>&lt;p&gt;
TriNet&#65306;&#38450;&#27490;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;ASR&#20013;&#23436;&#20840;&#25110;&#32531;&#24930;&#23849;&#28291;&#30340;&#31283;&#23450;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TriNet: stabilizing self-supervised learning from complete or slow collapse on ASR. (arXiv:2301.00656v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;TriNet&#37319;&#29992;&#19977;&#20998;&#25903;&#32467;&#26500;&#65292;&#21487;&#38450;&#27490;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;ASR&#20013;&#30340;&#23849;&#28291;&#65292;&#24182;&#22312;&#19979;&#28216;ASR&#20219;&#21153;&#20013;&#27604;SOTA&#26041;&#27861;Data2vec&#23454;&#29616;&#20102;6.06%&#30340;&#30456;&#23545;&#21333;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#65288;WERR&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#38754;&#20020;&#30528;&#31361;&#28982;&#30340;&#20449;&#24687;&#23849;&#28291;&#25110;&#32531;&#24930;&#30340;&#32500;&#24230;&#23849;&#28291;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TriNet&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#20998;&#25903;&#32467;&#26500;&#26469;&#38450;&#27490;&#23849;&#28291;&#21644;&#31283;&#23450;&#39044;&#35757;&#32451;&#12290;TriNet&#23398;&#20064;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#28508;&#22312;&#23884;&#20837;&#31354;&#38388;&#65292;&#24182;&#23558;&#20854;&#24182;&#20837;&#21040;&#19968;&#20010;&#26356;&#39640;&#32423;&#21035;&#30340;&#31354;&#38388;&#20013;&#20197;&#39044;&#27979;&#30001;&#20923;&#32467;&#30340;&#25945;&#24072;&#29983;&#25104;&#30340;&#34394;&#20551;&#30446;&#26631;&#21521;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#31283;&#23450;&#21644;&#21152;&#36895;&#20102;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#19979;&#28216;&#22522;&#20934;ASR&#20219;&#21153;&#20013;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;Data2vec&#23454;&#29616;&#20102;6.06&#65285;&#30340;&#30456;&#23545;&#21333;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#65288;WERR&#65289;&#12290;&#25105;&#20204;&#20250;&#22312;https://github.com/tencent-ailab/ &#19978;&#21457;&#24067;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) models confront challenges of abrupt informational collapse or slow dimensional collapse. We propose TriNet, which introduces a novel triple-branch architecture for preventing collapse and stabilizing the pre-training. TriNet learns the SSL latent embedding space and incorporates it to a higher level space for predicting pseudo target vectors generated by a frozen teacher. Our experimental results show that the proposed method notably stabilizes and accelerates pre-training and achieves a relative word error rate reduction (WERR) of 6.06% compared to the state-of-the-art (SOTA) Data2vec for a downstream benchmark ASR task. We will release our code at https://github.com/tencent-ailab/.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30693;&#35782;&#22270;&#35889;&#30340;&#29992;&#25143;&#19979;&#19968;&#27493;&#24847;&#22270;&#39044;&#27979;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#25903;&#20184;&#23453;&#32593;&#32476;&#24179;&#21488;&#19978;&#23545;1&#20159;&#27963;&#36291;&#29992;&#25143;&#30340;&#26381;&#21153;&#65292;&#24182;&#19988;&#22312;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2301.00503</link><description>&lt;p&gt;
&#25903;&#20184;&#23453;&#29992;&#25143;&#19979;&#19968;&#27493;&#24847;&#22270;&#39044;&#27979;&#30340;&#27010;&#24565;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
A Concept Knowledge Graph for User Next Intent Prediction at Alipay. (arXiv:2301.00503v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#30693;&#35782;&#22270;&#35889;&#30340;&#29992;&#25143;&#19979;&#19968;&#27493;&#24847;&#22270;&#39044;&#27979;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22312;&#25903;&#20184;&#23453;&#32593;&#32476;&#24179;&#21488;&#19978;&#23545;1&#20159;&#27963;&#36291;&#29992;&#25143;&#30340;&#26381;&#21153;&#65292;&#24182;&#19988;&#22312;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#27010;&#24565;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#29992;&#25143;&#19979;&#19968;&#27493;&#24847;&#22270;&#39044;&#27979;&#30340;&#25216;&#26415;&#12290;&#35813;&#31995;&#32479;&#24050;&#22312;&#25903;&#20184;&#23453;&#32593;&#32476;&#24179;&#21488;&#19978;&#37096;&#32626;&#65292;&#20026;&#36229;&#36807;1&#20159;&#27963;&#36291;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;AlipayKG&#65292;&#29992;&#20110;&#26174;&#24335;&#22320;&#25551;&#36848;&#29992;&#25143;&#24847;&#22270;&#30340;&#31163;&#32447;&#27010;&#24565;&#30693;&#35782;&#22270;&#35889;&#65292;&#27169;&#25311;&#20102;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#12289;&#20016;&#23500;&#30340;&#20869;&#23481;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#23558;&#26469;&#33258;&#30693;&#35782;&#22270;&#35889;&#30340;&#19987;&#23478;&#35268;&#21017;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#20197;&#25512;&#26029;&#22312;&#32447;&#29992;&#25143;&#30340;&#19979;&#19968;&#27493;&#24847;&#22270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper illustrates the technologies of user next intent prediction with a concept knowledge graph. The system has been deployed on the Web at Alipay, serving more than 100 million daily active users. To explicitly characterize user intent, we propose AlipayKG, which is an offline concept knowledge graph in the Life-Service domain modeling the historical behaviors of users, the rich content interacted by users and the relations between them. We further introduce a Transformer-based model which integrates expert rules from the knowledge graph to infer the online user's next intent. Experimental results demonstrate that the proposed system can effectively enhance the performance of the downstream tasks while retaining explainability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;SuS-X&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#21517;&#31216;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36801;&#31227;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.16198</link><description>&lt;p&gt;
SuS-X&#65306;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#21517;&#31216;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36801;&#31227;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SuS-X: Training-Free Name-Only Transfer of Vision-Language Models. (arXiv:2211.16198v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SuS-X&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#22522;&#20110;&#21517;&#31216;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36801;&#31227;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#24050;&#25104;&#20026;&#35757;&#32451;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;CLIP&#22312;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#21644;&#26816;&#32034;&#26041;&#38754;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#35201;&#21457;&#25381;&#20854;&#20840;&#37096;&#28508;&#21147;&#65292;&#24494;&#35843;&#20173;&#28982;&#26159;&#24517;&#35201;&#30340;&#12290;&#24494;&#35843;&#25972;&#20010;CLIP&#27169;&#22411;&#20250;&#28040;&#32791;&#36164;&#28304;&#19988;&#19981;&#31283;&#23450;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#34429;&#28982;&#26088;&#22312;&#36991;&#20813;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#20173;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#20998;&#24067;&#20013;&#30340;&#22270;&#20687;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#21478;&#19968;&#31181;&#26041;&#27861;&#8212;&#8212;&#26080;&#38656;&#35757;&#32451;&#30340;&#8220;&#20165;&#22522;&#20110;&#21517;&#31216;&#36801;&#31227;&#8221;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;SuS-X&#65292;&#30001;&#20004;&#20010;&#20851;&#38190;&#26500;&#24314;&#22359;&#8212;&#8212;SuS&#21644;TIP-X&#32452;&#25104;&#65292;&#26082;&#19981;&#38656;&#35201;&#23494;&#38598;&#30340;&#24494;&#35843;&#65292;&#20063;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#26631;&#35760;&#25968;&#25454;&#12290;SuS-X&#22312;19&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet effective way to train large-scale vision-language models. CLIP demonstrates impressive zero-shot classification and retrieval on diverse downstream tasks. However, to leverage its full potential, fine-tuning still appears to be necessary. Fine-tuning the entire CLIP model can be resource-intensive and unstable. Moreover, recent methods that aim to circumvent this need for fine-tuning still require access to images from the target distribution. In this paper, we pursue a different approach and explore the regime of training-free "name-only transfer" in which the only knowledge we possess about the downstream task comprises the names of downstream target categories. We propose a novel method, SuS-X, consisting of two key building blocks -- SuS and TIP-X, that requires neither intensive fine-tuning nor costly labelled data. SuS-X achieves state-of-the-art zero-shot classification results on 19 benchmark datasets. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20307;&#31995;&#19979;&#20195;&#29702;&#20154;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#24314;&#31435;&#20102;&#20840;&#32852;&#37030;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#20195;&#29702;&#20154;&#23398;&#20064;&#27169;&#22411;&#12290;&#20854;&#20013;&#65292;&#23548;&#33322;&#21363;&#25915;&#20987;&#32773;&#25152;&#24895;&#65288;NAW&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#32780;&#22522;&#20110;&#31163;&#32676;&#28857;&#26816;&#27979;&#30340;&#38450;&#24481;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;NAW&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20195;&#29702;&#20154;&#23398;&#20064;&#30340;&#20840;&#23616;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.14769</link><description>&lt;p&gt;
&#21521;&#23548;&#33322;&#21363;&#25915;&#20987;&#32773;&#25152;&#24895;&#65311;&#24314;&#31435;&#25308;&#21344;&#24237;&#40065;&#26834;&#24615;&#30340;&#32852;&#37030;&#23398;&#20064;&#20307;&#31995;&#19979;&#30340;&#20195;&#29702;&#20154;
&lt;/p&gt;
&lt;p&gt;
Navigation as Attackers Wish? Towards Building Byzantine-Robust Embodied Agents under Federated Learning. (arXiv:2211.14769v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20307;&#31995;&#19979;&#20195;&#29702;&#20154;&#23398;&#20064;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#24314;&#31435;&#20102;&#20840;&#32852;&#37030;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#20195;&#29702;&#20154;&#23398;&#20064;&#27169;&#22411;&#12290;&#20854;&#20013;&#65292;&#23548;&#33322;&#21363;&#25915;&#20987;&#32773;&#25152;&#24895;&#65288;NAW&#65289;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#32780;&#22522;&#20110;&#31163;&#32676;&#28857;&#26816;&#27979;&#30340;&#38450;&#24481;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;NAW&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#20195;&#29702;&#20154;&#23398;&#20064;&#30340;&#20840;&#23616;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#21270;&#20307;&#31995;&#19979;&#30340;&#20195;&#29702;&#20154;&#23398;&#20064;&#36890;&#36807;&#22312;&#26412;&#22320;&#23458;&#25143;&#31471;&#65288;&#21363;&#19981;&#21516;&#29615;&#22659;&#65289;&#20013;&#20445;&#25345;&#25968;&#25454;&#26469;&#20445;&#25252;&#20010;&#20154;&#35270;&#35273;&#29615;&#22659;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#19979;&#65292;&#30001;&#20110;&#26412;&#22320;&#25968;&#25454;&#23545;&#26381;&#21153;&#22120;&#26159;&#19981;&#21487;&#35775;&#38382;&#30340;&#65292;&#25915;&#20987;&#32773;&#21487;&#33021;&#36731;&#26131;&#22320;&#27745;&#26579;&#26412;&#22320;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#22312;&#19981;&#34987;&#36890;&#30693;&#30340;&#24773;&#20917;&#19979;&#22312;&#20195;&#29702;&#20154;&#20013;&#24314;&#31435;&#21518;&#38376;&#12290;&#20351;&#29992;&#36825;&#26679;&#30340;&#20195;&#29702;&#20154;&#20250;&#23545;&#20154;&#31867;&#26500;&#25104;&#28508;&#22312;&#21361;&#23475;&#65292;&#22240;&#20026;&#25915;&#20987;&#32773;&#21487;&#20197;&#36731;&#26494;&#22320;&#36890;&#36807;&#21518;&#38376;&#25805;&#32437;&#20195;&#29702;&#20154;&#36827;&#34892;&#23548;&#33322;&#21644;&#25511;&#21046;&#12290;&#20026;&#20102;&#23454;&#29616;&#20840;&#32852;&#37030;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#20195;&#29702;&#20154;&#23398;&#20064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20219;&#21153;&#20013;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#38656;&#35201;&#36319;&#38543;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#23548;&#33322;&#23460;&#20869;&#29615;&#22659;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25915;&#20987;&#31574;&#30053;&#65292;&#21363;&#23548;&#33322;&#21363;&#25915;&#20987;&#32773;&#25152;&#24895;&#65288;NAW&#65289;&#65292;&#20854;&#20013;&#24694;&#24847;&#23458;&#25143;&#31471;&#36890;&#36807;&#25805;&#32437;&#26412;&#22320;&#36712;&#36857;&#25968;&#25454;&#26469;&#21521;&#20840;&#23616;&#27169;&#22411;&#26893;&#20837;&#21518;&#38376;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;NAW&#21487;&#20197;&#23454;&#29616;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#32780;&#19988;&#24615;&#33021;&#19979;&#38477;&#24494;&#19981;&#36275;&#36947;&#12290;&#20026;&#20102;&#38450;&#27490;NAW&#25915;&#20987;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#24481;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#31163;&#32676;&#28857;&#26816;&#27979;&#30340;&#27010;&#24565;&#26469;&#35782;&#21035;&#21644;&#21024;&#38500;&#24694;&#24847;&#23458;&#25143;&#31471;&#12290;&#25105;&#20204;&#22312;VLN&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#38450;&#24481;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20943;&#36731;NAW&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#32852;&#37030;&#21270;&#20307;&#31995;&#19979;&#20195;&#29702;&#20154;&#23398;&#20064;&#30340;&#20840;&#23616;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated embodied agent learning protects the data privacy of individual visual environments by keeping data locally at each client (the individual environment) during training. However, since the local data is inaccessible to the server under federated learning, attackers may easily poison the training data of the local client to build a backdoor in the agent without notice. Deploying such an agent raises the risk of potential harm to humans, as the attackers may easily navigate and control the agent as they wish via the backdoor. Towards Byzantine-robust federated embodied agent learning, in this paper, we study the attack and defense for the task of vision-and-language navigation (VLN), where the agent is required to follow natural language instructions to navigate indoor environments. First, we introduce a simple but effective attack strategy, Navigation as Wish (NAW), in which the malicious client manipulates local trajectory data to implant a backdoor into the global model. Resu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27963;&#36291;&#20851;&#31995;&#21457;&#29616;(ARD)&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;&#20013;&#21306;&#20998;&#24050;&#30693;&#21644;&#26032;&#20851;&#31995;&#20197;&#21450;&#26631;&#35760;&#26032;&#20851;&#31995;&#31867;&#22411;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#24120;&#35268;&#21644;&#26356;&#36890;&#29992;&#30340;&#35774;&#32622;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.04215</link><description>&lt;p&gt;
&#27963;&#36291;&#20851;&#31995;&#21457;&#29616;&#65306;&#36890;&#21521;&#36890;&#29992;&#21644;&#26631;&#31614;&#24863;&#30693;&#30340;&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Active Relation Discovery: Towards General and Label-aware Open Relation Extraction. (arXiv:2211.04215v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.04215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27963;&#36291;&#20851;&#31995;&#21457;&#29616;(ARD)&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;&#20013;&#21306;&#20998;&#24050;&#30693;&#21644;&#26032;&#20851;&#31995;&#20197;&#21450;&#26631;&#35760;&#26032;&#20851;&#31995;&#31867;&#22411;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#24120;&#35268;&#21644;&#26356;&#36890;&#29992;&#30340;&#35774;&#32622;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;(OpenRE)&#26088;&#22312;&#21457;&#29616;&#24320;&#25918;&#39046;&#22495;&#30340;&#26032;&#20851;&#31995;&#12290;&#20197;&#21069;&#30340;OpenRE&#26041;&#27861;&#20027;&#35201;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;(1)&#26080;&#27861;&#20805;&#20998;&#21306;&#20998;&#24050;&#30693;&#20851;&#31995;&#21644;&#26032;&#20851;&#31995;&#12290;&#24403;&#23558;&#24120;&#35268;&#27979;&#35797;&#35774;&#32622;&#25193;&#23637;&#21040;&#26356;&#36890;&#29992;&#30340;&#35774;&#32622;&#26102;&#65292;&#20854;&#20013;&#27979;&#35797;&#25968;&#25454;&#21487;&#33021;&#20063;&#26469;&#33258;&#24050;&#30693;&#31867;&#21035;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;(2)&#22312;&#23454;&#38469;&#24212;&#29992;&#20043;&#21069;&#24517;&#39035;&#36827;&#34892;&#20108;&#27425;&#26631;&#27880;&#12290;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#20026;&#26032;&#20851;&#31995;&#26631;&#35760;&#20154;&#31867;&#21487;&#35835;&#21644;&#26377;&#24847;&#20041;&#30340;&#31867;&#22411;&#65292;&#36825;&#26159;&#19979;&#28216;&#20219;&#21153;&#36843;&#20999;&#38656;&#35201;&#30340;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27963;&#36291;&#20851;&#31995;&#21457;&#29616;(ARD)&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#20851;&#31995;&#24322;&#24120;&#20540;&#26816;&#27979;&#26469;&#21306;&#20998;&#24050;&#30693;&#21644;&#26032;&#20851;&#31995;&#65292;&#24182;&#28041;&#21450;&#20027;&#21160;&#23398;&#20064;&#29992;&#20110;&#26631;&#35760;&#26032;&#20851;&#31995;&#12290;&#23545;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;ARD&#22312;&#24120;&#35268;&#21644;&#25105;&#20204;&#25552;&#20986;&#30340;&#26356;&#36890;&#29992;&#30340;&#35774;&#32622;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open Relation Extraction (OpenRE) aims to discover novel relations from open domains. Previous OpenRE methods mainly suffer from two problems: (1) Insufficient capacity to discriminate between known and novel relations. When extending conventional test settings to a more general setting where test data might also come from seen classes, existing approaches have a significant performance decline. (2) Secondary labeling must be performed before practical application. Existing methods cannot label human-readable and meaningful types for novel relations, which is urgently required by the downstream tasks. To address these issues, we propose the Active Relation Discovery (ARD) framework, which utilizes relational outlier detection for discriminating known and novel relations and involves active learning for labeling novel relations. Extensive experiments on three real-world datasets show that ARD significantly outperforms previous state-of-the-art methods on both conventional and our propos
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21518;&#32512;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411; (SUREALM)&#65292;&#35813;&#27169;&#22411;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#27169;&#25311;&#21452;&#21521;&#19978;&#19979;&#25991;&#25928;&#24212;&#12290;&#37319;&#29992;&#23884;&#20837;&#26816;&#32034;&#22120;,&#22312;&#24207;&#21015;&#29983;&#25104;&#36807;&#31243;&#20013;&#25628;&#32034;&#20855;&#26377;&#30456;&#20284;&#21333;&#35789;&#21382;&#21490;&#30340;&#35757;&#32451;&#21477;&#23376;&#25968;&#25454;&#23384;&#20648;&#12290;&#22312;DSTC9&#21475;&#35821;&#23545;&#35805;&#35821;&#26009;&#24211;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2211.03053</link><description>&lt;p&gt;
&#21518;&#32512;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Suffix Retrieval-Augmented Language Modeling. (arXiv:2211.03053v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03053
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21518;&#32512;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411; (SUREALM)&#65292;&#35813;&#27169;&#22411;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#27169;&#25311;&#21452;&#21521;&#19978;&#19979;&#25991;&#25928;&#24212;&#12290;&#37319;&#29992;&#23884;&#20837;&#26816;&#32034;&#22120;,&#22312;&#24207;&#21015;&#29983;&#25104;&#36807;&#31243;&#20013;&#25628;&#32034;&#20855;&#26377;&#30456;&#20284;&#21333;&#35789;&#21382;&#21490;&#30340;&#35757;&#32451;&#21477;&#23376;&#25968;&#25454;&#23384;&#20648;&#12290;&#22312;DSTC9&#21475;&#35821;&#23545;&#35805;&#35821;&#26009;&#24211;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#65288;LM&#65289;&#20351;&#29992;&#21333;&#35789;&#21382;&#21490;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#21333;&#35789;&#12290;&#30456;&#21453;&#65292;BERT&#21033;&#29992;&#21477;&#23376;&#20013;&#30340;&#21452;&#21521;&#21333;&#35789;&#20449;&#24687;&#26469;&#39044;&#27979;&#33945;&#38754;&#20301;&#32622;&#30340;&#21333;&#35789;&#12290;&#34429;&#28982;BERT&#22312;&#24207;&#21015;&#32534;&#30721;&#26041;&#38754;&#24456;&#26377;&#25928;&#65292;&#20294;&#23427;&#26412;&#36136;&#19978;&#26159;&#38750;&#22240;&#26524;&#30340;&#65292;&#19981;&#36866;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21518;&#32512;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65288;SUREALM&#65289;&#65292;&#23427;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#27169;&#25311;&#21452;&#21521;&#19978;&#19979;&#25991;&#25928;&#24212;&#12290;SUREALM&#37319;&#29992;&#23884;&#20837;&#26816;&#32034;&#22120;&#65292;&#22312;&#24207;&#21015;&#29983;&#25104;&#36807;&#31243;&#20013;&#25628;&#32034;&#20855;&#26377;&#30456;&#20284;&#21333;&#35789;&#21382;&#21490;&#30340;&#35757;&#32451;&#21477;&#23376;&#25968;&#25454;&#23384;&#20648;&#12290;&#29305;&#21035;&#26159;&#65292;&#26816;&#32034;&#21040;&#30340;&#21477;&#23376;&#30340;&#21518;&#32512;&#37096;&#20998;&#27169;&#25311;&#20102;&#8220;&#26410;&#26469;&#8221;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#22312;DSTC9&#21475;&#35821;&#23545;&#35805;&#35821;&#26009;&#24211;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#26174;&#31034;&#20102;&#19982;&#31454;&#20105;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#39564;&#35777;&#38598;&#21644;&#27979;&#35797;&#38598;&#19978;&#26377;&#33391;&#22909;&#30340;&#21333;&#35789;&#22256;&#24785;&#24230;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal language modeling (LM) uses word history to predict the next word. BERT, on the other hand, makes use of bi-directional word information in a sentence to predict words at masked positions. While BERT is effective in sequence encoding, it is non-causal by nature and is not designed for sequence generation. In this paper, we propose a novel language model, SUffix REtrieval-Augmented LM (SUREALM), that simulates a bi-directional contextual effect in an autoregressive manner. SUREALM employs an embedding retriever to search for training sentences in a data store that share similar word history during sequence generation. In particular, the suffix portions of the retrieved sentences mimick the "future" context. We evaluated our proposed model on the DSTC9 spoken dialogue corpus and showed promising word perplexity reduction on the validation and test set compared to competitive baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#37319;&#29992;Perceiver&#32534;&#30721;&#22120;&#21644;Dynamic Latent Access(DLA)&#35757;&#32451;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;MuST-C&#25968;&#25454;&#38598;&#30340;&#19977;&#31181;&#35821;&#35328;&#23545;&#19978;&#36798;&#21040;Transformer&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#22312;&#25512;&#26029;&#26102;&#26131;&#20110;&#36866;&#24212;&#19981;&#21516;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;&#32763;&#35793;&#36136;&#37327;&#27809;&#26377;&#26174;&#33879;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2210.16264</link><description>&lt;p&gt;
&#21160;&#24577;&#28508;&#22312;&#24863;&#30693;&#22120;&#39640;&#25928;&#35821;&#38899;&#32763;&#35793;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Efficient Speech Translation with Dynamic Latent Perceivers. (arXiv:2210.16264v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#37319;&#29992;Perceiver&#32534;&#30721;&#22120;&#21644;Dynamic Latent Access(DLA)&#35757;&#32451;&#30340;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;MuST-C&#25968;&#25454;&#38598;&#30340;&#19977;&#31181;&#35821;&#35328;&#23545;&#19978;&#36798;&#21040;Transformer&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24182;&#19988;&#22312;&#25512;&#26029;&#26102;&#26131;&#20110;&#36866;&#24212;&#19981;&#21516;&#30340;&#35745;&#31639;&#39044;&#31639;&#65292;&#32763;&#35793;&#36136;&#37327;&#27809;&#26377;&#26174;&#33879;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Transformer&#32593;&#32476;&#26550;&#26500;&#36880;&#28176;&#25104;&#20026;&#35821;&#38899;&#32763;&#35793;&#39046;&#22495;&#30340;&#20027;&#27969;&#65292;&#23454;&#29616;&#20102;&#32763;&#35793;&#36136;&#37327;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;&#20294;&#30001;&#20110;&#35821;&#38899;&#20449;&#21495;&#30340;&#38271;&#24230;&#36739;&#38271;&#65292;&#32780;Transformer&#30340;&#22797;&#26434;&#24230;&#21576;&#20108;&#27425;&#22686;&#38271;&#65292;&#22240;&#27492;&#20026;&#20102;&#20351;&#20854;&#36866;&#29992;&#20110;&#35821;&#38899;&#32763;&#35793;&#65292;&#24517;&#39035;&#37319;&#29992;&#19979;&#37319;&#26679;&#31574;&#30053;&#12290;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#37319;&#29992;Perceiver&#32534;&#30721;&#22120;&#23558;&#35821;&#38899;&#36755;&#20837;&#26144;&#23556;&#21040;&#22266;&#23450;&#38271;&#24230;&#30340;&#28508;&#22312;&#34920;&#24449;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;Perceiver&#35757;&#32451;&#26041;&#27861;Dynamic Latent Access(DLA)&#65292;&#36890;&#36807;&#35299;&#38145;&#26356;&#22823;&#28508;&#22312;&#31354;&#38388;&#32780;&#19981;&#22686;&#21152;&#35745;&#31639;&#36127;&#25285;&#12290;&#37319;&#29992;DLA&#30340;&#35821;&#38899;&#21040;&#25991;&#26412;Perceiver&#27169;&#22411;&#33021;&#22815;&#22312;MuST-C&#25968;&#25454;&#38598;&#30340;&#19977;&#31181;&#35821;&#35328;&#23545;&#19978;&#36798;&#21040;Transformer&#22522;&#32447;&#27169;&#22411;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;DLA&#35757;&#32451;&#30340;&#27169;&#22411;&#26131;&#20110;&#22312;&#25512;&#26029;&#26102;&#36866;&#24212;DLA&#65292;&#21487;&#20197;&#26681;&#25454;&#19981;&#21516;&#30340;&#35745;&#31639;&#39044;&#31639;&#28789;&#27963;&#37096;&#32626;&#65292;&#32763;&#35793;&#36136;&#37327;&#27809;&#26377;&#26174;&#33879;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have been the dominant architecture for Speech Translation in recent years, achieving significant improvements in translation quality. Since speech signals are longer than their textual counterparts, and due to the quadratic complexity of the Transformer, a down-sampling step is essential for its adoption in Speech Translation. Instead, in this research, we propose to ease the complexity by using a Perceiver encoder to map the speech inputs to a fixed-length latent representation. Furthermore, we introduce a novel way of training Perceivers, with Dynamic Latent Access (DLA), unlocking larger latent spaces without any additional computational overhead. Speech-to-Text Perceivers with DLA can match the performance of Transformer baselines across three language pairs in MuST-C. Finally, a DLA-trained model is easily adaptable to DLA at inference, and can be flexibly deployed with various computational budgets, without significant drops in translation quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#31070;&#32463;&#22330;&#27169;&#22411;&#65292;&#35299;&#37322;&#20102;&#35821;&#38899;&#23545;&#40784;&#26102;&#38388;&#30340;&#36229;&#21457;&#38899;&#29616;&#35937;&#12290;&#36890;&#36807;&#23545;&#21333;&#35789;&#30340;&#26368;&#23567;&#23545;&#25163;&#36827;&#34892;&#25233;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#27604;&#24615;&#36229;&#21457;&#38899;&#12290;&#20266;&#21333;&#35789;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36229;&#21457;&#38899;&#29616;&#35937;&#19982;&#23454;&#26102;&#35821;&#38899;&#35745;&#21010;&#21644;&#21046;&#20316;&#26377;&#20851;&#12290;&#19982;&#23454;&#38469;&#21333;&#35789;&#30456;&#27604;&#65292;&#20266;&#21333;&#35789;&#20013;&#36229;&#21457;&#38899;&#29616;&#35937;&#30340;&#31243;&#24230;&#21644;&#33539;&#22260;&#26377;&#25152;&#38477;&#20302;&#65292;&#34920;&#26126;&#35789;&#27719;&#21644;&#38899;&#20301;&#35745;&#21010;&#23618;&#20043;&#38388;&#30340;&#20132;&#20114;&#28608;&#27963;&#22312;&#36229;&#21457;&#38899;&#19978;&#36215;&#21040;&#20102;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2209.12278</link><description>&lt;p&gt;
&#35821;&#38899;&#35745;&#21010;&#20013;&#30340;&#31070;&#32463;&#25233;&#21046;&#26377;&#21161;&#20110;&#23545;&#27604;&#24615;&#36229;&#21457;&#38899;
&lt;/p&gt;
&lt;p&gt;
Neural inhibition during speech planning contributes to contrastive hyperarticulation. (arXiv:2209.12278v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#31070;&#32463;&#22330;&#27169;&#22411;&#65292;&#35299;&#37322;&#20102;&#35821;&#38899;&#23545;&#40784;&#26102;&#38388;&#30340;&#36229;&#21457;&#38899;&#29616;&#35937;&#12290;&#36890;&#36807;&#23545;&#21333;&#35789;&#30340;&#26368;&#23567;&#23545;&#25163;&#36827;&#34892;&#25233;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#27604;&#24615;&#36229;&#21457;&#38899;&#12290;&#20266;&#21333;&#35789;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36229;&#21457;&#38899;&#29616;&#35937;&#19982;&#23454;&#26102;&#35821;&#38899;&#35745;&#21010;&#21644;&#21046;&#20316;&#26377;&#20851;&#12290;&#19982;&#23454;&#38469;&#21333;&#35789;&#30456;&#27604;&#65292;&#20266;&#21333;&#35789;&#20013;&#36229;&#21457;&#38899;&#29616;&#35937;&#30340;&#31243;&#24230;&#21644;&#33539;&#22260;&#26377;&#25152;&#38477;&#20302;&#65292;&#34920;&#26126;&#35789;&#27719;&#21644;&#38899;&#20301;&#35745;&#21010;&#23618;&#20043;&#38388;&#30340;&#20132;&#20114;&#28608;&#27963;&#22312;&#36229;&#21457;&#38899;&#19978;&#36215;&#21040;&#20102;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21333;&#35789;&#22312;&#33021;&#22815;&#23558;&#23427;&#20204;&#19982;&#26368;&#23567;&#23545;&#25163;&#21306;&#20998;&#30340;&#35821;&#38899;&#32500;&#24230;&#19978;&#23384;&#22312;&#36229;&#21457;&#38899;&#29616;&#35937;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21160;&#24577;&#31070;&#32463;&#22330;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#37322;&#36825;&#31181;&#23545;&#27604;&#24615;&#36229;&#21457;&#38899;&#29616;&#35937;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23545;&#21333;&#35789;&#30340;&#26368;&#23567;&#23545;&#25163;&#30340;&#25233;&#21046;&#20316;&#29992;&#65292;&#26469;&#23454;&#29616;&#35821;&#38899;&#23545;&#40784;&#26102;&#38388;&#30340;&#36229;&#21457;&#38899;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#26032;&#30340;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#20266;&#21333;&#35789;&#20013;&#30340;&#22833;&#38899;&#22622;&#36741;&#38899;&#35821;&#38899;&#23545;&#40784;&#26102;&#38388;&#30340;&#23545;&#27604;&#24615;&#36229;&#21457;&#38899;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20266;&#21333;&#35789;&#20013;&#23384;&#22312;&#23545;&#27604;&#24615;&#36229;&#21457;&#38899;&#29616;&#35937;&#65292;&#35828;&#26126;&#36229;&#21457;&#38899;&#29616;&#35937;&#19982;&#23454;&#26102;&#35821;&#38899;&#35745;&#21010;&#21644;&#21046;&#20316;&#26377;&#20851;&#12290;&#19982;&#23454;&#38469;&#21333;&#35789;&#20013;&#30340;&#36229;&#21457;&#38899;&#30456;&#27604;&#65292;&#20266;&#21333;&#35789;&#20013;&#36229;&#21457;&#38899;&#29616;&#35937;&#30340;&#33539;&#22260;&#21644;&#31243;&#24230;&#26377;&#25152;&#38477;&#20302;&#65292;&#34920;&#26126;&#35789;&#27719;&#21644;&#38899;&#20301;&#35745;&#21010;&#23618;&#20043;&#38388;&#30340;&#20132;&#20114;&#28608;&#27963;&#22312;&#36229;&#21457;&#38899;&#19978;&#36215;&#21040;&#20102;&#20316;&#29992;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#32479;&#19968;&#20174;&#23545;&#27604;&#24615;&#36229;&#21457;&#38899;&#21040;&#38899;&#20301;&#37051;&#22495;&#25928;&#24212;&#21644;&#35821;&#38899;&#30165;&#36857;&#25928;&#24212;&#31561;&#19968;&#20123;&#26126;&#26174;&#19981;&#21516;&#30340;&#29616;&#35937;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous work has demonstrated that words are hyperarticulated on dimensions of speech that differentiate them from a minimal pair competitor. This phenomenon has been termed contrastive hyperarticulation (CH). We present a dynamic neural field (DNF) model of voice onset time (VOT) planning that derives CH from an inhibitory influence of the minimal pair competitor during planning. We test some predictions of the model with a novel experiment investigating CH of voiceless stop consonant VOT in pseudowords. The results demonstrate a CH effect in pseudowords, consistent with a basis for the effect in the real-time planning and production of speech. The scope and magnitude of CH in pseudowords was reduced compared to CH in real words, consistent with a role for interactive activation between lexical and phonological levels of planning. We discuss the potential of our model to unify an apparently disparate set of phenomena, from CH to phonological neighborhood effects to phonetic trace eff
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;11&#31181;&#20027;&#35201;&#35821;&#35328;&#30340;&#35821;&#26009;&#36827;&#34892;&#24207;&#25968;&#27169;&#24335;&#20998;&#26512;&#65292;&#21457;&#29616;&#19981;&#21516;&#30340;&#35821;&#35328;&#26377;&#30528;&#29420;&#29305;&#30340;&#27169;&#24335;&#32467;&#26500;&#20998;&#24067;&#65292;&#36825;&#20123;&#20998;&#24067;&#30340;&#27874;&#21160;&#21487;&#20197;&#30830;&#23450;&#25991;&#26412;&#30340;&#21382;&#21490;&#26102;&#26399;&#21644;&#20316;&#32773;&#65292;&#36825;&#24378;&#35843;&#20102;&#24207;&#25968;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#22312;&#35821;&#35328;&#23398;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.11175</link><description>&lt;p&gt;
&#35789;&#27719;&#27169;&#24335;&#30340;&#24207;&#25968;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Ordinal analysis of lexical patterns. (arXiv:2208.11175v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11175
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;11&#31181;&#20027;&#35201;&#35821;&#35328;&#30340;&#35821;&#26009;&#36827;&#34892;&#24207;&#25968;&#27169;&#24335;&#20998;&#26512;&#65292;&#21457;&#29616;&#19981;&#21516;&#30340;&#35821;&#35328;&#26377;&#30528;&#29420;&#29305;&#30340;&#27169;&#24335;&#32467;&#26500;&#20998;&#24067;&#65292;&#36825;&#20123;&#20998;&#24067;&#30340;&#27874;&#21160;&#21487;&#20197;&#30830;&#23450;&#25991;&#26412;&#30340;&#21382;&#21490;&#26102;&#26399;&#21644;&#20316;&#32773;&#65292;&#36825;&#24378;&#35843;&#20102;&#24207;&#25968;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#22312;&#35821;&#35328;&#23398;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35789;&#26159;&#36830;&#25509;&#24605;&#24819;&#21644;&#20107;&#29289;&#30340;&#22522;&#26412;&#35821;&#35328;&#21333;&#20301;&#65292;&#20294;&#21333;&#35789;&#19981;&#20250;&#21333;&#29420;&#20986;&#29616;&#22312;&#25991;&#26412;&#24207;&#21015;&#20013;&#12290;&#21477;&#27861;&#35268;&#21017;&#30340;&#23384;&#22312;&#23548;&#33268;&#37051;&#36817;&#21333;&#35789;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;&#20351;&#29992;&#24207;&#25968;&#27169;&#24335;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;11&#31181;&#20027;&#35201;&#35821;&#35328;&#30340;&#35789;&#27719;&#32479;&#35745;&#20851;&#31995;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#21033;&#29992;&#21508;&#31181;&#19981;&#21516;&#26041;&#24335;&#26469;&#34920;&#36798;&#21333;&#35789;&#20851;&#31995;&#65292;&#20135;&#29983;&#20102;&#29420;&#29305;&#30340;&#27169;&#24335;&#32467;&#26500;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#32473;&#23450;&#35821;&#35328;&#30340;&#36825;&#20123;&#27169;&#24335;&#20998;&#24067;&#30340;&#27874;&#21160;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#30830;&#23450;&#25991;&#26412;&#25776;&#20889;&#30340;&#21382;&#21490;&#26102;&#26399;&#21644;&#20316;&#32773;&#12290;&#32508;&#19978;&#25152;&#36848;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#24207;&#25968;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#22312;&#35821;&#35328;&#31867;&#22411;&#23398;&#12289;&#21382;&#21490;&#35821;&#35328;&#23398;&#21644;&#25991;&#20307;&#23398;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Words are fundamental linguistic units that connect thoughts and things through meaning. However, words do not appear independently in a text sequence. The existence of syntactic rules induces correlations among neighboring words. Using an ordinal pattern approach, we present an analysis of lexical statistical connections for 11 major languages. We find that the diverse manners that languages utilize to express word relations give rise to unique pattern structural distributions. Furthermore, fluctuations of these pattern distributions for a given language can allow us to determine both the historical period when the text was written and its author. Taken together, our results emphasize the relevance of ordinal time series analysis in linguistic typology, historical linguistics and stylometry.
&lt;/p&gt;</description></item><item><title>&#20154;&#20204;&#24456;&#38590;&#36776;&#21035;AI&#29983;&#25104;&#30340;&#35821;&#35328;&#24418;&#24335;&#65292;&#22240;&#20026;&#36890;&#24120;&#37319;&#29992;&#30340;&#21028;&#26029;&#21551;&#21457;&#24335;&#31639;&#27861;&#20986;&#29616;&#20102;&#19968;&#20123;&#32570;&#38519;&#65292;&#38656;&#35201;&#20351;&#29992;&#26356;&#20026;&#22797;&#26434;&#30340;&#35821;&#35328;&#20998;&#26512;&#24037;&#20855;&#21644;&#25945;&#32946;&#26469;&#25552;&#39640;&#20154;&#20204;&#30340;&#21028;&#26029;&#21147;&#12290;</title><link>http://arxiv.org/abs/2206.07271</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#20110;AI&#35821;&#35328;&#29983;&#25104;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#23384;&#22312;&#32570;&#38519;
&lt;/p&gt;
&lt;p&gt;
Human heuristics for AI-generated language are flawed. (arXiv:2206.07271v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07271
&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#24456;&#38590;&#36776;&#21035;AI&#29983;&#25104;&#30340;&#35821;&#35328;&#24418;&#24335;&#65292;&#22240;&#20026;&#36890;&#24120;&#37319;&#29992;&#30340;&#21028;&#26029;&#21551;&#21457;&#24335;&#31639;&#27861;&#20986;&#29616;&#20102;&#19968;&#20123;&#32570;&#38519;&#65292;&#38656;&#35201;&#20351;&#29992;&#26356;&#20026;&#22797;&#26434;&#30340;&#35821;&#35328;&#20998;&#26512;&#24037;&#20855;&#21644;&#25945;&#32946;&#26469;&#25552;&#39640;&#20154;&#20204;&#30340;&#21028;&#26029;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#35821;&#35328;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#20154;&#31867;&#20132;&#27969;&#30456;&#20114;&#34701;&#21512;&#12290;&#22312;&#32842;&#22825;&#12289;&#37038;&#20214;&#21644;&#31038;&#20132;&#23186;&#20307;&#20013;&#65292;AI&#31995;&#32479;&#24314;&#35758;&#21333;&#35789;&#12289;&#23436;&#25104;&#21477;&#23376;&#25110;&#20135;&#29983;&#25972;&#20010;&#23545;&#35805;&#12290;&#20154;&#20204;&#36890;&#24120;&#26080;&#27861;&#37492;&#21035;AI&#20135;&#29983;&#30340;&#35821;&#35328;&#65292;&#32780;&#23558;&#20854;&#35270;&#20026;&#20154;&#31867;&#32534;&#20889;&#30340;&#35821;&#35328;&#65292;&#36825;&#24341;&#21457;&#20102;&#26377;&#20851;&#26032;&#24418;&#24335;&#27450;&#39575;&#21644;&#25805;&#32437;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#31867;&#22914;&#20309;&#20998;&#36776;AI&#29983;&#25104;&#30340;&#26368;&#20026;&#20010;&#20154;&#21270;&#21644;&#37325;&#35201;&#30340;&#35821;&#35328;&#24418;&#24335;&#20043;&#19968;&#8212;&#8212;&#21475;&#22836;&#33258;&#25105;&#34920;&#36848;&#12290;&#22312;&#20845;&#20010;&#23454;&#39564;&#20013;&#65292;&#21442;&#19982;&#32773;&#65288;N=4,600&#65289;&#26080;&#27861;&#22312;&#19987;&#19994;&#12289;&#37202;&#24215;&#20197;&#21450;&#32422;&#20250;&#24773;&#22659;&#20013;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;AI&#35821;&#35328;&#27169;&#22411;&#25152;&#29983;&#25104;&#30340;&#33258;&#25105;&#34920;&#36848;&#12290;&#35821;&#35328;&#29305;&#24449;&#30340;&#35745;&#31639;&#20998;&#26512;&#34920;&#26126;&#65292;&#20154;&#31867;&#23545;&#20110;AI&#29983;&#25104;&#30340;&#35821;&#35328;&#21028;&#26029;&#23384;&#22312;&#21551;&#21457;&#24335;&#31639;&#27861;&#30340;&#32570;&#38519;&#65292;&#20363;&#22914;&#23558;&#31532;&#19968;&#20154;&#31216;&#20195;&#35789;&#12289;&#32553;&#30053;&#35789;&#20351;&#29992;&#25110;&#23478;&#24237;&#35805;&#39064;&#19982;&#20154;&#31867;&#32534;&#20889;&#30340;&#35821;&#35328;&#32852;&#31995;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#21551;&#21457;&#24335;&#31639;&#27861;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;AI&#29983;&#25104;&#30340;&#35821;&#35328;&#65292;&#24314;&#35758;&#20351;&#29992;&#26356;&#20026;&#22797;&#26434;&#30340;&#35821;&#35328;&#20998;&#26512;&#24037;&#20855;&#21644;&#25945;&#32946;&#26469;&#25913;&#21892;&#20154;&#31867;&#30340;&#21028;&#26029;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human communication is increasingly intermixed with language generated by AI. Across chat, email, and social media, AI systems suggest words, complete sentences, or produce entire conversations. AI-generated language is often not identified as such but presented as language written by humans, raising concerns about novel forms of deception and manipulation. Here, we study how humans discern whether verbal self-presentations, one of the most personal and consequential forms of language, were generated by AI. In six experiments, participants (N = 4,600) were unable to detect self-presentations generated by state-of-the-art AI language models in professional, hospitality, and dating contexts. A computational analysis of language features shows that human judgments of AI-generated language are hindered by intuitive but flawed heuristics such as associating first-person pronouns, use of contractions, or family topics with human-written language. We experimentally demonstrate that these heur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38376;&#25511;&#23618;&#38388;&#21327;&#20316;&#65288;GIC&#65289;&#26426;&#21046;&#26469;&#25913;&#36827;CTC-based&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#22312;CTC-based&#27169;&#22411;&#20013;&#24341;&#20837;&#25991;&#26412;&#20449;&#24687;&#26469;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#19977;&#20010;&#23454;&#39564;&#37117;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#24378;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2205.12462</link><description>&lt;p&gt;
&#22522;&#20110;&#38376;&#25511;&#23618;&#38388;&#21327;&#20316;&#25913;&#36827;CTC ASR &#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving CTC-based ASR Models with Gated Interlayer Collaboration. (arXiv:2205.12462v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38376;&#25511;&#23618;&#38388;&#21327;&#20316;&#65288;GIC&#65289;&#26426;&#21046;&#26469;&#25913;&#36827;CTC-based&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#22312;CTC-based&#27169;&#22411;&#20013;&#24341;&#20837;&#25991;&#26412;&#20449;&#24687;&#26469;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#19977;&#20010;&#23454;&#39564;&#37117;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#24378;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CTC-based ASR &#27169;&#22411;&#22312;&#27809;&#26377;&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#24448;&#24448;&#32570;&#20047;&#23545;&#26465;&#20214;&#20381;&#36182;&#21644;&#25991;&#26412;&#20132;&#20114;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38376;&#25511;&#23618;&#38388;&#21327;&#20316;&#65288;GIC&#65289;&#26426;&#21046;&#26469;&#25913;&#36827;CTC-based&#27169;&#22411;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#23558;&#25991;&#26412;&#20449;&#24687;&#24341;&#20837;&#27169;&#22411;&#65292;&#20174;&#32780;&#25918;&#26494;CTC-based&#27169;&#22411;&#30340;&#26465;&#20214;&#29420;&#31435;&#20551;&#35774;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#26631;&#35760;&#23884;&#20837;&#30340;&#21152;&#26435;&#21644;&#35270;&#20026;&#27599;&#20010;&#20301;&#32622;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#20854;&#20013;&#20301;&#32622;&#29305;&#23450;&#30340;&#26435;&#37325;&#26159;&#36890;&#36807;&#23618;&#38388;&#36741;&#21161;CTC losses&#26500;&#24314;&#30340;softmax&#27010;&#29575;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#24320;&#21457;&#38376;&#25511;&#21333;&#20803;&#23558;&#25991;&#26412;&#34920;&#31034;&#19982;&#22768;&#23398;&#29305;&#24449;&#34701;&#21512;&#12290;&#22312;AISHELL-1&#65292;TEDLIUM2 &#21644; AIDATATANG corpus &#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CTC-based automatic speech recognition (ASR) models without the external language model usually lack the capacity to model conditional dependencies and textual interactions. In this paper, we present a Gated Interlayer Collaboration (GIC) mechanism to improve the performance of CTC-based models, which introduces textual information into the model and thus relaxes the conditional independence assumption of CTC-based models. Specifically, we consider the weighted sum of token embeddings as the textual representation for each position, where the position-specific weights are the softmax probability distribution constructed via inter-layer auxiliary CTC losses. The textual representations are then fused with acoustic features by developing a gate unit. Experiments on AISHELL-1, TEDLIUM2, and AIDATATANG corpora show that the proposed method outperforms several strong baselines.
&lt;/p&gt;</description></item><item><title>Relphormer&#26159;&#19968;&#31181;&#26032;&#30340;Transformer&#21464;&#20307;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#12290;&#23427;&#24341;&#20837;&#20102;Triple2Seq&#21644;&#22686;&#24378;&#24335;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#35299;&#20915;&#22522;&#26412;Transformer&#26550;&#26500;&#22312;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2205.10852</link><description>&lt;p&gt;
Relphormer&#65306;&#20851;&#31995;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Relphormer: Relational Graph Transformer for Knowledge Graph Representations. (arXiv:2205.10852v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10852
&lt;/p&gt;
&lt;p&gt;
Relphormer&#26159;&#19968;&#31181;&#26032;&#30340;Transformer&#21464;&#20307;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#12290;&#23427;&#24341;&#20837;&#20102;Triple2Seq&#21644;&#22686;&#24378;&#24335;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#35299;&#20915;&#22522;&#26412;Transformer&#26550;&#26500;&#22312;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#24418;&#25366;&#25496;&#31561;&#24191;&#27867;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;remarkable&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#26412;&#30340;Transformer&#26550;&#26500;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#34920;&#31034;&#20013;&#24182;&#27809;&#26377;&#21462;&#24471;&#24456;&#22909;&#30340;&#25913;&#36827;&#65292;&#20854;&#20013;&#24179;&#31227;&#36317;&#31163;&#27169;&#22411;&#25903;&#37197;&#20102;&#36825;&#20010;&#39046;&#22495;&#12290;&#38656;&#27880;&#24847;&#30340;&#26159;&#65292;&#22522;&#26412;&#30340;Transformer&#26550;&#26500;&#38590;&#20197;&#25429;&#25417;&#21040;&#30693;&#35782;&#22270;&#35889;&#30340;&#20869;&#22312;&#24322;&#26500;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#30340;Transformer&#21464;&#20307;&#65292;&#21517;&#20026;Relphormer&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Triple2Seq&#65292;&#21487;&#20197;&#21160;&#24577;&#22320;&#37319;&#26679;&#19978;&#19979;&#25991;&#21270;&#30340;&#23376;&#22270;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#32531;&#35299;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#24335;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#23545;&#20851;&#31995;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#20445;&#25345;&#23454;&#20307;&#21644;&#20851;&#31995;&#20869;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#25513;&#34109;&#24335;&#30693;&#35782;&#24314;&#27169;&#26469;&#23454;&#29616;&#36890;&#29992;&#30340;&#30693;&#35782;&#22270;&#24418;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved remarkable performance in widespread fields, including natural language processing, computer vision and graph mining. However, vanilla Transformer architectures have not yielded promising improvements in the Knowledge Graph (KG) representations, where the translational distance paradigm dominates this area. Note that vanilla Transformer architectures struggle to capture the intrinsically heterogeneous structural and semantic information of knowledge graphs. To this end, we propose a new variant of Transformer for knowledge graph representations dubbed Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample contextualized sub-graph sequences as the input to alleviate the heterogeneity issue. We propose a novel structure-enhanced self-attention mechanism to encode the relational information and keep the semantic information within entities and relations. Moreover, we utilize masked knowledge modeling for general knowledge graph representa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#36716;&#21270;&#20026;&#29983;&#25104;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20851;&#31995;&#24341;&#23548;&#28436;&#31034;&#21644;&#23454;&#20307;&#24863;&#30693;&#20998;&#23618;&#35299;&#30721;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24555;&#36895;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#27604;&#22522;&#32447;&#26356;&#22909;&#25110;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#26356;&#24555;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#20013;&#25991;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;AliopenKG500&#12290;</title><link>http://arxiv.org/abs/2202.02113</link><description>&lt;p&gt;
&#20174;&#21306;&#20998;&#21040;&#29983;&#25104;&#65306;&#22522;&#20110;&#29983;&#25104;&#21464;&#25442;&#22120;&#30340;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. (arXiv:2202.02113v7 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#36716;&#21270;&#20026;&#29983;&#25104;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#20851;&#31995;&#24341;&#23548;&#28436;&#31034;&#21644;&#23454;&#20307;&#24863;&#30693;&#20998;&#23618;&#35299;&#30721;&#26469;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24555;&#36895;&#25512;&#26029;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#27604;&#22522;&#32447;&#26356;&#22909;&#25110;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#26356;&#24555;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#20013;&#25991;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;AliopenKG500&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#35299;&#20915;&#20102;&#25193;&#23637;&#32570;&#22833;&#19977;&#20803;&#32452;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20316;GenKGC&#30340;&#26041;&#27861;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#36716;&#21270;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#20851;&#31995;&#24341;&#23548;&#28436;&#31034;&#21644;&#23454;&#20307;&#24863;&#30693;&#20998;&#23618;&#35299;&#30721;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#24555;&#36895;&#25512;&#26029;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#27604;&#22522;&#32447;&#26356;&#22909;&#25110;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20197;&#21069;&#20855;&#26377;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23454;&#29616;&#26356;&#24555;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#20013;&#25991;&#30693;&#35782;&#22270;&#35889;&#25968;&#25454;&#38598;AliopenKG500&#65292;&#20379;&#30740;&#31350;&#30446;&#30340;&#20351;&#29992;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/zjunlp/PromptKG/tree/main/GenKGC&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion aims to address the problem of extending a KG with missing triples. In this paper, we provide an approach GenKGC, which converts knowledge graph completion to sequence-to-sequence generation task with the pre-trained language model. We further introduce relation-guided demonstration and entity-aware hierarchical decoding for better representation learning and fast inference. Experimental results on three datasets show that our approach can obtain better or comparable performance than baselines and achieve faster inference speed compared with previous methods with pre-trained language models. We also release a new large-scale Chinese knowledge graph dataset AliopenKG500 for research purpose. Code and datasets are available in https://github.com/zjunlp/PromptKG/tree/main/GenKGC.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KGAN&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#32593;&#32476;&#65292;&#23558;&#22806;&#37096;&#30693;&#35782;&#21644;&#19978;&#19979;&#25991;&#12289;&#21477;&#27861;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#20174;&#22810;&#20010;&#35282;&#24230;&#25429;&#33719;&#24773;&#24863;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22810;&#35270;&#35282;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2201.04831</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#32593;&#32476;&#30340;&#22810;&#35270;&#35282;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Augmented Network Towards Multiview Representation Learning for Aspect-based Sentiment Analysis. (arXiv:2201.04831v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.04831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KGAN&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#32593;&#32476;&#65292;&#23558;&#22806;&#37096;&#30693;&#35782;&#21644;&#19978;&#19979;&#25991;&#12289;&#21477;&#27861;&#20449;&#24687;&#30456;&#32467;&#21512;&#65292;&#20174;&#22810;&#20010;&#35282;&#24230;&#25429;&#33719;&#24773;&#24863;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22810;&#35270;&#35282;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#26159;&#24773;&#24863;&#20998;&#26512;&#30340;&#19968;&#39033;&#32454;&#31890;&#24230;&#20219;&#21153;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#38271;&#21477;&#23376;&#24182;&#33719;&#21462;&#20934;&#30830;&#30340;&#26041;&#38754;&#29305;&#23450;&#20449;&#24687;&#65292;&#36890;&#24120;&#38656;&#35201;&#35821;&#35328;&#21644;&#24120;&#35782;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37319;&#29992;&#22797;&#26434;&#21644;&#20302;&#25928;&#30340;&#26041;&#27861;&#26469;&#21253;&#21547;&#22806;&#37096;&#30693;&#35782;&#65292;&#20363;&#22914;&#30452;&#25509;&#25628;&#32034;&#22270;&#24418;&#33410;&#28857;&#12290;&#27492;&#22806;&#65292;&#22806;&#37096;&#30693;&#35782;&#21644;&#35821;&#35328;&#20449;&#24687;&#20043;&#38388;&#30340;&#20114;&#34917;&#24615;&#36824;&#27809;&#26377;&#24471;&#21040;&#24443;&#24213;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#22270;&#22686;&#24378;&#32593;&#32476;KGAN&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#23558;&#22806;&#37096;&#30693;&#35782;&#19982;&#26126;&#30830;&#30340;&#21477;&#27861;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#30456;&#32467;&#21512;&#12290;&#29305;&#21035;&#22320;&#65292;KGAN&#20174;&#22810;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#25429;&#33719;&#24773;&#24863;&#29305;&#24449;&#34920;&#31034;&#65292;&#21363;&#22522;&#20110;&#19978;&#19979;&#25991;&#12289;&#21477;&#27861;&#21644;&#30693;&#35782;&#30340;&#12290;&#39318;&#20808;&#65292;KGAN&#24182;&#34892;&#23398;&#20064;&#19978;&#19979;&#25991;&#21644;&#21477;&#27861;&#34920;&#31034;&#65292;&#20197;&#20805;&#20998;&#25552;&#21462;&#35821;&#20041;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;KGAN&#23558;&#22806;&#37096;&#30693;&#35782;&#19982;&#19978;&#19979;&#25991;&#21644;&#21477;&#27861;&#20449;&#24687;&#30456;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment analysis (ABSA) is a fine-grained task of sentiment analysis. To better comprehend long complicated sentences and obtain accurate aspect-specific information, linguistic and commonsense knowledge are generally required in this task. However, most current methods employ complicated and inefficient approaches to incorporate external knowledge, e.g., directly searching the graph nodes. Additionally, the complementarity between external knowledge and linguistic information has not been thoroughly studied. To this end, we propose a knowledge graph augmented network KGAN, which aims to effectively incorporate external knowledge with explicitly syntactic and contextual information. In particular, KGAN captures the sentiment feature representations from multiple different perspectives, i.e., context-, syntaxand knowledge-based. First, KGAN learns the contextual and syntactic representations in parallel to fully extract the semantic features. Then, KGAN integrates the k
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32858;&#31867;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;DECAR&#65292;&#29992;&#20110;&#23398;&#20064;&#36890;&#29992;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#24314;&#31435;&#22312;&#20808;&#21069;&#33258;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#30784;&#20043;&#19978;&#65292;&#21033;&#29992;&#31163;&#32447;&#32858;&#31867;&#30340;&#20266;&#26631;&#31614;&#26469;&#35299;&#20915;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;Audioset&#25968;&#25454;&#38598;&#30340;&#24179;&#34913;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2110.08895</link><description>&lt;p&gt;
DECAR: &#22522;&#20110;&#28145;&#24230;&#32858;&#31867;&#30340;&#36890;&#29992;&#38899;&#39057;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DECAR: Deep Clustering for learning general-purpose Audio Representations. (arXiv:2110.08895v4 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.08895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#32858;&#31867;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;DECAR&#65292;&#29992;&#20110;&#23398;&#20064;&#36890;&#29992;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#24314;&#31435;&#22312;&#20808;&#21069;&#33258;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#30784;&#20043;&#19978;&#65292;&#21033;&#29992;&#31163;&#32447;&#32858;&#31867;&#30340;&#20266;&#26631;&#31614;&#26469;&#35299;&#20915;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;Audioset&#25968;&#25454;&#38598;&#30340;&#24179;&#34913;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DECAR&#65292;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#38899;&#39057;&#36890;&#29992;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22522;&#20110;&#32858;&#31867;&#65292;&#21033;&#29992;&#31163;&#32447;&#32858;&#31867;&#27493;&#39588;&#25552;&#20379;&#30446;&#26631;&#26631;&#31614;&#65292;&#20316;&#20026;&#20266;&#26631;&#31614;&#26469;&#35299;&#20915;&#39044;&#27979;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22522;&#30784;&#19978;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#12289;&#26131;&#20110;&#20351;&#29992;&#30340;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;Audioset&#25968;&#25454;&#38598;&#30340;&#24179;&#34913;&#23376;&#38598;&#19978;&#39044;&#35757;&#32451;DECAR&#23884;&#20837;&#65292;&#24182;&#23558;&#36825;&#20123;&#34920;&#31034;&#20256;&#36882;&#21040;9&#20010;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#38899;&#12289;&#38899;&#20048;&#12289;&#21160;&#29289;&#22768;&#38899;&#21644;&#22768;&#23398;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#24182;&#20844;&#24320;&#20102;&#25152;&#26377;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce DECAR, a self-supervised pre-training approach for learning general-purpose audio representations. Our system is based on clustering: it utilizes an offline clustering step to provide target labels that act as pseudo-labels for solving a prediction task. We develop on top of recent advances in self-supervised learning for computer vision and design a lightweight, easy-to-use self-supervised pre-training scheme. We pre-train DECAR embeddings on a balanced subset of the large-scale Audioset dataset and transfer those representations to 9 downstream classification tasks, including speech, music, animal sounds, and acoustic scenes. Furthermore, we conduct ablation studies identifying key design choices and also make all our code and pre-trained models publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#35757;&#32451;&#20026;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;&#25968;&#23398;&#26041;&#31243;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#24182;&#24494;&#35843;Transformer&#27169;&#22411;&#35299;&#20915;&#31526;&#21495;&#25968;&#23398;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#30340;&#35757;&#32451;&#26679;&#26412;&#27604;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23569;1.5&#20010;&#25968;&#37327;&#32423;&#65292;&#19988;&#22312;&#31215;&#20998;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2110.03501</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20063;&#26159;&#31526;&#21495;&#25968;&#23398;&#27714;&#35299;&#22120;&#65281;
&lt;/p&gt;
&lt;p&gt;
Pretrained Language Models are Symbolic Mathematics Solvers too!. (arXiv:2110.03501v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#35757;&#32451;&#20026;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;&#25968;&#23398;&#26041;&#31243;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#24182;&#24494;&#35843;Transformer&#27169;&#22411;&#35299;&#20915;&#31526;&#21495;&#25968;&#23398;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#30340;&#35757;&#32451;&#26679;&#26412;&#27604;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23569;1.5&#20010;&#25968;&#37327;&#32423;&#65292;&#19988;&#22312;&#31215;&#20998;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#31526;&#21495;&#25968;&#23398;&#38382;&#39064;&#19968;&#30452;&#26159;&#38656;&#35201;&#32452;&#21512;&#25512;&#29702;&#21644;&#37325;&#22797;&#30340;&#20154;&#31867;&#21019;&#36896;&#21147;&#30340;&#39046;&#22495;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35832;&#22914;transformer&#20043;&#31867;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#36890;&#29992;&#30340;&#65292;&#24182;&#19988;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#20204;&#21487;&#20197;&#34987;&#35757;&#32451;&#20026;&#29992;&#20110;&#35299;&#20915;&#22797;&#26434;&#30340;&#25968;&#23398;&#26041;&#31243;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#12290;&#36825;&#20123;&#22823;&#22411;Transformer&#27169;&#22411;&#38656;&#35201;&#26497;&#20854;&#24222;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#31526;&#21495;&#25968;&#23398;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26679;&#26412;&#26377;&#25928;&#30340;&#26041;&#24335;&#26469;&#35299;&#20915;&#31526;&#21495;&#20219;&#21153;&#65292;&#39318;&#20808;&#36890;&#36807;&#35821;&#35328;&#32763;&#35793;&#23545;Transformer&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#20197;&#35299;&#20915;&#31526;&#21495;&#25968;&#23398;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#31215;&#20998;&#20219;&#21153;&#19978;&#20351;&#29992;&#20102;&#22823;&#32422;1.5&#20010;&#25968;&#37327;&#32423;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#36798;&#21040;&#20102;&#19982;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19982;&#38024;&#23545;&#31526;&#21495;&#25968;&#23398;&#30340;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#25216;&#26415;&#30456;&#27604;&#20351;&#29992;&#20102;&#36739;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#24494;&#20998;&#26041;&#31243;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Solving symbolic mathematics has always been of in the arena of human ingenuity that needs compositional reasoning and recurrence. However, recent studies have shown that large-scale language models such as transformers are universal and surprisingly can be trained as a sequence-to-sequence task to solve complex mathematical equations. These large transformer models need humongous amounts of training data to generalize to unseen symbolic mathematics problems. In this paper, we present a sample efficient way of solving the symbolic tasks by first pretraining the transformer model with language translation and then fine-tuning the pretrained transformer model to solve the downstream task of symbolic mathematics. We achieve comparable accuracy on the integration task with our pretrained model while using around $1.5$ orders of magnitude less number of training samples with respect to the state-of-the-art deep learning for symbolic mathematics. The test accuracy on differential equation ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32473;&#27169;&#22411;&#31532;&#20108;&#27425;&#12289;&#31532;&#19977;&#27425;&#29978;&#33267;&#31532;k&#27425;&#24605;&#32771;&#26426;&#20250;&#30340;&#24605;&#36335;&#27969;&#32593;&#32476;&#65292;&#20854;&#21033;&#29992;&#33258;&#25105;&#26657;&#27491;&#26426;&#21046;&#21644;&#26799;&#24230;&#26356;&#26032;&#33021;&#22815;&#32416;&#27491;&#33258;&#36523;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2107.12220</link><description>&lt;p&gt;
&#24605;&#36335;&#27969;&#32593;&#32476;&#65306;&#20174;&#21333;&#19968;&#39044;&#27979;&#21040;&#27169;&#22411;&#24605;&#36335;&#30340;&#20018;&#32852;
&lt;/p&gt;
&lt;p&gt;
Thought Flow Nets: From Single Predictions to Trains of Model Thought. (arXiv:2107.12220v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.12220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32473;&#27169;&#22411;&#31532;&#20108;&#27425;&#12289;&#31532;&#19977;&#27425;&#29978;&#33267;&#31532;k&#27425;&#24605;&#32771;&#26426;&#20250;&#30340;&#24605;&#36335;&#27969;&#32593;&#32476;&#65292;&#20854;&#21033;&#29992;&#33258;&#25105;&#26657;&#27491;&#26426;&#21046;&#21644;&#26799;&#24230;&#26356;&#26032;&#33021;&#22815;&#32416;&#27491;&#33258;&#36523;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#21487;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#31867;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26102;&#65292;&#36890;&#24120;&#20250;&#21019;&#24314;&#19968;&#31995;&#21015;&#24605;&#36335;&#65288;&#28041;&#21450;&#30452;&#35273;&#20915;&#31574;&#12289;&#21453;&#24605;&#12289;&#38169;&#35823;&#26356;&#27491;&#31561;&#65289;&#20197;&#36798;&#25104;&#20915;&#23450;&#12290;&#20294;&#26159;&#65292;&#22914;&#20170;&#30340;&#27169;&#22411;&#22823;&#22810;&#34987;&#35757;&#32451;&#20026;&#23558;&#36755;&#20837;&#26144;&#23556;&#21040;&#21333;&#19968;&#19988;&#22266;&#23450;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#35753;&#27169;&#22411;&#26377;&#31532;&#20108;&#12289;&#31532;&#19977;&#21644;&#31532; k &#27425;&#24605;&#32771;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#20174;&#40657;&#26684;&#23572;&#30340;&#36777;&#35777;&#27861;&#20013;&#33719;&#24471;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;&#24605;&#36335;&#27969;&#30340;&#27010;&#24565;&#65292;&#21019;&#24314;&#20102;&#19968;&#31995;&#21015;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#26657;&#27491;&#26426;&#21046;&#65292;&#23427;&#34987;&#35757;&#32451;&#29992;&#20110;&#20272;&#35745;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#22522;&#20110;&#27491;&#30830;&#24615;&#39044;&#27979;&#30340;&#26799;&#24230;&#25191;&#34892;&#36845;&#20195;&#39044;&#27979;&#26356;&#26032;&#12290;&#25105;&#20204;&#20197;&#38382;&#31572;&#20026;&#20363;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#65288;i&#65289;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#32416;&#27491;&#33258;&#24049;&#30340;&#39044;&#27979;&#65292;&#65288;ii&#65289;&#23427;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#24605;&#36335;&#27969;&#30340;&#35821;&#20041;&#26657;&#39564;&#36827;&#34892;&#20102;&#23450;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
When humans solve complex problems, they typically create a sequence of ideas (involving an intuitive decision, reflection, error correction, etc.) in order to reach a conclusive decision. Contrary to this, today's models are mostly trained to map an input to one single and fixed output. In this paper, we investigate how we can give models the opportunity of a second, third and $k$-th thought. Taking inspiration from Hegel's dialectics, we propose the concept of a thought flow which creates a sequence of predictions. We present a self-correction mechanism that is trained to estimate the model's correctness and performs iterative prediction updates based on the correctness prediction's gradient. We introduce our method at the example of question answering and conduct extensive experiments that demonstrate (i) our method's ability to correct its own predictions and (ii) its potential to notably improve model performances. In addition, we conduct a qualitative analysis of thought flow cor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#22522;&#20110;&#33258;&#25105;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24207;&#21015;&#32534;&#30721;&#22120;&#34920;&#31034;&#30340;&#26041;&#24335;&#65292;&#23558;&#20854;&#35270;&#20026;&#22810;&#38454;&#22270;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#38454;&#22270;&#30340;&#27169;&#22411;&#26469;&#25551;&#36848;&#36825;&#20123;&#22270;&#32467;&#26500;&#65292;&#24182;&#23558;SAN&#27169;&#22411;&#30340;&#32534;&#30721;&#36716;&#25442;&#20026;MoG&#30340;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Graph-Transformer&#30340;&#27169;&#22411;&#26469;&#22686;&#24378;&#25429;&#33719;&#22810;&#20010;&#19981;&#21516;&#38454;&#32423;&#30340;&#23376;&#22270;&#30340;&#33021;&#21147;&#65292;&#24182;&#20851;&#27880;&#39640;&#38454;&#23376;&#22270;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2101.06397</link><description>&lt;p&gt;
&#29702;&#35299;&#22522;&#20110;&#23618;&#24863;&#30693;&#24207;&#21015;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#20026;&#22810;&#38454;&#22270;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
To Understand Representation of Layer-aware Sequence Encoders as Multi-order-graph. (arXiv:2101.06397v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.06397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#22522;&#20110;&#33258;&#25105;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24207;&#21015;&#32534;&#30721;&#22120;&#34920;&#31034;&#30340;&#26041;&#24335;&#65292;&#23558;&#20854;&#35270;&#20026;&#22810;&#38454;&#22270;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#38454;&#22270;&#30340;&#27169;&#22411;&#26469;&#25551;&#36848;&#36825;&#20123;&#22270;&#32467;&#26500;&#65292;&#24182;&#23558;SAN&#27169;&#22411;&#30340;&#32534;&#30721;&#36716;&#25442;&#20026;MoG&#30340;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Graph-Transformer&#30340;&#27169;&#22411;&#26469;&#22686;&#24378;&#25429;&#33719;&#22810;&#20010;&#19981;&#21516;&#38454;&#32423;&#30340;&#23376;&#22270;&#30340;&#33021;&#21147;&#65292;&#24182;&#20851;&#27880;&#39640;&#38454;&#23376;&#22270;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;SAN&#65289;&#24207;&#21015;&#32534;&#30721;&#22120;&#34920;&#31034;&#30340;&#35299;&#37322;&#26041;&#24335;&#65292;&#23558;&#27169;&#22411;&#25429;&#33719;&#30340;&#20449;&#24687;&#21644;&#27169;&#22411;&#30340;&#32534;&#30721;&#20998;&#21035;&#35270;&#20026;&#22270;&#32467;&#26500;&#21644;&#36825;&#20123;&#22270;&#32467;&#26500;&#30340;&#29983;&#25104;&#12290;&#35813;&#35299;&#37322;&#36866;&#29992;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;SAN&#30340;&#27169;&#22411;&#65292;&#24182;&#21487;&#20197;&#35299;&#37322;&#25429;&#33719;&#32467;&#26500;&#25110;&#35821;&#35328;&#20449;&#24687;&#30340;&#33021;&#21147;&#12289;&#27169;&#22411;&#28145;&#24230;&#21644;&#21477;&#23376;&#38271;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an explanation of representation for self-attention network (SAN) based neural sequence encoders, which regards the information captured by the model and the encoding of the model as graph structure and the generation of these graph structures respectively. The proposed explanation applies to existing works on SAN-based models and can explain the relationship among the ability to capture the structural or linguistic information, depth of model, and length of sentence, and can also be extended to other models such as recurrent neural network based models. We also propose a revisited multigraph called Multi-order-Graph (MoG) based on our explanation to model the graph structures in the SAN-based model as subgraphs in MoG and convert the encoding of SAN-based model to the generation of MoG. Based on our explanation, we further introduce a Graph-Transformer by enhancing the ability to capture multiple subgraphs of different orders and focusing on subgraphs of high
&lt;/p&gt;</description></item></channel></rss>