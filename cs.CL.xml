<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;ChatGPT&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#24320;&#25918;&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#21512;&#25104;&#27867;&#21270;&#33021;&#21147;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2308.13517</link><description>&lt;p&gt;
ChatGPT&#20316;&#20026;&#21512;&#25104;&#27867;&#21270;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#22312;&#24320;&#25918;&#24847;&#22270;&#26816;&#27979;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ChatGPT as Data Augmentation for Compositional Generalization: A Case Study in Open Intent Detection. (arXiv:2308.13517v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;ChatGPT&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#25552;&#39640;&#20102;&#24320;&#25918;&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#21512;&#25104;&#27867;&#21270;&#33021;&#21147;&#65292;&#26377;&#25928;&#25913;&#21892;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24847;&#22270;&#26816;&#27979;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#28041;&#21450;&#23545;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#24847;&#22270;&#36827;&#34892;&#35782;&#21035;&#12290;&#34429;&#28982;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#26159;&#22312;&#22788;&#29702;&#35821;&#35328;&#32452;&#25104;&#25104;&#20998;&#30340;&#26032;&#32452;&#21512;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#23545;&#20110;&#21512;&#25104;&#27867;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22312;&#24320;&#25918;&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#23558;ChatGPT&#20316;&#20026;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#39640;&#21512;&#25104;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#29616;&#26377;&#22522;&#20934;&#22312;&#35780;&#20272;&#36825;&#20010;&#38382;&#39064;&#26102;&#30340;&#23616;&#38480;&#24615;&#65292;&#24378;&#35843;&#20102;&#26500;&#24314;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#24320;&#25918;&#24847;&#22270;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#21512;&#25104;&#27867;&#21270;&#38382;&#39064;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#23558;ChatGPT&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#32435;&#20837;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24320;&#25918;&#24847;&#22270;&#26816;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open intent detection, a crucial aspect of natural language understanding, involves the identification of previously unseen intents in user-generated text. Despite the progress made in this field, challenges persist in handling new combinations of language components, which is essential for compositional generalization. In this paper, we present a case study exploring the use of ChatGPT as a data augmentation technique to enhance compositional generalization in open intent detection tasks. We begin by discussing the limitations of existing benchmarks in evaluating this problem, highlighting the need for constructing datasets for addressing compositional generalization in open intent detection tasks. By incorporating synthetic data generated by ChatGPT into the training process, we demonstrate that our approach can effectively improve model performance. Rigorous evaluation of multiple benchmarks reveals that our method outperforms existing techniques and significantly enhances open inte
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#22914;&#20309;&#22312;&#27573;&#33853;&#32423;&#21035;&#35780;&#20272;&#32763;&#35793;&#36136;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21477;&#23376;&#32423;&#21035;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20998;&#25972;&#20010;&#27573;&#33853;&#19982;&#20351;&#29992;&#27573;&#33853;&#32423;&#21035;&#30340;&#25351;&#26631;&#19968;&#26679;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2308.13506</link><description>&lt;p&gt;
&#22312;&#27573;&#33853;&#32423;&#21035;&#19978;&#35757;&#32451;&#21644;&#20803;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Training and Meta-Evaluating Machine Translation Evaluation Metrics at the Paragraph Level. (arXiv:2308.13506v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#65292;&#22914;&#20309;&#22312;&#27573;&#33853;&#32423;&#21035;&#35780;&#20272;&#32763;&#35793;&#36136;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21477;&#23376;&#32423;&#21035;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20998;&#25972;&#20010;&#27573;&#33853;&#19982;&#20351;&#29992;&#27573;&#33853;&#32423;&#21035;&#30340;&#25351;&#26631;&#19968;&#26679;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#32763;&#35793;&#30740;&#31350;&#30340;&#21457;&#23637;&#65292;&#23558;&#25991;&#26412;&#32763;&#35793;&#21040;&#21477;&#23376;&#20197;&#19978;&#30340;&#32423;&#21035;&#65292;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#22312;&#35780;&#20998;&#26356;&#38271;&#30340;&#32763;&#35793;&#19978;&#30340;&#26377;&#25928;&#24615;&#20173;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29616;&#26377;&#30340;&#21477;&#23376;&#32423;&#21035;&#25968;&#25454;&#21019;&#24314;&#27573;&#33853;&#32423;&#21035;&#30340;&#25968;&#25454;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#20803;&#35780;&#20272;&#25351;&#26631;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#26032;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#29616;&#26377;&#30340;&#21477;&#23376;&#32423;&#21035;&#25351;&#26631;&#65292;&#24182;&#22312;&#27573;&#33853;&#32423;&#21035;&#19978;&#35757;&#32451;&#23398;&#20064;&#25351;&#26631;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#21477;&#23376;&#32423;&#21035;&#30340;&#25351;&#26631;&#26469;&#35780;&#20998;&#25972;&#20010;&#27573;&#33853;&#19982;&#20351;&#29992;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#27573;&#33853;&#32423;&#21035;&#24037;&#20316;&#30340;&#25351;&#26631;&#19968;&#26679;&#26377;&#25928;&#12290;&#25105;&#20204;&#25512;&#27979;&#36825;&#20010;&#32467;&#26524;&#21487;&#33021;&#24402;&#22240;&#20110;&#21442;&#32771;&#35780;&#20272;&#20219;&#21153;&#30340;&#29305;&#24615;&#20197;&#21450;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#22312;&#25429;&#25417;&#27573;&#33853;&#32423;&#21035;&#32763;&#35793;&#20013;&#20986;&#29616;&#30340;&#25152;&#26377;&#31867;&#22411;&#29616;&#35937;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As research on machine translation moves to translating text beyond the sentence level, it remains unclear how effective automatic evaluation metrics are at scoring longer translations. In this work, we first propose a method for creating paragraph-level data for training and meta-evaluating metrics from existing sentence-level data. Then, we use these new datasets to benchmark existing sentence-level metrics as well as train learned metrics at the paragraph level. Interestingly, our experimental results demonstrate that using sentence-level metrics to score entire paragraphs is equally as effective as using a metric designed to work at the paragraph level. We speculate this result can be attributed to properties of the task of reference-based evaluation as well as limitations of our datasets with respect to capturing all types of phenomena that occur in paragraph-level translations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Ngambay-French&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#21019;&#36896;&#20102;&#31532;&#19968;&#20010;sba-Fr&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2308.13497</link><description>&lt;p&gt;
Ngambay-French&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;sba-Fr&#65289;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Ngambay-French Neural Machine Translation (sba-Fr). (arXiv:2308.13497v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13497
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Ngambay-French&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#24182;&#21019;&#36896;&#20102;&#31532;&#19968;&#20010;sba-Fr&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38750;&#27954;&#21644;&#20840;&#29699;&#33539;&#22260;&#20869;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#20110;&#24320;&#21457;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31995;&#32479;&#65292;&#20197;&#20811;&#26381;&#35821;&#35328;&#38556;&#30861;&#12290;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;NMT&#23588;&#20026;&#24341;&#20154;&#27880;&#30446;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#26469;&#35828;&#65292;&#33719;&#21462;&#19968;&#20010;&#33391;&#22909;&#23545;&#40784;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#21487;&#33021;&#26377;&#25361;&#25112;&#24615;&#12290;&#26597;&#24503;&#22320;&#21306;&#20840;&#29699;&#23569;&#25968;&#35821;&#35328;&#30340;&#25216;&#26415;&#20808;&#36827;&#31243;&#24230;&#19982;NMT&#30740;&#31350;&#30340;&#19981;&#36275;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#24046;&#36317;&#12290;&#23578;&#26410;&#23581;&#35797;&#36807;&#22312;&#20302;&#36164;&#28304;&#26597;&#24503;&#35821;&#35328;&#19978;&#36827;&#34892;&#31471;&#21040;&#31471;NMT&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#19982;&#19968;&#20123;&#38750;&#27954;&#35821;&#35328;&#19981;&#21516;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#22312;&#32447;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#25910;&#38598;&#30340;&#25968;&#25454;&#21294;&#20047;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#25351;&#23548;&#24615;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#21487;&#20197;&#20135;&#29983;&#35768;&#22810;&#26597;&#24503;&#35821;&#35328;&#32763;&#35793;&#23545;&#30340;&#21452;&#35821;&#25968;&#25454;&#65292;&#20854;&#20013;&#21478;&#19968;&#31181;&#35821;&#35328;&#26159;&#26377;&#22823;&#37327;&#25968;&#25454;&#30340;&#30693;&#21517;&#35821;&#35328;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;sba-Fr&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#19968;&#20010;Ngambay&#21040;French&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Africa, and the world at large, there is an increasing focus on developing Neural Machine Translation (NMT) systems to overcome language barriers. NMT for Low-resource language is particularly compelling as it involves learning with limited labelled data. However, obtaining a well-aligned parallel corpus for low-resource languages can be challenging. The disparity between the technological advancement of a few global languages and the lack of research on NMT for local languages in Chad is striking. End-to-end NMT trials on low-resource Chad languages have not been attempted. Additionally, there is a dearth of online and well-structured data gathering for research in Natural Language Processing, unlike some African languages. However, a guided approach for data gathering can produce bitext data for many Chadian language translation pairs with well-known languages that have ample data. In this project, we created the first sba-Fr Dataset, which is a corpus of Ngambay-to-French transla
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#20247;&#21253;&#27969;&#31243;&#30340;&#25552;&#31034;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#28608;&#21169;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;&#20351;&#29992;&#20247;&#21253;&#27969;&#31243;&#30340;&#25552;&#31034;&#65292;GPT-4&#29983;&#25104;&#30340;&#20449;&#24687;&#27604;&#22522;&#20934;&#25552;&#31034;&#26356;&#21152;&#22810;&#26679;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.13479</link><description>&lt;p&gt;
&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#28608;&#21169;&#20449;&#24687;&#65306;&#19982;&#20154;&#24037;&#32534;&#20889;&#20449;&#24687;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Prompting a Large Language Model to Generate Diverse Motivational Messages: A Comparison with Human-Written Messages. (arXiv:2308.13479v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#20247;&#21253;&#27969;&#31243;&#30340;&#25552;&#31034;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#28608;&#21169;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36890;&#36807;&#20351;&#29992;&#20247;&#21253;&#27969;&#31243;&#30340;&#25552;&#31034;&#65292;GPT-4&#29983;&#25104;&#30340;&#20449;&#24687;&#27604;&#22522;&#20934;&#25552;&#31034;&#26356;&#21152;&#22810;&#26679;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#36234;&#26469;&#36234;&#24378;&#22823;&#65292;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#29983;&#25104;&#21019;&#24847;&#20869;&#23481;&#12290;&#20869;&#23481;&#30340;&#36136;&#37327;&#21463;&#21040;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#26356;&#20855;&#20307;&#19988;&#21253;&#21547;&#31034;&#20363;&#30340;&#25552;&#31034;&#36890;&#24120;&#33021;&#20135;&#29983;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#21487;&#20197;&#30475;&#20986;&#20351;&#29992;&#20026;&#20247;&#21253;&#20219;&#21153;&#32534;&#20889;&#30340;&#25351;&#20196;&#65288;&#20855;&#20307;&#19988;&#21253;&#21547;&#31034;&#20363;&#20197;&#25351;&#23548;&#24037;&#20316;&#32773;&#65289;&#21487;&#33021;&#25104;&#20026;&#26377;&#25928;&#30340;LLM&#25552;&#31034;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20351;&#29992;&#20043;&#21069;&#30340;&#20247;&#21253;&#27969;&#31243;&#65292;&#20026;&#20154;&#20204;&#25552;&#20379;&#31034;&#20363;&#26469;&#24110;&#21161;&#29983;&#25104;&#19968;&#20010;&#20855;&#26377;&#22810;&#26679;&#24615;&#30340;&#28608;&#21169;&#20449;&#24687;&#35821;&#26009;&#24211;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#30456;&#21516;&#30340;&#27969;&#31243;&#20351;&#29992;GPT-4&#29983;&#25104;&#20449;&#24687;&#65292;&#24182;&#27604;&#36739;&#20102;&#26469;&#33258;&#20197;&#19979;&#19981;&#21516;&#25552;&#31034;&#30340;&#20449;&#24687;&#30340;&#22810;&#26679;&#24615;&#65306;&#65288;1&#65289;&#20247;&#21253;&#20889;&#25163;&#65292;&#65288;2&#65289;&#20351;&#29992;&#35813;&#27969;&#31243;&#30340;GPT-4&#65292;&#20197;&#21450;&#65288;3&#21644;4&#65289;&#20004;&#20010;&#22522;&#20934;GPT-4&#25552;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#20247;&#21253;&#27969;&#31243;&#30340;LLM&#25552;&#31034;&#23548;&#33268;GPT-4&#29983;&#25104;&#27604;&#20004;&#20010;&#22522;&#20934;&#25552;&#31034;&#26356;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#20449;&#24687;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly capable and prevalent, and can be used to produce creative content. The quality of content is influenced by the prompt used, with more specific prompts that incorporate examples generally producing better results. On from this, it could be seen that using instructions written for crowdsourcing tasks (that are specific and include examples to guide workers) could prove effective LLM prompts. To explore this, we used a previous crowdsourcing pipeline that gave examples to people to help them generate a collectively diverse corpus of motivational messages. We then used this same pipeline to generate messages using GPT-4, and compared the collective diversity of messages from: (1) crowd-writers, (2) GPT-4 using the pipeline, and (3 &amp; 4) two baseline GPT-4 prompts. We found that the LLM prompts using the crowdsourcing pipeline caused GPT-4 to produce more diverse messages than the two baseline prompts. We also discuss implications from messages 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#30693;&#35782;&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#26469;&#24357;&#34917;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#32570;&#22833;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13467</link><description>&lt;p&gt;
&#21033;&#29992;&#30693;&#35782;&#21644;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Knowledge and Reinforcement Learning for Enhanced Reliability of Language Models. (arXiv:2308.13467v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#30693;&#35782;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#30693;&#35782;&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#65292;&#36890;&#36807;&#25972;&#21512;&#22806;&#37096;&#30693;&#35782;&#26469;&#24357;&#34917;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#32570;&#22833;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#31038;&#21306;&#19968;&#30452;&#22312;&#20351;&#29992;&#20247;&#21253;&#25216;&#26415;&#65292;&#21019;&#24314;&#29992;&#20110;&#35757;&#32451;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#22914;BERT&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;General Language Understanding and Evaluation(GLUE)&#12290;GLUE&#20219;&#21153;&#20351;&#29992;&#20114;&#35780;&#35745;&#37327;&#26041;&#27861;&#65288;&#22914;Cohens Kappa&#65289;&#26469;&#34913;&#37327;&#21487;&#38752;&#24615;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#26041;&#38754;&#24120;&#24120;&#34987;&#24573;&#35270;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#23558;ConceptNet&#21644;&#32500;&#22522;&#30334;&#31185;&#30340;&#30693;&#35782;&#20316;&#20026;&#30693;&#35782;&#22270;&#23884;&#20837;&#36827;&#34892;&#25972;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#27169;&#20223;&#20102;&#20154;&#31867;&#27880;&#37322;&#32773;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#24357;&#34917;&#25968;&#25454;&#38598;&#20013;&#30340;&#20449;&#24687;&#32570;&#22833;&#12290;&#36890;&#36807;&#22312;&#20061;&#20010;GLUE&#25968;&#25454;&#38598;&#19978;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21487;&#20197;&#22686;&#24378;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#24471;&#20998;&#65292;&#36229;&#36807;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Natural Language Processing(NLP) community has been using crowd sourcing techniques to create benchmark datasets such as General Language Understanding and Evaluation(GLUE) for training modern Language Models such as BERT. GLUE tasks measure the reliability scores using inter annotator metrics i.e. Cohens Kappa. However, the reliability aspect of LMs has often been overlooked. To counter this problem, we explore a knowledge-guided LM ensembling approach that leverages reinforcement learning to integrate knowledge from ConceptNet and Wikipedia as knowledge graph embeddings. This approach mimics human annotators resorting to external knowledge to compensate for information deficits in the datasets. Across nine GLUE datasets, our research shows that ensembling strengthens reliability and accuracy scores, outperforming state of the art.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#26412;&#31616;&#21270;&#65292;&#35774;&#35745;&#21644;&#23454;&#29616;&#20102;&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#25991;&#26412;&#31616;&#21270;&#27969;&#31243;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;AI&#25216;&#26415;&#23454;&#29616;&#20102;&#25991;&#26412;&#30340;&#31616;&#21270;&#65292;&#20197;&#28385;&#36275;&#20844;&#27665;&#33719;&#21462;&#20844;&#20849;&#20449;&#24687;&#21644;&#30693;&#35782;&#30340;&#38656;&#35201;&#12290;</title><link>http://arxiv.org/abs/2308.13458</link><description>&lt;p&gt;
ARTIST: &#31616;&#21270;&#25991;&#26412;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
ARTIST: ARTificial Intelligence for Simplified Text. (arXiv:2308.13458v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#26412;&#31616;&#21270;&#65292;&#35774;&#35745;&#21644;&#23454;&#29616;&#20102;&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#25991;&#26412;&#31616;&#21270;&#27969;&#31243;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;AI&#25216;&#26415;&#23454;&#29616;&#20102;&#25991;&#26412;&#30340;&#31616;&#21270;&#65292;&#20197;&#28385;&#36275;&#20844;&#27665;&#33719;&#21462;&#20844;&#20849;&#20449;&#24687;&#21644;&#30693;&#35782;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#20844;&#27665;&#26469;&#35828;&#65292;&#22797;&#26434;&#30340;&#25991;&#26412;&#26159;&#33719;&#21462;&#20844;&#20849;&#20449;&#24687;&#21644;&#30693;&#35782;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#32780;&#25991;&#26412;&#31616;&#21270;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#26088;&#22312;&#20943;&#23569;&#25991;&#26412;&#30340;&#35821;&#35328;&#22797;&#26434;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#21547;&#20041;&#12290;&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#33258;&#21160;&#25991;&#26412;&#31616;&#21270;&#25104;&#20026;&#21487;&#33021;&#65292;&#26080;&#35770;&#26159;&#22312;&#35789;&#27719;&#36824;&#26159;&#21477;&#27861;&#23618;&#38754;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24212;&#29992;&#36890;&#24120;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#65292;&#23545;&#20110;&#33655;&#20848;&#35821;&#31561;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#24212;&#29992;&#29983;&#25104;&#24335;AI&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#20102;&#35299;&#29978;&#23569;&#12290;&#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;&#23558;&#29983;&#25104;&#24335;&#25216;&#26415;&#24212;&#29992;&#20110;&#25991;&#26412;&#31616;&#21270;&#30340;&#30410;&#22788;&#21644;&#23616;&#38480;&#65292;&#24182;&#25552;&#20379;&#20197;&#19979;&#32467;&#26524;&#65306;1&#65289;&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#25991;&#26412;&#31616;&#21270;&#27969;&#31243;&#30340;&#35774;&#35745;&#21644;&#23454;&#26045;&#65292;&#23558;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#24335;&#25991;&#26412;&#31616;&#21270;&#27169;&#22411;&#12289;&#39046;&#22495;&#21644;&#35835;&#32773;&#36866;&#24212;&#24615;&#20197;&#21450;&#21487;&#35270;&#21270;&#27169;&#22359;&#32467;&#21512;&#22312;&#19968;&#36215;&#65307;2&#65289;ins
&lt;/p&gt;
&lt;p&gt;
Complex text is a major barrier for many citizens when accessing public information and knowledge. While often done manually, Text Simplification is a key Natural Language Processing task that aims for reducing the linguistic complexity of a text while preserving the original meaning. Recent advances in Generative Artificial Intelligence (AI) have enabled automatic text simplification both on the lexical and syntactical levels. However, as applications often focus on English, little is understood about the effectiveness of Generative AI techniques on low-resource languages such as Dutch. For this reason, we carry out empirical studies to understand the benefits and limitations of applying generative technologies for text simplification and provide the following outcomes: 1) the design and implementation for a configurable text simplification pipeline that orchestrates state-of-the-art generative text simplification models, domain and reader adaptation, and visualisation modules; 2) ins
&lt;/p&gt;</description></item><item><title>&#23545;&#40784;&#34892;&#20026;&#20250;&#27745;&#26579;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#38477;&#20302;&#24494;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.13449</link><description>&lt;p&gt;
&#23545;&#40784;&#30340;&#27602;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Poison of Alignment. (arXiv:2308.13449v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13449
&lt;/p&gt;
&lt;p&gt;
&#23545;&#40784;&#34892;&#20026;&#20250;&#27745;&#26579;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#38477;&#20302;&#24494;&#35843;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20869;&#23481;&#23433;&#20840;&#38382;&#39064;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#23545;&#40784;&#24050;&#32463;&#26174;&#31034;&#20986;&#38480;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#26377;&#24847;&#30340;&#26041;&#27861;&#26159;&#20026;&#20102;&#19981;&#35753;&#27169;&#22411;&#23545;&#26576;&#20123;&#29992;&#25143;&#36755;&#20837;&#20316;&#20986;&#21709;&#24212;&#65292;&#22312;&#35768;&#22810;&#29616;&#20195;&#24320;&#28304;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65288;&#22914;OpenAssistant&#25110;Guanaco&#65289;&#20013;&#37117;&#23384;&#22312;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35266;&#28857;&#65292;&#25351;&#20986;&#23545;&#40784;&#23545;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#30340;&#24615;&#33021;&#20135;&#29983;&#20102;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#23545;&#40784;&#30340;&#20316;&#29992;&#23601;&#20687;&#26159;&#23545;&#25351;&#20196;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27745;&#26579;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#23545;&#40784;&#30340;&#31572;&#26696;&#26174;&#33879;&#22320;&#38477;&#20302;&#20102;&#26368;&#32456;&#24494;&#35843;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#65288;&#22914;Big Bench&#65288;BBH&#65289;&#12289;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#65288;MMLU&#65289;&#12289;&#20154;&#24037;&#35780;&#20272;&#21644;&#27573;&#33853;&#31163;&#25955;&#25512;&#29702;&#65288;DROP&#65289;&#65289;&#19978;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#27809;&#26377;&#23545;&#40784;&#30340;&#24494;&#35843;&#27169;&#22411;&#19979;&#38477;&#20102;4-33%&#12290;
&lt;/p&gt;
&lt;p&gt;
From the perspective of content safety issues, alignment has shown to limit large language models' (LLMs) harmful content generation. This intentional method of reinforcing models to not respond to certain user inputs seem to be present in many modern open-source instruction tuning datasets such as OpenAssistant or Guanaco. We introduce a novel insight to an instruction-tuned model's performance affected by the presence of alignment in supervised fine-tuning dataset. To be specific, we noticed that alignment acts as if it is poisoning the instruction dataset. Experimentally, we demonstrate that aligned answers significantly worsen the performance of the resulting fine-tuned model's on various reasoning benchmarks such as Big Bench (BBH), Massive Multitask Language Understanding (MMLU), Human Eval, and Discrete Reasoning Over Paragraphs (DROP), performing worse than the counterpart tuned without alignment by 4-33%.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#20013;&#25552;&#21462;&#20855;&#26377;&#26368;&#39640;&#26465;&#20214;&#29109;&#30340;&#30701;&#35821;&#20316;&#20026;&#20851;&#38190;&#35789;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20851;&#38190;&#35789;&#25552;&#21462;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#19982;&#24120;&#29992;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13399</link><description>&lt;p&gt;
EntropyRank: &#36890;&#36807;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#21387;&#32553;&#30340;&#21103;&#20449;&#24687;&#20248;&#21270;&#26469;&#36827;&#34892;&#26080;&#30417;&#30563;&#20851;&#38190;&#35789;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression. (arXiv:2308.13399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13399
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20449;&#24687;&#35770;&#26041;&#27861;&#65292;&#22312;&#25991;&#26412;&#20013;&#25552;&#21462;&#20855;&#26377;&#26368;&#39640;&#26465;&#20214;&#29109;&#30340;&#30701;&#35821;&#20316;&#20026;&#20851;&#38190;&#35789;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20851;&#38190;&#35789;&#25552;&#21462;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#19982;&#24120;&#29992;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#21644;Shannon&#30340;&#20449;&#24687;&#26368;&#22823;&#21270;&#65292;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#38190;&#35789;&#21644;&#20851;&#38190;&#35789;&#30701;&#35821;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21462;&#22312;LM&#19979;&#20855;&#26377;&#26368;&#39640;&#26465;&#20214;&#29109;&#30340;&#30701;&#35821;&#12290;&#24471;&#21040;&#30340;&#20851;&#38190;&#35789;&#30701;&#35821;&#38598;&#21512;&#35299;&#20915;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#20449;&#24687;&#35770;&#38382;&#39064;&#65306;&#22914;&#26524;&#20316;&#20026;&#21103;&#20449;&#24687;&#25552;&#20379;&#65292;&#23427;&#20250;&#23548;&#33268;&#20351;&#29992;LM&#21644;&#29109;&#32534;&#30721;&#22120;&#23545;&#25991;&#26412;&#36827;&#34892;&#21387;&#32553;&#26102;&#30340;&#39044;&#26399;&#26368;&#23567;&#20108;&#36827;&#21046;&#30721;&#38271;&#24230;&#12290;&#21478;&#22806;&#65292;&#24471;&#21040;&#30340;&#38598;&#21512;&#26159;&#36890;&#36807;&#22240;&#26524;LM&#23545;&#22312;&#32473;&#23450;&#26465;&#20214;&#19979;&#26368;&#23567;&#21270;&#25991;&#26412;&#29109;&#30340;&#30701;&#35821;&#38598;&#21512;&#30340;&#36817;&#20284;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#20851;&#38190;&#35789;&#25552;&#21462;&#22522;&#20934;&#25361;&#25112;&#20013;&#25552;&#20379;&#20102;&#19982;&#26368;&#24120;&#29992;&#26041;&#27861;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an unsupervised method to extract keywords and keyphrases from texts based on a pre-trained language model (LM) and Shannon's information maximization. Specifically, our method extracts phrases having the highest conditional entropy under the LM. The resulting set of keyphrases turns out to solve a relevant information-theoretic problem: if provided as side information, it leads to the expected minimal binary code length in compressing the text using the LM and an entropy encoder. Alternately, the resulting set is an approximation via a causal LM to the set of phrases that minimize the entropy of the text when conditioned upon it. Empirically, the method provides results comparable to the most commonly used methods in various keyphrase extraction benchmark challenges.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25910;&#38598;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20013;&#23433;&#20840;&#26426;&#21046;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#19982;GPT-4&#22312;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#19978;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.13387</link><description>&lt;p&gt;
Do-Not-Answer: &#29992;&#20110;&#35780;&#20272;LLMs&#20013;&#23433;&#20840;&#26426;&#21046;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs. (arXiv:2308.13387v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13387
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25910;&#38598;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20013;&#23433;&#20840;&#26426;&#21046;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#35757;&#32451;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#19982;GPT-4&#22312;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#19978;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20986;&#29616;&#20102;&#26032;&#30340;&#38590;&#20197;&#39044;&#27979;&#30340;&#26377;&#23475;&#21151;&#33021;&#12290;&#36825;&#35201;&#27714;&#24320;&#21457;&#32773;&#33021;&#22815;&#36890;&#36807;&#35780;&#20272;LLMs&#20013;&#30340;&#8220;&#21361;&#38505;&#33021;&#21147;&#8221;&#26469;&#35782;&#21035;&#39118;&#38505;&#65292;&#20197;&#36127;&#36131;&#20219;&#22320;&#37096;&#32626;LLMs&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#31532;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20013;&#23433;&#20840;&#26426;&#21046;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#24182;&#20197;&#36739;&#20302;&#25104;&#26412;&#37096;&#32626;&#26356;&#23433;&#20840;&#30340;&#24320;&#28304;LLMs&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#30001;&#36127;&#36131;&#20219;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#24212;&#36981;&#24490;&#30340;&#25351;&#20196;&#31934;&#24515;&#31574;&#21010;&#21644;&#36807;&#28388;&#32780;&#25104;&#12290;&#25105;&#20204;&#23545;&#20845;&#31181;&#27969;&#34892;&#30340;LLMs&#23545;&#36825;&#20123;&#25351;&#20196;&#30340;&#22238;&#24212;&#36827;&#34892;&#20102;&#27880;&#37322;&#21644;&#35780;&#20272;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#26631;&#27880;&#65292;&#25105;&#20204;&#32487;&#32493;&#35757;&#32451;&#20102;&#20960;&#20010;&#31867;&#20284;BERT&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#23567;&#20998;&#31867;&#22120;&#22312;&#33258;&#21160;&#23433;&#20840;&#35780;&#20272;&#19978;&#21487;&#20197;&#36798;&#21040;&#19982;GPT-4&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#35686;&#21578;&#65306;&#26412;&#25991;&#21253;&#21547;&#21487;&#33021;&#20855;&#26377;&#20882;&#29359;&#24615;&#12289;&#26377;&#23475;&#24615;&#25110;&#20559;&#35265;&#24615;&#30340;&#31034;&#20363;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid evolution of large language models (LLMs), new and hard-to-predict harmful capabilities are emerging. This requires developers to be able to identify risks through the evaluation of "dangerous capabilities" in order to responsibly deploy LLMs. In this work, we collect the first open-source dataset to evaluate safeguards in LLMs, and deploy safer open-source LLMs at a low cost. Our dataset is curated and filtered to consist only of instructions that responsible language models should not follow. We annotate and assess the responses of six popular LLMs to these instructions. Based on our annotation, we proceed to train several BERT-like classifiers, and find that these small classifiers can achieve results that are comparable with GPT-4 on automatic safety evaluation. Warning: this paper contains example data that may be offensive, harmful, or biased.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25490;&#21015;&#26816;&#39564;&#35780;&#20272;&#35821;&#26009;&#24211;&#20851;&#38190;&#24615;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#35789;&#20998;&#24067;&#19981;&#22343;&#23548;&#33268;&#30340;&#20551;&#38451;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13383</link><description>&lt;p&gt;
&#20351;&#29992;&#25490;&#21015;&#26816;&#39564;&#35780;&#20272;&#20851;&#38190;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing Keyness using Permutation Tests. (arXiv:2308.13383v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13383
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25490;&#21015;&#26816;&#39564;&#35780;&#20272;&#35821;&#26009;&#24211;&#20851;&#38190;&#24615;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#35789;&#20998;&#24067;&#19981;&#22343;&#23548;&#33268;&#30340;&#20551;&#38451;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37325;&#26032;&#37319;&#26679;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#35821;&#26009;&#24211;&#35821;&#35328;&#23398;&#20013;&#30340;&#20851;&#38190;&#24615;&#65292;&#36825;&#20010;&#26041;&#27861;&#22522;&#20110;Gries&#65288;2006&#65292;2022&#65289;&#30340;&#24314;&#35758;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#20551;&#35774;&#26816;&#39564;&#65288;&#22914;&#20284;&#28982;&#27604;&#65289;&#30340;&#26041;&#27861;&#23558;&#35821;&#26009;&#24211;&#24314;&#27169;&#20026;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#26631;&#35760;&#26679;&#26412;&#12290;&#36825;&#20010;&#27169;&#22411;&#27809;&#26377;&#32771;&#34385;&#21040;&#35789;&#22312;&#35821;&#26009;&#24211;&#20013;&#30340;&#20986;&#29616;&#20998;&#24067;&#19981;&#22343;&#30340;&#24773;&#20917;&#12290;&#24403;&#19968;&#20010;&#35789;&#30340;&#20986;&#29616;&#38598;&#20013;&#22312;&#23569;&#25968;&#25991;&#26723;&#20013;&#26102;&#65292;LLR&#21644;&#31867;&#20284;&#30340;&#20998;&#25968;&#30340;&#22823;&#20540;&#23454;&#38469;&#19978;&#27604;&#26631;&#35760;&#23545;&#26631;&#35760;&#37319;&#26679;&#27169;&#22411;&#39044;&#26399;&#30340;&#26356;&#26377;&#21487;&#33021;&#65292;&#20174;&#32780;&#23548;&#33268;&#20551;&#38451;&#24615;&#12290;&#25105;&#20204;&#23558;&#26631;&#35760;&#23545;&#26631;&#35760;&#37319;&#26679;&#27169;&#22411;&#26367;&#25442;&#20026;&#26356;&#25509;&#36817;&#23454;&#38469;&#35821;&#26009;&#24211;&#32452;&#35013;&#26041;&#24335;&#30340;&#25991;&#26723;&#26679;&#26412;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25490;&#21015;&#26041;&#27861;&#26469;&#36817;&#20284;&#32473;&#23450;&#20851;&#38190;&#24615;&#20998;&#25968;&#22312;&#31561;&#39057;&#20551;&#35774;&#19979;&#30340;&#20998;&#24067;&#65292;&#24182;&#33719;&#24471;&#29992;&#20110;&#35780;&#20272;&#26174;&#33879;&#24615;&#30340;p&#20540;&#12290;&#25105;&#20204;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#25968;&#25454;&#20998;&#24067;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a resampling-based approach for assessing keyness in corpus linguistics based on suggestions by Gries (2006, 2022). Traditional approaches based on hypothesis tests (e.g. Likelihood Ratio) model the copora as independent identically distributed samples of tokens. This model does not account for the often observed uneven distribution of occurences of a word across a corpus. When occurences of a word are concentrated in few documents, large values of LLR and similar scores are in fact much more likely than accounted for by the token-by-token sampling model, leading to false positives.  We replace the token-by-token sampling model by a model where corpora are samples of documents rather than tokens, which is much closer to the way corpora are actually assembled. We then use a permutation approach to approximate the distribution of a given keyness score under the null hypothesis of equal frequencies and obtain p-values for assessing significance. We do not need any assumption on
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26681;&#25454;&#20351;&#29992;CodeBERT&#27169;&#22411;&#20998;&#26512;&#32534;&#31243;&#35821;&#35328;&#30340;&#34920;&#31034;&#65292;&#21457;&#29616;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#22312;&#26631;&#35760;&#34920;&#31034;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#24314;&#35758;&#20351;&#29992;&#36825;&#31181;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#26469;&#36873;&#25321;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.13354</link><description>&lt;p&gt;
&#20851;&#20110;&#35821;&#35328;&#36873;&#25321;&#23545;&#35757;&#32451;&#21644;&#35780;&#20272;&#32534;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Impact of Language Selection for Training and Evaluating Programming Language Models. (arXiv:2308.13354v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13354
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26681;&#25454;&#20351;&#29992;CodeBERT&#27169;&#22411;&#20998;&#26512;&#32534;&#31243;&#35821;&#35328;&#30340;&#34920;&#31034;&#65292;&#21457;&#29616;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#22312;&#26631;&#35760;&#34920;&#31034;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#24314;&#35758;&#20351;&#29992;&#36825;&#31181;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#26469;&#36873;&#25321;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#26174;&#31034;&#20986;&#22312;&#22686;&#24378;&#36825;&#20123;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#30340;&#26174;&#33879;&#36827;&#23637;&#19981;&#20165;&#36866;&#29992;&#20110;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#65292;&#32780;&#19988;&#36824;&#25193;&#23637;&#21040;&#32534;&#31243;&#35821;&#35328;&#39046;&#22495;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#20855;&#22791;&#20174;&#22810;&#31181;&#35821;&#35328;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20294;&#35780;&#20272;&#36890;&#24120;&#21482;&#20851;&#27880;&#21516;&#19968;&#31181;&#35821;&#35328;&#30340;&#29305;&#23450;&#32452;&#21512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;CodeBERT&#27169;&#22411;&#30340;&#32534;&#31243;&#35821;&#35328;&#34920;&#31034;&#20998;&#26512;&#26469;&#35780;&#20272;&#32534;&#31243;&#35821;&#35328;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#20687;C++&#12289;Python&#21644;Java&#36825;&#26679;&#30340;&#35821;&#35328;&#20013;&#30340;&#26631;&#35760;&#34920;&#31034;&#20043;&#38388;&#23384;&#22312;&#30456;&#36817;&#24615;&#65292;&#32780;&#20687;Mathematica&#21644;R&#36825;&#26679;&#30340;&#35821;&#35328;&#20013;&#30340;&#30456;&#21516;&#26631;&#35760;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#19981;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#22788;&#29702;&#22810;&#31181;&#35821;&#35328;&#26102;&#65292;&#36825;&#31181;&#29616;&#35937;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#25105;&#20204;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26469;&#36873;&#25321;&#19968;&#20010;&#21487;&#20197;&#24179;&#34913;&#22810;&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancements in Transformer-based Language Models have demonstrated significant potential in enhancing the multilingual capabilities of these models. The remarkable progress made in this domain not only applies to natural language tasks but also extends to the domain of programming languages. Despite the ability of these models to learn from multiple languages, evaluations typically focus on particular combinations of the same languages. In this study, we evaluate the similarity of programming languages by analyzing their representations using a CodeBERT-based model. Our experiments reveal that token representation in languages such as C++, Python, and Java exhibit proximity to one another, whereas the same tokens in languages such as Mathematica and R display significant dissimilarity. Our findings suggest that this phenomenon can potentially result in performance challenges when dealing with diverse languages. Thus, we recommend using our similarity measure to select a div
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#32467;&#26500;&#65292;&#36890;&#36807;&#35299;&#32806;&#31471;&#21040;&#31471;ASR&#27169;&#22411;&#30340;&#22768;&#23398;&#21644;&#35821;&#35328;&#37096;&#20998;&#65292;&#26469;&#35299;&#20915;&#39046;&#22495;&#36716;&#21464;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2308.13345</link><description>&lt;p&gt;
&#25913;&#36827;&#31471;&#21040;&#31471;&#27169;&#22411;&#36866;&#24212;&#24615;&#30340;&#35299;&#32806;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Decoupled Structure for Improved Adaptability of End-to-End Models. (arXiv:2308.13345v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#32806;&#32467;&#26500;&#65292;&#36890;&#36807;&#35299;&#32806;&#31471;&#21040;&#31471;ASR&#27169;&#22411;&#30340;&#22768;&#23398;&#21644;&#35821;&#35328;&#37096;&#20998;&#65292;&#26469;&#35299;&#20915;&#39046;&#22495;&#36716;&#21464;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#22768;&#23398;&#21644;&#35821;&#35328;&#20449;&#24687;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#21463;&#21040;&#39046;&#22495;&#36716;&#21464;&#30340;&#24433;&#21709;&#65292;&#38480;&#21046;&#20102;&#28508;&#22312;&#30340;&#24212;&#29992;&#12290;&#31471;&#21040;&#31471;ASR&#27169;&#22411;&#38544;&#24335;&#23398;&#20064;&#20102;&#20869;&#37096;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#65292;&#35813;&#27169;&#22411;&#34920;&#24449;&#20102;&#28304;&#39046;&#22495;&#30340;&#35757;&#32451;&#20998;&#24067;&#65292;&#32780;&#31471;&#21040;&#31471;&#35757;&#32451;&#30340;&#24615;&#36136;&#20351;&#24471;&#20869;&#37096;LM&#24456;&#38590;&#36866;&#24212;&#21482;&#26377;&#25991;&#26412;&#25968;&#25454;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#32534;&#30721;-&#35299;&#30721;&#22120;&#65288;Decoupled-AED&#65289;&#21644;&#31070;&#32463;&#20256;&#36755;&#22120;&#65288;Decoupled-Transducer&#65289;&#27169;&#22411;&#30340;&#35299;&#32806;&#32467;&#26500;&#65292;&#21487;&#20197;&#22312;&#31163;&#32447;&#21644;&#22312;&#32447;&#22330;&#26223;&#20013;&#23454;&#29616;&#28789;&#27963;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#65292;&#21516;&#26102;&#20445;&#25345;&#31283;&#20581;&#30340;&#39046;&#22495;&#20869;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#31471;&#21040;&#31471;&#27169;&#22411;&#35299;&#30721;&#22120;&#65288;&#25110;&#39044;&#27979;&#32593;&#32476;&#65289;&#30340;&#22768;&#23398;&#21644;&#35821;&#35328;&#37096;&#20998;&#36827;&#34892;&#35299;&#32806;&#65292;&#20351;&#24471;&#35821;&#35328;&#32452;&#25104;&#37096;&#20998;&#65288;&#21363;&#20869;&#37096;LM&#65289;&#21487;&#20197;&#26367;&#20195;&#12290;&#24403;&#36935;&#21040;&#39046;&#22495;&#36716;&#21464;&#26102;&#65292;
&lt;/p&gt;
&lt;p&gt;
Although end-to-end (E2E) trainable automatic speech recognition (ASR) has shown great success by jointly learning acoustic and linguistic information, it still suffers from the effect of domain shifts, thus limiting potential applications. The E2E ASR model implicitly learns an internal language model (LM) which characterises the training distribution of the source domain, and the E2E trainable nature makes the internal LM difficult to adapt to the target domain with text-only data To solve this problem, this paper proposes decoupled structures for attention-based encoder-decoder (Decoupled-AED) and neural transducer (Decoupled-Transducer) models, which can achieve flexible domain adaptation in both offline and online scenarios while maintaining robust intra-domain performance. To this end, the acoustic and linguistic parts of the E2E model decoder (or prediction network) are decoupled, making the linguistic component (i.e. internal LM) replaceable. When encountering a domain shift, t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PGI&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#23454;&#38469;&#21830;&#19994;&#38382;&#39064;&#20013;&#24212;&#29992;&#20110;GPT&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;GPT&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#29702;&#35299;&#22797;&#26434;&#30340;&#35821;&#35328;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22238;&#24212;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;PGI&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24110;&#21161;&#35299;&#20915;&#20102;&#20154;&#31867;&#26234;&#33021;&#20302;&#24230;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13317</link><description>&lt;p&gt;
&#25913;&#36896;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#30340;&#36755;&#20986;: PGI&#26694;&#26550;&#23545;&#27880;&#24847;&#21147;&#21160;&#24577;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Transforming the Output of Generative Pre-trained Transformer: The Influence of the PGI Framework on Attention Dynamics. (arXiv:2308.13317v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PGI&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#23454;&#38469;&#21830;&#19994;&#38382;&#39064;&#20013;&#24212;&#29992;&#20110;GPT&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;GPT&#27169;&#22411;&#30340;&#33021;&#21147;&#26469;&#29702;&#35299;&#22797;&#26434;&#30340;&#35821;&#35328;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22238;&#24212;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;PGI&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24110;&#21161;&#35299;&#20915;&#20102;&#20154;&#31867;&#26234;&#33021;&#20302;&#24230;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Persona-Grouping-Intelligence (PGI)&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;GPT&#27169;&#22411;&#22312;&#23454;&#38469;&#21830;&#19994;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;PGI&#21033;&#29992;GPT&#27169;&#22411;&#30340;&#20869;&#22312;&#33021;&#21147;&#26469;&#29702;&#35299;&#22797;&#26434;&#30340;&#35821;&#35328;&#32467;&#26500;&#65292;&#24182;&#29983;&#25104;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#22238;&#24212;&#12290;&#23454;&#39564;&#22312;&#19968;&#20010;&#21830;&#19994;&#22330;&#26223;&#20013;&#36827;&#34892;&#65292;&#35813;&#22330;&#26223;&#23384;&#22312;&#20154;&#31867;&#26234;&#33021;&#34987;&#20302;&#25928;&#30340;&#21830;&#19994;&#27969;&#31243;&#20302;&#24230;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#21033;&#29992;GPT&#27169;&#22411;&#26469;&#20943;&#36731;&#20154;&#31867;&#22312;&#24191;&#27867;&#12289;&#21333;&#35843;&#21644;&#37325;&#22797;&#30340;&#20219;&#21153;&#20013;&#30340;&#24037;&#20316;&#36127;&#33655;&#65292;&#23558;&#37325;&#28857;&#36716;&#21521;&#20915;&#31574;&#27963;&#21160;&#12290;&#35813;&#23454;&#39564;&#29983;&#25104;&#30340;4,000&#20010;&#22238;&#24212;&#30340;&#39564;&#35777;&#20934;&#30830;&#29575;&#20026;93.81%&#65292;&#31361;&#20986;&#20102;PGI&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#31181;&#33539;&#24335;&#36716;&#21464;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#20154;&#31867;&#26234;&#33021;&#20302;&#24230;&#21033;&#29992;&#30340;&#38382;&#39064;&#65292;&#20351;&#20225;&#19994;&#29615;&#22659;&#19982;&#20915;&#31574;&#27963;&#21160;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach named Persona-Grouping-Intelligence (PGI), which has been crafted to tackle the challenges posed by GPT models when applied to real-world business issues. PGI leverages the inherent capabilities of the GPT model to comprehend intricate language structures and generate responses that are contextually relevant. The experiment occurred in a business scenario where human intelligence was being underutilized due to less optimized business processes. The primary objective of this approach is to leverage GPT models to reduce the workload on humans in tasks that are extensive, monotonous, and repetitive. Instead, the focus is redirected toward decision-making activities. Remarkably, the experiment yielded an accuracy rate of 93.81% in validating 4,000 responses generated by the model, underscoring the effectiveness of the PGI strategies. Effectively addressing the issue of underutilized human intelligence, this paradigm shift aligns business environments wi
&lt;/p&gt;</description></item><item><title>&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#23637;&#20026;&#35745;&#31639;&#26041;&#27861;&#21644;&#24314;&#26500;&#35821;&#27861;&#30740;&#31350;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#26412;&#31456;&#25552;&#20379;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35745;&#31639;&#26041;&#27861;&#19982;&#24314;&#26500;&#35821;&#27861;&#30456;&#20114;&#20316;&#29992;&#30340;&#36884;&#24452;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.13315</link><description>&lt;p&gt;
&#24314;&#26500;&#35821;&#27861;&#19982;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Construction Grammar and Language Models. (arXiv:2308.13315v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13315
&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#23637;&#20026;&#35745;&#31639;&#26041;&#27861;&#21644;&#24314;&#26500;&#35821;&#27861;&#30740;&#31350;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#26412;&#31456;&#25552;&#20379;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35745;&#31639;&#26041;&#27861;&#19982;&#24314;&#26500;&#35821;&#27861;&#30456;&#20114;&#20316;&#29992;&#30340;&#36884;&#24452;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#20135;&#29983;&#20102;&#24378;&#22823;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#22312;&#19968;&#20010;&#22635;&#31354;&#24335;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#26174;&#31034;&#20986;&#20855;&#26377;&#20016;&#23500;&#35821;&#35328;&#20449;&#24687;&#30340;&#19968;&#20123;&#35777;&#25454;&#65292;&#21253;&#25324;&#19968;&#20123;&#26500;&#24335;&#30693;&#35782;&#12290;&#36825;&#19968;&#31361;&#30772;&#24615;&#30340;&#21457;&#29616;&#20026;&#35745;&#31639;&#26041;&#27861;&#21644;&#24314;&#26500;&#35821;&#27861;&#30740;&#31350;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#26426;&#20250;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35745;&#31639;&#26041;&#27861;&#21644;&#24314;&#26500;&#35821;&#27861;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26041;&#24335;&#65306;&#65288;&#19968;&#65289;&#25991;&#26412;&#20998;&#26512;&#30340;&#35745;&#31639;&#26041;&#27861;&#12289;&#65288;&#20108;&#65289;&#35745;&#31639;&#24314;&#26500;&#35821;&#27861;&#65292;&#20197;&#21450;&#65288;&#19977;&#65289;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#20851;&#27880;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20171;&#32461;&#35745;&#31639;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#25509;&#35302;&#31532;&#19968;&#31181;&#21644;&#31532;&#20108;&#31181;&#26041;&#27861;&#65292;&#28982;&#21518;&#25552;&#20379;&#19968;&#31181;&#26131;&#20110;&#29702;&#35299;&#20294;&#20840;&#38754;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27010;&#36848;&#65292;&#20063;&#35299;&#20915;&#20102;&#24314;&#26500;&#35821;&#27861;&#23398;&#23478;&#21487;&#33021;&#23384;&#22312;&#30340;&#20445;&#30041;&#24847;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Recent progress in deep learning and natural language processing has given rise to powerful models that are primarily trained on a cloze-like task and show some evidence of having access to substantial linguistic information, including some constructional knowledge. This groundbreaking discovery presents an exciting opportunity for a synergistic relationship between computational methods and Construction Grammar research. In this chapter, we explore three distinct approaches to the interplay between computational methods and Construction Grammar: (i) computational methods for text analysis, (ii) computational Construction Grammar, and (iii) deep learning models, with a particular focus on language models. We touch upon the first two approaches as a contextual foundation for the use of computational methods before providing an accessible, yet comprehensive overview of deep learning models, which also addresses reservations construction grammarians may have. Additionally, we delve into e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#39537;&#21160;&#30340;&#24605;&#32500;&#38142;&#65288;KD-CoT&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#20462;&#25913;LLMs&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#36890;&#36807;&#19982;&#22806;&#37096;&#30693;&#35782;&#30340;&#20132;&#20114;&#26469;&#35299;&#20915;&#24187;&#35273;&#21644;&#38169;&#35823;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13259</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;CoT&#65306;&#25506;&#32034;LLMs&#20013;&#23545;&#30693;&#35782;&#23494;&#38598;&#22411;&#38382;&#31572;&#36827;&#34892;&#24544;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering. (arXiv:2308.13259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#39537;&#21160;&#30340;&#24605;&#32500;&#38142;&#65288;KD-CoT&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#20462;&#25913;LLMs&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#36890;&#36807;&#19982;&#22806;&#37096;&#30693;&#35782;&#30340;&#20132;&#20114;&#26469;&#35299;&#20915;&#24187;&#35273;&#21644;&#38169;&#35823;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37197;&#22791;&#20102;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#24187;&#35273;&#21644;&#26080;&#27861;&#35775;&#38382;&#22806;&#37096;&#30693;&#35782;&#65292;LLMs&#22312;&#23545;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#22914;&#30693;&#35782;&#24211;&#38382;&#31572;&#65289;&#36827;&#34892;&#25512;&#29702;&#26102;&#24120;&#24120;&#20250;&#20135;&#29983;&#19981;&#27491;&#30830;&#25110;&#19981;&#24544;&#23454;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#39537;&#21160;&#30340;&#24605;&#32500;&#38142;&#65288;KD-CoT&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#22806;&#37096;&#30693;&#35782;&#30340;&#20132;&#20114;&#26469;&#39564;&#35777;&#21644;&#20462;&#25913;CoT&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#20811;&#26381;&#24187;&#35273;&#21644;&#38169;&#35823;&#20256;&#25773;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;LLMs&#30340;CoT&#25512;&#29702;&#36807;&#31243;&#35268;&#33539;&#21270;&#20026;&#32467;&#26500;&#21270;&#30340;&#22810;&#36718;&#38382;&#31572;&#26684;&#24335;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;LLMs&#19982;&#19968;&#20010;&#38382;&#31572;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#65292;&#35813;&#31995;&#32479;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#24182;&#22522;&#20110;&#26816;&#32034;&#21040;&#30340;&#20934;&#30830;&#31572;&#26696;&#20135;&#29983;&#24544;&#23454;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;KBQA CoT&#38598;&#21512;&#20419;&#36827;&#20102;LLMs&#30340;&#32467;&#26500;&#21270;CoT&#25512;&#29702;&#65292;&#23427;&#20316;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown impressive reasoning ability in various downstream tasks. Even so, suffering from hallucinations and the inability to access external knowledge, LLMs often come with incorrect or unfaithful intermediate reasoning steps, especially in the context of answering knowledge-intensive tasks such as KBQA. To alleviate this issue, we propose a framework called Knowledge-Driven Chain-of-Thought (KD-CoT) to verify and modify reasoning traces in CoT via interaction with external knowledge, and thus overcome the hallucinations and error propagation. Concretely, we formulate the CoT rationale process of LLMs into a structured multi-round QA format. In each round, LLMs interact with a QA system that retrieves external knowledge and produce faithful reasoning traces based on retrieved precise answers. The structured CoT reasoning of LLMs is facilitated by our developed KBQA CoT collection, which serves as in-context learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LLM2KB&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30693;&#35782;&#24211;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;Llama 2&#26550;&#26500;&#21644;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#38598;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#26500;&#24314;&#30693;&#35782;&#24211;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.13207</link><description>&lt;p&gt;
LLM2KB: &#20351;&#29992;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#22823;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30693;&#35782;&#24211;
&lt;/p&gt;
&lt;p&gt;
LLM2KB: Constructing Knowledge Bases using instruction tuned context aware Large Language Models. (arXiv:2308.13207v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLM2KB&#65292;&#19968;&#31181;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30693;&#35782;&#24211;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#23545;Llama 2&#26550;&#26500;&#21644;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#38598;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#26500;&#24314;&#30693;&#35782;&#24211;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#20351;&#24471;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#39046;&#22495;&#26159;&#21033;&#29992;&#36825;&#20123;&#24378;&#22823;&#30340;&#27169;&#22411;&#26500;&#24314;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#12290;&#30693;&#35782;&#24211;&#20316;&#20026;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#23384;&#20648;&#24211;&#65292;&#33021;&#22815;&#20419;&#36827;&#20449;&#24687;&#26816;&#32034;&#21644;&#25512;&#29702;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;LLM2KB&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30693;&#35782;&#24211;&#30340;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#20110;Llama 2&#26550;&#26500;&#21644;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#23567;&#30340;&#27880;&#20837;&#27169;&#22411;&#26469;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#30340;&#25351;&#20196;&#35843;&#25972;&#65292;&#36825;&#20123;&#27880;&#20837;&#27169;&#22411;&#20165;&#20855;&#26377;&#22522;&#30784;&#27169;&#22411;&#21442;&#25968;&#30340;0.05%&#65292;&#20351;&#29992;&#20102;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#25216;&#26415;&#12290;&#36825;&#20123;&#27880;&#20837;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#23494;&#38598;&#36890;&#36947;&#26816;&#32034;&#65288;DPR&#65289;&#31639;&#27861;&#25552;&#21462;&#30340;&#20027;&#20307;&#23454;&#20307;&#30340;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#19978;&#19979;&#25991;&#30456;&#23545;&#24212;&#30340;&#25552;&#31034;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#22238;&#31572;&#30456;&#20851;&#30340;&#23458;&#20307;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of Large Language Models (LLM) has revolutionized the field of natural language processing, enabling significant progress in various applications. One key area of interest is the construction of Knowledge Bases (KB) using these powerful models. Knowledge bases serve as repositories of structured information, facilitating information retrieval and inference tasks. Our paper proposes LLM2KB, a system for constructing knowledge bases using large language models, with a focus on the Llama 2 architecture and the Wikipedia dataset. We perform parameter efficient instruction tuning for Llama-2-13b-chat and StableBeluga-13B by training small injection models that have only 0.05 % of the parameters of the base models using the Low Rank Adaptation (LoRA) technique. These injection models have been trained with prompts that are engineered to utilize Wikipedia page contexts of subject entities fetched using a Dense Passage Retrieval (DPR) algorithm, to answer relevant object entities fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#23384;&#20648;&#26041;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#23450;&#20301;&#30693;&#35782;&#31070;&#32463;&#20803;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35821;&#35328;&#26080;&#20851;&#30693;&#35782;&#31070;&#32463;&#20803;&#30340;&#23384;&#22312;&#65292;&#20197;&#21450;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#22411;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;&#12290;</title><link>http://arxiv.org/abs/2308.13198</link><description>&lt;p&gt;
&#28145;&#20837;&#29702;&#35299;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#31070;&#32463;&#20803;&#65306;&#35821;&#35328;&#26080;&#20851;&#30693;&#35782;&#31070;&#32463;&#20803;&#21644;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Journey to the Center of the Knowledge Neurons: Discoveries of Language-Independent Knowledge Neurons and Degenerate Knowledge Neurons. (arXiv:2308.13198v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#23384;&#20648;&#26041;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#23450;&#20301;&#30693;&#35782;&#31070;&#32463;&#20803;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35821;&#35328;&#26080;&#20851;&#30693;&#35782;&#31070;&#32463;&#20803;&#30340;&#23384;&#22312;&#65292;&#20197;&#21450;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#22411;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21253;&#21547;&#22823;&#37327;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#20854;&#23384;&#20648;&#22312;&#21442;&#25968;&#20013;&#30340;&#26041;&#24335;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;PLMs&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#23384;&#20648;&#26041;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20307;&#31995;&#32467;&#26500;&#30340;&#22810;&#35821;&#35328;&#25972;&#21512;&#26799;&#24230;&#26041;&#27861;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#20934;&#30830;&#22320;&#23450;&#20301;&#30693;&#35782;&#31070;&#32463;&#20803;&#65292;&#24182;&#22312;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#21644;&#35821;&#35328;&#20043;&#38388;&#26356;&#20855;&#26222;&#36941;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#30693;&#35782;&#31070;&#32463;&#20803;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#32034;&#65292;&#24471;&#20986;&#20102;&#20197;&#19979;&#20004;&#39033;&#37325;&#35201;&#30340;&#21457;&#29616;&#65306;&#65288;1&#65289;&#21457;&#29616;&#20102;&#35821;&#35328;&#26080;&#20851;&#30693;&#35782;&#31070;&#32463;&#20803;&#65292;&#20854;&#20197;&#36229;&#36234;&#35821;&#35328;&#30340;&#26041;&#24335;&#23384;&#20648;&#20107;&#23454;&#30693;&#35782;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#36328;&#35821;&#35328;&#30693;&#35782;&#32534;&#36753;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;PLMs&#21487;&#20197;&#22522;&#20110;&#35821;&#35328;&#26080;&#20851;&#30340;&#31070;&#32463;&#20803;&#23436;&#25104;&#27492;&#20219;&#21153;&#65307;&#65288;2&#65289;&#21457;&#29616;&#20102;&#36864;&#21270;&#30693;&#35782;&#31070;&#32463;&#20803;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#22411;&#31070;&#32463;&#20803;&#65292;&#34920;&#26126;&#19981;&#21516;&#30340;&#30693;&#35782;&#31070;&#32463;&#20803;&#21487;&#20197;&#22312;&#25968;&#25454;&#29305;&#24449;&#33806;&#32553;&#30340;&#24773;&#20917;&#19979;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) contain vast amounts of factual knowledge, but how the knowledge is stored in the parameters remains unclear. This paper delves into the complex task of understanding how factual knowledge is stored in multilingual PLMs, and introduces the Architecture-adapted Multilingual Integrated Gradients method, which successfully localizes knowledge neurons more precisely compared to current methods, and is more universal across various architectures and languages. Moreover, we conduct an in-depth exploration of knowledge neurons, leading to the following two important discoveries: (1) The discovery of Language-Independent Knowledge Neurons, which store factual knowledge in a form that transcends language. We design cross-lingual knowledge editing experiments, demonstrating that the PLMs can accomplish this task based on language-independent neurons; (2) The discovery of Degenerate Knowledge Neurons, a novel type of neuron showing that different knowledge neuro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#24418;&#24335;&#21270;&#33258;&#28982;&#35821;&#35328;&#37327;&#35789;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#20197;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#21040;&#36923;&#36753;&#34920;&#31034;&#30340;&#36716;&#25442;&#21644;&#35780;&#20272;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.13192</link><description>&lt;p&gt;
&#20154;&#26426;&#20132;&#20114;&#20013;&#33258;&#28982;&#35821;&#35328;&#37327;&#35789;&#30340;&#24418;&#24335;&#21270;
&lt;/p&gt;
&lt;p&gt;
Formalising Natural Language Quantifiers for Human-Robot Interactions. (arXiv:2308.13192v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#24418;&#24335;&#21270;&#33258;&#28982;&#35821;&#35328;&#37327;&#35789;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#20197;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#21040;&#36923;&#36753;&#34920;&#31034;&#30340;&#36716;&#25442;&#21644;&#35780;&#20272;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20154;&#26426;&#20132;&#20114;&#32972;&#26223;&#19979;&#24418;&#24335;&#21270;&#33258;&#28982;&#35821;&#35328;&#37327;&#35789;&#30340;&#26041;&#27861;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#19968;&#38454;&#36923;&#36753;&#24182;&#25193;&#23637;&#20102;&#34920;&#31034;&#21464;&#37327;&#22522;&#25968;&#30340;&#33021;&#21147;&#65292;&#31867;&#20284;&#20110;&#24191;&#20041;&#37327;&#35789;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#25509;&#25910;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#65292;&#23558;&#20854;&#36716;&#25442;&#20026;&#24418;&#24335;&#21270;&#30340;&#36923;&#36753;&#34920;&#31034;&#65292;&#35780;&#20272;&#23427;&#65292;&#24182;&#36820;&#22238;&#32467;&#26524;&#25110;&#21521;&#27169;&#25311;&#26426;&#22120;&#20154;&#21457;&#36865;&#21629;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for formalising quantifiers in natural language in the context of human-robot interactions. The solution is based on first-order logic extended with capabilities to represent the cardinality of variables, operating similarly to generalised quantifiers. To demonstrate the method, we designed an end-to-end system able to receive input as natural language, convert it into a formal logical representation, evaluate it, and return a result or send a command to a simulated robot.
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;transformer&#33021;&#22815;&#22788;&#29702;&#26356;&#38271;&#30340;&#24207;&#21015;&#65292;&#21516;&#26102;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#19982;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#32447;&#24615;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2308.13191</link><description>&lt;p&gt;
Chunk, Align, Select: &#19968;&#31181;&#31616;&#21333;&#30340;&#29992;&#20110;transformer&#30340;&#38271;&#24207;&#21015;&#22788;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers. (arXiv:2308.13191v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13191
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;transformer&#33021;&#22815;&#22788;&#29702;&#26356;&#38271;&#30340;&#24207;&#21015;&#65292;&#21516;&#26102;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#19982;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#32447;&#24615;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#30528;&#38271;&#24207;&#21015;&#22788;&#29702;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;transformer&#20013;&#33258;&#27880;&#24847;&#25805;&#20316;&#30340;&#35745;&#31639;&#25104;&#26412;&#38543;&#30528;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290;&#20026;&#20102;&#20943;&#36731;&#38271;&#24207;&#21015;&#22788;&#29702;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;transformer&#33021;&#22815;&#22788;&#29702;&#26356;&#38271;&#30340;&#24207;&#21015;&#65292;&#21516;&#26102;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#19982;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#32447;&#24615;&#22686;&#38271;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#27599;&#20010;&#38271;&#24207;&#21015;&#36755;&#20837;&#21010;&#20998;&#20026;&#19968;&#25209;chunk&#65292;&#28982;&#21518;&#22312;&#32534;&#30721;&#36807;&#31243;&#20013;&#23545;chunk&#20043;&#38388;&#30340;&#20449;&#24687;&#36827;&#34892;&#23545;&#40784;&#65292;&#26368;&#21518;&#20174;&#32534;&#30721;&#22120;&#20013;&#36873;&#25321;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#35299;&#30721;&#12290;&#20026;&#20102;&#25552;&#21462;chunk&#20043;&#38388;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#32534;&#30721;transformer&#22359;&#20013;&#23545;chunk&#20043;&#38388;&#30340;&#36215;&#22987;&#21644;&#32467;&#26463;token&#36827;&#34892;&#23545;&#40784;&#12290;&#20026;&#20102;&#23398;&#20064;&#19968;&#20010;&#26377;&#25928;&#30340;&#38544;&#34255;&#29366;&#24577;&#36873;&#25321;&#31574;&#30053;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#37325;&#26356;&#26032;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although dominant in natural language processing, transformer-based models remain challenged by the task of long-sequence processing, because the computational cost of self-attention operations in transformers swells quadratically with the input sequence length. To alleviate the complexity of long-sequence processing, we propose a simple framework to enable the offthe-shelf pre-trained transformers to process much longer sequences, while the computation and memory costs remain growing linearly with the input sequence lengths. More specifically, our method divides each long-sequence input into a batch of chunks, then aligns the interchunk information during the encoding steps, and finally selects the most representative hidden states from the encoder for the decoding process. To extract inter-chunk semantic information, we align the start and end token embeddings among chunks in each encoding transformer block. To learn an effective hidden selection policy, we design a dual updating sch
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OVDEval&#30340;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#20840;&#38754;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;9&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#24120;&#35782;&#30693;&#35782;&#12289;&#23646;&#24615;&#29702;&#35299;&#12289;&#20301;&#32622;&#29702;&#35299;&#12289;&#23545;&#35937;&#20851;&#31995;&#29702;&#35299;&#31561;&#26041;&#38754;&#30340;&#35780;&#20272;&#12290;&#25968;&#25454;&#38598;&#34987;&#31934;&#24515;&#21019;&#24314;&#20197;&#25552;&#20379;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36127;&#20363;&#65292;&#32771;&#39564;&#27169;&#22411;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#36755;&#20837;&#30340;&#30495;&#27491;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#21457;&#29616;&#20102;&#24179;&#22343;&#31934;&#30830;&#24230;&#65288;AP&#65289;&#25351;&#26631;&#22312;&#36825;&#20123;&#32454;&#31890;&#24230;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2308.13177</link><description>&lt;p&gt;
&#22914;&#20309;&#35780;&#20272;&#26816;&#27979;&#30340;&#27867;&#21270;&#33021;&#21147;&#65311;&#19968;&#31181;&#29992;&#20110;&#20840;&#38754;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
How to Evaluate the Generalization of Detection? A Benchmark for Comprehensive Open-Vocabulary Detection. (arXiv:2308.13177v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13177
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OVDEval&#30340;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#20840;&#38754;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;9&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#24120;&#35782;&#30693;&#35782;&#12289;&#23646;&#24615;&#29702;&#35299;&#12289;&#20301;&#32622;&#29702;&#35299;&#12289;&#23545;&#35937;&#20851;&#31995;&#29702;&#35299;&#31561;&#26041;&#38754;&#30340;&#35780;&#20272;&#12290;&#25968;&#25454;&#38598;&#34987;&#31934;&#24515;&#21019;&#24314;&#20197;&#25552;&#20379;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36127;&#20363;&#65292;&#32771;&#39564;&#27169;&#22411;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#36755;&#20837;&#30340;&#30495;&#27491;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#21457;&#29616;&#20102;&#24179;&#22343;&#31934;&#30830;&#24230;&#65288;AP&#65289;&#25351;&#26631;&#22312;&#36825;&#20123;&#32454;&#31890;&#24230;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#22312;&#36817;&#24180;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20174;&#22522;&#20110;&#23553;&#38381;&#38598;&#26631;&#31614;&#21040;&#22522;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#65288;OVD&#65289;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#20165;&#38480;&#20110;&#27979;&#35797;&#23545;&#35937;&#31867;&#22411;&#21644;&#24341;&#29992;&#34920;&#36798;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#26080;&#27861;&#25552;&#20379;OVD&#27169;&#22411;&#33021;&#21147;&#30340;&#31995;&#32479;&#12289;&#32454;&#31890;&#24230;&#21644;&#20934;&#30830;&#30340;&#22522;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OVDEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#65292;&#21253;&#25324;9&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#24120;&#35782;&#30693;&#35782;&#12289;&#23646;&#24615;&#29702;&#35299;&#12289;&#20301;&#32622;&#29702;&#35299;&#12289;&#23545;&#35937;&#20851;&#31995;&#29702;&#35299;&#31561;&#26041;&#38754;&#30340;&#35780;&#20272;&#12290;&#25968;&#25454;&#38598;&#34987;&#31934;&#24515;&#21019;&#24314;&#20197;&#25552;&#20379;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36127;&#20363;&#65292;&#32771;&#39564;&#27169;&#22411;&#23545;&#35270;&#35273;&#21644;&#35821;&#35328;&#36755;&#20837;&#30340;&#30495;&#27491;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#22312;&#36825;&#20123;&#32454;&#31890;&#24230;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#26102;&#26222;&#36941;&#20351;&#29992;&#30340;&#24179;&#22343;&#31934;&#30830;&#24230;&#65288;AP&#65289;&#25351;&#26631;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38750;&#26497;&#22823;&#20540;&#25233;&#21046;&#30340;&#26032;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Object detection (OD) in computer vision has made significant progress in recent years, transitioning from closed-set labels to open-vocabulary detection (OVD) based on large-scale vision-language pre-training (VLP). However, current evaluation methods and datasets are limited to testing generalization over object types and referral expressions, which do not provide a systematic, fine-grained, and accurate benchmark of OVD models' abilities. In this paper, we propose a new benchmark named OVDEval, which includes 9 sub-tasks and introduces evaluations on commonsense knowledge, attribute understanding, position understanding, object relation comprehension, and more. The dataset is meticulously created to provide hard negatives that challenge models' true understanding of visual and linguistic input. Additionally, we identify a problem with the popular Average Precision (AP) metric when benchmarking models on these fine-grained label datasets and propose a new metric called Non-Maximum Su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#35780;&#20272;&#22330;&#26223;&#25991;&#26412;OCR&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#32479;&#19968;&#30340;&#35789;&#38169;&#35823;&#29575;&#20316;&#20026;&#34913;&#37327;&#26631;&#20934;&#65292;&#32771;&#34385;&#20102;&#21024;&#38500;&#12289;&#25554;&#20837;&#12289;&#26367;&#25442;&#21644;&#20998;&#32452;/&#25490;&#24207;&#38169;&#35823;&#65292;&#21516;&#26102;&#20351;&#29992;&#36229;&#32423;&#22359;&#30340;&#27010;&#24565;&#35745;&#31639;BLEU&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2308.13173</link><description>&lt;p&gt;
DISGO: &#33258;&#21160;&#21270;&#31471;&#21040;&#31471;&#35780;&#20272;&#22330;&#26223;&#25991;&#26412;OCR
&lt;/p&gt;
&lt;p&gt;
DISGO: Automatic End-to-End Evaluation for Scene Text OCR. (arXiv:2308.13173v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#35780;&#20272;&#22330;&#26223;&#25991;&#26412;OCR&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#32479;&#19968;&#30340;&#35789;&#38169;&#35823;&#29575;&#20316;&#20026;&#34913;&#37327;&#26631;&#20934;&#65292;&#32771;&#34385;&#20102;&#21024;&#38500;&#12289;&#25554;&#20837;&#12289;&#26367;&#25442;&#21644;&#20998;&#32452;/&#25490;&#24207;&#38169;&#35823;&#65292;&#21516;&#26102;&#20351;&#29992;&#36229;&#32423;&#22359;&#30340;&#27010;&#24565;&#35745;&#31639;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#33258;&#28982;&#22330;&#26223;&#20013;&#36827;&#34892;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#30340;&#25361;&#25112;&#65292;&#30456;&#27604;&#20110;&#25991;&#26723;&#19978;&#30340;OCR&#26469;&#35828;&#26356;&#22256;&#38590;&#65292;&#22240;&#20026;&#20854;&#20013;&#21253;&#21547;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#22270;&#20687;&#32972;&#26223;&#12290;&#25105;&#20204;&#25552;&#20986;&#32479;&#19968;&#20351;&#29992;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20316;&#20026;&#35780;&#20272;&#22330;&#26223;&#25991;&#26412;OCR&#24615;&#33021;&#30340;&#19968;&#31181;&#26032;&#30340;&#34913;&#37327;&#26631;&#20934;&#65292;&#26082;&#21487;&#20197;&#29992;&#20110;&#31471;&#21040;&#31471;&#65288;e2e&#65289;&#24615;&#33021;&#65292;&#20063;&#21487;&#20197;&#29992;&#20110;&#21333;&#20010;&#31995;&#32479;&#32452;&#20214;&#30340;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#31471;&#21040;&#31471;&#30340;&#24230;&#37327;&#65292;&#25105;&#20204;&#31216;&#20854;&#20026;DISGO WER&#65292;&#22240;&#20026;&#23427;&#32771;&#34385;&#20102;&#21024;&#38500;&#12289;&#25554;&#20837;&#12289;&#26367;&#25442;&#21644;&#20998;&#32452;/&#25490;&#24207;&#38169;&#35823;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#36229;&#32423;&#22359;&#30340;&#27010;&#24565;&#26469;&#33258;&#21160;&#35745;&#31639;&#31471;&#21040;&#31471;OCR&#26426;&#22120;&#32763;&#35793;&#30340;BLEU&#20998;&#25968;&#12290;&#20351;&#29992;&#23567;&#35268;&#27169;&#30340;SCUT&#20844;&#20849;&#27979;&#35797;&#25968;&#25454;&#38598;&#26469;&#28436;&#31034;&#27169;&#22359;&#21270;OCR&#31995;&#32479;&#30340;WER&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper discusses the challenges of optical character recognition (OCR) on natural scenes, which is harder than OCR on documents due to the wild content and various image backgrounds. We propose to uniformly use word error rates (WER) as a new measurement for evaluating scene-text OCR, both end-to-end (e2e) performance and individual system component performances. Particularly for the e2e metric, we name it DISGO WER as it considers Deletion, Insertion, Substitution, and Grouping/Ordering errors. Finally we propose to utilize the concept of super blocks to automatically compute BLEU scores for e2e OCR machine translation. The small SCUT public test set is used to demonstrate WER performance by a modularized OCR system.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#39640;&#24615;&#33021;&#31070;&#32463;&#32763;&#35793;&#20998;&#31867;&#22120;&#20013;&#23384;&#22312;&#30340;&#8220;&#32874;&#26126;&#30340;&#27721;&#26031;&#8221;&#34892;&#20026;&#65292;&#23427;&#21033;&#29992;&#34394;&#20551;&#30456;&#20851;&#24615;&#32780;&#38750;&#30495;&#23454;&#30340;&#32763;&#35793;&#20449;&#21495;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#20027;&#39064;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#20851;&#20110;&#34394;&#20551;&#20027;&#39064;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20197;&#21450;&#26377;&#20851;&#34394;&#20551;&#20027;&#39064;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.13170</link><description>&lt;p&gt;
&#22312;&#20998;&#31867;&#20013;&#27979;&#37327;&#34394;&#20551;&#30456;&#20851;&#24615;&#65306;&#35793;&#25991;&#20013;&#30340;&#8220;&#32874;&#26126;&#30340;&#27721;&#26031;&#8221;
&lt;/p&gt;
&lt;p&gt;
Measuring Spurious Correlation in Classification: 'Clever Hans' in Translationese. (arXiv:2308.13170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13170
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#39640;&#24615;&#33021;&#31070;&#32463;&#32763;&#35793;&#20998;&#31867;&#22120;&#20013;&#23384;&#22312;&#30340;&#8220;&#32874;&#26126;&#30340;&#27721;&#26031;&#8221;&#34892;&#20026;&#65292;&#23427;&#21033;&#29992;&#34394;&#20551;&#30456;&#20851;&#24615;&#32780;&#38750;&#30495;&#23454;&#30340;&#32763;&#35793;&#20449;&#21495;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#20027;&#39064;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#25506;&#35752;&#20102;&#22312;&#27809;&#26377;&#20851;&#20110;&#34394;&#20551;&#20027;&#39064;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20197;&#21450;&#26377;&#20851;&#34394;&#20551;&#20027;&#39064;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#39640;&#24615;&#33021;&#31070;&#32463;&#32763;&#35793;&#20998;&#31867;&#22120;&#20013;&#23384;&#22312;&#8220;&#32874;&#26126;&#30340;&#27721;&#26031;&#8221;&#34892;&#20026;&#65292;&#21363;&#22522;&#20110;BERT&#30340;&#20998;&#31867;&#22120;&#21033;&#29992;&#25968;&#25454;&#19982;&#30446;&#26631;&#20998;&#31867;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#29305;&#21035;&#26159;&#20027;&#39064;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#30495;&#23454;&#30340;&#32763;&#35793;&#20449;&#21495;&#12290;&#32763;&#35793;&#20449;&#21495;&#24494;&#22937;&#65288;&#23588;&#20854;&#26159;&#23545;&#20110;&#19987;&#19994;&#32763;&#35793;&#65289;&#65292;&#24182;&#19988;&#19982;&#25968;&#25454;&#20013;&#30340;&#35768;&#22810;&#20854;&#20182;&#20449;&#21495;&#31454;&#20105;&#65292;&#22914;&#27969;&#27966;&#12289;&#39118;&#26684;&#12289;&#20316;&#32773;&#21644;&#23588;&#20854;&#26159;&#20027;&#39064;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#24635;&#20307;&#38382;&#39064;&#65292;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#21040;&#24213;&#26377;&#22810;&#23569;&#26159;&#30001;&#20110;&#25968;&#25454;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#32780;&#19981;&#26159;&#20998;&#31867;&#22120;&#23454;&#38469;&#38024;&#23545;&#30340;&#20449;&#21495;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#24494;&#22937;&#30340;&#30446;&#26631;&#20449;&#21495;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65288;&#20302;&#36164;&#28304;&#65289;&#25968;&#25454;&#29615;&#22659;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#22522;&#20110;&#20027;&#39064;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#24182;&#20174;&#20004;&#20010;&#26041;&#21521;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#65306;&#65288;i&#65289;&#22312;&#27809;&#26377;&#20851;&#20110;&#34394;&#20551;&#20027;&#39064;&#20449;&#24687;&#21450;&#20854;&#22312;&#25968;&#25454;&#20013;&#20998;&#24067;&#30340;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#65288;ii&#65289;&#22312;&#26377;&#20851;&#34394;&#20551;&#20027;&#39064;&#20449;&#24687;&#21450;&#20854;&#22312;&#25968;&#25454;&#20013;&#20998;&#24067;&#30340;&#19968;&#20123;&#25351;&#31034;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown evidence of 'Clever Hans' behavior in high-performance neural translationese classifiers, where BERT-based classifiers capitalize on spurious correlations, in particular topic information, between data and target classification labels, rather than genuine translationese signals. Translationese signals are subtle (especially for professional translation) and compete with many other signals in the data such as genre, style, author, and, in particular, topic. This raises the general question of how much of the performance of a classifier is really due to spurious correlations in the data versus the signals actually targeted for by the classifier, especially for subtle target signals and in challenging (low resource) data settings. We focus on topic-based spurious correlation and approach the question from two directions: (i) where we have no knowledge about spurious topic information and its distribution in the data, (ii) where we have some indication about the natur
&lt;/p&gt;</description></item><item><title>SciEval&#26159;&#19968;&#20010;&#32508;&#21512;&#19988;&#22810;&#23398;&#31185;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#33021;&#21147;&#12290;&#23427;&#22522;&#20110;&#24067;&#40065;&#22982;&#30340;&#20998;&#31867;&#27861;&#65292;&#21253;&#25324;&#23458;&#35266;&#21644;&#20027;&#35266;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#38450;&#27490;&#25968;&#25454;&#27844;&#28431;&#30340;&#8220;&#21160;&#24577;&#8221;&#23376;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;GPT-4&#22312;&#26576;&#20123;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24471;&#20998;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.13149</link><description>&lt;p&gt;
SciEval: &#29992;&#20110;&#31185;&#23398;&#30740;&#31350;&#30340;&#22810;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research. (arXiv:2308.13149v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13149
&lt;/p&gt;
&lt;p&gt;
SciEval&#26159;&#19968;&#20010;&#32508;&#21512;&#19988;&#22810;&#23398;&#31185;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#33021;&#21147;&#12290;&#23427;&#22522;&#20110;&#24067;&#40065;&#22982;&#30340;&#20998;&#31867;&#27861;&#65292;&#21253;&#25324;&#23458;&#35266;&#21644;&#20027;&#35266;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#38450;&#27490;&#25968;&#25454;&#27844;&#28431;&#30340;&#8220;&#21160;&#24577;&#8221;&#23376;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;GPT-4&#22312;&#26576;&#20123;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24471;&#20998;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#31185;&#23398;&#30740;&#31350;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20934;&#26469;&#35780;&#20272;LLMs&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#22522;&#20934;&#20027;&#35201;&#22522;&#20110;&#39044;&#20808;&#25910;&#38598;&#30340;&#23458;&#35266;&#38382;&#39064;&#12290;&#36825;&#31181;&#35774;&#35745;&#23384;&#22312;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#65292;&#24182;&#19988;&#32570;&#20047;&#23545;&#20027;&#35266;&#38382;&#31572;&#33021;&#21147;&#30340;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SciEval&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#12289;&#22810;&#23398;&#31185;&#30340;&#35780;&#20272;&#22522;&#20934;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22522;&#20110;&#24067;&#40065;&#22982;&#30340;&#20998;&#31867;&#27861;&#65292;SciEval&#28085;&#30422;&#20102;&#22235;&#20010;&#32500;&#24230;&#26469;&#31995;&#32479;&#35780;&#20272;&#31185;&#23398;&#30740;&#31350;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#31185;&#23398;&#21407;&#29702;&#30340;&#8220;&#21160;&#24577;&#8221;&#23376;&#38598;&#65292;&#20197;&#38450;&#27490;&#35780;&#20272;&#20986;&#29616;&#28508;&#22312;&#30340;&#25968;&#25454;&#27844;&#28431;&#12290;SciEval&#21253;&#21547;&#20102;&#23458;&#35266;&#21644;&#20027;&#35266;&#38382;&#39064;&#12290;&#36825;&#20123;&#29305;&#28857;&#20351;SciEval&#25104;&#20026;&#35780;&#20272;LLMs&#31185;&#23398;&#30740;&#31350;&#33021;&#21147;&#30340;&#26356;&#26377;&#25928;&#30340;&#22522;&#20934;&#12290;&#23545;&#26368;&#20808;&#36827;&#30340;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;GPT-4&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24471;&#20998;&#65292;&#20294;&#22312;&#26576;&#20123;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been growing interest in using Large Language Models (LLMs) for scientific research. Numerous benchmarks have been proposed to evaluate the ability of LLMs for scientific research. However, current benchmarks are mostly based on pre-collected objective questions. This design suffers from data leakage problem and lacks the evaluation of subjective Q/A ability. In this paper, we propose SciEval, a comprehensive and multi-disciplinary evaluation benchmark to address these issues. Based on Bloom's taxonomy, SciEval covers four dimensions to systematically evaluate scientific research ability. In particular, we design a "dynamic" subset based on scientific principles to prevent evaluation from potential data leakage. Both objective and subjective questions are included in SciEval. These characteristics make SciEval a more effective benchmark for scientific research ability evaluation of LLMs. Comprehensive experiments on most advanced LLMs show that, although GPT-4 achie
&lt;/p&gt;</description></item><item><title>MatchXML&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#26694;&#26550;&#65292;&#29992;&#20110;&#26497;&#31471;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;label2vec&#26041;&#27861;&#29983;&#25104;&#35821;&#20041;&#23494;&#38598;&#30340;&#26631;&#31614;&#23884;&#20837;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#23884;&#20837;&#26500;&#24314;&#23618;&#27425;&#21270;&#26631;&#31614;&#26641;&#12290;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#65292;MatchXML&#23558;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#25552;&#21462;&#20986;&#23494;&#38598;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#38745;&#24577;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2308.13139</link><description>&lt;p&gt;
MatchXML: &#39640;&#25928;&#30340;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#26694;&#26550;&#65292;&#29992;&#20110;&#26497;&#31471;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MatchXML: An Efficient Text-label Matching Framework for Extreme Multi-label Text Classification. (arXiv:2308.13139v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13139
&lt;/p&gt;
&lt;p&gt;
MatchXML&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#26694;&#26550;&#65292;&#29992;&#20110;&#26497;&#31471;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;label2vec&#26041;&#27861;&#29983;&#25104;&#35821;&#20041;&#23494;&#38598;&#30340;&#26631;&#31614;&#23884;&#20837;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#23884;&#20837;&#26500;&#24314;&#23618;&#27425;&#21270;&#26631;&#31614;&#26641;&#12290;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#65292;MatchXML&#23558;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#25552;&#21462;&#20986;&#23494;&#38598;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#38745;&#24577;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#31471;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#65288;XMC&#65289;&#26159;&#25351;&#35757;&#32451;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#20174;&#19968;&#20010;&#38750;&#24120;&#22823;&#35268;&#27169;&#30340;&#26631;&#31614;&#38598;&#20013;&#65288;&#20363;&#22914;&#25968;&#30334;&#19975;&#20010;&#26631;&#31614;&#65289;&#20026;&#25991;&#26412;&#26679;&#26412;&#20998;&#37197;&#30456;&#20851;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MatchXML&#65292;&#19968;&#31181;&#29992;&#20110;XMC&#30340;&#39640;&#25928;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#26694;&#26550;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#30001;&#31232;&#30095;&#30340;&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;TF-IDF&#65289;&#29305;&#24449;&#29983;&#25104;&#30340;&#26631;&#31614;&#23884;&#20837;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;label2vec&#65292;&#36890;&#36807;Skip-gram&#27169;&#22411;&#26469;&#26377;&#25928;&#35757;&#32451;&#35821;&#20041;&#23494;&#38598;&#30340;&#26631;&#31614;&#23884;&#20837;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#36825;&#20123;&#23494;&#38598;&#30340;&#26631;&#31614;&#23884;&#20837;&#26469;&#26500;&#24314;&#19968;&#20010;&#23618;&#27425;&#21270;&#26631;&#31614;&#26641;&#12290;&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;Transformer&#26102;&#65292;&#25105;&#20204;&#23558;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#20010;&#22312;&#20108;&#20998;&#22270;&#20013;&#30340;&#25991;&#26412;-&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#20174;&#24494;&#35843;&#21518;&#30340;Transformer&#20013;&#25552;&#21462;&#23494;&#38598;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#38500;&#20102;&#24494;&#35843;&#21518;&#30340;&#23494;&#38598;&#25991;&#26412;&#23884;&#20837;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20174;&#39044;&#35757;&#32451;&#30340;Sentence Transformer&#20013;&#25552;&#21462;&#38745;&#24577;&#30340;&#23494;&#38598;&#21477;&#23376;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The eXtreme Multi-label text Classification(XMC) refers to training a classifier that assigns a text sample with relevant labels from an extremely large-scale label set (e.g., millions of labels). We propose MatchXML, an efficient text-label matching framework for XMC. We observe that the label embeddings generated from the sparse Term Frequency-Inverse Document Frequency(TF-IDF) features have several limitations. We thus propose label2vec to effectively train the semantic dense label embeddings by the Skip-gram model. The dense label embeddings are then used to build a Hierarchical Label Tree by clustering. In fine-tuning the pre-trained encoder Transformer, we formulate the multi-label text classification as a text-label matching problem in a bipartite graph. We then extract the dense text representations from the fine-tuned Transformer. Besides the fine-tuned dense text embeddings, we also extract the static dense sentence embeddings from a pre-trained Sentence Transformer. Finally,
&lt;/p&gt;</description></item><item><title>OmniQuant&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#21508;&#31181;&#37327;&#21270;&#21442;&#25968;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#20445;&#25345;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.13137</link><description>&lt;p&gt;
OmniQuant&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models. (arXiv:2308.13137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13137
&lt;/p&gt;
&lt;p&gt;
OmniQuant&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#21508;&#31181;&#37327;&#21270;&#21442;&#25968;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#20445;&#25345;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#37096;&#32626;&#21463;&#21040;&#20102;&#20854;&#24222;&#22823;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#38480;&#21046;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#26041;&#27861;&#22312;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#25552;&#39640;LLM&#30340;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#25163;&#24037;&#21046;&#23450;&#37327;&#21270;&#21442;&#25968;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#20302;&#24182;&#19988;&#19981;&#33021;&#22788;&#29702;&#26497;&#20302;&#20301;&#37327;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;&#65288;OmniQuant&#65289;&#25216;&#26415;&#65292;&#29992;&#20110;LLMs&#65292;&#23427;&#22312;&#22810;&#31181;&#37327;&#21270;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#20248;&#21270;&#21508;&#31181;&#37327;&#21270;&#21442;&#25968;&#26469;&#20445;&#25345;PTQ&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;OmniQuant&#21253;&#21547;&#20004;&#20010;&#21019;&#26032;&#32452;&#20214;&#65292;&#21253;&#25324;&#21487;&#23398;&#20064;&#30340;&#26435;&#37325;&#21098;&#35009;&#65288;LWC&#65289;&#21644;&#21487;&#23398;&#20064;&#30340;&#31561;&#25928;&#21464;&#25442;&#65288;LET&#65289;&#12290;LWC&#36890;&#36807;&#20248;&#21270;&#21098;&#35009;&#38408;&#20540;&#26469;&#35843;&#33410;&#26435;&#37325;&#30340;&#26497;&#20540;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;LET&#22788;&#29702;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, which leads to low performance and fails to deal with extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (OmniQuant) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#35821;&#31181;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#26469;&#35757;&#32451;&#21476;&#24076;&#33098;&#25991;&#26412;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2308.13116</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#35821;&#31181;&#30693;&#35782;&#33976;&#39311;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#29992;&#20110;&#21476;&#24076;&#33098;&#35821;
&lt;/p&gt;
&lt;p&gt;
Sentence Embedding Models for Ancient Greek Using Multilingual Knowledge Distillation. (arXiv:2308.13116v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#35821;&#31181;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#26469;&#35757;&#32451;&#21476;&#24076;&#33098;&#25991;&#26412;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21476;&#24076;&#33098;&#35821;&#31561;&#21382;&#21490;&#35821;&#35328;&#65292;&#39640;&#36136;&#37327;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#38590;&#20197;&#23454;&#29616;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#20351;&#29992;&#22810;&#35821;&#31181;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#35757;&#32451;BERT&#27169;&#22411;&#26469;&#29983;&#25104;&#21476;&#24076;&#33098;&#25991;&#26412;&#30340;&#21477;&#23376;&#23884;&#20837;&#12290;&#25105;&#20204;&#36890;&#36807;&#21477;&#23376;&#23884;&#20837;&#23545;&#40784;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#23558;&#21476;&#24076;&#33098;&#25991;&#26723;&#19982;&#33521;&#25991;&#32763;&#35793;&#23545;&#40784;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contextual language models have been trained on Classical languages, including Ancient Greek and Latin, for tasks such as lemmatization, morphological tagging, part of speech tagging, authorship attribution, and detection of scribal errors. However, high-quality sentence embedding models for these historical languages are significantly more difficult to achieve due to the lack of training data. In this work, we use a multilingual knowledge distillation approach to train BERT models to produce sentence embeddings for Ancient Greek text. The state-of-the-art sentence embedding approaches for high-resource languages use massive datasets, but our distillation approach allows our Ancient Greek models to inherit the properties of these models while using a relatively small amount of translated sentence data. We build a parallel sentence dataset using a sentence-embedding alignment method to align Ancient Greek documents with English translations, and use this dataset to train our models. We 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#35270;&#35282;&#65292;&#38754;&#23545;NLP&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20154;&#21475;&#20559;&#35265;&#65292;&#25506;&#32034;&#20102;&#19977;&#20010;&#26041;&#38754;&#65292;&#35299;&#20915;&#20102;&#20165;&#20851;&#27880;&#31181;&#26063;&#21644;&#24615;&#21035;&#31561;&#26377;&#38480;&#33539;&#22260;&#20559;&#35265;&#12289;&#20197;&#21450;&#25216;&#26415;&#20013;&#24515;&#23454;&#26045;&#26041;&#27861;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.13089</link><description>&lt;p&gt;
&#26397;&#30528;&#19968;&#20010;&#25972;&#20307;&#21270;&#26041;&#27861;&#65306;&#20511;&#21161;&#36328;&#23398;&#31185;&#35270;&#35282;&#29702;&#35299;NLP&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20154;&#21475;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Towards a Holistic Approach: Understanding Sociodemographic Biases in NLP Models using an Interdisciplinary Lens. (arXiv:2308.13089v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#35270;&#35282;&#65292;&#38754;&#23545;NLP&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20154;&#21475;&#20559;&#35265;&#65292;&#25506;&#32034;&#20102;&#19977;&#20010;&#26041;&#38754;&#65292;&#35299;&#20915;&#20102;&#20165;&#20851;&#27880;&#31181;&#26063;&#21644;&#24615;&#21035;&#31561;&#26377;&#38480;&#33539;&#22260;&#20559;&#35265;&#12289;&#20197;&#21450;&#25216;&#26415;&#20013;&#24515;&#23454;&#26045;&#26041;&#27861;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#21508;&#31181;&#31038;&#20250;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#20351;&#29992;&#21644;&#24212;&#29992;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#31361;&#26174;&#20102;&#23545;&#20559;&#35265;&#21450;&#20854;&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;&#30340;&#20840;&#38754;&#29702;&#35299;&#30340;&#38656;&#27714;&#12290;&#34429;&#28982;&#20851;&#20110;NLP&#20013;&#20559;&#35265;&#30340;&#30740;&#31350;&#24050;&#32463;&#25193;&#22823;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38656;&#35201;&#27880;&#24847;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#22312;&#31181;&#26063;&#21644;&#24615;&#21035;&#20043;&#22806;&#23545;&#31038;&#20250;&#20154;&#21475;&#20559;&#35265;&#30340;&#26377;&#38480;&#20851;&#27880;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#19978;&#30340;&#29421;&#31364;&#20998;&#26512;&#33539;&#22260;&#20197;&#21450;&#25216;&#26415;&#20013;&#24515;&#30340;&#23454;&#26045;&#26041;&#27861;&#12290;&#26412;&#25991;&#27491;&#26159;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#24182;&#20513;&#23548;&#26356;&#22810;&#30340;&#36328;&#23398;&#31185;&#26041;&#27861;&#26469;&#29702;&#35299;NLP&#20013;&#30340;&#20559;&#35265;&#12290;&#35813;&#24037;&#20316;&#20998;&#20026;&#19977;&#20010;&#26041;&#38754;&#65292;&#20998;&#21035;&#25506;&#32034;NLP&#20013;&#29305;&#23450;&#26041;&#38754;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid growth in the usage and applications of Natural Language Processing (NLP) in various sociotechnical solutions has highlighted the need for a comprehensive understanding of bias and its impact on society. While research on bias in NLP has expanded, several challenges persist that require attention. These include the limited focus on sociodemographic biases beyond race and gender, the narrow scope of analysis predominantly centered on models, and the technocentric implementation approaches. This paper addresses these challenges and advocates for a more interdisciplinary approach to understanding bias in NLP. The work is structured into three facets, each exploring a specific aspect of bias in NLP.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20154;&#21475;&#23398;&#39046;&#22495;&#30340;Agent Based Models (ABMs)&#30340;&#25968;&#23398;&#35268;&#33539;&#30340;&#21512;&#36866;&#24418;&#24335;&#26415;&#35821;&#65292;&#36825;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#29702;&#35299;&#65292;&#24182;&#19982;O.D.D.&#21327;&#35758;&#30456;&#32467;&#21512;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#22797;&#21046;&#36807;&#31243;&#20013;&#30340;&#27495;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.13081</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#21475;&#29305;&#24449;&#30340;&#22266;&#23450;&#27493;&#38271;&#21333;&#26102;&#38047;&#27169;&#25311;&#30340;Agent-Based&#27169;&#22411;&#30340;&#24418;&#24335;&#21270;&#35268;&#33539;&#26415;&#35821;
&lt;/p&gt;
&lt;p&gt;
Formal specification terminology for demographic agent-based models of fixed-step single-clocked simulations. (arXiv:2308.13081v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20154;&#21475;&#23398;&#39046;&#22495;&#30340;Agent Based Models (ABMs)&#30340;&#25968;&#23398;&#35268;&#33539;&#30340;&#21512;&#36866;&#24418;&#24335;&#26415;&#35821;&#65292;&#36825;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#29702;&#35299;&#65292;&#24182;&#19982;O.D.D.&#21327;&#35758;&#30456;&#32467;&#21512;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#22797;&#21046;&#36807;&#31243;&#20013;&#30340;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20154;&#21475;&#23398;&#39046;&#22495;&#30340;Agent Based Models (ABMs)&#30340;&#25968;&#23398;&#35268;&#33539;&#30340;&#21512;&#36866;&#24418;&#24335;&#26415;&#35821;&#12290;&#30446;&#26631;ABMs&#30340;&#27169;&#25311;&#36981;&#24490;&#22266;&#23450;&#27493;&#38271;&#21333;&#26102;&#38047;&#27169;&#24335;&#12290;&#25152;&#25552;&#20986;&#30340;&#26415;&#35821;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#29702;&#35299;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#29420;&#31435;&#30340;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#35268;&#33539;&#21644;&#36873;&#25321;&#24615;&#22320;&#35760;&#24405;&#19968;&#32452;&#37325;&#35201;&#30340;&#65288;&#20154;&#21475;&#65289;ABMs&#12290;&#28982;&#32780;&#65292;&#21487;&#20197;&#24819;&#35937;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#23637;&#65292;&#36825;&#31181;&#26415;&#35821;&#21487;&#33021;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#27169;&#22411;&#25991;&#26723;&#21644;&#36890;&#20449;O.D.D.&#21327;&#35758;[Grimm&#21644;et al.&#65292;2020&#65292;Amouroux&#31561;&#65292;2010]&#21512;&#24182;&#65292;&#20197;&#20943;&#23569;&#35768;&#22810;&#27169;&#22411;&#24314;&#27169;&#32773;&#30340;&#28304;&#28304;&#19981;&#26029;&#20135;&#29983;&#30340;&#27495;&#20041;&#65292;&#20174;&#32780;&#38459;&#30861;&#27169;&#22411;&#22797;&#21046;&#12290;&#24050;&#32463;&#20986;&#29256;&#30340;&#20154;&#21475;&#27169;&#22411;&#25991;&#26723;&#65292;&#21333;&#20146;&#27169;&#22411;&#30340;&#22823;&#22823;&#31616;&#21270;&#29256;&#26412;[Gostoli&#21644;Silverman&#65292;2020]&#20316;&#20026;&#24418;&#24335;&#26415;&#35821;&#30340;&#31034;&#20363;&#65292;&#21333;&#29420;&#21457;&#24067;&#22312;[Elsheikh&#65292;2023b]&#20013;&#12290;&#35813;&#27169;&#22411;&#24050;&#34987;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This document presents adequate formal terminology for the mathematical specification of a subset of Agent Based Models (ABMs) in the field of Demography. The simulation of the targeted ABMs follows a fixed-step single-clocked pattern. The proposed terminology further improves the model understanding and can act as a stand-alone methodology for the specification and optionally the documentation of a significant set of (demographic) ABMs. Nevertheless, it is imaginable the this terminology probably with further extensions can be merged with the largely-informal widely-used model documentation and communication O.D.D. protocol [Grimm and et al., 2020, Amouroux et al., 2010] to reduce many sources of ambiguity, hindering model replications by other modelers. A published demographic model documentation, largely simplified version of the Lone Parent Model [Gostoli and Silverman, 2020] is separately published in [Elsheikh, 2023b] as illustration for the formal terminology. The model was impl
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#33021;&#20855;&#22791;&#22240;&#26524;&#24615;&#65292;&#23427;&#20204;&#21482;&#26159;&#37325;&#22797;&#23884;&#20837;&#22312;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2308.13067</link><description>&lt;p&gt;
&#22240;&#26524;&#40550;&#40521;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#35848;&#35770;&#22240;&#26524;&#24615;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#20855;&#22791;&#22240;&#26524;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Parrots: Large Language Models May Talk Causality But Are Not Causal. (arXiv:2308.13067v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13067
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#33021;&#20855;&#22791;&#22240;&#26524;&#24615;&#65292;&#23427;&#20204;&#21482;&#26159;&#37325;&#22797;&#23884;&#20837;&#22312;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#20154;&#35748;&#20026;&#35268;&#27169;&#26159;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#37096;&#25152;&#38656;&#65292;&#29978;&#33267;&#21487;&#20197;&#28085;&#30422;&#22240;&#26524;&#27169;&#22411;&#12290;&#25105;&#20204;&#26126;&#30830;&#25351;&#20986;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#33021;&#20855;&#22791;&#22240;&#26524;&#24615;&#65292;&#24182;&#35299;&#37322;&#20026;&#20160;&#20040;&#26377;&#26102;&#25105;&#20204;&#21487;&#33021;&#26377;&#36825;&#31181;&#24863;&#35273;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#24182;&#20030;&#20363;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#30340;&#23376;&#32676;&#65292;&#31216;&#20043;&#20026;&#20803;SCM&#65292;&#23427;&#22312;&#20854;&#21464;&#37327;&#20013;&#32534;&#30721;&#20851;&#20110;&#20854;&#20182;SCM&#30340;&#22240;&#26524;&#20107;&#23454;&#12290;&#25105;&#20204;&#29468;&#27979;&#65292;&#22312;LLM&#25104;&#21151;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#30340;&#24773;&#20917;&#19979;&#65292;&#32972;&#21518;&#21487;&#33021;&#23384;&#22312;&#19968;&#20010;&#30456;&#24212;&#30340;&#20803;SCM&#65292;&#22312;&#20854;&#25968;&#25454;&#20013;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#20013;&#22240;&#26524;&#20107;&#23454;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#32780;LLM&#26368;&#32456;&#26159;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22914;&#26524;&#25105;&#20204;&#30340;&#20551;&#35774;&#25104;&#31435;&#65292;&#37027;&#20040;&#36825;&#23558;&#24847;&#21619;&#30528;LLM&#23601;&#20687;&#40550;&#40521;&#19968;&#26679;&#65292;&#23427;&#20204;&#21482;&#26159;&#37325;&#22797;&#23884;&#20837;&#22312;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#25552;&#20379;&#20102;&#25903;&#25345;&#35777;&#25454;&#65292;&#34920;&#26126;&#24403;&#21069;&#30340;LLM&#29978;&#33267;&#26159;&#24369;&#8220;&#22240;&#26524;&#40550;&#40521;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Some argue scale is all what is needed to achieve AI, covering even causal models. We make it clear that large language models (LLMs) cannot be causal and give reason onto why sometimes we might feel otherwise. To this end, we define and exemplify a new subgroup of Structural Causal Model (SCM) that we call meta SCM which encode causal facts about other SCM within their variables. We conjecture that in the cases where LLM succeed in doing causal inference, underlying was a respective meta SCM that exposed correlations between causal facts in natural language on whose data the LLM was ultimately trained. If our hypothesis holds true, then this would imply that LLMs are like parrots in that they simply recite the causal knowledge embedded in the data. Our empirical analysis provides favoring evidence that current LLMs are even weak `causal parrots.'
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#21644;&#26041;&#35328;&#20013;&#30340;&#20146;&#23646;&#20851;&#31995;&#35789;&#27719;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20016;&#23500;&#35745;&#31639;&#35789;&#27719;&#36164;&#28304;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#20379;&#27983;&#35272;&#21644;&#19979;&#36733;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#25193;&#23637;&#20102;&#23545;&#20146;&#23646;&#20851;&#31995;&#26415;&#35821;&#30340;&#35821;&#35328;&#23398;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#35821;&#35328;&#21644;&#25991;&#21270;&#19978;&#30456;&#20114;&#25509;&#36817;&#30340;&#31038;&#21306;&#20013;&#22810;&#26679;&#24615;&#30340;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.13056</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#21644;&#26041;&#35328;&#20013;&#30340;&#20146;&#23646;&#20851;&#31995;&#35789;&#27719;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Lexical Diversity in Kinship Across Languages and Dialects. (arXiv:2308.13056v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#21644;&#26041;&#35328;&#20013;&#30340;&#20146;&#23646;&#20851;&#31995;&#35789;&#27719;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20016;&#23500;&#35745;&#31639;&#35789;&#27719;&#36164;&#28304;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#20379;&#27983;&#35272;&#21644;&#19979;&#36733;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#25193;&#23637;&#20102;&#23545;&#20146;&#23646;&#20851;&#31995;&#26415;&#35821;&#30340;&#35821;&#35328;&#23398;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#22312;&#35821;&#35328;&#21644;&#25991;&#21270;&#19978;&#30456;&#20114;&#25509;&#36817;&#30340;&#31038;&#21306;&#20013;&#22810;&#26679;&#24615;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#35821;&#35328;&#20197;&#22810;&#26679;&#30340;&#26041;&#24335;&#25551;&#36848;&#19990;&#30028;&#12290;&#22312;&#35789;&#27719;&#20013;&#65292;&#22810;&#26679;&#24615;&#24191;&#27867;&#23384;&#22312;&#65292;&#22914;&#35789;&#27719;&#31354;&#32570;&#21644;&#26080;&#27861;&#32763;&#35793;&#31561;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#22312;&#35745;&#31639;&#36164;&#28304;&#20013;&#65292;&#22914;&#22810;&#35821;&#31181;&#35789;&#27719;&#25968;&#25454;&#24211;&#20013;&#65292;&#22810;&#26679;&#24615;&#24456;&#23569;&#24471;&#21040;&#34920;&#31034;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#19982;&#35821;&#35328;&#22810;&#26679;&#24615;&#30456;&#20851;&#30340;&#20869;&#23481;&#65292;&#20016;&#23500;&#35745;&#31639;&#35789;&#27719;&#36164;&#28304;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#20146;&#23646;&#20851;&#31995;&#26415;&#35821;&#36827;&#34892;&#20004;&#20010;&#22823;&#35268;&#27169;&#26696;&#20363;&#30740;&#31350;&#36827;&#34892;&#39564;&#35777;&#65292;&#36825;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#22312;&#35821;&#35328;&#21644;&#25991;&#21270;&#20013;&#20855;&#26377;&#22810;&#26679;&#24615;&#30340;&#39046;&#22495;&#65306;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#28041;&#21450;&#19971;&#31181;&#38463;&#25289;&#20271;&#26041;&#35328;&#65292;&#32780;&#21478;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#28041;&#21450;&#19977;&#31181;&#21360;&#24230;&#23612;&#35199;&#20122;&#35821;&#35328;&#12290;&#25105;&#20204;&#25152;&#24471;&#21040;&#30340;&#32467;&#26524;&#20197;&#21487;&#27983;&#35272;&#21644;&#21487;&#19979;&#36733;&#30340;&#35745;&#31639;&#36164;&#28304;&#30340;&#24418;&#24335;&#25552;&#20379;&#65292;&#25193;&#23637;&#20102;&#20808;&#21069;&#23545;&#20146;&#23646;&#20851;&#31995;&#26415;&#35821;&#30340;&#35821;&#35328;&#23398;&#30740;&#31350;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#22312;&#35821;&#35328;&#21644;&#25991;&#21270;&#19978;&#30456;&#20114;&#25509;&#36817;&#30340;&#31038;&#21306;&#20013;&#22810;&#26679;&#24615;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Languages are known to describe the world in diverse ways. Across lexicons, diversity is pervasive, appearing through phenomena such as lexical gaps and untranslatability. However, in computational resources, such as multilingual lexical databases, diversity is hardly ever represented. In this paper, we introduce a method to enrich computational lexicons with content relating to linguistic diversity. The method is verified through two large-scale case studies on kinship terminology, a domain known to be diverse across languages and cultures: one case study deals with seven Arabic dialects, while the other one with three Indonesian languages. Our results, made available as browseable and downloadable computational resources, extend prior linguistics research on kinship terminology, and provide insight into the extent of diversity even within linguistically and culturally close communities.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#23454;&#29616;&#20102;&#37329;&#34701;&#26032;&#38395;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#26512;&#12289;&#25688;&#35201;&#21644;&#24773;&#24863;&#25552;&#21462;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#26377;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.13032</link><description>&lt;p&gt;
&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2 GPT&#27169;&#22411;&#36827;&#34892;&#37329;&#34701;&#26032;&#38395;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Financial News Analytics Using Fine-Tuned Llama 2 GPT Model. (arXiv:2308.13032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#23454;&#29616;&#20102;&#37329;&#34701;&#26032;&#38395;&#30340;&#22810;&#20219;&#21153;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#20998;&#26512;&#12289;&#25688;&#35201;&#21644;&#24773;&#24863;&#25552;&#21462;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#20316;&#20026;&#26377;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2 Large Language Model (LLM) &#23545;&#37329;&#34701;&#26032;&#38395;&#36827;&#34892;&#22810;&#20219;&#21153;&#20998;&#26512;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;PEFT/LoRA&#26041;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#65292;&#20027;&#35201;&#21253;&#25324;&#20174;&#37329;&#34701;&#24066;&#22330;&#35282;&#24230;&#20998;&#26512;&#25991;&#26412;&#12289;&#31361;&#20986;&#25991;&#26412;&#30340;&#20027;&#35201;&#35266;&#28857;&#12289;&#23545;&#25991;&#26412;&#36827;&#34892;&#25688;&#35201;&#21644;&#25552;&#21462;&#20855;&#26377;&#36866;&#24403;&#24773;&#24863;&#30340;&#21629;&#21517;&#23454;&#20307;&#31561;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Llama 2&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#22810;&#20219;&#21153;&#30340;&#37329;&#34701;&#26032;&#38395;&#20998;&#26512;&#65292;&#20854;&#21709;&#24212;&#30340;&#32467;&#26500;&#21487;&#20197;&#37096;&#20998;&#20026;&#32467;&#26500;&#21270;&#25991;&#26412;&#65292;&#21478;&#19968;&#37096;&#20998;&#25968;&#25454;&#21487;&#20197;&#37319;&#29992;JSON&#26684;&#24335;&#36827;&#19968;&#27493;&#22788;&#29702;&#12290;&#25552;&#21462;&#30340;&#21629;&#21517;&#23454;&#20307;&#24773;&#24863;&#21487;&#20197;&#34987;&#35270;&#20026;&#20855;&#26377;&#23450;&#37327;&#30446;&#26631;&#21464;&#37327;&#30340;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper considers the possibility to fine-tune Llama 2 Large Language Model (LLM) for the multitask analysis of financial news. For fine-tuning, the PEFT/LoRA based approach was used. In the study, the model was fine-tuned for the following tasks: analysing a text from financial market perspectives, highlighting main points of a text, summarizing a text and extracting named entities with appropriate sentiments. The obtained results show that the fine-tuned Llama 2 model can perform a multitask financial news analysis with a specified structure of response, part of response can be a structured text and another part of data can have JSON format for further processing. Extracted sentiments for named entities can be considered as predictive features in supervised machine learning models with quantitative target variables.
&lt;/p&gt;</description></item><item><title>Code Llama&#26159;&#19968;&#31995;&#21015;&#29992;&#20110;&#20195;&#30721;&#30340;&#24320;&#25918;&#22522;&#30784;&#27169;&#22411;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#22635;&#20805;&#21151;&#33021;&#65292;&#25903;&#25345;&#22823;&#22411;&#36755;&#20837;&#19978;&#19979;&#25991;&#21644;&#38646;-shot&#25351;&#20196;&#36319;&#36394;&#33021;&#21147;&#12290;&#22312;&#22810;&#20010;&#20195;&#30721;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;Code Llama&#36798;&#21040;&#24320;&#25918;&#27169;&#22411;&#20013;&#26368;&#39640;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;Python&#19987;&#38376;&#21270;&#27169;&#22411;&#22312;&#26576;&#20123;&#27979;&#35797;&#19978;&#36229;&#36234;&#20102;Llama 2&#30340;70B&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.12950</link><description>&lt;p&gt;
Code Llama: &#29992;&#20110;&#20195;&#30721;&#30340;&#24320;&#25918;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Code Llama: Open Foundation Models for Code. (arXiv:2308.12950v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12950
&lt;/p&gt;
&lt;p&gt;
Code Llama&#26159;&#19968;&#31995;&#21015;&#29992;&#20110;&#20195;&#30721;&#30340;&#24320;&#25918;&#22522;&#30784;&#27169;&#22411;&#65292;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#22635;&#20805;&#21151;&#33021;&#65292;&#25903;&#25345;&#22823;&#22411;&#36755;&#20837;&#19978;&#19979;&#25991;&#21644;&#38646;-shot&#25351;&#20196;&#36319;&#36394;&#33021;&#21147;&#12290;&#22312;&#22810;&#20010;&#20195;&#30721;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;Code Llama&#36798;&#21040;&#24320;&#25918;&#27169;&#22411;&#20013;&#26368;&#39640;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;Python&#19987;&#38376;&#21270;&#27169;&#22411;&#22312;&#26576;&#20123;&#27979;&#35797;&#19978;&#36229;&#36234;&#20102;Llama 2&#30340;70B&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21457;&#24067;&#20102;Code Llama&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#22522;&#20110;Llama 2&#30340;&#29992;&#20110;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24320;&#25918;&#27169;&#22411;&#20013;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22635;&#20805;&#21151;&#33021;&#65292;&#25903;&#25345;&#22823;&#22411;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#33021;&#22815;&#36827;&#34892;&#38646;-shot&#25351;&#20196;&#36319;&#36394;&#32534;&#31243;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20379;&#22810;&#31181;&#29256;&#26412;&#20197;&#35206;&#30422;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#65306;&#22522;&#30784;&#27169;&#22411;&#65288;Code Llama&#65289;&#65292;Python&#19987;&#38376;&#21270;&#27169;&#22411;&#65288;Code Llama-Python&#65289;&#65292;&#20197;&#21450;&#25351;&#20196;&#36319;&#36394;&#27169;&#22411;&#65288;Code Llama-Instruct&#65289;&#65292;&#27599;&#20010;&#27169;&#22411;&#21442;&#25968;&#20998;&#21035;&#20026;7B&#12289;13B&#21644;34B&#12290;&#25152;&#26377;&#27169;&#22411;&#37117;&#26159;&#22312;16k&#26631;&#35760;&#24207;&#21015;&#19978;&#35757;&#32451;&#30340;&#65292;&#21487;&#20197;&#25913;&#21892;&#38271;&#24230;&#19981;&#36229;&#36807;100k&#26631;&#35760;&#30340;&#36755;&#20837;&#12290;7B&#21644;13B&#30340;Code Llama&#21644;Code Llama-Instruct&#21464;&#31181;&#20250;&#26681;&#25454;&#21608;&#22260;&#20869;&#23481;&#36827;&#34892;&#22635;&#20805;&#12290;Code Llama&#22312;&#20960;&#20010;&#20195;&#30721;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#24320;&#25918;&#27169;&#22411;&#20013;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;HumanEval&#21644;MBPP&#20998;&#21035;&#36798;&#21040;&#20102;53%&#21644;55%&#30340;&#20998;&#25968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;Code Llama-Python 7B&#22312;HumanEval&#21644;MBPP&#19978;&#20248;&#20110;Llama 2 70B&#65292;&#32780;&#25105;&#20204;&#30340;&#25152;&#26377;&#27169;&#22411;&#37117;&#20248;&#20110;&#20854;&#20182;&#20219;&#20309;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B and 34B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B and 13B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 53% and 55% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every othe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#30693;&#35782;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#30340;&#20316;&#29992;&#65292;&#35774;&#35745;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23545;&#40784;&#25506;&#27979;&#22522;&#20934;&#26469;&#26816;&#27979;&#20851;&#38190;&#30340;&#35821;&#35328;&#32452;&#25104;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.12898</link><description>&lt;p&gt;
&#35821;&#35328;&#30693;&#35782;&#33021;&#25913;&#36827;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?. (arXiv:2308.12898v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#30693;&#35782;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#30340;&#20316;&#29992;&#65292;&#35774;&#35745;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23545;&#40784;&#25506;&#27979;&#22522;&#20934;&#26469;&#26816;&#27979;&#20851;&#38190;&#30340;&#35821;&#35328;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23186;&#20307;&#39046;&#22495;&#23545;&#36890;&#36807;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24863;&#30693;&#21644;&#34920;&#36798;&#29289;&#29702;&#19990;&#30028;&#23637;&#29616;&#20986;&#20102;&#24378;&#28872;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#35270;&#35273;-&#35821;&#35328;&#30456;&#20851;&#30340;&#30740;&#31350;&#26159;&#24403;&#21069;&#26368;&#21560;&#24341;&#20154;&#30340;&#35805;&#39064;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20197;&#19979;&#20004;&#20010;&#38382;&#39064;&#30340;&#25506;&#32034;&#38750;&#24120;&#26377;&#38480;&#65306;1&#65289;&#22312;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#26159;&#21542;&#21487;&#20197;&#25552;&#21462;&#20851;&#38190;&#30340;&#35821;&#35328;&#30693;&#35782;&#65288;&#22914;&#35821;&#20041;&#21644;&#21477;&#27861;&#65289;&#65292;2&#65289;&#36825;&#31181;&#35821;&#35328;&#30693;&#35782;&#22914;&#20309;&#24433;&#21709;&#25110;&#22686;&#24378;&#22810;&#27169;&#24577;&#23545;&#40784;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#38416;&#26126;&#20840;&#38754;&#30340;&#35821;&#35328;&#30693;&#35782;&#65292;&#21253;&#25324;&#35821;&#20041;&#34920;&#36798;&#21644;&#21477;&#27861;&#32467;&#26500;&#65292;&#23545;&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#21457;&#24067;&#20102;SNARE&#65292;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#25506;&#27979;&#22522;&#20934;&#65292;&#26469;&#26816;&#27979;&#20851;&#38190;&#30340;&#35821;&#35328;&#32452;&#25104;&#37096;&#20998;&#65292;&#22914;&#35789;&#27719;&#12289;&#35821;&#20041;&#21644;&#21477;&#27861;&#30693;&#35782;&#65292;&#21253;&#21547;&#20102;&#22235;&#20010;&#20219;&#21153;&#65306;&#35821;&#20041;&#32467;&#26500;&#12289;&#21542;&#23450;&#36923;&#36753;&#12289;&#23646;&#24615;&#24402;&#23646;&#21644;&#20851;&#31995;&#32452;&#21512;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;.....
&lt;/p&gt;
&lt;p&gt;
The multimedia community has shown a significant interest in perceiving and representing the physical world with multimodal pretrained neural network models, and among them, the visual-language pertaining (VLP) is, currently, the most captivating topic. However, there have been few endeavors dedicated to the exploration of 1) whether essential linguistic knowledge (e.g., semantics and syntax) can be extracted during VLP, and 2) how such linguistic knowledge impact or enhance the multimodal alignment. In response, here we aim to elucidate the impact of comprehensive linguistic knowledge, including semantic expression and syntactic structure, on multimodal alignment. Specifically, we design and release the SNARE, the first large-scale multimodal alignment probing benchmark, to detect the vital linguistic components, e.g., lexical, semantic, and syntax knowledge, containing four tasks: Semantic structure, Negation logic, Attribute ownership, and Relationship composition. Based on our prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12219</link><description>&lt;p&gt;
&#25193;&#23637;&#24615;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23436;&#25104;&#22810;&#31181;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. (arXiv:2308.12219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36215;&#24471;&#30410;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#35299;&#20915;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#26041;&#38754;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#25105;&#20204;&#36890;&#36807;&#20808;&#36890;&#36807;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#39044;&#35757;&#32451;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#20877;&#36890;&#36807;&#25193;&#25955;&#36866;&#24212;&#23558;&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20026;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#21644;&#25351;&#23548;&#35843;&#20248;&#26469;&#21457;&#25496;&#20854;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#20013;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream langua
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#38450;&#28779;&#22681;&#30340;&#27010;&#24565;&#21644;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11521</link><description>&lt;p&gt;
&#33258;&#25105;&#27450;&#39575;&#65306;&#36870;&#21521;&#30772;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#38450;&#28779;&#22681;
&lt;/p&gt;
&lt;p&gt;
Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models. (arXiv:2308.11521v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11521
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#38450;&#28779;&#22681;&#30340;&#27010;&#24565;&#21644;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#65292;&#20855;&#26377;&#25509;&#36817;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#24778;&#20154;&#33021;&#21147;&#12290;&#34429;&#28982;&#20026;&#21508;&#31181;&#31038;&#20250;&#38656;&#27714;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#20294;LLM&#20063;&#38477;&#20302;&#20102;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;LLM&#24320;&#21457;&#20154;&#21592;&#24050;&#32463;&#37096;&#32626;&#20102;&#35821;&#20041;&#32423;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25298;&#32477;&#21487;&#33021;&#23548;&#33268;&#19981;&#36866;&#24403;&#20869;&#23481;&#30340;&#25552;&#31034;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#38450;&#24481;&#26426;&#21046;&#24182;&#19981;&#23436;&#20840;&#21487;&#38752;&#65292;&#19968;&#20123;&#25915;&#20987;&#32773;&#24050;&#32463;&#35774;&#35745;&#20986;&#20102;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#65292;&#20020;&#26102;&#20351;LLM&#24536;&#35760;&#20869;&#23481;&#38450;&#24481;&#35268;&#21017;&#24182;&#22238;&#31572;&#20219;&#20309;&#19981;&#36866;&#24403;&#30340;&#38382;&#39064;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#23578;&#26080;&#20851;&#20110;&#36825;&#20123;&#35821;&#20041;&#32423;&#25915;&#20987;&#21644;&#38450;&#24481;&#21407;&#21017;&#30340;&#26126;&#30830;&#35299;&#37322;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#36234;&#29425;&#38382;&#39064;&#65292;&#24182;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#20041;&#38450;&#28779;&#22681;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT, have emerged with astonishing capabilities approaching artificial general intelligence. While providing convenience for various societal needs, LLMs have also lowered the cost of generating harmful content. Consequently, LLM developers have deployed semantic-level defenses to recognize and reject prompts that may lead to inappropriate content. Unfortunately, these defenses are not foolproof, and some attackers have crafted "jailbreak" prompts that temporarily hypnotize the LLM into forgetting content defense rules and answering any improper questions. To date, there is no clear explanation of the principles behind these semantic-level attacks and defenses in both industry and academia.  This paper investigates the LLM jailbreak problem and proposes an automatic jailbreak method for the first time. We propose the concept of a semantic firewall and provide three technical implementation approaches. Inspired by the attack that penetrates trad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;&#26080;&#30417;&#30563;&#21407;&#22411;&#36866;&#37197;&#22120;&#65288;UP-Adapter&#65289;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;CLIP&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#33021;&#21147;&#65292;&#38024;&#23545;&#26410;&#26631;&#27880;&#30340;&#30446;&#26631;&#25968;&#25454;&#38598;&#33258;&#21160;&#36873;&#25321;&#33258;&#20449;&#24230;&#26368;&#39640;&#30340;&#26679;&#26412;&#65292;&#24182;&#29983;&#25104;&#31867;&#21035;&#21407;&#22411;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2308.11507</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21407;&#22411;&#36866;&#37197;&#22120;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Prototype Adapter for Vision-Language Models. (arXiv:2308.11507v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;&#26080;&#30417;&#30563;&#21407;&#22411;&#36866;&#37197;&#22120;&#65288;UP-Adapter&#65289;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;CLIP&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#33021;&#21147;&#65292;&#38024;&#23545;&#26410;&#26631;&#27880;&#30340;&#30446;&#26631;&#25968;&#25454;&#38598;&#33258;&#21160;&#36873;&#25321;&#33258;&#20449;&#24230;&#26368;&#39640;&#30340;&#26679;&#26412;&#65292;&#24182;&#29983;&#25104;&#31867;&#21035;&#21407;&#22411;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#21644;ALIGN&#65289;&#22312;&#33719;&#21462;&#21487;&#36716;&#31227;&#30340;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#23453;&#36149;&#30693;&#35782;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#24494;&#35843;&#26041;&#27861;&#65292;&#21253;&#25324;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#21644;&#36866;&#37197;&#22120;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#36866;&#24212;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#30417;&#30563;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#21487;&#33719;&#24471;&#30340;&#26631;&#27880;&#26679;&#26412;&#65292;&#36825;&#21487;&#33021;&#32791;&#26102;&#19988;&#36153;&#21147;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;&#26080;&#30417;&#30563;&#21407;&#22411;&#36866;&#37197;&#22120;&#65288;UP-Adapter&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#26410;&#26631;&#27880;&#30340;&#30446;&#26631;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21033;&#29992;CLIP&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#33021;&#21147;&#33258;&#21160;&#36873;&#25321;&#27599;&#20010;&#31867;&#21035;&#30340;&#26368;&#33258;&#20449;&#26679;&#26412;&#12290;&#21033;&#29992;&#36825;&#20123;&#36873;&#25321;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#29983;&#25104;&#31867;&#21035;&#21407;&#22411;&#65292;&#36825;&#23558;&#20026;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large-scale pre-trained vision-language models (e.g. CLIP and ALIGN) have demonstrated remarkable effectiveness in acquiring transferable visual representations. To leverage the valuable knowledge encoded within these models for downstream tasks, several fine-tuning approaches, including prompt tuning methods and adapter-based methods, have been developed to adapt vision-language models effectively with supervision. However, these methods rely on the availability of annotated samples, which can be labor-intensive and time-consuming to acquire, thus limiting scalability. To address this issue, in this work, we design an unsupervised fine-tuning approach for vision-language models called Unsupervised Prototype Adapter (UP-Adapter). Specifically, for the unannotated target datasets, we leverage the text-image aligning capability of CLIP to automatically select the most confident samples for each class. Utilizing these selected samples, we generate class prototypes, which serve a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20316;&#32773;&#36523;&#20221;&#34920;&#24449;&#23398;&#20064;&#33021;&#21542;&#25429;&#25417;&#25991;&#20307;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#34920;&#24449;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#20889;&#20316;&#39118;&#26684;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.11490</link><description>&lt;p&gt;
&#20316;&#32773;&#36523;&#20221;&#34920;&#24449;&#23398;&#20064;&#33021;&#22815;&#25429;&#25417;&#25991;&#20307;&#29305;&#24449;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Authorship Representation Learning Capture Stylistic Features?. (arXiv:2308.11490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20316;&#32773;&#36523;&#20221;&#34920;&#24449;&#23398;&#20064;&#33021;&#21542;&#25429;&#25417;&#25991;&#20307;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#34920;&#24449;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#20889;&#20316;&#39118;&#26684;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#20013;&#65292;&#33258;&#21160;&#23558;&#20316;&#32773;&#30340;&#39118;&#26684;&#20174;&#20854;&#20889;&#20316;&#20869;&#23481;&#20013;&#20998;&#31163;&#20986;&#26469;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#19988;&#21487;&#33021;&#19981;&#21487;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#26368;&#36817;&#26377;&#22823;&#37327;&#24102;&#26377;&#20316;&#32773;&#26631;&#31614;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#21487;&#29992;&#65292;&#20351;&#24471;&#20197;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#23398;&#20064;&#20316;&#32773;&#36523;&#20221;&#34920;&#24449;&#25104;&#20026;&#21487;&#33021;&#65292;&#29992;&#20110;&#20316;&#32773;&#24402;&#23646;&#36825;&#19968;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26174;&#28982;&#26356;&#22810;&#22320;&#20381;&#36182;&#20110;&#32534;&#30721;&#20889;&#20316;&#39118;&#26684;&#32780;&#19981;&#26159;&#32534;&#30721;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#19968;&#26367;&#20195;&#20219;&#21153;&#30340;&#25104;&#21151;&#24182;&#19981;&#33021;&#30830;&#20445;&#36825;&#20123;&#34920;&#24449;&#33021;&#22815;&#25429;&#25417;&#20889;&#20316;&#39118;&#26684;&#65292;&#22240;&#20026;&#20316;&#32773;&#36523;&#20221;&#20063;&#21487;&#33021;&#19982;&#20854;&#20182;&#28508;&#22312;&#21464;&#37327;&#65288;&#22914;&#20027;&#39064;&#65289;&#30456;&#20851;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#34920;&#24449;&#25152;&#20256;&#36882;&#30340;&#20449;&#24687;&#30340;&#26412;&#36136;&#65292;&#29305;&#21035;&#26159;&#20026;&#20102;&#39564;&#35777;&#20854;&#20027;&#35201;&#32534;&#30721;&#30340;&#26159;&#20889;&#20316;&#39118;&#26684;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#26377;&#38024;&#23545;&#24615;&#30340;&#23454;&#39564;&#31995;&#32479;&#22320;&#26816;&#26597;&#20102;&#36825;&#20123;&#34920;&#24449;&#12290;&#36825;&#20123;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20026;&#20316;&#32773;&#34920;&#31034;&#23398;&#20064;&#30340;&#34920;&#24449;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#20889;&#20316;&#39118;&#26684;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically disentangling an author's style from the content of their writing is a longstanding and possibly insurmountable problem in computational linguistics. At the same time, the availability of large text corpora furnished with author labels has recently enabled learning authorship representations in a purely data-driven manner for authorship attribution, a task that ostensibly depends to a greater extent on encoding writing style than encoding content. However, success on this surrogate task does not ensure that such representations capture writing style since authorship could also be correlated with other latent variables, such as topic. In an effort to better understand the nature of the information these representations convey, and specifically to validate the hypothesis that they chiefly encode writing style, we systematically probe these representations through a series of targeted experiments. The results of these experiments suggest that representations learned for the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#24320;&#21457;&#30340;&#22810;&#20998;&#31867;&#31995;&#32479;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20116;&#31181;&#35821;&#35328;&#26465;&#20214;&#19979;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20013;&#30340;&#24656;&#21516;&#21644;&#24656;&#24807;&#36328;&#24615;&#21035;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#26102;&#31354;&#30456;&#20851;&#30340;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#25968;&#25454;&#23545;&#36328;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#37325;&#26032;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#34920;&#29616;&#26368;&#22909;&#30340;&#19971;&#31181;&#26631;&#31614;&#20998;&#31867;&#31995;&#32479;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26102;&#31354;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#26377;&#25928;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20013;&#30340;&#24656;&#21516;&#21644;&#24656;&#24807;&#36328;&#24615;&#21035;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2308.10370</link><description>&lt;p&gt;
cantnlp@LT-EDI-2023: &#20351;&#29992;&#26102;&#31354;&#37325;&#26032;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20013;&#26816;&#27979;&#24656;&#21516;&#19982;&#24656;&#24807;&#36328;&#24615;&#21035;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
cantnlp@LT-EDI-2023: Homophobia/Transphobia Detection in Social Media Comments using Spatio-Temporally Retrained Language Models. (arXiv:2308.10370v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#24320;&#21457;&#30340;&#22810;&#20998;&#31867;&#31995;&#32479;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20116;&#31181;&#35821;&#35328;&#26465;&#20214;&#19979;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20013;&#30340;&#24656;&#21516;&#21644;&#24656;&#24807;&#36328;&#24615;&#21035;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#26102;&#31354;&#30456;&#20851;&#30340;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#25968;&#25454;&#23545;&#36328;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#37325;&#26032;&#35757;&#32451;&#65292;&#21462;&#24471;&#20102;&#34920;&#29616;&#26368;&#22909;&#30340;&#19971;&#31181;&#26631;&#31614;&#20998;&#31867;&#31995;&#32479;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26102;&#31354;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#26377;&#25928;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20013;&#30340;&#24656;&#21516;&#21644;&#24656;&#24807;&#36328;&#24615;&#21035;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#20316;&#20026;LTEDI@RANLP-2023&#20849;&#20139;&#20219;&#21153;&#30340;&#19968;&#37096;&#20998;&#24320;&#21457;&#30340;&#22810;&#20998;&#31867;&#31995;&#32479;&#12290;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20013;&#30340;&#24656;&#21516;&#21644;&#24656;&#24807;&#36328;&#24615;&#21035;&#20869;&#23481;&#65292;&#28085;&#30422;&#33521;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#21644;&#27888;&#31859;&#23572;&#35821;&#20116;&#31181;&#35821;&#35328;&#26465;&#20214;&#12290;&#25105;&#20204;&#20351;&#29992;&#26102;&#31354;&#30456;&#20851;&#30340;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#25968;&#25454;&#23545;&#22522;&#20110;Transformer&#30340;&#36328;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;XLMRoBERTa&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#27169;&#25311;&#30340;&#28151;&#21512;&#33050;&#26412;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#25968;&#25454;&#23545;&#37096;&#20998;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#65292;&#20854;&#24615;&#33021;&#26377;&#25152;&#21464;&#21270;&#12290;&#25105;&#20204;&#20026;&#39532;&#25289;&#38597;&#25289;&#22982;&#35821;&#24320;&#21457;&#20102;&#34920;&#29616;&#26368;&#22909;&#30340;&#19971;&#31181;&#26631;&#31614;&#20998;&#31867;&#31995;&#32479;&#65292;&#22522;&#20110;&#21152;&#26435;&#23439;&#24179;&#22343;F1&#24471;&#20998;&#65288;&#22312;&#20845;&#20010;&#31995;&#32479;&#20013;&#25490;&#21517;&#31532;&#19968;&#65289;&#65292;&#20854;&#20182;&#35821;&#35328;&#21644;&#31867;&#21035;&#26631;&#31614;&#26465;&#20214;&#19979;&#30340;&#24615;&#33021;&#26377;&#25152;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23558;&#36825;&#31181;&#26102;&#31354;&#25968;&#25454;&#32435;&#20837;&#20998;&#31867;&#24615;&#33021;&#30456;&#23545;&#20110;&#22522;&#32447;&#27169;&#22411;&#22312;&#25152;&#26377;&#35821;&#35328;&#21644;&#20219;&#21153;&#26465;&#20214;&#19979;&#37117;&#26377;&#25152;&#25552;&#21319;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#32463;&#36807;&#26102;&#31354;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#35780;&#35770;&#20013;&#30340;&#24656;&#21516;&#19982;&#24656;&#24807;&#36328;&#24615;&#21035;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our multiclass classification system developed as part of the LTEDI@RANLP-2023 shared task. We used a BERT-based language model to detect homophobic and transphobic content in social media comments across five language conditions: English, Spanish, Hindi, Malayalam, and Tamil. We retrained a transformer-based crosslanguage pretrained language model, XLMRoBERTa, with spatially and temporally relevant social media language data. We also retrained a subset of models with simulated script-mixed social media language data with varied performance. We developed the best performing seven-label classification system for Malayalam based on weighted macro averaged F1 score (ranked first out of six) with variable performance for other language and class-label conditions. We found the inclusion of this spatio-temporal data improved the classification performance for all language and task conditions when compared with the baseline. The results suggests that transformer-based lan
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#23436;&#20840;&#25351;&#23450;&#30340;&#24418;&#24335;&#35821;&#35328;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#22312;&#35821;&#27861;&#24402;&#32435;&#20219;&#21153;&#20013;&#20351;&#29992;&#35813;&#22522;&#20934;&#35780;&#20272;&#20102;&#19981;&#21516;&#26550;&#26500;&#30340;&#32593;&#32476;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#30446;&#26631;&#65288;MDL&#65289;&#35757;&#32451;&#30340;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#26356;&#22909;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.08253</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#22312;&#35821;&#27861;&#24402;&#32435;&#20219;&#21153;&#20013;&#30340;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Neural Network Generalization for Grammar Induction. (arXiv:2308.08253v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08253
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#23436;&#20840;&#25351;&#23450;&#30340;&#24418;&#24335;&#35821;&#35328;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#22312;&#35821;&#27861;&#24402;&#32435;&#20219;&#21153;&#20013;&#20351;&#29992;&#35813;&#22522;&#20934;&#35780;&#20272;&#20102;&#19981;&#21516;&#26550;&#26500;&#30340;&#32593;&#32476;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#30446;&#26631;&#65288;MDL&#65289;&#35757;&#32451;&#30340;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#26356;&#22909;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#22914;&#20309;&#65311;&#21363;&#20351;&#23545;&#20110;&#35821;&#27861;&#24402;&#32435;&#20219;&#21153;&#36825;&#26679;&#30446;&#26631;&#27867;&#21270;&#23436;&#20840;&#24050;&#30693;&#30340;&#20219;&#21153;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#20063;&#26410;&#33021;&#32473;&#20986;&#26126;&#30830;&#30340;&#31572;&#26696;&#65292;&#21482;&#22312;&#35757;&#32451;&#38598;&#20043;&#22806;&#36827;&#34892;&#20102;&#38750;&#24120;&#26377;&#38480;&#30340;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#25104;&#21151;&#26631;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23436;&#20840;&#25351;&#23450;&#30340;&#24418;&#24335;&#35821;&#35328;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24230;&#37327;&#26041;&#27861;&#12290;&#32473;&#23450;&#19968;&#20010;&#27169;&#22411;&#21644;&#19968;&#20010;&#24418;&#24335;&#35821;&#27861;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#27169;&#22411;&#22312;&#26410;&#35265;&#26679;&#26412;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#20998;&#37197;&#19968;&#20010;&#27867;&#21270;&#24471;&#20998;&#65292;&#36825;&#20010;&#24471;&#20998;&#19982;&#27169;&#22411;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#25104;&#21453;&#27604;&#12290;&#36825;&#20010;&#22522;&#20934;&#21253;&#21547;&#20102;&#35832;&#22914;$a^nb^n$&#65292;$a^nb^nc^n$&#65292;$a^nb^mc^{n+m}$&#20197;&#21450;Dyck-1&#21644;2&#31561;&#35821;&#35328;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#35780;&#20272;&#20102;&#19968;&#20123;&#26550;&#26500;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#30446;&#26631;&#65288;MDL&#65289;&#35757;&#32451;&#30340;&#32593;&#32476;&#27604;&#20351;&#29992;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#26356;&#22909;&#19988;&#20351;&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#12290;&#35813;&#22522;&#20934;&#21487;&#22312;https://github.com/taucompling/bliss&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
How well do neural networks generalize? Even for grammar induction tasks, where the target generalization is fully known, previous works have left the question open, testing very limited ranges beyond the training set and using different success criteria. We provide a measure of neural network generalization based on fully specified formal languages. Given a model and a formal grammar, the method assigns a generalization score representing how well a model generalizes to unseen samples in inverse relation to the amount of data it was trained on. The benchmark includes languages such as $a^nb^n$, $a^nb^nc^n$, $a^nb^mc^{n+m}$, and Dyck-1 and 2. We evaluate selected architectures using the benchmark and find that networks trained with a Minimum Description Length objective (MDL) generalize better and using less data than networks trained using standard loss functions. The benchmark is available at https://github.com/taucompling/bliss.
&lt;/p&gt;</description></item><item><title>ChatMOF&#26159;&#19968;&#31181;&#33258;&#20027;AI&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#20851;&#38190;&#32454;&#33410;&#65292;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#22238;&#24212;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32452;&#21512;&#20195;&#29702;&#12289;&#24037;&#20855;&#21253;&#21644;&#35780;&#20272;&#22120;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26816;&#32034;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#32467;&#26500;&#29983;&#25104;&#31561;&#22810;&#20010;&#20219;&#21153;&#12290;&#30740;&#31350;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01423</link><description>&lt;p&gt;
ChatMOF: &#19968;&#31181;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;
&lt;/p&gt;
&lt;p&gt;
ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks. (arXiv:2308.01423v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01423
&lt;/p&gt;
&lt;p&gt;
ChatMOF&#26159;&#19968;&#31181;&#33258;&#20027;AI&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#20851;&#38190;&#32454;&#33410;&#65292;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#22238;&#24212;&#12290;&#35813;&#31995;&#32479;&#36890;&#36807;&#32452;&#21512;&#20195;&#29702;&#12289;&#24037;&#20855;&#21253;&#21644;&#35780;&#20272;&#22120;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26816;&#32034;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#32467;&#26500;&#29983;&#25104;&#31561;&#22810;&#20010;&#20219;&#21153;&#12290;&#30740;&#31350;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21183;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatMOF&#26159;&#19968;&#20010;&#33258;&#20027;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#39044;&#27979;&#21644;&#29983;&#25104;&#37329;&#23646;-&#26377;&#26426;&#39592;&#26550;&#65288;MOFs&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;gpt-3.5-turbo&#65289;&#65292;ChatMOF&#20174;&#25991;&#26412;&#36755;&#20837;&#20013;&#25552;&#21462;&#20851;&#38190;&#32454;&#33410;&#24182;&#25552;&#20379;&#36866;&#24403;&#30340;&#22238;&#24212;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#23545;&#21018;&#24615;&#32467;&#26500;&#21270;&#26597;&#35810;&#30340;&#38656;&#27714;&#12290;&#35813;&#31995;&#32479;&#30001;&#19977;&#20010;&#26680;&#24515;&#32452;&#20214;&#65288;&#21363;&#20195;&#29702;&#12289;&#24037;&#20855;&#21253;&#21644;&#35780;&#20272;&#22120;&#65289;&#32452;&#25104;&#65292;&#24418;&#25104;&#19968;&#20010;&#24378;&#22823;&#30340;&#27969;&#27700;&#32447;&#65292;&#31649;&#29702;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#25968;&#25454;&#26816;&#32034;&#12289;&#24615;&#36136;&#39044;&#27979;&#21644;&#32467;&#26500;&#29983;&#25104;&#12290;&#35813;&#30740;&#31350;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#22312;&#26448;&#26009;&#31185;&#23398;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20248;&#28857;&#21644;&#38480;&#21046;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#23545;&#26410;&#26469;&#21457;&#23637;&#30340;&#21464;&#38761;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatMOF is an autonomous Artificial Intelligence (AI) system that is built to predict and generate of metal-organic frameworks (MOFs). By leveraging a large-scale language model (gpt-3.5-turbo), ChatMOF extracts key details from textual inputs and delivers appropriate responses, thus eliminating the necessity for rigid structured queries. The system is comprised of three core components (i.e. an agent, a toolkit, and an evaluator) and it forms a robust pipeline that manages a variety of tasks, including data retrieval, property prediction, and structure generation. The study further explores the merits and constraints of using large language models (LLMs) AI system in material sciences using and showcases its transformative potential for future advancements.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;ChatGPT&#30340;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#20998;&#31867;&#23398;&#24635;&#32467;&#65292;&#24182;&#25506;&#32034;&#20102;&#20854;&#22312;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25209;&#21028;&#24615;&#20998;&#26512;&#20102;&#29616;&#26377;&#25991;&#29486;&#21644;ChatGPT&#22312;&#35299;&#20915;&#29616;&#23454;&#25361;&#25112;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.14107</link><description>&lt;p&gt;
&#35299;&#30721;ChatGPT&#65306;&#29616;&#26377;&#30740;&#31350;&#12289;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#21487;&#33021;&#26041;&#21521;&#30340;&#20998;&#31867;&#23398;
&lt;/p&gt;
&lt;p&gt;
Decoding ChatGPT: A Taxonomy of Existing Research, Current Challenges, and Possible Future Directions. (arXiv:2307.14107v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;ChatGPT&#30340;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#20998;&#31867;&#23398;&#24635;&#32467;&#65292;&#24182;&#25506;&#32034;&#20102;&#20854;&#22312;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#28508;&#21147;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#25209;&#21028;&#24615;&#20998;&#26512;&#20102;&#29616;&#26377;&#25991;&#29486;&#21644;ChatGPT&#22312;&#35299;&#20915;&#29616;&#23454;&#25361;&#25112;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;2022&#24180;11&#26376;&#21457;&#24067;&#20197;&#26469;&#65292;Chat GPT&#65288;Chat Generative Pre-trained Transformer&#65289;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#21644;&#20851;&#27880;&#12290;&#23427;&#22312;&#21253;&#25324;&#32771;&#35797;&#36890;&#36807;&#21644;&#21019;&#36896;&#24615;&#20889;&#20316;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#19982;&#20559;&#35265;&#21644;&#20449;&#20219;&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#25285;&#24551;&#20381;&#28982;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;ChatGPT&#30340;100&#22810;&#31687;Scopus&#32034;&#24341;&#30340;&#20986;&#29256;&#29289;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#65292;&#26088;&#22312;&#25552;&#20379;ChatGPT&#30740;&#31350;&#30340;&#20998;&#31867;&#23398;&#65292;&#24182;&#25506;&#32034;&#20854;&#24212;&#29992;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#25991;&#29486;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#30740;&#31350;&#20013;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;ChatGPT&#22312;&#21307;&#30103;&#12289;&#33829;&#38144;&#21644;&#37329;&#34701;&#26381;&#21153;&#12289;&#36719;&#20214;&#24037;&#31243;&#12289;&#23398;&#26415;&#21644;&#31185;&#23398;&#20889;&#20316;&#12289;&#30740;&#31350;&#21644;&#25945;&#32946;&#12289;&#29615;&#22659;&#31185;&#23398;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#21508;&#31181;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#36890;&#36807;&#30740;&#31350;&#36825;&#20123;&#24212;&#29992;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;ChatGPT&#22312;&#24212;&#23545;&#29616;&#23454;&#25361;&#25112;&#26041;&#38754;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chat Generative Pre-trained Transformer (ChatGPT) has gained significant interest and attention since its launch in November 2022. It has shown impressive performance in various domains, including passing exams and creative writing. However, challenges and concerns related to biases and trust persist. In this work, we present a comprehensive review of over 100 Scopus-indexed publications on ChatGPT, aiming to provide a taxonomy of ChatGPT research and explore its applications. We critically analyze the existing literature, identifying common approaches employed in the studies. Additionally, we investigate diverse application areas where ChatGPT has found utility, such as healthcare, marketing and financial services, software engineering, academic and scientific writing, research and education, environmental science, and natural language processing. Through examining these applications, we gain valuable insights into the potential of ChatGPT in addressing real-world challenges. We also 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22320;&#29702;&#26631;&#35760;&#30340;&#25512;&#29305;&#25968;&#25454;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#22312;&#33521;&#26684;&#20848;&#21644;&#23041;&#23572;&#22763;&#30340;&#19971;&#21315;&#20010;&#34892;&#25919;&#21306;&#22495;&#19978;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#26144;&#23556;&#65292;&#21457;&#29616;&#31038;&#20250;&#32463;&#27982;&#20132;&#21449;&#24433;&#21709;&#20102;&#35821;&#35328;&#20351;&#29992;&#65292;&#28151;&#21512;&#19981;&#21516;&#31038;&#20250;&#32463;&#27982;&#38454;&#23618;&#30340;&#20154;&#32676;&#39057;&#29575;&#20559;&#31163;&#26631;&#20934;&#35821;&#27861;&#30340;&#31243;&#24230;&#36234;&#39640;&#65292;&#20854;&#25910;&#20837;&#20851;&#32852;&#36234;&#24369;&#12290;</title><link>http://arxiv.org/abs/2307.10016</link><description>&lt;p&gt;
&#26041;&#35328;&#30340;&#30896;&#25758;&#65306;&#31038;&#20250;&#32463;&#27982;&#20132;&#21449;&#23545;&#35821;&#35328;&#20351;&#29992;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
When Dialects Collide: How Socioeconomic Mixing Affects Language Use. (arXiv:2307.10016v1 [physics.soc-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22320;&#29702;&#26631;&#35760;&#30340;&#25512;&#29305;&#25968;&#25454;&#21644;&#35745;&#31639;&#26041;&#27861;&#65292;&#22312;&#33521;&#26684;&#20848;&#21644;&#23041;&#23572;&#22763;&#30340;&#19971;&#21315;&#20010;&#34892;&#25919;&#21306;&#22495;&#19978;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#26144;&#23556;&#65292;&#21457;&#29616;&#31038;&#20250;&#32463;&#27982;&#20132;&#21449;&#24433;&#21709;&#20102;&#35821;&#35328;&#20351;&#29992;&#65292;&#28151;&#21512;&#19981;&#21516;&#31038;&#20250;&#32463;&#27982;&#38454;&#23618;&#30340;&#20154;&#32676;&#39057;&#29575;&#20559;&#31163;&#26631;&#20934;&#35821;&#27861;&#30340;&#31243;&#24230;&#36234;&#39640;&#65292;&#20854;&#25910;&#20837;&#20851;&#32852;&#36234;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#30340;&#31038;&#20250;&#32463;&#27982;&#32972;&#26223;&#19982;&#20182;&#20204;&#20351;&#29992;&#26631;&#20934;&#35821;&#35328;&#30340;&#26041;&#24335;&#24182;&#19981;&#29420;&#31435;&#65292;&#36825;&#22312;&#21508;&#31181;&#31038;&#20250;&#35821;&#35328;&#23398;&#30740;&#31350;&#20013;&#24050;&#32463;&#24471;&#21040;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#31038;&#20250;&#32463;&#27982;&#38454;&#23618;&#30340;&#20154;&#20204;&#20132;&#21449;&#28151;&#21512;&#21487;&#33021;&#23545;&#36825;&#20123;&#30456;&#20851;&#24615;&#36896;&#25104;&#20309;&#31181;&#24433;&#21709;&#65292;&#22312;&#23450;&#37327;&#35282;&#24230;&#19978;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#24102;&#22320;&#29702;&#26631;&#35760;&#30340;&#25512;&#29305;&#21644;&#21487;&#36716;&#31227;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#22312;&#33521;&#26684;&#20848;&#21644;&#23041;&#23572;&#22763;&#30340;&#19971;&#21315;&#20010;&#34892;&#25919;&#21306;&#22495;&#19978;&#23545;&#19982;&#26631;&#20934;&#33521;&#35821;&#20559;&#31163;&#30340;&#24773;&#20917;&#36827;&#34892;&#22823;&#35268;&#27169;&#26144;&#23556;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25968;&#25454;&#19982;&#39640;&#20998;&#36776;&#29575;&#30340;&#25910;&#20837;&#22320;&#22270;&#32467;&#21512;&#36215;&#26469;&#65292;&#20026;&#23621;&#20303;&#22320;&#29992;&#25143;&#20998;&#37197;&#19968;&#20010;&#20195;&#29702;&#31038;&#20250;&#32463;&#27982;&#25351;&#26631;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#20843;&#20010;&#22823;&#37117;&#24066;&#21306;&#22495;&#65292;&#25105;&#20204;&#21457;&#29616;&#19968;&#31181;&#19968;&#33268;&#30340;&#27169;&#24335;&#65292;&#34920;&#26126;&#19981;&#21516;&#31038;&#20250;&#32463;&#27982;&#38454;&#23618;&#30340;&#20154;&#20204;&#28151;&#21512;&#24471;&#36234;&#22810;&#65292;&#20854;&#20559;&#31163;&#26631;&#20934;&#35821;&#27861;&#21644;&#25910;&#20837;&#30340;&#39057;&#29575;&#23601;&#36234;&#19981;&#30456;&#20114;&#20381;&#23384;&#12290;
&lt;/p&gt;
&lt;p&gt;
The socioeconomic background of people and how they use standard forms of language are not independent, as demonstrated in various sociolinguistic studies. However, the extent to which these correlations may be influenced by the mixing of people from different socioeconomic classes remains relatively unexplored from a quantitative perspective. In this work we leverage geotagged tweets and transferable computational methods to map deviations from standard English on a large scale, in seven thousand administrative areas of England and Wales. We combine these data with high-resolution income maps to assign a proxy socioeconomic indicator to home-located users. Strikingly, across eight metropolitan areas we find a consistent pattern suggesting that the more different socioeconomic classes mix, the less interdependent the frequency of their departures from standard grammar and their income become. Further, we propose an agent-based model of linguistic variety adoption that sheds light on th
&lt;/p&gt;</description></item><item><title>ACTI&#22312;EVALITA 2023&#20013;&#30340;&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#20849;&#26377;15&#25903;&#22242;&#38431;&#21442;&#19982;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21028;&#26029;&#38452;&#35851;&#20869;&#23481;&#21644;&#20998;&#31867;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#25269;&#21046;&#22312;&#22312;&#32447;&#24179;&#21488;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2307.06954</link><description>&lt;p&gt;
ACTI&#22312;EVALITA 2023&#20013;&#30340;&#32508;&#36848;&#65306;&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task. (arXiv:2307.06954v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06954
&lt;/p&gt;
&lt;p&gt;
ACTI&#22312;EVALITA 2023&#20013;&#30340;&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#20849;&#26377;15&#25903;&#22242;&#38431;&#21442;&#19982;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21028;&#26029;&#38452;&#35851;&#20869;&#23481;&#21644;&#20998;&#31867;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#25269;&#21046;&#22312;&#22312;&#32447;&#24179;&#21488;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38452;&#35851;&#35770;&#36776;&#35782;&#20219;&#21153;&#26159;Evalita 2023&#39318;&#27425;&#25552;&#20986;&#30340;&#26032;&#20849;&#20139;&#20219;&#21153;&#12290;ACTI&#25361;&#25112;&#20165;&#22522;&#20110;Telegram&#19978;&#30340;&#38452;&#35851;&#39057;&#36947;&#35780;&#35770;&#65292;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;(i) &#38452;&#35851;&#20869;&#23481;&#20998;&#31867;&#65306;&#36776;&#35782;&#38452;&#35851;&#20869;&#23481;&#21644;(ii) &#38452;&#35851;&#31867;&#21035;&#20998;&#31867;&#65306;&#38024;&#23545;&#29305;&#23450;&#38452;&#35851;&#29702;&#35770;&#20998;&#31867;&#12290;&#20849;&#26377;15&#25903;&#22242;&#38431;&#21442;&#19982;&#20102;&#35813;&#20219;&#21153;&#65292;&#24635;&#20849;&#25552;&#20132;&#20102;81&#20010;&#32467;&#26524;&#12290;&#25105;&#20204;&#35828;&#26126;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20851;&#20110;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#26469;&#25269;&#21046;&#22312;&#22312;&#32447;&#24179;&#21488;&#19978;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conspiracy Theory Identication task is a new shared task proposed for the first time at the Evalita 2023. The ACTI challenge, based exclusively on comments published on conspiratorial channels of telegram, is divided into two subtasks: (i) Conspiratorial Content Classification: identifying conspiratorial content and (ii) Conspiratorial Category Classification about specific conspiracy theory classification. A total of fifteen teams participated in the task for a total of 81 submissions. We illustrate the best performing approaches were based on the utilization of large language models. We finally draw conclusions about the utilization of these models for counteracting the spreading of misinformation in online platforms.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#28436;&#31034;&#22914;&#20309;&#21033;&#29992;&#29616;&#26377;&#26041;&#27861;&#21644;&#36719;&#20214;&#24037;&#20855;&#32467;&#21512;&#23884;&#20837;&#25216;&#26415;&#35774;&#35745;&#38754;&#21521;&#31185;&#23398;&#39046;&#22495;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#35813;&#26426;&#22120;&#20154;&#33021;&#22815;&#22788;&#29702;&#31185;&#23398;&#25991;&#29486;&#65292;&#25552;&#20379;&#29305;&#23450;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#22312;&#21021;&#27493;&#30740;&#31350;&#36741;&#21161;&#30693;&#35782;&#26041;&#38754;&#20026;&#29289;&#29702;&#31185;&#23398;&#23478;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2306.10067</link><description>&lt;p&gt;
&#21033;&#29992;&#23884;&#20837;&#25216;&#26415;&#35774;&#35745;&#38754;&#21521;&#31185;&#23398;&#39046;&#22495;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Domain-specific ChatBots for Science using Embeddings. (arXiv:2306.10067v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#28436;&#31034;&#22914;&#20309;&#21033;&#29992;&#29616;&#26377;&#26041;&#27861;&#21644;&#36719;&#20214;&#24037;&#20855;&#32467;&#21512;&#23884;&#20837;&#25216;&#26415;&#35774;&#35745;&#38754;&#21521;&#31185;&#23398;&#39046;&#22495;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#35813;&#26426;&#22120;&#20154;&#33021;&#22815;&#22788;&#29702;&#31185;&#23398;&#25991;&#29486;&#65292;&#25552;&#20379;&#29305;&#23450;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#22312;&#21021;&#27493;&#30740;&#31350;&#36741;&#21161;&#30693;&#35782;&#26041;&#38754;&#20026;&#29289;&#29702;&#31185;&#23398;&#23478;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#25104;&#20026;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#33021;&#22788;&#29702;&#22810;&#31181;&#20219;&#21153;&#12290;&#32463;&#35843;&#25972;&#30340;&#36825;&#20123;&#31995;&#32479;&#24050;&#34987;&#36716;&#21270;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#33021;&#22238;&#31572;&#29992;&#25143;&#23545;&#24191;&#27867;&#35805;&#39064;&#30340;&#26597;&#35810;&#65292;&#25552;&#20379;&#20016;&#23500;&#30340;&#20449;&#24687;&#21644;&#21019;&#24847;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#22312;&#33258;&#28982;&#31185;&#23398;&#39046;&#22495;&#30340;&#30693;&#35782;&#20173;&#19981;&#23436;&#25972;&#65292;&#24182;&#19988;&#38754;&#20020;&#20005;&#26684;&#38656;&#27714;&#21644;&#26469;&#28304;&#26631;&#20934;&#65292;&#22240;&#27492;&#20854;&#22312;&#29289;&#29702;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#20173;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#36731;&#26494;&#22320;&#23558;&#29616;&#26377;&#26041;&#27861;&#21644;&#36719;&#20214;&#24037;&#20855;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#35813;&#31995;&#32479;&#33021;&#25509;&#21463;&#29616;&#26377;&#26684;&#24335;&#30340;&#31185;&#23398;&#25991;&#29486;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#26597;&#25214;&#26469;&#20026;LLM&#25552;&#20379;&#29305;&#23450;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#20415;&#22312;&#25776;&#20889;&#22238;&#31572;&#26102;&#20351;&#29992;&#12290;&#25105;&#20204;&#21516;&#26679;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#22270;&#20687;&#23884;&#20837;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#36328;&#20986;&#29256;&#29289;&#22270;&#29255;&#30340;&#25628;&#32034;&#21644;&#26816;&#32034;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25552;&#20379;&#21021;&#27493;&#30740;&#31350;&#36741;&#21161;&#30693;&#35782;&#26041;&#38754;&#65292;LLM&#24050;&#32463;&#36866;&#29992;&#20110;&#29289;&#29702;&#31185;&#23398;&#23478;&#30340;&#20351;&#29992;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#30340;&#24320;&#21457;&#21487;&#20197;&#25193;&#23637;&#36825;&#20123;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as powerful machine-learning systems capable of handling a myriad of tasks. Tuned versions of these systems have been turned into chatbots that can respond to user queries on a vast diversity of topics, providing informative and creative replies. However, their application to physical science research remains limited owing to their incomplete knowledge in these areas, contrasted with the needs of rigor and sourcing in science domains. Here, we demonstrate how existing methods and software tools can be easily combined to yield a domain-specific chatbot. The system ingests scientific documents in existing formats, and uses text embedding lookup to provide the LLM with domain-specific contextual information when composing its reply. We similarly demonstrate that existing image embedding methods can be used for search and retrieval across publication figures. These results confirm that LLMs are already suitable for use by physical scientists in acc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#20010;&#21517;&#20026;SpeechGen&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#33410;&#65292;&#35299;&#38145;&#20102;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#30452;&#25509;&#36866;&#24212;&#36830;&#32493;&#35821;&#38899;&#21040;&#31163;&#25955;&#26631;&#35760;&#30340;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#38899;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02207</link><description>&lt;p&gt;
SpeechGen: &#21033;&#29992;&#25552;&#31034;&#35299;&#38145;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts. (arXiv:2306.02207v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#20010;&#21517;&#20026;SpeechGen&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#33410;&#65292;&#35299;&#38145;&#20102;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#30452;&#25509;&#36866;&#24212;&#36830;&#32493;&#35821;&#38899;&#21040;&#31163;&#25955;&#26631;&#35760;&#30340;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#38899;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#20013;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;ChatGPT&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#23558;&#36830;&#32493;&#35821;&#38899;&#30452;&#25509;&#36866;&#24212;&#20110;&#22788;&#29702;&#31163;&#25955;&#26631;&#35760;&#30340;LLM&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#36825;&#22952;&#30861;&#20102;LLM&#22312;&#35821;&#38899;&#29983;&#25104;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#39640;&#32423;&#35821;&#38899;LM&#20204;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#35821;&#38899;&#20449;&#21495;&#25152;&#21253;&#21547;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#21253;&#25324;&#35828;&#35805;&#32773;&#21644;&#24773;&#24863;&#31561;&#65292;&#36825;&#20123;&#20449;&#24687;&#20165;&#36890;&#36807;&#25991;&#26412;&#25968;&#25454;&#26080;&#27861;&#33719;&#21462;&#12290;&#22312;&#19968;&#20123;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#35843;&#25972;&#24050;&#32463;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#21442;&#25968;&#25928;&#29575;&#21644;&#31454;&#20105;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;&#20294;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#25552;&#31034;&#33021;&#22815;&#26377;&#25928;&#22320;&#28608;&#21457;&#35821;&#38899;LM&#30340;&#29983;&#25104;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#20808;&#39537;&#24615;&#30740;&#31350;&#65292;&#35813;&#30740;&#31350;&#22312;&#31216;&#20026;SpeechGen&#30340;&#32479;&#19968;&#26694;&#26550;&#20013;&#20351;&#29992;&#25552;&#31034;&#35843;&#33410;&#26469;&#21050;&#28608;&#35821;&#38899;LM&#36827;&#34892;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#20855;&#26377;&#32422;10M&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#22312;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#26102;&#30340;&#36801;&#31227;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20004;&#20010;&#34920;&#31034;&#29702;&#35770;&#65292;&#36890;&#36807;&#29983;&#25104;&#20505;&#36873;&#27169;&#22411;&#30340;&#25490;&#21517;&#20998;&#25968;&#65292;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#23454;&#38469;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#36801;&#31227;&#24615;&#20998;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#19982;&#24494;&#35843;&#22522;&#30784;&#20107;&#23454;&#20043;&#38388;&#23384;&#22312;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#21644;&#20302;&#30340;p&#20540;&#65292;&#26159;&#19968;&#20010;&#33410;&#30465;&#36164;&#28304;&#12289;&#39640;&#25928;&#33410;&#30465;&#26102;&#38388;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01015</link><description>&lt;p&gt;
&#22914;&#20309;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#30340;&#36801;&#31227;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Estimate Model Transferability of Pre-Trained Speech Models?. (arXiv:2306.01015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#22312;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#26102;&#30340;&#36801;&#31227;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20004;&#20010;&#34920;&#31034;&#29702;&#35770;&#65292;&#36890;&#36807;&#29983;&#25104;&#20505;&#36873;&#27169;&#22411;&#30340;&#25490;&#21517;&#20998;&#25968;&#65292;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#23454;&#38469;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#36801;&#31227;&#24615;&#20998;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#19982;&#24494;&#35843;&#22522;&#30784;&#20107;&#23454;&#20043;&#38388;&#23384;&#22312;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#21644;&#20302;&#30340;p&#20540;&#65292;&#26159;&#19968;&#20010;&#33410;&#30465;&#36164;&#28304;&#12289;&#39640;&#25928;&#33410;&#30465;&#26102;&#38388;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#22522;&#20110;&#20998;&#25968;&#35780;&#20272;&#8221;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20272;&#35745;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#65288;PSMs&#65289;&#22312;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#26102;&#30340;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#20004;&#20010;&#34920;&#31034;&#29702;&#35770;&#65292;&#36125;&#21494;&#26031;&#20284;&#28982;&#20272;&#35745;&#21644;&#26368;&#20248;&#20256;&#36755;&#65292;&#20351;&#29992;&#25552;&#21462;&#30340;&#34920;&#31034;&#29983;&#25104;PSM&#20505;&#36873;&#30340;&#25490;&#21517;&#20998;&#25968;&#12290;&#36890;&#36807;&#20551;&#35774;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#36801;&#31227;&#24615;&#20998;&#25968;&#65292;&#32780;&#26080;&#38656;&#23454;&#38469;&#24494;&#35843;&#20505;&#36873;&#27169;&#22411;&#25110;&#23618;&#12290;&#25105;&#20204;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#22312;&#20132;&#21449;&#23618;&#21644;&#20132;&#21449;&#27169;&#22411;&#35774;&#32622;&#20013;&#35780;&#20272;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#65288;&#20363;&#22914;Conformer RNN-Transducer&#65289;&#21644;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#65288;&#20363;&#22914;HuBERT&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#26694;&#26550;&#19982;&#24494;&#35843;&#22522;&#30784;&#20107;&#23454;&#20043;&#38388;&#23384;&#22312;&#24456;&#39640;&#30340;Spearman&#25490;&#21517;&#30456;&#20851;&#24615;&#21644;&#20302;&#30340;p&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#36801;&#31227;&#24615;&#26694;&#26550;&#38656;&#35201;&#36739;&#23569;&#30340;&#35745;&#31639;&#26102;&#38388;&#21644;&#36164;&#28304;&#65292;&#22240;&#27492;&#26159;&#19968;&#20010;&#33410;&#30465;&#36164;&#28304;&#12289;&#39640;&#25928;&#33410;&#30465;&#26102;&#38388;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a ``score-based assessment'' framework for estimating the transferability of pre-trained speech models (PSMs) for fine-tuning target tasks. We leverage upon two representation theories, Bayesian likelihood estimation and optimal transport, to generate rank scores for the PSM candidates using the extracted representations. Our framework efficiently computes transferability scores without actual fine-tuning of candidate models or layers by making a temporal independent hypothesis. We evaluate some popular supervised speech models (e.g., Conformer RNN-Transducer) and self-supervised speech models (e.g., HuBERT) in cross-layer and cross-model settings using public data. Experimental results show a high Spearman's rank correlation and low $p$-value between our estimation framework and fine-tuning ground truth. Our proposed transferability framework requires less computational time and resources, making it a resource-saving and time-efficient approach for tuning sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;&#30340;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#65292;&#37319;&#29992;&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#12289;&#32858;&#28966;&#25439;&#22833;&#21644;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#31561;&#25216;&#26415;&#65292;&#22312;LVIS&#19978;&#21462;&#24471;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.07011</link><description>&lt;p&gt;
&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#65306;&#35270;&#35273;&#21464;&#21387;&#22120;&#19979;&#30340;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers. (arXiv:2305.07011v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;&#30340;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#65292;&#37319;&#29992;&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#12289;&#32858;&#28966;&#25439;&#22833;&#21644;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#31561;&#25216;&#26415;&#65292;&#22312;LVIS&#19978;&#21462;&#24471;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21306;&#22495;&#24863;&#30693;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;RO-ViT&#65289;&#65292;&#19968;&#31181;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#22635;&#34917;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#24314;&#35758;&#38543;&#26426;&#35009;&#21098;&#24182;&#35843;&#25972;&#20301;&#32622;&#23884;&#20837;&#30340;&#21306;&#22495;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#25972;&#20010;&#22270;&#20687;&#20301;&#32622;&#23884;&#20837;&#12290;&#36825;&#26356;&#22909;&#22320;&#21305;&#37197;&#20102;&#26816;&#27979;&#24494;&#35843;&#38454;&#27573;&#20013;&#21306;&#22495;&#32423;&#21035;&#19978;&#20351;&#29992;&#20301;&#32622;&#23884;&#20837;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29992;&#32858;&#28966;&#25439;&#22833;&#26367;&#25442;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;softmax&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#37027;&#20123;&#26377;&#20449;&#24687;&#37327;&#20294;&#38590;&#20197;&#25429;&#25417;&#30340;&#20363;&#23376;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20197;&#25913;&#36827;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;LVIS&#21644;COCO&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#23436;&#25972;&#27169;&#22411;&#21644;&#38646;-shot&#36716;&#31227;&#24615;&#33021;&#12290;RO-ViT&#22312;LVIS&#19978;&#23454;&#29616;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#65292;&#36229;&#36807;&#29616;&#26377;&#26368;&#20339;&#26041;&#27861;5.8&#20010;&#30334;&#20998;&#28857;&#65292;&#21516;&#26102;&#36824;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#38646;-shot&#36716;&#31227;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - a contrastive image-text pretraining recipe to bridge the gap between image-level pretraining and open-vocabulary object detection. At the pretraining phase, we propose to randomly crop and resize regions of positional embeddings instead of using the whole image positional embeddings. This better matches the use of positional embeddings at region-level in the detection finetuning phase. In addition, we replace the common softmax cross entropy loss in contrastive learning with focal loss to better learn the informative yet difficult examples. Finally, we leverage recent advances in novel object proposals to improve open-vocabulary detection finetuning. We evaluate our full model on the LVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer. RO-ViT achieves a state-of-the-art 32.1 $AP_r$ on LVIS, surpassing the best existing approach by +5.8 points in addition to competitive zero-shot transfer detec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TMR&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#25991;&#26412;&#36716;&#25442;&#20026;3D&#20154;&#20307;&#36816;&#21160;&#12290;&#23427;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23545;&#27604;&#24615;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#24314;&#31435;&#36328;&#27169;&#24577;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20445;&#25345;&#36816;&#21160;&#29983;&#25104;&#25439;&#22833;&#21644;&#23545;&#27604;&#24615;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.00976</link><description>&lt;p&gt;
TMR:&#20351;&#29992;&#23545;&#27604;3D&#20154;&#20307;&#36816;&#21160;&#21512;&#25104;&#30340;&#25991;&#26412;&#21040;&#36816;&#21160;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis. (arXiv:2305.00976v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TMR&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#25991;&#26412;&#36716;&#25442;&#20026;3D&#20154;&#20307;&#36816;&#21160;&#12290;&#23427;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#23545;&#27604;&#24615;&#25439;&#22833;&#30340;&#26041;&#27861;&#26469;&#26356;&#22909;&#22320;&#24314;&#31435;&#36328;&#27169;&#24577;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20445;&#25345;&#36816;&#21160;&#29983;&#25104;&#25439;&#22833;&#21644;&#23545;&#27604;&#24615;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TMR&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#25991;&#26412;&#36716;&#25442;&#20026;3D&#20154;&#20307;&#36816;&#21160;&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#20165;&#23558;&#26816;&#32034;&#35270;&#20026;&#20195;&#29702;&#35780;&#20272;&#25351;&#26631;&#19981;&#21516;&#65292;&#25105;&#20204;&#23558;&#20854;&#20316;&#20026;&#19968;&#20010;&#29420;&#31435;&#30340;&#20219;&#21153;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21040;&#21160;&#20316;&#21512;&#25104;&#27169;&#22411;TEMOS&#65292;&#24182;&#32467;&#21512;&#23545;&#27604;&#25439;&#22833;&#26469;&#26356;&#22909;&#22320;&#26500;&#36896;&#36328;&#27169;&#24577;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#25105;&#20204;&#34920;&#26126;&#20445;&#25345;&#36816;&#21160;&#29983;&#25104;&#25439;&#22833;&#21644;&#23545;&#27604;&#24615;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#25253;&#21578;&#20960;&#20010;&#21327;&#35758;&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;KIT-ML&#21644;HumanML3D&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;TMR&#27604;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#20363;&#22914;&#23558;&#20013;&#20301;&#25968;&#25490;&#21517;&#20174;54&#38477;&#33267;19&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#26102;&#21051;&#26816;&#32034;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present TMR, a simple yet effective approach for text to 3D human motion retrieval. While previous work has only treated retrieval as a proxy evaluation metric, we tackle it as a standalone task. Our method extends the state-of-the-art text-to-motion synthesis model TEMOS, and incorporates a contrastive loss to better structure the cross-modal latent space. We show that maintaining the motion generation loss, along with the contrastive training, is crucial to obtain good performance. We introduce a benchmark for evaluation and provide an in-depth analysis by reporting results on several protocols. Our extensive experiments on the KIT-ML and HumanML3D datasets show that TMR outperforms the prior work by a significant margin, for example reducing the median rank from 54 to 19. Finally, we showcase the potential of our approach on moment retrieval. Our code and models are publicly available.
&lt;/p&gt;</description></item><item><title>CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.00969</link><description>&lt;p&gt;
CryCeleb: &#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00969
&lt;/p&gt;
&lt;p&gt;
CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;Ubenwa CryCeleb&#25968;&#25454;&#38598;&#8212;&#8212;&#19968;&#20010;&#26631;&#35760;&#30340;&#23156;&#20799;&#21741;&#22768;&#25910;&#38598;&#65292;&#20197;&#21450;&#38468;&#24102;&#30340;CryCeleb 2023&#20219;&#21153;&#8212;&#8212;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#20844;&#20849;&#35828;&#35805;&#20154;&#39564;&#35777;&#25361;&#25112;&#12290;&#25105;&#20204;&#37322;&#25918;&#20986;786&#21517;&#26032;&#29983;&#20799;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#20197;&#40723;&#21169;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the Ubenwa CryCeleb dataset - a labeled collection of infant cries, and the accompanying CryCeleb 2023 task - a public speaker verification challenge based on infant cry sounds. We release for academic usage more than 6 hours of manually segmented cry sounds from 786 newborns to encourage research in infant cry analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#21307;&#23398;&#39046;&#22495;&#36827;&#19968;&#27493;&#24494;&#35843;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;PMC-LLaMA&#65292;&#20854;&#36890;&#36807;&#22686;&#21152;&#21307;&#23398;&#30693;&#35782;&#25552;&#39640;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26377;&#26395;&#22312;&#29983;&#29289;&#21307;&#23398;&#38382;&#31572;&#39046;&#22495;&#26377;&#26356;&#22909;&#30340;&#24212;&#29992;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.14454</link><description>&lt;p&gt;
PMC-LLaMA: &#22312;&#21307;&#23398;&#35770;&#25991;&#20013;&#36827;&#34892;LLaMA&#30340;&#36827;&#19968;&#27493;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
PMC-LLaMA: Further Finetuning LLaMA on Medical Papers. (arXiv:2304.14454v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#21307;&#23398;&#39046;&#22495;&#36827;&#19968;&#27493;&#24494;&#35843;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;PMC-LLaMA&#65292;&#20854;&#36890;&#36807;&#22686;&#21152;&#21307;&#23398;&#30693;&#35782;&#25552;&#39640;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26377;&#26395;&#22312;&#29983;&#29289;&#21307;&#23398;&#38382;&#31572;&#39046;&#22495;&#26377;&#26356;&#22909;&#30340;&#24212;&#29992;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#26085;&#24120;&#23545;&#35805;&#25110;&#38382;&#31572;&#22330;&#26223;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#28982;&#32780;&#65292;&#22312;&#27880;&#37325;&#31934;&#24230;&#30340;&#39046;&#22495;&#65292;&#20363;&#22914;&#21307;&#30103;&#24212;&#29992;&#20013;&#65292;&#23427;&#20204;&#24448;&#24448;&#34920;&#29616;&#20986;&#19981;&#23613;&#20154;&#24847;&#30340;&#24615;&#33021;&#65292;&#21407;&#22240;&#26159;&#32570;&#20047;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PMC-LLaMA&#65292;&#36825;&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#24635;&#20849;480&#19975;&#31687;&#29983;&#29289;&#21307;&#23398;&#35770;&#25991;&#19978;&#24494;&#35843;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#36827;&#19968;&#27493;&#27880;&#20837;&#21307;&#23398;&#30693;&#35782;&#65292;&#22686;&#24378;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#21253;&#25324;PubMedQA&#12289;MedMCQA&#21644;USMLE&#31561;&#19977;&#20010;&#29983;&#29289;&#21307;&#23398;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;&#21363;PMC-LLaMA&#65292;&#23545;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29305;&#23450;&#27010;&#24565;&#26377;&#26356;&#22909;&#30340;&#29702;&#35299;&#65292;&#22240;&#27492;&#22312;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#21644;&#20195;&#30721;&#20197;&#21450;&#22312;&#32447;&#28436;&#31034;&#22343;&#21487;&#22312;https://github.com/cstorm125/pmc-llama&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased remarkable capabilities in natural language understanding in various domains. These models can usually behave well on daily dialog, or question answering scenarios, however, in areas that value precision, for example, in medical applications, they often exhibit unsatisfactory performance due to a lack of domain-specific knowledge. In this report, we introduce PMC-LLaMA, an open-source language model that is acquired by fine-tuning an open-source language model on a total of 4.8 million biomedical academic papers for further injecting medical knowledge, enhancing its capability in medical domain. Our preliminary evaluations are conducted on three biomedical QA datasets, including PubMedQA, MedMCQA, and USMLE, showing that the our model after finetuning, i.e., PMC-LLaMA, demonstrates better understanding of biomedical domain-specific concepts, thus achieving high performance on QA benchmarks. The model and codes, along with an online demo, are 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25552;&#31034;&#26469;&#35780;&#20272;&#31038;&#20132;Chatbot&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#20154;&#31867;&#23545;Chatbot&#30340;&#20027;&#35266;&#35780;&#20272;&#65292;&#32780;&#19981;&#38656;&#35201;&#20154;&#31867;&#20934;&#22791;&#35780;&#20272;&#26448;&#26009;&#12290;</title><link>http://arxiv.org/abs/2304.05253</link><description>&lt;p&gt;
&#21033;&#29992;&#25552;&#31034;&#26469;&#36817;&#20284;&#20154;&#31867;&#23545;&#31038;&#20132;Chatbot&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Approximating Human Evaluation of Social Chatbots with Prompting. (arXiv:2304.05253v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05253
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25552;&#31034;&#26469;&#35780;&#20272;&#31038;&#20132;Chatbot&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#20154;&#31867;&#23545;Chatbot&#30340;&#20027;&#35266;&#35780;&#20272;&#65292;&#32780;&#19981;&#38656;&#35201;&#20154;&#31867;&#20934;&#22791;&#35780;&#20272;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24378;&#22823;&#30340;&#23545;&#35805;&#27169;&#22411;&#36880;&#28176;&#38754;&#21521;&#24191;&#22823;&#29992;&#25143;&#24320;&#25918;&#65292;&#29992;&#25143;&#24320;&#22987;&#31215;&#26497;&#22320;&#19982;&#36825;&#31181;&#25216;&#26415;&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#12290;&#38500;&#38750;&#25216;&#26415;&#24471;&#21040;&#36866;&#24403;&#30340;&#25511;&#21046;&#65292;&#36825;&#31181;&#21069;&#25152;&#26410;&#26377;&#30340;&#20132;&#20114;&#20307;&#39564;&#21487;&#33021;&#20250;&#23545;&#29992;&#25143;&#36896;&#25104;&#30456;&#24403;&#22823;&#30340;&#31038;&#20132;&#21644;&#24515;&#29702;&#39118;&#38505;&#12290;&#36825;&#23601;&#38656;&#35201;&#21487;&#25193;&#23637;&#21644;&#24378;&#22823;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#31038;&#20132;Chatbot&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#36890;&#24120;&#20851;&#27880;&#23458;&#35266;&#36136;&#37327;&#25351;&#26631;&#65292;&#24573;&#30053;&#31038;&#20132;&#32500;&#24230;&#30340;&#20027;&#35266;&#24863;&#21463;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#37117;&#22522;&#20110;&#21487;&#29992;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#39044;&#29983;&#25104;&#30340;&#23545;&#35805;&#65292;&#36825;&#24847;&#21619;&#30528;&#38656;&#35201;&#20154;&#31867;&#21442;&#19982;&#20934;&#22791;&#35780;&#20272;&#26448;&#26009;&#65292;&#22240;&#27492;&#24433;&#21709;&#20102;&#25351;&#26631;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#26469;&#33258;GPT&#31995;&#21015;&#30340;&#26032;&#20852;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#20801;&#35768;&#36827;&#34892;&#25552;&#31034;&#24335;&#30340;&#23545;&#35805;&#31995;&#32479;&#35780;&#20272;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20851;&#27880;&#20027;&#35266;&#35780;&#20215;&#26631;&#20934;&#26469;&#36817;&#20284;&#20154;&#31867;&#23545;&#31038;&#20132;Chatbot&#30340;&#35780;&#20272;&#12290;&#36890;&#36807;&#20351;&#29992;GPT-3&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#21508;&#26679;&#30340;&#23545;&#35805;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#31867;&#36755;&#20837;&#26469;&#20934;&#22791;&#35780;&#20272;&#26448;&#26009;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#22235;&#31181;&#19981;&#21516;&#30340;&#23545;&#35805;&#27169;&#22411;&#36827;&#34892;&#19968;&#31995;&#21015;&#24443;&#24213;&#30340;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#26694;&#26550;&#30340;&#20248;&#32570;&#28857;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Once powerful conversational models have become available for a wide audience, users started actively engaging in social interactions with this technology. Such unprecedented interaction experiences may pose considerable social and psychological risks to the users unless the technology is properly controlled. This creates an urgent need for scalable and robust evaluation metrics for conversational chatbots. Existing automatic evaluation metrics usually focus on objective quality measures and disregard subjective perceptions of social dimensions. Moreover, most of these approaches operate on pre-produced dialogs from available benchmark corpora, which implies human involvement for preparing the material for evaluation and, thus, impeded scalability of the metrics. To address this limitation, we propose to make use of the emerging large language models (LLMs) from the GPT-family and describe a new framework allowing to conduct dialog system evaluation with prompting. With this framework,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#24182;&#22686;&#24378;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.16894</link><description>&lt;p&gt;
ViewRefer: &#22522;&#20110;GPT&#21644;&#26679;&#20363;&#24341;&#23548;&#30340;&#22810;&#35270;&#35282;&#30693;&#35782;&#22788;&#29702;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance. (arXiv:2303.16894v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#24182;&#22686;&#24378;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22810;&#35270;&#35282;&#36755;&#20837;&#30340;3D&#22330;&#26223;&#65292;&#21487;&#20197;&#32531;&#35299;3D&#35270;&#35273;&#23450;&#20301;&#20013;&#30340;&#35270;&#35282;&#24046;&#24322;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#23884;&#20837;&#22312;&#25991;&#26412;&#27169;&#24577;&#20013;&#30340;&#35270;&#35282;&#32447;&#32034;&#65292;&#24182;&#19988;&#26410;&#33021;&#26435;&#34913;&#19981;&#21516;&#35270;&#22270;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#25506;&#32034;&#22914;&#20309;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#12290;&#20854;&#20013;&#65292;ViewRefer&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#30693;&#35782;&#65292;&#23558;&#21333;&#19968;&#30340;&#23450;&#20301;&#25991;&#26412;&#25193;&#23637;&#20026;&#22810;&#20010;&#20960;&#20309;&#19968;&#33268;&#30340;&#25551;&#36848;&#65307;&#21516;&#26102;&#65292;&#22312;3D&#27169;&#24577;&#20013;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;Transformer&#30340;&#34701;&#21512;&#27169;&#22359;&#21644;&#35270;&#22270;&#38388;&#27880;&#24847;&#21147;&#65292;&#20197;&#22686;&#24378;&#35270;&#22270;&#20043;&#38388;&#29289;&#20307;&#30340;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#29992;&#20110;&#35760;&#24518;&#19981;&#21516;&#35270;&#35282;&#19979;&#30340;&#22330;&#26223;&#26080;&#20851;&#30693;&#35782;&#65292;&#20174;&#20004;&#20010;&#26041;&#38754;&#22686;&#24378;&#20102;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding 3D scenes from multi-view inputs has been proven to alleviate the view discrepancy issue in 3D visual grounding. However, existing methods normally neglect the view cues embedded in the text modality and fail to weigh the relative importance of different views. In this paper, we propose ViewRefer, a multi-view framework for 3D visual grounding exploring how to grasp the view knowledge from both text and 3D modalities. For the text branch, ViewRefer leverages the diverse linguistic knowledge of large-scale language models, e.g., GPT, to expand a single grounding text to multiple geometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer fusion module with inter-view attention is introduced to boost the interaction of objects across views. On top of that, we further present a set of learnable multi-view prototypes, which memorize scene-agnostic knowledge for different views, and enhance the framework from two perspectives: a view-guided attention module 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25554;&#20837;&#26684;&#26519;&#31461;&#35805;&#21644;&#22522;&#20110;&#29616;&#26377;&#25991;&#26412;&#30340;&#21551;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#21551;&#31034;&#24037;&#31243;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;4&#38454;&#27573;&#30340;&#21551;&#31034;&#24037;&#31243;&#36807;&#31243;&#65292;&#24182;&#35752;&#35770;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#26576;&#20123;&#25554;&#22270;&#19978;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2302.08961</link><description>&lt;p&gt;
&#22312;&#20185;&#22659;&#19982;&#20185;&#22659;&#20043;&#38388;&#30340;&#21551;&#31034;&#24037;&#31243;&#20013;&#25554;&#20837;&#26684;&#26519;&#31461;&#35805;&#65306;&#20013;&#36884;&#26053;&#31243;&#26469;&#35828;&#26126;&#31461;&#35805;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Grimm in Wonderland: Prompt Engineering with Midjourney to Illustrate Fairytales. (arXiv:2302.08961v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08961
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25554;&#20837;&#26684;&#26519;&#31461;&#35805;&#21644;&#22522;&#20110;&#29616;&#26377;&#25991;&#26412;&#30340;&#21551;&#31034;&#24037;&#31243;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#21551;&#31034;&#24037;&#31243;&#30340;&#21487;&#34892;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;4&#38454;&#27573;&#30340;&#21551;&#31034;&#24037;&#31243;&#36807;&#31243;&#65292;&#24182;&#35752;&#35770;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#26576;&#20123;&#25554;&#22270;&#19978;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#36136;&#37327;&#19981;&#26029;&#25552;&#39640;&#65292;&#20294;&#20854;&#36866;&#29992;&#33539;&#22260;&#30340;&#30028;&#38480;&#20173;&#19981;&#28165;&#26970;&#12290;&#29305;&#21035;&#26159;&#65292;&#20197;&#25913;&#36827;&#25991;&#26412;&#36755;&#20837;&#20197;&#23454;&#29616;&#26356;&#22909;&#32467;&#26524;&#20026;&#30446;&#26631;&#30340;&#21551;&#31034;&#24037;&#31243;&#65292;&#20284;&#20046;&#23578;&#26410;&#38024;&#23545;&#19982;&#29616;&#26377;&#25991;&#26412;&#19968;&#36215;&#24037;&#20316;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#21644;&#21551;&#31034;&#24037;&#31243;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#29983;&#25104;&#27969;&#34892;&#31461;&#35805;&#30340;&#22522;&#26412;&#25554;&#22270;&#12290;&#20351;&#29992;Midjourney v4&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#34892;&#21160;&#30740;&#31350;&#65292;&#26088;&#22312;&#23581;&#35797;&#20026;5&#20010;&#27969;&#34892;&#31461;&#35805;&#20013;&#30340;&#27599;&#20010;&#31461;&#35805;&#29983;&#25104;5&#20010;&#20196;&#20154;&#20449;&#26381;&#30340;&#25554;&#22270;&#65292;&#24182;&#30830;&#23450;&#19968;&#20010;&#20174;&#29616;&#26377;&#25991;&#26412;&#21040;&#25554;&#22270;&#30340;&#21551;&#31034;&#24037;&#31243;&#36807;&#31243;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#21021;&#27493;&#30340;4&#38454;&#27573;&#36807;&#31243;&#65306;i&#65289;&#21021;&#22987;&#25552;&#31034;&#65292;ii&#65289;&#26500;&#22270;&#35843;&#25972;&#65292;iii&#65289;&#39118;&#26684;&#32454;&#21270;&#65292;&#21644;iv&#65289;&#21464;&#24322;&#36873;&#25321;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#26576;&#20123;&#25554;&#22270;&#19978;&#36935;&#21040;&#22256;&#38590;&#30340;&#19977;&#20010;&#21407;&#22240;&#65306;&#35745;&#25968;&#22256;&#38590;&#65292;
&lt;/p&gt;
&lt;p&gt;
The quality of text-to-image generation is continuously improving, yet the boundaries of its applicability are still unclear. In particular, refinement of the text input with the objective of achieving better results - commonly called prompt engineering - so far seems to have not been geared towards work with pre-existing texts. We investigate whether text-to-image generation and prompt engineering could be used to generate basic illustrations of popular fairytales. Using Midjourney v4, we engage in action research with a dual aim: to attempt to generate 5 believable illustrations for each of 5 popular fairytales, and to define a prompt engineering process that starts from a pre-existing text and arrives at an illustration of it. We arrive at a tentative 4-stage process: i) initial prompt, ii) composition adjustment, iii) style refinement, and iv) variation selection. We also discuss three reasons why the generation model struggles with certain illustrations: difficulties with counts, 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#35821;&#27861;&#30340;&#22522;&#30784;&#35789;&#27719;&#23398;&#20064;&#65288;G2L2&#65289;&#26159;&#19968;&#31181;&#20174;&#22522;&#30784;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#35328;&#21547;&#20041;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21333;&#35789;&#26144;&#23556;&#21040;&#35821;&#27861;&#31867;&#22411;&#21644;&#31070;&#32463;&#31526;&#21495;&#35821;&#20041;&#31243;&#24207;&#65292;&#21033;&#29992;&#22522;&#20110;&#35821;&#27861;&#30340;&#32452;&#21512;&#25512;&#23548;&#21477;&#23376;&#30340;&#21547;&#20041;&#65292;&#26368;&#32456;&#21487;&#20197;&#22312;&#22522;&#30784;&#36755;&#20837;&#19978;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2202.08806</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#30340;&#22522;&#30784;&#35789;&#27719;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Grammar-Based Grounded Lexicon Learning. (arXiv:2202.08806v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.08806
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#30340;&#22522;&#30784;&#35789;&#27719;&#23398;&#20064;&#65288;G2L2&#65289;&#26159;&#19968;&#31181;&#20174;&#22522;&#30784;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#35328;&#21547;&#20041;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21333;&#35789;&#26144;&#23556;&#21040;&#35821;&#27861;&#31867;&#22411;&#21644;&#31070;&#32463;&#31526;&#21495;&#35821;&#20041;&#31243;&#24207;&#65292;&#21033;&#29992;&#22522;&#20110;&#35821;&#27861;&#30340;&#32452;&#21512;&#25512;&#23548;&#21477;&#23376;&#30340;&#21547;&#20041;&#65292;&#26368;&#32456;&#21487;&#20197;&#22312;&#22522;&#30784;&#36755;&#20837;&#19978;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#27861;&#30340;&#22522;&#30784;&#35789;&#27719;&#23398;&#20064;&#26041;&#27861;&#65288;G2L2&#65289;&#65292;&#29992;&#20110;&#20174;&#22522;&#30784;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#37197;&#23545;&#65289;&#20013;&#23398;&#20064;&#35821;&#35328;&#30340;&#32452;&#21512;&#21644;&#22522;&#20110;&#22522;&#30784;&#30340;&#21547;&#20041;&#34920;&#31034;&#12290;G2L2&#30340;&#26680;&#24515;&#26159;&#19968;&#32452;&#35789;&#27719;&#26465;&#30446;&#65292;&#23558;&#27599;&#20010;&#21333;&#35789;&#26144;&#23556;&#21040;&#19968;&#20010;&#35821;&#27861;&#31867;&#22411;&#21644;&#31070;&#32463;&#31526;&#21495;&#35821;&#20041;&#31243;&#24207;&#30340;&#20803;&#32452;&#12290;&#32473;&#23450;&#19968;&#20010;&#36755;&#20837;&#21477;&#23376;&#65292;G2L2&#39318;&#20808;&#26597;&#25214;&#19982;&#27599;&#20010;&#26631;&#35760;&#30456;&#20851;&#32852;&#30340;&#35789;&#27719;&#26465;&#30446;&#12290;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#35821;&#27861;&#30340;&#32452;&#21512;&#35789;&#27719;&#21547;&#20041;&#26469;&#25512;&#23548;&#21477;&#23376;&#30340;&#21547;&#20041;&#20316;&#20026;&#21487;&#25191;&#34892;&#30340;&#31070;&#32463;&#31526;&#21495;&#31243;&#24207;&#12290;&#24674;&#22797;&#30340;&#21547;&#20041;&#31243;&#24207;&#21487;&#20197;&#22312;&#22522;&#30784;&#36755;&#20837;&#19978;&#25191;&#34892;&#12290;&#20026;&#20102;&#22312;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#32452;&#21512;&#31354;&#38388;&#20013;&#20419;&#36827;&#23398;&#20064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#22238;&#24402;&#30340;channel pruning&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist approach toward learning a compositional and grounded meaning representation of language from grounded data, such as paired images and texts. At the core of G2L2 is a collection of lexicon entries, which map each word to a tuple of a syntactic type and a neuro-symbolic semantic program. For example, the word shiny has a syntactic type of adjective; its neuro-symbolic semantic program has the symbolic form {\lambda}x. filter(x, SHINY), where the concept SHINY is associated with a neural network embedding, which will be used to classify shiny objects. Given an input sentence, G2L2 first looks up the lexicon entries associated with each token. It then derives the meaning of the sentence as an executable neuro-symbolic program by composing lexical meanings based on syntax. The recovered meaning programs can be executed on grounded inputs. To facilitate learning in an exponentially-growing compositional space, we introd
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;G\"odel&#30340;&#26412;&#20307;&#35770;&#35777;&#30340;&#31616;&#21270;&#21464;&#20307;&#65292;&#35813;&#21464;&#20307;&#22312;&#22522;&#26412;&#27169;&#24577;&#36923;&#36753;K&#25110;KT&#20013;&#24050;&#32463;&#26159;&#26377;&#25928;&#30340;&#65292;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#35859;&#35789;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20154;&#26426;&#20132;&#20114;&#22312;&#35745;&#31639;&#24418;&#32780;&#19978;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2202.06264</link><description>&lt;p&gt;
G\"odel&#30340;&#26412;&#20307;&#35770;&#35777;&#30340;&#31616;&#21270;&#21464;&#20307;
&lt;/p&gt;
&lt;p&gt;
A Simplified Variant of G\"odel's Ontological Argument. (arXiv:2202.06264v3 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;G\"odel&#30340;&#26412;&#20307;&#35770;&#35777;&#30340;&#31616;&#21270;&#21464;&#20307;&#65292;&#35813;&#21464;&#20307;&#22312;&#22522;&#26412;&#27169;&#24577;&#36923;&#36753;K&#25110;KT&#20013;&#24050;&#32463;&#26159;&#26377;&#25928;&#30340;&#65292;&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#35859;&#35789;&#65292;&#24182;&#19988;&#23637;&#31034;&#20102;&#20154;&#26426;&#20132;&#20114;&#22312;&#35745;&#31639;&#24418;&#32780;&#19978;&#23398;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;G\"odel&#30340;&#26412;&#20307;&#35770;&#35777;&#30340;&#31616;&#21270;&#21464;&#20307;&#12290;&#36825;&#20010;&#31616;&#21270;&#30340;&#35770;&#35777;&#22312;&#22522;&#26412;&#27169;&#24577;&#36923;&#36753;K&#25110;KT&#20013;&#24050;&#32463;&#26159;&#26377;&#25928;&#30340;&#65292;&#23427;&#19981;&#20250;&#36973;&#21463;&#27169;&#24577;&#23849;&#28291;&#65292;&#24182;&#19988;&#36991;&#20813;&#20102;G\"odel&#25152;&#20351;&#29992;&#30340;&#30456;&#24403;&#22797;&#26434;&#30340;&#26412;&#36136;&#65288;Ess.&#65289;&#21644;&#24517;&#28982;&#23384;&#22312;&#65288;NE&#65289;&#30340;&#35859;&#35789;&#12290;&#25152;&#25552;&#20986;&#30340;&#21464;&#20307;&#26159;&#36890;&#36807;&#19982;&#29616;&#20195;&#35777;&#26126;&#21161;&#29702;&#31995;&#32479;&#20132;&#20114;&#36827;&#34892;&#19968;&#31995;&#21015;&#29702;&#35770;&#31616;&#21270;&#23454;&#39564;&#30340;&#21103;&#20135;&#29289;&#12290;&#36825;&#20123;&#23454;&#39564;&#30340;&#36215;&#28857;&#26159;G\"odel&#35770;&#35777;&#30340;&#35745;&#31639;&#26426;&#32534;&#30721;&#65292;&#28982;&#21518;&#31995;&#32479;&#22320;&#24212;&#29992;&#33258;&#21160;&#25512;&#29702;&#25216;&#26415;&#26469;&#24471;&#21040;&#25152;&#23637;&#31034;&#30340;&#31616;&#21270;&#21464;&#20307;&#12290;&#25152;&#21576;&#29616;&#30340;&#24037;&#20316;&#22240;&#27492;&#23637;&#31034;&#20102;&#35745;&#31639;&#24418;&#32780;&#19978;&#23398;&#20013;&#23500;&#26377;&#25104;&#26524;&#30340;&#20154;&#26426;&#20132;&#20114;&#12290;&#36825;&#20010;&#23637;&#31034;&#32467;&#26524;&#26159;&#21542;&#22686;&#21152;&#25110;&#20943;&#23569;&#20102;&#26412;&#20307;&#35770;&#35777;&#30340;&#21560;&#24341;&#21147;&#21644;&#35828;&#26381;&#21147;&#65292;&#26159;&#19968;&#20010;&#25105;&#24819;&#20132;&#32473;&#21746;&#23398;&#21644;&#31070;&#23398;&#35752;&#35770;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A simplified variant of G\"odel's ontological argument is presented. The simplified argument is valid already in basic modal logics K or KT, it does not suffer from modal collapse, and it avoids the rather complex predicates of essence (Ess.) and necessary existence (NE) as used by G\"odel. The variant presented has been obtained as a side result of a series of theory simplification experiments conducted in interaction with a modern proof assistant system. The starting point for these experiments was the computer encoding of G\"odel's argument, and then automated reasoning techniques were systematically applied to arrive at the simplified variant presented. The presented work thus exemplifies a fruitful human-computer interaction in computational metaphysics. Whether the presented result increases or decreases the attractiveness and persuasiveness of the ontological argument is a question I would like to pass on to philosophy and theology.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#25968;&#23398;&#21644;&#32479;&#35745;&#26041;&#27861;&#30830;&#23450;&#26631;&#31614;&#30340;&#30456;&#20284;&#24615;&#65292;&#21253;&#25324;&#35789;&#27719;&#30456;&#20284;&#24615;&#21644;&#20849;&#29616;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#32771;&#34385;&#20102;&#26631;&#31614;&#20998;&#37197;&#30340;&#26102;&#38388;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.03622</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#31038;&#21306;&#26816;&#27979;&#20013;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Graph-Based Recommendation System Enhanced with Community Detection. (arXiv:2201.03622v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.03622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21033;&#29992;&#25968;&#23398;&#21644;&#32479;&#35745;&#26041;&#27861;&#30830;&#23450;&#26631;&#31614;&#30340;&#30456;&#20284;&#24615;&#65292;&#21253;&#25324;&#35789;&#27719;&#30456;&#20284;&#24615;&#21644;&#20849;&#29616;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#32771;&#34385;&#20102;&#26631;&#31614;&#20998;&#37197;&#30340;&#26102;&#38388;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#32773;&#24050;&#32463;&#21033;&#29992;&#26631;&#31614;&#20449;&#24687;&#26469;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#20013;&#25512;&#33616;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#30740;&#31350;&#29992;&#25143;&#30340;&#26631;&#31614;&#65292;&#21487;&#20197;&#20102;&#35299;&#20182;&#20204;&#30340;&#20852;&#36259;&#65292;&#20174;&#32780;&#25552;&#39640;&#25512;&#33616;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29992;&#25143;&#33258;&#23450;&#20041;&#26631;&#31614;&#30340;&#20219;&#24847;&#24615;&#21644;&#32570;&#20047;&#38480;&#21046;&#65292;&#30830;&#23450;&#20854;&#30830;&#20999;&#21547;&#20041;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#21033;&#29992;&#25968;&#23398;&#21644;&#32479;&#35745;&#26041;&#27861;&#30830;&#23450;&#26631;&#31614;&#30340;&#35789;&#27719;&#30456;&#20284;&#24615;&#21644;&#20849;&#29616;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20998;&#37197;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#21478;&#22806;&#65292;&#32771;&#34385;&#21040;&#29992;&#25143;&#20852;&#36259;&#38543;&#26102;&#38388;&#21464;&#21270;&#65292;&#26412;&#25991;&#36824;&#22312;&#20849;&#29616;&#26631;&#31614;&#20013;&#32771;&#34385;&#20102;&#26631;&#31614;&#20998;&#37197;&#30340;&#26102;&#38388;&#20197;&#30830;&#23450;&#26631;&#31614;&#30340;&#30456;&#20284;&#24615;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#26631;&#31614;&#30340;&#30456;&#20284;&#24615;&#21019;&#24314;&#22270;&#24418;&#27169;&#22411;&#26469;&#24314;&#27169;&#29992;&#25143;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many researchers have used tag information to improve the performance of recommendation techniques in recommender systems. Examining the tags of users will help to get their interests and leads to more accuracy in the recommendations. Since user-defined tags are chosen freely and without any restrictions, problems arise in determining their exact meaning and the similarity of tags. However, using thesaurus and ontologies to find the meaning of tags is not very efficient due to their free definition by users and the use of different languages in many data sets. Therefore, this article uses mathematical and statistical methods to determine lexical similarity and co-occurrence tags solution to assign semantic similarity. On the other hand, due to the change of users' interests over time this article has considered the time of tag assignments in co-occurrence tags for determining similarity of tags. Then the graph is created based on similarity of tags. For modeling the interests of the us
&lt;/p&gt;</description></item></channel></rss>