<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#34920;&#31034;&#30340;&#26041;&#24335;&#23545;&#24515;&#20869;&#30005;&#22270;&#36827;&#34892;&#25554;&#20540;&#21644;&#25151;&#39076;&#20998;&#31867;&#12290;&#30456;&#27604;&#20854;&#20182;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25151;&#39076;&#20998;&#31867;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01115</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#26412;&#34920;&#31034;&#35299;&#35835;&#24515;&#20869;&#30005;&#22270;
&lt;/p&gt;
&lt;p&gt;
Interpretation of Intracardiac Electrograms Through Textual Representations
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25991;&#26412;&#34920;&#31034;&#30340;&#26041;&#24335;&#23545;&#24515;&#20869;&#30005;&#22270;&#36827;&#34892;&#25554;&#20540;&#21644;&#25151;&#39076;&#20998;&#31867;&#12290;&#30456;&#27604;&#20854;&#20182;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25151;&#39076;&#20998;&#31867;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#25151;&#39076;(AFib)&#30340;&#19981;&#35268;&#21017;&#30005;&#27963;&#21160;&#19968;&#30452;&#26159;&#24515;&#30005;&#22270;&#23398;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#23545;&#20110;&#20005;&#37325;&#30340;&#25151;&#39076;&#30149;&#20363;&#65292;&#36827;&#34892;&#23548;&#31649;&#28040;&#34701;&#20197;&#33719;&#21462;&#24515;&#20869;&#30005;&#22270;(EGMs)&#12290;EGMs&#25552;&#20379;&#20102;&#24515;&#33039;&#30005;&#27963;&#21160;&#30340;&#22797;&#26434;&#32454;&#33410;&#21644;&#23616;&#37096;&#21270;&#20449;&#24687;&#65292;&#26159;&#21487;&#35299;&#37322;&#30340;&#24515;&#33039;&#30740;&#31350;&#30340;&#29702;&#24819;&#27169;&#24335;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#36827;&#23637;&#20351;&#24471;&#19968;&#20123;&#30740;&#31350;&#21487;&#20197;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#37322;&#25151;&#39076;&#20013;&#30340;EGMs&#12290;&#27492;&#22806;&#65292;&#35821;&#35328;&#27169;&#22411;(LMs)&#22312;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#21307;&#30103;&#39046;&#22495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LMs&#26469;&#36890;&#36807;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#23545;EGM&#25554;&#20540;&#21644;&#25151;&#39076;&#20998;&#31867;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#23558;EGM&#24418;&#24335;&#21270;&#20026;&#25991;&#26412;&#24207;&#21015;&#65292;&#24182;&#19982;&#20854;&#20182;&#34920;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#25151;&#39076;&#20998;&#31867;&#26041;&#38754;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35299;&#37322;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the irregular electrical activity of atrial fibrillation (AFib) has been a key challenge in electrocardiography. For serious cases of AFib, catheter ablations are performed to collect intracardiac electrograms (EGMs). EGMs offer intricately detailed and localized electrical activity of the heart and are an ideal modality for interpretable cardiac studies. Recent advancements in artificial intelligence (AI) has allowed some works to utilize deep learning frameworks to interpret EGMs during AFib. Additionally, language models (LMs) have shown exceptional performance in being able to generalize to unseen domains, especially in healthcare. In this study, we are the first to leverage pretrained LMs for finetuning of EGM interpolation and AFib classification via masked language modeling. We formulate the EGM as a textual sequence and present competitive performances on AFib classification compared against other representations. Lastly, we provide a comprehensive interpretabilit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27169;&#22411;&#35843;&#20248;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#65292;&#25104;&#21151;&#22312;&#24187;&#35273;&#26816;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;77.8%&#21644;79.9%&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#32452;&#32455;&#32773;&#30340;&#22522;&#32447;&#21644;&#31454;&#36187;&#20013;&#20854;&#20182;&#21442;&#36187;&#32773;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2404.01210</link><description>&lt;p&gt;
AILS-NTUA&#21442;&#21152;SemEval-2024&#20219;&#21153;6&#65306;&#29992;&#20110;&#24187;&#35273;&#26816;&#27979;&#21644;&#20998;&#26512;&#30340;&#39640;&#25928;&#27169;&#22411;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
AILS-NTUA at SemEval-2024 Task 6: Efficient model tuning for hallucination detection and analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01210
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27169;&#22411;&#35843;&#20248;&#31574;&#30053;&#65292;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#65292;&#25104;&#21151;&#22312;&#24187;&#35273;&#26816;&#27979;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;77.8%&#21644;79.9%&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#32452;&#32455;&#32773;&#30340;&#22522;&#32447;&#21644;&#31454;&#36187;&#20013;&#20854;&#20182;&#21442;&#36187;&#32773;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#22242;&#38431;&#38024;&#23545;SemEval-2024&#20219;&#21153;6 - SHROOM&#30340;&#25552;&#20132;&#20869;&#23481;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#24187;&#35273;&#21644;&#30456;&#20851;&#21487;&#35266;&#27979;&#36807;&#24230;&#29983;&#25104;&#38169;&#35823;&#30340;&#20849;&#20139;&#20219;&#21153;&#12290;&#21442;&#19982;&#32773;&#34987;&#35201;&#27714;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#65292;&#20197;&#35782;&#21035;&#27969;&#21033;&#36807;&#24230;&#29983;&#25104;&#30340;&#24187;&#35273;&#26696;&#20363;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21253;&#25324;&#23545;&#24187;&#35273;&#26816;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#26368;&#25104;&#21151;&#30340;&#31574;&#30053;&#28041;&#21450;&#21019;&#24314;&#36825;&#20123;&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#32467;&#26524;&#22312;&#27169;&#22411;&#19981;&#21487;&#30693;&#21644;&#27169;&#22411;&#24863;&#30693;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;77.8&#65285;&#21644;79.9&#65285;&#65292;&#36229;&#36807;&#20102;&#32452;&#32455;&#32773;&#30340;&#22522;&#32447;&#65292;&#24182;&#22312;&#19982;&#31454;&#36187;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#32467;&#26524;&#36827;&#34892;&#23545;&#27604;&#26102;&#21462;&#24471;&#26174;&#30528;&#25104;&#26524;&#65292;&#35813;&#31454;&#36187;&#25253;&#21578;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;84.7&#65285;&#21644;81.3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01210v1 Announce Type: new  Abstract: In this paper, we present our team's submissions for SemEval-2024 Task-6 - SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes. The participants were asked to perform binary classification to identify cases of fluent overgeneration hallucinations. Our experimentation included fine-tuning a pre-trained model on hallucination detection and a Natural Language Inference (NLI) model. The most successful strategy involved creating an ensemble of these models, resulting in accuracy rates of 77.8% and 79.9% on model-agnostic and model-aware datasets respectively, outperforming the organizers' baseline and achieving notable results when contrasted with the top-performing results in the competition, which reported accuracies of 84.7% and 81.3% correspondingly.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#22270;&#25968;&#25454;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22359;&#22686;&#24378;&#65292;&#25552;&#20379;&#32622;&#20449;&#24230;&#35780;&#20998;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#22270;&#23436;&#25104;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.00589</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#22270;&#25968;&#25454;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of Large Language Model for Uncertainty Aware Graph Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00589
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#22270;&#25968;&#25454;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22359;&#22686;&#24378;&#65292;&#25552;&#20379;&#32622;&#20449;&#24230;&#35780;&#20998;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#22270;&#23436;&#25104;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#22270;&#25968;&#25454;&#26159;&#19968;&#39033;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#25216;&#26415;&#65292;&#20363;&#22914;&#22522;&#20110;&#20960;&#20309;&#21644;&#30697;&#38453;&#20998;&#35299;&#30340;&#25216;&#26415;&#65292;&#20381;&#36182;&#20110;&#23545;&#25968;&#25454;&#20851;&#31995;&#30340;&#20551;&#35774;&#65292;&#22312;&#22788;&#29702;&#22823;&#22411;&#21644;&#22797;&#26434;&#30340;&#22270;&#25968;&#25454;&#26102;&#21464;&#24471;&#19981;&#36275;&#22815;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23637;&#31034;&#20102;&#22788;&#29702;&#22823;&#22411;&#22270;&#25968;&#25454;&#30340;&#33391;&#22909;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#35299;&#37322;&#12290;&#20026;&#20102;&#20351;&#22270;&#22788;&#29702;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#22686;&#24378;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#27169;&#22359;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#21147;&#37327;&#65292;&#20197;&#25552;&#20379;&#29983;&#25104;&#31572;&#26696;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22270;&#22788;&#29702;&#20219;&#21153;&#19978;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#23454;&#39564;&#65306;&#23569;&#26679;&#26412;&#30693;&#35782;&#22270;&#23436;&#25104;&#21644;&#22270;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;LLM&#22312;&#21508;&#20010;&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00589v1 Announce Type: cross  Abstract: Handling graph data is one of the most difficult tasks. Traditional techniques, such as those based on geometry and matrix factorization, rely on assumptions about the data relations that become inadequate when handling large and complex graph data. On the other hand, deep learning approaches demonstrate promising results in handling large graph data, but they often fall short of providing interpretable explanations. To equip the graph processing with both high accuracy and explainability, we introduce a novel approach that harnesses the power of a large language model (LLM), enhanced by an uncertainty-aware module to provide a confidence score on the generated answer. We experiment with our approach on two graph processing tasks: few-shot knowledge graph completion and graph classification. Our results demonstrate that through parameter efficient fine-tuning, the LLM surpasses state-of-the-art algorithms by a substantial margin across
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.15744</link><description>&lt;p&gt;
&#35770;&#20027;&#21160;&#23398;&#20064;&#32773;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Fragility of Active Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21482;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#26377;&#25928;&#65292;&#23545;&#25991;&#26412;&#20998;&#31867;&#20174;&#19994;&#32773;&#30340;&#24314;&#35758;&#26159;&#36873;&#25321;&#36866;&#24403;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#21516;&#26679;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#25216;&#26415;&#26088;&#22312;&#36890;&#36807;&#36845;&#20195;&#36873;&#25321;&#26368;&#26377;&#21487;&#33021;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#23454;&#20363;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#26631;&#27880;&#39044;&#31639;&#12290;&#28982;&#32780;&#65292;&#19982;&#38543;&#26426;&#25277;&#26679;&#30456;&#27604;&#65292;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#65288;&#20363;&#22914;&#19981;&#21516;&#25968;&#25454;&#38598;&#65292;&#20998;&#31867;&#22120;&#65289;&#65292;&#23427;&#20204;&#30340;&#30410;&#22788;&#24182;&#19981;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22240;&#32032;&#30340;&#32452;&#21512;&#22914;&#20309;&#21487;&#33021;&#25513;&#30422;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#30340;&#20219;&#20309;&#25910;&#30410;&#12290;&#19987;&#27880;&#20110;&#25991;&#26412;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;&#36827;&#34892;&#20998;&#31867;&#65292;&#25105;&#20204;&#22312;&#22823;&#32422;1000&#20010;&#23454;&#39564;&#20013;&#20005;&#26684;&#35780;&#20272;&#20102;AL&#25216;&#26415;&#65292;&#36825;&#20123;&#23454;&#39564;&#22312;&#25968;&#25454;&#38598;&#12289;&#25209;&#22823;&#23567;&#12289;&#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#26041;&#38754;&#21464;&#21270;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;AL&#21482;&#22312;&#19968;&#32452;&#26377;&#38480;&#30340;&#24773;&#22659;&#20013;&#26377;&#25928;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#20351;&#29992;&#19982;&#29616;&#23454;&#19990;&#30028;&#26399;&#26395;&#26356;&#22909;&#23545;&#40784;&#30340;&#24230;&#37327;&#30340;&#38382;&#39064;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#24433;&#21709;&#22312;&#20110;&#23545;&#20174;&#19994;&#32773;&#30340;&#27934;&#23519;&#65306;(a) &#25991;&#26412;&#34920;&#31034;&#21644;&#20998;&#31867;&#22120;&#30340;&#36873;&#25321;&#19982;AL&#25216;&#26415;&#30340;&#36873;&#25321;&#19968;&#26679;&#37325;&#35201;&#65292;(b) &#36873;&#25321;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15744v1 Announce Type: cross  Abstract: Active learning (AL) techniques aim to maximally utilize a labeling budget by iteratively selecting instances that are most likely to improve prediction accuracy. However, their benefit compared to random sampling has not been consistent across various setups, e.g., different datasets, classifiers. In this empirical study, we examine how a combination of different factors might obscure any gains from an AL technique.   Focusing on text classification, we rigorously evaluate AL techniques over around 1000 experiments that vary wrt the dataset, batch size, text representation and the classifier. We show that AL is only effective in a narrow set of circumstances. We also address the problem of using metrics that are better aligned with real world expectations.   The impact of this study is in its insights for a practitioner: (a) the choice of text representation and classifier is as important as that of an AL technique, (b) choice of the 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;39&#31687;&#26368;&#26032;&#35770;&#25991;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#26410;&#23545;&#8220;&#25991;&#21270;&#8221;&#36827;&#34892;&#23450;&#20041;&#65292;&#32780;&#26159;&#22312;&#29305;&#23450;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#30740;&#31350;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#65292;&#30041;&#19979;&#35768;&#22810;&#26410;&#34987;&#25506;&#31350;&#30340;&#26377;&#36259;&#21644;&#37325;&#35201;&#26041;&#38754;&#65292;&#22914;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15412</link><description>&lt;p&gt;
&#22312;LLMs&#20013;&#27979;&#37327;&#21644;&#24314;&#27169;&#8220;&#25991;&#21270;&#8221;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Towards Measuring and Modeling "Culture" in LLMs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15412
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;39&#31687;&#26368;&#26032;&#35770;&#25991;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#26410;&#23545;&#8220;&#25991;&#21270;&#8221;&#36827;&#34892;&#23450;&#20041;&#65292;&#32780;&#26159;&#22312;&#29305;&#23450;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#30740;&#31350;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#65292;&#30041;&#19979;&#35768;&#22810;&#26410;&#34987;&#25506;&#31350;&#30340;&#26377;&#36259;&#21644;&#37325;&#35201;&#26041;&#38754;&#65292;&#22914;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21576;&#29616;&#20102;&#23545;39&#31687;&#26368;&#26032;&#35770;&#25991;&#30340;&#35843;&#26597;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#27809;&#26377;&#19968;&#31687;&#30740;&#31350;&#23450;&#20041;&#8220;&#25991;&#21270;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#12289;&#22810;&#23618;&#38754;&#30340;&#27010;&#24565;&#65307;&#30456;&#21453;&#65292;&#23427;&#20204;&#22312;&#19968;&#20123;&#29305;&#21035;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20195;&#34920;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#38754;&#31216;&#20026;&#25991;&#21270;&#30340;&#20195;&#29702;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#32455;&#22312;&#20154;&#21475;&#32479;&#35745;&#12289;&#35821;&#20041;&#21644;&#35821;&#35328;&#25991;&#21270;&#20132;&#20114;&#20195;&#29702;&#30340;&#19977;&#20010;&#32500;&#24230;&#19978;&#12290;&#25105;&#20204;&#36824;&#23545;&#37319;&#29992;&#30340;&#25506;&#26597;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#21482;&#26377;&#8220;&#25991;&#21270;&#8221;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#22914;&#20215;&#20540;&#35266;&#21644;&#30446;&#26631;&#65292;&#34987;&#30740;&#31350;&#20102;&#65292;&#30041;&#19979;&#20102;&#20960;&#20010;&#20854;&#20182;&#26377;&#36259;&#19988;&#37325;&#35201;&#30340;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#22823;&#37327;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#65288;Hershcovich&#31561;&#20154;&#65292;2022&#65289;&#30340;&#26410;&#34987;&#25506;&#31350;&#12290;&#21478;&#22806;&#20004;&#20010;&#20851;&#38190;&#30340;&#31354;&#30333;&#26159;&#30446;&#21069;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#24773;&#22659;&#24615;&#30340;&#32570;&#20047;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15412v1 Announce Type: cross  Abstract: We present a survey of 39 recent papers that aim to study cultural representation and inclusion in large language models. We observe that none of the studies define "culture," which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of "culture." We call these aspects the proxies of cultures, and organize them across three dimensions of demographic, semantic and linguistic-cultural interaction proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of "culture," such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situatedness of the current methods. Based on these observations
&lt;/p&gt;</description></item><item><title>PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15388</link><description>&lt;p&gt;
LLaVA-PruMerge: &#33258;&#36866;&#24212;&#20196;&#29260;&#20943;&#23569;&#29992;&#20110;&#39640;&#25928;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15388
&lt;/p&gt;
&lt;p&gt;
PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#36890;&#36807;&#36830;&#25509;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;LMMs&#21253;&#25324;&#20102;&#26356;&#22797;&#26434;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22914;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#36825;&#26174;&#33879;&#22686;&#21152;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#20196;&#29260;&#20943;&#23569;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#31867;&#20284;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#35768;&#22810;&#35270;&#35273;&#20196;&#29260;&#22312;&#31354;&#38388;&#19978;&#26159;&#20887;&#20313;&#30340;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PruMerge&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21487;&#27604;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15388v1 Announce Type: cross  Abstract: Large Multimodal Models (LMMs) have shown significant reasoning capabilities by connecting a visual encoder and a large language model. LMMs typically use a fixed amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content. Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which increase the number of visual tokens significantly. However, due to the design of the Transformer architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many visual tokens are spatially redundant. Based on this, we propose PruMerge, a novel adaptive visual token reduction approach, which largely reduces the number of visual tokens while maintaining comparable model performance. We first select 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#30524;&#25511;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#38477;&#20302;&#23545;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;</title><link>https://arxiv.org/abs/2403.12416</link><description>&lt;p&gt;
&#38024;&#23545;&#25918;&#23556;&#23398;&#30340;&#30524;&#25511;&#24341;&#23548;&#22810;&#27169;&#24577;&#23545;&#40784;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Eye-gaze Guided Multi-modal Alignment Framework for Radiology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12416
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#30524;&#25511;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#23545;&#40784;&#26694;&#26550;&#65292;&#21487;&#38477;&#20302;&#23545;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#26694;&#26550;&#20013;&#65292;&#36328;&#27169;&#24577;&#29305;&#24449;&#30340;&#23545;&#40784;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24378;&#35843;&#20840;&#23616;&#25110;&#23616;&#37096;&#27169;&#24577;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#21033;&#29992;&#22823;&#37327;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33258;&#24213;&#21521;&#19978;&#30340;&#26041;&#27861;&#22312;&#25918;&#23556;&#23398;&#20013;&#24120;&#24120;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25918;&#23556;&#31185;&#21307;&#29983;&#22312;&#35786;&#26029;&#35780;&#20272;&#36807;&#31243;&#20013;&#21516;&#27493;&#25910;&#38598;&#30340;&#30524;&#25511;&#25968;&#25454;&#65292;&#23558;&#33016;&#37096;X&#32447;&#33258;&#28982;&#22320;&#19982;&#35786;&#26029;&#25991;&#26412;&#30456;&#20851;&#32852;&#65292;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#26088;&#22312;&#20943;&#23569;&#23545;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12416v1 Announce Type: cross  Abstract: In multi-modal frameworks, the alignment of cross-modal features presents a significant challenge. The predominant approach in multi-modal pre-training emphasizes either global or local alignment between modalities, utilizing extensive datasets. This bottom-up driven method often suffers from a lack of interpretability, a critical concern in radiology. Previous studies have integrated high-level labels in medical images or text, but these still rely on manual annotation, a costly and labor-intensive process. Our work introduces a novel approach by using eye-gaze data, collected synchronously by radiologists during diagnostic evaluations. This data, indicating radiologists' focus areas, naturally links chest X-rays to diagnostic texts. We propose the Eye-gaze Guided Multi-modal Alignment (EGMA) framework to harness eye-gaze data for better alignment of image and text features, aiming to reduce reliance on manual annotations and thus cut
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;QAQ&#65292;&#19968;&#31181;&#29992;&#20110;KV&#32531;&#23384;&#30340;&#36136;&#37327;&#33258;&#36866;&#24212;&#37327;&#21270;&#26041;&#26696;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20851;&#38190;&#32531;&#23384;&#21644;&#20540;&#32531;&#23384;&#23545;&#37327;&#21270;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#65292;&#22240;&#27492;&#21046;&#23450;&#20102;&#19981;&#21516;&#30340;&#37327;&#21270;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.04643</link><description>&lt;p&gt;
QAQ&#65306;&#29992;&#20110;LLM KV&#32531;&#23384;&#30340;&#36136;&#37327;&#33258;&#36866;&#24212;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QAQ: Quality Adaptive Quantization for LLM KV Cache
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04643
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;QAQ&#65292;&#19968;&#31181;&#29992;&#20110;KV&#32531;&#23384;&#30340;&#36136;&#37327;&#33258;&#36866;&#24212;&#37327;&#21270;&#26041;&#26696;&#65292;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20851;&#38190;&#32531;&#23384;&#21644;&#20540;&#32531;&#23384;&#23545;&#37327;&#21270;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#65292;&#22240;&#27492;&#21046;&#23450;&#20102;&#19981;&#21516;&#30340;&#37327;&#21270;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#30340;&#20986;&#29616;&#22312;NLP&#24212;&#29992;&#20013;&#24341;&#21457;&#20102;&#19968;&#27874;&#26032;&#30340;&#31361;&#30772;&#65292;&#23588;&#20854;&#22312;&#35832;&#22914;&#38382;&#31572;&#31995;&#32479;&#21644;&#25991;&#26412;&#29983;&#25104;&#31561;&#39046;&#22495;&#12290;&#38543;&#30528;&#23545;&#26356;&#38271;&#19978;&#19979;&#25991;&#30340;&#38656;&#27714;&#22686;&#38271;&#65292;&#27169;&#22411;&#37096;&#32626;&#20013;&#20986;&#29616;&#20102;&#19968;&#20010;&#37325;&#35201;&#29942;&#39048;&#65292;&#21363;&#30001;&#20110;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#32447;&#24615;&#22686;&#21152;&#32780;&#23548;&#33268;&#30340;Key-Value (KV) cache&#30340;&#25193;&#23637;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#21508;&#31181;&#20551;&#35774;&#65292;&#20363;&#22914;&#26681;&#25454;&#27880;&#24847;&#21147;&#20998;&#25968;&#23545;KV cache&#36827;&#34892;&#25490;&#24207;&#20197;&#36827;&#34892;&#26367;&#25442;&#25110;&#39537;&#36880;&#65292;&#20197;&#21387;&#32553;KV cache&#24182;&#25552;&#39640;&#27169;&#22411;&#21534;&#21520;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31574;&#30053;&#20351;&#29992;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#39537;&#36880;&#20851;&#38190;&#30340;KV&#32531;&#23384;&#65292;&#20174;&#32780;&#20005;&#37325;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QAQ&#65292;&#19968;&#31181;&#29992;&#20110;KV&#32531;&#23384;&#30340;&#36136;&#37327;&#33258;&#36866;&#24212;&#37327;&#21270;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20851;&#38190;&#32531;&#23384;&#21644;&#20540;&#32531;&#23384;&#23545;&#37327;&#21270;&#34920;&#29616;&#20986;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#65292;&#20174;&#32780;&#24341;&#21457;&#20102;&#38024;&#23545;&#23427;&#20204;&#30340;&#38750;&#22343;&#21248;&#37327;&#21270;&#31574;&#30053;&#30340;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04643v1 Announce Type: new  Abstract: The emergence of LLMs has ignited a fresh surge of breakthroughs in NLP applications, particularly in domains such as question-answering systems and text generation. As the need for longer context grows, a significant bottleneck in model deployment emerges due to the linear expansion of the Key-Value (KV) cache with the context length. Existing methods primarily rely on various hypotheses, such as sorting the KV cache based on attention scores for replacement or eviction, to compress the KV cache and improve model throughput. However, heuristics used by these strategies may wrongly evict essential KV cache, which can significantly degrade model performance. In this paper, we propose QAQ, a Quality Adaptive Quantization scheme for the KV cache. We theoretically demonstrate that key cache and value cache exhibit distinct sensitivities to quantization, leading to the formulation of separate quantization strategies for their non-uniform quan
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#38543;&#26426;&#20559;&#22909;&#32763;&#36716;&#30340;&#31574;&#30053;&#20248;&#21270;&#36890;&#29992;&#26694;&#26550;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#23384;&#22312;&#22024;&#26434;&#21453;&#39304;&#26102;&#30340;DPO&#31639;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#20154;&#31867;&#20852;&#36259;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.00409</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;DPO: &#29992;&#26377;&#22122;&#21453;&#39304;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Provably Robust DPO: Aligning Language Models with Noisy Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00409
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#38543;&#26426;&#20559;&#22909;&#32763;&#36716;&#30340;&#31574;&#30053;&#20248;&#21270;&#36890;&#29992;&#26694;&#26550;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#29702;&#35299;&#23384;&#22312;&#22024;&#26434;&#21453;&#39304;&#26102;&#30340;DPO&#31639;&#27861;&#65292;&#20174;&#32780;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#20154;&#31867;&#20852;&#36259;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20174;&#22522;&#20110;&#21916;&#22909;&#21453;&#39304;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#19982;&#20154;&#31867;&#20852;&#36259;&#23545;&#40784;&#30340;&#26377;&#21069;&#26223;&#26041;&#27861;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#40784;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#23545;&#39640;&#36136;&#37327;&#20154;&#31867;&#21916;&#22909;&#25968;&#25454;&#30340;&#20381;&#36182;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26500;&#25104;&#20102;&#29942;&#39048;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25968;&#25454;&#38598;&#20013;&#26377;&#22122;&#65288;&#19981;&#27491;&#30830;&#21644;&#27169;&#31946;&#65289;&#30340;&#20559;&#22909;&#23545;&#21487;&#33021;&#20250;&#38480;&#21046;&#35821;&#35328;&#27169;&#22411;&#20934;&#30830;&#25429;&#25417;&#20154;&#31867;&#24847;&#22270;&#12290;&#34429;&#28982;&#20174;&#19994;&#32773;&#26368;&#36817;&#25552;&#20986;&#20102;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#20943;&#36731;&#22122;&#22768;&#20559;&#22909;&#30340;&#24433;&#21709;&#65292;&#20294;&#23545;&#23427;&#20204;&#30340;&#24037;&#20316;&#23436;&#25972;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#38754;&#21521;&#22312;&#38543;&#26426;&#20559;&#22909;&#32763;&#36716;&#23384;&#22312;&#30340;&#31574;&#30053;&#20248;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#31639;&#27861;&#65292;&#22240;&#20026;&#23427;&#20551;&#35774;&#20559;&#22909;&#36981;&#24490; Bradley-Te
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00409v1 Announce Type: cross  Abstract: Learning from preference-based feedback has recently gained traction as a promising approach to align language models with human interests. While these aligned generative models have demonstrated impressive capabilities across various tasks, their dependence on high-quality human preference data poses a bottleneck in practical applications. Specifically, noisy (incorrect and ambiguous) preference pairs in the dataset might restrict the language models from capturing human intent accurately. While practitioners have recently proposed heuristics to mitigate the effect of noisy preferences, a complete theoretical understanding of their workings remain elusive.   In this work, we aim to bridge this gap by by introducing a general framework for policy optimization in the presence of random preference flips. We focus on the direct preference optimization (DPO) algorithm in particular since it assumes that preferences adhere to the Bradley-Te
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Ex&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20107;&#23454;&#38169;&#35823;&#35828;&#26126;&#27493;&#39588;&#26469;&#20462;&#27491;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#26469;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#21644;&#25346;&#38047;&#26102;&#38388;</title><link>https://arxiv.org/abs/2402.17097</link><description>&lt;p&gt;
&#20462;&#22797;: &#22312;&#35828;&#26126;&#21518;&#20462;&#27491;LLM&#21709;&#24212;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17097
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Ex&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20107;&#23454;&#38169;&#35823;&#35828;&#26126;&#27493;&#39588;&#26469;&#20462;&#27491;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#26469;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#21644;&#25346;&#38047;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#26159;LLM&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#65292;&#25105;&#20204;&#38656;&#35201;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#20197;&#20415;&#21487;&#38752;&#22320;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#26816;&#26597;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#30456;&#24212;&#22320;&#36827;&#34892;&#20462;&#35746;&#65292;&#20197;&#20943;&#23569;&#24187;&#35273;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Re-Ex&#65292;&#19968;&#31181;&#20462;&#35746;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#20107;&#23454;&#38169;&#35823;&#35828;&#26126;&#27493;&#39588;&#30340;&#26032;&#27493;&#39588;&#12290; Re-Ex&#20351;&#29992;3&#20010;&#27493;&#39588;&#23545;LLM&#30340;&#21021;&#22987;&#21709;&#24212;&#36827;&#34892;&#20462;&#35746;&#65306;&#39318;&#20808;&#65292;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#33719;&#21462;&#21709;&#24212;&#20013;&#20107;&#23454;&#38169;&#35823;&#30340;&#35777;&#25454;&#65307;&#31532;&#20108;&#65292;&#35201;&#27714;LLM&#26681;&#25454;&#31532;&#19968;&#27493;&#20013;&#25910;&#38598;&#30340;&#35777;&#25454;&#35299;&#37322;&#21709;&#24212;&#20013;&#30340;&#38382;&#39064;&#37096;&#20998;&#65307;&#26368;&#21518;&#65292;LLM&#20351;&#29992;&#22312;&#31532;&#20108;&#27493;&#20013;&#33719;&#24471;&#30340;&#35299;&#37322;&#23545;&#21709;&#24212;&#36827;&#34892;&#20462;&#35746;&#12290;&#38500;&#20102;&#35828;&#26126;&#27493;&#39588;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#21644;&#25346;&#38047;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17097v1 Announce Type: cross  Abstract: Mitigating hallucination issues is one of the main challenges of LLMs we need to overcome, in order to reliably use them in real-world scenarios. Recently, various methods are proposed to check the factual errors in the LLM-generated texts and revise them accordingly, to reduce the hallucination issue. In this paper, we propose Re-Ex, a method of revising LLM-generated texts, which introduces a novel step dubbed as the factual error explanation step. Re-Ex revises the initial response of LLMs using 3-steps: first, external tools are used to get the evidences on the factual errors in the response; second, LLMs are instructed to explain the problematic parts of the response based on the evidences gathered in the first step; finally, LLMs revise the response using the explanation obtained in the second step. In addition to the explanation step, we propose new prompting techniques to reduce the amount of tokens and wall-clock time required
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#20219;&#21153;&#30340;&#20004;&#31181;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#37096;&#20998;&#23448;&#26041;&#22522;&#20934;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12730</link><description>&lt;p&gt;
UMBCLU&#22312;SemEval-2024&#20219;&#21153;1A&#21644;1C&#20013;&#30340;&#34920;&#29616;&#65306;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#26426;&#22120;&#32763;&#35793;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with and without machine translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12730
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#20219;&#21153;&#30340;&#20004;&#31181;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#37096;&#20998;&#23448;&#26041;&#22522;&#20934;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#20026;SemEval-2024&#20219;&#21153;1&#24320;&#21457;&#30340;&#31995;&#32479;&#65292;&#8220;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#8221;&#12290; &#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#35782;&#21035;&#30446;&#26631;&#35821;&#35328;&#20013;&#23646;&#20110;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#38598;&#21512;&#30340;&#20004;&#20010;&#21477;&#23376;&#20043;&#38388;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65288;STR&#65289;&#30340;&#27169;&#22411;&#12290; &#25105;&#20204;&#21442;&#19982;&#20102;&#23376;&#20219;&#21153;A&#21644;C&#65292;&#24182;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#30417;&#30563;&#21644;&#36328;&#35821;&#35328;&#35757;&#32451;&#12290; &#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290; &#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#21477;&#23376;&#23884;&#20837;LLMs&#30340;&#32452;&#21512;&#65292;&#25105;&#20204;&#20026;&#23376;&#20219;&#21153;A&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;STR&#27169;&#22411;&#65292;TranSem&#65292;&#24182;&#23545;STR&#25968;&#25454;&#19978;&#30340;T5&#31995;&#21015;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#29992;&#20110;&#23376;&#20219;&#21153;C&#30340;FineSem&#12290; &#25105;&#20204;&#22312;&#23376;&#20219;&#21153;A&#20013;7&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#32467;&#26524;&#27604;3&#31181;&#35821;&#35328;&#30340;&#23448;&#26041;&#22522;&#20934;&#26356;&#22909;&#65292;&#32780;&#19982;&#20854;&#20182;4&#31181;&#35821;&#35328;&#30340;&#22522;&#20934;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12730v1 Announce Type: cross  Abstract: This paper describes the system we developed for SemEval-2024 Task 1, "Semantic Textual Relatedness for African and Asian Languages." The aim of the task is to build a model that can identify semantic textual relatedness (STR) between two sentences of a target language belonging to a collection of African and Asian languages. We participated in Subtasks A and C and explored supervised and cross-lingual training leveraging large language models (LLMs). Pre-trained large language models have been extensively used for machine translation and semantic similarity. Using a combination of machine translation and sentence embedding LLMs, we developed a unified STR model, TranSem, for subtask A and fine-tuned the T5 family of models on the STR data, FineSem, for use in subtask C. Our model results for 7 languages in subtask A were better than the official baseline for 3 languages and on par with the baseline for the remaining 4 languages. Our m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#26469;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#26500;&#24314;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#65292;&#24182;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#39564;&#35777;&#12290;&#20182;&#20204;&#25554;&#20837;&#35757;&#32451;&#22909;&#30340;&#21152;&#27861;&#27169;&#22411;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#20182;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.02619</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Increasing Trust in Language Models through the Reuse of Verified Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#32463;&#36807;&#39564;&#35777;&#30340;&#30005;&#36335;&#26469;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#36890;&#36807;&#26500;&#24314;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#65292;&#24182;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#23436;&#20840;&#39564;&#35777;&#12290;&#20182;&#20204;&#25554;&#20837;&#35757;&#32451;&#22909;&#30340;&#21152;&#27861;&#27169;&#22411;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#20182;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20294;&#23427;&#20204;&#30340;&#35757;&#32451;&#32463;&#24120;&#24573;&#30053;&#32597;&#35265;&#30340;&#36793;&#30028;&#24773;&#20917;&#65292;&#38477;&#20302;&#20102;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#20005;&#26684;&#30340;&#21487;&#20449;&#24230;&#26631;&#20934;&#65292;&#21363;&#20219;&#21153;&#31639;&#27861;&#21644;&#30005;&#36335;&#23454;&#29616;&#24517;&#39035;&#32463;&#36807;&#39564;&#35777;&#65292;&#32771;&#34385;&#21040;&#36793;&#30028;&#24773;&#20917;&#65292;&#24182;&#19988;&#27809;&#26377;&#24050;&#30693;&#30340;&#25925;&#38556;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#25968;&#23398;&#21644;&#36923;&#36753;&#35268;&#33539;&#30340;&#26694;&#26550;&#26469;&#26500;&#24314;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#35757;&#32451;&#20986;&#28385;&#36275;&#36825;&#19968;&#26631;&#20934;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;n&#20301;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#36827;&#34892;&#20102;&#23436;&#20840;&#39564;&#35777;&#12290;&#20026;&#20102;&#23637;&#31034;&#32463;&#36807;&#39564;&#35777;&#30340;&#27169;&#22359;&#30340;&#37325;&#22797;&#20351;&#29992;&#24615;&#65292;&#25105;&#20204;&#23558;&#35757;&#32451;&#22909;&#30340;&#25972;&#25968;&#21152;&#27861;&#27169;&#22411;&#25554;&#20837;&#21040;&#19968;&#20010;&#26410;&#32463;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#65292;&#24182;&#35757;&#32451;&#32452;&#21512;&#27169;&#22411;&#21516;&#26102;&#25191;&#34892;&#21152;&#27861;&#21644;&#20943;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#21152;&#27861;&#30005;&#36335;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#37325;&#22797;&#20351;&#29992;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#26356;&#22797;&#26434;&#30340;&#20943;&#27861;&#27169;&#22411;&#30340;&#39564;&#35777;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22914;&#20309;&#23558;&#32463;&#36807;&#39564;&#35777;&#30340;&#20219;&#21153;&#27169;&#22359;&#25554;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#21033;&#29992;&#27169;&#22411;&#30340;&#37325;&#22797;&#20351;&#29992;&#26469;&#25552;&#39640;&#21487;&#39564;&#35777;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) are increasingly used for a wide range of prediction tasks, but their training can often neglect rare edge cases, reducing their reliability. Here, we define a stringent standard of trustworthiness whereby the task algorithm and circuit implementation must be verified, accounting for edge cases, with no known failure modes. We show that a transformer model can be trained to meet this standard if built using mathematically and logically specified frameworks. In this paper, we fully verify a model for n-digit integer addition. To exhibit the reusability of verified modules, we insert the trained integer addition model into an untrained model and train the combined model to perform both addition and subtraction. We find extensive reuse of the addition circuits for both tasks, easing verification of the more complex subtractor model. We discuss how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of languag
&lt;/p&gt;</description></item><item><title>HuixiangDou&#26159;&#19968;&#31181;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25216;&#26415;&#21161;&#25163;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#24320;&#28304;&#31639;&#27861;&#39033;&#30446;&#30456;&#20851;&#38382;&#39064;&#30340;&#28145;&#20837;&#22238;&#31572;&#26469;&#21327;&#21161;&#31639;&#27861;&#24320;&#21457;&#20154;&#21592;&#12290;&#35813;&#21161;&#25163;&#24050;&#34987;&#25104;&#21151;&#25972;&#21512;&#21040;&#32676;&#32842;&#24037;&#20855;&#20013;&#65292;&#26377;&#25928;&#22320;&#22238;&#31572;&#29992;&#25143;&#30340;&#25216;&#26415;&#38382;&#39064;&#65292;&#24182;&#20855;&#22791;&#35780;&#20998;&#33021;&#21147;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#38271;&#19978;&#19979;&#25991;&#31561;&#20851;&#38190;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.08772</link><description>&lt;p&gt;
HuixiangDou&#65306;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#25216;&#26415;&#21161;&#25163;&#20811;&#26381;&#32676;&#32842;&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
HuixiangDou: Overcoming Group Chat Scenarios with LLM-based Technical Assistance. (arXiv:2401.08772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08772
&lt;/p&gt;
&lt;p&gt;
HuixiangDou&#26159;&#19968;&#31181;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25216;&#26415;&#21161;&#25163;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#24320;&#28304;&#31639;&#27861;&#39033;&#30446;&#30456;&#20851;&#38382;&#39064;&#30340;&#28145;&#20837;&#22238;&#31572;&#26469;&#21327;&#21161;&#31639;&#27861;&#24320;&#21457;&#20154;&#21592;&#12290;&#35813;&#21161;&#25163;&#24050;&#34987;&#25104;&#21151;&#25972;&#21512;&#21040;&#32676;&#32842;&#24037;&#20855;&#20013;&#65292;&#26377;&#25928;&#22320;&#22238;&#31572;&#29992;&#25143;&#30340;&#25216;&#26415;&#38382;&#39064;&#65292;&#24182;&#20855;&#22791;&#35780;&#20998;&#33021;&#21147;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#38271;&#19978;&#19979;&#25991;&#31561;&#20851;&#38190;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#30340;&#25216;&#26415;&#21161;&#25163;HuixiangDou&#12290;&#35813;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#23545;&#19982;&#24320;&#28304;&#31639;&#27861;&#39033;&#30446;&#30456;&#20851;&#30340;&#38382;&#39064;&#25552;&#20379;&#28145;&#20837;&#30340;&#22238;&#31572;&#65292;&#22914;&#26469;&#33258;OpenMMLab&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#28145;&#24230;&#23398;&#20064;&#39033;&#30446;&#65292;&#26469;&#21327;&#21161;&#31639;&#27861;&#24320;&#21457;&#20154;&#21592;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#23558;&#35813;&#21161;&#25163;&#25972;&#21512;&#21040;&#21363;&#26102;&#28040;&#24687;&#24037;&#20855;&#65288;&#22914;&#24494;&#20449;&#21644;Lark&#65289;&#30340;&#32676;&#32842;&#20013;&#12290;&#36890;&#36807;&#20960;&#27425;&#36845;&#20195;&#25913;&#36827;&#21644;&#35797;&#39564;&#65292;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#25216;&#26415;&#32842;&#22825;&#21161;&#25163;&#65292;&#33021;&#22815;&#22312;&#19981;&#36896;&#25104;&#28040;&#24687;&#27867;&#28389;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#22238;&#31572;&#29992;&#25143;&#30340;&#25216;&#26415;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#21253;&#25324;: 1) &#20026;&#32676;&#32842;&#22330;&#26223;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#27969;&#27700;&#32447;; 2) &#39564;&#35777;&#20102;&#25991;&#26412;2&#21521;&#37327;&#22312;&#20219;&#21153;&#25298;&#32477;&#20013;&#30340;&#21487;&#38752;&#24615;&#33021;; 3) &#30830;&#23450;&#20102;&#25216;&#26415;&#21161;&#25163;&#20135;&#21697;&#20013;LLM&#30340;&#19977;&#20010;&#20851;&#38190;&#35201;&#27714;&#65292;&#21363;&#35780;&#20998;&#33021;&#21147;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#38271;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#24050;&#32463;&#23436;&#25104;&#20102;&#36825;&#20123;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;
In this work, we present HuixiangDou, a technical assistant powered by Large Language Models (LLM). This system is designed to assist algorithm developers by providing insightful responses to questions related to open-source algorithm projects, such as computer vision and deep learning projects from OpenMMLab. We further explore the integration of this assistant into the group chats of instant messaging (IM) tools such as WeChat and Lark. Through several iterative improvements and trials, we have developed a sophisticated technical chat assistant capable of effectively answering users' technical questions without causing message flooding. This paper's contributions include: 1) Designing an algorithm pipeline specifically for group chat scenarios; 2) Verifying the reliable performance of text2vec in task rejection; 3) Identifying three critical requirements for LLMs in technical-assistant-like products, namely scoring ability, In-Context Learning (ICL), and Long Context. We have made th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;NLP&#27169;&#22411;&#30340;&#23884;&#20837;&#23618;&#32423;&#36827;&#34892;&#25805;&#20316;&#65292;&#20511;&#37492;&#20102;&#26368;&#26032;&#30340;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#36890;&#36807;&#23884;&#20837;&#36716;&#25442;&#26469;&#28040;&#38500;&#38544;&#21547;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.06499</link><description>&lt;p&gt;
TaCo&#65306;&#36890;&#36807;&#20449;&#24687;&#35770;&#21644;&#21487;&#35299;&#37322;&#24615;&#22312;NLP&#20013;&#30340;&#36755;&#20986;&#23884;&#20837;&#20013;&#23454;&#29616;&#26377;&#38024;&#23545;&#24615;&#30340;&#27010;&#24565;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
TaCo: Targeted Concept Removal in Output Embeddings for NLP via Information Theory and Explainability. (arXiv:2312.06499v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.06499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;NLP&#27169;&#22411;&#30340;&#23884;&#20837;&#23618;&#32423;&#36827;&#34892;&#25805;&#20316;&#65292;&#20511;&#37492;&#20102;&#26368;&#26032;&#30340;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#36890;&#36807;&#23884;&#20837;&#36716;&#25442;&#26469;&#28040;&#38500;&#38544;&#21547;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20449;&#24687;&#35770;&#34920;&#26126;&#65292;&#20026;&#20102;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#27169;&#22411;&#19981;&#24212;&#33021;&#22815;&#39044;&#27979;&#25935;&#24863;&#21464;&#37327;&#65292;&#22914;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#12290;&#28982;&#32780;&#65292;&#19982;&#36825;&#20123;&#21464;&#37327;&#30456;&#20851;&#30340;&#20449;&#24687;&#36890;&#24120;&#20197;&#38544;&#24335;&#30340;&#26041;&#24335;&#20986;&#29616;&#22312;&#35821;&#35328;&#20013;&#65292;&#36825;&#32473;&#35782;&#21035;&#21644;&#20943;&#23569;&#20559;&#35265;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22312;NLP&#27169;&#22411;&#30340;&#23884;&#20837;&#23618;&#32423;&#19978;&#25805;&#20316;&#65292;&#29420;&#31435;&#20110;&#20855;&#20307;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20511;&#37492;&#20102;&#26368;&#36817;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#36827;&#23637;&#65292;&#24182;&#37319;&#29992;&#23884;&#20837;&#36716;&#25442;&#26469;&#28040;&#38500;&#36873;&#23450;&#21464;&#37327;&#20013;&#30340;&#38544;&#24335;&#20449;&#24687;&#12290;&#36890;&#36807;&#30452;&#25509;&#25805;&#32437;&#26368;&#21518;&#19968;&#23618;&#30340;&#23884;&#20837;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#27169;&#22411;&#20013;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#37325;&#22823;&#20462;&#25913;&#25110;&#37325;&#35757;&#32451;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#21518;&#22788;&#29702;&#26041;&#27861;&#26174;&#33879;&#38477;&#20302;&#20102;&#19982;&#24615;&#21035;&#30456;&#20851;&#30340;&#20851;&#32852;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fairness of Natural Language Processing (NLP) models has emerged as a crucial concern. Information theory indicates that to achieve fairness, a model should not be able to predict sensitive variables, such as gender, ethnicity, and age. However, information related to these variables often appears implicitly in language, posing a challenge in identifying and mitigating biases effectively. To tackle this issue, we present a novel approach that operates at the embedding level of an NLP model, independent of the specific architecture. Our method leverages insights from recent advances in XAI techniques and employs an embedding transformation to eliminate implicit information from a selected variable. By directly manipulating the embeddings in the final layer, our approach enables a seamless integration into existing models without requiring significant modifications or retraining. In evaluation, we show that the proposed post-hoc approach significantly reduces gender-related associati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07923</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Expresssive Power of Transformers with Chain of Thought. (arXiv:2310.07923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#20123;&#20986;&#20154;&#24847;&#26009;&#22320;&#31616;&#21333;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#20363;&#22914;&#26816;&#26597;&#22270;&#20013;&#26159;&#21542;&#23384;&#22312;&#36830;&#25509;&#30340;&#20004;&#20010;&#33410;&#28857;&#65292;&#25110;&#27169;&#25311;&#26377;&#38480;&#29366;&#24577;&#26426;&#65292;&#36825;&#20123;&#38382;&#39064;&#34987;&#35777;&#26126;&#26080;&#27861;&#30001;&#31435;&#21363;&#35835;&#21462;&#36755;&#20837;&#21518;&#22238;&#31572;&#30340;&#26631;&#20934;Transformer&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#20801;&#35768;Transformer&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#25110;&#8220;&#33609;&#31295;&#32440;&#8221;&#65292;&#21363;&#22312;&#22238;&#31572;&#20043;&#21069;&#29983;&#25104;&#24182;&#20381;&#36182;&#19968;&#31995;&#21015;&#20013;&#38388;token&#65292;&#21487;&#20197;&#25913;&#21892;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#38382;&#65306;&#36825;&#31181;&#20013;&#38388;&#29983;&#25104;&#26159;&#21542;&#20174;&#26681;&#26412;&#19978;&#25193;&#23637;&#20102;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;Transformer&#30340;&#35745;&#31639;&#33021;&#21147;&#65311;&#25105;&#20204;&#34920;&#26126;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#20294;&#22686;&#21152;&#30340;&#31243;&#24230;&#20851;&#38190;&#21462;&#20915;&#20110;&#20013;&#38388;&#29983;&#25104;&#30340;&#25968;&#37327;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#36755;&#20837;&#38271;&#24230;&#26469;&#35828;&#65292;&#20855;&#26377;&#23545;&#25968;&#32423;&#35299;&#30721;&#27493;&#39588;&#30340;Transformer&#35299;&#30721;&#22120;&#20165;&#30053;&#24494;&#25512;&#21160;&#20102;&#26631;&#20934;Transformer&#30340;&#26497;&#38480;&#65292;&#32780;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#21017;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#65288;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a "chain of thought" or "scratchpad", i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask: Does such intermediate generation fundamentally extend the computational power of a decoder-only transformer? We show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps adds a clear new ability (under standard compl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;Benchmark&#27979;&#35797;&#65292;&#21457;&#29616;&#20351;&#29992;Prompt Tuning&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Trie&#25628;&#32034;&#26469;&#35299;&#20915;&#29983;&#25104;&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.12075</link><description>&lt;p&gt;
&#20351;&#29992;Prompt&#35843;&#20248;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21152;&#36895;&#20027;&#39064;&#25237;&#36164;
&lt;/p&gt;
&lt;p&gt;
Accelerating Thematic Investment with Prompt Tuned Pretrained Language Models. (arXiv:2309.12075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;Benchmark&#27979;&#35797;&#65292;&#21457;&#29616;&#20351;&#29992;Prompt Tuning&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Trie&#25628;&#32034;&#26469;&#35299;&#20915;&#29983;&#25104;&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt Tuning&#20316;&#20026;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#26041;&#27861;&#65292;&#27491;&#22312;&#25104;&#20026;&#32454;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#23545;Prompt Tuning&#21644;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23558;&#20854;&#24212;&#29992;&#20110;&#23558;&#20844;&#21496;&#20998;&#31867;&#20026;&#25237;&#36164;&#20844;&#21496;&#19987;&#26377;&#30340;&#34892;&#19994;&#20998;&#31867;&#27861;&#65292;&#20197;&#25903;&#25345;&#20854;&#20027;&#39064;&#25237;&#36164;&#31574;&#30053;&#12290;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;PLMs&#36827;&#34892;&#25991;&#26412;&#21040;&#25991;&#26412;&#20998;&#31867;&#32463;&#24120;&#34987;&#25253;&#21578;&#20026;&#20248;&#20110;&#20351;&#29992;&#20998;&#31867;&#22836;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#22312;&#27599;&#20010;&#26631;&#31614;&#30001;&#22810;&#20010;&#20196;&#29260;&#32452;&#25104;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#65288;a&#65289;&#29983;&#25104;&#30340;&#26631;&#31614;&#21487;&#33021;&#19981;&#21305;&#37197;&#34892;&#19994;&#20998;&#31867;&#27861;&#20013;&#30340;&#20219;&#20309;&#26631;&#31614;&#65307;&#65288;b&#65289;&#22312;&#32454;&#35843;&#38454;&#27573;&#65292;&#24517;&#39035;&#20197;&#20219;&#24847;&#39034;&#24207;&#25552;&#20379;&#22810;&#20010;&#26631;&#31614;&#65307;&#65288;c&#65289;&#27169;&#22411;&#20026;&#27599;&#20010;&#26631;&#31614;&#25552;&#20379;&#20108;&#36827;&#21046;&#20915;&#31574;&#65292;&#32780;&#19981;&#26159;&#36866;&#24403;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;&#36890;&#36807;&#24212;&#29992;Trie&#25628;&#32034;&#26469;&#35299;&#20915;&#38480;&#21046;&#65288;a&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt Tuning is emerging as a scalable and cost-effective method to fine-tune Pretrained Language Models (PLMs). This study benchmarks the performance and computational efficiency of Prompt Tuning and baseline methods on a multi-label text classification task. This is applied to the use case of classifying companies into an investment firm's proprietary industry taxonomy, supporting their thematic investment strategy. Text-to-text classification with PLMs is frequently reported to outperform classification with a classification head, but has several limitations when applied to a multi-label classification problem where each label consists of multiple tokens: (a) Generated labels may not match any label in the industry taxonomy; (b) During fine-tuning, multiple labels must be provided in an arbitrary order; (c) The model provides a binary decision for each label, rather than an appropriate confidence score. Limitation (a) is addressed by applying constrained decoding using Trie Search,
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20843;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#25688;&#35201;&#20219;&#21153;&#19978;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#23454;&#39564;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#20339;&#36866;&#24212;&#30340;&#27169;&#22411;&#30340;&#25688;&#35201;&#22312;&#23436;&#25972;&#24615;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.07430</link><description>&lt;p&gt;
&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts. (arXiv:2309.07430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20843;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#25688;&#35201;&#20219;&#21153;&#19978;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#23454;&#39564;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#20339;&#36866;&#24212;&#30340;&#27169;&#22411;&#30340;&#25688;&#35201;&#22312;&#23436;&#25972;&#24615;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#24037;&#20316;&#20013;&#65292;&#27983;&#35272;&#22823;&#37327;&#30340;&#25991;&#26412;&#25968;&#25454;&#24182;&#24635;&#32467;&#20851;&#38190;&#20449;&#24687;&#23545;&#20020;&#24202;&#21307;&#29983;&#30340;&#26102;&#38388;&#20998;&#37197;&#36896;&#25104;&#20102;&#24456;&#22823;&#30340;&#36127;&#25285;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#21508;&#31181;&#20020;&#24202;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#23578;&#26410;&#24471;&#21040;&#20005;&#26684;&#30340;&#26816;&#39564;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20843;&#20010;LLMs&#36827;&#34892;&#20102;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;&#19981;&#21516;&#30340;&#25688;&#35201;&#20219;&#21153;&#65306;&#25918;&#23556;&#23398;&#25253;&#21578;&#12289;&#24739;&#32773;&#38382;&#39064;&#12289;&#30149;&#21382;&#35760;&#24405;&#21644;&#21307;&#24739;&#23545;&#35805;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#27169;&#22411;&#21644;&#36866;&#24212;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;LLMs&#30340;&#26368;&#26032;&#36827;&#23637;&#21487;&#33021;&#19981;&#20250;&#24102;&#26469;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19982;&#20845;&#21517;&#21307;&#29983;&#36827;&#34892;&#30340;&#20020;&#24202;&#38405;&#35835;&#32773;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#20339;&#36866;&#24212;&#30340;LLM&#30340;&#25688;&#35201;&#22312;&#23436;&#25972;&#24615;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#25688;&#35201;&#12290;&#25105;&#20204;&#30340;&#36827;&#19968;&#27493;&#23450;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;LLMs&#21644;&#20154;&#31867;&#22312;&#38754;&#23545;&#30340;&#20849;&#21516;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sifting through vast textual data and summarizing key information imposes a substantial burden on how clinicians allocate their time. Although large language models (LLMs) have shown immense promise in natural language processing (NLP) tasks, their efficacy across diverse clinical summarization tasks has not yet been rigorously examined. In this work, we employ domain adaptation methods on eight LLMs, spanning six datasets and four distinct summarization tasks: radiology reports, patient questions, progress notes, and doctor-patient dialogue. Our thorough quantitative assessment reveals trade-offs between models and adaptation methods in addition to instances where recent advances in LLMs may not lead to improved results. Further, in a clinical reader study with six physicians, we depict that summaries from the best adapted LLM are preferable to human summaries in terms of completeness and correctness. Our ensuing qualitative analysis delineates mutual challenges faced by both LLMs and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20840;&#29699;&#35266;&#28857;&#30340;&#20195;&#34920;&#24615;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#36328;&#22269;&#35843;&#26597;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23450;&#20041;&#19968;&#20010;&#30456;&#20284;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#40664;&#35748;&#24773;&#20917;&#19979;&#35821;&#35328;&#27169;&#22411;&#30340;&#22238;&#24212;&#26356;&#20542;&#21521;&#20110;&#26576;&#20123;&#20154;&#32676;&#30340;&#35266;&#28857;&#65292;&#20294;&#24403;&#27169;&#22411;&#32771;&#34385;&#29305;&#23450;&#22269;&#23478;&#30340;&#35266;&#28857;&#26102;&#65292;&#22238;&#24212;&#20250;&#26356;&#21152;&#36148;&#36817;&#35813;&#22269;&#23478;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.16388</link><description>&lt;p&gt;
&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#20013;&#20027;&#35266;&#20840;&#29699;&#35266;&#28857;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Measuring the Representation of Subjective Global Opinions in Language Models. (arXiv:2306.16388v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20840;&#29699;&#35266;&#28857;&#30340;&#20195;&#34920;&#24615;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#36328;&#22269;&#35843;&#26597;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23450;&#20041;&#19968;&#20010;&#30456;&#20284;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#40664;&#35748;&#24773;&#20917;&#19979;&#35821;&#35328;&#27169;&#22411;&#30340;&#22238;&#24212;&#26356;&#20542;&#21521;&#20110;&#26576;&#20123;&#20154;&#32676;&#30340;&#35266;&#28857;&#65292;&#20294;&#24403;&#27169;&#22411;&#32771;&#34385;&#29305;&#23450;&#22269;&#23478;&#30340;&#35266;&#28857;&#26102;&#65292;&#22238;&#24212;&#20250;&#26356;&#21152;&#36148;&#36817;&#35813;&#22269;&#23478;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#26080;&#27861;&#20844;&#24179;&#22320;&#20195;&#34920;&#31038;&#20250;&#38382;&#39064;&#20013;&#22810;&#26679;&#21270;&#30340;&#20840;&#29699;&#35266;&#28857;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#37327;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#30340;&#22238;&#31572;&#19982;&#21738;&#20123;&#20154;&#30340;&#35266;&#28857;&#26356;&#20026;&#30456;&#20284;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;GlobalOpinionQA&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;&#36328;&#22269;&#35843;&#26597;&#30340;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#26088;&#22312;&#25429;&#25417;&#19981;&#21516;&#22269;&#23478;&#20851;&#20110;&#20840;&#29699;&#38382;&#39064;&#30340;&#22810;&#26679;&#35266;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#22269;&#23478;&#20026;&#26465;&#20214;&#65292;&#37327;&#21270;&#20102;LLM&#29983;&#25104;&#30340;&#35843;&#26597;&#22238;&#31572;&#19982;&#20154;&#31867;&#22238;&#31572;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#32463;&#36807;&#23466;&#27861;AI&#22521;&#35757;&#30340;LLM&#36827;&#34892;&#20102;&#19977;&#20010;&#23454;&#39564;&#65292;&#20998;&#21035;&#32771;&#34385;&#20854;&#24110;&#21161;&#24615;&#12289;&#35802;&#23454;&#24615;&#21644;&#26080;&#23475;&#24615;&#12290;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;LLM&#30340;&#22238;&#24212;&#26356;&#20542;&#21521;&#20110;&#19982;&#26576;&#20123;&#20154;&#32676;&#30340;&#35266;&#28857;&#26356;&#31867;&#20284;&#65292;&#20363;&#22914;&#26469;&#33258;&#32654;&#22269;&#12289;&#27431;&#27954;&#21644;&#21335;&#32654;&#27954;&#30340;&#20154;&#32676;&#65292;&#20984;&#26174;&#20102;&#28508;&#22312;&#30340;&#20559;&#35265;&#12290;&#24403;&#25105;&#20204;&#25552;&#31034;&#27169;&#22411;&#32771;&#34385;&#26576;&#20010;&#29305;&#23450;&#22269;&#23478;&#30340;&#35266;&#28857;&#26102;&#65292;&#22238;&#24212;&#20250;&#26356;&#21152;&#31867;&#20284;&#20110;&#35813;&#22269;&#23478;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) may not equitably represent diverse global perspectives on societal issues. In this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. We first build a dataset, GlobalOpinionQA, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. Next, we define a metric that quantifies the similarity between LLM-generated survey responses and human responses, conditioned on country. With our framework, we run three experiments on an LLM trained to be helpful, honest, and harmless with Constitutional AI. By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases. When we prompt the model to consider a particular country's perspective, responses shift to be more similar to the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#27169;&#24335;&#35774;&#32622;&#19979;&#65292;&#21482;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#36741;&#21161;&#26469;&#36827;&#34892;&#35821;&#27861;&#24402;&#32435;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;LLM&#30340;&#32431;&#25991;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.10564</link><description>&lt;p&gt;
&#26080;&#35270;&#35273;&#22522;&#32447;&#30340;&#22810;&#27169;&#24335;&#35821;&#27861;&#24402;&#32435;
&lt;/p&gt;
&lt;p&gt;
A Vision-free Baseline for Multimodal Grammar Induction. (arXiv:2212.10564v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#27169;&#24335;&#35774;&#32622;&#19979;&#65292;&#21482;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#36741;&#21161;&#26469;&#36827;&#34892;&#35821;&#27861;&#24402;&#32435;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;LLM&#30340;&#32431;&#25991;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37197;&#23545;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#20449;&#21495;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#65288;&#22914;MSCOCO&#65289;&#20013;&#30340;&#35821;&#27861;&#24402;&#32435;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21482;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#27169;&#24335;&#35774;&#32622;&#19979;&#26159;&#21542;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#36741;&#21161;&#26469;&#36827;&#34892;&#35821;&#27861;&#24402;&#32435;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#32431;&#25991;&#26412;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;LLM&#30340;C-PCFG&#65288;LC-PCFG&#65289;&#65292;&#22312;&#21508;&#31181;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#22810;&#27169;&#24335;&#26041;&#27861;&#65292;&#24182;&#19988;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#35821;&#27861;&#24402;&#32435;&#24615;&#33021;&#12290;&#19982;&#24102;&#22270;&#20687;&#30340;&#35821;&#27861;&#24402;&#32435;&#30456;&#27604;&#65292;LC-PCFG&#22312;&#35821;&#26009;&#24211;F1&#24471;&#20998;&#19978;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;7.9&#20010;&#28857;&#65292;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;85&#65285;&#65292;&#35757;&#32451;&#36895;&#24230;&#21152;&#24555;&#20102;1.7&#20493;&#12290;&#22312;&#19977;&#20010;&#36741;&#21161;&#35270;&#39057;&#30340;&#35821;&#27861;&#24402;&#32435;&#22522;&#20934;&#20013;&#65292;LC-PCFG&#22312;&#35821;&#26009;&#24211;F1&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26368;&#22810;7.7&#20010;&#28857;&#65292;&#35757;&#32451;&#36895;&#24230;&#21152;&#24555;&#20102;8.8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past work has shown that paired vision-language signals substantially improve grammar induction in multimodal datasets such as MSCOCO. We investigate whether advancements in large language models (LLMs) that are only trained with text could provide strong assistance for grammar induction in multimodal settings. We find that our text-only approach, an LLM-based C-PCFG (LC-PCFG), outperforms previous multi-modal methods, and achieves state-of-the-art grammar induction performance for various multimodal datasets. Compared to image-aided grammar induction, LC-PCFG outperforms the prior state-of-the-art by 7.9 Corpus-F1 points, with an 85% reduction in parameter count and 1.7x faster training speed. Across three video-assisted grammar induction benchmarks, LC-PCFG outperforms prior state-of-the-art by up to 7.7 Corpus-F1, with 8.8x faster training. These results shed light on the notion that text-only language models might include visually grounded cues that aid in grammar induction in mult
&lt;/p&gt;</description></item></channel></rss>