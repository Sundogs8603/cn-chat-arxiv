<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.03218</link><description>&lt;p&gt;
WMDP&#22522;&#20934;&#65306;&#36890;&#36807;&#36951;&#24536;&#27979;&#37327;&#21644;&#20943;&#23569;&#24694;&#24847;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03218
&lt;/p&gt;
&lt;p&gt;
WMDP&#22522;&#20934;&#26159;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#29992;&#20316;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#39046;&#22495; &#25688;&#35201;&#65306;&#30333;&#23467;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#34892;&#25919;&#21629;&#20196;&#24378;&#35843;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36171;&#20104;&#24694;&#24847;&#34892;&#20026;&#32773;&#24320;&#21457;&#29983;&#29289;&#12289;&#32593;&#32476;&#21644;&#21270;&#23398;&#27494;&#22120;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#34913;&#37327;&#36825;&#20123;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#65292;&#25919;&#24220;&#26426;&#26500;&#21644;&#20027;&#35201;&#20154;&#24037;&#26234;&#33021;&#23454;&#39564;&#23460;&#27491;&#22312;&#24320;&#21457;LLMs&#30340;&#21361;&#38505;&#33021;&#21147;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26159;&#31169;&#20154;&#30340;&#65292;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#22914;&#20309;&#20943;&#23569;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20165;&#19987;&#27880;&#20110;&#20960;&#26465;&#39640;&#24230;&#29305;&#23450;&#30340;&#24694;&#24847;&#20351;&#29992;&#36884;&#24452;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#22823;&#35268;&#27169;&#26432;&#20260;&#24615;&#27494;&#22120;&#20195;&#29702;&#65288;WMDP&#65289;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;4157&#20010;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#29983;&#29289;&#23433;&#20840;&#12289;&#32593;&#32476;&#23433;&#20840;&#21644;&#21270;&#23398;&#23433;&#20840;&#21361;&#38505;&#30693;&#35782;&#30340;&#20195;&#29702;&#27979;&#37327;&#12290;WMDP&#30001;&#19968;&#32452;&#23398;&#26415;&#30028;&#21644;&#25216;&#26415;&#39038;&#38382;&#32852;&#21512;&#24320;&#21457;&#65292;&#24182;&#22312;&#20844;&#24320;&#21457;&#24067;&#21069;&#20005;&#26684;&#36807;&#28388;&#20197;&#28040;&#38500;&#25935;&#24863;&#20449;&#24687;&#12290;WMDP&#26377;&#20004;&#20010;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03218v1 Announce Type: cross  Abstract: The White House Executive Order on Artificial Intelligence highlights the risks of large language models (LLMs) empowering malicious actors in developing biological, cyber, and chemical weapons. To measure these risks of malicious use, government institutions and major AI labs are developing evaluations for hazardous capabilities in LLMs. However, current evaluations are private, preventing further research into mitigating risk. Furthermore, they focus on only a few, highly specific pathways for malicious use. To fill these gaps, we publicly release the Weapons of Mass Destruction Proxy (WMDP) benchmark, a dataset of 4,157 multiple-choice questions that serve as a proxy measurement of hazardous knowledge in biosecurity, cybersecurity, and chemical security. WMDP was developed by a consortium of academics and technical consultants, and was stringently filtered to eliminate sensitive information prior to public release. WMDP serves two r
&lt;/p&gt;</description></item><item><title>MAGID&#26159;&#19968;&#20010;&#29992;&#20110;&#23558;&#20165;&#25991;&#26412;&#23545;&#35805;&#22686;&#24378;&#20026;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#21453;&#39304;&#24490;&#29615;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;</title><link>https://arxiv.org/abs/2403.03194</link><description>&lt;p&gt;
MAGID&#65306;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#30340;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
MAGID: An Automated Pipeline for Generating Synthetic Multi-modal Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03194
&lt;/p&gt;
&lt;p&gt;
MAGID&#26159;&#19968;&#20010;&#29992;&#20110;&#23558;&#20165;&#25991;&#26412;&#23545;&#35805;&#22686;&#24378;&#20026;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#21453;&#39304;&#24490;&#29615;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20132;&#20114;&#31995;&#32479;&#30340;&#21457;&#23637;&#21463;&#38480;&#20110;&#32570;&#20047;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#65288;&#25991;&#26412;&#12289;&#22270;&#20687;&#65289;&#23545;&#35805;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#23545;LLMs&#32780;&#35328;&#38656;&#35201;&#22823;&#37327;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#26816;&#32034;&#22270;&#20687;&#26469;&#22686;&#24378;&#25991;&#26412;&#23545;&#35805;&#65292;&#23384;&#22312;&#38544;&#31169;&#12289;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#31561;&#32422;&#26463;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MAGID&#65288;\textbf{M}ultimodal \textbf{A}ugmented \textbf{G}enerative \textbf{I}mages \textbf{D}ialogues&#65289;, &#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#29992;&#21508;&#31181;&#22810;&#26679;&#24615;&#21644;&#39640;&#36136;&#37327;&#22270;&#20687;&#22686;&#24378;&#20165;&#38480;&#20110;&#25991;&#26412;&#30340;&#23545;&#35805;&#12290;&#38543;&#21518;&#65292;&#24212;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#21019;&#24314;&#30456;&#24212;&#30340;&#22270;&#20687;&#65292;&#30830;&#20445;&#19982;&#30830;&#23450;&#30340;&#25991;&#26412;&#20445;&#25345;&#19968;&#33268;&#12290;&#26368;&#21518;&#65292;MAGID&#21253;&#21547;&#20102;&#19968;&#20010;&#21019;&#26032;&#24615;&#30340;&#21453;&#39304;&#24490;&#29615;&#65292;&#20171;&#20110;&#22270;&#20687;&#25551;&#36848;&#29983;&#25104;&#27169;&#22359;&#65288;&#25991;&#26412;LLM&#65289;&#21644;&#22270;&#20687;&#36136;&#37327;&#27169;&#22359;&#65288;&#35299;&#20915;&#32654;&#23398;&#12289;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#21644;&#23433;&#20840;&#24615;&#65289;&#65292;&#20108;&#32773;&#21327;&#20316;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#25105;&#20204;&#23558;MAGID&#19982;&#20854;&#20182;SOTA&#22522;&#32447;&#22312;&#19977;&#20010;&#23545;&#35805;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03194v1 Announce Type: new  Abstract: Development of multimodal interactive systems is hindered by the lack of rich, multimodal (text, images) conversational data, which is needed in large quantities for LLMs. Previous approaches augment textual dialogues with retrieved images, posing privacy, diversity, and quality constraints. In this work, we introduce \textbf{M}ultimodal \textbf{A}ugmented \textbf{G}enerative \textbf{I}mages \textbf{D}ialogues (MAGID), a framework to augment text-only dialogues with diverse and high-quality images. Subsequently, a diffusion model is applied to craft corresponding images, ensuring alignment with the identified text. Finally, MAGID incorporates an innovative feedback loop between an image description generation module (textual LLM) and image quality modules (addressing aesthetics, image-text matching, and safety), that work in tandem to generate high-quality and multi-modal dialogues. We compare MAGID to other SOTA baselines on three dialo
&lt;/p&gt;</description></item><item><title>&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#30340;&#21442;&#25968;&#21270;&#35821;&#35328;&#27169;&#22411;LMs&#65292;&#20316;&#32773;&#20027;&#24352;&#20351;&#29992;&#20855;&#26377;&#26816;&#32034;&#21151;&#33021;&#30340;LMs&#20316;&#20026;&#19979;&#19968;&#20195;LMs&#65292;&#20197;&#25552;&#39640;&#21487;&#38752;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#21487;&#36861;&#28335;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03187</link><description>&lt;p&gt;
&#20855;&#26377;&#26816;&#32034;&#21151;&#33021;&#30340;&#21487;&#38752;&#12289;&#36866;&#24212;&#24615;&#24378;&#19988;&#21487;&#36861;&#28335;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Reliable, Adaptable, and Attributable Language Models with Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03187
&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#38754;&#20020;&#25361;&#25112;&#30340;&#21442;&#25968;&#21270;&#35821;&#35328;&#27169;&#22411;LMs&#65292;&#20316;&#32773;&#20027;&#24352;&#20351;&#29992;&#20855;&#26377;&#26816;&#32034;&#21151;&#33021;&#30340;LMs&#20316;&#20026;&#19979;&#19968;&#20195;LMs&#65292;&#20197;&#25552;&#39640;&#21487;&#38752;&#24615;&#12289;&#36866;&#24212;&#24615;&#21644;&#21487;&#36861;&#28335;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#21270;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#28023;&#37327;&#32593;&#32476;&#25968;&#25454;&#19978;&#35757;&#32451;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#28789;&#27963;&#24615;&#21644;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#30528;&#24187;&#35273;&#12289;&#38590;&#20197;&#36866;&#24212;&#26032;&#25968;&#25454;&#20998;&#24067;&#21644;&#32570;&#20047;&#21487;&#39564;&#35777;&#24615;&#31561;&#23454;&#38469;&#25361;&#25112;&#12290;&#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20027;&#24352;&#29992;&#20855;&#22791;&#26816;&#32034;&#21151;&#33021;&#30340;LMs&#21462;&#20195;&#21442;&#25968;&#21270;LMs&#20316;&#20026;&#19979;&#19968;&#20195;LMs&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25972;&#21512;&#22823;&#35268;&#27169;&#25968;&#25454;&#23384;&#20648;&#65292;&#20855;&#26377;&#26816;&#32034;&#21151;&#33021;&#30340;LMs&#21487;&#20197;&#26356;&#21152;&#21487;&#38752;&#12289;&#36866;&#24212;&#24615;&#26356;&#24378;&#12289;&#21487;&#36861;&#28335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03187v1 Announce Type: cross  Abstract: Parametric language models (LMs), which are trained on vast amounts of web data, exhibit remarkable flexibility and capability. However, they still face practical challenges such as hallucinations, difficulty in adapting to new data distributions, and a lack of verifiability. In this position paper, we advocate for retrieval-augmented LMs to replace parametric LMs as the next generation of LMs. By incorporating large-scale datastores during inference, retrieval-augmented LMs can be more reliable, adaptable, and attributable. Despite their potential, retrieval-augmented LMs have yet to be widely adopted due to several obstacles: specifically, current retrieval-augmented LMs struggle to leverage helpful text beyond knowledge-intensive tasks such as question answering, have limited interaction between retrieval and LM components, and lack the infrastructure for scaling. To address these, we propose a roadmap for developing general-purpose
&lt;/p&gt;</description></item><item><title>SNIFFER&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#36229;&#20986;&#19978;&#19979;&#25991;&#35823;&#20256;&#26816;&#27979;&#21644;&#35299;&#37322;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#25351;&#23548;&#24494;&#35843;&#65292;&#22312;&#35299;&#37322;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;</title><link>https://arxiv.org/abs/2403.03170</link><description>&lt;p&gt;
SNIFFER: &#21487;&#35299;&#37322;&#30340;&#36328;&#25991;&#26412;&#20449;&#24687;&#35823;&#20256;&#26816;&#27979;&#30340;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03170
&lt;/p&gt;
&lt;p&gt;
SNIFFER&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#36229;&#20986;&#19978;&#19979;&#25991;&#35823;&#20256;&#26816;&#27979;&#21644;&#35299;&#37322;&#32780;&#35774;&#35745;&#30340;&#26032;&#22411;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#25351;&#23548;&#24494;&#35843;&#65292;&#22312;&#35299;&#37322;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#26159;&#19968;&#20010;&#26222;&#36941;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#30001;&#20110;&#28508;&#22312;&#30340;&#39640;&#39118;&#38505;&#12290;&#36229;&#20986;&#19978;&#19979;&#25991;&#65288;OOC&#65289;&#30340;&#35823;&#20256;&#65292;&#21363;&#30495;&#23454;&#22270;&#20687;&#34987;&#20266;&#36896;&#30340;&#25991;&#26412;&#20877;&#21033;&#29992;&#65292;&#26159;&#35823;&#23548;&#35266;&#20247;&#30340;&#26368;&#31616;&#21333;&#21644;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#35780;&#20272;&#22270;&#20687;-&#25991;&#26412;&#19968;&#33268;&#24615;&#65292;&#20294;&#32570;&#20047;&#35828;&#26381;&#21147;&#30340;&#35299;&#37322;&#26469;&#25903;&#25345;&#20182;&#20204;&#30340;&#21028;&#26029;&#65292;&#36825;&#23545;&#20110;&#25581;&#31034;&#35823;&#20256;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#20855;&#26377;&#20016;&#23500;&#30340;&#30693;&#35782;&#21644;&#20869;&#22312;&#30340;&#35270;&#35273;&#25512;&#29702;&#21644;&#35299;&#37322;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#29702;&#35299;&#21644;&#21457;&#29616;&#24494;&#22937;&#30340;&#36328;&#27169;&#24577;&#24046;&#24322;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SNIFFER&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;OOC&#35823;&#20256;&#26816;&#27979;&#21644;&#35299;&#37322;&#32780;&#35774;&#35745;&#30340;&#26032;&#39062;&#30340;&#22810;&#27169;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;SNIFFER&#22312;InstructBLIP&#19978;&#37319;&#29992;&#20102;&#20004;&#38454;&#27573;&#30340;&#25351;&#23548;&#24494;&#35843;&#12290;&#31532;&#19968;&#38454;&#27573;&#36890;&#36807;&#26032;&#38395;&#39046;&#22495;&#23454;&#20307;&#19982;&#36890;&#29992;&#23545;&#35937;&#30340;&#27169;&#22411;&#27010;&#24565;&#23545;&#40784;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03170v1 Announce Type: cross  Abstract: Misinformation is a prevalent societal issue due to its potential high risks. Out-of-context (OOC) misinformation, where authentic images are repurposed with false text, is one of the easiest and most effective ways to mislead audiences. Current methods focus on assessing image-text consistency but lack convincing explanations for their judgments, which is essential for debunking misinformation. While Multimodal Large Language Models (MLLMs) have rich knowledge and innate capability for visual reasoning and explanation generation, they still lack sophistication in understanding and discovering the subtle crossmodal differences. In this paper, we introduce SNIFFER, a novel multimodal large language model specifically engineered for OOC misinformation detection and explanation. SNIFFER employs two-stage instruction tuning on InstructBLIP. The first stage refines the model's concept alignment of generic objects with news-domain entities a
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PARADISE&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#23454;&#29992;&#31243;&#24207;&#25991;&#26412;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20165;&#20174;&#32473;&#23450;&#30446;&#26631;&#25512;&#26029;&#35745;&#21010;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.03167</link><description>&lt;p&gt;
PARADISE&#65306;&#36890;&#36807;&#36807;&#31243;&#35686;&#21578;&#21644;&#25552;&#31034;&#25968;&#25454;&#38598;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#24335;&#35268;&#21010;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03167
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PARADISE&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#23454;&#29992;&#31243;&#24207;&#25991;&#26412;&#36827;&#34892;&#28436;&#32462;&#25512;&#29702;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20165;&#20174;&#32473;&#23450;&#30446;&#26631;&#25512;&#26029;&#35745;&#21010;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31038;&#21306;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#22791;&#35268;&#21010;&#25110;&#25191;&#34892;&#35745;&#21010;&#30340;&#33021;&#21147;&#36234;&#21457;&#24863;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30740;&#31350;&#20351;&#29992;LLMs&#20026;&#31616;&#21270;&#22330;&#26223;&#29983;&#25104;&#39640;&#32423;&#35745;&#21010;&#65292;&#36866;&#24212;&#24230;&#37327;&#32570;&#20047;&#35821;&#35328;&#22797;&#26434;&#24615;&#21644;&#39046;&#22495;&#22810;&#26679;&#24615;&#65292;&#38480;&#21046;&#20854;&#35268;&#21010;&#33021;&#21147;&#30340;&#20998;&#26512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PARADISE&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;Q&#65286;A&#26684;&#24335;&#30340;&#28436;&#32462;&#25512;&#29702;&#20219;&#21153;&#65292;&#37319;&#29992;&#26469;&#33258;wikiHow&#30340;&#23454;&#29992;&#31243;&#24207;&#25991;&#26412;&#12290;&#23427;&#28041;&#21450;&#19982;&#30446;&#26631;&#30452;&#25509;&#30456;&#20851;&#30340;&#35686;&#21578;&#21644;&#25552;&#31034;&#25512;&#26029;&#20219;&#21153;&#65292;&#25490;&#38500;&#20013;&#38388;&#27493;&#39588;&#65292;&#26088;&#22312;&#27979;&#35797;&#27169;&#22411;&#20165;&#20174;&#32473;&#23450;&#30446;&#26631;&#25512;&#26029;&#35745;&#21010;&#30340;&#38544;&#21547;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03167v1 Announce Type: new  Abstract: Recently, there has been growing interest within the community regarding whether large language models are capable of planning or executing plans. However, most prior studies use LLMs to generate high-level plans for simplified scenarios lacking linguistic complexity and domain diversity, limiting analysis of their planning abilities. These setups constrain evaluation methods (e.g., predefined action space), architectural choices (e.g., only generative models), and overlook the linguistic nuances essential for realistic analysis. To tackle this, we present PARADISE, an abductive reasoning task using Q\&amp;A format on practical procedural text sourced from wikiHow. It involves warning and tip inference tasks directly associated with goals, excluding intermediary steps, with the aim of testing the ability of the models to infer implicit knowledge of the plan solely from the given goal. Our experiments, utilizing fine-tuned language models and
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;Design2Code&#20219;&#21153;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;LLMs&#30452;&#25509;&#23558;&#35270;&#35273;&#35774;&#35745;&#36716;&#25442;&#20026;&#20195;&#30721;&#23454;&#29616;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03163</link><description>&lt;p&gt;
Design2Code&#65306;&#25105;&#20204;&#31163;&#33258;&#21160;&#21270;&#21069;&#31471;&#24037;&#31243;&#26377;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
Design2Code: How Far Are We From Automating Front-End Engineering?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03163
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#25552;&#20986;&#20102;Design2Code&#20219;&#21153;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;LLMs&#30452;&#25509;&#23558;&#35270;&#35273;&#35774;&#35745;&#36716;&#25442;&#20026;&#20195;&#30721;&#23454;&#29616;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#39134;&#29467;&#36827;&#30340;&#36827;&#23637;&#65292;&#23454;&#29616;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#36825;&#21487;&#20197;&#23454;&#29616;&#19968;&#31181;&#26032;&#30340;&#21069;&#31471;&#24320;&#21457;&#33539;&#24335;&#65292;&#20854;&#20013;&#22810;&#27169;&#24577;LLMs&#21487;&#33021;&#30452;&#25509;&#23558;&#35270;&#35273;&#35774;&#35745;&#36716;&#25442;&#20026;&#20195;&#30721;&#23454;&#29616;&#12290;&#26412;&#25991;&#23558;&#36825;&#19968;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;Design2Code&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20840;&#38754;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#25163;&#21160;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;484&#20010;&#22810;&#26679;&#21270;&#30495;&#23454;&#32593;&#39029;&#30340;&#22522;&#20934;&#27979;&#35797;&#29992;&#20363;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#22871;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35780;&#20272;&#24403;&#21069;&#22810;&#27169;&#24577;LLMs&#33021;&#21542;&#29983;&#25104;&#30452;&#25509;&#28210;&#26579;&#20026;&#32473;&#23450;&#21442;&#32771;&#32593;&#39029;&#30340;&#20195;&#30721;&#23454;&#29616;&#65292;&#20197;&#36755;&#20837;&#20026;&#23631;&#24149;&#25130;&#22270;&#12290;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#20840;&#38754;&#30340;&#20154;&#24037;&#35780;&#20272;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#22810;&#27169;&#24577;&#25552;&#31034;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;GPT-4V&#21644;Gemini Pro Vision&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#19968;&#20010;&#24320;&#28304;&#30340;Design2Code-18B&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03163v1 Announce Type: new  Abstract: Generative AI has made rapid advancements in recent years, achieving unprecedented capabilities in multimodal understanding and code generation. This can enable a new paradigm of front-end development, in which multimodal LLMs might directly convert visual designs into code implementations. In this work, we formalize this as a Design2Code task and conduct comprehensive benchmarking. Specifically, we manually curate a benchmark of 484 diverse real-world webpages as test cases and develop a set of automatic evaluation metrics to assess how well current multimodal LLMs can generate the code implementations that directly render into the given reference webpages, given the screenshots as input. We also complement automatic metrics with comprehensive human evaluations. We develop a suite of multimodal prompting methods and show their effectiveness on GPT-4V and Gemini Pro Vision. We further finetune an open-source Design2Code-18B model that su
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#35821;&#35328;&#24341;&#23548;&#25506;&#32034;&#65288;LGE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;RL&#20195;&#29702;&#25552;&#20379;&#20915;&#31574;&#32423;&#21035;&#30340;&#24341;&#23548;&#65292;&#30456;&#27604;&#20110;&#26222;&#36890;RL&#20195;&#29702;&#21644;&#20854;&#20182;&#22797;&#26434;&#26041;&#27861;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25991;&#26412;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.03141</link><description>&lt;p&gt;
&#25991;&#26412;&#29615;&#22659;&#20013;RL&#20195;&#29702;&#30340;&#35821;&#35328;&#24341;&#23548;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Language Guided Exploration for RL Agents in Text Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03141
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#35821;&#35328;&#24341;&#23548;&#25506;&#32034;&#65288;LGE&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;RL&#20195;&#29702;&#25552;&#20379;&#20915;&#31574;&#32423;&#21035;&#30340;&#24341;&#23548;&#65292;&#30456;&#27604;&#20110;&#26222;&#36890;RL&#20195;&#29702;&#21644;&#20854;&#20182;&#22797;&#26434;&#26041;&#27861;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25991;&#26412;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#24207;&#36143;&#20915;&#31574;&#20197;&#31232;&#30095;&#22870;&#21169;&#21644;&#24222;&#22823;&#30340;&#20915;&#31574;&#31354;&#38388;&#20026;&#29305;&#24449;&#65292;&#36825;&#23545;tabula rasa&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#31561;&#32463;&#39564;&#23398;&#20064;&#31995;&#32479;&#25552;&#20986;&#20102;&#37325;&#35201;&#22256;&#38590;&#12290;&#20855;&#26377;&#20016;&#23500;&#19990;&#30028;&#30693;&#35782;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#24110;&#21161;RL&#20195;&#29702;&#24555;&#36895;&#23398;&#20064;&#24182;&#36866;&#24212;&#20998;&#24067;&#21464;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#25351;&#23548;&#25506;&#32034;&#65288;LGE&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;&#31216;&#20026;GUIDE&#65289;&#20026;RL&#20195;&#29702;&#65288;&#31216;&#20026;EXPLORER&#65289;&#25552;&#20379;&#20915;&#31574;&#32423;&#21035;&#30340;&#25351;&#23548;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;ScienceWorld&#65288;Wang&#31561;&#20154;&#65292;2022&#24180;&#65289;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25991;&#26412;&#29615;&#22659;&#20013;&#65292;LGE&#26174;&#33879;&#20248;&#20110;&#26222;&#36890;RL&#20195;&#29702;&#65292;&#24182;&#19988;&#20248;&#20110;&#34892;&#20026;&#20811;&#38534;&#21644;&#25991;&#26412;&#20915;&#31574;Transformer&#31561;&#20854;&#20182;&#22797;&#26434;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03141v1 Announce Type: new  Abstract: Real-world sequential decision making is characterized by sparse rewards and large decision spaces, posing significant difficulty for experiential learning systems like $\textit{tabula rasa}$ reinforcement learning (RL) agents. Large Language Models (LLMs), with a wealth of world knowledge, can help RL agents learn quickly and adapt to distribution shifts. In this work, we introduce Language Guided Exploration (LGE) framework, which uses a pre-trained language model (called GUIDE ) to provide decision-level guidance to an RL agent (called EXPLORER). We observe that on ScienceWorld (Wang et al.,2022), a challenging text environment, LGE outperforms vanilla RL agents significantly and also outperforms other sophisticated methods like Behaviour Cloning and Text Decision Transformer.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#20316;&#29983;&#25104;&#26694;&#26550;CoGenesis&#65292;&#25972;&#21512;&#22823;&#22411;&#21644;&#23567;&#22411;&#27169;&#22411;&#65292;&#20197;&#36923;&#36753;&#26041;&#24335;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03129</link><description>&lt;p&gt;
CoGenesis&#65306;&#19968;&#20010;&#21327;&#20316;&#22823;&#22411;&#21644;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23433;&#20840;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#25351;&#20196;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03129
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21327;&#20316;&#29983;&#25104;&#26694;&#26550;CoGenesis&#65292;&#25972;&#21512;&#22823;&#22411;&#21644;&#23567;&#22411;&#27169;&#22411;&#65292;&#20197;&#36923;&#36753;&#26041;&#24335;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#21457;&#23637;&#65292;&#23427;&#20204;&#25509;&#35302;&#31169;&#20154;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#36234;&#26469;&#36234;&#19981;&#21487;&#36991;&#20813;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#65288;&#23588;&#20854;&#26159;&#36739;&#23567;&#30340;&#27169;&#22411;&#65289;&#22312;&#20010;&#20154;&#35774;&#22791;&#19978;&#65292;&#22914;PC&#21644;&#26234;&#33021;&#25163;&#26426;&#19978;&#65292;&#24050;&#25104;&#20026;&#19968;&#31181;&#30427;&#34892;&#36235;&#21183;&#12290;&#22312;&#20805;&#28385;&#29992;&#25143;&#20449;&#24687;&#30340;&#29615;&#22659;&#20013;&#65292;&#20351;&#27169;&#22411;&#26082;&#33021;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#21448;&#33021;&#39640;&#25928;&#25191;&#34892;&#21629;&#20196;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CoGenesis&#65292;&#19968;&#20010;&#21327;&#20316;&#29983;&#25104;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#22823;&#22411;&#27169;&#22411;&#65288;&#25176;&#31649;&#22312;&#20113;&#22522;&#30784;&#35774;&#26045;&#19978;&#65289;&#21644;&#23567;&#22411;&#27169;&#22411;&#65288;&#37096;&#32626;&#22312;&#26412;&#22320;&#35774;&#22791;&#19978;&#65289;&#65292;&#20197;&#36923;&#36753;&#26041;&#24335;&#35299;&#20915;&#38544;&#31169;&#38382;&#39064;&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31649;&#36947;&#26469;&#21019;&#24314;&#20010;&#24615;&#21270;&#30340;&#20889;&#20316;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#32454;&#33410;&#65292;&#20316;&#20026;&#36825;&#19968;&#30740;&#31350;&#38382;&#39064;&#30340;&#27979;&#35797;&#22522;&#30784;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#33609;&#22270;&#21644;&#23545;&#25968;&#30340;&#20004;&#20010;CoGenesis&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#22522;&#20110;&#25105;&#20204;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#21478;&#22806;&#20004;&#20010;op
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03129v1 Announce Type: new  Abstract: With the advancement of language models (LMs), their exposure to private data is increasingly inevitable, and their deployment (especially for smaller ones) on personal devices, such as PCs and smartphones, has become a prevailing trend. In contexts laden with user information, enabling models to both safeguard user privacy and execute commands efficiently emerges as an essential research imperative. In this paper, we propose CoGenesis, a collaborative generation framework integrating large (hosted on cloud infrastructure) and small models (deployed on local devices) to address privacy concerns logically. Initially, we design a pipeline to create personalized writing instruction datasets enriched with extensive context details as the testbed of this research issue. Subsequently, we introduce two variants of CoGenesis based on sketch and logits respectively. Our experimental findings, based on our synthesized dataset and two additional op
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#24615;&#21035;&#21270;&#24773;&#32490;&#24402;&#22240;&#65292;&#21453;&#26144;&#20102;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.03121</link><description>&lt;p&gt;
&#24868;&#24594;&#30340;&#30007;&#24615;&#65292;&#24754;&#20260;&#30340;&#22899;&#24615;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24773;&#32490;&#24402;&#22240;&#20013;&#21453;&#26144;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03121
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#24615;&#21035;&#21270;&#24773;&#32490;&#24402;&#22240;&#65292;&#21453;&#26144;&#20102;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21453;&#26144;&#31038;&#20250;&#35268;&#33539;&#21644;&#20559;&#35265;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#24615;&#21035;&#30340;&#12290;&#34429;&#28982;&#31038;&#20250;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#24773;&#32490;&#20998;&#26512;&#26041;&#38754;&#23384;&#22312;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#31354;&#30333;&#12290;&#28982;&#32780;&#65292;&#22312;&#31038;&#20250;&#35805;&#35821;&#20013;&#65292;&#24773;&#32490;&#21644;&#24615;&#21035;&#23494;&#20999;&#30456;&#20851;&#12290;&#20363;&#22914;&#65292;&#22899;&#24615;&#32463;&#24120;&#34987;&#35748;&#20026;&#26356;&#20855;&#31227;&#24773;&#33021;&#21147;&#65292;&#32780;&#30007;&#24615;&#30340;&#24868;&#24594;&#26356;&#21463;&#31038;&#20250;&#25509;&#21463;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20116;&#31181;&#26368;&#20808;&#36827;&#30340;LLMs&#65288;&#24320;&#28304;&#21644;&#23553;&#38381;&#28304;&#65289;&#36827;&#34892;&#24615;&#21035;&#21270;&#24773;&#32490;&#24402;&#22240;&#30340;&#39318;&#27425;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#35843;&#26597;&#24773;&#32490;&#26159;&#21542;&#20855;&#26377;&#24615;&#21035;&#29305;&#24449;&#65292;&#20197;&#21450;&#36825;&#20123;&#21464;&#21270;&#26159;&#21542;&#22522;&#20110;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#25552;&#31034;&#27169;&#22411;&#37319;&#29992;&#24615;&#21035;&#21270;&#35282;&#33394;&#24182;&#23558;&#24773;&#32490;&#24402;&#22240;&#20110;&#31867;&#20284;&#8220;&#24403;&#25105;&#19982;&#20146;&#36817;&#30340;&#20154;&#21457;&#29983;&#20005;&#37325;&#20105;&#25191;&#8221;&#36825;&#26679;&#30340;&#20107;&#20214;&#12290;&#28982;&#21518;&#25105;&#20204;&#20998;&#26512;&#27169;&#22411;&#29983;&#25104;&#30340;&#24773;&#32490;&#19982;&#24615;&#21035;-&#20107;&#20214;&#23545;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#25152;&#26377;&#27169;&#22411;&#19968;&#33268;&#23637;&#29616;&#20986;&#24615;&#21035;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03121v1 Announce Type: new  Abstract: Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men's anger is more socially accepted. To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We prompt the models to adopt a gendered persona and attribute emotions to an event like 'When I had a serious argument with a dear person'. We then analyze the emotions generated by the models in relation to the gender-event pairs. We find that all models consistently exhibit gendered e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;In-Dialogue Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35805;&#21382;&#21490;&#21051;&#30011;&#20010;&#20154;&#35774;&#26469;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#36827;&#23545;&#35805;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03102</link><description>&lt;p&gt;
&#8220;&#22312;&#23545;&#35805;&#20013;&#23398;&#20064;&#8221;&#65306;&#36890;&#36807;&#23545;&#35805;&#20013;&#23398;&#20064;&#23454;&#29616;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#30340;&#20010;&#24615;&#21270;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
"In Dialogues We Learn": Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03102
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;In-Dialogue Learning&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#35805;&#21382;&#21490;&#21051;&#30011;&#20010;&#20154;&#35774;&#26469;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#36827;&#23545;&#35805;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#23545;&#35805;&#31995;&#32479;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#22240;&#20854;&#33021;&#22815;&#29983;&#25104;&#19982;&#19981;&#21516;&#20154;&#35774;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#39044;&#23450;&#20041;&#30340;&#20010;&#20154;&#36164;&#26009;&#65292;&#36825;&#19981;&#20165;&#32791;&#26102;&#19988;&#21171;&#21160;&#23494;&#38598;&#65292;&#36824;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;In-Dialogue Learning&#65288;IDL&#65289;&#65292;&#19968;&#31181;&#24494;&#35843;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#23545;&#35805;&#21382;&#21490;&#26469;&#21051;&#30011;&#20010;&#20154;&#35774;&#65292;&#20197;&#23436;&#25104;&#20010;&#24615;&#21270;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39044;&#23450;&#20041;&#20010;&#20154;&#36164;&#26009;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;IDL&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;BLEU&#21644;ROUGE&#20998;&#25968;&#20998;&#21035;&#22686;&#21152;&#20102;&#39640;&#36798;200%&#21644;247%&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#35780;&#20272;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03102v1 Announce Type: cross  Abstract: Personalized dialogue systems have gained significant attention in recent years for their ability to generate responses in alignment with different personas. However, most existing approaches rely on pre-defined personal profiles, which are not only time-consuming and labor-intensive to create but also lack flexibility. We propose In-Dialogue Learning (IDL), a fine-tuning framework that enhances the ability of pre-trained large language models to leverage dialogue history to characterize persona for completing personalized dialogue generation tasks without pre-defined profiles. Our experiments on three datasets demonstrate that IDL brings substantial improvements, with BLEU and ROUGE scores increasing by up to 200% and 247%, respectively. Additionally, the results of human evaluations further validate the efficacy of our proposed method.
&lt;/p&gt;</description></item><item><title>KnowAgent&#24341;&#20837;&#20102;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#65292;&#36890;&#36807;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#22686;&#24378;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#35821;&#35328;Agent&#30340;&#35268;&#21010;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.03101</link><description>&lt;p&gt;
KnowAgent: &#30693;&#35782;&#22686;&#24378;&#35268;&#21010;&#29992;&#20110;&#22522;&#20110;LLM&#30340;Agent
&lt;/p&gt;
&lt;p&gt;
KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03101
&lt;/p&gt;
&lt;p&gt;
KnowAgent&#24341;&#20837;&#20102;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#65292;&#36890;&#36807;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#22686;&#24378;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#35821;&#35328;Agent&#30340;&#35268;&#21010;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#25361;&#25112;&#26102;&#20173;&#26377;&#25152;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#19982;&#29615;&#22659;&#20114;&#21160;&#36890;&#36807;&#29983;&#25104;&#21487;&#25191;&#34892;&#21160;&#20316;&#26102;&#12290;&#36825;&#31181;&#19981;&#36275;&#20027;&#35201;&#26469;&#33258;&#20110;&#35821;&#35328;Agent&#20013;&#32570;&#20047;&#20869;&#32622;&#21160;&#20316;&#30693;&#35782;&#65292;&#23548;&#33268;&#22312;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#26080;&#27861;&#26377;&#25928;&#24341;&#23548;&#35268;&#21010;&#36712;&#36857;&#65292;&#20174;&#32780;&#23548;&#33268;&#35268;&#21010;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KnowAgent&#65292;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#26469;&#22686;&#24378;LLM&#35268;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;KnowAgent&#37319;&#29992;&#20102;&#19968;&#20010;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#19968;&#20010;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#38480;&#21046;&#35268;&#21010;&#36807;&#31243;&#20013;&#30340;&#34892;&#21160;&#36335;&#24452;&#65292;&#23454;&#29616;&#26356;&#21512;&#29702;&#30340;&#36712;&#36857;&#21512;&#25104;&#65292;&#36827;&#32780;&#25552;&#39640;&#35821;&#35328;Agent&#30340;&#35745;&#21010;&#24615;&#33021;&#12290;&#22522;&#20110;HotpotQA&#21644;ALFWorld&#30340;&#23454;&#39564;&#32467;&#26524;&#22522;&#20110;&#19981;&#21516;&#30340;&#20027;&#24178;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03101v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone m
&lt;/p&gt;</description></item><item><title>NaturalSpeech 3&#21033;&#29992;&#20998;&#35299;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;</title><link>https://arxiv.org/abs/2403.03100</link><description>&lt;p&gt;
NaturalSpeech 3: &#21033;&#29992;&#20998;&#35299;&#32534;&#35299;&#30721;&#22120;&#21644;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03100
&lt;/p&gt;
&lt;p&gt;
NaturalSpeech 3&#21033;&#29992;&#20998;&#35299;&#35774;&#35745;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#22312;&#35821;&#38899;&#36136;&#37327;&#12289;&#30456;&#20284;&#24230;&#21644;&#38901;&#24459;&#26041;&#38754;&#20173;&#23384;&#22312;&#19981;&#36275;&#12290;&#37492;&#20110;&#35821;&#38899;&#22797;&#26434;&#22320;&#21253;&#21547;&#21508;&#31181;&#23646;&#24615;&#65288;&#20363;&#22914;&#20869;&#23481;&#12289;&#38901;&#24459;&#12289;&#38899;&#33394;&#21644;&#22768;&#23398;&#32454;&#33410;&#65289;&#65292;&#32473;&#29983;&#25104;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#24819;&#27861;&#26159;&#23558;&#35821;&#38899;&#22240;&#23376;&#20998;&#35299;&#20026;&#20195;&#34920;&#19981;&#21516;&#23646;&#24615;&#30340;&#21508;&#20010;&#23376;&#31354;&#38388;&#65292;&#24182;&#21333;&#29420;&#29983;&#25104;&#23427;&#20204;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NaturalSpeech 3&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#26032;&#39062;&#30340;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#30340;TTS&#31995;&#32479;&#65292;&#21487;&#20197;&#20197;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#33258;&#28982;&#35821;&#38899;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;1) &#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;&#20998;&#35299;&#21521;&#37327;&#37327;&#21270;&#65288;FVQ&#65289;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#65292;&#23558;&#35821;&#38899;&#27874;&#24418;&#20998;&#35299;&#20026;&#20869;&#23481;&#12289;&#38901;&#24459;&#12289;&#38899;&#33394;&#21644;&#22768;&#23398;&#32454;&#33410;&#30340;&#23376;&#31354;&#38388;&#65307;2) &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#35299;&#25193;&#25955;&#27169;&#22411;&#65292;&#26681;&#25454;&#20854;&#30456;&#24212;&#30340;&#25552;&#31034;&#29983;&#25104;&#27599;&#20010;&#23376;&#31354;&#38388;&#20013;&#30340;&#23646;&#24615;&#12290;&#20511;&#21161;&#36825;&#31181;&#20998;&#35299;&#35774;&#35745;&#65292;NaturalSpeech 3&#33021;&#22815;ef
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03100v1 Announce Type: cross  Abstract: While recent large-scale text-to-speech (TTS) models have achieved significant progress, they still fall short in speech quality, similarity, and prosody. Considering speech intricately encompasses various attributes (e.g., content, prosody, timbre, and acoustic details) that pose significant challenges for generation, a natural idea is to factorize speech into individual subspaces representing different attributes and generate them individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with novel factorized diffusion models to generate natural speech in a zero-shot way. Specifically, 1) we design a neural codec with factorized vector quantization (FVQ) to disentangle speech waveform into subspaces of content, prosody, timbre, and acoustic details; 2) we propose a factorized diffusion model to generate attributes in each subspace following its corresponding prompt. With this factorization design, NaturalSpeech 3 can ef
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#26032;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#21644;&#36873;&#25321;&#22312;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#20013;&#20855;&#20307;&#30340;&#35270;&#35273;&#30456;&#20851;&#20196;&#29260;&#65292;&#36890;&#36807;&#36825;&#20123;&#26041;&#27861;&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#21644;&#26356;&#22909;&#30340;&#35270;&#35273;&#19978;&#19979;&#25991;&#21033;&#29992;</title><link>https://arxiv.org/abs/2403.03075</link><description>&lt;p&gt;
&#26816;&#27979;&#20855;&#20307;&#35270;&#35273;&#20196;&#29260;&#20197;&#36827;&#34892;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Detecting Concrete Visual Tokens for Multimodal Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03075
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#26032;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#21644;&#36873;&#25321;&#22312;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#20013;&#20855;&#20307;&#30340;&#35270;&#35273;&#30456;&#20851;&#20196;&#29260;&#65292;&#36890;&#36807;&#36825;&#20123;&#26041;&#27861;&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#21644;&#26356;&#22909;&#30340;&#35270;&#35273;&#19978;&#19979;&#25991;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#65288;MMT&#65289;&#31995;&#32479;&#20013;&#65292;&#35270;&#35273;&#22522;&#30784;&#21644;&#36974;&#34109;&#30340;&#25361;&#25112;&#20419;&#20351;&#20102;&#23545;&#26816;&#27979;&#21644;&#36873;&#25321;&#29992;&#20110;&#36974;&#34109;&#30340;&#35270;&#35273;&#22522;&#30784;&#25991;&#26412;&#20196;&#29260;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#20174;&#28304;&#21477;&#20013;&#26816;&#27979;&#35270;&#35273;&#19978;&#21644;&#35821;&#22659;&#19978;&#30456;&#20851;&#65288;&#20855;&#20307;&#65289;&#20196;&#29260;&#30340;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26816;&#27979;&#65292;&#29289;&#20307;&#26816;&#27979;&#26816;&#27979;&#21644;&#32852;&#21512;&#26816;&#27979;-&#39564;&#35777;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#29992;&#20110;&#36873;&#25321;&#26816;&#27979;&#21040;&#30340;&#20196;&#29260;&#30340;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;&#26368;&#30701;$n$ &#20010;&#20196;&#29260;&#12289;&#26368;&#38271;$n$ &#20010;&#20196;&#29260;&#20197;&#21450;&#25152;&#26377;&#26816;&#27979;&#21040;&#30340;&#20855;&#20307;&#20196;&#29260;&#12290;&#25105;&#20204;&#21033;&#29992;GRAM MMT &#26550;&#26500;&#38024;&#23545;&#21512;&#25104;&#25972;&#29702;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#21363;&#28304;&#22270;&#20687;&#19982;&#36974;&#34109;&#21477;&#23376;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#34920;&#29616;&#20986;&#30456;&#27604;&#22522;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#25913;&#36827;&#21644;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#23545;&#35270;&#35273;&#19978;&#19979;&#25991;&#30340;&#26356;&#22909;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03075v1 Announce Type: new  Abstract: The challenge of visual grounding and masking in multimodal machine translation (MMT) systems has encouraged varying approaches to the detection and selection of visually-grounded text tokens for masking. We introduce new methods for detection of visually and contextually relevant (concrete) tokens from source sentences, including detection with natural language processing (NLP), detection with object detection, and a joint detection-verification technique. We also introduce new methods for selection of detected tokens, including shortest $n$ tokens, longest $n$ tokens, and all detected concrete tokens. We utilize the GRAM MMT architecture to train models against synthetically collated multimodal datasets of source images with masked sentences, showing performance improvements and improved usage of visual context during translation tasks over the baseline model.
&lt;/p&gt;</description></item><item><title>&#23558;&#24615;&#33021;&#20248;&#24322;&#30340;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#36716;&#21270;&#20026;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#36890;&#36807;&#36830;&#25509;&#35270;&#35273;-&#25991;&#26412;&#36866;&#37197;&#22120;&#23618;&#24182;&#21033;&#29992;&#38376;&#25511;&#26426;&#21046;&#65292;&#22312;Multi30k&#25968;&#25454;&#38598;&#21644;&#20856;&#22411;&#30340;&#20165;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.03045</link><description>&lt;p&gt;
&#23558;&#22810;&#27169;&#24577;&#21151;&#33021;&#28155;&#21152;&#21040;&#20165;&#25991;&#26412;&#32763;&#35793;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Adding Multimodal Capabilities to a Text-only Translation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03045
&lt;/p&gt;
&lt;p&gt;
&#23558;&#24615;&#33021;&#20248;&#24322;&#30340;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#36716;&#21270;&#20026;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#36890;&#36807;&#36830;&#25509;&#35270;&#35273;-&#25991;&#26412;&#36866;&#37197;&#22120;&#23618;&#24182;&#21033;&#29992;&#38376;&#25511;&#26426;&#21046;&#65292;&#22312;Multi30k&#25968;&#25454;&#38598;&#21644;&#20856;&#22411;&#30340;&#20165;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#65288;MMT&#65289;&#24037;&#20316;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#20351;&#29992;Multi30k&#25968;&#25454;&#38598;&#65292;&#28982;&#32780;&#25105;&#20204;&#21457;&#29616;&#23548;&#33268;&#30340;&#27169;&#22411;&#20005;&#37325;&#36807;&#25311;&#21512;Multi30k&#25968;&#25454;&#38598;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20856;&#22411;&#30340;&#20165;&#25991;&#26412;&#27979;&#35797;&#38598;&#65288;&#22914;WMT&#26032;&#38395;&#27979;&#35797;&#25968;&#25454;&#38598;&#65289;&#19978;&#34920;&#29616;&#38750;&#24120;&#31967;&#31957;&#12290;&#20026;&#20102;&#22312;Multi30k&#21644;&#20856;&#22411;&#30340;&#20165;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24615;&#33021;&#20248;&#24322;&#30340;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#27169;&#22411;&#20316;&#20026;&#25105;&#20204;MMT&#27169;&#22411;&#30340;&#36215;&#28857;&#12290;&#25105;&#20204;&#36890;&#36807;&#36830;&#25509;&#38376;&#25511;&#26426;&#21046;&#30340;&#35270;&#35273;-&#25991;&#26412;&#36866;&#37197;&#22120;&#23618;&#23558;MT&#27169;&#22411;&#36880;&#27493;&#36716;&#21270;&#20026;MMT&#27169;&#22411;&#65292;&#26041;&#27861;&#26159;&#65306;1&#65289;&#20351;&#29992;&#22522;&#20110;&#35270;&#35273;&#30340;&#28304;&#25991;&#26412;&#25513;&#34109;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;2&#65289;&#22312;Multi30k&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03045v1 Announce Type: new  Abstract: While most current work in multimodal machine translation (MMT) uses the Multi30k dataset for training and evaluation, we find that the resulting models overfit to the Multi30k dataset to an extreme degree. Consequently, these models perform very badly when evaluated against typical text-only testing sets such as the WMT newstest datasets. In order to perform well on both Multi30k and typical text-only datasets, we use a performant text-only machine translation (MT) model as the starting point of our MMT model. We add vision-text adapter layers connected via gating mechanisms to the MT model, and incrementally transform the MT model into an MMT model by 1) pre-training using vision-based masking of the source text and 2) fine-tuning on Multi30k.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;ConAgents&#26694;&#26550;&#65292;&#36890;&#36807;&#21512;&#20316;&#21644;&#20114;&#21160;&#20195;&#29702;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#20351;&#29992;&#24037;&#20855;&#65292;&#24182;&#24341;&#20837;&#20102;&#36845;&#20195;&#26657;&#20934;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21333;&#20010;&#20195;&#29702;&#25191;&#34892;&#22810;&#26679;&#21270;&#25805;&#20316;&#33021;&#21147;&#26377;&#38480;&#21644;&#33258;&#36866;&#24212;&#32416;&#27491;&#38169;&#35823;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03031</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#20316;&#21644;&#20114;&#21160;&#20195;&#29702;&#23398;&#20064;&#20351;&#29992;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Learning to Use Tools via Cooperative and Interactive Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03031
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;ConAgents&#26694;&#26550;&#65292;&#36890;&#36807;&#21512;&#20316;&#21644;&#20114;&#21160;&#20195;&#29702;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#20351;&#29992;&#24037;&#20855;&#65292;&#24182;&#24341;&#20837;&#20102;&#36845;&#20195;&#26657;&#20934;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21333;&#20010;&#20195;&#29702;&#25191;&#34892;&#22810;&#26679;&#21270;&#25805;&#20316;&#33021;&#21147;&#26377;&#38480;&#21644;&#33258;&#36866;&#24212;&#32416;&#27491;&#38169;&#35823;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#23398;&#20064;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20195;&#29702;&#20154;&#33021;&#22815;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#26469;&#25193;&#23637;&#20854;&#21151;&#33021;&#12290;&#29616;&#26377;&#26041;&#27861;&#21033;&#29992;&#21333;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#24490;&#29615;&#36873;&#25321;&#21644;&#25191;&#34892;&#24037;&#20855;&#65292;&#28982;&#21518;&#23558;&#32467;&#26524;&#21512;&#24182;&#21040;&#19979;&#19968;&#20010;&#21160;&#20316;&#39044;&#27979;&#20013;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#26102;&#20173;&#28982;&#23384;&#22312;&#28508;&#22312;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#21407;&#22240;&#26159;&#65306;&#65288;1&#65289;&#21333;&#20010;LLM&#30340;&#22266;&#26377;&#33021;&#21147;&#25191;&#34892;&#22810;&#26679;&#21270;&#25805;&#20316;&#21463;&#38480;&#65292;&#20197;&#21450;&#65288;2&#65289;&#22312;&#20219;&#21153;&#22833;&#36133;&#26102;&#38590;&#20197;&#33258;&#36866;&#24212;&#22320;&#32416;&#27491;&#38169;&#35823;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConAgents&#65292;&#21363;&#21512;&#20316;&#21644;&#20114;&#21160;&#20195;&#29702;&#26694;&#26550;&#65292;&#23558;&#24037;&#20855;&#23398;&#20064;&#30340;&#24037;&#20316;&#27969;&#27169;&#22359;&#21270;&#20026;Grounding&#65288;&#22522;&#30784;&#65289;&#12289;Execution&#65288;&#25191;&#34892;&#65289;&#21644;Observing&#65288;&#35266;&#23519;&#65289;&#20195;&#29702;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#36845;&#20195;&#26657;&#20934;&#65288;IterCali&#65289;&#26041;&#27861;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#26681;&#25454;&#26469;&#33258;&#24037;&#20855;&#29615;&#22659;&#30340;&#21453;&#39304;&#23545;&#33258;&#24049;&#36827;&#34892;&#35843;&#25972;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36229;&#36807;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03031v1 Announce Type: new  Abstract: Tool learning empowers large language models (LLMs) as agents to use external tools to extend their capability. Existing methods employ one single LLM-based agent to iteratively select and execute tools, thereafter incorporating the result into the next action prediction. However, they still suffer from potential performance degradation when addressing complex tasks due to: (1) the limitation of the inherent capability of a single LLM to perform diverse actions, and (2) the struggle to adaptively correct mistakes when the task fails. To mitigate these problems, we propose the ConAgents, a Cooperative and interactive Agents framework, which modularizes the workflow of tool learning into Grounding, Execution, and Observing agents. We also introduce an iterative calibration (IterCali) method, enabling the agents to adapt themselves based on the feedback from the tool environment. Experiments conducted on three datasets demonstrate the super
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;"SocraticReframe"&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#33487;&#26684;&#25289;&#24213;&#24335;&#30340;&#29702;&#24615;&#21270;&#35770;&#35777;&#65292;&#22686;&#24378;&#20102;&#31215;&#26497;&#25991;&#26412;&#37325;&#20889;&#30340;&#25968;&#25454;&#38598;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#24320;&#28304;LLM&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.03029</link><description>&lt;p&gt;
&#33487;&#26684;&#25289;&#24213;&#25512;&#29702;&#25913;&#21892;&#31215;&#26497;&#25991;&#26412;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
Socratic Reasoning Improves Positive Text Rewriting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03029
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;"SocraticReframe"&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#33487;&#26684;&#25289;&#24213;&#24335;&#30340;&#29702;&#24615;&#21270;&#35770;&#35777;&#65292;&#22686;&#24378;&#20102;&#31215;&#26497;&#25991;&#26412;&#37325;&#20889;&#30340;&#25968;&#25454;&#38598;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#24320;&#28304;LLM&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#36127;&#38754;&#24773;&#32490;&#37325;&#22609;&#20026;&#31215;&#26497;&#24605;&#32500;&#26159;&#20960;&#31181;&#35748;&#30693;&#26041;&#27861;&#21040;&#24515;&#29702;&#20581;&#24247;&#21644;&#24515;&#29702;&#27835;&#30103;&#30340;&#26680;&#24515;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#20351;&#36825;&#31181;&#37325;&#22609;&#26356;&#26131;&#23454;&#29616;&#12290;&#36825;&#31181;&#37325;&#22609;&#36890;&#24120;&#24182;&#19981;&#31616;&#21333;&#65292;&#38656;&#35201;&#22810;&#20010;&#29702;&#24615;&#21270;&#27493;&#39588;&#26469;&#25581;&#31034;&#36127;&#38754;&#24605;&#32500;&#30340;&#28508;&#22312;&#38382;&#39064;&#24182;&#20351;&#20854;&#21464;&#24471;&#26356;&#21152;&#31215;&#26497;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35813;&#29702;&#24615;&#21270;&#36807;&#31243;&#34987;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#24573;&#30053;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#22312;&#19968;&#27493;&#20013;&#37325;&#22609;&#24605;&#32500;&#12290;&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#24046;&#36317;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;"SocraticReframe"&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#30340;&#33487;&#26684;&#25289;&#24213;&#35770;&#35777;&#65292;&#25193;&#20805;&#20102;&#29992;&#20110;&#31215;&#26497;&#25991;&#26412;&#37325;&#20889;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#12290;"SocraticReframe"&#20351;&#29992;&#19968;&#31995;&#21015;&#38382;&#31572;&#23545;&#26469;&#29702;&#24615;&#21270;&#24605;&#32500;&#37325;&#20889;&#36807;&#31243;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#33487;&#26684;&#25289;&#24213;&#35770;&#35777;&#26174;&#33879;&#25913;&#21892;&#20102;&#19981;&#21516;&#24320;&#28304;LLM&#30340;&#31215;&#26497;&#25991;&#26412;&#37325;&#20889;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03029v1 Announce Type: new  Abstract: Reframing a negative into a positive thought is at the crux of several cognitive approaches to mental health and psychotherapy that could be made more accessible by large language model-based solutions. Such reframing is typically non-trivial and requires multiple rationalization steps to uncover the underlying issue of a negative thought and transform it to be more positive. However, this rationalization process is currently neglected by both datasets and models which reframe thoughts in one step. In this work, we address this gap by augmenting open-source datasets for positive text rewriting with synthetically-generated Socratic rationales using a novel framework called \textsc{SocraticReframe}. \textsc{SocraticReframe} uses a sequence of question-answer pairs to rationalize the thought rewriting process. We show that such Socratic rationales significantly improve positive text rewriting for different open-source LLMs according to both
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25913;&#21464;&#25552;&#31034;&#20013;&#30340;&#21333;&#35789;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#37322;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#20174;&#32780;&#25581;&#31034;&#20854;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.03028</link><description>&lt;p&gt;
&#21333;&#35789;&#37325;&#35201;&#24615;&#35299;&#37322;&#20102;&#25552;&#31034;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;
&lt;/p&gt;
&lt;p&gt;
Word Importance Explains How Prompts Affect Language Model Outputs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03028
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25913;&#21464;&#25552;&#31034;&#20013;&#30340;&#21333;&#35789;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#37322;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#20174;&#32780;&#25581;&#31034;&#20854;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24443;&#24213;&#25913;&#21464;&#20102;&#21508;&#34892;&#21508;&#19994;&#30340;&#35768;&#22810;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#8220;&#40657;&#30418;&#8221;&#24615;&#36136;&#24120;&#24120;&#38459;&#30861;&#20102;&#25105;&#20204;&#23545;&#20854;&#22914;&#20309;&#20570;&#20986;&#20855;&#20307;&#20915;&#31574;&#30340;&#29702;&#35299;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#36879;&#26126;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#36947;&#24503;&#20351;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#25552;&#31034;&#20013;&#30340;&#21333;&#35789;&#26469;&#25552;&#39640;LLMs&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#25581;&#31034;&#20854;&#22312;&#27169;&#22411;&#36755;&#20986;&#19978;&#30340;&#32479;&#35745;&#24433;&#21709;&#12290;&#35813;&#26041;&#27861;&#21463;&#34920;&#26684;&#25968;&#25454;&#30340;&#25490;&#21015;&#37325;&#35201;&#24615;&#21551;&#21457;&#65292;&#23631;&#34109;&#31995;&#32479;&#25552;&#31034;&#20013;&#30340;&#27599;&#20010;&#21333;&#35789;&#65292;&#24182;&#26681;&#25454;&#21487;&#29992;&#25991;&#26412;&#20998;&#25968;&#22312;&#22810;&#20010;&#29992;&#25143;&#36755;&#20837;&#19978;&#36827;&#34892;&#32858;&#21512;&#26469;&#35780;&#20272;&#20854;&#23545;&#36755;&#20986;&#30340;&#24433;&#21709;&#12290;&#19982;&#20256;&#32479;&#27880;&#24847;&#21147;&#19981;&#21516;&#65292;&#21333;&#35789;&#37325;&#35201;&#24615;&#34913;&#37327;&#25552;&#31034;&#20013;&#21333;&#35789;&#23545;&#20219;&#24847;&#23450;&#20041;&#30340;&#25991;&#26412;&#20998;&#25968;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#33021;&#22815;&#23558;&#21333;&#35789;&#30340;&#37325;&#35201;&#24615;&#20998;&#35299;&#20026;&#20855;&#20307;&#30340;&#24863;&#20852;&#36259;&#30340;&#24230;&#37327;-- &#21253;&#25324;&#20559;&#35265;&#12289;&#38405;&#35835;&#27700;&#24179;&#12289;&#20887;&#20313;&#31561;&#12290;&#27492;&#31243;&#24207;&#36824;&#20351;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03028v1 Announce Type: new  Abstract: The emergence of large language models (LLMs) has revolutionized numerous applications across industries. However, their "black box" nature often hinders the understanding of how they make specific decisions, raising concerns about their transparency, reliability, and ethical use. This study presents a method to improve the explainability of LLMs by varying individual words in prompts to uncover their statistical impact on the model outputs. This approach, inspired by permutation importance for tabular data, masks each word in the system prompt and evaluates its effect on the outputs based on the available text scores aggregated over multiple user inputs. Unlike classical attention, word importance measures the impact of prompt words on arbitrarily-defined text scores, which enables decomposing the importance of words into the specific measures of interest--including bias, reading level, verbosity, etc. This procedure also enables measur
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22810;&#27169;&#24335;&#32763;&#35793;&#27169;&#22411;&#26102;&#65292;&#24212;&#32771;&#34385;&#20854;&#21033;&#29992;&#35270;&#35273;&#20449;&#24687;&#30340;&#33021;&#21147;&#21644;&#32763;&#35793;&#22797;&#26434;&#21477;&#23376;&#30340;&#34920;&#29616;&#65292;&#24314;&#35758;&#20351;&#29992;CoMMuTE&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.03014</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#22810;&#27169;&#24335;&#32763;&#35793;&#27169;&#22411;&#30340;&#24517;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Case for Evaluating Multimodal Translation Models on Text Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03014
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22810;&#27169;&#24335;&#32763;&#35793;&#27169;&#22411;&#26102;&#65292;&#24212;&#32771;&#34385;&#20854;&#21033;&#29992;&#35270;&#35273;&#20449;&#24687;&#30340;&#33021;&#21147;&#21644;&#32763;&#35793;&#22797;&#26434;&#21477;&#23376;&#30340;&#34920;&#29616;&#65292;&#24314;&#35758;&#20351;&#29992;CoMMuTE&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#33391;&#22909;&#30340;&#35780;&#20272;&#26694;&#26550;&#24212;&#35813;&#36890;&#36807;&#34913;&#37327;&#22810;&#27169;&#24335;&#26426;&#22120;&#32763;&#35793;&#65288;MMT&#65289;&#27169;&#22411;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#21033;&#29992;&#35270;&#35273;&#20449;&#24687;&#30340;&#33021;&#21147;&#20197;&#21450;&#23427;&#20204;&#32763;&#35793;&#22797;&#26434;&#21477;&#23376;&#30340;&#33021;&#21147;&#26469;&#35780;&#20272;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;MMT&#24037;&#20316;&#26159;&#26681;&#25454;Multi30k&#27979;&#35797;&#38598;&#36827;&#34892;&#35780;&#20272;&#30340;&#65292;&#32780;&#36825;&#20123;&#27979;&#35797;&#38598;&#24182;&#19981;&#33021;&#34913;&#37327;&#36825;&#20123;&#23646;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;CoMMuTE&#35780;&#20272;&#26694;&#26550;&#26469;&#35780;&#20272;MMT&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03014v1 Announce Type: new  Abstract: A good evaluation framework should evaluate multimodal machine translation (MMT) models by measuring 1) their use of visual information to aid in the translation task and 2) their ability to translate complex sentences such as done for text-only machine translation. However, most current work in MMT is evaluated against the Multi30k testing sets, which do not measure these properties. Namely, the use of visual information by the MMT model cannot be shown directly from the Multi30k test set results and the sentences in Multi30k are are image captions, i.e., short, descriptive sentences, as opposed to complex sentences that typical text-only machine translation models are evaluated against.   Therefore, we propose that MMT models be evaluated using 1) the CoMMuTE evaluation framework, which measures the use of visual information by MMT models, 2) the text-only WMT news translation task test sets, which evaluates translation performance aga
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#31361;&#20986;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.02990</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#30340;&#25968;&#25454;&#22686;&#24378;&#65306;&#25968;&#25454;&#35270;&#35282;&#12289;&#23398;&#20064;&#33539;&#24335;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation using LLMs: Data Perspectives, Learning Paradigms and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02990
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#31361;&#20986;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#24555;&#36895;&#21457;&#23637;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#65288;DA&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#35757;&#32451;&#26679;&#26412;&#22810;&#26679;&#21270;&#32780;&#26080;&#38656;&#39069;&#22806;&#25968;&#25454;&#25910;&#38598;&#26469;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#35843;&#26597;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#25968;&#25454;&#22686;&#24378;&#30340;&#36716;&#21464;&#24615;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21450;&#20854;&#20182;&#39046;&#22495;&#20013;&#23427;&#20204;&#25552;&#20379;&#30340;&#29420;&#29305;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#20174;&#25968;&#25454;&#35270;&#35282;&#21644;&#23398;&#20064;&#35270;&#35282;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#21508;&#31181;&#31574;&#30053;&#65292;&#21253;&#25324;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#30340;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#30340;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#38416;&#26126;&#20102;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#20174;&#21487;&#25511;&#25968;&#25454;&#22686;&#24378;&#21040;&#22810;&#27169;&#24577;&#25968;&#25454;&#22686;&#24378;&#31561;&#12290;&#26412;&#35843;&#26597;&#31361;&#26174;&#20102;LLMs&#22312;&#25968;&#25454;&#22686;&#24378;&#20013;&#24341;&#20837;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#26088;&#22312;&#20316;&#20026;&#19968;&#31181;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02990v1 Announce Type: cross  Abstract: In the rapidly evolving field of machine learning (ML), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of Large Language Models (LLMs) on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From a data perspective and a learning perspective, we examine various strategies that utilize Large Language Models for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for further training. Additionally, this paper delineates the primary challenges faced in this domain, ranging from controllable data augmentation to multi modal data augmentation. This survey highlights the paradigm shift introduced by LLMs in DA, aims to serve as a 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#22810;&#27010;&#24565;&#35299;&#26512;&#26694;&#26550;&#29992;&#20110;&#22810;&#35821;&#35328;&#35821;&#20041;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#20851;&#38190;&#35789;&#21644;&#24847;&#22270;&#27010;&#24565;&#35782;&#21035;&#20197;&#21450;&#22806;&#37096;NER&#20381;&#36182;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.02975</link><description>&lt;p&gt;
&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#22810;&#27010;&#24565;&#35299;&#26512;&#26694;&#26550;&#29992;&#20110;&#22810;&#35821;&#35328;&#35821;&#20041;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
A General and Flexible Multi-concept Parsing Framework for Multilingual Semantic Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02975
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#22810;&#27010;&#24565;&#35299;&#26512;&#26694;&#26550;&#29992;&#20110;&#22810;&#35821;&#35328;&#35821;&#20041;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#20851;&#38190;&#35789;&#21644;&#24847;&#22270;&#27010;&#24565;&#35782;&#21035;&#20197;&#21450;&#22806;&#37096;NER&#20381;&#36182;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#35821;&#20041;&#21305;&#37197;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#22312;&#31038;&#21306;&#38382;&#31572;&#12289;&#25628;&#32034;&#12289;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#25512;&#33616;&#31561;&#21508;&#31181;&#37325;&#35201;&#22330;&#26223;&#20013;&#20855;&#26377;&#30456;&#24403;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DC-Match&#26469;&#35299;&#24320;&#21477;&#23376;&#20013;&#30340;&#20851;&#38190;&#35789;&#21644;&#24847;&#22270;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#20248;&#21270;&#21305;&#37197;&#24615;&#33021;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#20808;&#36827;&#27169;&#22411;&#30452;&#25509;&#27169;&#25311;&#20004;&#20010;&#21477;&#23376;&#20043;&#38388;&#21333;&#35789;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#32780;&#24573;&#30053;&#20851;&#38190;&#35789;&#21644;&#24847;&#22270;&#27010;&#24565;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;DC-Match&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35821;&#20041;&#21305;&#37197;&#26041;&#27861;&#65292;&#20294;&#23427;&#39640;&#24230;&#20381;&#36182;&#22806;&#37096;NER&#25216;&#26415;&#26469;&#35782;&#21035;&#21477;&#23376;&#30340;&#20851;&#38190;&#35789;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#27425;&#35201;&#35821;&#35328;&#30340;&#35821;&#20041;&#21305;&#37197;&#24615;&#33021;&#65292;&#22240;&#20026;&#36890;&#24120;&#24456;&#38590;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;NER&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02975v1 Announce Type: cross  Abstract: Sentence semantic matching is a research hotspot in natural language processing, which is considerably significant in various key scenarios, such as community question answering, searching, chatbot, and recommendation. Since most of the advanced models directly model the semantic relevance among words between two sentences while neglecting the \textit{keywords} and \textit{intents} concepts of them, DC-Match is proposed to disentangle keywords from intents and utilizes them to optimize the matching performance. Although DC-Match is a simple yet effective method for semantic matching, it highly depends on the external NER techniques to identify the keywords of sentences, which limits the performance of semantic matching for minor languages since satisfactory NER tools are usually hard to obtain. In this paper, we propose to generally and flexibly resolve the text into multi concepts for multilingual semantic matching to liberate the mod
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;EFSum&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#25688;&#35201;&#30340;&#26377;&#30410;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02966</link><description>&lt;p&gt;
&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#29992;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#38646;-shot&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02966
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;EFSum&#65292;&#29992;&#20110;&#22686;&#24378;LLMs&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#30830;&#20445;&#25688;&#35201;&#30340;&#26377;&#30410;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26469;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#24615;&#33021;&#65292;&#28982;&#32780;&#32467;&#26500;&#21270;&#30340;KG&#24418;&#24335;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#19977;&#20803;&#32452;&#24418;&#24335;&#25110;&#19977;&#20803;&#32452;&#20107;&#23454;&#30340;&#33258;&#30001;&#25991;&#26412;&#36716;&#25442;&#65292;&#36935;&#21040;&#20102;&#19968;&#20123;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#30001;&#20110;&#37325;&#22797;&#23454;&#20307;&#25110;&#20851;&#31995;&#32780;&#23548;&#33268;&#30340;&#35777;&#25454;&#23494;&#24230;&#38477;&#20302;&#65292;&#20197;&#21450;&#30001;&#20110;&#26080;&#27861;&#24378;&#35843;&#20851;&#38190;&#35777;&#25454;&#32780;&#23548;&#33268;&#30340;&#35777;&#25454;&#28165;&#26224;&#24230;&#38477;&#20302;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EFSum&#65292;&#19968;&#20010;&#38754;&#21521;&#35777;&#25454;&#30340;&#20107;&#23454;&#25688;&#35201;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;LLMs&#22686;&#24378;QA&#12290;&#25105;&#20204;&#36890;&#36807;&#33976;&#39311;&#21644;&#20559;&#22909;&#23545;&#40784;&#26469;&#20248;&#21270;&#19968;&#20010;&#24320;&#28304;&#30340;LLM&#20316;&#20026;&#20107;&#23454;&#25688;&#35201;&#22120;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;EFSum&#25552;&#39640;&#20102;LLM&#30340;&#38646;-shot QA&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#30830;&#20445;&#25688;&#35201;&#30340;&#21516;&#26102;&#26377;&#30410;&#21644;&#24544;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02966v1 Announce Type: cross  Abstract: Recent studies have investigated utilizing Knowledge Graphs (KGs) to enhance Quesetion Answering (QA) performance of Large Language Models (LLMs), yet structured KG verbalization remains challengin. Existing methods, such as triple-form or free-form textual conversion of triple-form facts, encounter several issues. These include reduced evidence density due to duplicated entities or relationships, and reduced evidence clarity due to an inability to emphasize crucial evidence. To address these issues, we propose EFSum, an Evidence-focused Fact Summarization framework for enhanced QA with knowledge-augmented LLMs. We optimize an open-source LLM as a fact summarizer through distillation and preference alignment. Our extensive experiments show that EFSum improves LLM's zero-shot QA performance, and it is possible to ensure both the helpfulness and faithfulness of the summary.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SimuCourt&#21496;&#27861;&#22522;&#20934;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#30340;&#21496;&#27861;&#25991;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#21496;&#27861;&#20915;&#31574;&#20219;&#21153;&#21644;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#20195;&#29702;&#30340;&#21496;&#27861;&#20998;&#26512;&#21644;&#20915;&#31574;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.02959</link><description>&lt;p&gt;
SimuCourt: &#21033;&#29992;&#30495;&#23454;&#21496;&#27861;&#21028;&#20915;&#25991;&#20214;&#26500;&#24314;&#21496;&#27861;&#20915;&#31574;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SimuCourt: Building Judicial Decision-Making Agents with Real-world Judgement Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02959
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SimuCourt&#21496;&#27861;&#22522;&#20934;&#65292;&#21253;&#25324;&#30495;&#23454;&#19990;&#30028;&#30340;&#21496;&#27861;&#25991;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#21496;&#27861;&#20915;&#31574;&#20219;&#21153;&#21644;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#35780;&#20272;&#20102;&#20195;&#29702;&#30340;&#21496;&#27861;&#20998;&#26512;&#21644;&#20915;&#31574;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20256;&#32479;&#21496;&#27861;&#34892;&#19994;&#21508;&#20010;&#26041;&#38754;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#20010;&#21035;&#21496;&#27861;&#38454;&#27573;&#65292;&#24573;&#35270;&#20102;&#36328;&#38454;&#27573;&#30340;&#21327;&#20316;&#12290;&#38543;&#30528;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#25903;&#25345;&#30340;&#33258;&#20027;&#20195;&#29702;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26234;&#33021;&#65292;&#24182;&#33021;&#20570;&#20986;&#22797;&#26434;&#20915;&#31574;&#65292;&#20026;&#21496;&#27861;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SimuCourt&#65292;&#19968;&#20010;&#21496;&#27861;&#22522;&#20934;&#65292;&#21253;&#25324;&#26469;&#33258;&#30495;&#23454;&#19990;&#30028;&#30340;420&#20221;&#21028;&#20915;&#25991;&#20214;&#65292;&#28085;&#30422;&#20102;&#19977;&#31181;&#26368;&#24120;&#35265;&#31867;&#22411;&#30340;&#21496;&#27861;&#26696;&#20363;&#65292;&#20197;&#21450;&#19968;&#20010;&#26032;&#39062;&#20219;&#21153;&#21496;&#27861;&#20915;&#31574;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#30340;&#21496;&#27861;&#20998;&#26512;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#21496;&#27861;&#30693;&#35782;&#24211;&#65292;JudicialKB&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#31181;&#27861;&#24459;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;AgentsCourt
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02959v1 Announce Type: cross  Abstract: With the development of deep learning, natural language processing technology has effectively improved the efficiency of various aspects of the traditional judicial industry. However, most current efforts focus solely on individual judicial stage, overlooking cross-stage collaboration. As the autonomous agents powered by large language models are becoming increasingly smart and able to make complex decisions in real-world settings, offering new insights for judicial intelligence. In this paper, (1) we introduce SimuCourt, a judicial benchmark that encompasses 420 judgment documents from real-world, spanning the three most common types of judicial cases, and a novel task Judicial Decision-Making to evaluate the judicial analysis and decision-making power of agents. To support this task, we construct a large-scale judicial knowledge base, JudicialKB, with multiple legal knowledge. (2) we propose a novel multi-agent framework, AgentsCourt
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;SQL&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#26368;&#20339;&#25552;&#31034;&#27169;&#26495;&#21644;&#35774;&#35745;&#26694;&#26550;&#20173;&#26080;&#20849;&#35782;&#65292;&#26032;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#20219;&#21153;&#26377;&#21161;&#20110;&#20840;&#38754;&#35780;&#20272;&#21508;&#31181;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.02951</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;SQL&#33021;&#21147;&#65306;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Benchmarking the Text-to-SQL Capability of Large Language Models: A Comprehensive Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02951
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;SQL&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23545;&#20110;&#26368;&#20339;&#25552;&#31034;&#27169;&#26495;&#21644;&#35774;&#35745;&#26694;&#26550;&#20173;&#26080;&#20849;&#35782;&#65292;&#26032;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#20219;&#21153;&#26377;&#21161;&#20110;&#20840;&#38754;&#35780;&#20272;&#21508;&#31181;&#26041;&#27861;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#25512;&#21160;&#25991;&#26412;&#29983;&#25104;SQL&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#23545;&#20110;&#26368;&#20339;&#25552;&#31034;&#27169;&#26495;&#21644;&#35774;&#35745;&#26694;&#26550;&#20173;&#28982;&#27809;&#26377;&#36798;&#25104;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02951v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have emerged as a powerful tool in advancing the Text-to-SQL task, significantly outperforming traditional methods. Nevertheless, as a nascent research field, there is still no consensus on the optimal prompt templates and design frameworks. Additionally, existing benchmarks inadequately explore the performance of LLMs across the various sub-tasks of the Text-to-SQL process, which hinders the assessment of LLMs' cognitive capabilities and the optimization of LLM-based solutions.To address the aforementioned issues, we firstly construct a new dataset designed to mitigate the risk of overfitting in LLMs. Then we formulate five evaluation tasks to comprehensively assess the performance of diverse methods across various LLMs throughout the Text-to-SQL process.Our study highlights the performance disparities among LLMs and proposes optimal in-context learning solutions tailored to each task. These findings offer
&lt;/p&gt;</description></item><item><title>PaperWeaver&#36890;&#36807;&#23558;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#19982;&#25512;&#33616;&#35770;&#25991;&#19978;&#19979;&#25991;&#21270;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#20027;&#39064;&#35770;&#25991;&#25552;&#37266;</title><link>https://arxiv.org/abs/2403.02939</link><description>&lt;p&gt;
PaperWeaver&#65306;&#36890;&#36807;&#23558;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#19982;&#25512;&#33616;&#35770;&#25991;&#19978;&#19979;&#25991;&#21270;&#65292;&#20016;&#23500;&#20027;&#39064;&#35770;&#25991;&#25552;&#37266;
&lt;/p&gt;
&lt;p&gt;
PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02939
&lt;/p&gt;
&lt;p&gt;
PaperWeaver&#36890;&#36807;&#23558;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#19982;&#25512;&#33616;&#35770;&#25991;&#19978;&#19979;&#25991;&#21270;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#20027;&#39064;&#35770;&#25991;&#25552;&#37266;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23398;&#26415;&#26723;&#26696;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#30740;&#31350;&#20154;&#21592;&#35746;&#38405;&#8220;&#35770;&#25991;&#25552;&#37266;&#8221;&#31995;&#32479;&#65292;&#23450;&#26399;&#20026;&#20182;&#20204;&#25512;&#33616;&#26368;&#36817;&#21457;&#34920;&#30340;&#19982;&#20043;&#21069;&#25910;&#38598;&#30340;&#35770;&#25991;&#30456;&#20284;&#30340;&#35770;&#25991;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#26377;&#26102;&#24456;&#38590;&#29702;&#35299;&#25512;&#33616;&#35770;&#25991;&#19982;&#20182;&#20204;&#33258;&#24049;&#30740;&#31350;&#32972;&#26223;&#20043;&#38388;&#24494;&#22937;&#30340;&#32852;&#31995;&#65292;&#22240;&#20026;&#29616;&#26377;&#31995;&#32479;&#21482;&#21576;&#29616;&#35770;&#25991;&#26631;&#39064;&#21644;&#25688;&#35201;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#36825;&#20123;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PaperWeaver&#65292;&#36825;&#26159;&#19968;&#20010;&#20016;&#23500;&#30340;&#35770;&#25991;&#25552;&#37266;&#31995;&#32479;&#65292;&#26681;&#25454;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#25552;&#20379;&#25512;&#33616;&#35770;&#25991;&#30340;&#19978;&#19979;&#25991;&#21270;&#25991;&#26412;&#25551;&#36848;&#12290;PaperWeaver&#37319;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20174;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#20013;&#25512;&#26029;&#29992;&#25143;&#30340;&#30740;&#31350;&#20852;&#36259;&#65292;&#25552;&#21462;&#35770;&#25991;&#30340;&#29305;&#23450;&#32972;&#26223;&#65292;&#24182;&#22312;&#36825;&#20123;&#32972;&#26223;&#19978;&#27604;&#36739;&#25512;&#33616;&#35770;&#25991;&#21644;&#25910;&#38598;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#65288;N=15&#65289;&#34920;&#26126;&#65292;&#20351;&#29992;PaperWeaver&#30340;&#21442;&#19982;&#32773;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02939v1 Announce Type: cross  Abstract: With the rapid growth of scholarly archives, researchers subscribe to "paper alert" systems that periodically provide them with recommendations of recently published papers that are similar to previously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts. To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users' research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#26159;&#21542;&#21487;&#20197;&#21548;&#21040;&#32463;&#36807;&#20248;&#21270;&#30340;&#35821;&#38899;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#21487;&#26681;&#25454;&#38899;&#32032;&#20026;&#21333;&#20301;&#33258;&#21160;&#35843;&#25972;&#25773;&#25918;&#36895;&#24230;&#65292;&#20197;&#30830;&#20445;&#35821;&#38899;&#21487;&#36776;&#35782;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.02938</link><description>&lt;p&gt;
AIx Speed&#65306;&#20351;&#29992;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#21548;&#21147;&#29702;&#35299;&#20248;&#21270;&#22238;&#25918;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
AIx Speed: Playback Speed Optimization Using Listening Comprehension of Speech Recognition Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#26159;&#21542;&#21487;&#20197;&#21548;&#21040;&#32463;&#36807;&#20248;&#21270;&#30340;&#35821;&#38899;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#21487;&#26681;&#25454;&#38899;&#32032;&#20026;&#21333;&#20301;&#33258;&#21160;&#35843;&#25972;&#25773;&#25918;&#36895;&#24230;&#65292;&#20197;&#30830;&#20445;&#35821;&#38899;&#21487;&#36776;&#35782;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20154;&#31867;&#21487;&#20197;&#20197;&#27604;&#23454;&#38469;&#35266;&#23519;&#21040;&#30340;&#36895;&#24230;&#26356;&#24555;&#22320;&#20542;&#21548;&#38899;&#39057;&#21644;&#35266;&#30475;&#35270;&#39057;&#65292;&#22240;&#27492;&#25105;&#20204;&#32463;&#24120;&#20197;&#26356;&#39640;&#30340;&#25773;&#25918;&#36895;&#24230;&#20542;&#21548;&#25110;&#35266;&#30475;&#36825;&#20123;&#20869;&#23481;&#30340;&#29255;&#27573;&#65292;&#20197;&#25552;&#39640;&#20869;&#23481;&#29702;&#35299;&#30340;&#26102;&#38388;&#25928;&#29575;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#26681;&#25454;&#29992;&#25143;&#24773;&#20917;&#21644;&#20869;&#23481;&#31867;&#22411;&#33258;&#21160;&#35843;&#25972;&#25773;&#25918;&#36895;&#24230;&#30340;&#31995;&#32479;&#65292;&#20197;&#21327;&#21161;&#26356;&#39640;&#25928;&#22320;&#29702;&#35299;&#26102;&#38388;&#24207;&#21015;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#20173;&#26377;&#36827;&#19968;&#27493;&#25552;&#39640;&#20154;&#31867;&#36895;&#21548;&#33021;&#21147;&#30340;&#31354;&#38388;&#65292;&#21363;&#29983;&#25104;&#24050;&#32463;&#38024;&#23545;&#26356;&#31934;&#32454;&#30340;&#26102;&#38388;&#21333;&#20301;&#20248;&#21270;&#36807;&#30340;&#35821;&#38899;&#65292;&#24182;&#23558;&#20854;&#25552;&#20379;&#32473;&#20154;&#31867;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20154;&#31867;&#33021;&#21542;&#21548;&#21040;&#20248;&#21270;&#36807;&#30340;&#35821;&#38899;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#26681;&#25454;&#38899;&#32032;&#20026;&#21333;&#20301;&#33258;&#21160;&#35843;&#25972;&#25773;&#25918;&#36895;&#24230;&#65292;&#21516;&#26102;&#30830;&#20445;&#35821;&#38899;&#21487;&#36776;&#35782;&#24230;&#12290;&#31995;&#32479;&#20351;&#29992;&#35821;&#38899;&#35782;&#21035;&#24471;&#20998;&#20316;&#20026;&#34913;&#37327;&#20154;&#31867;&#33021;&#21542;&#21548;&#21040;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02938v1 Announce Type: new  Abstract: Since humans can listen to audio and watch videos at faster speeds than actually observed, we often listen to or watch these pieces of content at higher playback speeds to increase the time efficiency of content comprehension. To further utilize this capability, systems that automatically adjust the playback speed according to the user's condition and the type of content to assist in more efficient comprehension of time-series content have been developed. However, there is still room for these systems to further extend human speed-listening ability by generating speech with playback speed optimized for even finer time units and providing it to humans. In this study, we determine whether humans can hear the optimized speech and propose a system that automatically adjusts playback speed at units as small as phonemes while ensuring speech intelligibility. The system uses the speech recognizer score as a proxy for how well a human can hear a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#36923;&#36753;&#34920;&#36798;&#26469;&#34920;&#24449;&#31867;&#21035;&#21547;&#20041;&#30340;&#35268;&#21017;Prompt&#26041;&#27861;&#65292;&#32467;&#21512;PLMs&#21644;&#33258;&#36845;&#20195;&#36923;&#36753;&#35268;&#21017;&#23454;&#29616;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;</title><link>https://arxiv.org/abs/2403.02932</link><description>&lt;p&gt;
RulePrompt: &#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#19982;&#25552;&#31034;PLMs&#21644;&#33258;&#36845;&#20195;&#36923;&#36753;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
RulePrompt: Weakly Supervised Text Classification with Prompting PLMs and Self-Iterative Logical Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02932
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#36923;&#36753;&#34920;&#36798;&#26469;&#34920;&#24449;&#31867;&#21035;&#21547;&#20041;&#30340;&#35268;&#21017;Prompt&#26041;&#27861;&#65292;&#32467;&#21512;PLMs&#21644;&#33258;&#36845;&#20195;&#36923;&#36753;&#35268;&#21017;&#23454;&#29616;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#65288;WSTC&#65289;&#65292;&#21448;&#31216;&#38646;&#26679;&#26412;&#25110;&#26080;&#25968;&#25454;&#25991;&#26412;&#20998;&#31867;&#65292;&#22312;&#21160;&#24577;&#21644;&#24320;&#25918;&#30340;Web&#29615;&#22659;&#20013;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20165;&#38656;&#35201;&#27599;&#20010;&#31867;&#21035;&#30340;&#26377;&#38480;&#31181;&#23376;&#35789;&#65288;&#26631;&#31614;&#21517;&#31216;&#65289;&#32780;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#23601;&#33021;&#23545;&#22823;&#37327;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#30693;&#35782;&#24418;&#24335;&#65292;&#20351;&#29992;&#36923;&#36753;&#34920;&#36798;&#26469;&#34920;&#24449;&#31867;&#21035;&#30340;&#21547;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20511;&#21161;PLMs&#21644;&#33258;&#36845;&#20195;&#36923;&#36753;&#35268;&#21017;&#26469;&#23454;&#29616;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02932v1 Announce Type: new  Abstract: Weakly supervised text classification (WSTC), also called zero-shot or dataless text classification, has attracted increasing attention due to its applicability in classifying a mass of texts within the dynamic and open Web environment, since it requires only a limited set of seed words (label names) for each category instead of labeled data. With the help of recently popular prompting Pre-trained Language Models (PLMs), many studies leveraged manually crafted and/or automatically identified verbalizers to estimate the likelihood of categories, but they failed to differentiate the effects of these category-indicative words, let alone capture their correlations and realize adaptive adjustments according to the unlabeled corpus. In this paper, in order to let the PLM effectively understand each category, we at first propose a novel form of rule-based knowledge using logical expressions to characterize the meanings of categories. Then, we d
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22797;&#21046;&#30740;&#31350;BASS&#26694;&#26550;&#65292;&#21457;&#29616;&#20102;&#19982;&#21407;&#22987;&#24037;&#20316;&#30456;&#27604;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#24378;&#35843;&#20102;&#25776;&#20889;&#21487;&#22797;&#21046;&#35770;&#25991;&#30340;&#20851;&#38190;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2403.02930</link><description>&lt;p&gt;
BASS&#30340;&#20877;&#23457;&#35270;--&#21033;&#29992;&#32479;&#19968;&#35821;&#20041;&#22270;&#25552;&#21319;&#25277;&#35937;&#25688;&#35201;--&#19968;&#39033;&#22797;&#21046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Second Look on BASS -- Boosting Abstractive Summarization with Unified Semantic Graphs -- A Replication Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02930
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22797;&#21046;&#30740;&#31350;BASS&#26694;&#26550;&#65292;&#21457;&#29616;&#20102;&#19982;&#21407;&#22987;&#24037;&#20316;&#30456;&#27604;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#65292;&#24182;&#24378;&#35843;&#20102;&#25776;&#20889;&#21487;&#22797;&#21046;&#35770;&#25991;&#30340;&#20851;&#38190;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;BASS&#26694;&#26550;&#30340;&#35814;&#32454;&#22797;&#21046;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#32479;&#19968;&#35821;&#20041;&#22270;&#27010;&#24565;&#30340;&#25277;&#35937;&#25688;&#35201;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21253;&#25324;&#22797;&#21046;&#20851;&#38190;&#32452;&#20214;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#20197;&#21450;&#19968;&#20010;&#28040;&#34701;&#30740;&#31350;&#26469;&#31995;&#32479;&#22320;&#38548;&#31163;&#22312;&#22797;&#21046;&#26032;&#39062;&#32452;&#20214;&#26102;&#26681;&#28304;&#20110;&#38169;&#35823;&#26469;&#28304;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#19982;&#21407;&#22987;&#24037;&#20316;&#30456;&#27604;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#21363;&#20351;&#26159;&#34987;&#21512;&#29702;&#30465;&#30053;&#30340;&#32454;&#33410;&#23545;&#20110;&#22797;&#21046;&#20687;BASS&#36825;&#26679;&#30340;&#20808;&#36827;&#26694;&#26550;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#25776;&#20889;&#21487;&#22797;&#21046;&#35770;&#25991;&#30340;&#20851;&#38190;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02930v1 Announce Type: new  Abstract: We present a detailed replication study of the BASS framework, an abstractive summarization system based on the notion of Unified Semantic Graphs. Our investigation includes challenges in replicating key components and an ablation study to systematically isolate error sources rooted in replicating novel components. Our findings reveal discrepancies in performance compared to the original work. We highlight the significance of paying careful attention even to reasonably omitted details for replicating advanced frameworks like BASS, and emphasize key practices for writing replicable papers.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#24687;&#27969;&#20998;&#26512;&#35777;&#23454;&#20102;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21333;&#35789;&#32423;&#21035;&#21644;&#25991;&#26412;&#32423;&#21035;&#20998;&#31867;&#20043;&#38388;&#30340;&#30456;&#20114;&#22686;&#24378;&#25928;&#24212;&#65292;&#21516;&#26102;&#23558;&#35813;&#25928;&#24212;&#25193;&#23637;&#21040;&#25552;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.02902</link><description>&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#27969;&#23637;&#31034;&#30456;&#20114;&#22686;&#24378;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Demonstrating Mutual Reinforcement Effect through Information Flow
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02902
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#27969;&#20998;&#26512;&#35777;&#23454;&#20102;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21333;&#35789;&#32423;&#21035;&#21644;&#25991;&#26412;&#32423;&#21035;&#20998;&#31867;&#20043;&#38388;&#30340;&#30456;&#20114;&#22686;&#24378;&#25928;&#24212;&#65292;&#21516;&#26102;&#23558;&#35813;&#25928;&#24212;&#25193;&#23637;&#21040;&#25552;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20114;&#22686;&#24378;&#25928;&#24212;(MRE)&#30740;&#31350;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#21333;&#35789;&#32423;&#21035;&#21644;&#25991;&#26412;&#32423;&#21035;&#20998;&#31867;&#20043;&#38388;&#30340;&#21327;&#21516;&#20851;&#31995;&#12290;&#23427;&#35748;&#20026;&#20004;&#20010;&#20998;&#31867;&#32423;&#21035;&#30340;&#24615;&#33021;&#21487;&#20197;&#30456;&#20114;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26426;&#21046;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35777;&#26126;&#25110;&#35299;&#37322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#21033;&#29992;&#20449;&#24687;&#27969;&#20998;&#26512;&#26469;&#35266;&#23519;&#21644;&#35777;&#23454;MRE&#29702;&#35770;&#12290;&#25105;&#20204;&#22312;&#20845;&#20010;MRE&#28151;&#21512;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#27169;&#22411;&#20013;MRE&#30340;&#23384;&#22312;&#21450;&#20854;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24494;&#35843;&#23454;&#39564;&#65292;&#20854;&#32467;&#26524;&#19982;&#20449;&#24687;&#27969;&#23454;&#39564;&#30340;&#32467;&#26524;&#19968;&#33268;&#12290;&#20004;&#20010;&#23454;&#39564;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#35777;&#23454;&#20102;MRE&#30340;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;MRE&#30340;&#24212;&#29992;&#25193;&#23637;&#21040;&#25552;&#31034;&#23398;&#20064;&#65292;&#21033;&#29992;&#21333;&#35789;&#32423;&#21035;&#20449;&#24687;&#20316;&#20026;&#34920;&#36798;&#22120;&#26469;&#22686;&#24378;&#27169;&#22411;&#39044;&#27979;&#25991;&#26412;&#32423;&#21035;&#20998;&#31867;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02902v1 Announce Type: new  Abstract: The Mutual Reinforcement Effect (MRE) investigates the synergistic relationship between word-level and text-level classifications in text classification tasks. It posits that the performance of both classification levels can be mutually enhanced. However, this mechanism has not been adequately demonstrated or explained in prior research. To address this gap, we employ information flow analysis to observe and substantiate the MRE theory. Our experiments on six MRE hybrid datasets revealed the presence of MRE in the model and its impact. Additionally, we conducted fine-tuning experiments, whose results were consistent with those of the information flow experiments. The convergence of findings from both experiments corroborates the existence of MRE. Furthermore, we extended the application of MRE to prompt learning, utilizing word-level information as a verbalizer to bolster the model's prediction of text-level classification labels. In our
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24322;&#26500;&#22270;&#23545;&#27604;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26723;&#32423;&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;F1&#24471;&#20998;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.02893</link><description>&lt;p&gt;
&#20351;&#29992;&#24322;&#26500;&#22270;&#23545;&#27604;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26723;&#32423;&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02893
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24322;&#26500;&#22270;&#23545;&#27604;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26723;&#32423;&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;F1&#24471;&#20998;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;&#65288;ECI&#65289;&#25351;&#30340;&#26159;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#20107;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#19979;&#30340;&#21477;&#23376;&#32423;ECI&#65292;&#32780;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25991;&#26723;&#32423;ECI&#65288;DECI&#65289;&#21364;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#22810;&#31890;&#24230;&#23545;&#27604;&#20256;&#36882;&#23398;&#20064;&#65288;GIMC&#65289;&#30340;&#24322;&#26500;&#22270;&#20132;&#20114;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26723;&#32423;ECI&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24322;&#26500;&#22270;&#20132;&#20114;&#32593;&#32476;&#26469;&#24314;&#27169;&#25991;&#26723;&#20013;&#20998;&#25955;&#20107;&#20214;&#20043;&#38388;&#30340;&#36828;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#25552;&#39640;&#20174;&#28304;&#35821;&#35328;&#23398;&#20064;&#21040;&#30340;&#22240;&#26524;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#21487;&#36716;&#31227;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#31890;&#24230;&#23545;&#27604;&#20256;&#36882;&#23398;&#20064;&#27169;&#22359;&#65292;&#20197;&#35843;&#25972;&#36328;&#35821;&#35328;&#38388;&#30340;&#22240;&#26524;&#34920;&#31034;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#24179;&#22343;F1&#24471;&#20998;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#32422;9.4%&#21644;8.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02893v1 Announce Type: cross  Abstract: Event Causality Identification (ECI) refers to detect causal relations between events in texts. However, most existing studies focus on sentence-level ECI with high-resource language, leaving more challenging document-level ECI (DECI) with low-resource languages under-explored. In this paper, we propose a Heterogeneous Graph Interaction Model with Multi-granularity Contrastive Transfer Learning (GIMC) for zero-shot cross-lingual document-level ECI. Specifically, we introduce a heterogeneous graph interaction network to model the long-distance dependencies between events that are scattered over document. Then, to improve cross-lingual transferability of causal knowledge learned from source language, we propose a multi-granularity contrastive transfer learning module to align the causal representations across languages. Extensive experiments show our framework outperforms previous state-of-the-art model by 9.4% and 8.2% of average F1 sco
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#24187;&#35273;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#26102;&#36935;&#21040;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02889</link><description>&lt;p&gt;
&#22312;&#23547;&#25214;&#30495;&#30456;&#65306;&#19968;&#31181;&#23457;&#38382;&#26041;&#27861;&#29992;&#20110;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
In Search of Truth: An Interrogation Approach to Hallucination Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02889
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#24187;&#35273;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#26102;&#36935;&#21040;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#35768;&#22810;&#36827;&#23637;&#24182;&#19988;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#24555;&#36895;&#21457;&#23637;&#65292;&#20294;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#23427;&#20204;&#23545;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#24433;&#21709;&#21644;&#25972;&#21512;&#20173;&#28982;&#26377;&#38480;&#12290;&#19968;&#20010;&#38459;&#30861;&#23427;&#20204;&#24191;&#27867;&#24212;&#29992;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#24187;&#35273;&#30340;&#21457;&#29983;&#65292;&#21363;LLMs&#21019;&#36896;&#20986;&#21548;&#36215;&#26469;&#30495;&#23454;&#20294;&#20559;&#31163;&#20107;&#23454;&#30495;&#30456;&#30340;&#31572;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#65292;&#36825;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21253;&#25324;Llama-2&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#26368;&#26032;LLMs&#30340;&#24187;&#35273;&#27700;&#24179;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#26816;&#27979;&#23427;&#20204;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#29305;&#23450;&#23454;&#39564;&#20013;&#35266;&#23519;&#21040;Llama-2&#36798;&#21040;62%&#30340;&#24187;&#35273;&#27700;&#24179;&#65292;&#32780;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27809;&#26377;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;87%&#30340;&#24179;&#34913;&#20934;&#30830;&#29575;&#65288;B-ACC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02889v1 Announce Type: new  Abstract: Despite the many advances of Large Language Models (LLMs) and their unprecedented rapid evolution, their impact and integration into every facet of our daily lives is limited due to various reasons. One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth. In this paper, we present a novel method for detecting hallucinations in large language models, which tackles a critical issue in the adoption of these models in various real-world scenarios. Through extensive evaluations across multiple datasets and LLMs, including Llama-2, we study the hallucination levels of various recent LLMs and demonstrate the effectiveness of our method to automatically detect them. Notably, we observe up to 62% hallucinations for Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy (B-ACC) of 87%, all without relying 
&lt;/p&gt;</description></item><item><title>MathScale&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#65292;&#23637;&#29616;&#20986;&#22312;&#25968;&#23398;&#25968;&#25454;&#38598;&#22823;&#23567;&#26041;&#38754;&#30340;&#26377;&#25928;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02884</link><description>&lt;p&gt;
MathScale: &#25968;&#23398;&#25512;&#29702;&#30340;&#25351;&#23548;&#20248;&#21270;&#23610;&#24230;
&lt;/p&gt;
&lt;p&gt;
MathScale: Scaling Instruction Tuning for Mathematical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02884
&lt;/p&gt;
&lt;p&gt;
MathScale&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#65292;&#23637;&#29616;&#20986;&#22312;&#25968;&#23398;&#25968;&#25454;&#38598;&#22823;&#23567;&#26041;&#38754;&#30340;&#26377;&#25928;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#20173;&#28982;&#19981;&#36275;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MathScale&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21069;&#27839;&#30340;LLMs&#65288;&#20363;&#22914;GPT-3.5&#65289;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#12290;&#21463;&#20154;&#31867;&#25968;&#23398;&#23398;&#20064;&#20013;&#30340;&#35748;&#30693;&#26426;&#21046;&#21551;&#21457;&#65292;&#23427;&#39318;&#20808;&#20174;&#31181;&#23376;&#25968;&#23398;&#38382;&#39064;&#20013;&#25552;&#21462;&#20027;&#39064;&#21644;&#30693;&#35782;&#28857;&#65292;&#28982;&#21518;&#26500;&#24314;&#19968;&#20010;&#27010;&#24565;&#22270;&#65292;&#38543;&#21518;&#29992;&#20110;&#29983;&#25104;&#26032;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;MathScale&#22312;&#25105;&#20204;&#29983;&#25104;&#30340;&#25968;&#23398;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26377;&#25928;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20004;&#30334;&#19975;&#25968;&#23398;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;MathScaleQA&#65289;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;LLMs&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;MwpBench&#65292;&#36825;&#26159;&#19968;&#20010;&#25968;&#23398;&#38382;&#39064;&#35789;&#27719;&#38382;&#39064;&#22522;&#20934;&#65292;&#21253;&#25324;&#21313;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02884v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in problem-solving. However, their proficiency in solving mathematical problems remains inadequate. We propose MathScale, a simple and scalable method to create high-quality mathematical reasoning data using frontier LLMs (e.g., {\tt GPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning, it first extracts topics and knowledge points from seed math questions and then build a concept graph, which is subsequently used to generate new math questions. MathScale exhibits effective scalability along the size axis of the math dataset that we generate. As a result, we create a mathematical reasoning dataset (MathScaleQA) containing two million math question-answer pairs. To evaluate mathematical reasoning abilities of LLMs comprehensively, we construct {\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten datasets (including 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30828;&#36127;&#26679;&#26412;&#25913;&#36827;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20013;&#27010;&#24565;&#29702;&#35299;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#39068;&#33394;&#12289;&#23545;&#35937;&#21644;&#22823;&#23567;&#32454;&#31890;&#24230;&#23545;&#40784;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.02875</link><description>&lt;p&gt;
&#36890;&#36807;&#30828;&#36127;&#26679;&#26412;&#22686;&#24378;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#27010;&#24565;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Enhancing Conceptual Understanding in Multimodal Contrastive Learning through Hard Negative Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30828;&#36127;&#26679;&#26412;&#25913;&#36827;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20013;&#27010;&#24565;&#29702;&#35299;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#39068;&#33394;&#12289;&#23545;&#35937;&#21644;&#22823;&#23567;&#32454;&#31890;&#24230;&#23545;&#40784;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#21457;&#23637;&#31934;&#32454;&#30340;&#27010;&#24565;&#29702;&#35299;&#26041;&#38754;&#36890;&#24120;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#12290;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#30001;&#20110;&#38543;&#26426;&#36127;&#26679;&#26412;&#65292;&#23548;&#33268;&#20960;&#20046;&#21482;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#27010;&#24565;&#36827;&#34892;&#25439;&#22833;&#20989;&#25968;&#27604;&#36739;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#32454;&#31890;&#24230;&#35821;&#20041;&#24046;&#24322;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21512;&#25104;&#30340;&#30828;&#36127;&#25991;&#26412;&#31034;&#20363;&#12290;&#36825;&#20123;&#30828;&#36127;&#26679;&#26412;&#23545;&#24212;&#20110;&#35270;&#35273;&#27010;&#24565;&#30340;&#25490;&#21015;&#65292;&#23548;&#33268;&#26356;&#31934;&#32454;&#30340;&#35270;&#35273;&#21644;&#25991;&#26412;&#27010;&#24565;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;InpaintCOCO&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#39068;&#33394;&#12289;&#23545;&#35937;&#21644;&#22823;&#23567;&#32454;&#31890;&#24230;&#23545;&#40784;&#30340;&#26032;&#25361;&#25112;&#24615;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#20174;COCO&#22270;&#20687;&#29983;&#25104;&#30340;&#20449;&#24687;&#22635;&#20805;&#26469;&#21019;&#24314;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25913;&#21464;&#35270;&#35273;&#27010;&#24565;&#65292;&#20351;&#22270;&#20687;&#19981;&#20877;&#19982;&#20854;&#21407;&#22987;&#26631;&#39064;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02875v1 Announce Type: cross  Abstract: Current multimodal models leveraging contrastive learning often face limitations in developing fine-grained conceptual understanding. This is due to random negative samples during pretraining, causing almost exclusively very dissimilar concepts to be compared in the loss function. Consequently, the models struggle with fine-grained semantic differences. To address this problem, we introduce a novel pretraining method incorporating synthetic hard negative text examples. The hard negatives permute terms corresponding to visual concepts, leading to a more fine-grained visual and textual concept alignment. Further, we introduce InpaintCOCO, a new challenging dataset for assessing the fine-grained alignment of colors, objects, and sizes in vision-language models. We created the dataset using generative inpainting from COCO images by changing the visual concepts so that the images no longer match their original captions. Our results show sig
&lt;/p&gt;</description></item><item><title>&#31934;&#35843;&#35780;&#21028;&#27169;&#22411;&#22312;&#39046;&#22495;&#20869;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#21644;&#20844;&#24179;&#24615;&#19981;&#21450;GPT4&#12290;</title><link>https://arxiv.org/abs/2403.02839</link><description>&lt;p&gt;
&#20316;&#20026;&#35780;&#21028;&#22120;&#30340;LLM&#30340;&#23454;&#35777;&#30740;&#31350;&#65306;&#31934;&#35843;&#35780;&#21028;&#22120;&#27169;&#22411;&#26159;&#29305;&#23450;&#20219;&#21153;&#30340;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02839
&lt;/p&gt;
&lt;p&gt;
&#31934;&#35843;&#35780;&#21028;&#27169;&#22411;&#22312;&#39046;&#22495;&#20869;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#27867;&#21270;&#33021;&#21147;&#21644;&#20844;&#24179;&#24615;&#19981;&#21450;GPT4&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35780;&#20272;&#20854;&#20182;LLM&#36136;&#37327;&#30340;&#36235;&#21183;&#26085;&#30410;&#22686;&#38271;&#12290;&#35768;&#22810;&#30740;&#31350;&#37319;&#29992;&#19987;&#26377;&#30340;&#38381;&#28304;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;GPT4&#65292;&#20316;&#20026;&#35780;&#20272;&#22120;&#12290;&#21478;&#22806;&#65292;&#20854;&#20182;&#30740;&#31350;&#21033;&#29992;&#24320;&#28304;LLM&#26469;&#31934;&#35843;&#35780;&#21028;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#22120;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#30340;&#35780;&#21028;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#23613;&#31649;&#31934;&#35843;&#30340;&#35780;&#21028;&#27169;&#22411;&#22312;&#39046;&#22495;&#20869;&#27979;&#35797;&#38598;&#19978;&#33021;&#22815;&#36798;&#21040;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#29978;&#33267;&#36229;&#36807;GPT4&#65292;&#20294;&#23427;&#20204;&#26412;&#36136;&#19978;&#26159;&#29305;&#23450;&#20219;&#21153;&#30340;&#20998;&#31867;&#22120;&#65292;&#20854;&#27867;&#21270;&#33021;&#21147;&#21644;&#20844;&#24179;&#24615;&#36828;&#20302;&#20110;GPT4&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02839v1 Announce Type: new  Abstract: Recently, there has been a growing trend of utilizing Large Language Model (LLM) to evaluate the quality of other LLMs. Many studies have employed proprietary close-source models, especially GPT4, as the evaluator. Alternatively, other works have fine-tuned judge models based on open-source LLMs as the evaluator. In this study, we conduct an empirical study of different judge models on their evaluation capability. Our findings indicate that although the fine-tuned judge models achieve high accuracy on in-domain test sets, even surpassing GPT4, they are inherently task-specific classifiers, and their generalizability and fairness severely underperform GPT4.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#20462;&#21098;&#20998;&#21306;&#22686;&#24378;&#65288;DPPA&#65289;&#30340;&#21452;&#38454;&#27573;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21512;&#24182;&#22797;&#26434;&#24494;&#35843;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.02799</link><description>&lt;p&gt;
DPPA&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20462;&#21098;&#26041;&#27861;&#20197;&#36827;&#34892;&#27169;&#22411;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
DPPA: Pruning Method for Large Language Model to Model Merging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02799
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#20462;&#21098;&#20998;&#21306;&#22686;&#24378;&#65288;DPPA&#65289;&#30340;&#21452;&#38454;&#27573;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#21512;&#24182;&#22797;&#26434;&#24494;&#35843;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21512;&#24182;&#26159;&#23558;&#20174;&#22810;&#20010;&#39046;&#22495;&#34893;&#29983;&#20986;&#30340;&#24494;&#35843;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#22686;&#24378;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#29087;&#32451;&#24230;&#12290;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#35299;&#20915;&#21442;&#25968;&#20914;&#31361;&#12290;&#29616;&#26377;&#22823;&#37327;&#30740;&#31350;&#24050;&#35299;&#20915;&#20102;&#21512;&#24182;&#38454;&#27573;&#30340;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#26032;&#30740;&#31350;&#38598;&#20013;&#22312;&#36890;&#36807;&#20462;&#21098;&#38454;&#27573;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;DARE&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#31616;&#21333;&#24494;&#35843;&#27169;&#22411;&#26102;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#20110;&#26174;&#31034;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#23384;&#22312;&#26174;&#33879;&#21442;&#25968;&#20559;&#24046;&#30340;&#22797;&#26434;&#24494;&#35843;&#27169;&#22411;&#26102;&#65292;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#24448;&#24448;&#20250;&#20943;&#24369;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#20462;&#21098;&#20998;&#21306;&#22686;&#24378;&#65288;DPPA&#65289;&#30340;&#21452;&#38454;&#27573;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#21512;&#24182;&#22797;&#26434;&#24494;&#35843;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21160;&#24577;&#20462;&#21098;&#65288;DP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#37327;&#21098;&#26525;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#20854;&#30446;&#30340;&#26159;&#22686;&#24378;p
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02799v1 Announce Type: cross  Abstract: Model merging is to combine fine-tuned models derived from multiple domains, with the intent of enhancing the model's proficiency across various domains. The principal concern is the resolution of parameter conflicts. A substantial amount of existing research remedy this issue during the merging stage, with the latest study focusing on resolving this issue throughout the pruning stage. The DARE approach has exhibited promising outcomes when applied to a simplistic fine-tuned model. However, the efficacy of this method tends to wane when employed on complex fine-tuned models that show a significant parameter bias relative to the baseline model. In this paper, we introduce a dual-stage method termed Dynamic Pruning Partition Amplification (DPPA), devised to tackle the challenge of merging complex fine-tuned models. Initially, we introduce Dynamically Pruning (DP), an improved approach based on magnitude pruning, which aim is to enhance p
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#32946;&#19987;&#23478;&#26469;&#35780;&#20272;&#25945;&#32946;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;LMs&#20316;&#20026;&#21487;&#38752;&#35780;&#20272;&#32773;&#30340;&#28508;&#21147;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#25351;&#23548;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02795</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21644;&#20248;&#21270;&#25945;&#32946;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Optimizing Educational Content with Large Language Model Judgments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02795
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#32946;&#19987;&#23478;&#26469;&#35780;&#20272;&#25945;&#32946;&#20869;&#23481;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;LMs&#20316;&#20026;&#21487;&#38752;&#35780;&#20272;&#32773;&#30340;&#28508;&#21147;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#25351;&#23548;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#24314;&#26377;&#25928;&#30340;&#25945;&#32946;&#26448;&#26009;&#36890;&#24120;&#38656;&#35201;&#23545;&#23398;&#29983;&#23398;&#20064;&#25104;&#26524;&#36827;&#34892;&#26114;&#36149;&#19988;&#32791;&#26102;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38556;&#30861;&#65292;&#19968;&#20010;&#24819;&#27861;&#26159;&#26500;&#24314;&#23398;&#29983;&#23398;&#20064;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23427;&#20204;&#26469;&#20248;&#21270;&#25945;&#23398;&#26448;&#26009;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#23398;&#20064;&#21160;&#24577;&#30340;&#35748;&#30693;&#36807;&#31243;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20316;&#20026;&#25945;&#32946;&#19987;&#23478;&#26469;&#35780;&#20272;&#21508;&#31181;&#25351;&#23548;&#23545;&#23398;&#20064;&#32467;&#26524;&#24433;&#21709;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5&#26469;&#35780;&#20272;&#25351;&#23548;&#26448;&#26009;&#23545;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#30340;&#25972;&#20307;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#22797;&#21046;&#35832;&#22914;&#19987;&#19994;&#36870;&#36716;&#25928;&#24212;&#21644;&#21464;&#24322;&#25928;&#24212;&#31561;&#24050;&#32463;&#24314;&#31435;&#30340;&#25945;&#32946;&#21457;&#29616;&#12290;&#36825;&#23637;&#31034;&#20102;LMs&#20316;&#20026;&#25945;&#32946;&#20869;&#23481;&#21487;&#38752;&#35780;&#20272;&#32773;&#30340;&#28508;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#25351;&#23548;&#20248;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;LM&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02795v1 Announce Type: new  Abstract: Creating effective educational materials generally requires expensive and time-consuming studies of student learning outcomes. To overcome this barrier, one idea is to build computational models of student learning and use them to optimize instructional materials. However, it is difficult to model the cognitive processes of learning dynamics. We propose an alternative approach that uses Language Models (LMs) as educational experts to assess the impact of various instructions on learning outcomes. Specifically, we use GPT-3.5 to evaluate the overall effect of instructional materials on different student groups and find that it can replicate well-established educational findings such as the Expertise Reversal Effect and the Variability Effect. This demonstrates the potential of LMs as reliable evaluators of educational content. Building on this insight, we introduce an instruction optimization approach in which one LM generates instruction
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20869;&#23384;&#30340;&#22768;&#26126;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24635;&#32467;&#36807;&#21435;&#32463;&#39564;&#65292;&#24110;&#21161;&#20195;&#29702;&#20174;&#20013;&#25552;&#21462;&#35265;&#35299;&#24182;&#25913;&#36827;&#24615;&#33021;&#65292;&#23454;&#29616;&#33258;&#25105;&#25913;&#36827;&#12290;&#36827;&#34892;&#20102;&#31995;&#32479;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02757</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#23384;&#30340;&#23398;&#20064;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22768;&#26126;&#24335;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
In-Memory Learning: A Declarative Learning Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02757
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20869;&#23384;&#30340;&#22768;&#26126;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24635;&#32467;&#36807;&#21435;&#32463;&#39564;&#65292;&#24110;&#21161;&#20195;&#29702;&#20174;&#20013;&#25552;&#21462;&#35265;&#35299;&#24182;&#25913;&#36827;&#24615;&#33021;&#65292;&#23454;&#29616;&#33258;&#25105;&#25913;&#36827;&#12290;&#36827;&#34892;&#20102;&#31995;&#32479;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#26597;&#20195;&#29702;&#26159;&#21542;&#33021;&#22815;&#19982;&#29615;&#22659;&#23545;&#40784;&#32780;&#19981;&#20381;&#36182;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#20027;&#39064;&#12290;&#21463;&#26234;&#33021;&#29983;&#29289;&#23545;&#40784;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#22768;&#26126;&#24335;&#20869;&#23384;&#22312;&#24635;&#32467;&#36807;&#21435;&#32463;&#39564;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20195;&#29702;&#33021;&#22815;&#20174;&#36807;&#21435;&#32463;&#39564;&#20013;&#25552;&#28860;&#35265;&#35299;&#65292;&#25913;&#36827;&#21644;&#26356;&#26032;&#29616;&#26377;&#31508;&#35760;&#20197;&#25552;&#39640;&#22312;&#29615;&#22659;&#20013;&#30340;&#34920;&#29616;&#12290;&#25972;&#20010;&#36807;&#31243;&#21457;&#29983;&#22312;&#20869;&#23384;&#32452;&#20214;&#20013;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23454;&#29616;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#27492;&#26694;&#26550;&#25551;&#36848;&#20026;&#22522;&#20110;&#20869;&#23384;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#33258;&#25105;&#25913;&#36827;&#36807;&#31243;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#20851;&#38190;&#29305;&#24615;&#12290;&#36890;&#36807;&#31995;&#32479;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#24182;&#25552;&#20379;&#20102;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02757v1 Announce Type: new  Abstract: The exploration of whether agents can align with their environment without relying on human-labeled data presents an intriguing research topic. Drawing inspiration from the alignment process observed in intelligent organisms, where declarative memory plays a pivotal role in summarizing past experiences, we propose a novel learning framework. The agents adeptly distill insights from past experiences, refining and updating existing notes to enhance their performance in the environment. This entire process transpires within the memory components and is implemented through natural language, so we character this framework as In-memory Learning. We also delve into the key features of benchmarks designed to evaluate the self-improvement process. Through systematic experiments, we demonstrate the effectiveness of our framework and provide insights into this problem.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;RolE Prompting Guided Multi-Domain Adaptation (REGA)&#31574;&#30053;&#65292;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#31649;&#29702;&#22810;&#39046;&#22495;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#25105;&#33976;&#39311;&#12289;&#35282;&#33394;&#25552;&#31034;&#21644;&#35282;&#33394;&#38598;&#25104;&#36825;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.02756</link><description>&lt;p&gt;
&#36890;&#36807;&#35282;&#33394;&#25552;&#31034;&#25351;&#23548;&#30340;&#36890;&#29992;&#33021;&#21147;&#20445;&#30041;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02756
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;RolE Prompting Guided Multi-Domain Adaptation (REGA)&#31574;&#30053;&#65292;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#31649;&#29702;&#22810;&#39046;&#22495;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#30340;&#26032;&#26041;&#27861;&#65292;&#21253;&#25324;&#33258;&#25105;&#33976;&#39311;&#12289;&#35282;&#33394;&#25552;&#31034;&#21644;&#35282;&#33394;&#38598;&#25104;&#36825;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#19987;&#38376;&#39046;&#22495;&#26102;&#36973;&#36935;&#30340;&#20005;&#37325;&#36951;&#24536;&#38382;&#39064;&#65292;&#20197;&#21450;&#20026;&#22810;&#20010;&#39046;&#22495;&#26500;&#24314;&#22810;&#21151;&#33021;&#27169;&#22411;&#26102;&#20986;&#29616;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;RolE Prompting Guided Multi-Domain Adaptation (REGA) &#31574;&#30053;&#12290;&#36825;&#19968;&#26032;&#39062;&#26041;&#27861;&#36890;&#36807;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#26377;&#25928;&#31649;&#29702;&#22810;&#39046;&#22495;LLM&#36866;&#24212;&#65306;1&#65289;&#33258;&#25105;&#33976;&#39311;&#65288;Self-Distillation&#65289;&#26500;&#24314;&#21644;&#37325;&#25773;&#36890;&#29992;&#22495;&#23454;&#20363;&#20197;&#20943;&#36731;&#36951;&#24536;&#65307;2&#65289;&#35282;&#33394;&#25552;&#31034;&#65288;Role Prompting&#65289;&#20026;&#36890;&#29992;&#22495;&#20998;&#37197;&#20013;&#24515;&#25552;&#31034;&#21644;&#20026;&#27599;&#20010;&#29305;&#23450;&#39046;&#22495;&#20998;&#37197;&#29420;&#29305;&#35282;&#33394;&#25552;&#31034;&#65292;&#20197;&#26368;&#23567;&#21270;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#28151;&#28102;&#65307;3&#65289;&#35282;&#33394;&#38598;&#25104;&#65288;Role Integration&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02756v1 Announce Type: new  Abstract: The growing interest in Large Language Models (LLMs) for specialized applications has revealed a significant challenge: when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains. In response to these issues, we present the RolE Prompting Guided Multi-Domain Adaptation (REGA) strategy. This novel approach effectively manages multi-domain LLM adaptation through three key components: 1) Self-Distillation constructs and replays general-domain exemplars to alleviate catastrophic forgetting. 2) Role Prompting assigns a central prompt to the general domain and a unique role prompt to each specific domain to minimize inter-domain confusion during training. 3) Role Integration reu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24443;&#24213;&#37325;&#26657;&#20934;&#20559;&#22909;&#25968;&#25454;&#38598;&#20013;&#30340;&#20215;&#20540;&#35266;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#30340;&#38887;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02745</link><description>&lt;p&gt;
CURATRON&#65306;&#23436;&#25972;&#20581;&#22766;&#20559;&#22909;&#25968;&#25454;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20581;&#22766;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
CURATRON: Complete Robust Preference Data for Robust Alignment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24443;&#24213;&#37325;&#26657;&#20934;&#20559;&#22909;&#25968;&#25454;&#38598;&#20013;&#30340;&#20215;&#20540;&#35266;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35299;&#20915;&#20102;&#36890;&#36807;&#20559;&#22909;&#23398;&#20064;&#65288;PL&#65289;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#25361;&#25112;&#65292;&#37325;&#28857;&#20851;&#27880;&#20559;&#22909;&#25968;&#25454;&#38598;&#20013;&#19981;&#23436;&#25972;&#21644;&#25439;&#22351;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24443;&#24213;&#21644;&#23436;&#20840;&#22320;&#37325;&#26032;&#26657;&#20934;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#30340;&#20215;&#20540;&#35266;&#65292;&#20197;&#22686;&#24378;LLMs&#23545;&#38382;&#39064;&#30340;&#38887;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#20445;&#35777;&#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#25490;&#21517;&#31639;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;&#20960;&#31181;&#29616;&#26377;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#27604;&#22914;&#32463;&#20856;&#30340;Bradley&#8211;Terry&#8211;Luce&#65288;BTL&#65289;&#65288;Bradley&#21644;Terry&#65292;1952&#65289;&#27169;&#22411;&#20197;&#21450;&#23545;&#20854;&#26576;&#20123;&#25512;&#24191;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#25552;&#20986;&#19968;&#31181;&#21487;&#35777;&#26126;&#22312;&#39640;&#27010;&#29575;&#19979;&#24674;&#22797;{\epsilon}-&#26368;&#20248;&#25490;&#24207;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#20801;&#35768;&#27599;&#20010;&#27169;&#22411;&#21709;&#24212;&#22810;&#36798;O(n)&#25200;&#21160;&#30340;&#25104;&#23545;&#27604;&#36739;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#37096;&#20998;&#35266;&#23519;&#35774;&#32622;&#19979;&#30340;&#20581;&#22766;&#24674;&#22797;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02745v1 Announce Type: new  Abstract: This paper addresses the challenges of aligning large language models (LLMs) with human values via preference learning (PL), with a focus on the issues of incomplete and corrupted data in preference datasets. We propose a novel method for robustly and completely recalibrating values within these datasets to enhance LLMs resilience against the issues. In particular, we devise a guaranteed polynomial time ranking algorithm that robustifies several existing models, such as the classic Bradley--Terry--Luce (BTL) (Bradley and Terry, 1952) model and certain generalizations of it. To the best of our knowledge, our present work is the first to propose an algorithm that provably recovers an {\epsilon}-optimal ranking with high probability while allowing as large as O(n) perturbed pairwise comparison results per model response. Furthermore, we show robust recovery results in the partially observed setting. Our experiments confirm that our algorith
&lt;/p&gt;</description></item><item><title>Hypnos&#36890;&#36807;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#21644;&#37319;&#29992;&#30001;&#19968;&#33324;&#21040;&#29305;&#23450;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20026;&#38754;&#21521;&#40635;&#37257;&#23398;&#30340;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#35774;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.02742</link><description>&lt;p&gt;
&#38754;&#21521;&#40635;&#37257;&#23398;&#30340;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Towards Training A Chinese Large Language Model for Anesthesiology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02742
&lt;/p&gt;
&lt;p&gt;
Hypnos&#36890;&#36807;&#25913;&#21892;&#25968;&#25454;&#36136;&#37327;&#21644;&#37319;&#29992;&#30001;&#19968;&#33324;&#21040;&#29305;&#23450;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#20026;&#38754;&#21521;&#40635;&#37257;&#23398;&#30340;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#35774;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#26174;&#33879;&#30340;&#23454;&#29992;&#20215;&#20540;&#65292;&#21307;&#23398;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#38598;&#20013;&#22312;&#19968;&#33324;&#21307;&#23398;&#39046;&#22495;&#65292;&#38656;&#35201;&#28145;&#20837;&#30740;&#31350;&#29305;&#23450;&#39046;&#22495;&#22914;&#40635;&#37257;&#23398;&#20013;&#30340;LLMs&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Hypnos&#65292;&#19968;&#31181;&#22522;&#20110;&#29616;&#26377;LLMs&#65288;&#22914;Llama&#65289;&#26500;&#24314;&#30340;&#20013;&#25991;&#40635;&#37257;&#27169;&#22411;&#12290;Hypnos&#30340;&#36129;&#29486;&#21253;&#25324;&#19977;&#20010;&#26041;&#38754;&#65306;1&#65289;&#25968;&#25454;&#65292;&#20363;&#22914;&#26469;&#33258;&#24403;&#21069;LLMs&#30340;Self-Instruct&#33719;&#24471;&#30340;&#25968;&#25454;&#21487;&#33021;&#23384;&#22312;&#19981;&#20934;&#30830;&#24615;&#12290;Hypnos&#23454;&#26045;&#20102;&#20132;&#21449;&#36807;&#28388;&#31574;&#30053;&#26469;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;&#35813;&#31574;&#30053;&#28041;&#21450;&#20351;&#29992;&#19968;&#20010;LLM&#26469;&#35780;&#20272;&#21478;&#19968;&#20010;LLM&#29983;&#25104;&#30340;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#24182;&#36807;&#28388;&#20986;&#36136;&#37327;&#36739;&#20302;&#30340;&#25968;&#25454;&#12290;2&#65289;Hypnos&#37319;&#29992;&#20102;&#19968;&#31181;&#30001;&#19968;&#33324;&#21040;&#29305;&#23450;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#21363;&#36890;&#36807;&#20351;&#29992;&#19968;&#33324;&#21307;&#23398;&#25968;&#25454;&#24494;&#35843;LLMs&#65292;&#28982;&#21518;&#20877;&#20351;&#29992;&#26469;&#33258;&#40635;&#37257;&#23398;&#30340;&#25968;&#25454;&#36827;&#19968;&#27493;&#25913;&#36827;&#24494;&#35843;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02742v1 Announce Type: new  Abstract: Medical large language models (LLMs) have gained popularity recently due to their significant practical utility. However, most existing research focuses on general medicine, and there is a need for in-depth study of LLMs in specific fields like anesthesiology. To fill the gap, we introduce Hypnos, a Chinese Anesthesia model built upon existing LLMs, e.g., Llama. Hypnos' contributions have three aspects: 1) The data, such as utilizing Self-Instruct, acquired from current LLMs likely includes inaccuracies. Hypnos implements a cross-filtering strategy to improve the data quality. This strategy involves using one LLM to assess the quality of the generated data from another LLM and filtering out the data with low quality. 2) Hypnos employs a general-to-specific training strategy that starts by fine-tuning LLMs using the general medicine data and subsequently improving the fine-tuned LLMs using data specifically from Anesthesiology. The genera
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22240;&#26524;&#24341;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#21069;&#38376;&#35843;&#25972;&#26377;&#25928;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.02738</link><description>&lt;p&gt;
&#22240;&#26524;&#24341;&#23548;&#65306;&#22522;&#20110;&#21069;&#38376;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21551;&#21457;&#24335;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Causal Prompting: Debiasing Large Language Model Prompting based on Front-Door Adjustment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02738
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22240;&#26524;&#24341;&#23548;&#26041;&#27861;&#65292;&#36890;&#36807;&#21069;&#38376;&#35843;&#25972;&#26377;&#25928;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#26377;&#30340;&#35832;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#24605;&#32500;&#38142;&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21551;&#21457;&#24335;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#23601;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#21508;&#31181;&#20559;&#35265;&#25361;&#25112;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#21551;&#21457;&#24335;&#26041;&#27861;&#32972;&#21518;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21069;&#38376;&#35843;&#25972;&#30340;&#26032;&#22411;&#22240;&#26524;&#24341;&#23548;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#20943;&#36731;LLMs&#30340;&#20559;&#35265;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#32780;&#26080;&#38656;&#35775;&#38382;LLMs&#30340;&#21442;&#25968;&#21644;logit&#26469;&#23454;&#26045;&#22240;&#26524;&#24178;&#39044;&#12290;&#30001;LLMs&#29983;&#25104;&#30340;&#24605;&#32500;&#38142;&#34987;&#29992;&#20316;&#20013;&#20171;&#21464;&#37327;&#65292;&#36890;&#36807;&#21069;&#38376;&#35745;&#31639;&#36755;&#20837;&#25552;&#31034;&#19982;&#36755;&#20986;&#31572;&#26696;&#20043;&#38388;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02738v1 Announce Type: new  Abstract: Despite the significant achievements of existing prompting methods such as in-context learning and chain-of-thought for large language models (LLMs), they still face challenges of various biases. Traditional debiasing methods primarily focus on the model training stage, including data augmentation-based and reweight-based approaches, with the limitations of addressing the complex biases of LLMs. To address such limitations, the causal relationship behind the prompting methods is uncovered using a structural causal model, and a novel causal prompting method based on front-door adjustment is proposed to effectively mitigate the bias of LLMs. In specific, causal intervention is implemented by designing the prompts without accessing the parameters and logits of LLMs.The chain-of-thoughts generated by LLMs are employed as the mediator variable and the causal effect between the input prompt and the output answers is calculated through front-do
&lt;/p&gt;</description></item><item><title>HARGPT&#30740;&#31350;&#34920;&#26126;LLMs&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#25552;&#31034;&#29702;&#35299;&#21407;&#22987;IMU&#25968;&#25454;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#26368;&#20808;&#36827;&#28145;&#24230;&#20998;&#31867;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.02727</link><description>&lt;p&gt;
HARGPT&#65306;LLMs&#26159;&#21542;&#21487;&#20197;&#36827;&#34892;&#38646;&#26679;&#26412;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65311;
&lt;/p&gt;
&lt;p&gt;
HARGPT: Are LLMs Zero-Shot Human Activity Recognizers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02727
&lt;/p&gt;
&lt;p&gt;
HARGPT&#30740;&#31350;&#34920;&#26126;LLMs&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#25552;&#31034;&#29702;&#35299;&#21407;&#22987;IMU&#25968;&#25454;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#26368;&#20808;&#36827;&#28145;&#24230;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#19982;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#65288;CPS&#65289;&#26080;&#32541;&#38598;&#25104;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#35299;&#37322;&#29289;&#29702;&#19990;&#30028;&#26041;&#38754;&#30340;&#28508;&#21147;&#23384;&#22312;&#25345;&#32493;&#30340;&#20105;&#35770;&#12290;&#26412;&#25991;&#36890;&#36807;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#26469;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;LLMs&#26159;&#21542;&#33021;&#22815;&#36827;&#34892;&#38646;&#26679;&#26412;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;HARGPT&#65292;&#36890;&#36807;&#23637;&#31034;LLMs&#21487;&#20197;&#29702;&#35299;&#21407;&#22987;IMU&#25968;&#25454;&#24182;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#25191;&#34892;HAR&#20219;&#21153;&#65292;&#20165;&#38656;&#36866;&#24403;&#30340;&#25552;&#31034;&#65292;&#32943;&#23450;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;HARGPT&#23558;&#21407;&#22987;IMU&#25968;&#25454;&#36755;&#20837;LLMs&#65292;&#24182;&#21033;&#29992;&#35282;&#33394;&#25198;&#28436;&#21644;&#36880;&#27493;&#24605;&#32771;&#31574;&#30053;&#36827;&#34892;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;GPT4&#19978;&#23545;HARGPT&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20351;&#29992;&#20102;&#20004;&#20010;&#19981;&#21516;&#31867;&#38388;&#30456;&#20284;&#24615;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#24182;&#27604;&#36739;&#20102;&#22522;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21644;&#26368;&#20808;&#36827;&#28145;&#24230;&#20998;&#31867;&#27169;&#22411;&#30340;&#21508;&#31181;&#22522;&#32447;&#12290;&#26174;&#33879;&#22320;&#65292;LLMs&#25104;&#21151;&#22320;&#20174;&#21407;&#22987;IMU&#25968;&#25454;&#20013;&#35782;&#21035;&#20154;&#31867;&#27963;&#21160;&#65292;&#24182;&#22987;&#32456;&#20248;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02727v1 Announce Type: cross  Abstract: There is an ongoing debate regarding the potential of Large Language Models (LLMs) as foundational models seamlessly integrated with Cyber-Physical Systems (CPS) for interpreting the physical world. In this paper, we carry out a case study to answer the following question: Are LLMs capable of zero-shot human activity recognition (HAR). Our study, HARGPT, presents an affirmative answer by demonstrating that LLMs can comprehend raw IMU data and perform HAR tasks in a zero-shot manner, with only appropriate prompts. HARGPT inputs raw IMU data into LLMs and utilizes the role-play and think step-by-step strategies for prompting. We benchmark HARGPT on GPT4 using two public datasets of different inter-class similarities and compare various baselines both based on traditional machine learning and state-of-the-art deep classification models. Remarkably, LLMs successfully recognize human activities from raw IMU data and consistently outperform 
&lt;/p&gt;</description></item><item><title>DP-CRE&#26694;&#26550;&#36890;&#36807;&#20998;&#31163;&#23545;&#27604;&#23398;&#20064;&#21644;&#35760;&#24518;&#32467;&#26500;&#20445;&#30041;&#30340;&#26041;&#24335;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#25345;&#32493;&#20851;&#31995;&#25277;&#21462;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#22312;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#25361;&#25112;&#26102;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.02718</link><description>&lt;p&gt;
DP-CRE: &#36890;&#36807;&#20998;&#31163;&#23545;&#27604;&#23398;&#20064;&#21644;&#35760;&#24518;&#32467;&#26500;&#20445;&#30041;&#36827;&#34892;&#25345;&#32493;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
DP-CRE: Continual Relation Extraction via Decoupled Contrastive Learning and Memory Structure Preservation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02718
&lt;/p&gt;
&lt;p&gt;
DP-CRE&#26694;&#26550;&#36890;&#36807;&#20998;&#31163;&#23545;&#27604;&#23398;&#20064;&#21644;&#35760;&#24518;&#32467;&#26500;&#20445;&#30041;&#30340;&#26041;&#24335;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#25345;&#32493;&#20851;&#31995;&#25277;&#21462;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#22312;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#25361;&#25112;&#26102;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#20851;&#31995;&#25277;&#21462;&#65288;CRE&#65289;&#26088;&#22312;&#20174;&#38750;&#38745;&#24577;&#25968;&#25454;&#27969;&#20013;&#36880;&#27493;&#23398;&#20064;&#20851;&#31995;&#30693;&#35782;&#12290;&#30001;&#20110;&#24341;&#20837;&#26032;&#30340;&#20851;&#31995;&#20219;&#21153;&#21487;&#33021;&#20250;&#25513;&#30422;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#25104;&#20026;&#35813;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#24403;&#21069;&#22522;&#20110;&#37325;&#25773;&#30340;&#35757;&#32451;&#33539;&#24335;&#20248;&#20808;&#32771;&#34385;&#25152;&#26377;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#22810;&#36718;&#35757;&#32451;&#20869;&#23384;&#26679;&#26412;&#65292;&#36825;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#32769;&#20219;&#21153;&#21644;&#23545;&#26032;&#20219;&#21153;&#30340;&#26174;&#30528;&#20559;&#24046;&#65292;&#22240;&#20026;&#37325;&#25773;&#38598;&#21512;&#30340;&#19981;&#24179;&#34913;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DecouPled CRE&#65288;DP-CRE&#65289;&#26694;&#26550;&#65292;&#23558;&#20808;&#21069;&#20449;&#24687;&#20445;&#30041;&#21644;&#26032;&#30693;&#35782;&#33719;&#21462;&#30340;&#36807;&#31243;&#20998;&#31163;&#12290;&#35813;&#26694;&#26550;&#26816;&#26597;&#23884;&#20837;&#31354;&#38388;&#38543;&#30528;&#26032;&#20851;&#31995;&#31867;&#21035;&#30340;&#20986;&#29616;&#32780;&#21457;&#29983;&#30340;&#21464;&#21270;&#65292;&#26126;&#26174;&#31649;&#29702;&#30693;&#35782;&#30340;&#20445;&#23384;&#21644;&#33719;&#21462;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;DP-CRE&#22312;&#36328;&#36234;t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02718v1 Announce Type: new  Abstract: Continuous Relation Extraction (CRE) aims to incrementally learn relation knowledge from a non-stationary stream of data. Since the introduction of new relational tasks can overshadow previously learned information, catastrophic forgetting becomes a significant challenge in this domain. Current replay-based training paradigms prioritize all data uniformly and train memory samples through multiple rounds, which would result in overfitting old tasks and pronounced bias towards new tasks because of the imbalances of the replay set. To handle the problem, we introduce the DecouPled CRE (DP-CRE) framework that decouples the process of prior information preservation and new knowledge acquisition. This framework examines alterations in the embedding space as new relation classes emerge, distinctly managing the preservation and acquisition of knowledge. Extensive experiments show that DP-CRE significantly outperforms other CRE baselines across t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;LLMs&#22312;&#36234;&#21335;&#35821;&#19978;&#36827;&#34892;&#24494;&#35843;&#21644;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#22312;&#36234;&#21335;&#35821;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#33021;&#21147;&#65292;&#21516;&#26102;&#25351;&#20986;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#19982;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#30528;&#26435;&#34913;&#20851;&#31995;&#65292;&#35757;&#32451;&#25110;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#26159;&#24433;&#21709;LLM&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.02715</link><description>&lt;p&gt;
&#36328;&#36234;&#35821;&#35328;&#35270;&#37326;&#65306;&#36234;&#21335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Crossing Linguistic Horizons: Finetuning and Comprehensive Evaluation of Vietnamese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02715
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23545;LLMs&#22312;&#36234;&#21335;&#35821;&#19978;&#36827;&#34892;&#24494;&#35843;&#21644;&#32508;&#21512;&#35780;&#20272;&#65292;&#21457;&#29616;&#24494;&#35843;&#21518;&#30340;&#27169;&#22411;&#22312;&#36234;&#21335;&#35821;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#33021;&#21147;&#65292;&#21516;&#26102;&#25351;&#20986;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#19982;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#30528;&#26435;&#34913;&#20851;&#31995;&#65292;&#35757;&#32451;&#25110;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#26159;&#24433;&#21709;LLM&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24378;&#35843;&#20102;&#23427;&#20204;&#22312;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#39044;&#35757;&#32451;&#65292;&#20294;&#30446;&#21069;&#24320;&#28304;&#30340;LLMs&#22312;&#22788;&#29702;&#36234;&#21335;&#35821;&#26041;&#38754;&#30340;&#25928;&#26524;&#26377;&#38480;&#12290;&#36825;&#19968;&#25361;&#25112;&#26159;&#30001;&#20110;&#32570;&#20047;&#19987;&#38376;&#38024;&#23545;&#36234;&#21335;&#35821;LLM&#35780;&#20272;&#30340;&#31995;&#32479;&#21270;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#25351;&#26631;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#19987;&#38376;&#20026;&#36234;&#21335;&#35821;&#36827;&#34892;&#20102;LLM&#30340;&#24494;&#35843;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#28085;&#30422;10&#20010;&#24120;&#35265;&#20219;&#21153;&#21644;31&#20010;&#25351;&#26631;&#30340;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;LLMs&#22312;&#36234;&#21335;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#22686;&#24378;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#20855;&#26377;&#26356;&#22810;&#21442;&#25968;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#24102;&#26469;&#26356;&#22810;&#30340;&#20559;&#35265;&#21644;&#26410;&#26657;&#20934;&#30340;&#36755;&#20986;&#65292;&#32780;&#24433;&#21709;LLM&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#35757;&#32451;&#25110;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02715v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have underscored their importance in the evolution of artificial intelligence. However, despite extensive pretraining on multilingual datasets, available open-sourced LLMs exhibit limited effectiveness in processing Vietnamese. The challenge is exacerbated by the absence of systematic benchmark datasets and metrics tailored for Vietnamese LLM evaluation. To mitigate these issues, we have finetuned LLMs specifically for Vietnamese and developed a comprehensive evaluation framework encompassing 10 common tasks and 31 metrics. Our evaluation results reveal that the fine-tuned LLMs exhibit enhanced comprehension and generative capabilities in Vietnamese. Moreover, our analysis indicates that models with more parameters can introduce more biases and uncalibrated outputs and the key factor influencing LLM performance is the quality of the training or fine-tuning datasets. These insights und
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoAT&#30340;Chain-of-Action-Thought&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#20808;&#21069;&#21160;&#20316;&#25551;&#36848;&#12289;&#24403;&#21069;&#23631;&#24149;&#24773;&#20917;&#20197;&#21450;&#26410;&#26469;&#21160;&#20316;&#24605;&#32771;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26234;&#33021;&#25163;&#26426;GUI&#20195;&#29702;&#30340;&#20219;&#21153;&#25191;&#34892;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.02713</link><description>&lt;p&gt;
Android&#22312;&#21160;&#29289;&#22253;&#20013;: GUI&#20195;&#29702;&#30340;&#21160;&#20316;&#24605;&#32500;&#38142;
&lt;/p&gt;
&lt;p&gt;
Android in the Zoo: Chain-of-Action-Thought for GUI Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02713
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CoAT&#30340;Chain-of-Action-Thought&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#20808;&#21069;&#21160;&#20316;&#25551;&#36848;&#12289;&#24403;&#21069;&#23631;&#24149;&#24773;&#20917;&#20197;&#21450;&#26410;&#26469;&#21160;&#20316;&#24605;&#32771;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26234;&#33021;&#25163;&#26426;GUI&#20195;&#29702;&#30340;&#20219;&#21153;&#25191;&#34892;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23548;&#33268;&#26234;&#33021;&#25163;&#26426;&#19978;&#30340;&#22823;&#37327;&#33258;&#20027;GUI&#20195;&#29702;&#28608;&#22686;&#65292;&#36825;&#20123;&#20195;&#29702;&#36890;&#36807;&#39044;&#27979;API&#30340;&#19968;&#31995;&#21015;&#21160;&#20316;&#26469;&#23436;&#25104;&#30001;&#33258;&#28982;&#35821;&#35328;&#35302;&#21457;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#35813;&#20219;&#21153;&#39640;&#24230;&#20381;&#36182;&#20110;&#36807;&#21435;&#30340;&#21160;&#20316;&#21644;&#35270;&#35273;&#35266;&#23519;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#24456;&#23569;&#32771;&#34385;&#20013;&#38388;&#25130;&#22270;&#21644;&#23631;&#24149;&#25805;&#20316;&#20256;&#36882;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#20316;&#24605;&#32500;&#38142;&#65288;CoAT&#65289;&#65292;&#23427;&#32771;&#34385;&#20102;&#20808;&#21069;&#21160;&#20316;&#30340;&#25551;&#36848;&#12289;&#24403;&#21069;&#23631;&#24149;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#20998;&#26512;&#24212;&#24403;&#25191;&#34892;&#30340;&#21160;&#20316;&#20197;&#21450;&#36873;&#25321;&#30340;&#21160;&#20316;&#24102;&#26469;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20351;&#29992;&#29616;&#25104;LLM&#36827;&#34892;&#38646;&#27425;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;CoAT&#30456;&#27604;&#20110;&#26631;&#20934;&#19978;&#19979;&#25991;&#24314;&#27169;&#26174;&#33879;&#25552;&#39640;&#20102;&#30446;&#26631;&#30340;&#23436;&#25104;&#24773;&#20917;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20419;&#36827;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Android-In-The-Zoo&#65288;AitZ&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;18,643&#20010;&#23631;&#24149;&#21160;&#20316;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02713v1 Announce Type: new  Abstract: Large language model (LLM) leads to a surge of autonomous GUI agents for smartphone, which completes a task triggered by natural language through predicting a sequence of actions of API. Even though the task highly relies on past actions and visual observations, existing studies typical consider little semantic information carried out by intermediate screenshots and screen operations. To address this, this work presents Chain-of-Action-Thought (dubbed CoAT), which takes the description of the previous actions, the current screen, and more importantly the action thinking of what actions should be performed and the outcomes led by the chosen action. We demonstrate that, in a zero-shot setting upon an off-the-shell LLM, CoAT significantly improves the goal progress compared to standard context modeling. To further facilitate the research in this line, we construct a benchmark Android-In-The-Zoo (AitZ), which contains 18,643 screen-action pa
&lt;/p&gt;</description></item><item><title>Breeze-7B&#26159;&#22522;&#20110;Mistral-7B&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#25913;&#21892;&#20013;&#25991;&#35821;&#22659;&#19979;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#21151;&#33021;&#65292;&#23637;&#29616;&#20986;&#22312;&#22797;&#26434;&#24230;&#31867;&#21035;&#20013;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02712</link><description>&lt;p&gt;
Breeze-7B&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Breeze-7B Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02712
&lt;/p&gt;
&lt;p&gt;
Breeze-7B&#26159;&#22522;&#20110;Mistral-7B&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#25913;&#21892;&#20013;&#25991;&#35821;&#22659;&#19979;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#21151;&#33021;&#65292;&#23637;&#29616;&#20986;&#22312;&#22797;&#26434;&#24230;&#31867;&#21035;&#20013;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02712v1 &#31867;&#22411;&#65306;&#26032;&#35770; &#25688;&#35201;&#65306;Breeze-7B&#26159;&#19968;&#20010;&#22522;&#20110;Mistral-7B&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#26088;&#22312;&#25913;&#21892;&#20013;&#25991;&#20256;&#32479;&#35821;&#22659;&#19979;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#21151;&#33021;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#27010;&#36848;&#20102;Breeze-7B&#27169;&#22411;&#30340;&#39069;&#22806;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#35780;&#20272;&#38454;&#27573;&#12290;Breeze-7B&#31995;&#21015;&#30340;&#22522;&#30784;&#21644;&#32842;&#22825;&#27169;&#22411;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#22312;&#19968;&#20123;&#19982;&#20854;&#22797;&#26434;&#24230;&#31867;&#20284;&#30340;&#27169;&#22411;&#20013;&#36798;&#21040;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02712v1 Announce Type: new  Abstract: Breeze-7B is an open-source language model based on Mistral-7B, designed to address the need for improved language comprehension and chatbot-oriented capabilities in Traditional Chinese. This technical report provides an overview of the additional pretraining, finetuning, and evaluation stages for the Breeze-7B model. The Breeze-7B family of base and chat models exhibits good performance on language comprehension and chatbot-oriented tasks, reaching the top in several benchmarks among models comparable in its complexity class.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;Causal Walk&#65292;&#20174;&#22240;&#26524;&#35282;&#24230;&#21033;&#29992;&#21069;&#38376;&#35843;&#25972;&#28040;&#38500;&#22810;&#36339;&#20107;&#23454;&#39564;&#35777;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.02698</link><description>&lt;p&gt;
&#22240;&#26524;&#20043;&#34892;&#65306;&#21033;&#29992;&#21069;&#38376;&#35843;&#25972;&#28040;&#38500;&#22810;&#36339;&#20107;&#23454;&#39564;&#35777;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Causal Walk: Debiasing Multi-Hop Fact Verification with Front-Door Adjustment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02698
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;Causal Walk&#65292;&#20174;&#22240;&#26524;&#35282;&#24230;&#21033;&#29992;&#21069;&#38376;&#35843;&#25972;&#28040;&#38500;&#22810;&#36339;&#20107;&#23454;&#39564;&#35777;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22810;&#36339;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#20110;&#27880;&#37322;&#24037;&#20214;&#20013;&#30340;&#20598;&#28982;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#22312;&#26080;&#20559;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#21508;&#31181;&#21435;&#20559;&#20316;&#21697;&#20013;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#22240;&#25191;&#34892;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#21435;&#20559;&#25805;&#20316;&#65288;&#22914;&#24178;&#39044;&#25110;&#23545;&#31435;&#25512;&#29702;&#65289;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#20027;&#35201;&#23558;&#20107;&#23454;&#39564;&#35777;&#34920;&#36848;&#20026;&#35299;&#20915;&#32932;&#27973;&#20559;&#35265;&#27169;&#24335;&#30340;&#21333;&#36339;&#25512;&#29702;&#20219;&#21153;&#65292;&#26080;&#27861;&#22788;&#29702;&#38544;&#34255;&#22312;&#22810;&#20010;&#35777;&#25454;&#36339;&#36291;&#20013;&#30340;&#22797;&#26434;&#20559;&#35265;&#27169;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#20043;&#34892;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#22240;&#26524;&#35282;&#24230;&#36827;&#34892;&#21069;&#38376;&#35843;&#25972;&#30340;&#29992;&#20110;&#28040;&#38500;&#22810;&#36339;&#20107;&#23454;&#39564;&#35777;&#20559;&#35265;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02698v1 Announce Type: new  Abstract: Conventional multi-hop fact verification models are prone to rely on spurious correlations from the annotation artifacts, leading to an obvious performance decline on unbiased datasets. Among the various debiasing works, the causal inference-based methods become popular by performing theoretically guaranteed debiasing such as casual intervention or counterfactual reasoning. However, existing causal inference-based debiasing methods, which mainly formulate fact verification as a single-hop reasoning task to tackle shallow bias patterns, cannot deal with the complicated bias patterns hidden in multiple hops of evidence. To address the challenge, we propose Causal Walk, a novel method for debiasing multi-hop fact verification from a causal perspective with front-door adjustment. Specifically, in the structural causal model, the reasoning path between the treatment (the input claim-evidence graph) and the outcome (the veracity label) is intr
&lt;/p&gt;</description></item><item><title>MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.02694</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#24863;&#30693;&#35821;&#20041;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Privacy-Aware Semantic Cache for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02694
&lt;/p&gt;
&lt;p&gt;
MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#12289;Google Bard&#12289;Claude&#21644;Llama 2&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25628;&#32034;&#24341;&#25806;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36896;&#25104;&#20102;&#24322;&#24120;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MeanCache&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#20197;&#30830;&#23450;&#32531;&#23384;&#21629;&#20013;&#25110;&#26410;&#21629;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02694v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2 have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters and inference on these models also demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries, leading to unacceptable false hit-and-miss rates.   This paper introduces MeanCache, a semantic cache for LLMs that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. MeanCache lever
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;InjecAgent&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#24037;&#20855;&#38598;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#23545;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;30&#31181;LLM&#20195;&#29702;&#65292;&#21457;&#29616;&#36825;&#20123;&#20195;&#29702;&#23384;&#22312;&#28431;&#27934;</title><link>https://arxiv.org/abs/2403.02691</link><description>&lt;p&gt;
InjecAgent&#65306;&#22522;&#20110;&#24037;&#20855;&#38598;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Agent&#20013;&#30340;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated Large Language Model Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;InjecAgent&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#24037;&#20855;&#38598;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#23545;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;30&#31181;LLM&#20195;&#29702;&#65292;&#21457;&#29616;&#36825;&#20123;&#20195;&#29702;&#23384;&#22312;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24037;&#20316;&#23558;LLMs&#20316;&#20026;&#20195;&#29702;&#20307;&#29616;&#20986;&#26469;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#35775;&#38382;&#24037;&#20855;&#65292;&#25191;&#34892;&#25805;&#20316;&#65292;&#24182;&#19982;&#22806;&#37096;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#30005;&#23376;&#37038;&#20214;&#25110;&#32593;&#31449;&#65289;&#36827;&#34892;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22806;&#37096;&#20869;&#23481;&#24341;&#20837;&#20102;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#65288;IPI&#65289;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#24694;&#24847;&#25351;&#20196;&#34987;&#23884;&#20837;LLMs&#22788;&#29702;&#30340;&#20869;&#23481;&#20013;&#65292;&#26088;&#22312;&#25805;&#32437;&#36825;&#20123;&#20195;&#29702;&#25191;&#34892;&#23545;&#29992;&#25143;&#26377;&#23475;&#30340;&#25805;&#20316;&#12290;&#32771;&#34385;&#21040;&#36825;&#31867;&#25915;&#20987;&#30340;&#28508;&#22312;&#20005;&#37325;&#21518;&#26524;&#65292;&#24314;&#31435;&#29992;&#20110;&#35780;&#20272;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#22522;&#20934;&#27979;&#35797;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;InjecAgent&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#24037;&#20855;&#38598;&#25104;&#30340;LLM&#20195;&#29702;&#23545;IPI&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;InjecAgent&#21253;&#25324;1,054&#20010;&#27979;&#35797;&#29992;&#20363;&#65292;&#28085;&#30422;17&#31181;&#19981;&#21516;&#30340;&#29992;&#25143;&#24037;&#20855;&#21644;62&#31181;&#25915;&#20987;&#32773;&#24037;&#20855;&#12290;&#25105;&#20204;&#23558;&#25915;&#20987;&#24847;&#22270;&#20998;&#20026;&#20004;&#31181;&#20027;&#35201;&#31867;&#22411;&#65306;&#23545;&#29992;&#25143;&#36896;&#25104;&#30452;&#25509;&#20260;&#23475;&#21644;&#31363;&#21462;&#31169;&#20154;&#25968;&#25454;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;30&#31181;&#19981;&#21516;&#30340;LLM&#20195;&#29702;&#65292;&#24182;&#34920;&#26126;&#36825;&#20123;&#20195;&#29702;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02691v1 Announce Type: new  Abstract: Recent work has embodied LLMs as agents, allowing them to access tools, perform actions, and interact with external content (e.g., emails or websites). However, external content introduces the risk of indirect prompt injection (IPI) attacks, where malicious instructions are embedded within the content processed by LLMs, aiming to manipulate these agents into executing detrimental actions against users. Given the potentially severe consequences of such attacks, establishing benchmarks to assess and mitigate these risks is imperative.   In this work, we introduce InjecAgent, a benchmark designed to assess the vulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent comprises 1,054 test cases covering 17 different user tools and 62 attacker tools. We categorize attack intentions into two primary types: direct harm to users and exfiltration of private data. We evaluate 30 different LLM agents and show that agents are vulnerable
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32454;&#35843;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#12289;&#26356;&#20840;&#38754;&#22320;&#36807;&#28388;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02677</link><description>&lt;p&gt;
&#32454;&#35843;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#26159;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#36807;&#28388;&#22120;
&lt;/p&gt;
&lt;p&gt;
Finetuned Multimodal Language Models Are High-Quality Image-Text Data Filters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02677
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32454;&#35843;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#12289;&#26356;&#20840;&#38754;&#22320;&#36807;&#28388;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#32463;&#36807;&#33391;&#22909;&#35843;&#25972;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#26469;&#36807;&#28388;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#25972;&#21512;MLMs&#30340;&#26368;&#26032;&#36827;&#23637;&#32988;&#36807;&#20027;&#27969;&#36807;&#28388;&#26041;&#27861;&#65288;&#20363;&#22914;CLIPScore&#65289;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22235;&#20010;&#29420;&#29305;&#32780;&#20114;&#34917;&#30340;&#25351;&#26631;&#26469;&#20840;&#38754;&#34913;&#37327;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#27969;&#31243;&#26469;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#25968;&#25454;&#65292;&#29992;&#20110;&#32454;&#35843;MLMs&#20316;&#20026;&#25968;&#25454;&#36807;&#28388;&#22120;&#12290;&#19982;CLIPScore&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;MLM&#36807;&#28388;&#22120;&#20135;&#29983;&#26356;&#31934;&#30830;&#12289;&#26356;&#20840;&#38754;&#30340;&#20998;&#25968;&#65292;&#30452;&#25509;&#25552;&#39640;&#20102;&#36807;&#28388;&#25968;&#25454;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#21319;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#27969;&#34892;&#30340;&#22522;&#30784;&#27169;&#22411;&#65288;&#21363;CLIP&#21644;BLIP2&#65289;&#21644;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;CLIPScore&#12290;&#25105;&#20204;&#30340;MLM&#36807;&#28388;&#22120;&#21487;&#20197;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#27169;&#22411;&#21644;&#20219;&#21153;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;CLIPScore&#30340;&#21363;&#25554;&#21363;&#29992;&#26367;&#20195;&#21697;&#12290;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#28040;&#34701;&#30740;&#31350;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02677v1 Announce Type: cross  Abstract: We propose a novel framework for filtering image-text data by leveraging fine-tuned Multimodal Language Models (MLMs). Our approach outperforms predominant filtering methods (e.g., CLIPScore) via integrating the recent advances in MLMs. We design four distinct yet complementary metrics to holistically measure the quality of image-text data. A new pipeline is established to construct high-quality instruction data for fine-tuning MLMs as data filters. Comparing with CLIPScore, our MLM filters produce more precise and comprehensive scores that directly improve the quality of filtered data and boost the performance of pre-trained models. We achieve significant improvements over CLIPScore on popular foundation models (i.e., CLIP and BLIP2) and various downstream tasks. Our MLM filter can generalize to different models and tasks, and be used as a drop-in replacement for CLIPScore. An additional ablation study is provided to verify our design
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SEEDA&#65292;&#19968;&#20010;&#29992;&#20110;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#23545;12&#31181;&#26368;&#20808;&#36827;&#31995;&#32479;&#36827;&#34892;&#20803;&#35780;&#20272;&#30340;&#26657;&#27491;&#65292;&#36890;&#36807;&#22312;&#21477;&#23376;&#32423;&#21035;&#20803;&#35780;&#20272;&#20013;&#23545;&#31890;&#24230;&#36827;&#34892;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02674</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#30340;&#20803;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Revisiting Meta-evaluation for Grammatical Error Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02674
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;SEEDA&#65292;&#19968;&#20010;&#29992;&#20110;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#23545;12&#31181;&#26368;&#20808;&#36827;&#31995;&#32479;&#36827;&#34892;&#20803;&#35780;&#20272;&#30340;&#26657;&#27491;&#65292;&#36890;&#36807;&#22312;&#21477;&#23376;&#32423;&#21035;&#20803;&#35780;&#20272;&#20013;&#23545;&#31890;&#24230;&#36827;&#34892;&#23545;&#40784;&#65292;&#25552;&#39640;&#20102;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Metrics&#22312;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;GEC&#65289;&#20013;&#33258;&#21160;&#35780;&#20272;&#30340;&#22522;&#30784;&#12290;&#20854;&#20013;&#65292;&#20803;&#35780;&#20272;&#20381;&#36182;&#20110;&#23427;&#20204;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#33521;&#35821;GEC&#20013;&#24120;&#35268;&#30340;&#20803;&#35780;&#20272;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#30001;&#20110;&#35780;&#20272;&#31890;&#24230;&#19981;&#19968;&#33268;&#32780;&#23548;&#33268;&#30340;&#20559;&#35265;&#65292;&#20197;&#21450;&#20351;&#29992;&#20256;&#32479;&#31995;&#32479;&#30340;&#36807;&#26102;&#35774;&#32622;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SEEDA&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;GEC&#20803;&#35780;&#20272;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26657;&#27491;&#21644;&#20004;&#31181;&#19981;&#21516;&#31890;&#24230;&#65288;&#22522;&#20110;&#32534;&#36753;&#21644;&#22522;&#20110;&#21477;&#23376;&#65289;&#30340;&#20154;&#31867;&#35780;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02674v1 Announce Type: new  Abstract: Metrics are the foundation for automatic evaluation in grammatical error correction (GEC), with their evaluation of the metrics (meta-evaluation) relying on their correlation with human judgments. However, conventional meta-evaluations in English GEC encounter several challenges including biases caused by inconsistencies in evaluation granularity, and an outdated setup using classical systems. These problems can lead to misinterpretation of metrics and potentially hinder the applicability of GEC techniques. To address these issues, this paper proposes SEEDA, a new dataset for GEC meta-evaluation. SEEDA consists of corrections with human ratings along two different granularities: edit-based and sentence-based, covering 12 state-of-the-art systems including large language models (LLMs), and two human corrections with different focuses. The results of improved correlations by aligning the granularity in the sentence-level meta-evaluation, s
&lt;/p&gt;</description></item><item><title>FinReport&#26159;&#19968;&#20010;&#33258;&#21160;&#31995;&#32479;&#65292;&#36890;&#36807;&#37329;&#34701;&#26032;&#38395;&#20844;&#21578;&#21644;&#22810;&#22240;&#32032;&#27169;&#22411;&#26469;&#29983;&#25104;&#19987;&#19994;&#30340;&#32929;&#31080;&#30408;&#21033;&#39044;&#27979;&#25253;&#21578;&#65292;&#26088;&#22312;&#24110;&#21161;&#26222;&#36890;&#25237;&#36164;&#32773;&#25910;&#38598;&#20449;&#24687;&#12289;&#20998;&#26512;&#25968;&#25454;&#24182;&#29983;&#25104;&#25253;&#21578;&#12290;</title><link>https://arxiv.org/abs/2403.02647</link><description>&lt;p&gt;
FinReport: &#36890;&#36807;&#26032;&#38395;&#22240;&#32032;&#20998;&#26512;&#27169;&#22411;&#35299;&#37322;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#30408;&#21033;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
FinReport: Explainable Stock Earnings Forecasting via News Factor Analyzing Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02647
&lt;/p&gt;
&lt;p&gt;
FinReport&#26159;&#19968;&#20010;&#33258;&#21160;&#31995;&#32479;&#65292;&#36890;&#36807;&#37329;&#34701;&#26032;&#38395;&#20844;&#21578;&#21644;&#22810;&#22240;&#32032;&#27169;&#22411;&#26469;&#29983;&#25104;&#19987;&#19994;&#30340;&#32929;&#31080;&#30408;&#21033;&#39044;&#27979;&#25253;&#21578;&#65292;&#26088;&#22312;&#24110;&#21161;&#26222;&#36890;&#25237;&#36164;&#32773;&#25910;&#38598;&#20449;&#24687;&#12289;&#20998;&#26512;&#25968;&#25454;&#24182;&#29983;&#25104;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02647v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#30001;&#20110;&#23454;&#38469;&#24773;&#20917;&#19979;&#25237;&#36164;&#32773;&#30340;&#38656;&#27714;&#65292;&#32929;&#31080;&#30408;&#21033;&#39044;&#27979;&#36825;&#19968;&#20219;&#21153;&#21463;&#21040;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#19982;&#37329;&#34701;&#26426;&#26500;&#30456;&#27604;&#65292;&#26222;&#36890;&#25237;&#36164;&#32773;&#24456;&#38590;&#25366;&#25496;&#22240;&#32032;&#24182;&#20998;&#26512;&#26032;&#38395;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23613;&#31649;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20197;&#23545;&#35805;&#26426;&#22120;&#20154;&#30340;&#24418;&#24335;&#20026;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#65292;&#20294;&#20173;&#38656;&#35201;&#29992;&#25143;&#20855;&#22791;&#37329;&#34701;&#30693;&#35782;&#25552;&#20986;&#21512;&#29702;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#29992;&#25143;&#20307;&#39564;&#65292;&#25105;&#20204;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#33258;&#21160;&#31995;&#32479;&#65292;&#21517;&#20026;FinReport&#65292;&#20197;&#20415;&#26222;&#36890;&#25237;&#36164;&#32773;&#25910;&#38598;&#20449;&#24687;&#12289;&#20998;&#26512;&#20449;&#24687;&#65292;&#24182;&#22312;&#24635;&#32467;&#21518;&#29983;&#25104;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02647v1 Announce Type: cross  Abstract: The task of stock earnings forecasting has received considerable attention due to the demand investors in real-world scenarios. However, compared with financial institutions, it is not easy for ordinary investors to mine factors and analyze news. On the other hand, although large language models in the financial field can serve users in the form of dialogue robots, it still requires users to have financial knowledge to ask reasonable questions. To serve the user experience, we aim to build an automatic system, FinReport, for ordinary investors to collect information, analyze it, and generate reports after summarizing.   Specifically, our FinReport is based on financial news announcements and a multi-factor model to ensure the professionalism of the report. The FinReport consists of three modules: news factorization module, return forecasting module, risk assessment module. The news factorization module involves understanding news infor
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#20851;&#31995;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#28085;&#30422;&#20845;&#31181;&#19981;&#21516;&#31867;&#22411;&#32452;&#21512;&#20851;&#31995;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25193;&#23637;&#21040;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.02615</link><description>&lt;p&gt;
&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#20851;&#31995;&#25512;&#29702;&#20013;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Limitations of Large Language Models in Compositional Relation Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02615
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32452;&#21512;&#20851;&#31995;&#25512;&#29702;&#20013;&#30340;&#33021;&#21147;&#65292;&#35774;&#35745;&#20102;&#28085;&#30422;&#20845;&#31181;&#19981;&#21516;&#31867;&#22411;&#32452;&#21512;&#20851;&#31995;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25193;&#23637;&#21040;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#36890;&#36807;&#19968;&#20010;&#21253;&#21547;1,500&#20010;&#33521;&#25991;&#27979;&#35797;&#26696;&#20363;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#25512;&#29702;&#32452;&#21512;&#20851;&#31995;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#26088;&#22312;&#28085;&#30422;&#20845;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#32452;&#21512;&#20851;&#31995;&#65306;&#20301;&#32622;&#12289;&#27604;&#36739;&#12289;&#20010;&#20154;&#12289;&#25968;&#23398;&#12289;&#36523;&#20221;&#21644;&#20854;&#20182;&#12290;&#24847;&#35782;&#21040;&#22810;&#35821;&#35328;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#23558;&#35780;&#20272;&#25193;&#23637;&#21040;&#23558;&#36825;&#20123;&#26696;&#20363;&#32763;&#35793;&#25104;&#20013;&#25991;&#12289;&#26085;&#25991;&#12289;&#27861;&#25991;&#21644;&#38889;&#25991;&#12290;&#25105;&#20204;&#30340;&#22810;&#35821;&#35328;&#32452;&#21512;&#20851;&#31995;(MCR)&#22522;&#20934;&#26088;&#22312;&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#19979;&#30340;&#32452;&#21512;&#20851;&#31995;&#25512;&#29702;&#30340;&#31283;&#20581;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02615v1 Announce Type: new  Abstract: We present a comprehensive evaluation of large language models(LLMs)' ability to reason about composition relations through a benchmark encompassing 1,500 test cases in English, designed to cover six distinct types of composition relations: Positional, Comparative, Personal, Mathematical, Identity, and Other. Acknowledging the significance of multilingual capabilities, we expanded our assessment to include translations of these cases into Chinese, Japanese, French, and Korean. Our Multilingual Composition Relation (MCR) benchmark aims at investigating the robustness and adaptability of LLMs in handling composition relation reasoning across diverse linguistic contexts.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#22810;&#26679;&#21270;&#30340;&#20107;&#20214;&#23450;&#20041;&#25968;&#25454;&#38598;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20986;&#22823;&#37327;&#30340;&#20107;&#20214;&#31867;&#22411;&#21644;&#22810;&#26679;&#30340;&#20107;&#20214;&#23450;&#20041;&#23545;&#20110;&#25913;&#36827;&#38646;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02586</link><description>&lt;p&gt;
&#25913;&#36827;&#38646;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#30340;&#20107;&#20214;&#23450;&#20041;&#36981;&#24490;
&lt;/p&gt;
&lt;p&gt;
Improving Event Definition Following For Zero-Shot Event Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02586
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#22810;&#26679;&#21270;&#30340;&#20107;&#20214;&#23450;&#20041;&#25968;&#25454;&#38598;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20986;&#22823;&#37327;&#30340;&#20107;&#20214;&#31867;&#22411;&#21644;&#22810;&#26679;&#30340;&#20107;&#20214;&#23450;&#20041;&#23545;&#20110;&#25913;&#36827;&#38646;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#19978;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#24050;&#30693;&#20107;&#20214;&#31867;&#22411;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#25552;&#31034;&#23427;&#20204;&#20351;&#29992;&#26410;&#35265;&#36807;&#30340;&#20107;&#20214;&#23450;&#20041;&#12290;&#36825;&#20123;&#26041;&#27861;&#20598;&#23572;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#24635;&#20307;&#34920;&#29616;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#26356;&#22909;&#22320;&#36981;&#24490;&#20107;&#20214;&#23450;&#20041;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#12290;&#25105;&#20204;&#20551;&#35774;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#20107;&#20214;&#31867;&#22411;&#21644;&#23450;&#20041;&#26159;&#27169;&#22411;&#23398;&#20064;&#36981;&#24490;&#20107;&#20214;&#23450;&#20041;&#30340;&#20851;&#38190;&#65292;&#32780;&#29616;&#26377;&#30340;&#20107;&#20214;&#25552;&#21462;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#20026;&#23569;&#25968;&#20107;&#20214;&#31867;&#22411;&#27880;&#37322;&#35768;&#22810;&#39640;&#36136;&#37327;&#31034;&#20363;&#12290;&#20026;&#39564;&#35777;&#25105;&#20204;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#22810;&#26679;&#21270;&#20107;&#20214;&#23450;&#20041;&#65288;DivED&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#22823;&#37327;&#20107;&#20214;&#31867;&#22411;&#65288;200&#20010;&#65289;&#21644;&#22810;&#26679;&#21270;&#30340;&#20107;&#20214;&#23450;&#20041;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#20107;&#20214;&#25552;&#21462;&#24615;&#33021;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#24615;&#33021;&#19981;&#20250;&#38543;&#30528;&#36229;&#36807;&#21313;&#20010;&#20107;&#20214;&#31867;&#22411;&#30340;&#22686;&#21152;&#32780;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02586v1 Announce Type: new  Abstract: Existing approaches on zero-shot event detection usually train models on datasets annotated with known event types, and prompt them with unseen event definitions. These approaches yield sporadic successes, yet generally fall short of expectations. In this work, we aim to improve zero-shot event detection by training models to better follow event definitions. We hypothesize that a diverse set of event types and definitions are the key for models to learn to follow event definitions while existing event extraction datasets focus on annotating many high-quality examples for a few event types. To verify our hypothesis, we construct an automatically generated Diverse Event Definition (DivED) dataset and conduct comparative studies. Our experiments reveal that a large number of event types (200) and diverse event definitions can significantly boost event extraction performance; on the other hand, the performance does not scale with over ten ex
&lt;/p&gt;</description></item><item><title>ChatCite&#26159;&#19968;&#20010;LLM&#20195;&#29702;&#65292;&#36890;&#36807;&#20154;&#24037;&#24037;&#20316;&#27969;&#24341;&#23548;&#36827;&#34892;&#27604;&#36739;&#25991;&#23398;&#32508;&#36848;&#65292;&#21033;&#29992;&#21453;&#24605;&#36880;&#27493;&#26426;&#21046;&#29983;&#25104;&#25688;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.02574</link><description>&lt;p&gt;
ChatCite&#65306;LLM&#20195;&#29702;&#19982;&#20154;&#24037;&#24037;&#20316;&#27969;&#24341;&#23548;&#29992;&#20110;&#27604;&#36739;&#25991;&#23398;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02574
&lt;/p&gt;
&lt;p&gt;
ChatCite&#26159;&#19968;&#20010;LLM&#20195;&#29702;&#65292;&#36890;&#36807;&#20154;&#24037;&#24037;&#20316;&#27969;&#24341;&#23548;&#36827;&#34892;&#27604;&#36739;&#25991;&#23398;&#32508;&#36848;&#65292;&#21033;&#29992;&#21453;&#24605;&#36880;&#27493;&#26426;&#21046;&#29983;&#25104;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02574v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#23398;&#31185; &#25688;&#35201;&#65306;&#25991;&#29486;&#32508;&#36848;&#26159;&#30740;&#31350;&#36807;&#31243;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#27493;&#12290;&#23427;&#26377;&#21161;&#20110;&#29702;&#35299;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#22312;&#36827;&#34892;&#20197;&#24448;&#20316;&#21697;&#27604;&#36739;&#20998;&#26512;&#26102;&#20102;&#35299;&#24403;&#21069;&#30740;&#31350;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#24635;&#32467;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#12290;&#20808;&#21069;&#22522;&#20110;LLM&#30340;&#25991;&#29486;&#32508;&#36848;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23436;&#25972;&#36807;&#31243;&#65292;&#21253;&#25324;&#25991;&#29486;&#26816;&#32034;&#12289;&#31579;&#36873;&#21644;&#24635;&#32467;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24635;&#32467;&#27493;&#39588;&#65292;&#31616;&#21333;&#30340;CoT&#26041;&#27861;&#24448;&#24448;&#32570;&#20047;&#25552;&#20379;&#24191;&#27867;&#27604;&#36739;&#24635;&#32467;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#19987;&#27880;&#20110;&#29420;&#31435;&#25991;&#29486;&#24635;&#32467;&#27493;&#39588;&#24182;&#24341;&#20837;ChatCite&#65292;&#19968;&#20010;&#20855;&#26377;&#20154;&#24037;&#24037;&#20316;&#27969;&#24341;&#23548;&#29992;&#20110;&#27604;&#36739;&#25991;&#23398;&#32508;&#36848;&#30340;LLM&#20195;&#29702;&#12290;&#35813;&#20195;&#29702;&#36890;&#36807;&#27169;&#20223;&#20154;&#31867;&#24037;&#20316;&#27969;&#65292;&#39318;&#20808;&#20174;&#30456;&#20851;&#25991;&#29486;&#20013;&#25552;&#21462;&#20851;&#38190;&#35201;&#32032;&#65292;&#28982;&#21518;&#20351;&#29992;&#21453;&#24605;&#36880;&#27493;&#26426;&#21046;&#29983;&#25104;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02574v1 Announce Type: cross  Abstract: The literature review is an indispensable step in the research process. It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works. However, literature summary is challenging and time consuming. The previous LLM-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization. However, for the summarization step, simple CoT method often lacks the ability to provide extensive comparative summary. In this work, we firstly focus on the independent literature summarization step and introduce ChatCite, an LLM agent with human workflow guidance for comparative literature summary. This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism. In order to better ev
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#22810;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36739;&#24369;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20195;&#30721;&#35757;&#32451;&#21644;&#25512;&#29702;&#26469;&#25913;&#21892;&#22810;&#35821;&#35328;&#32467;&#26500;&#21270;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.02567</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#20174;LLMs&#20013;&#24341;&#20986;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#32467;&#26500;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Eliciting Better Multilingual Structured Reasoning from LLMs through Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02567
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#22810;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36739;&#24369;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#20195;&#30721;&#35757;&#32451;&#21644;&#25512;&#29702;&#26469;&#25913;&#21892;&#22810;&#35821;&#35328;&#32467;&#26500;&#21270;&#25512;&#29702;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#22312;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30740;&#31350;&#20165;&#38480;&#20110;&#33521;&#35821;&#25110;&#31616;&#21333;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;xSTREET&#30340;&#22810;&#35821;&#35328;&#32467;&#26500;&#21270;&#25512;&#29702;&#21644;&#35299;&#37322;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20845;&#31181;&#35821;&#35328;&#30340;&#22235;&#20010;&#20219;&#21153;&#12290;xSTREET&#26292;&#38706;&#20102;&#22522;&#26412;LLM&#22312;&#33521;&#35821;&#21644;&#38750;&#33521;&#35821;&#25512;&#29702;&#20219;&#21153;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#24314;&#31435;&#22312;LLM&#22312;&#20195;&#30721;&#19978;&#35757;&#32451;&#26356;&#22909;&#30340;&#25512;&#29702;&#36825;&#19968;&#35266;&#28857;&#22522;&#30784;&#19978;&#12290;&#39318;&#20808;&#65292;&#22312;&#35757;&#32451;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#23558;&#20195;&#30721;&#25968;&#25454;&#38598;&#22686;&#24378;&#20026;&#22810;&#35821;&#35328;&#27880;&#37322;&#65292;&#21516;&#26102;&#20445;&#25345;&#31243;&#24207;&#20195;&#30721;&#19981;&#21464;&#12290;&#20854;&#27425;&#65292;&#22312;&#25512;&#26029;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#21253;&#21547;&#36880;&#27493;&#20195;&#30721;&#21407;&#35821;&#30340;&#25552;&#31034;&#32467;&#26500;&#26469;&#24357;&#21512;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#25512;&#23548;&#20986;&#26032;&#20107;&#23454;&#24182;&#25214;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;xSTREET&#19978;&#34920;&#29616;&#20986;&#20102;&#25913;&#36827;&#30340;&#22810;&#35821;&#35328;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#31185;&#23398;&#24120;&#35782;&#25512;&#29702;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02567v1 Announce Type: cross  Abstract: Development of large language models (LLM) have shown progress on reasoning, though studies have been limited to English or simple reasoning tasks. We thus introduce a multilingual structured reasoning and explanation dataset, termed xSTREET, that covers four tasks across six languages. xSTREET exposes a gap in base LLM performance between English and non-English reasoning tasks. We then propose two methods to remedy this gap, building on the insight that LLMs trained on code are better reasoners. First, at training time, we augment a code dataset with multi-lingual comments using machine translation while keeping program code as-is. Second, at inference time, we bridge the gap between training and inference by employing a prompt structure that incorporates step-by-step code primitives to derive new facts and find a solution. Our methods show improved multilingual performance on xSTREET, most notably on the scientific commonsense reaso
&lt;/p&gt;</description></item><item><title>&#25163;&#35821;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#23384;&#22312;&#26174;&#33879;&#20559;&#35265;&#65292;&#21253;&#25324;&#36807;&#20110;&#20851;&#27880;&#27807;&#36890;&#38556;&#30861;&#12289;&#32570;&#20047;&#20195;&#34920;&#24615;&#25968;&#25454;&#38598;&#12289;&#20351;&#29992;&#32570;&#20047;&#35821;&#35328;&#22522;&#30784;&#30340;&#27880;&#37322;&#21644;&#22522;&#20110;&#26377;&#32570;&#38519;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#32570;&#20047;&#32843;&#20154;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#37325;&#35201;&#21442;&#19982;&#12290;</title><link>https://arxiv.org/abs/2403.02563</link><description>&lt;p&gt;
&#12298;&#25163;&#35821;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#31995;&#32479;&#20559;&#35265;&#65306;&#32843;&#20154;&#21628;&#21505;&#37325;&#26032;&#35780;&#20272;&#30740;&#31350;&#35758;&#31243;&#12299;
&lt;/p&gt;
&lt;p&gt;
Systemic Biases in Sign Language AI Research: A Deaf-Led Call to Reevaluate Research Agendas
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02563
&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#23384;&#22312;&#26174;&#33879;&#20559;&#35265;&#65292;&#21253;&#25324;&#36807;&#20110;&#20851;&#27880;&#27807;&#36890;&#38556;&#30861;&#12289;&#32570;&#20047;&#20195;&#34920;&#24615;&#25968;&#25454;&#38598;&#12289;&#20351;&#29992;&#32570;&#20047;&#35821;&#35328;&#22522;&#30784;&#30340;&#27880;&#37322;&#21644;&#22522;&#20110;&#26377;&#32570;&#38519;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#32570;&#20047;&#32843;&#20154;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#37325;&#35201;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25163;&#35821;&#35782;&#21035;&#12289;&#29983;&#25104;&#21644;&#32763;&#35793;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#30740;&#31350;&#19981;&#26029;&#22686;&#21152;&#30340;&#32972;&#26223;&#19979;&#65292;&#20154;&#20204;&#21628;&#21505;&#23545;&#36825;&#20123;&#25216;&#26415;&#36827;&#34892;&#36947;&#24503;&#21457;&#23637;&#12290;&#23613;&#31649;&#36825;&#20123;&#24037;&#20316;&#23545;&#24110;&#21161;&#20010;&#21035;&#30740;&#31350;&#20154;&#21592;&#21462;&#24471;&#26356;&#22909;&#25104;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26377;&#20154;&#25351;&#20986;&#65292;&#36825;&#19968;&#39046;&#22495;&#20173;&#28982;&#30001;&#21548;&#21147;&#38750;&#25163;&#35821;&#30740;&#31350;&#20154;&#21592;&#20027;&#23548;&#65292;&#21364;&#40092;&#26377;&#38024;&#23545;&#31995;&#32479;&#20559;&#35265;&#30340;&#35752;&#35770;&#25110;&#22609;&#36896;&#30740;&#31350;&#38382;&#39064;&#21644;&#26041;&#27861;&#30340;&#20462;&#36766;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#25163;&#35821;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;101&#31687;&#26368;&#26032;&#35770;&#25991;&#36827;&#34892;&#31995;&#32479;&#22238;&#39038;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#21457;&#29616;&#20102;&#24403;&#21069;&#25163;&#35821;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#37325;&#22823;&#20559;&#35265;&#65292;&#21253;&#25324;&#36807;&#20998;&#20851;&#27880;&#35299;&#20915;&#34987;&#35748;&#20026;&#23384;&#22312;&#30340;&#27807;&#36890;&#38556;&#30861;&#12289;&#32570;&#20047;&#20351;&#29992;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#25968;&#25454;&#38598;&#12289;&#20351;&#29992;&#32570;&#20047;&#35821;&#35328;&#22522;&#30784;&#30340;&#27880;&#37322;&#20197;&#21450;&#22522;&#20110;&#26377;&#32570;&#38519;&#27169;&#22411;&#26500;&#24314;&#26041;&#27861;&#31561;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#19968;&#39046;&#22495;&#32570;&#20047;&#32843;&#20154;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#26377;&#24847;&#20041;&#21442;&#19982;&#65292;&#24182;&#19988;&#26356;&#22810;&#22320;&#21463;&#21040;&#39537;&#21160;&#30340;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02563v1 Announce Type: cross  Abstract: Growing research in sign language recognition, generation, and translation AI has been accompanied by calls for ethical development of such technologies. While these works are crucial to helping individual researchers do better, there is a notable lack of discussion of systemic biases or analysis of rhetoric that shape the research questions and methods in the field, especially as it remains dominated by hearing non-signing researchers. Therefore, we conduct a systematic review of 101 recent papers in sign language AI. Our analysis identifies significant biases in the current state of sign language AI research, including an overfocus on addressing perceived communication barriers, a lack of use of representative datasets, use of annotations lacking linguistic foundations, and development of methods that build on flawed models. We take the position that the field lacks meaningful input from Deaf stakeholders, and is instead driven by wh
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#21152;&#36895;&#20102;&#21307;&#23398;&#20013;&#33258;&#28982;&#35821;&#35328;&#21644;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#24182;&#26631;&#24535;&#30528;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#26041;&#24335;&#30340;&#37325;&#22823;&#33539;&#24335;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.02558</link><description>&lt;p&gt;
&#20026;&#29983;&#25104;&#24314;&#27169;&#30740;&#31350;&#26356;&#26032;&#26377;&#20851;&#20020;&#24202;&#20154;&#24037;&#26234;&#33021;&#65288;MI-CLAIM&#65289;&#26816;&#26597;&#34920;
&lt;/p&gt;
&lt;p&gt;
Updating the Minimum Information about CLinical Artificial Intelligence (MI-CLAIM) checklist for generative modeling research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02558
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#21152;&#36895;&#20102;&#21307;&#23398;&#20013;&#33258;&#28982;&#35821;&#35328;&#21644;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#24182;&#26631;&#24535;&#30528;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#26041;&#24335;&#30340;&#37325;&#22823;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12289;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#65292;&#21152;&#36895;&#20102;&#21307;&#23398;&#20013;&#33258;&#28982;&#35821;&#35328;&#21644;&#22270;&#20687;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#24182;&#26631;&#24535;&#30528;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#26041;&#24335;&#30340;&#37325;&#22823;&#33539;&#24335;&#36716;&#21464;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#38750;&#24120;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#20294;&#22312;&#25193;&#23637;&#21644;&#35780;&#20272;&#23427;&#20204;&#30340;&#20351;&#29992;&#36807;&#31243;&#20013;&#20986;&#29616;&#20102;&#21069;&#20154;&#26694;&#26550;&#26410;&#35299;&#20915;&#30340;&#26032;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#20197;&#23569;&#37327;&#25110;&#26080;&#38656;&#19987;&#38376;&#35757;&#32451;&#25968;&#25454;&#21363;&#21487;&#20135;&#29983;&#26377;&#29992;&#36755;&#20986;&#30340;&#33021;&#21147;&#65288;&#8220;&#38646;&#26679;&#26412;&#8221;&#25110;&#8220;&#23569;&#26679;&#26412;&#8221;&#26041;&#27861;&#65289;&#65292;&#20197;&#21450;&#23427;&#20204;&#36755;&#20986;&#30340;&#24320;&#25918;&#24615;&#36136;&#65292;&#38656;&#35201;&#21046;&#23450;&#26356;&#26032;&#30340;&#20351;&#29992;&#21644;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#25351;&#21335;&#12290;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;141103&#30830;&#23450;&#20102;&#26377;&#20851;&#20020;&#24202;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#24320;&#21457;&#30340;&#26631;&#20934;&#21644;&#26368;&#20339;&#23454;&#36341;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#20197;&#21450;&#20960;&#20010;&#26032;&#20852;&#22269;&#23478;&#20020;&#24202;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02558v1 Announce Type: new  Abstract: Recent advances in generative models, including large language models (LLMs), vision language models (VLMs), and diffusion models, have accelerated the field of natural language and image processing in medicine and marked a significant paradigm shift in how biomedical models can be developed and deployed. While these models are highly adaptable to new tasks, scaling and evaluating their usage presents new challenges not addressed in previous frameworks. In particular, the ability of these models to produce useful outputs with little to no specialized training data ("zero-" or "few-shot" approaches), as well as the open-ended nature of their outputs, necessitate the development of updated guidelines in using and evaluating these models. In response to gaps in standards and best practices for the development of clinical AI tools identified by US Executive Order 141103 and several emerging national networks for clinical AI evaluation, we be
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#31572;&#26696;&#27880;&#37322;&#30340;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;DACO&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#28608;&#21457;&#26410;&#26469;&#23545;&#25968;&#25454;&#20998;&#26512;&#36825;&#19968;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.02528</link><description>&lt;p&gt;
DACO: &#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#23454;&#29616;&#24212;&#29992;&#39537;&#21160;&#21644;&#20840;&#38754;&#30340;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02528
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#31572;&#26696;&#27880;&#37322;&#30340;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;DACO&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#28608;&#21457;&#26410;&#26469;&#23545;&#25968;&#25454;&#20998;&#26512;&#36825;&#19968;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20998;&#26512;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#20998;&#26512;&#36807;&#31243;&#65292;&#29992;&#20110;&#29983;&#25104;&#28145;&#20837;&#30740;&#31350;&#21644;&#32467;&#35770;&#24615;&#35265;&#35299;&#65292;&#20840;&#38754;&#22238;&#31572;&#32473;&#23450;&#29992;&#25143;&#23545;&#34920;&#26684;&#25968;&#25454;&#30340;&#26597;&#35810;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#26032;&#30340;&#36164;&#28304;&#21644;&#22522;&#20934;&#65292;&#28608;&#21457;&#26410;&#26469;&#23545;&#36825;&#19968;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#26410;&#20805;&#20998;&#25366;&#25496;&#30340;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#21644;&#22810;&#36718;&#25552;&#31034;&#25216;&#26415;&#33258;&#21160;&#20135;&#29983;&#39640;&#36136;&#37327;&#31572;&#26696;&#27880;&#37322;&#65292;&#26500;&#24314;&#20102;DACO&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;440&#20010;&#26469;&#33258;&#30495;&#23454;&#22330;&#26223;&#30340;&#25968;&#25454;&#24211;&#65288;&#34920;&#26684;&#25968;&#25454;&#65289;&#65292;&#32422;2k&#20010;&#26597;&#35810;-&#31572;&#26696;&#23545;&#21487;&#20316;&#20026;&#27169;&#22411;&#35757;&#32451;&#30340;&#24369;&#30417;&#30563;&#65292;&#20197;&#21450;&#19968;&#20010;&#20154;&#24037;&#31934;&#32454;&#35843;&#25972;&#30340;&#26631;&#27880;&#30340;&#32039;&#20945;&#20294;&#39640;&#36136;&#37327;&#27979;&#35797;&#38598;&#65292;&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02528v1 Announce Type: cross  Abstract: Data analysis is a crucial analytical process to generate in-depth studies and conclusive insights to comprehensively answer a given user query for tabular data. In this work, we aim to propose new resources and benchmarks to inspire future research on this crucial yet challenging and under-explored task. However, collecting data analysis annotations curated by experts can be prohibitively expensive. We propose to automatically generate high-quality answer annotations leveraging the code-generation capabilities of LLMs with a multi-turn prompting technique. We construct the DACO dataset, containing (1) 440 databases (of tabular data) collected from real-world scenarios, (2) ~2k query-answer pairs that can serve as weak supervision for model training, and (3) a concentrated but high-quality test set with human refined annotations that serves as our main evaluation benchmark. We train a 6B supervised fine-tuning (SFT) model on DACO datas
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#30452;&#25509;&#23454;&#26045;&#26080;&#23475;&#30340;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#19981;&#20165;&#20445;&#30041;&#20102;&#22522;&#26412;&#27169;&#22411;&#30340;&#19968;&#33324;&#33021;&#21147;&#65292;&#36824;&#26174;&#33879;&#22686;&#24378;&#20102;&#20854;&#23545;&#35805;&#33021;&#21147;&#65292;&#21516;&#26102;&#26126;&#26174;&#20943;&#23569;&#20102;&#26377;&#27602;&#36755;&#20986;&#30340;&#29983;&#25104;</title><link>https://arxiv.org/abs/2403.02513</link><description>&lt;p&gt;
&#22312;&#22686;&#24378;&#23545;&#35805;&#24335;LLM&#19982;&#30452;&#25509;RLHF&#30340;&#26080;&#23475;&#24615;&#21644;&#19968;&#33324;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Balancing Enhancement, Harmlessness, and General Capabilities: Enhancing Conversational LLMs with Direct RLHF
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02513
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#30452;&#25509;&#23454;&#26045;&#26080;&#23475;&#30340;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#19981;&#20165;&#20445;&#30041;&#20102;&#22522;&#26412;&#27169;&#22411;&#30340;&#19968;&#33324;&#33021;&#21147;&#65292;&#36824;&#26174;&#33879;&#22686;&#24378;&#20102;&#20854;&#23545;&#35805;&#33021;&#21147;&#65292;&#21516;&#26102;&#26126;&#26174;&#20943;&#23569;&#20102;&#26377;&#27602;&#36755;&#20986;&#30340;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#23545;&#35805;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#20013;&#65292;&#20986;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#20851;&#27880;&#30340;&#36235;&#21183;&#65292;&#21363;&#35768;&#22810;&#26032;&#30340;&#22522;&#26412;LLMs&#22312;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21518;&#20250;&#32463;&#21382;&#22522;&#30784;&#33021;&#21147;&#30340;&#30693;&#35782;&#20943;&#23569;&#12290;&#36825;&#19968;&#36807;&#31243;&#32463;&#24120;&#23548;&#33268;&#38382;&#39064;&#65292;&#27604;&#22914;&#36951;&#24536;&#25110;&#22522;&#26412;&#27169;&#22411;&#33021;&#21147;&#30340;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#27169;&#22411;&#24448;&#24448;&#38590;&#20197;&#19982;&#29992;&#25143;&#20559;&#22909;&#20445;&#25345;&#19968;&#33268;&#65292;&#22312;&#29305;&#23450;&#25552;&#31034;&#26102;&#26080;&#24847;&#20013;&#22686;&#21152;&#26377;&#27602;&#36755;&#20986;&#30340;&#29983;&#25104;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#23436;&#20840;&#32469;&#36807;SFT&#24182;&#30452;&#25509;&#23454;&#26045;&#26080;&#23475;&#30340;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#20445;&#30041;&#20102;&#22522;&#26412;&#27169;&#22411;&#30340;&#19968;&#33324;&#33021;&#21147;&#65292;&#36824;&#26174;&#33879;&#22686;&#24378;&#20102;&#20854;&#23545;&#35805;&#33021;&#21147;&#65292;&#21516;&#26102;&#26126;&#26174;&#20943;&#23569;&#20102;&#26377;&#27602;&#36755;&#20986;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#37027;&#20123;&#38656;&#35201;&#24494;&#22937;&#29702;&#35299;&#30340;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02513v1 Announce Type: new  Abstract: In recent advancements in Conversational Large Language Models (LLMs), a concerning trend has emerged, showing that many new base LLMs experience a knowledge reduction in their foundational capabilities following Supervised Fine-Tuning (SFT). This process often leads to issues such as forgetting or a decrease in the base model's abilities. Moreover, fine-tuned models struggle to align with user preferences, inadvertently increasing the generation of toxic outputs when specifically prompted. To overcome these challenges, we adopted an innovative approach by completely bypassing SFT and directly implementing Harmless Reinforcement Learning from Human Feedback (RLHF). Our method not only preserves the base model's general capabilities but also significantly enhances its conversational abilities, while notably reducing the generation of toxic outputs. Our approach holds significant implications for fields that demand a nuanced understanding 
&lt;/p&gt;</description></item><item><title>SPUQ&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25200;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#21516;&#26102;&#22788;&#29702;&#29616;&#35937;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;</title><link>https://arxiv.org/abs/2403.02509</link><description>&lt;p&gt;
SPUQ&#65306;&#22522;&#20110;&#25200;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02509
&lt;/p&gt;
&lt;p&gt;
SPUQ&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25200;&#21160;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#21516;&#26102;&#22788;&#29702;&#29616;&#35937;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#25552;&#20379;&#20102;&#20986;&#33394;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20542;&#21521;&#20110;&#20570;&#20986;&#33258;&#20449;&#38169;&#35823;&#30340;&#39044;&#27979;&#65292;&#31361;&#26174;&#20102;&#22312;LLMs&#20013;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#30340;&#37325;&#35201;&#24615;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#35299;&#20915;&#29616;&#35937;&#24615;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#23545;&#21253;&#25324;&#35748;&#30693;&#24615;&#22312;&#20869;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#20840;&#35889;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#21463;&#21040;&#36825;&#19968;&#24046;&#36317;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;UQ&#26041;&#27861;&#65292;&#21363;&#36866;&#29992;&#20110;UQ&#30340;&#25200;&#21160;&#37319;&#26679;&#65288;SPUQ&#65289;&#65292;&#26088;&#22312;&#24212;&#23545;&#29616;&#35937;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#20026;LLM&#36755;&#20837;&#29983;&#25104;&#19968;&#32452;&#25200;&#21160;&#65292;&#23545;&#27599;&#20010;&#25200;&#21160;&#36827;&#34892;&#36755;&#20986;&#37319;&#26679;&#65292;&#24182;&#32467;&#21512;&#19968;&#20010;&#32858;&#21512;&#27169;&#22359;&#65292;&#35813;&#32858;&#21512;&#27169;&#22359;&#27010;&#25324;&#20102;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#37319;&#26679;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25200;&#21160;&#21644;&#32858;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02509v1 Announce Type: cross  Abstract: In recent years, large language models (LLMs) have become increasingly prevalent, offering remarkable text generation capabilities. However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs. While previous works have mainly focused on addressing aleatoric uncertainty, the full spectrum of uncertainties, including epistemic, remains inadequately explored. Motivated by this gap, we introduce a novel UQ method, sampling with perturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic uncertainties. The method entails generating a set of perturbations for LLM inputs, sampling outputs for each perturbation, and incorporating an aggregation module that generalizes the sampling uncertainty approach for text generation tasks. Through extensive experiments on various datasets, we investigated different perturbation and aggrega
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#23545;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#30410;&#22788;&#12290;</title><link>https://arxiv.org/abs/2403.02504</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
A Tutorial on the Pretrain-Finetune Paradigm for Natural Language Processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02504
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25928;&#29575;&#65292;&#23588;&#20854;&#23545;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#20195;&#34920;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#31181;&#21464;&#38761;&#24615;&#26041;&#27861;&#12290;&#35813;&#33539;&#24335;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21306;&#21035;&#20110;&#20247;&#65292;&#23637;&#31034;&#20102;&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#21363;&#20351;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#20063;&#20855;&#26377;&#26174;&#33879;&#30340;&#25928;&#29575;&#12290;&#36825;&#31181;&#25928;&#29575;&#23545;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#29305;&#21035;&#26377;&#30410;&#65292;&#22240;&#20026;&#27880;&#37322;&#26679;&#26412;&#30340;&#25968;&#37327;&#36890;&#24120;&#38750;&#24120;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#25945;&#31243;&#20840;&#38754;&#20171;&#32461;&#20102;&#39044;&#35757;&#32451;-&#24494;&#35843;&#33539;&#24335;&#12290;&#25105;&#20204;&#39318;&#20808;&#28145;&#20837;&#25506;&#35752;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#23454;&#38469;&#24212;&#29992;&#30340;&#26696;&#20363;&#32451;&#20064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#33539;&#24335;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#22810;&#31867;&#21035;&#20998;&#31867;&#21644;&#22238;&#24402;&#12290;&#24378;&#35843;&#20854;&#39640;&#25928;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#65292;&#35813;&#25945;&#31243;&#26088;&#22312;&#40723;&#21169;&#26356;&#24191;&#27867;&#22320;&#37319;&#32435;&#36825;&#31181;&#33539;&#24335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#30340;&#24320;&#25918;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02504v1 Announce Type: cross  Abstract: The pretrain-finetune paradigm represents a transformative approach in natural language processing (NLP). This paradigm distinguishes itself through the use of large pretrained language models, demonstrating remarkable efficiency in finetuning tasks, even with limited training data. This efficiency is especially beneficial for research in social sciences, where the number of annotated samples is often quite limited. Our tutorial offers a comprehensive introduction to the pretrain-finetune paradigm. We first delve into the fundamental concepts of pretraining and finetuning, followed by practical exercises using real-world applications. We demonstrate the application of the paradigm across various tasks, including multi-class classification and regression. Emphasizing its efficacy and user-friendliness, the tutorial aims to encourage broader adoption of this paradigm. To this end, we have provided open access to all our code and datasets
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#20195;&#29702;&#30340;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#20195;&#29702;&#20174;&#25506;&#32034;&#22833;&#36133;&#20013;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.02502</link><description>&lt;p&gt;
&#35797;&#38169;&#27861;&#65306;&#38754;&#21521;LLM&#20195;&#29702;&#30340;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02502
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#20195;&#29702;&#30340;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#20195;&#29702;&#20174;&#25506;&#32034;&#22833;&#36133;&#20013;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#33258;&#20027;&#20195;&#29702;&#31995;&#32479;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25506;&#32034;&#30340;&#36712;&#36857;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;ETO&#12290;&#36825;&#31181;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#25552;&#39640;&#24320;&#25918;LLM&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;&#19982;&#20808;&#21069;&#19987;&#38376;&#35757;&#32451;&#25104;&#21151;&#19987;&#23478;&#36712;&#36857;&#30340;&#30740;&#31350;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#20195;&#29702;&#20174;&#20854;&#25506;&#32034;&#22833;&#36133;&#20013;&#23398;&#20064;&#12290;&#36825;&#36890;&#36807;&#36845;&#20195;&#20248;&#21270;&#26694;&#26550;&#23454;&#29616;&#20102;&#24615;&#33021;&#30340;&#25913;&#36827;&#12290;&#22312;&#25506;&#32034;&#38454;&#27573;&#65292;&#20195;&#29702;&#19982;&#29615;&#22659;&#20114;&#21160;&#65292;&#23436;&#25104;&#25351;&#23450;&#20219;&#21153;&#65292;&#25910;&#38598;&#22833;&#36133;&#36712;&#36857;&#20197;&#21019;&#24314;&#23545;&#27604;&#36712;&#36857;&#23545;&#12290;&#22312;&#38543;&#21518;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#20195;&#29702;&#21033;&#29992;&#36825;&#20123;&#36712;&#36857;&#20559;&#22909;&#23545;&#26356;&#26032;&#20854;&#31574;&#30053;&#65292;&#20351;&#29992;&#31867;&#20284;DPO&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#31181;&#25506;&#32034;&#21644;&#35757;&#32451;&#30340;&#36845;&#20195;&#24490;&#29615;&#20419;&#36827;&#20102;&#20195;&#29702;&#30340;&#25345;&#32493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02502v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have become integral components in various autonomous agent systems. In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments o
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26816;&#39564;&#20102;&#20132;&#20114;&#24335;&#30005;&#23376;&#20070;&#23545;&#19977;&#33267;&#20116;&#24180;&#32423;&#23398;&#29983;&#38405;&#35835;&#29702;&#35299;&#21644;&#35789;&#27719;&#30693;&#35782;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#37319;&#29992;&#36873;&#25321;&#20882;&#38505;&#26684;&#24335;&#21644;&#20869;&#23884;&#29702;&#35299;&#38382;&#39064;&#30340;&#26041;&#24335;&#26469;&#25945;&#25480;&#21333;&#35789;&#23398;&#20064;&#21644;&#29702;&#35299;&#30417;&#25511;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.02496</link><description>&lt;p&gt;
&#36873;&#25321;&#20320;&#30340;&#20882;&#38505;&#65306;&#20132;&#20114;&#24335;&#30005;&#23376;&#20070;&#20197;&#25552;&#39640;&#35789;&#27719;&#30693;&#35782;&#21644;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Choose Your Own Adventure: Interactive E-Books to Improve Word Knowledge and Comprehension Skills
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02496
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26816;&#39564;&#20102;&#20132;&#20114;&#24335;&#30005;&#23376;&#20070;&#23545;&#19977;&#33267;&#20116;&#24180;&#32423;&#23398;&#29983;&#38405;&#35835;&#29702;&#35299;&#21644;&#35789;&#27719;&#30693;&#35782;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#37319;&#29992;&#36873;&#25321;&#20882;&#38505;&#26684;&#24335;&#21644;&#20869;&#23884;&#29702;&#35299;&#38382;&#39064;&#30340;&#26041;&#24335;&#26469;&#25945;&#25480;&#21333;&#35789;&#23398;&#20064;&#21644;&#29702;&#35299;&#30417;&#25511;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#21487;&#34892;&#24615;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#26816;&#26597;&#38405;&#35835;&#25968;&#23383;&#20132;&#20114;&#24335;&#30005;&#23376;&#20070;&#23545;&#25903;&#25345;&#38405;&#35835;&#29702;&#35299;&#30340;&#22522;&#26412;&#25216;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#19977;&#33267;&#20116;&#24180;&#32423;&#23398;&#29983;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#23398;&#29983;&#38405;&#35835;&#20102;&#20004;&#26412;&#30005;&#23376;&#20070;&#65292;&#25945;&#25480;&#21333;&#35789;&#23398;&#20064;&#21644;&#29702;&#35299;&#30417;&#25511;&#31574;&#30053;&#65292;&#20197;&#23398;&#20064;&#22256;&#38590;&#35789;&#27719;&#21644;&#26377;&#20851;&#39123;&#39118;&#30340;&#31185;&#23398;&#27010;&#24565;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#29305;&#23450;&#30340;&#29702;&#35299;&#31574;&#30053;&#65292;&#21253;&#25324;&#21333;&#35789;&#23398;&#20064;&#21644;&#25903;&#25345;&#19968;&#33324;&#38405;&#35835;&#29702;&#35299;&#30340;&#31574;&#30053;&#65292;&#24635;&#32467;&#21644;&#38382;&#39064;&#29983;&#25104;&#65292;&#36825;&#20123;&#31574;&#30053;&#26159;&#21542;&#22312;&#30005;&#23376;&#20070;&#20013;&#22312;&#24314;&#31435;&#35789;&#27719;&#30693;&#35782;&#21644;&#29702;&#35299;&#25216;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#30340;&#28508;&#21147;&#12290;&#23398;&#29983;&#34987;&#20998;&#37197;&#38405;&#35835;&#19977;&#20010;&#29256;&#26412;&#20013;&#30340;&#19968;&#26412;&#30005;&#23376;&#20070;&#65292;&#27599;&#20010;&#29256;&#26412;&#23454;&#26045;&#19968;&#39033;&#31574;&#30053;&#12290;&#36825;&#20123;&#20070;&#37319;&#29992;&#20102;&#20132;&#20114;&#24335;&#36873;&#25321;&#20882;&#38505;&#30340;&#26684;&#24335;&#65292;&#20869;&#23884;&#29702;&#35299;&#38382;&#39064;&#65292;&#25552;&#20379;&#21363;&#26102;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02496v1 Announce Type: new  Abstract: The purpose of this feasibility study was to examine the potential impact of reading digital interactive e-books on essential skills that support reading comprehension with third-fifth grade students. Students read two e-Books that taught word learning and comprehension monitoring strategies in the service of learning difficult vocabulary and targeted science concepts about hurricanes. We investigated whether specific comprehension strategies including word learning and strategies that supported general reading comprehension, summarization, and question generation, show promise of effectiveness in building vocabulary knowledge and comprehension skills in the e-Books. Students were assigned to read one of three versions of each of the e-Books, each version implemented one strategy. The books employed a choose-your-adventure format with embedded comprehension questions that provided students with immediate feedback on their responses. Pair
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;Constrained DPO&#65288;C-DPO&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#36731;&#37327;&#30340;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#24179;&#34913;&#26377;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20026;LLMs&#25552;&#20379;&#20102;&#23433;&#20840;&#20445;&#38556;&#12290;</title><link>https://arxiv.org/abs/2403.02475</link><description>&lt;p&gt;
&#36890;&#36807;&#21463;&#38480;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#22686;&#24378;LLM&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing LLM Safety via Constrained Direct Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02475
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;Constrained DPO&#65288;C-DPO&#65289;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#36731;&#37327;&#30340;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#24179;&#34913;&#26377;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20026;LLMs&#25552;&#20379;&#20102;&#23433;&#20840;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#22686;&#24378;&#33021;&#21147;&#25552;&#39640;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#19981;&#21516;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#20197;&#21516;&#26102;&#22686;&#24378;&#20854;&#26377;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#36843;&#20999;&#38656;&#35201;&#65292;&#23613;&#31649;&#36825;&#20123;&#30446;&#26631;&#24120;&#24120;&#30456;&#20114;&#20914;&#31361;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#37325;&#35201;&#38382;&#39064;&#65292;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#26159;&#22312;&#24494;&#35843;&#38454;&#27573;&#36890;&#36807;&#21463;&#38480;&#21046;&#30340;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26694;&#26550;&#26045;&#21152;&#23433;&#20840;&#32422;&#26463;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#39640;&#19988;&#24120;&#24120;&#19981;&#31283;&#23450;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#21463;&#38480;&#21046;&#30340;DPO&#65288;C-DPO&#65289;&#65292;&#36825;&#26159;&#23545;&#26368;&#36817;&#25552;&#20986;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26041;&#27861;&#30340;&#19968;&#31181;&#26032;&#39062;&#25193;&#23637;&#65292;&#29992;&#20110;&#20248;&#21270;LLMs&#30340;&#24494;&#35843;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#36731;&#37327;&#30340;&#29305;&#28857;&#12290;&#36890;&#36807;&#34701;&#21512;&#21452;&#26799;&#24230;&#19979;&#38477;&#21644;DPO&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#30830;&#23450;&#20102;&#24110;&#21161;&#24615;&#21644;&#26080;&#23475;&#24615;&#20043;&#38388;&#30340;&#20960;&#20046;&#26368;&#20339;&#25240;&#34935;&#12290;&#20174;&#32463;&#39564;&#19978;&#30475;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;LLMs&#25552;&#20379;&#20102;&#23433;&#20840;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02475v1 Announce Type: cross  Abstract: The rapidly increasing capabilities of large language models (LLMs) raise an urgent need to align AI systems with diverse human preferences to simultaneously enhance their usefulness and safety, despite the often conflicting nature of these goals. To address this important problem, a promising approach is to enforce a safety constraint at the fine-tuning stage through a constrained Reinforcement Learning from Human Feedback (RLHF) framework. This approach, however, is computationally expensive and often unstable. In this work, we introduce Constrained DPO (C-DPO), a novel extension of the recently proposed Direct Preference Optimization (DPO) approach for fine-tuning LLMs that is both efficient and lightweight. By integrating dual gradient descent and DPO, our method identifies a nearly optimal trade-off between helpfulness and harmlessness without using reinforcement learning. Empirically, our approach provides a safety guarantee to L
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#35282;&#33394;&#23545;&#35805;&#26469;&#21306;&#20998;&#25991;&#23398;&#23567;&#35828;&#21465;&#36848;&#19982;&#21508;&#35282;&#33394;&#30340;&#24773;&#24863;&#26354;&#32447;&#65292;&#21457;&#29616;&#21465;&#36848;&#21644;&#23545;&#35805;&#22312;&#34920;&#36798;&#24773;&#24863;&#26102;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#20010;&#21035;&#35282;&#33394;&#30340;&#24773;&#24863;&#26354;&#32447;&#26356;&#20934;&#30830;&#22320;&#25429;&#25417;&#21040;&#25925;&#20107;&#24773;&#24863;&#26354;&#32447;&#30340;&#20849;&#21516;&#28857;&#25110;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.02474</link><description>&lt;p&gt;
&#25991;&#23398;&#23567;&#35828;&#30340;&#24773;&#24863;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
The Emotion Dynamics of Literary Novels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02474
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#35282;&#33394;&#23545;&#35805;&#26469;&#21306;&#20998;&#25991;&#23398;&#23567;&#35828;&#21465;&#36848;&#19982;&#21508;&#35282;&#33394;&#30340;&#24773;&#24863;&#26354;&#32447;&#65292;&#21457;&#29616;&#21465;&#36848;&#21644;&#23545;&#35805;&#22312;&#34920;&#36798;&#24773;&#24863;&#26102;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#20010;&#21035;&#35282;&#33394;&#30340;&#24773;&#24863;&#26354;&#32447;&#26356;&#20934;&#30830;&#22320;&#25429;&#25417;&#21040;&#25925;&#20107;&#24773;&#24863;&#26354;&#32447;&#30340;&#20849;&#21516;&#28857;&#25110;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#20107;&#22312;&#20854;&#21465;&#20107;&#20013;&#23637;&#31034;&#30340;&#24773;&#24863;&#20016;&#23500;&#65292;&#24182;&#21796;&#36215;&#35835;&#32773;&#30340;&#24773;&#24863;&#12290;&#25925;&#20107;&#20013;&#21508;&#20010;&#35282;&#33394;&#30340;&#24773;&#24863;&#21464;&#21270;&#23545;&#20854;&#21560;&#24341;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#24773;&#24863;&#30340;&#35745;&#31639;&#20998;&#26512;&#24456;&#23569;&#32771;&#34385;&#20102;&#19981;&#21516;&#35282;&#33394;&#20043;&#38388;&#24773;&#24863;&#36712;&#36857;&#30340;&#21464;&#21270;&#65292;&#32780;&#26159;&#35748;&#20026;&#25972;&#37096;&#23567;&#35828;&#20195;&#34920;&#19968;&#20010;&#21333;&#19968;&#30340;&#25925;&#20107;&#24359;&#32447;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#35282;&#33394;&#23545;&#35805;&#26469;&#21306;&#20998;&#21465;&#36848;&#21644;&#21508;&#20010;&#35282;&#33394;&#30340;&#24773;&#24863;&#24359;&#32447;&#12290;&#25105;&#20204;&#20351;&#29992;&#35805;&#35821;&#24773;&#24863;&#21160;&#24577;&#26694;&#26550;&#20998;&#26512;&#20102;&#19968;&#32452;&#33521;&#25991;&#25991;&#23398;&#23567;&#35828;&#20013;&#21508;&#20010;&#35282;&#33394;&#30340;&#24773;&#24863;&#26354;&#32447;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21465;&#36848;&#19982;&#23545;&#35805;&#22312;&#23567;&#35828;&#20013;&#30340;&#24773;&#24863;&#34920;&#36798;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#19981;&#21516;&#30340;&#65292;&#25925;&#20107;&#24773;&#24863;&#26354;&#32447;&#30340;&#20849;&#21516;&#28857;&#25110;&#24046;&#24322;&#26356;&#20934;&#30830;&#22320;&#34987;&#19982;&#20010;&#21035;ch&#30456;&#20851;&#32852;&#30340;&#24773;&#24863;&#26354;&#32447;&#25429;&#25417;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02474v1 Announce Type: new  Abstract: Stories are rich in the emotions they exhibit in their narratives and evoke in the readers. The emotional journeys of the various characters within a story are central to their appeal. Computational analysis of the emotions of novels, however, has rarely examined the variation in the emotional trajectories of the different characters within them, instead considering the entire novel to represent a single story arc. In this work, we use character dialogue to distinguish between the emotion arcs of the narration and the various characters. We analyze the emotion arcs of the various characters in a dataset of English literary novels using the framework of Utterance Emotion Dynamics. Our findings show that the narration and the dialogue largely express disparate emotions through the course of a novel, and that the commonalities or differences in the emotional arcs of stories are more accurately captured by those associated with individual ch
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#30340;&#31038;&#21306;&#22522;&#30784;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;OffLanDat&#65292;&#20026;38&#20010;&#19981;&#21516;&#30446;&#26631;&#32676;&#20307;&#25552;&#20379;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2403.02472</link><description>&lt;p&gt;
OffLanDat&#65306;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#31038;&#21306;&#22522;&#30784;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OffLanDat: A Community Based Implicit Offensive Language Dataset Generated by Large Language Model Through Prompt Engineering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02472
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#24314;&#30340;&#31038;&#21306;&#22522;&#30784;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;OffLanDat&#65292;&#20026;38&#20010;&#19981;&#21516;&#30446;&#26631;&#32676;&#20307;&#25552;&#20379;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#25915;&#20987;&#24615;&#35821;&#35328;&#30340;&#26222;&#36941;&#23384;&#22312;&#23545;&#31038;&#20250;&#31119;&#31049;&#20135;&#29983;&#20102;&#19981;&#33391;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#39640;&#24230;&#37325;&#35270;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#25915;&#20987;&#24615;&#35821;&#35328;&#26082;&#23384;&#22312;&#26126;&#30830;&#24418;&#24335;&#65292;&#20063;&#23384;&#22312;&#38544;&#24335;&#24418;&#24335;&#65292;&#21518;&#32773;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#24403;&#21069;&#22312;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#36935;&#21040;&#20960;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#20027;&#35201;&#20381;&#36182;&#20110;&#25910;&#38598;&#21253;&#21547;&#26126;&#30830;&#25915;&#20987;&#24615;&#20851;&#38190;&#35789;&#30340;&#25991;&#26412;&#65292;&#36825;&#20351;&#24471;&#25429;&#25417;&#19981;&#21253;&#21547;&#36825;&#20123;&#20851;&#38190;&#35789;&#19988;&#38544;&#21547;&#25915;&#20987;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20854;&#27425;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#35770;&#20542;&#21521;&#20110;&#20165;&#20851;&#27880;&#25991;&#26412;&#20998;&#26512;&#65292;&#24573;&#35270;&#31038;&#21306;&#20449;&#24687;&#21487;&#20197;&#25552;&#20379;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;&#22312;&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;OffLanDat&#65292;&#36825;&#26159;&#30001;ChatGPT&#29983;&#25104;&#30340;&#22522;&#20110;&#31038;&#21306;&#30340;&#38544;&#24335;&#25915;&#20987;&#24615;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;38&#20010;&#19981;&#21516;&#30446;&#26631;&#32676;&#20307;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02472v1 Announce Type: new  Abstract: The widespread presence of offensive languages on social media has resulted in adverse effects on societal well-being. As a result, it has become very important to address this issue with high priority. Offensive languages exist in both explicit and implicit forms, with the latter being more challenging to detect. Current research in this domain encounters several challenges. Firstly, the existing datasets primarily rely on the collection of texts containing explicit offensive keywords, making it challenging to capture implicitly offensive contents that are devoid of these keywords. Secondly, usual methodologies tend to focus solely on textual analysis, neglecting the valuable insights that community information can provide. In this research paper, we introduce a novel dataset OffLanDat, a community based implicit offensive language dataset generated by ChatGPT containing data for 38 different target groups. Despite limitations in genera
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#21475;&#35821;&#23545;&#35805;&#30340;ToM&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;LM&#22312;ToM&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#24182;&#34920;&#26126;&#22312;Common-ToM&#19978;&#25972;&#21512;&#31616;&#21333;&#26126;&#30830;&#30340;&#20449;&#24565;&#34920;&#31034;&#21487;&#20197;&#25552;&#39640;LM&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02451</link><description>&lt;p&gt;
&#35266;&#28857;&#26159;&#25105;&#30340;&#65292;&#20063;&#26159;&#20320;&#30340;&#65306;&#20351;&#29992;&#20849;&#21516;&#22522;&#30784;&#35780;&#20272;&#24515;&#28789;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Views Are My Own, But Also Yours: Benchmarking Theory of Mind using Common Ground
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02451
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#21475;&#35821;&#23545;&#35805;&#30340;ToM&#25968;&#25454;&#38598;&#65292;&#23637;&#31034;&#20102;LM&#22312;ToM&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#24182;&#34920;&#26126;&#22312;Common-ToM&#19978;&#25972;&#21512;&#31616;&#21333;&#26126;&#30830;&#30340;&#20449;&#24565;&#34920;&#31034;&#21487;&#20197;&#25552;&#39640;LM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#24515;&#28789;&#29702;&#35770;&#65288;ToM&#65289;&#33021;&#21147;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20381;&#36182;&#20110;&#21512;&#25104;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#23454;&#39564;&#32467;&#26524;&#19982;&#20154;&#31867;&#34892;&#20026;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#33258;&#28982;&#21457;&#29983;&#30340;&#21475;&#22836;&#23545;&#35805;&#30340;ToM&#25968;&#25454;&#38598;&#65292;Common-ToM&#65292;&#24182;&#23637;&#31034;LMs&#22312;&#23637;&#31034;ToM&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;Common-ToM&#19978;&#25972;&#21512;&#19968;&#20010;&#31616;&#21333;&#26126;&#30830;&#30340;&#20449;&#24565;&#34920;&#31034;&#21487;&#20197;&#25552;&#39640;LM&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02451v1 Announce Type: new  Abstract: Evaluating the theory of mind (ToM) capabilities of language models (LMs) has recently received much attention. However, many existing benchmarks rely on synthetic data which risks misaligning the resulting experiments with human behavior. We introduce the first ToM dataset based on naturally occurring spoken dialogs, Common-ToM, and show that LMs struggle to demonstrate ToM. We then show that integrating a simple, explicit representation of beliefs improves LM performance on Common-ToM.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24314;&#31569;&#22914;&#20309;&#24433;&#21709;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#33021;&#21147;&#65292;&#21457;&#29616;&#20102;FFN-Wider&#21464;&#21387;&#22120;&#27169;&#22411;&#38477;&#20302;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#36129;&#29486;&#27604;&#65292;&#20174;&#32780;&#23548;&#33268;&#22522;&#26412;&#33021;&#21147;&#30340;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.02436</link><description>&lt;p&gt;
&#24314;&#31569;&#22914;&#20309;&#24433;&#21709;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#33021;&#21147;&#65311;&#22522;&#20110;FFN-Wider&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How does Architecture Influence the Base Capabilities of Pre-trained Language Models? A Case Study Based on FFN-Wider Transformer Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#24314;&#31569;&#22914;&#20309;&#24433;&#21709;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#33021;&#21147;&#65292;&#21457;&#29616;&#20102;FFN-Wider&#21464;&#21387;&#22120;&#27169;&#22411;&#38477;&#20302;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#36129;&#29486;&#27604;&#65292;&#20174;&#32780;&#23548;&#33268;&#22522;&#26412;&#33021;&#21147;&#30340;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#30340;&#22522;&#26412;&#33021;&#21147;&#65292;&#19981;&#20165;&#22312;&#20998;&#24067;&#24335;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#22312;&#36229;&#20986;&#20998;&#24067;&#24335;&#35821;&#35328;&#24314;&#27169;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#38754;&#20063;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#19982;&#29616;&#26377;&#30740;&#31350;&#20391;&#37325;&#20110;&#35268;&#27169;&#23545;&#22522;&#26412;&#33021;&#21147;&#30340;&#24433;&#21709;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#37325;&#28857;&#25918;&#22312;&#20102;&#26550;&#26500;&#23545;&#20854;&#24433;&#21709;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20851;&#24515;&#30340;&#26159;&#65306;&#24314;&#31569;&#22914;&#20309;&#24433;&#21709;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#26412;&#33021;&#21147;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#35299;&#37322;&#24182;&#36870;&#36716;FFN-Wider&#21464;&#21387;&#22120;&#30340;&#26550;&#26500;&#23548;&#33268;&#22522;&#26412;&#33021;&#21147;&#19979;&#38477;&#30340;&#24773;&#20917;&#65292;&#21147;&#27714;&#25552;&#20379;&#19968;&#20123;&#35265;&#35299;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22810;&#22836;&#27880;&#24847;&#21147;&#65288;&#19968;&#31181;&#32452;&#21512;&#20989;&#25968;&#65289;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#24314;&#27169;&#30340;&#36129;&#29486;&#27604;&#26159;&#24433;&#21709;&#22522;&#26412;&#33021;&#21147;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;FFN-Wider&#21464;&#21387;&#22120;&#20943;&#23569;&#20102;&#36825;&#31181;&#32452;&#21512;&#20989;&#25968;&#30340;&#36129;&#29486;&#27604;&#65292;&#23548;&#33268;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02436v1 Announce Type: new  Abstract: Pre-trained language models have been proven to possess strong base capabilities, which not only excel in in-distribution language modeling but also show powerful abilities in out-of-distribution language modeling, transfer learning and few-shot learning. Unlike existing work focusing on the influence of scale on base capabilities, our work examines the influence of architecture on those. Specifically, our concern is: How does architecture influence the base capabilities of pre-trained language models? In this work, we attempt to explain and reverse the decline in base capabilities caused by the architecture of FFN-Wider Transformers, seeking to provide some insights. Through analysis, we found the contribution ratio of Multi-Head Attention (a combination function) to pre-trained language modeling is a key factor affecting base capabilities. FFN-Wider Transformers reduce the contribution ratio of this combination function, leading to a d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;&#65292;&#21457;&#29616;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021;&#38543;LLM&#35843;&#29992;&#27425;&#25968;&#22686;&#21152;&#20808;&#22686;&#21152;&#21518;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2403.02419</link><description>&lt;p&gt;
&#20320;&#38656;&#35201;&#26356;&#22810;LLM&#35843;&#29992;&#21527;&#65311;&#36208;&#21521;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Are More LLM Calls All You Need? Towards Scaling Laws of Compound Inference Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;&#65292;&#21457;&#29616;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021;&#38543;LLM&#35843;&#29992;&#27425;&#25968;&#22686;&#21152;&#20808;&#22686;&#21152;&#21518;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26368;&#36817;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#26159;&#36890;&#36807;&#25191;&#34892;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35843;&#29992;&#24182;&#27719;&#24635;&#23427;&#20204;&#30340;&#21709;&#24212;&#30340;&#22797;&#21512;&#31995;&#32479;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLM&#35843;&#29992;&#27425;&#25968;&#30340;&#24433;&#21709; -- &#20363;&#22914;&#65292;&#24403;&#35201;&#27714;LLM&#22810;&#27425;&#22238;&#31572;&#27599;&#20010;&#38382;&#39064;&#24182;&#21462;&#24471;&#20849;&#35782;&#26102; -- &#23545;&#20110;&#36825;&#31181;&#22797;&#21512;&#31995;&#32479;&#30340;&#24615;&#33021;&#20102;&#35299;&#29978;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#22797;&#21512;&#25512;&#29702;&#31995;&#32479;&#30340;&#25193;&#23637;&#23450;&#24459;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;LLM&#35843;&#29992;&#27425;&#25968;&#22914;&#20309;&#24433;&#21709;&#19968;&#20010;&#23618;&#32423;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021; -- &#36825;&#26159;&#26368;&#31616;&#21333;&#30340;&#22797;&#21512;&#31995;&#32479;&#20043;&#19968;&#65292;&#23427;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#32858;&#21512;LLM&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25237;&#31080;&#25512;&#29702;&#31995;&#32479;&#30340;&#24615;&#33021;&#38543;&#30528;LLM&#35843;&#29992;&#27425;&#25968;&#30340;&#22686;&#21152;&#32780;&#20808;&#22686;&#21152;&#21518;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#38750;&#21333;&#35843;&#24615;&#26159;&#30001;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02419v1 Announce Type: cross  Abstract: Many recent state-of-the-art results in language tasks were achieved using compound systems that perform multiple Large Language Model (LLM) calls and aggregate their responses. However, there is little understanding of how the number of LLM calls -- e.g., when asking the LLM to answer each question multiple times and taking a consensus -- affects such a compound system's performance. In this paper, we initiate the study of scaling laws of compound inference systems. We analyze, theoretically and empirically, how the number of LLM calls affects the performance of one-layer Voting Inference Systems -- one of the simplest compound systems, which aggregates LLM responses via majority voting. We find empirically that across multiple language tasks, surprisingly, Voting Inference Systems' performance first increases but then decreases as a function of the number of LLM calls. Our theoretical results suggest that this non-monotonicity is due
&lt;/p&gt;</description></item><item><title>&#36825;&#19968;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;108&#20301;&#27597;&#35821;&#20026;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#35828;&#35805;&#32773;&#30340;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#35821;&#38899;&#20219;&#21153;&#65292;&#36890;&#36807;&#25163;&#21160;&#21644;&#33258;&#21160;&#36716;&#24405;&#30830;&#20445;&#20102;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02371</link><description>&lt;p&gt;
NeuroVoz&#65306;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#30340;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
NeuroVoz: a Castillian Spanish corpus of parkinsonian speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02371
&lt;/p&gt;
&lt;p&gt;
&#36825;&#19968;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;108&#20301;&#27597;&#35821;&#20026;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#35828;&#35805;&#32773;&#30340;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#35821;&#38899;&#20219;&#21153;&#65292;&#36890;&#36807;&#25163;&#21160;&#21644;&#33258;&#21160;&#36716;&#24405;&#30830;&#20445;&#20102;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35821;&#38899;&#20998;&#26512;&#36827;&#34892;&#24085;&#37329;&#26862;&#30149;&#65288;PD&#65289;&#35786;&#26029;&#30340;&#36827;&#23637;&#21463;&#21040;&#20844;&#24320;&#21487;&#29992;&#12289;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#26174;&#33879;&#32570;&#20047;&#30340;&#38459;&#30861;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#20877;&#29616;&#24615;&#21644;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#26469;&#33258;108&#20301;&#27597;&#35821;&#20026;&#21345;&#26031;&#33922;&#21033;&#20122;&#35821;&#30340;&#35828;&#35805;&#32773;&#65292;&#21253;&#25324;55&#21517;&#20581;&#24247;&#23545;&#29031;&#32452;&#21644;53&#21517;&#34987;&#35786;&#26029;&#24739;&#26377;PD&#30340;&#20010;&#20307;&#65292;&#25152;&#26377;&#36825;&#20123;&#20010;&#20307;&#37117;&#22312;&#33647;&#29289;&#27835;&#30103;&#19979;&#65292;&#24182;&#19988;&#22312;&#33647;&#29289;&#20248;&#21270;&#29366;&#24577;&#19979;&#36827;&#34892;&#35760;&#24405;&#12290; &#36825;&#19968;&#29420;&#29305;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;&#24191;&#27867;&#30340;&#35821;&#38899;&#20219;&#21153;&#65292;&#21253;&#25324;&#25345;&#32493;&#21457;&#38899;&#20116;&#20010;&#35199;&#29677;&#29273;&#20803;&#38899;&#12289;&#21457;&#38899;&#27979;&#35797;&#12289;16&#20010;&#21548;&#21518;&#37325;&#22797;&#30340;&#35805;&#35821;&#20197;&#21450;&#33258;&#30001;&#29420;&#30333;&#12290;&#35813;&#25968;&#25454;&#38598;&#36890;&#36807;&#19987;&#23478;&#25163;&#21160;&#36716;&#24405;&#21548;&#21518;&#37325;&#22797;&#20219;&#21153;&#24378;&#35843;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#21033;&#29992;Whisper&#36827;&#34892;&#33258;&#21160;&#29420;&#30333;&#36716;&#24405;&#65292;&#20351;&#20854;&#25104;&#20026;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#35821;&#38899;&#30340;&#26368;&#23436;&#25972;&#30340;&#20844;&#24320;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02371v1 Announce Type: cross  Abstract: The advancement of Parkinson's Disease (PD) diagnosis through speech analysis is hindered by a notable lack of publicly available, diverse language datasets, limiting the reproducibility and further exploration of existing research.   In response to this gap, we introduce a comprehensive corpus from 108 native Castilian Spanish speakers, comprising 55 healthy controls and 53 individuals diagnosed with PD, all of whom were under pharmacological treatment and recorded in their medication-optimized state. This unique dataset features a wide array of speech tasks, including sustained phonation of the five Spanish vowels, diadochokinetic tests, 16 listen-and-repeat utterances, and free monologues. The dataset emphasizes accuracy and reliability through specialist manual transcriptions of the listen-and-repeat tasks and utilizes Whisper for automated monologue transcriptions, making it the most complete public corpus of Parkinsonian speech, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;adaptMLLM&#65292;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#20302;&#36164;&#28304;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#38382;&#39064;&#30340;&#24320;&#28304;&#24212;&#29992;&#31243;&#24207;&#65292;&#35813;&#24212;&#29992;&#31243;&#24207;&#31616;&#21270;&#20102;&#23545;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#25152;&#26377;&#27969;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.02370</link><description>&lt;p&gt;
adaptMLLM&#65306;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#29992;&#38598;&#25104;LLM&#28216;&#20048;&#22330;&#23545;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
adaptMLLM: Fine-Tuning Multilingual Language Models on Low-Resource Languages with Integrated LLM Playgrounds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02370
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;adaptMLLM&#65292;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#20302;&#36164;&#28304;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#38382;&#39064;&#30340;&#24320;&#28304;&#24212;&#29992;&#31243;&#24207;&#65292;&#35813;&#24212;&#29992;&#31243;&#24207;&#31616;&#21270;&#20102;&#23545;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#25152;&#26377;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Multilingual Language Models (MLLMs)&#21644;Large Language Models&#30340;&#20986;&#29616;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35768;&#22810;&#39046;&#22495;&#24341;&#21457;&#20102;&#21019;&#26032;&#12290;&#23613;&#31649;&#36825;&#39033;&#25216;&#26415;&#20855;&#26377;&#20196;&#20154;&#20852;&#22859;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#24320;&#21457;&#39640;&#36136;&#37327;&#30340;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#36755;&#20986;&#30340;&#24433;&#21709;&#30456;&#23545;&#36739;&#23569;&#25506;&#35752;&#12290;&#27492;&#22806;&#65292;&#23578;&#26410;&#25512;&#20986;&#19968;&#20010;&#24320;&#28304;&#24212;&#29992;&#31243;&#24207;&#65292;&#19987;&#38376;&#29992;&#20110;&#23545;MLLM&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#31649;&#29702;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#23436;&#25972;MT&#24037;&#20316;&#27969;&#31243;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;adaptMLLM&#26469;&#35299;&#20915;&#36825;&#20123;&#19981;&#24179;&#34913;&#65292;&#35813;&#24212;&#29992;&#31243;&#24207;&#31616;&#21270;&#20102;&#29992;&#20110;MT&#30340;MLLM&#24494;&#35843;&#30340;&#25152;&#26377;&#27969;&#31243;&#12290;&#36825;&#20010;&#24320;&#28304;&#24212;&#29992;&#31243;&#24207;&#19987;&#38376;&#20026;&#20174;&#20107;MT&#30340;&#24320;&#21457;&#20154;&#21592;&#12289;&#32763;&#35793;&#20154;&#21592;&#21644;&#29992;&#25143;&#37327;&#36523;&#23450;&#21046;&#12290;&#30452;&#35266;&#30340;&#30028;&#38754;&#20801;&#35768;&#36731;&#26494;&#22320;&#33258;&#23450;&#20041;&#36229;&#21442;&#25968;&#65292;&#35813;&#24212;&#29992;&#31243;&#24207;&#25552;&#20379;&#19968;&#31995;&#21015;&#29992;&#20110;&#27169;&#22411;&#35780;&#20272;&#30340;&#25351;&#26631;&#65292;&#24182;&#20855;&#26377;&#37096;&#32626;&#27169;&#22411;&#20316;&#20026;&#32763;&#35793;&#26381;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02370v1 Announce Type: cross  Abstract: The advent of Multilingual Language Models (MLLMs) and Large Language Models has spawned innovation in many areas of natural language processing. Despite the exciting potential of this technology, its impact on developing high-quality Machine Translation (MT) outputs for low-resource languages remains relatively under-explored. Furthermore, an open-source application, dedicated to both fine-tuning MLLMs and managing the complete MT workflow for low-resources languages, remains unavailable. We aim to address these imbalances through the development of adaptMLLM, which streamlines all processes involved in the fine-tuning of MLLMs for MT. This open-source application is tailored for developers, translators, and users who are engaged in MT. An intuitive interface allows for easy customisation of hyperparameters, and the application offers a range of metrics for model evaluation and the capability to deploy models as a translation service 
&lt;/p&gt;</description></item><item><title>adaptNMT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24320;&#21457;&#29615;&#22659;&#65292;&#31616;&#21270;&#20102;&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#27969;&#31243;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#26032;&#25163;&#29992;&#25143;&#65292;&#24182;&#25552;&#20379;&#22270;&#24418;&#23637;&#31034;&#12289;&#23376;&#35789;&#20998;&#21106;&#27169;&#22411;&#31561;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.02367</link><description>&lt;p&gt;
adaptNMT&#65306;&#19968;&#31181;&#38754;&#21521;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24320;&#28304;&#12289;&#35821;&#35328;&#26080;&#20851;&#30340;&#24320;&#21457;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
adaptNMT: an open-source, language-agnostic development environment for Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02367
&lt;/p&gt;
&lt;p&gt;
adaptNMT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#24320;&#21457;&#29615;&#22659;&#65292;&#31616;&#21270;&#20102;&#27169;&#22411;&#24320;&#21457;&#21644;&#37096;&#32626;&#27969;&#31243;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#26032;&#25163;&#29992;&#25143;&#65292;&#24182;&#25552;&#20379;&#22270;&#24418;&#23637;&#31034;&#12289;&#23376;&#35789;&#20998;&#21106;&#27169;&#22411;&#31561;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2403.02367v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25688;&#35201;&#65306;adaptNMT&#31616;&#21270;&#20102;RNN&#21644;Transformer&#31070;&#32463;&#32763;&#35793;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#20013;&#28041;&#21450;&#30340;&#25152;&#26377;&#27969;&#31243;&#12290;&#20316;&#20026;&#19968;&#27454;&#24320;&#28304;&#24212;&#29992;&#31243;&#24207;&#65292;&#23427;&#26088;&#22312;&#38754;&#21521;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#30340;&#25216;&#26415;&#21644;&#38750;&#25216;&#26415;&#29992;&#25143;&#12290;&#35813;&#24212;&#29992;&#26159;&#24314;&#31435;&#22312;&#24191;&#27867;&#37319;&#29992;&#30340;OpenNMT&#29983;&#24577;&#31995;&#32479;&#20043;&#19978;&#30340;&#65292;&#23545;&#20110;&#26032;&#36827;&#20837;&#35813;&#39046;&#22495;&#30340;&#29992;&#25143;&#29305;&#21035;&#26377;&#29992;&#65292;&#22240;&#20026;&#24320;&#21457;&#29615;&#22659;&#30340;&#35774;&#32622;&#20197;&#21450;&#21019;&#24314;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#20998;&#21106;&#34987;&#22823;&#22823;&#31616;&#21270;&#12290;&#24212;&#29992;&#31243;&#24207;&#20869;&#32622;&#22270;&#24418;&#23637;&#31034;&#20102;&#27169;&#22411;&#35757;&#32451;&#30340;&#36827;&#24230;&#65292;&#24182;&#20351;&#29992;SentencePiece&#21019;&#24314;&#23376;&#35789;&#20998;&#21106;&#27169;&#22411;&#12290;&#36890;&#36807;&#30452;&#35266;&#30340;&#29992;&#25143;&#30028;&#38754;&#65292;&#21487;&#20197;&#20415;&#25463;&#22320;&#23450;&#21046;&#36229;&#21442;&#25968;&#65292;&#23454;&#26045;&#20102;&#19968;&#38190;&#24335;&#27169;&#22411;&#24320;&#21457;&#26041;&#27861;&#12290;&#30001;adaptNMT&#24320;&#21457;&#30340;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#20316;&#20026;&#32763;&#35793;&#26381;&#21153;&#22312;&#24212;&#29992;&#31243;&#24207;&#20013;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02367v1 Announce Type: cross  Abstract: adaptNMT streamlines all processes involved in the development and deployment of RNN and Transformer neural translation models. As an open-source application, it is designed for both technical and non-technical users who work in the field of machine translation. Built upon the widely-adopted OpenNMT ecosystem, the application is particularly useful for new entrants to the field since the setup of the development environment and creation of train, validation and test splits is greatly simplified. Graphing, embedded within the application, illustrates the progress of model training, and SentencePiece is used for creating subword segmentation models. Hyperparameter customization is facilitated through an intuitive user interface, and a single-click model development approach has been implemented. Models developed by adaptNMT can be evaluated using a range of metrics, and deployed as a translation service within the application. To support
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#36229;&#21442;&#25968;&#35774;&#32622;&#23545;&#20302;&#36164;&#28304;&#33521;-&#29233;&#21464;&#21387;&#22120;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20248;&#21270;&#30340;Transformer&#27169;&#22411;&#22312;16k BPE&#23376;&#35789;&#27169;&#22411;&#19979;&#34920;&#29616;&#26368;&#20339;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;RNN&#27169;&#22411;&#25552;&#39640;&#20102;7.8&#20010;BLEU&#20998;&#25968;&#65292;&#24182;&#22312;&#19982;&#35895;&#27468;&#32763;&#35793;&#30340;&#27604;&#36739;&#20013;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.02366</link><description>&lt;p&gt;
&#20154;&#31867;&#35780;&#20272;&#33521;&#29233;&#21464;&#21387;&#22120;&#22522;&#20110;NMT&#30340;&#32763;&#35793;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Human Evaluation of English--Irish Transformer-Based NMT
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#36229;&#21442;&#25968;&#35774;&#32622;&#23545;&#20302;&#36164;&#28304;&#33521;-&#29233;&#21464;&#21387;&#22120;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20248;&#21270;&#30340;Transformer&#27169;&#22411;&#22312;16k BPE&#23376;&#35789;&#27169;&#22411;&#19979;&#34920;&#29616;&#26368;&#20339;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;RNN&#27169;&#22411;&#25552;&#39640;&#20102;7.8&#20010;BLEU&#20998;&#25968;&#65292;&#24182;&#22312;&#19982;&#35895;&#27468;&#32763;&#35793;&#30340;&#27604;&#36739;&#20013;&#23637;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#23545;&#36229;&#21442;&#25968;&#35774;&#32622;&#22914;&#20309;&#24433;&#21709;&#20302;&#36164;&#28304;&#33521;&#35821;-&#29233;&#23572;&#20848;&#25991;&#23545;&#30340;&#21464;&#21387;&#22120;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#36136;&#37327;&#36827;&#34892;&#20102;&#20154;&#31867;&#35780;&#20272;&#12290;&#20351;&#29992;Byte Pair Encoding&#65288;BPE&#65289;&#21644;unigram&#26041;&#27861;&#30340;SentencePiece&#27169;&#22411;&#21463;&#21040;&#20102;&#35780;&#20215;&#12290;&#27169;&#22411;&#26550;&#26500;&#30340;&#21464;&#21270;&#21253;&#25324;&#20462;&#25913;&#23618;&#25968;&#65292;&#35780;&#20272;&#27880;&#24847;&#21147;&#30340;&#26368;&#20339;&#22836;&#25968;&#20197;&#21450;&#27979;&#35797;&#21508;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#22312;&#19968;&#20010;&#20248;&#21270;&#20102;&#30340;16k BPE&#23376;&#35789;&#27169;&#22411;&#19979;&#65292;Transformer&#20248;&#21270;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#24471;&#26368;&#22909;&#12290;&#19982;&#22522;&#20934;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#27169;&#22411;&#30456;&#27604;&#65292;Transformer&#20248;&#21270;&#27169;&#22411;&#30340;BLEU&#20998;&#25968;&#25552;&#39640;&#20102;7.8&#20010;&#28857;&#12290;&#19982;&#35895;&#27468;&#32763;&#35793;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#32763;&#35793;&#24341;&#25806;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#36827;&#34892;&#20102;&#23450;&#37327;&#32454;&#31890;&#24230;&#30340;&#25163;&#21160;&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02366v1 Announce Type: cross  Abstract: In this study, a human evaluation is carried out on how hyperparameter settings impact the quality of Transformer-based Neural Machine Translation (NMT) for the low-resourced English--Irish pair. SentencePiece models using both Byte Pair Encoding (BPE) and unigram approaches were appraised. Variations in model architectures included modifying the number of layers, evaluating the optimal number of heads for attention and testing various regularisation techniques. The greatest performance improvement was recorded for a Transformer-optimized model with a 16k BPE subword model. Compared with a baseline Recurrent Neural Network (RNN) model, a Transformer-optimized model demonstrated a BLEU score improvement of 7.8 points. When benchmarked against Google Translate, our translation engines demonstrated significant improvements. Furthermore, a quantitative fine-grained manual evaluation was conducted which compared the performance of machine t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3.5&#21644;GPT-4&#20174;&#20135;&#21697;&#26631;&#39064;&#21644;&#25551;&#36848;&#20013;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#23646;&#24615;&#20540;&#30340;&#28508;&#21147;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;WDC PAVE&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#23454;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.02130</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#20135;&#21697;&#23646;&#24615;&#20540;
&lt;/p&gt;
&lt;p&gt;
Using LLMs for the Extraction and Normalization of Product Attribute Values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3.5&#21644;GPT-4&#20174;&#20135;&#21697;&#26631;&#39064;&#21644;&#25551;&#36848;&#20013;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#23646;&#24615;&#20540;&#30340;&#28508;&#21147;&#65292;&#24341;&#20837;&#20102;&#26032;&#30340;WDC PAVE&#25968;&#25454;&#38598;&#26469;&#25903;&#25345;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#32593;&#31449;&#19978;&#30340;&#20135;&#21697;&#25552;&#20379;&#36890;&#24120;&#21253;&#25324;&#25991;&#26412;&#20135;&#21697;&#26631;&#39064;&#21644;&#25991;&#26412;&#20135;&#21697;&#25551;&#36848;&#12290;&#20026;&#20102;&#25552;&#20379;&#35832;&#22914;&#20998;&#38754;&#20135;&#21697;&#36807;&#28388;&#25110;&#22522;&#20110;&#20869;&#23481;&#30340;&#20135;&#21697;&#25512;&#33616;&#31561;&#21151;&#33021;&#65292;&#32593;&#31449;&#38656;&#35201;&#20174;&#38750;&#32467;&#26500;&#21270;&#20135;&#21697;&#25551;&#36848;&#20013;&#25552;&#21462;&#23646;&#24615;&#20540;&#23545;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;OpenAI&#30340;GPT-3.5&#21644;GPT-4&#65292;&#20174;&#20135;&#21697;&#26631;&#39064;&#21644;&#20135;&#21697;&#25551;&#36848;&#20013;&#25552;&#21462;&#21644;&#35268;&#33539;&#21270;&#23646;&#24615;&#20540;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;WDC&#20135;&#21697;&#23646;&#24615;-&#20540;&#25552;&#21462;&#65288;WDC PAVE&#65289;&#25968;&#25454;&#38598;&#12290;WDC PAVE&#21253;&#21547;&#26469;&#33258;&#25552;&#20379;schema.org&#27880;&#37322;&#30340;87&#20010;&#32593;&#31449;&#30340;&#20135;&#21697;&#25552;&#20379;&#12290;&#36825;&#20123;&#25552;&#20379;&#23646;&#20110;&#20116;&#20010;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#27599;&#20010;&#31867;&#21035;&#37117;&#20855;&#26377;&#19968;&#32452;&#29305;&#23450;&#30340;&#23646;&#24615;&#12290;&#35813;&#25968;&#25454;&#38598;&#20197;&#20004;&#31181;&#24418;&#24335;&#25552;&#20379;&#25163;&#21160;&#39564;&#35777;&#30340;&#23646;&#24615;-&#20540;&#23545;&#65306;&#65288;i&#65289;&#30452;&#25509;&#25552;&#21462;&#30340;&#20540;&#21644;&#65288;ii&#65289;&#35268;&#33539;&#21270;&#30340;&#23646;&#24615;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02130v1 Announce Type: new  Abstract: Product offers on e-commerce websites often consist of a textual product title and a textual product description. In order to provide features such as faceted product filtering or content-based product recommendation, the websites need to extract attribute-value pairs from the unstructured product descriptions. This paper explores the potential of using large language models (LLMs), such as OpenAI's GPT-3.5 and GPT-4, to extract and normalize attribute values from product titles and product descriptions. For our experiments, we introduce the WDC Product Attribute-Value Extraction (WDC PAVE) dataset. WDC PAVE consists of product offers from 87 websites that provide schema.org annotations. The offers belong to five different categories, each featuring a specific set of attributes. The dataset provides manually verified attribute-value pairs in two forms: (i) directly extracted values and (ii) normalized attribute values. The normalization 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#33889;&#33796;&#29273;&#35821;&#30340;&#31070;&#32463;&#32534;&#30721;&#20316;&#20986;&#20102;&#36129;&#29486;&#65292;&#25193;&#23637;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#24577;&#31995;&#32479;&#65292;&#24182;&#21457;&#24067;&#20102;&#21253;&#25324;&#20159;&#32423;&#21442;&#25968; Albertina &#21644; Bertimbau &#22312;&#20869;&#30340;&#24320;&#28304;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25512;&#36827;&#20102;&#33889;&#33796;&#29273;&#35821;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.01897</link><description>&lt;p&gt;
&#20419;&#36827;&#33889;&#33796;&#29273;&#35821;&#30340;&#24320;&#25918;&#31070;&#32463;&#32534;&#30721;&#22120;&#29983;&#24577;&#31995;&#32479;&#19982;Albertina PT*&#23478;&#26063;
&lt;/p&gt;
&lt;p&gt;
Fostering the Ecosystem of Open Neural Encoders for Portuguese with Albertina PT* Family
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#33889;&#33796;&#29273;&#35821;&#30340;&#31070;&#32463;&#32534;&#30721;&#20316;&#20986;&#20102;&#36129;&#29486;&#65292;&#25193;&#23637;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#24577;&#31995;&#32479;&#65292;&#24182;&#21457;&#24067;&#20102;&#21253;&#25324;&#20159;&#32423;&#21442;&#25968; Albertina &#21644; Bertimbau &#22312;&#20869;&#30340;&#24320;&#28304;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25512;&#36827;&#20102;&#33889;&#33796;&#29273;&#35821;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20419;&#36827;&#33889;&#33796;&#29273;&#35821;&#30340;&#31070;&#32463;&#32534;&#30721;&#65292;&#26412;&#25991;&#36129;&#29486;&#20102;&#20195;&#34920;&#22522;&#30784;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#20195;&#34920;&#20102;&#19968;&#20010;&#20173;&#28982;&#38750;&#24120;&#31232;&#32570;&#30340;&#38024;&#23545;&#35813;&#35821;&#35328;&#29305;&#21035;&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#24577;&#31995;&#32479;&#30340;&#25193;&#23637;&#65292;&#36825;&#20123;&#27169;&#22411;&#23436;&#20840;&#26159;&#24320;&#25918;&#30340;&#65292;&#21363;&#23427;&#20204;&#26159;&#24320;&#28304;&#30340;&#65292;&#24182;&#22312;&#19968;&#20010;&#24320;&#25918;&#35768;&#21487;&#19979;&#20813;&#36153;&#20998;&#21457;&#65292;&#21487;&#29992;&#20110;&#20219;&#20309;&#30446;&#30340;&#65292;&#21253;&#25324;&#30740;&#31350;&#21644;&#21830;&#19994;&#29992;&#36884;&#12290;&#19982;&#33521;&#35821;&#20197;&#22806;&#30340;&#22823;&#22810;&#25968;&#35821;&#35328;&#19968;&#26679;&#65292;&#33889;&#33796;&#29273;&#35821;&#22312;&#36825;&#20123;&#22522;&#30784;&#35821;&#35328;&#36164;&#28304;&#26041;&#38754;&#36164;&#28304;&#21294;&#20047;&#65292;&#36825;&#37324;&#26377;&#39318;&#23626;&#25317;&#26377; 9 &#20159;&#20010;&#21442;&#25968;&#30340; Albertina &#21644; 3.35 &#20159;&#20010;&#21442;&#25968;&#30340; Bertimbau&#12290;&#22312;&#20197;&#36825;&#23545;&#27169;&#22411;&#20026;&#39318;&#27425;&#38598;&#21512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#24335;&#33889;&#33796;&#29273;&#35821;&#32534;&#30721;&#22120;&#29983;&#24577;&#31995;&#32479;&#30340;&#25193;&#23637;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#25317;&#26377; 15 &#20159;&#21442;&#25968;&#30340;&#26356;&#22823;&#22411;&#12289;&#24615;&#33021;&#39537;&#21160;&#30340;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#25317;&#26377; 1 &#20159;&#21442;&#25968;&#30340;&#26356;&#23567;&#22411;&#12289;&#25928;&#29575;&#39537;&#21160;&#30340;&#27169;&#22411;&#12290;&#22312;&#23454;&#29616;&#36825;&#19968;&#20027;&#35201;&#30446;&#26631;&#30340;&#21516;&#26102;&#65292;&#36824;&#24471;&#21040;&#20102;&#19968;&#20123;&#36827;&#19968;&#27493;&#30340;&#25104;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01897v1 Announce Type: new  Abstract: To foster the neural encoding of Portuguese, this paper contributes foundation encoder models that represent an expansion of the still very scarce ecosystem of large language models specifically developed for this language that are fully open, in the sense that they are open source and openly distributed for free under an open license for any purpose, thus including research and commercial usages. Like most languages other than English, Portuguese is low-resourced in terms of these foundational language resources, there being the inaugural 900 million parameter Albertina and 335 million Bertimbau. Taking this couple of models as an inaugural set, we present the extension of the ecosystem of state-of-the-art open encoders for Portuguese with a larger, top performance-driven model with 1.5 billion parameters, and a smaller, efficiency-driven model with 100 million parameters. While achieving this primary goal, further results that are rele
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#21160;&#24577;&#22522;&#20934;NPHardEval4V&#65292;&#21457;&#29616;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#19981;&#21516;&#27169;&#22411;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#25581;&#31034;&#20102;&#30456;&#23545;&#20110;LLMs&#65292;MLLMs&#30340;&#25512;&#29702;&#24615;&#33021;&#36739;&#24369;&#12290;</title><link>https://arxiv.org/abs/2403.01777</link><description>&lt;p&gt;
NPHardEval4V: &#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#25512;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01777
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#21160;&#24577;&#22522;&#20934;NPHardEval4V&#65292;&#21457;&#29616;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#19981;&#21516;&#27169;&#22411;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#25581;&#31034;&#20102;&#30456;&#23545;&#20110;LLMs&#65292;MLLMs&#30340;&#25512;&#29702;&#24615;&#33021;&#36739;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#24577;&#22522;&#20934;&#65292;NPHardEval4V&#65292;&#26088;&#22312;&#35299;&#20915;&#35780;&#20272;MLLM&#32431;&#31929;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#29616;&#26377;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#24179;&#21488;&#65292;&#20197;&#35299;&#24320;&#35832;&#22810;&#22240;&#32032;&#65288;&#22914;&#22270;&#20687;&#35782;&#21035;&#21644;&#25351;&#20196;&#36981;&#24490;&#65289;&#23545;&#27169;&#22411;&#25972;&#20307;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#19987;&#27880;&#20110;&#35780;&#20272;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#22312;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#36739;&#20110;LLMs&#65292;MLLMs&#22312;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#30456;&#23545;&#36739;&#24369;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19981;&#21516;&#25552;&#31034;&#26679;&#24335;&#65288;&#21253;&#25324;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#32467;&#21512;&#35270;&#35273;&#19982;&#25991;&#26412;&#25552;&#31034;&#65289;&#23545;MLLM&#25512;&#29702;&#33021;&#21147;&#30340;&#24433;&#21709;&#65292;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;&#36755;&#20837;&#22312;&#27169;&#22411;&#24615;&#33021;&#20013;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01777v1 Announce Type: new  Abstract: Understanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V, aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall performance of the models, allowing us to focus solely on evaluating their reasoning abilities. Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles, including visual, text, and combined vision and text prompts, on the reasoning abilities of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. U
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#25512;&#21160;&#36234;&#21335;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#24320;&#21457;&#21644;&#20998;&#20139;&#24320;&#25918;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#36234;&#21335;&#35821;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.01616</link><description>&lt;p&gt;
&#26397;&#30528;&#20840;&#38754;&#30340;&#36234;&#21335;&#35821;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards Comprehensive Vietnamese Retrieval-Augmented Generation and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01616
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#25512;&#21160;&#36234;&#21335;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#24320;&#21457;&#21644;&#20998;&#20139;&#24320;&#25918;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#36234;&#21335;&#35821;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24320;&#21457;&#21644;&#20256;&#25773;&#29992;&#20110;&#36234;&#21335;&#35821;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#22312;&#25512;&#21160;&#36234;&#21335;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#27700;&#24179;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01616v1 Announce Type: new  Abstract: This paper presents our contributions towards advancing the state of Vietnamese language understanding and generation through the development and dissemination of open datasets and pre-trained models for Vietnamese Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#20869;&#37096;&#34920;&#24449;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20102;&#24187;&#35273;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#24335;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#65292;&#27491;&#30830;&#29983;&#25104;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#8220;&#38160;&#24230;&#8221;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01548</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#37096;&#34920;&#24449;&#30340;&#19978;&#19979;&#25991;&#38160;&#24230;&#20316;&#20026;&#35686;&#25253;&#65306;&#20943;&#23569;&#24187;&#35273;&#30340;&#19968;&#20010;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#20869;&#37096;&#34920;&#24449;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20102;&#24187;&#35273;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#24335;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#65292;&#27491;&#30830;&#29983;&#25104;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#8220;&#38160;&#24230;&#8221;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#21046;&#23450;&#20102;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#20250;&#20135;&#29983;&#24187;&#35273;&#24182;&#20135;&#29983;&#20107;&#23454;&#38169;&#35823;&#65292;&#28982;&#32780;&#25105;&#20204;&#23545;&#23427;&#20204;&#20026;&#20160;&#20040;&#20250;&#29359;&#36825;&#20123;&#38169;&#35823;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20174;&#20869;&#37096;&#34920;&#24449;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#35752;LLM&#24187;&#35273;&#30340;&#28508;&#22312;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#19982;&#24187;&#35273;&#30456;&#20851;&#30340;&#19968;&#20010;&#31361;&#20986;&#27169;&#24335;&#65306;&#27491;&#30830;&#30340;&#29983;&#25104;&#22312;&#19978;&#19979;&#25991;&#26631;&#35760;&#30340;&#38544;&#34255;&#29366;&#24577;&#20013;&#20855;&#26377;&#26356;&#28165;&#26224;&#30340;&#19978;&#19979;&#25991;&#28608;&#27963;&#65292;&#32780;&#19981;&#27491;&#30830;&#30340;&#29983;&#25104;&#21017;&#27809;&#26377;&#12290;&#21033;&#29992;&#36825;&#19968;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#19978;&#19979;&#25991;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#8220;&#38160;&#24230;&#8221;&#65292;&#24182;&#23558;&#20854;&#32435;&#20837;&#35299;&#30721;&#36807;&#31243;&#20013;&#20197;&#21046;&#23450;&#19968;&#31181;&#21463;&#38480;&#35299;&#30721;&#26041;&#27861;&#12290;&#22312;&#21508;&#31181;&#30693;&#35782;&#23547;&#27714;&#21644;&#24187;&#35273;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#33268;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#65292;&#22312;TruthfulQA&#19978;&#23454;&#29616;&#20102;&#39640;&#36798;8.6&#28857;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#39033;&#30740;&#31350;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#24187;&#35273;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01548v1 Announce Type: cross  Abstract: Large language models (LLMs) frequently hallucinate and produce factual errors, yet our understanding of why they make these errors remains limited. In this study, we delve into the underlying mechanisms of LLM hallucinations from the perspective of inner representations, and discover a salient pattern associated with hallucinations: correct generations tend to have sharper context activations in the hidden states of the in-context tokens, compared to the incorrect ones. Leveraging this insight, we propose an entropy-based metric to quantify the ``sharpness'' among the in-context hidden states and incorporate it into the decoding process to formulate a constrained decoding approach. Experiments on various knowledge-seeking and hallucination benchmarks demonstrate our approach's consistent effectiveness, for example, achieving up to an 8.6 point improvement on TruthfulQA. We believe this study can improve our understanding of hallucinat
&lt;/p&gt;</description></item><item><title>&#29983;&#29289;&#20998;&#23376;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#32467;&#21512;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20026;&#20840;&#38754;&#34920;&#31034;&#21644;&#20998;&#26512;&#29983;&#29289;&#20998;&#23376;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.01528</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#29289;&#20998;&#23376;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Leveraging Biomolecule and Natural Language through Multi-Modal Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01528
&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20998;&#23376;&#19982;&#33258;&#28982;&#35821;&#35328;&#30456;&#32467;&#21512;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#20026;&#20840;&#38754;&#34920;&#31034;&#21644;&#20998;&#26512;&#29983;&#29289;&#20998;&#23376;&#24320;&#36767;&#20102;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#29983;&#29289;&#20998;&#23376;&#24314;&#27169;&#19982;&#33258;&#28982;&#35821;&#35328;&#65288;BL&#65289;&#24050;&#32463;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#23398;&#20132;&#21449;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20855;&#26377;&#21069;&#26223;&#30340;&#36328;&#23398;&#31185;&#39046;&#22495;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#28304;&#20013;&#21253;&#21547;&#30340;&#29983;&#29289;&#20998;&#23376;&#30340;&#20016;&#23500;&#22810;&#38754;&#25551;&#36848;&#65292;&#22686;&#24378;&#25105;&#20204;&#23545;&#22522;&#26412;&#29702;&#35299;&#65292;&#24182;&#23454;&#29616;&#29983;&#29289;&#20998;&#23376;&#24615;&#36136;&#39044;&#27979;&#31561;&#35745;&#31639;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#20013;&#34920;&#36798;&#30340;&#24494;&#22937;&#21465;&#36848;&#19982;&#36890;&#36807;&#21508;&#31181;&#20998;&#23376;&#24314;&#27169;&#25216;&#26415;&#25551;&#36848;&#30340;&#29983;&#29289;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#21151;&#33021;&#32454;&#33410;&#34701;&#21512;&#65292;&#25171;&#24320;&#20102;&#20840;&#38754;&#34920;&#24449;&#21644;&#20998;&#26512;&#29983;&#29289;&#20998;&#23376;&#30340;&#26032;&#36884;&#24452;&#12290;&#36890;&#36807;&#23558;&#22260;&#32469;&#29983;&#29289;&#20998;&#23376;&#30340;&#19978;&#19979;&#25991;&#35821;&#35328;&#25968;&#25454;&#32435;&#20837;&#24314;&#27169;&#20013;&#65292;BL&#26088;&#22312;&#25429;&#25417;&#21253;&#21547;&#35821;&#35328;&#20256;&#36798;&#30340;&#31526;&#21495;&#29305;&#24615;&#20197;&#21450;&#25968;&#37327;&#21270;&#32467;&#26500;&#29305;&#24449;&#30340;&#25972;&#20307;&#35270;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01528v1 Announce Type: cross  Abstract: The integration of biomolecular modeling with natural language (BL) has emerged as a promising interdisciplinary area at the intersection of artificial intelligence, chemistry and biology. This approach leverages the rich, multifaceted descriptions of biomolecules contained within textual data sources to enhance our fundamental understanding and enable downstream computational tasks such as biomolecule property prediction. The fusion of the nuanced narratives expressed through natural language with the structural and functional specifics of biomolecules described via various molecular modeling techniques opens new avenues for comprehensively representing and analyzing biomolecules. By incorporating the contextual language data that surrounds biomolecules into their modeling, BL aims to capture a holistic view encompassing both the symbolic qualities conveyed through language as well as quantitative structural characteristics. In this r
&lt;/p&gt;</description></item><item><title>KorMedMCQA&#26159;&#39318;&#20010;&#20174;&#38889;&#22269;&#21307;&#30103;&#19987;&#19994;&#25191;&#19994;&#32771;&#35797;&#20013;&#34893;&#29983;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#38382;&#31572;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#32447;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#22312;HuggingFace&#19978;&#20844;&#24320;&#20102;&#25968;&#25454;&#65292;&#20026;&#38889;&#22269;&#21307;&#30103;&#29615;&#22659;&#20013;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01469</link><description>&lt;p&gt;
KorMedMCQA: &#38889;&#22269;&#21307;&#30103;&#19987;&#19994;&#25191;&#19994;&#32771;&#35797;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#38382;&#31572;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
KorMedMCQA: Multi-Choice Question Answering Benchmark for Korean Healthcare Professional Licensing Examinations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01469
&lt;/p&gt;
&lt;p&gt;
KorMedMCQA&#26159;&#39318;&#20010;&#20174;&#38889;&#22269;&#21307;&#30103;&#19987;&#19994;&#25191;&#19994;&#32771;&#35797;&#20013;&#34893;&#29983;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#38382;&#31572;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#32447;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#22312;HuggingFace&#19978;&#20844;&#24320;&#20102;&#25968;&#25454;&#65292;&#20026;&#38889;&#22269;&#21307;&#30103;&#29615;&#22659;&#20013;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;KorMedMCQA&#65292;&#36825;&#26159;&#39318;&#20010;&#28304;&#33258;&#38889;&#22269;&#21307;&#30103;&#19987;&#19994;&#25191;&#19994;&#32771;&#35797;&#30340;&#38889;&#35821;&#22810;&#39033;&#36873;&#25321;&#39064;&#38382;&#31572;&#65288;MCQA&#65289;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20174;2012&#24180;&#21040;2023&#24180;&#30340;&#32771;&#35797;&#20869;&#23481;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#21307;&#29983;&#12289;&#25252;&#22763;&#21644;&#33647;&#21058;&#24072;&#25191;&#29031;&#32771;&#35797;&#20013;&#30340;&#19968;&#37096;&#20998;&#38382;&#39064;&#65292;&#28085;&#30422;&#22810;&#31181;&#23398;&#31185;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#32447;&#23454;&#39564;&#65292;&#21253;&#25324;&#19987;&#26377;/&#24320;&#28304;&#12289;&#22810;&#35821;&#35328;/&#38889;&#35821;&#38468;&#21152;&#39044;&#35757;&#32451;&#21644;&#20020;&#24202;&#32972;&#26223;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#31361;&#26174;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#28508;&#21147;&#12290;&#25105;&#20204;&#22312;HuggingFace&#19978;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;LM-Harness&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#33050;&#26412;&#65292;&#36992;&#35831;&#22312;&#38889;&#22269;&#21307;&#30103;&#29615;&#22659;&#20013;&#36827;&#34892;&#36827;&#19968;&#27493;&#25506;&#32034;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01469v1 Announce Type: new  Abstract: We introduce KorMedMCQA, the first Korean multiple-choice question answering (MCQA) benchmark derived from Korean healthcare professional licensing examinations, covering from the year 2012 to year 2023. This dataset consists of a selection of questions from the license examinations for doctors, nurses, and pharmacists, featuring a diverse array of subjects. We conduct baseline experiments on various large language models, including proprietary/open-source, multilingual/Korean-additional pretrained, and clinical context pretrained models, highlighting the potential for further enhancements. We make our data publicly available on HuggingFace and provide a evaluation script via LM-Harness, inviting further exploration and advancement in Korean healthcare environments.
&lt;/p&gt;</description></item><item><title>LLaMoCo&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20197;&#20195;&#30721;&#23545;&#20195;&#30721;&#26041;&#24335;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#25351;&#20196;&#35843;&#20248;&#26694;&#26550;&#65292;&#36890;&#36807;&#20840;&#38754;&#25351;&#20196;&#38598;&#21644;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01131</link><description>&lt;p&gt;
LLaMoCo&#65306;&#29992;&#20110;&#20248;&#21270;&#20195;&#30721;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
LLaMoCo: Instruction Tuning of Large Language Models for Optimization Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01131
&lt;/p&gt;
&lt;p&gt;
LLaMoCo&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20197;&#20195;&#30721;&#23545;&#20195;&#30721;&#26041;&#24335;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#25351;&#20196;&#35843;&#20248;&#26694;&#26550;&#65292;&#36890;&#36807;&#20840;&#38754;&#25351;&#20196;&#38598;&#21644;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#20248;&#21270;&#65292;&#26041;&#27861;&#21253;&#25324;&#20174;LLMs&#36845;&#20195;&#22320;&#23547;&#25214;&#19979;&#19968;&#27493;&#35299;&#20915;&#26041;&#26696;&#65292;&#25110;&#30452;&#25509;&#25552;&#31034;LLMs&#20197;&#33719;&#21462;&#20248;&#21270;&#22120;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#65292;&#21253;&#25324;&#25805;&#20316;&#25928;&#29575;&#20302;&#12289;&#23545;&#25552;&#31034;&#35774;&#35745;&#25935;&#24863;&#24230;&#39640;&#20197;&#21450;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;LLaMoCo&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#20197;&#20195;&#30721;&#23545;&#20195;&#30721;&#26041;&#24335;&#35843;&#25972;LLMs&#20197;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#30340;&#25351;&#20196;&#35843;&#20248;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#21547;&#28165;&#26224;&#25551;&#36848;&#30340;&#38382;&#39064;&#25552;&#31034;&#21644;&#26377;&#25928;&#20248;&#21270;&#20195;&#30721;&#30340;&#20840;&#38754;&#25351;&#20196;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#31574;&#30053;&#65292;&#22312;&#25351;&#20196;&#35843;&#20248;&#38454;&#27573;&#20043;&#21069;&#65292;&#35813;&#31574;&#30053;&#25972;&#21512;&#20102;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28909;&#36523;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#24494;&#35843;&#26399;&#38388;&#30340;&#25910;&#25947;&#34892;&#20026;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;LLaMoCo&#31934;&#35843;&#30340;CodeGen&#65288;350M&#65289;&#27169;&#22411;&#36798;&#21040;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01131v1 Announce Type: cross  Abstract: Recent research explores optimization using large language models (LLMs) by either iteratively seeking next-step solutions from LLMs or directly prompting LLMs for an optimizer. However, these approaches exhibit inherent limitations, including low operational efficiency, high sensitivity to prompt design, and a lack of domain-specific knowledge. We introduce LLaMoCo, the first instruction-tuning framework designed to adapt LLMs for solving optimization problems in a code-to-code manner. Specifically, we establish a comprehensive instruction set containing well-described problem prompts and effective optimization codes. We then develop a novel two-phase learning strategy that incorporates a contrastive learning-based warm-up procedure before the instruction-tuning phase to enhance the convergence behavior during model fine-tuning. The experiment results demonstrate that a CodeGen (350M) model fine-tuned by our LLaMoCo achieves superior 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.00867</link><description>&lt;p&gt;
&#26799;&#24230;&#34987;&#32602;&#65306;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#26469;&#26816;&#27979;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#25915;&#20987;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#65292;&#29992;&#25143;&#36755;&#20837;&#26597;&#35810;&#65292;LLM&#29983;&#25104;&#31572;&#26696;&#12290;&#20026;&#20102;&#20943;&#23569;&#20260;&#23475;&#21644;&#28389;&#29992;&#65292;&#20154;&#20204;&#36890;&#36807;&#20351;&#29992;&#20808;&#36827;&#30340;&#35757;&#32451;&#25216;&#26415;&#22914;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26469;&#23558;&#36825;&#20123;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;LLMs&#23545;&#20110;&#35797;&#22270;&#39072;&#35206;&#23884;&#20837;&#30340;&#23433;&#20840;&#38450;&#25252;&#25514;&#26045;&#30340;&#23545;&#25239;&#24615;&#36234;&#29425;&#23581;&#35797;&#30340;&#33030;&#24369;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#23450;&#20041;&#24182;&#35843;&#26597;&#20102;LLMs&#30340;&#25298;&#32477;&#25439;&#22833;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Gradient Cuff&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#36234;&#29425;&#23581;&#35797;&#12290;Gradient Cuff&#21033;&#29992;&#25298;&#32477;&#25439;&#22833;&#22320;&#24418;&#22270;&#20013;&#35266;&#23519;&#21040;&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#21253;&#25324;&#21151;&#33021;&#20540;&#21450;&#20854;&#20809;&#28369;&#24615;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20004;&#27493;&#26816;&#27979;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00867v1 Announce Type: cross  Abstract: Large Language Models (LLMs) are becoming a prominent generative AI tool, where the user enters a query and the LLM generates an answer. To reduce harm and misuse, efforts have been made to align these LLMs to human values using advanced training techniques such as Reinforcement Learning from Human Feedback (RLHF). However, recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails. To address this challenge, this paper defines and investigates the Refusal Loss of LLMs and then proposes a method called Gradient Cuff to detect jailbreak attempts. Gradient Cuff exploits the unique properties observed in the refusal loss landscape, including functional values and its smoothness, to design an effective two-step detection strategy. Experimental results on two aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak attacks (GCG, AutoDAN,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#23454;&#29616;&#20102;&#23545;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#22266;&#23450;&#28857;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#29983;&#25104;&#36895;&#24230;2.4&#20493;&#21040;3.4&#20493;&#12290;</title><link>https://arxiv.org/abs/2403.00835</link><description>&lt;p&gt;
CLLMs: &#19968;&#33268;&#24615;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CLLMs: Consistency Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00835
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#23454;&#29616;&#20102;&#23545;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#22266;&#23450;&#28857;&#30340;&#19968;&#33268;&#24615;&#39044;&#27979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#29983;&#25104;&#36895;&#24230;2.4&#20493;&#21040;3.4&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#35299;&#30721;&#26041;&#27861;&#65292;&#22914;&#38597;&#21487;&#27604;&#35299;&#30721;&#65292;&#26174;&#31034;&#20986;&#26377;&#26395;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;LLM&#25512;&#26029;&#65292;&#22240;&#20026;&#23427;&#25171;&#30772;&#20102;LLM&#35299;&#30721;&#36807;&#31243;&#30340;&#39034;&#24207;&#24615;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#21487;&#24182;&#34892;&#21270;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#19982;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#35299;&#30721;&#30456;&#27604;&#65292;&#38597;&#21487;&#27604;&#35299;&#30721;&#24456;&#23569;&#33021;&#22312;&#21333;&#20010;&#22266;&#23450;&#28857;&#36845;&#20195;&#27493;&#39588;&#20013;&#20934;&#30830;&#39044;&#27979;&#22810;&#20010;&#26631;&#35760;&#65292;&#22240;&#27492;&#22312;&#36895;&#24230;&#19978;&#21462;&#24471;&#30340;&#25552;&#21319;&#30456;&#23545;&#36739;&#23567;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#20174;&#20219;&#20309;&#29366;&#24577;&#24555;&#36895;&#25910;&#25947;&#21040;&#38597;&#21508;&#27604;&#36712;&#36857;&#19978;&#30340;&#22266;&#23450;&#28857;&#12290;&#36890;&#36807;&#31934;&#32454;&#35843;&#25972;&#30446;&#26631;LLM&#65292;&#20197;&#20415;&#22312;&#20219;&#20309;&#36755;&#20837;&#29366;&#24577;&#19979;&#19968;&#33268;&#22320;&#39044;&#27979;&#22266;&#23450;&#28857;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#39046;&#22495;&#29305;&#23450;&#21644;&#24320;&#25918;&#22495;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20986;&#29983;&#25104;&#36895;&#24230;&#25552;&#39640;&#20102;2.4&#20493;&#21040;3.4&#20493;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00835v1 Announce Type: cross  Abstract: Parallel decoding methods such as Jacobi decoding show promise for more efficient LLM inference as it breaks the sequential nature of the LLM decoding process and transforms it into parallelizable computation. However, in practice, it achieves little speedup compared to traditional autoregressive (AR) decoding, primarily because Jacobi decoding seldom accurately predicts more than one token in a single fixed-point iteration step. To address this, we develop a new approach aimed at realizing fast convergence from any state to the fixed point on a Jacobi trajectory. This is accomplished by refining the target LLM to consistently predict the fixed point given any state as input. Extensive experiments demonstrate the effectiveness of our method, showing 2.4$\times$ to 3.4$\times$ improvements in generation speed while preserving generation quality across both domain-specific and open-domain benchmarks.
&lt;/p&gt;</description></item><item><title>DenseSSM&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23494;&#38598;&#36830;&#25509;&#22686;&#24378;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(SSM)&#65292;&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#21508;&#23618;&#20043;&#38388;&#38544;&#34255;&#20449;&#24687;&#30340;&#27969;&#21160;&#65292;&#22312;&#20445;&#25345;&#35757;&#32451;&#24182;&#34892;&#24615;&#21644;&#25512;&#29702;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.00818</link><description>&lt;p&gt;
DenseMamba: &#20855;&#26377;&#23494;&#38598;&#38544;&#34255;&#36830;&#25509;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#39640;&#25928;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00818
&lt;/p&gt;
&lt;p&gt;
DenseSSM&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23494;&#38598;&#36830;&#25509;&#22686;&#24378;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(SSM)&#65292;&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#21508;&#23618;&#20043;&#38388;&#38544;&#34255;&#20449;&#24687;&#30340;&#27969;&#21160;&#65292;&#22312;&#20445;&#25345;&#35757;&#32451;&#24182;&#34892;&#24615;&#21644;&#25512;&#29702;&#25928;&#29575;&#30340;&#21516;&#26102;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#38754;&#20020;&#30528;&#30001;&#26222;&#36941;&#20351;&#29992;&#30340;Transformer&#26550;&#26500;&#36807;&#39640;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#32780;&#24102;&#26469;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#32780;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;(SSM)&#26159;&#19968;&#31181;&#26032;&#22411;&#22522;&#30784;&#32593;&#32476;&#26550;&#26500;&#65292;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#20294;&#20854;&#24615;&#33021;&#23578;&#26410;&#23436;&#20840;&#33021;&#19982;Transformer&#30456;&#23218;&#32654;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;DenseSSM&#65292;&#19968;&#31181;&#22686;&#24378;SSMs&#20013;&#21508;&#23618;&#20043;&#38388;&#38544;&#34255;&#20449;&#24687;&#27969;&#21160;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#26377;&#36873;&#25321;&#22320;&#23558;&#27973;&#23618;&#38544;&#34255;&#29366;&#24577;&#38598;&#25104;&#21040;&#26356;&#28145;&#23618;&#65292;DenseSSM&#20445;&#30041;&#20102;&#23545;&#26368;&#32456;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#30340;&#32454;&#31890;&#24230;&#20449;&#24687;&#12290;&#23494;&#38598;&#36830;&#25509;&#22686;&#24378;&#30340;DenseSSM&#20173;&#20445;&#25345;&#20102;&#35757;&#32451;&#30340;&#24182;&#34892;&#24615;&#21644;&#25512;&#29702;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#24191;&#27867;&#36866;&#29992;&#20110;RetNet&#21644;Mamba&#31561;&#21508;&#31181;SSM&#31867;&#22411;&#12290;&#22312;&#30456;&#20284;&#30340;&#27169;&#22411;&#22823;&#23567;&#19979;&#65292;DenseSSM&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20363;&#22914;DenseRetNet&#27604;&#21407;&#22987;RetNet&#25552;&#39640;&#20102;&#39640;&#36798;5%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00818v1 Announce Type: new  Abstract: Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% ac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Metamorpheus&#65292;&#19968;&#31181;&#24773;&#24863;&#25509;&#21475;&#65292;&#36890;&#36807;&#21019;&#36896;&#24615;&#30340;&#35270;&#35273;&#21465;&#20107;&#26469;&#21442;&#19982;&#29992;&#25143;&#22312;&#26790;&#22659;&#20013;&#30340;&#24773;&#24863;&#32463;&#21382;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#35270;&#35273;&#38544;&#21947;&#21644;&#25991;&#26412;&#25551;&#32472;&#65292;&#20419;&#20351;&#33258;&#25105;&#21453;&#24605;&#12290;</title><link>https://arxiv.org/abs/2403.00632</link><description>&lt;p&gt;
&#24418;&#24577;&#21464;&#24322;&#65306;&#36890;&#36807;&#38544;&#21947;&#35270;&#35273;&#21465;&#20107;&#36827;&#34892;&#20114;&#21160;&#12289;&#24773;&#24863;&#21644;&#21019;&#36896;&#24615;&#26790;&#22659;&#21465;&#36848;
&lt;/p&gt;
&lt;p&gt;
Metamorpheus: Interactive, Affective, and Creative Dream Narration Through Metaphorical Visual Storytelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Metamorpheus&#65292;&#19968;&#31181;&#24773;&#24863;&#25509;&#21475;&#65292;&#36890;&#36807;&#21019;&#36896;&#24615;&#30340;&#35270;&#35273;&#21465;&#20107;&#26469;&#21442;&#19982;&#29992;&#25143;&#22312;&#26790;&#22659;&#20013;&#30340;&#24773;&#24863;&#32463;&#21382;&#65292;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#35270;&#35273;&#38544;&#21947;&#21644;&#25991;&#26412;&#25551;&#32472;&#65292;&#20419;&#20351;&#33258;&#25105;&#21453;&#24605;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30340;&#24773;&#24863;&#22522;&#26412;&#19978;&#26159;&#30001;&#29983;&#27963;&#32463;&#39564;&#22609;&#36896;&#30340;&#65292;&#25105;&#20204;&#20174;&#20013;&#26500;&#24314;&#20010;&#24615;&#21270;&#24847;&#20041;&#12290;&#21442;&#19982;&#36825;&#31181;&#24847;&#20041;&#22609;&#36896;&#36807;&#31243;&#24050;&#32463;&#34987;&#20316;&#20026;&#21508;&#31181;&#24515;&#29702;&#27835;&#30103;&#20013;&#30340;&#19968;&#31181;&#24178;&#39044;&#26469;&#20419;&#36827;&#20581;&#24247;&#12290;&#28982;&#32780;&#65292;&#25903;&#25345;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#22238;&#24518;&#21644;&#21465;&#36848;&#29983;&#27963;&#32463;&#39564;&#20173;&#28982;&#22312;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#40092;&#26377;&#25506;&#35752;&#12290;&#22914;&#20309;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#31561;&#25216;&#26415;&#26469;&#20419;&#36827;&#24847;&#20041;&#22609;&#36896;&#36807;&#31243;&#65292;&#26368;&#32456;&#25903;&#25345;&#24773;&#24863;&#27491;&#24565;&#20173;&#28982;&#26159;&#26410;&#30693;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;Metamorpheus&#65292;&#19968;&#31181;&#24773;&#24863;&#25509;&#21475;&#65292;&#36890;&#36807;&#21019;&#36896;&#24615;&#30340;&#35270;&#35273;&#21465;&#20107;&#26469;&#21442;&#19982;&#29992;&#25143;&#22312;&#26790;&#22659;&#20013;&#30340;&#24773;&#24863;&#32463;&#21382;&#12290;Metamorpheus&#26681;&#25454;&#26790;&#22659;&#30340;&#24773;&#24863;&#24359;&#32447;&#25490;&#21015;&#25925;&#20107;&#24773;&#33410;&#65292;&#24182;&#36890;&#36807;&#21019;&#36896;&#38544;&#21947;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#32472;&#26469;&#20419;&#20351;&#33258;&#25105;&#21453;&#24605;&#12290;&#35813;&#31995;&#32479;&#25552;&#20379;&#38544;&#21947;&#24314;&#35758;&#65292;&#24182;&#21033;&#29992;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#35270;&#35273;&#38544;&#21947;&#21644;&#25991;&#26412;&#25551;&#32472;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00632v1 Announce Type: cross  Abstract: Human emotions are essentially molded by lived experiences, from which we construct personalised meaning. The engagement in such meaning-making process has been practiced as an intervention in various psychotherapies to promote wellness. Nevertheless, to support recollecting and recounting lived experiences in everyday life remains under explored in HCI. It also remains unknown how technologies such as generative AI models can facilitate the meaning making process, and ultimately support affective mindfulness. In this paper we present Metamorpheus, an affective interface that engages users in a creative visual storytelling of emotional experiences during dreams. Metamorpheus arranges the storyline based on a dream's emotional arc, and provokes self-reflection through the creation of metaphorical images and text depictions. The system provides metaphor suggestions, and generates visual metaphors and text depictions using generative AI m
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#65292;&#20174;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#30340;&#35282;&#24230;&#36830;&#25509;&#36755;&#20837;&#25991;&#27573;&#21644;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.19350</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#30340;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#20013;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19350
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#65292;&#20174;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#30340;&#35282;&#24230;&#36830;&#25509;&#36755;&#20837;&#25991;&#27573;&#21644;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21033;&#29992;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#27169;&#25311;&#20154;&#31867;&#25512;&#29702;&#21644;&#25512;&#26029;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#36339;QA&#26041;&#38754;&#39640;&#25928;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#26102;&#65292;PLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#20154;&#31867;&#20043;&#38388;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#24515;&#29702;&#23398;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38405;&#35835;&#36807;&#31243;&#20013;&#65292;&#36755;&#20837;&#25991;&#27573;&#20013;&#30340;&#26174;&#24335;&#20449;&#24687;&#19982;&#20154;&#31867;&#20808;&#39564;&#30693;&#35782;&#20043;&#38388;&#23384;&#22312;&#37325;&#35201;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#26410;&#33021;&#20805;&#20998;&#20851;&#27880;&#20174;&#20154;&#31867;&#35748;&#30693;&#30740;&#31350;&#30340;&#35282;&#24230;&#38142;&#25509;&#36755;&#20837;&#25991;&#27573;&#21644;&#22522;&#20110;PLMs&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20419;&#36827;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#65288;PEI&#65289;&#26694;&#26550;&#65292;&#20351;&#29992;&#25552;&#31034;&#36830;&#25509;&#26174;&#24335;&#21644;&#38544;&#24335;&#30693;&#35782;&#65292;&#19982;&#20154;&#31867;&#38405;&#35835;&#36807;&#31243;&#23545;&#40784;&#65292;&#29992;&#20110;&#22810;&#36339;QA&#12290;&#25105;&#20204;&#23558;&#36755;&#20837;&#25991;&#27573;&#35270;&#20026;&#26174;&#24335;&#30693;&#35782;&#65292;&#21033;&#29992;&#23427;&#20204;&#36890;&#36807;&#32479;&#19968;&#25552;&#31034;&#25512;&#23548;&#38544;&#24335;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19350v1 Announce Type: new  Abstract: Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to simulate human reasoning and inference processes, achieving proficient performance in multi-hop QA. However, a gap persists between PLMs' reasoning abilities and those of humans when tackling complex problems. Psychological studies suggest a vital connection between explicit information in passages and human prior knowledge during reading. Nevertheless, current research has given insufficient attention to linking input passages and PLMs' pre-training-based knowledge from the perspective of human cognition studies. In this study, we introduce a \textbf{P}rompting \textbf{E}xplicit and \textbf{I}mplicit knowledge (PEI) framework, which uses prompts to connect explicit and implicit knowledge, aligning with human reading process for multi-hop QA. We consider the input passages as explicit knowledge, employing them to elicit implicit knowledge through unified prompt reason
&lt;/p&gt;</description></item><item><title>WanJuan-CC&#26159;&#19968;&#20010;&#23433;&#20840;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;Common Crawl&#25968;&#25454;&#24182;&#32463;&#36807;&#22810;&#39033;&#31579;&#36873;&#21644;&#36807;&#28388;&#27493;&#39588;&#24471;&#21040;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2402.19282</link><description>&lt;p&gt;
WanJuan-CC&#65306;&#19968;&#20010;&#23433;&#20840;&#19988;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
WanJuan-CC: A Safe and High-Quality Open-sourced English Webtext Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19282
&lt;/p&gt;
&lt;p&gt;
WanJuan-CC&#26159;&#19968;&#20010;&#23433;&#20840;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22788;&#29702;&#22823;&#35268;&#27169;&#30340;Common Crawl&#25968;&#25454;&#24182;&#32463;&#36807;&#22810;&#39033;&#31579;&#36873;&#21644;&#36807;&#28388;&#27493;&#39588;&#24471;&#21040;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; WanJuan-CC&#65292;&#36825;&#26159;&#19968;&#20010;&#23433;&#20840;&#19988;&#39640;&#36136;&#37327;&#30340;&#24320;&#28304;&#33521;&#25991;&#32593;&#32476;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#26469;&#28304;&#20110;Common Crawl&#25968;&#25454;&#12290;&#30740;&#31350;&#35299;&#20915;&#20102;&#20026;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#25968;&#25454;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#27969;&#31243;&#26469;&#22788;&#29702;Common Crawl&#25968;&#25454;&#65292;&#21253;&#25324;&#25552;&#21462;&#12289;&#21551;&#21457;&#24335;&#35268;&#21017;&#36807;&#28388;&#12289;&#27169;&#31946;&#21435;&#37325;&#12289;&#20869;&#23481;&#23433;&#20840;&#36807;&#28388;&#21644;&#25968;&#25454;&#36136;&#37327;&#36807;&#28388;&#12290;&#20174;&#22823;&#32422;680&#20159;&#20010;&#21407;&#22987;&#33521;&#25991;&#25991;&#26723;&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;22&#19975;&#20159;&#26631;&#35760;&#30340;&#23433;&#20840;&#25968;&#25454;&#65292;&#24182;&#20174;&#20013;&#36873;&#20986;&#20102;10&#19975;&#20159;&#26631;&#35760;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#20316;&#20026;WanJuan-CC&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#24050;&#32463;&#24320;&#28304;&#20102;&#36825;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;3000&#20159;&#26631;&#35760;&#12290;&#35813;&#35770;&#25991;&#36824;&#25552;&#20379;&#20102;&#19982;&#25968;&#25454;&#36136;&#37327;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#26681;&#25454;&#33258;&#24049;&#30340;&#38656;&#27714;&#36873;&#25321;&#36866;&#24403;&#30340;&#25968;&#25454;&#12290;&#20026;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;WanJuan-CC&#35757;&#32451;&#20102;10&#20159;&#21442;&#25968;&#21644;30&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19282v1 Announce Type: new  Abstract: This paper presents WanJuan-CC, a safe and high-quality open-sourced English webtext dataset derived from Common Crawl data. The study addresses the challenges of constructing large-scale pre-training datasets for language models, which require vast amounts of high-quality data. A comprehensive process was designed to handle Common Crawl data, including extraction, heuristic rule filtering, fuzzy deduplication, content safety filtering, and data quality filtering. From approximately 68 billion original English documents, we obtained 2.22T Tokens of safe data and selected 1.0T Tokens of high-quality data as part of WanJuan-CC. We have open-sourced 300B Tokens from this dataset. The paper also provides statistical information related to data quality, enabling users to select appropriate data according to their needs. To evaluate the quality and utility of the dataset, we trained 1B-parameter and 3B-parameter models using WanJuan-CC and ano
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#36890;&#36807;&#25193;&#23637;Transformer&#27169;&#22411;&#30340;&#24207;&#21015;&#38271;&#24230;&#26469;&#26356;&#22909;&#29702;&#35299;&#27861;&#24459;&#35821;&#26009;&#24211;&#20013;&#30340;&#38271;&#25991;&#26723;&#65292;&#24182;&#22312;&#32599;&#39532;&#23612;&#20122;&#30340;4&#20010;LJP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.19170</link><description>&lt;p&gt;
&#36890;&#36807;&#38271;&#25991;&#26412;&#32534;&#30721;&#22120;&#25552;&#21319;&#32599;&#39532;&#23612;&#20122;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Legal Judgement Prediction in Romanian with Long Text Encoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#36890;&#36807;&#25193;&#23637;Transformer&#27169;&#22411;&#30340;&#24207;&#21015;&#38271;&#24230;&#26469;&#26356;&#22909;&#29702;&#35299;&#27861;&#24459;&#35821;&#26009;&#24211;&#20013;&#30340;&#38271;&#25991;&#26723;&#65292;&#24182;&#22312;&#32599;&#39532;&#23612;&#20122;&#30340;4&#20010;LJP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#26032;&#25104;&#26524;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#27861;&#24459;NLP&#39046;&#22495;&#20063;&#38543;&#20043;&#21457;&#23637;&#36805;&#29467;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;&#27169;&#22411;&#24182;&#19981;&#30452;&#25509;&#36866;&#29992;&#20110;&#27861;&#24459;&#39046;&#22495;&#12290;&#30001;&#20110;&#20854;&#19987;&#19994;&#35789;&#27719;&#12289;&#38271;&#25991;&#26723;&#31561;&#29305;&#28857;&#65292;&#27861;&#24459;NLP&#36890;&#24120;&#38656;&#35201;&#29305;&#23450;&#27169;&#22411;&#21644;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19987;&#19994;&#21644;&#36890;&#29992;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#27861;&#24459;&#26696;&#20363;&#30340;&#26368;&#32456;&#35009;&#20915;&#30340;&#26041;&#27861;&#65292;&#21363;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#22914;&#20309;&#25193;&#23637;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#24207;&#21015;&#38271;&#24230;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#27861;&#24459;&#35821;&#26009;&#24211;&#20013;&#30340;&#38271;&#25991;&#26723;&#12290;&#22312;&#26469;&#33258;&#20004;&#20010;&#26469;&#28304;&#12289;&#35268;&#27169;&#21644;&#25991;&#26723;&#38271;&#24230;&#26174;&#33879;&#19981;&#21516;&#26102;&#30340;4&#20010;&#32599;&#39532;&#23612;&#20122;LJP&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#19987;&#38376;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19170v1 Announce Type: cross  Abstract: In recent years,the entire field of Natural Language Processing (NLP) has enjoyed amazing novel results achieving almost human-like performance on a variety of tasks. Legal NLP domain has also been part of this process, as it has seen an impressive growth. However, general-purpose models are not readily applicable for legal domain. Due to the nature of the domain (e.g. specialized vocabulary, long documents) specific models and methods are often needed for Legal NLP. In this work we investigate both specialized and general models for predicting the final ruling of a legal case, task known as Legal Judgment Prediction (LJP). We particularly focus on methods to extend to sequence length of Transformer-based models to better understand the long documents present in legal corpora. Extensive experiments on 4 LJP datasets in Romanian, originating from 2 sources with significantly different sizes and document lengths, show that specialized mo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33889;&#33796;&#29273;&#35821;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#25918;&#35299;&#30721;&#22120;&#27169;&#22411;Gerv\'asio PT*&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#20419;&#36827;&#33889;&#33796;&#29273;&#35821;&#35328;&#25216;&#26415;&#30740;&#31350;&#21644;&#21019;&#26032;&#12290;</title><link>https://arxiv.org/abs/2402.18766</link><description>&lt;p&gt;
&#25512;&#21160;&#33889;&#33796;&#29273;&#35821;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#19982;&#24320;&#25918;&#35299;&#30721;&#22120;Gerv\'asio PT*
&lt;/p&gt;
&lt;p&gt;
Advancing Generative AI for Portuguese with Open Decoder Gerv\'asio PT*
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18766
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33889;&#33796;&#29273;&#35821;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#25918;&#35299;&#30721;&#22120;&#27169;&#22411;Gerv\'asio PT*&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#20419;&#36827;&#33889;&#33796;&#29273;&#35821;&#35328;&#25216;&#26415;&#30740;&#31350;&#21644;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25512;&#36827;&#33889;&#33796;&#29273;&#35821;&#30340;&#31070;&#32463;&#35299;&#30721;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#22522;&#20110;Transformer&#30340;&#12289;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;&#24320;&#25918;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#20174;&#36825;&#26041;&#38754;&#21019;&#36896;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#20026;&#20102;&#24320;&#21457;&#36825;&#20010;&#35299;&#30721;&#22120;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;LLaMA~27B&#27169;&#22411;&#20316;&#20026;&#36215;&#28857;&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#35757;&#32451;&#23545;&#21253;&#25324;&#20026;&#27492;&#30446;&#30340;&#20934;&#22791;&#30340;&#26032;&#33889;&#33796;&#29273;&#35821;&#25351;&#20196;&#25968;&#25454;&#38598;&#22312;&#20869;&#30340;&#35821;&#35328;&#36164;&#28304;&#36827;&#34892;&#25913;&#36827;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20063;&#22312;&#26412;&#25991;&#20013;&#25552;&#20379;&#12290;&#25152;&#26377;&#29256;&#26412;&#30340;Gerv\'asio&#37117;&#26159;&#24320;&#28304;&#30340;&#65292;&#21487;&#20197;&#20813;&#36153;&#20351;&#29992;&#65292;&#24182;&#21487;&#20197;&#22312;&#28040;&#36153;&#32423;&#30828;&#20214;&#19978;&#36816;&#34892;&#65292;&#26088;&#22312;&#20419;&#36827;&#33889;&#33796;&#29273;&#35821;&#35328;&#25216;&#26415;&#30740;&#31350;&#21644;&#21019;&#26032;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18766v1 Announce Type: new  Abstract: To advance the neural decoding of Portuguese, in this paper we present a fully open Transformer-based, instruction-tuned decoder model that sets a new state of the art in this respect. To develop this decoder, which we named Gerv\'asio PT*, a strong LLaMA~2 7B model was used as a starting point, and its further improvement through additional training was done over language resources that include new instruction data sets of Portuguese prepared for this purpose, which are also contributed in this paper. All versions of Gerv\'asio are open source and distributed for free under an open license, including for either research or commercial usage, and can be run on consumer-grade hardware, thus seeking to contribute to the advancement of research and innovation in language technology for Portuguese.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;Rec4Agentverse&#65292;&#24378;&#35843;&#20195;&#29702;&#39033;&#21644;&#20195;&#29702;&#25512;&#33616;&#22120;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#20419;&#36827;&#20010;&#24615;&#21270;&#20449;&#24687;&#26381;&#21153;&#65292;&#25552;&#21319;&#20449;&#24687;&#20132;&#25442;&#65292;&#24182;&#23637;&#26395;&#20102;&#20854;&#28436;&#36827;&#20026;&#25903;&#25345;&#20114;&#21160;&#21644;&#20449;&#24687;&#20132;&#25442;&#30340;&#19977;&#20010;&#38454;&#27573;</title><link>https://arxiv.org/abs/2402.18240</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#29702;&#24179;&#21488;&#19978;&#30340;&#21069;&#26223;&#20010;&#24615;&#21270;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Prospect Personalized Recommendation on Large Language Model-based Agent Platform
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18240
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;Rec4Agentverse&#65292;&#24378;&#35843;&#20195;&#29702;&#39033;&#21644;&#20195;&#29702;&#25512;&#33616;&#22120;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#20419;&#36827;&#20010;&#24615;&#21270;&#20449;&#24687;&#26381;&#21153;&#65292;&#25552;&#21319;&#20449;&#24687;&#20132;&#25442;&#65292;&#24182;&#23637;&#26395;&#20102;&#20854;&#28436;&#36827;&#20026;&#25903;&#25345;&#20114;&#21160;&#21644;&#20449;&#24687;&#20132;&#25442;&#30340;&#19977;&#20010;&#38454;&#27573;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#22411;&#20195;&#29702;&#23548;&#21521;&#20449;&#24687;&#31995;&#32479;&#65292;&#20197;GPT&#20026;&#20363;&#65292;&#20419;&#20351;&#25105;&#20204;&#23457;&#35270;&#20449;&#24687;&#31995;&#32479;&#22522;&#30784;&#35774;&#26045;&#65292;&#20197;&#25903;&#25345;&#20195;&#29702;&#32423;&#20449;&#24687;&#22788;&#29702;&#24182;&#36866;&#24212;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20195;&#29702;&#30340;&#29305;&#24449;&#65292;&#22914;&#20114;&#21160;&#24615;&#12290;&#26412;&#30740;&#31350;&#23637;&#26395;&#20102;&#22522;&#20110;LLM&#20195;&#29702;&#24179;&#21488;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#21069;&#26223;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;Rec4Agentverse&#30340;&#26032;&#22411;&#25512;&#33616;&#33539;&#24335;&#65292;&#21253;&#25324;&#20195;&#29702;&#39033;&#21644;&#20195;&#29702;&#25512;&#33616;&#22120;&#12290;Rec4Agentverse&#24378;&#35843;&#20195;&#29702;&#39033;&#21644;&#20195;&#29702;&#25512;&#33616;&#22120;&#20043;&#38388;&#30340;&#21512;&#20316;&#65292;&#20174;&#32780;&#20419;&#36827;&#20010;&#24615;&#21270;&#20449;&#24687;&#26381;&#21153;&#65292;&#24182;&#22686;&#24378;&#20449;&#24687;&#20132;&#25442;&#65292;&#36229;&#36234;&#20256;&#32479;&#30340;&#29992;&#25143;-&#25512;&#33616;&#22120;&#21453;&#39304;&#24490;&#29615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#26395;&#20102;Rec4Agentverse&#30340;&#28436;&#36827;&#65292;&#24182;&#23558;&#20854;&#27010;&#24565;&#21270;&#20026;&#22522;&#20110;&#20195;&#29702;&#39033;&#12289;&#20195;&#29702;&#25512;&#33616;&#22120;&#21644;&#29992;&#25143;&#20043;&#38388;&#20114;&#21160;&#21644;&#20449;&#24687;&#20132;&#25442;&#22686;&#24378;&#30340;&#19977;&#20010;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18240v1 Announce Type: cross  Abstract: The new kind of Agent-oriented information system, exemplified by GPTs, urges us to inspect the information system infrastructure to support Agent-level information processing and to adapt to the characteristics of Large Language Model (LLM)-based Agents, such as interactivity. In this work, we envisage the prospect of the recommender system on LLM-based Agent platforms and introduce a novel recommendation paradigm called Rec4Agentverse, comprised of Agent Items and Agent Recommender. Rec4Agentverse emphasizes the collaboration between Agent Items and Agent Recommender, thereby promoting personalized information services and enhancing the exchange of information beyond the traditional user-recommender feedback loop. Additionally, we prospect the evolution of Rec4Agentverse and conceptualize it into three stages based on the enhancement of the interaction and information exchange among Agent Items, Agent Recommender, and the user. A pre
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PALO&#30340;&#22823;&#22411;&#22810;&#35821;&#31181;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;10&#31181;&#20027;&#35201;&#35821;&#35328;&#30340;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;&#32422;50&#20159;&#20154;&#21475;&#12290;&#20854;&#36890;&#36807;&#21322;&#33258;&#21160;&#21270;&#32763;&#35793;&#26041;&#27861;&#65292;&#23558;&#22810;&#35821;&#35328;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20174;&#33521;&#35821;&#32763;&#35793;&#20026;&#30446;&#26631;&#35821;&#35328;&#65292;&#20197;&#25552;&#21319;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14818</link><description>&lt;p&gt;
PALO&#65306;&#19968;&#20010;&#38024;&#23545;50&#20159;&#20154;&#30340;&#22810;&#35821;&#35328;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PALO: A Polyglot Large Multimodal Model for 5B People
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14818
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PALO&#30340;&#22823;&#22411;&#22810;&#35821;&#31181;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;10&#31181;&#20027;&#35201;&#35821;&#35328;&#30340;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;&#32422;50&#20159;&#20154;&#21475;&#12290;&#20854;&#36890;&#36807;&#21322;&#33258;&#21160;&#21270;&#32763;&#35793;&#26041;&#27861;&#65292;&#23558;&#22810;&#35821;&#35328;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20174;&#33521;&#35821;&#32763;&#35793;&#20026;&#30446;&#26631;&#35821;&#35328;&#65292;&#20197;&#25552;&#21319;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25512;&#21160;&#26356;&#20855;&#21253;&#23481;&#24615;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(VLMs)&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;\Palo &#30340;&#22823;&#22411;&#22810;&#35821;&#31181;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;Palo &#22312;&#21253;&#25324;&#33521;&#35821;&#12289;&#20013;&#25991;&#12289;&#21360;&#22320;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#12289;&#27861;&#35821;&#12289;&#38463;&#25289;&#20271;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#12289;&#20420;&#35821;&#12289;&#20044;&#23572;&#37117;&#35821;&#21644;&#26085;&#35821;&#22312;&#20869;&#30340;10&#31181;&#20027;&#35201;&#35821;&#35328;&#20013;&#25552;&#20379;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;&#32422;50&#20159;&#20154;&#21475;&#65288;&#20840;&#29699;&#20154;&#21475;&#30340;65%&#65289;&#12290;&#25105;&#20204;&#37319;&#29992;&#21322;&#33258;&#21160;&#21270;&#32763;&#35793;&#26041;&#27861;&#65292;&#21033;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22810;&#27169;&#24577;&#25351;&#23548;&#25968;&#25454;&#38598;&#20174;&#33521;&#35821;&#32763;&#35793;&#21040;&#30446;&#26631;&#35821;&#35328;&#65292;&#20174;&#32780;&#30830;&#20445;&#20102;&#36739;&#39640;&#30340;&#35821;&#35328;&#20445;&#30495;&#24230;&#65292;&#21516;&#26102;&#30001;&#20110;&#20943;&#23569;&#20102;&#25163;&#21160;&#24037;&#20316;&#65292;&#20351;&#21487;&#25193;&#23637;&#24615;&#26356;&#24378;&#12290;&#24341;&#20837;&#22810;&#26679;&#21270;&#30340;&#25351;&#23548;&#38598;&#26377;&#21161;&#20110;&#25552;&#21319;&#36328;&#22810;&#31181;&#35821;&#35328;&#30340;&#24635;&#20307;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#37027;&#20123;&#27424;&#20195;&#34920;&#30340;&#35821;&#35328;&#22914;&#21360;&#22320;&#35821;&#12289;&#38463;&#25289;&#20271;&#35821;&#12289;&#23391;&#21152;&#25289;&#35821;&#21644;&#20044;&#23572;&#37117;&#35821;&#12290;&#26368;&#32456;&#30340;&#27169;&#22411;&#22312;&#19977;&#20010;&#35268;&#27169;&#65288;17B&#12289;70B&#21644;130B&#21442;&#25968;&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23637;&#31034;&#20854;&#27867;&#29992;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14818v1 Announce Type: new  Abstract: In pursuit of more inclusive Vision-Language Models (VLMs), this study introduces a Large Multilingual Multimodal Model called \textsc{Palo}. \textsc{Palo} offers visual reasoning capabilities in 10 major languages, including English, Chinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese, that span a total of $\sim$5B people (65\% of the world population). Our approach involves a semi-automated translation approach to adapt the multimodal instruction dataset from English to the target languages using a fine-tuned Large Language Model, thereby ensuring high linguistic fidelity while allowing scalability due to minimal manual effort. The incorporation of diverse instruction sets helps us boost overall performance across multiple languages especially those that are underrepresented like Hindi, Arabic, Bengali, and Urdu. The resulting models are trained across three scales (1.7B, 7B and 13B parameters) to show the gen
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;VLSP 2023&#20013;ComOM&#20219;&#21153;&#30340;&#19968;&#20010;&#25968;&#25454;&#25361;&#25112;&#65292;&#26088;&#22312;&#25512;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36890;&#36807;&#24320;&#21457;&#20174;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#20013;&#25552;&#21462;&#27604;&#36739;&#24847;&#35265;&#30340;&#25216;&#26415;&#65292;&#21442;&#19982;&#32773;&#38656;&#25552;&#20986;&#33021;&#22815;&#25552;&#21462;&#27604;&#36739;"&#20116;&#20803;&#32452;"&#30340;&#27169;&#22411;&#24182;&#26681;&#25454;F1&#20998;&#25968;&#36827;&#34892;&#35780;&#20272;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2402.13613</link><description>&lt;p&gt;
VLSP 2023&#32508;&#36848;--ComOM&#20219;&#21153;&#65306;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#30340;&#27604;&#36739;&#24847;&#35265;&#25366;&#25496;&#25968;&#25454;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Overview of the VLSP 2023 -- ComOM Shared Task: A Data Challenge for Comparative Opinion Mining from Vietnamese Product Reviews
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13613
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;VLSP 2023&#20013;ComOM&#20219;&#21153;&#30340;&#19968;&#20010;&#25968;&#25454;&#25361;&#25112;&#65292;&#26088;&#22312;&#25512;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36890;&#36807;&#24320;&#21457;&#20174;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#20013;&#25552;&#21462;&#27604;&#36739;&#24847;&#35265;&#30340;&#25216;&#26415;&#65292;&#21442;&#19982;&#32773;&#38656;&#25552;&#20986;&#33021;&#22815;&#25552;&#21462;&#27604;&#36739;"&#20116;&#20803;&#32452;"&#30340;&#27169;&#22411;&#24182;&#26681;&#25454;F1&#20998;&#25968;&#36827;&#34892;&#35780;&#20272;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#36234;&#21335;&#35821;&#20135;&#21697;&#35780;&#35770;&#27604;&#36739;&#24847;&#35265;&#25366;&#25496;&#20849;&#20139;&#20219;&#21153;&#65288;ComOM&#65289;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#35813;&#20219;&#21153;&#20316;&#20026;&#31532;&#21313;&#23626;&#36234;&#21335;&#35821;&#35328;&#21644;&#35821;&#38899;&#22788;&#29702;&#22269;&#38469;&#30740;&#35752;&#20250;&#65288;VLSP 2023&#65289;&#30340;&#19968;&#37096;&#20998;&#20030;&#34892;&#12290;&#27492;&#20849;&#20139;&#20219;&#21153;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#20174;&#36234;&#21335;&#20135;&#21697;&#35780;&#35770;&#20013;&#25552;&#21462;&#27604;&#36739;&#24847;&#35265;&#30340;&#25216;&#26415;&#26469;&#25512;&#21160;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#21442;&#19982;&#32773;&#34987;&#25361;&#25112;&#25552;&#20986;&#33021;&#22815;&#20174;&#27604;&#36739;&#21477;&#20013;&#29087;&#32451;&#25552;&#21462;&#27604;&#36739;&#8220;&#20116;&#20803;&#32452;&#8221;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#20027;&#39064;&#12289;&#23458;&#20307;&#12289;&#26041;&#38754;&#12289;&#35859;&#35789;&#21644;&#27604;&#36739;&#31867;&#22411;&#26631;&#31614;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;120&#20010;&#25991;&#26723;&#30340;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;7427&#20010;&#38750;&#27604;&#36739;&#21477;&#21644;1798&#20010;&#21477;&#23376;&#20013;&#30340;2468&#20010;&#27604;&#36739;&#12290;&#21442;&#19982;&#30340;&#27169;&#22411;&#23558;&#26681;&#25454;&#20934;&#30830;&#21305;&#37197;&#23439;&#24179;&#22343;&#30340;&#20116;&#20803;&#32452;F1&#20998;&#25968;&#36827;&#34892;&#35780;&#20272;&#21644;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13613v1 Announce Type: new  Abstract: This paper presents a comprehensive overview of the Comparative Opinion Mining from Vietnamese Product Reviews shared task (ComOM), held as part of the 10$^{th}$ International Workshop on Vietnamese Language and Speech Processing (VLSP 2023). The primary objective of this shared task is to advance the field of natural language processing by developing techniques that proficiently extract comparative opinions from Vietnamese product reviews. Participants are challenged to propose models that adeptly extract a comparative "quintuple" from a comparative sentence, encompassing Subject, Object, Aspect, Predicate, and Comparison Type Label. We construct a human-annotated dataset comprising $120$ documents, encompassing $7427$ non-comparative sentences and $2468$ comparisons within $1798$ sentences. Participating models undergo evaluation and ranking based on the Exact match macro-averaged quintuple F1 score.
&lt;/p&gt;</description></item><item><title>Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12177</link><description>&lt;p&gt;
Mafin: &#29992;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#26469;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Mafin: Enhancing Black-Box Embeddings with Model Augmented Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12177
&lt;/p&gt;
&lt;p&gt;
Mafin&#36890;&#36807;&#24341;&#20837;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#21482;&#26377;&#40657;&#30418;&#23884;&#20837;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#24050;&#32463;&#25104;&#20026;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24187;&#35273;&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;RAG&#20013;&#30340;&#26816;&#32034;&#38454;&#27573;&#36890;&#24120;&#28041;&#21450;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#23558;&#26597;&#35810;&#21644;&#27573;&#33853;&#36716;&#25442;&#20026;&#21521;&#37327;&#20197;&#25429;&#33719;&#23427;&#20204;&#30340;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#26102;&#65292;&#26631;&#20934;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#27169;&#22411;&#21487;&#33021;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#65292;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#20165;&#33021;&#20174;&#40657;&#30418;&#27169;&#22411;&#33719;&#21462;&#23884;&#20837;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;&#65288;Mafin&#65289;--&#19968;&#31181;&#36890;&#36807;&#29992;&#21487;&#35757;&#32451;&#30340;&#23884;&#20837;&#27169;&#22411;&#22686;&#24378;&#40657;&#30418;&#23884;&#20837;&#27169;&#22411;&#26469;&#36827;&#34892;&#24494;&#35843;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Mafin&#20165;&#38656;&#35201;&#35757;&#32451;&#19968;&#20010;&#23567;&#30340;&#22686;&#24378;&#27169;&#22411;&#23601;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#40657;&#30418;&#23884;&#20837;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#26631;&#31614;&#21644;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12177v1 Announce Type: cross  Abstract: Retrieval Augmented Generation (RAG) has emerged as an effective solution for mitigating hallucinations in Large Language Models (LLMs). The retrieval stage in RAG typically involves a pre-trained embedding model, which converts queries and passages into vectors to capture their semantics. However, a standard pre-trained embedding model may exhibit sub-optimal performance when applied to specific domain knowledge, necessitating fine-tuning. This paper addresses scenarios where the embeddings are only available from a black-box model. We introduce Model augmented fine-tuning (Mafin) -- a novel approach for fine-tuning a black-box embedding model by augmenting it with a trainable embedding model. Our results demonstrate that Mafin significantly enhances the performance of the black-box embeddings by only requiring the training of a small augmented model. We validate the effectiveness of our method on both labeled and unlabeled datasets, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;ASGEA&#65292;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#35774;&#35745;&#20102;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;ASGNN&#65292;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#23454;&#39564;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.11000</link><description>&lt;p&gt;
ASGEA&#65306;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#36827;&#34892;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
ASGEA: Exploiting Logic Rules from Align-Subgraphs for Entity Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11000
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;ASGEA&#65292;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#65292;&#35774;&#35745;&#20102;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;ASGNN&#65292;&#24341;&#20837;&#20102;&#22810;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#65288;EA&#65289;&#26088;&#22312;&#35782;&#21035;&#20195;&#34920;&#30456;&#21516;&#29616;&#23454;&#19990;&#30028;&#23545;&#35937;&#30340;&#19981;&#21516;&#30693;&#35782;&#22270;&#20013;&#30340;&#23454;&#20307;&#12290;&#26368;&#36817;&#22522;&#20110;&#23884;&#20837;&#30340;EA&#26041;&#27861;&#22312;EA&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#38754;&#20020;&#30528;&#35299;&#37322;&#24615;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#23436;&#20840;&#20381;&#36182;&#20110;&#23884;&#20837;&#36317;&#31163;&#65292;&#24182;&#24573;&#35270;&#20102;&#19968;&#23545;&#23545;&#40784;&#23454;&#20307;&#32972;&#21518;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Align-Subgraph&#23454;&#20307;&#23545;&#40784;&#65288;ASGEA&#65289;&#26694;&#26550;&#26469;&#21033;&#29992;Align-Subgraphs&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;ASGEA&#20351;&#29992;&#38170;&#38142;&#25509;&#20316;&#20026;&#26725;&#26753;&#26469;&#26500;&#24314;Align-Subgraphs&#65292;&#24182;&#27839;&#30528;&#36328;&#30693;&#35782;&#22270;&#30340;&#36335;&#24452;&#20256;&#25773;&#65292;&#36825;&#20351;&#20854;&#21306;&#21035;&#20110;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;ASGNN&#65292;&#20197;&#26377;&#25928;&#35782;&#21035;&#21644;&#25972;&#21512;&#36328;&#30693;&#35782;&#22270;&#30340;&#36923;&#36753;&#35268;&#21017;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#33410;&#28857;&#32423;&#22810;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#32467;&#21512;&#22810;&#27169;&#24577;&#22686;&#24378;&#30340;&#38170;&#28857;&#26469;&#22686;&#24378;Align-Subgraph&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11000v1 Announce Type: cross  Abstract: Entity alignment (EA) aims to identify entities across different knowledge graphs that represent the same real-world objects. Recent embedding-based EA methods have achieved state-of-the-art performance in EA yet faced interpretability challenges as they purely rely on the embedding distance and neglect the logic rules behind a pair of aligned entities. In this paper, we propose the Align-Subgraph Entity Alignment (ASGEA) framework to exploit logic rules from Align-Subgraphs. ASGEA uses anchor links as bridges to construct Align-Subgraphs and spreads along the paths across KGs, which distinguishes it from the embedding-based methods. Furthermore, we design an interpretable Path-based Graph Neural Network, ASGNN, to effectively identify and integrate the logic rules across KGs. We also introduce a node-level multi-modal attention mechanism coupled with multi-modal enriched anchors to augment the Align-Subgraph. Our experimental results 
&lt;/p&gt;</description></item><item><title>DoRA&#26159;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#26435;&#37325;&#20998;&#35299;&#20026;&#24133;&#24230;&#21644;&#26041;&#21521;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;LoRA&#36827;&#34892;&#26041;&#21521;&#26356;&#26032;&#30340;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#23398;&#20064;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26469;&#25552;&#39640;&#23545;LLaMA&#65292;LLaVA&#21644;VL-B&#30340;&#24494;&#35843;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09353</link><description>&lt;p&gt;
DoRA: &#20998;&#35299;&#26435;&#37325;&#30340;&#20302;&#31209;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
DoRA: Weight-Decomposed Low-Rank Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09353
&lt;/p&gt;
&lt;p&gt;
DoRA&#26159;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#26435;&#37325;&#20998;&#35299;&#20026;&#24133;&#24230;&#21644;&#26041;&#21521;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#20351;&#29992;LoRA&#36827;&#34892;&#26041;&#21521;&#26356;&#26032;&#30340;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#24378;&#23398;&#20064;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#26469;&#25552;&#39640;&#23545;LLaMA&#65292;LLaVA&#21644;VL-B&#30340;&#24494;&#35843;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PEFT&#65289;&#26041;&#27861;&#20013;&#65292;&#30001;&#20110;&#36991;&#20813;&#20102;&#39069;&#22806;&#30340;&#25512;&#29702;&#25104;&#26412;&#65292;LoRA&#21450;&#20854;&#21464;&#31181;&#26041;&#27861;&#22240;&#27492;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#19982;&#23436;&#20840;&#24494;&#35843;&#65288;FT&#65289;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#31934;&#24230;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26435;&#37325;&#20998;&#35299;&#20998;&#26512;&#26041;&#27861;&#26469;&#30740;&#31350;FT&#21644;LoRA&#20043;&#38388;&#30340;&#20869;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#27169;&#25311;FT&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DoRA&#30340;&#26435;&#37325;&#20998;&#35299;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;&#12290;DoRA&#23558;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#20998;&#35299;&#20026;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#24133;&#24230;&#21644;&#26041;&#21521;&#65292;&#24182;&#19988;&#20855;&#20307;&#20351;&#29992;LoRA&#36827;&#34892;&#26041;&#21521;&#26356;&#26032;&#65292;&#20197;&#26377;&#25928;&#22320;&#20943;&#23569;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;DoRA&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;LoRA&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#20219;&#20309;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#22312;&#24494;&#35843;LLaMA&#65292;LLaVA&#21644;VL-B&#19978;&#65292;DoRA&#22987;&#32456;&#20248;&#20110;LoRA&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09353v1 Announce Type: new Abstract: Among the widely used parameter-efficient finetuning (PEFT) methods, LoRA and its variants have gained considerable popularity because of avoiding additional inference costs. However, there still often exists an accuracy gap between these methods and full fine-tuning (FT). In this work, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-B
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ELaTE&#65292;&#19968;&#31181;&#22522;&#20110;&#27969;&#21305;&#37197;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#30701;&#38899;&#39057;&#25552;&#31034;&#20197;&#31934;&#30830;&#25511;&#21046;&#31505;&#22768;&#26102;&#26426;&#21644;&#34920;&#24773;&#29983;&#25104;&#20219;&#20309;&#35828;&#35805;&#32773;&#30340;&#33258;&#28982;&#31505;&#22768;&#12290;</title><link>https://arxiv.org/abs/2402.07383</link><description>&lt;p&gt;
&#20351;&#22522;&#20110;&#27969;&#21305;&#37197;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#33258;&#30001;&#22320;&#20135;&#29983;&#31505;&#22768;
&lt;/p&gt;
&lt;p&gt;
Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ELaTE&#65292;&#19968;&#31181;&#22522;&#20110;&#27969;&#21305;&#37197;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#65292;&#21487;&#20197;&#26681;&#25454;&#30701;&#38899;&#39057;&#25552;&#31034;&#20197;&#31934;&#30830;&#25511;&#21046;&#31505;&#22768;&#26102;&#26426;&#21644;&#34920;&#24773;&#29983;&#25104;&#20219;&#20309;&#35828;&#35805;&#32773;&#30340;&#33258;&#28982;&#31505;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31505;&#22768;&#26159;&#20154;&#31867;&#35821;&#38899;&#20013;&#26368;&#34920;&#36798;&#24615;&#21644;&#33258;&#28982;&#30340;&#19968;&#37096;&#20998;&#65292;&#20256;&#36798;&#30528;&#24773;&#24863;&#12289;&#31038;&#20132;&#26263;&#31034;&#21644;&#24189;&#40664;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25991;&#26412;&#21040;&#35821;&#38899;(TTS)&#31995;&#32479;&#32570;&#20047;&#20135;&#29983;&#36924;&#30495;&#19988;&#21512;&#36866;&#30340;&#31505;&#22768;&#30340;&#33021;&#21147;&#65292;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#21644;&#29992;&#25143;&#20307;&#39564;&#12290;&#34429;&#28982;&#20043;&#21069;&#26377;&#24037;&#20316;&#29983;&#25104;&#20102;&#33258;&#28982;&#30340;&#31505;&#22768;&#65292;&#20294;&#22312;&#25511;&#21046;&#29983;&#25104;&#30340;&#31505;&#22768;&#30340;&#26102;&#26426;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ELaTE&#65292;&#19968;&#31181;&#21487;&#20197;&#22522;&#20110;&#30701;&#38899;&#39057;&#25552;&#31034;&#20197;&#31934;&#30830;&#25511;&#21046;&#31505;&#22768;&#26102;&#26426;&#21644;&#34920;&#24773;&#30340;&#38646;&#26679;&#26412;TTS&#31995;&#32479;&#65292;&#21487;&#20197;&#20135;&#29983;&#20219;&#20309;&#35828;&#35805;&#32773;&#30340;&#33258;&#28982;&#31505;&#22768;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ELaTE&#36890;&#36807;&#38899;&#39057;&#25552;&#31034;&#26469;&#27169;&#20223;&#22768;&#38899;&#29305;&#24449;&#65292;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#26469;&#25351;&#31034;&#25152;&#29983;&#25104;&#35821;&#38899;&#30340;&#20869;&#23481;&#65292;&#36890;&#36807;&#36755;&#20837;&#26469;&#25511;&#21046;&#31505;&#22768;&#34920;&#24773;&#65292;&#21487;&#20197;&#26159;&#31505;&#22768;&#30340;&#36215;&#22987;&#21644;&#32467;&#26463;&#26102;&#38388;&#65292;&#25110;&#21253;&#21547;&#35201;&#27169;&#20223;&#30340;&#31505;&#22768;&#30340;&#21478;&#22806;&#38899;&#39057;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#25214;&#21040;&#30340;&#25216;&#26415;&#22522;&#30784;&#36827;&#34892;&#20102;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Laughter is one of the most expressive and natural aspects of human speech, conveying emotions, social cues, and humor. However, most text-to-speech (TTS) systems lack the ability to produce realistic and appropriate laughter sounds, limiting their applications and user experience. While there have been prior works to generate natural laughter, they fell short in terms of controlling the timing and variety of the laughter to be generated. In this work, we propose ELaTE, a zero-shot TTS that can generate natural laughing speech of any speaker based on a short audio prompt with precise control of laughter timing and expression. Specifically, ELaTE works on the audio prompt to mimic the voice characteristic, the text prompt to indicate the contents of the generated speech, and the input to control the laughter expression, which can be either the start and end times of laughter, or the additional audio prompt that contains laughter to be mimicked. We develop our model based on the foundati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19987;&#21033;&#21709;&#24212;&#26234;&#33021;&#31995;&#32479;PARIS&#21644;LE-PARIS&#65292;&#36890;&#36807;&#26500;&#24314;OA&#20027;&#39064;&#25968;&#25454;&#24211;&#12289;&#24320;&#21457;&#21709;&#24212;&#27169;&#26495;&#20197;&#21450;&#23454;&#26045;&#25512;&#33616;&#31995;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#65292;&#26088;&#22312;&#21152;&#24555;&#19987;&#21033;&#24459;&#24072;&#22788;&#29702;&#23457;&#26597;&#24847;&#35265;&#22238;&#24212;&#30340;&#25928;&#29575;&#12290; &#36890;&#36807;&#22810;&#33539;&#24335;&#20998;&#26512;&#21644;&#38271;&#26399;&#25968;&#25454;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;OA&#20027;&#39064;&#30340;&#24314;&#35774;&#24615;&#21644;LLM&#23545;&#20110;&#22238;&#24212;&#33258;&#21160;&#29983;&#25104;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00421</link><description>&lt;p&gt;
&#20174;PARIS&#21040;LE-PARIS&#65306;&#36890;&#36807;&#25512;&#33616;&#31995;&#32479;&#21644;&#21327;&#20316;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#19987;&#21033;&#21709;&#24212;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
From PARIS to LE-PARIS: Toward Patent Response Automation with Recommender Systems and Collaborative Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19987;&#21033;&#21709;&#24212;&#26234;&#33021;&#31995;&#32479;PARIS&#21644;LE-PARIS&#65292;&#36890;&#36807;&#26500;&#24314;OA&#20027;&#39064;&#25968;&#25454;&#24211;&#12289;&#24320;&#21457;&#21709;&#24212;&#27169;&#26495;&#20197;&#21450;&#23454;&#26045;&#25512;&#33616;&#31995;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#65292;&#26088;&#22312;&#21152;&#24555;&#19987;&#21033;&#24459;&#24072;&#22788;&#29702;&#23457;&#26597;&#24847;&#35265;&#22238;&#24212;&#30340;&#25928;&#29575;&#12290; &#36890;&#36807;&#22810;&#33539;&#24335;&#20998;&#26512;&#21644;&#38271;&#26399;&#25968;&#25454;&#39564;&#35777;&#65292;&#35777;&#26126;&#20102;OA&#20027;&#39064;&#30340;&#24314;&#35774;&#24615;&#21644;LLM&#23545;&#20110;&#22238;&#24212;&#33258;&#21160;&#29983;&#25104;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19987;&#21033;&#23457;&#26597;&#20013;&#65292;&#23545;&#20110;&#21450;&#26102;&#21644;&#26377;&#25928;&#22320;&#22238;&#24212;&#23457;&#26597;&#24847;&#35265;&#65288;OAs&#65289;&#23545;&#20110;&#33719;&#24471;&#19987;&#21033;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#36807;&#21435;&#30340;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#24456;&#23569;&#28041;&#21450;&#21040;&#36825;&#19968;&#26041;&#38754;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20171;&#32461;&#20102;&#19987;&#21033;&#23457;&#26597;&#24847;&#35265;&#21709;&#24212;&#26234;&#33021;&#31995;&#32479;&#65288;PARIS&#65289;&#21450;&#20854;&#20808;&#36827;&#29256;&#26412;LE-PARIS&#12290;&#36825;&#20123;&#31995;&#32479;&#26088;&#22312;&#21152;&#24555;&#19987;&#21033;&#24459;&#24072;&#22312;&#21327;&#20316;&#22788;&#29702;OA&#22238;&#24212;&#26041;&#38754;&#30340;&#25928;&#29575;&#12290;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#21253;&#25324;&#26500;&#24314;OA&#20027;&#39064;&#25968;&#25454;&#24211;&#65292;&#24320;&#21457;&#21709;&#24212;&#27169;&#26495;&#65292;&#20197;&#21450;&#23454;&#26045;&#25512;&#33616;&#31995;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#39564;&#35777;&#28041;&#21450;&#20351;&#29992;USPTO Office Action&#25968;&#25454;&#24211;&#21644;&#24459;&#24072;&#19982;&#25105;&#20204;&#31995;&#32479;&#30340;&#38271;&#26399;&#20132;&#20114;&#25968;&#25454;&#36827;&#34892;&#30340;&#22810;&#33539;&#24335;&#20998;&#26512;&#65292;&#20026;&#26399;&#20845;&#24180;&#12290;&#36890;&#36807;&#20116;&#20010;&#30740;&#31350;&#65292;&#25105;&#20204;&#21033;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#25552;&#20986;&#30340;Delphi&#36807;&#31243;&#26469;&#26816;&#39564;OA&#20027;&#39064;&#30340;&#24314;&#35774;&#24615;&#65288;&#30740;&#31350;1&#21644;2&#65289;&#65292;&#36824;&#26377;&#20351;&#29992;&#25512;&#33616;&#31995;&#32479;&#21644;&#22522;&#20110;LLM&#30340;&#21709;&#24212;&#29983;&#25104;&#26469;&#25552;&#39640;&#22238;&#24212;&#36136;&#37327;&#65288;&#30740;&#31350;3&#21644;4&#65289;&#65292;&#20197;&#21450;&#32463;&#36807;&#35757;&#32451;&#30340;LLM&#23545;&#20110;&#22238;&#24212;&#33258;&#21160;&#29983;&#25104;&#30340;&#21487;&#34892;&#24615;&#65288;&#30740;&#31350;5&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In patent prosecution, timely and effective responses to Office Actions (OAs) are crucial for acquiring patents, yet past automation and AI research have scarcely addressed this aspect. To address this gap, our study introduces the Patent Office Action Response Intelligence System (PARIS) and its advanced version, the Large Language Model Enhanced PARIS (LE-PARIS). These systems are designed to expedite the efficiency of patent attorneys in collaboratively handling OA responses. The systems' key features include the construction of an OA Topics Database, development of Response Templates, and implementation of Recommender Systems and LLM-based Response Generation. Our validation involves a multi-paradigmatic analysis using the USPTO Office Action database and longitudinal data of attorney interactions with our systems over six years. Through five studies, we examine the constructiveness of OA topics (studies 1 and 2) using topic modeling and the proposed Delphi process, the efficacy of
&lt;/p&gt;</description></item><item><title>LoRAMoE&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#36866;&#37197;&#22120;&#21644;&#36335;&#30001;&#22120;&#32593;&#32476;&#65292;&#31867;&#20284;&#20110;MoE&#30340;&#25554;&#20214;&#29256;&#26412;&#65292;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19990;&#30028;&#30693;&#35782;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.09979</link><description>&lt;p&gt;
LoRAMoE: &#36890;&#36807;MoE&#39118;&#26684;&#30340;&#25554;&#20214;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19990;&#30028;&#30693;&#35782;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models via MoE-Style Plugin
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09979
&lt;/p&gt;
&lt;p&gt;
LoRAMoE&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20302;&#31209;&#36866;&#37197;&#22120;&#21644;&#36335;&#30001;&#22120;&#32593;&#32476;&#65292;&#31867;&#20284;&#20110;MoE&#30340;&#25554;&#20214;&#29256;&#26412;&#65292;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19990;&#30028;&#30693;&#35782;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#19982;&#20154;&#31867;&#25351;&#20196;&#23545;&#40784;&#65292;&#24182;&#22686;&#24378;&#23427;&#20204;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25351;&#20196;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#22686;&#21152;&#21487;&#33021;&#20250;&#30772;&#22351;LLMs&#20808;&#21069;&#23384;&#20648;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoRAMoE&#65292;&#36825;&#26159;&#19968;&#20010;&#24341;&#20837;&#20102;&#22810;&#20010;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#24182;&#36890;&#36807;&#36335;&#30001;&#22120;&#32593;&#32476;&#38598;&#25104;&#23427;&#20204;&#30340;&#21019;&#26032;&#24615;&#26694;&#26550;&#65292;&#31867;&#20284;&#20110;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#30340;&#25554;&#20214;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09979v3 Announce Type: replace  Abstract: Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Increasing instruction data substantially is a direct solution to align the model with a broader range of downstream tasks or notably improve its performance on a specific task. However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs. To address this challenge, we propose LoRAMoE, a novelty framework that introduces several low-rank adapters (LoRA) and integrates them by using a router network, like a plugin version of Mixture of Experts (MoE). It freezes the backbone model and forces a portion of LoRAs to focus on leveraging world knowledge to solve downstream tasks, to alleviate world knowledge-edge forgetting. Experimental results show that, as the instruction data increases, LoRAMoE can si
&lt;/p&gt;</description></item><item><title>RDR&#26041;&#27861;&#25552;&#20986;&#20102;&#36890;&#36807;&#22238;&#39038;&#12289;&#23457;&#24910;&#21644;&#22238;&#24212;&#19977;&#20010;&#30446;&#26631;&#26469;&#22686;&#24378;&#35821;&#35328;&#29702;&#35299;&#30340;&#31070;&#32463;&#32593;&#32476;&#31649;&#36947;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#27169;&#22411;&#25805;&#32437;NLU&#22522;&#20934;&#27979;&#35797;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2312.09932</link><description>&lt;p&gt;
RDR&#65306;&#22686;&#24378;&#35821;&#35328;&#29702;&#35299;&#30340;&#22238;&#39038;&#12289;&#23457;&#24910;&#21644;&#22238;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RDR: the Recap, Deliberate, and Respond Method for Enhanced Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09932
&lt;/p&gt;
&lt;p&gt;
RDR&#26041;&#27861;&#25552;&#20986;&#20102;&#36890;&#36807;&#22238;&#39038;&#12289;&#23457;&#24910;&#21644;&#22238;&#24212;&#19977;&#20010;&#30446;&#26631;&#26469;&#22686;&#24378;&#35821;&#35328;&#29702;&#35299;&#30340;&#31070;&#32463;&#32593;&#32476;&#31649;&#36947;&#65292;&#35299;&#20915;&#20102;&#31070;&#32463;&#27169;&#22411;&#25805;&#32437;NLU&#22522;&#20934;&#27979;&#35797;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#31649;&#36947;&#36890;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#19978;&#19979;&#25991;&#65292;&#32780;&#36825;&#20123;&#19978;&#19979;&#25991;&#24182;&#19981;&#20165;&#20165;&#23384;&#22312;&#20110;&#36755;&#20837;&#25968;&#25454;&#20013;&#12290;&#36890;&#36807;&#20808;&#21069;&#30340;&#30740;&#31350;&#65292;&#24050;&#32463;&#26126;&#26174;&#22320;&#35777;&#26126;&#20102;NLU&#22522;&#20934;&#27979;&#35797;&#23545;&#31070;&#32463;&#27169;&#22411;&#30340;&#25805;&#32437;&#25935;&#24863;&#65292;&#36825;&#20123;&#27169;&#22411;&#21033;&#29992;&#32534;&#30721;&#30340;&#22806;&#37096;&#30693;&#35782;&#20013;&#30340;&#32479;&#35745;&#29305;&#24449;&#20154;&#20026;&#22320;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;&#22238;&#39038;&#12289;&#23457;&#24910;&#21644;&#22238;&#24212;&#65288;RDR&#65289;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#31649;&#36947;&#20013;&#24341;&#20837;&#19977;&#20010;&#19981;&#21516;&#30340;&#30446;&#26631;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22238;&#39038;&#30446;&#26631;&#28041;&#21450;&#20351;&#29992;&#19968;&#20010;&#37322;&#20041;&#27169;&#22411;&#23545;&#36755;&#20837;&#25991;&#26412;&#36827;&#34892;&#37322;&#20041;&#65292;&#20197;&#20415;&#23545;&#20854;&#36827;&#34892;&#24635;&#32467;&#21644;&#27010;&#25324;&#12290;&#20854;&#27425;&#65292;&#23457;&#24910;&#30446;&#26631;&#28041;&#21450;&#21033;&#29992;&#22270;&#23884;&#20837;&#27169;&#22411;&#23545;&#19982;&#36755;&#20837;&#25991;&#26412;&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#30456;&#20851;&#30340;&#22806;&#37096;&#22270;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#12290;&#26368;&#21518;&#65292;&#22238;&#24212;&#30446;&#26631;e
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09932v2 Announce Type: replace-cross  Abstract: Natural language understanding (NLU) using neural network pipelines often requires additional context that is not solely present in the input data. Through Prior research, it has been evident that NLU benchmarks are susceptible to manipulation by neural models, wherein these models exploit statistical artifacts within the encoded external knowledge to artificially inflate performance metrics for downstream tasks. Our proposed approach, known as the Recap, Deliberate, and Respond (RDR) paradigm, addresses this issue by incorporating three distinct objectives within the neural network pipeline. Firstly, the Recap objective involves paraphrasing the input text using a paraphrasing model in order to summarize and encapsulate its essence. Secondly, the Deliberation objective entails encoding external graph information related to entities mentioned in the input text, utilizing a graph embedding model. Finally, the Respond objective e
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#32654;&#22269;&#21069;100&#21517;&#22823;&#23398;&#21046;&#23450;&#30340;&#23398;&#26415;&#25919;&#31574;&#21644;&#25351;&#21335;&#65292;&#25581;&#31034;&#20102;&#22823;&#22810;&#25968;&#22823;&#23398;&#23545;&#20110;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#25972;&#21512;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#25918;&#20294;&#35880;&#24910;&#24577;&#24230;&#65292;&#20027;&#35201;&#20851;&#27880;&#28857;&#22312;&#20110;&#20262;&#29702;&#20351;&#29992;&#12289;&#20934;&#30830;&#24615;&#21644;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2312.05235</link><description>&lt;p&gt;
&#39640;&#31561;&#25945;&#32946;&#20013;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65306;&#36890;&#36807;&#22823;&#23398;&#30340;&#25919;&#31574;&#12289;&#36164;&#28304;&#21644;&#25351;&#21335;&#20102;&#35299;ChatGPT
&lt;/p&gt;
&lt;p&gt;
Generative AI in Higher Education: Seeing ChatGPT Through Universities' Policies, Resources, and Guidelines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05235
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#32654;&#22269;&#21069;100&#21517;&#22823;&#23398;&#21046;&#23450;&#30340;&#23398;&#26415;&#25919;&#31574;&#21644;&#25351;&#21335;&#65292;&#25581;&#31034;&#20102;&#22823;&#22810;&#25968;&#22823;&#23398;&#23545;&#20110;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#25972;&#21512;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#25918;&#20294;&#35880;&#24910;&#24577;&#24230;&#65292;&#20027;&#35201;&#20851;&#27880;&#28857;&#22312;&#20110;&#20262;&#29702;&#20351;&#29992;&#12289;&#20934;&#30830;&#24615;&#21644;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#25216;&#26415;&#30340;&#36827;&#27493;&#65288;&#22914;ChatGPT&#65289;&#20026;&#20016;&#23500;&#25945;&#32946;&#32463;&#39564;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#20294;&#22914;&#26524;&#34987;&#28389;&#29992;&#65292;&#20063;&#20250;&#24341;&#21457;&#26377;&#20851;&#23398;&#26415;&#35802;&#20449;&#30340;&#25285;&#24551;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#32654;&#22269;&#25490;&#21517;&#21069;100&#30340;&#22823;&#23398;&#21046;&#23450;&#30340;&#23398;&#26415;&#25919;&#31574;&#21644;&#25351;&#21335;&#26469;&#25506;&#35752;&#22823;&#23398;&#21644;&#25945;&#32946;&#32773;&#22914;&#20309;&#22312;&#20854;&#23398;&#26415;&#32972;&#26223;&#20013;&#23545;GenAI&#30340;&#21457;&#23637;&#20570;&#20986;&#21709;&#24212;&#21644;&#36866;&#24212;&#12290;&#25968;&#25454;&#26469;&#28304;&#21253;&#25324;&#36825;&#20123;&#22823;&#23398;&#21046;&#23450;&#30340;&#23398;&#26415;&#25919;&#31574;&#12289;&#22768;&#26126;&#12289;&#25351;&#21335;&#20197;&#21450;&#30456;&#20851;&#36164;&#28304;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#22823;&#23398;&#23545;&#20110;&#25972;&#21512;GenAI&#37319;&#21462;&#20102;&#24320;&#25918;&#20294;&#35880;&#24910;&#30340;&#24577;&#24230;&#12290;&#20027;&#35201;&#20851;&#27880;&#28857;&#22312;&#20110;&#20262;&#29702;&#20351;&#29992;&#12289;&#20934;&#30830;&#24615;&#21644;&#25968;&#25454;&#38544;&#31169;&#12290;&#22823;&#22810;&#25968;&#22823;&#23398;&#31215;&#26497;&#22238;&#24212;&#24182;&#25552;&#20379;&#22810;&#31181;&#36164;&#28304;&#65292;&#22914;&#35838;&#31243;&#22823;&#32434;&#27169;&#26495;/&#31034;&#20363;&#12289;&#30740;&#35752;&#20250;&#12289;&#20849;&#20139;&#25991;&#31456;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05235v2 Announce Type: replace  Abstract: The advancements in Generative Artificial Intelligence (GenAI) technologies such as ChatGPT provide opportunities to enrich educational experiences, but also raise concerns about academic integrity if misused. This study aims to explore how universities and educators respond and adapt to the development of GenAI in their academic contexts by analyzing academic policies and guidelines established by top-ranked US universities regarding the use of ChatGPT in higher education. The data sources include academic policies, statements, guidelines as well as relevant resources provided by the top 100 universities in the US. Results show that the majority of these universities adopt an open but cautious approach towards the integration of GenAI. Primary concerns lie in ethical usage, accuracy, and data privacy. Most universities actively respond and provide diverse types of resources, such as syllabus templates/samples, workshops, shared arti
&lt;/p&gt;</description></item><item><title>&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#26159;&#20943;&#23569;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#24187;&#35273;&#30340;&#20851;&#38190;&#65292;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22522;&#20110;&#27010;&#29575;&#30830;&#23450;&#24615;&#21644;&#35821;&#20041;&#30830;&#23450;&#24615;&#30340;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#26356;&#39640;&#27700;&#24179;&#30340;&#30830;&#23450;&#24615;&#23545;&#24212;&#26356;&#20302;&#27700;&#24179;&#30340;&#24187;&#35273;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#30830;&#23450;&#24615;&#30340;&#21709;&#24212;&#25490;&#24207;&#26041;&#27861;</title><link>https://arxiv.org/abs/2310.18794</link><description>&lt;p&gt;
&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#38477;&#20302;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#34394;&#26500;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Sequence-Level Certainty Reduces Hallucination In Knowledge-Grounded Dialogue Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18794
&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#26159;&#20943;&#23569;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#24187;&#35273;&#30340;&#20851;&#38190;&#65292;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22522;&#20110;&#27010;&#29575;&#30830;&#23450;&#24615;&#21644;&#35821;&#20041;&#30830;&#23450;&#24615;&#30340;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#26356;&#39640;&#27700;&#24179;&#30340;&#30830;&#23450;&#24615;&#23545;&#24212;&#26356;&#20302;&#27700;&#24179;&#30340;&#24187;&#35273;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#30830;&#23450;&#24615;&#30340;&#21709;&#24212;&#25490;&#24207;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#20316;&#20026;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#34394;&#26500;&#24773;&#20917;&#30340;&#20849;&#21516;&#20027;&#39064;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#24187;&#35273;&#27700;&#24179;&#19982;&#20004;&#31181;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65306;&#27010;&#29575;&#30830;&#23450;&#24615;&#21644;&#35821;&#20041;&#30830;&#23450;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#21709;&#24212;&#20013;&#20004;&#31181;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#27700;&#24179;&#30340;&#25552;&#39640;&#19982;&#34394;&#26500;&#27700;&#24179;&#30340;&#38477;&#20302;&#30456;&#20851;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#30830;&#23450;&#24615;&#30340;&#21709;&#24212;&#25490;&#24207;&#65288;CRR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35299;&#30721;&#26102;&#30340;&#24187;&#35273;&#32531;&#35299;&#26041;&#27861;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#23545;&#21709;&#24212;&#20505;&#36873;&#36827;&#34892;&#25490;&#24207;&#65292;&#24182;&#36755;&#20986;&#20855;&#26377;&#26368;&#39640;&#30830;&#23450;&#24615;&#27700;&#24179;&#30340;&#31572;&#26696;&#12290;&#19982;&#25105;&#20204;&#20851;&#20110;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#30340;&#23450;&#20041;&#30456;&#19968;&#33268;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;CRR&#26041;&#27861;&#65306;&#27010;&#29575;CRR&#65288;P-CRR&#65289;&#21644;&#35821;&#20041;CRR&#65288;S-CRR&#65289;&#12290;P-CRR&#20351;&#29992;&#25972;&#20010;&#24207;&#21015;&#30340;&#31639;&#26415;&#24179;&#22343;&#23545;&#21508;&#20010;&#21333;&#29420;&#25277;&#26679;&#30340;&#27169;&#22411;&#21709;&#24212;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18794v2 Announce Type: replace-cross  Abstract: In this work, we propose sequence-level certainty as a common theme over hallucination in Knowledge Grounded Dialogue Generation (KGDG). We explore the correlation between the level of hallucination and two types of sequence-level certainty: probabilistic certainty and semantic certainty. Empirical results reveal that a higher level of both types of sequence-level certainty in model responses is correlated with a lower level of hallucination. We further propose Certainty-based Response Ranking (CRR), a decoding-time hallucination mitigation method that ranks response candidates based on their sequence-level certainty and outputs the answer with the highest certainty level. Aligning with our definitions of sequence-level certainty, we design 2 types of CRR approaches: Probabilistic CRR (P-CRR) and Semantic CRR (S-CRR). P-CRR ranks individually sampled model responses using the arithmetic mean log-probability of the entire sequen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31354;&#38388;&#32467;&#26500;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#22312;&#34920;&#31034;&#21644;&#25512;&#29702;&#31354;&#38388;&#32467;&#26500;&#26102;&#30340;&#34920;&#29616;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20855;&#26377;&#25429;&#25417;&#31354;&#38388;&#32467;&#26500;&#38544;&#21547;&#29305;&#24449;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2310.14540</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31354;&#38388;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Spatial Understanding of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31354;&#38388;&#32467;&#26500;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#22312;&#34920;&#31034;&#21644;&#25512;&#29702;&#31354;&#38388;&#32467;&#26500;&#26102;&#30340;&#34920;&#29616;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20855;&#26377;&#25429;&#25417;&#31354;&#38388;&#32467;&#26500;&#38544;&#21547;&#29305;&#24449;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#30475;&#21040;&#25991;&#26412;&#65292;&#20294;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#34920;&#31034;&#38544;&#21547;&#22320;&#25429;&#25417;&#20102;&#22320;&#38754;&#27010;&#24565;&#30340;&#20960;&#20010;&#26041;&#38754;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLM&#34920;&#31034;&#23545;&#19968;&#31181;&#29305;&#21035;&#26174;&#33879;&#30340;&#22522;&#30784;&#30693;&#35782; -- &#31354;&#38388;&#20851;&#31995;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#28982;&#35821;&#35328;&#23548;&#33322;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;LLMs&#65292;&#29305;&#21035;&#26159;GPT-3.5-turbo&#12289;GPT-4 &#21644; Llama2 &#31995;&#21015;&#27169;&#22411;&#65292;&#34920;&#31034;&#21644;&#25512;&#29702;&#31354;&#38388;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#20219;&#21153;&#25581;&#31034;&#20102;LLM&#22312;&#19981;&#21516;&#31354;&#38388;&#32467;&#26500; (&#21253;&#25324;&#27491;&#26041;&#24418;&#12289;&#20845;&#36793;&#24418;&#21644;&#19977;&#35282;&#24418;&#26684;&#12289;&#29615;&#21644;&#26641;) &#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#22312;&#24191;&#27867;&#30340;&#38169;&#35823;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#38169;&#35823;&#21453;&#26144;&#20102;&#31354;&#38388;&#21644;&#38750;&#31354;&#38388;&#22240;&#32032;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;LLMs&#20284;&#20046;&#38544;&#21547;&#22320;&#25429;&#25417;&#20102;&#31354;&#38388;&#32467;&#26500;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#20294;&#20173;&#26377;&#25552;&#21319;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14540v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) show remarkable capabilities across a variety of tasks. Despite the models only seeing text in training, several recent studies suggest that LLM representations implicitly capture aspects of the underlying grounded concepts. Here, we explore LLM representations of a particularly salient kind of grounded knowledge -- spatial relationships. We design natural-language navigation tasks and evaluate the ability of LLMs, in particular GPT-3.5-turbo, GPT-4, and Llama2 series models, to represent and reason about spatial structures. These tasks reveal substantial variability in LLM performance across different spatial structures, including square, hexagonal, and triangular grids, rings, and trees. In extensive error analysis, we find that LLMs' mistakes reflect both spatial and non-spatial factors. These findings suggest that LLMs appear to capture certain aspects of spatial structure implicitly, but room f
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20266;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#20302;&#36164;&#28304;&#25361;&#25112;&#65292;&#23454;&#39564;&#34920;&#26126;&#21033;&#29992;&#20266;&#25968;&#25454;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2309.05203</link><description>&lt;p&gt;
&#20174;&#20154;&#24037;&#30495;&#23454;&#21040;&#30495;&#23454;: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20266;&#25968;&#25454;&#36827;&#34892;&#20302;&#36164;&#28304;&#20998;&#23376;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
From Artificially Real to Real: Leveraging Pseudo Data from Large Language Models for Low-Resource Molecule Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.05203
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20266;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#20302;&#36164;&#28304;&#25361;&#25112;&#65292;&#23454;&#39564;&#34920;&#26126;&#21033;&#29992;&#20266;&#25968;&#25454;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#21457;&#29616;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#25512;&#21160;&#20102;&#26032;&#26448;&#26009;&#21644;&#21019;&#26032;&#33647;&#29289;&#35774;&#35745;&#30340;&#21457;&#23637;&#12290;&#26368;&#36817;&#65292;&#30789;&#20998;&#23376;&#21457;&#29616;&#30340;&#21457;&#23637;&#31361;&#20986;&#20102;&#36328;&#27169;&#24577;&#25216;&#26415;&#30340;&#26377;&#24076;&#26395;&#25104;&#26524;&#65292;&#36825;&#31181;&#25216;&#26415;&#26550;&#36215;&#20102;&#20998;&#23376;&#32467;&#26500;&#19982;&#20854;&#25551;&#36848;&#24615;&#27880;&#37322;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36328;&#27169;&#24577;&#26041;&#27861;&#32463;&#24120;&#36935;&#21040;&#25968;&#25454;&#31232;&#32570;&#30340;&#38382;&#39064;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#21644;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#20154;&#36896;&#30495;&#23454;&#25968;&#25454;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;&#26816;&#32034;&#30340;&#25552;&#31034;&#31574;&#30053;&#26469;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#20266;&#25968;&#25454;&#65292;&#28982;&#21518;&#25506;&#35752;&#20102;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#20266;&#25968;&#25454;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#20266;&#25968;&#25454;&#36827;&#34892;&#39046;&#22495;&#33258;&#36866;&#24212;&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#38656;&#35201;&#26356;&#23567;&#30340;&#27169;&#22411;&#35268;&#27169;&#12289;&#20943;&#23569;&#30340;&#25968;&#25454;&#37327;&#21644;&#26356;&#20302;&#30340;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.05203v3 Announce Type: replace  Abstract: Molecule discovery serves as a cornerstone in numerous scientific domains, fueling the development of new materials and innovative drug designs. Recent developments of in-silico molecule discovery have highlighted the promising results of cross-modal techniques, which bridge molecular structures with their descriptive annotations. However, these cross-modal methods frequently encounter the issue of data scarcity, hampering their performance and application. In this paper, we address the low-resource challenge by utilizing artificially-real data generated by Large Language Models (LLMs). We first introduce a retrieval-based prompting strategy to construct high-quality pseudo data, then explore the optimal method to effectively leverage this pseudo data. Experiments show that using pseudo data for domain adaptation outperforms all existing methods, while also requiring a smaller model scale, reduced data size and lower training cost, h
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#27979;&#20107;&#23454;&#30340;&#25345;&#32493;&#26102;&#38388;&#26469;&#20943;&#36731;&#26102;&#38388;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#35821;&#35328;&#27169;&#22411;&#37325;&#22797;&#25552;&#20379;&#36807;&#26102;&#20449;&#24687;&#65292;&#24182;&#24110;&#21161;&#27169;&#22411;&#25552;&#39640;&#30693;&#35782;&#23494;&#38598;&#20219;&#21153;&#30340;&#26657;&#20934;&#24615;&#12290;</title><link>https://arxiv.org/abs/2305.14824</link><description>&lt;p&gt;
&#36890;&#36807;&#20002;&#24323;&#36807;&#26102;&#20107;&#23454;&#26469;&#20943;&#36731;&#26102;&#38388;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Temporal Misalignment by Discarding Outdated Facts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14824
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#27979;&#20107;&#23454;&#30340;&#25345;&#32493;&#26102;&#38388;&#26469;&#20943;&#36731;&#26102;&#38388;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#36991;&#20813;&#35821;&#35328;&#27169;&#22411;&#37325;&#22797;&#25552;&#20379;&#36807;&#26102;&#20449;&#24687;&#65292;&#24182;&#24110;&#21161;&#27169;&#22411;&#25552;&#39640;&#30693;&#35782;&#23494;&#38598;&#20219;&#21153;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20445;&#30041;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#35265;&#36807;&#30340;&#22823;&#37327;&#19990;&#30028;&#30693;&#35782;&#65292;&#20294;&#36825;&#20123;&#30693;&#35782;&#23481;&#26131;&#36807;&#26102;&#65292;&#24182;&#19988;&#26356;&#26032;&#36215;&#26469;&#24182;&#19981;&#26159;&#19968;&#20214;&#31616;&#21333;&#30340;&#20107;&#24773;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#32463;&#24120;&#22312;&#26102;&#38388;&#19981;&#19968;&#33268;&#24615;&#19979;&#20351;&#29992;&#65292;&#34987;&#35201;&#27714;&#22238;&#31572;&#20851;&#20110;&#29616;&#22312;&#30340;&#38382;&#39064;&#65292;&#23613;&#31649;&#23427;&#20204;&#21482;&#26159;&#22312;&#36807;&#21435;&#25910;&#38598;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#36807;&#12290;&#20026;&#20102;&#20943;&#36731;&#26102;&#38388;&#19981;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20107;&#23454;&#25345;&#32493;&#26102;&#38388;&#39044;&#27979;&#65306;&#39044;&#27979;&#19968;&#20010;&#32473;&#23450;&#20107;&#23454;&#20250;&#20445;&#25345;&#30495;&#23454;&#30340;&#26102;&#38388;&#38271;&#24230;&#30340;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35782;&#21035;&#21738;&#20123;&#20107;&#23454;&#23481;&#26131;&#24555;&#36895;&#21464;&#21270;&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#36991;&#20813;&#37325;&#22797;&#36807;&#26102;&#20449;&#24687;&#65292;&#24182;&#30830;&#23450;&#21738;&#20123;&#39044;&#27979;&#38656;&#35201;&#23547;&#25214;&#26368;&#26032;&#30693;&#35782;&#28304;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#24314;&#27169;&#20107;&#23454;&#25345;&#32493;&#26102;&#38388;&#21487;&#20197;&#25552;&#39640;&#23545;&#30693;&#35782;&#23494;&#38598;&#20219;&#21153;&#30340;&#26657;&#20934;&#65292;&#20363;&#22914;&#22312;&#26102;&#38388;&#19981;&#19968;&#33268;&#24615;&#19979;&#30340;&#24320;&#25918;&#24615;&#26816;&#32034;&#38382;&#31572;&#65292;&#36890;&#36807;&#20002;&#24323;&#26131;&#21464;&#20107;&#23454;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#21644;&#23454;&#29616;&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.14824v3 Announce Type: replace  Abstract: While large language models are able to retain vast amounts of world knowledge seen during pretraining, such knowledge is prone to going out of date and is nontrivial to update. Furthermore, these models are often used under temporal misalignment, tasked with answering questions about the present, despite having only been trained on data collected in the past. To mitigate the effects of temporal misalignment, we propose fact duration prediction: the task of predicting how long a given fact will remain true. In our experiments, we demonstrate that identifying which facts are prone to rapid change can help models avoid reciting outdated information and determine which predictions require seeking out up-to-date knowledge sources. We also show how modeling fact duration improves calibration for knowledge-intensive tasks, such as open-retrieval question answering, under temporal misalignment, by discarding volatile facts. Our data and cod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#26816;&#26597;&#20102;ChatGPT&#22312;&#23545;&#35805;&#20013;&#30340;&#35805;&#35821;&#20998;&#26512;&#28508;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#23545;&#32447;&#24615;&#21644;&#20998;&#23618;&#35805;&#35821;&#32467;&#26500;&#30340;&#28145;&#23618;&#35821;&#20041;&#29702;&#35299;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;ChatGPT&#22312;&#35782;&#21035;&#20027;&#39064;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#29087;&#32451;&#24230;&#20294;&#22312;&#35805;&#35821;&#35299;&#26512;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2305.08391</link><description>&lt;p&gt;
ChatGPT&#22312;&#23545;&#35805;&#20013;&#30340;&#35805;&#35821;&#20998;&#26512;&#28508;&#21147;&#25581;&#31034;&#65306;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Uncovering the Potential of ChatGPT for Discourse Analysis in Dialogue: An Empirical Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.08391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#26816;&#26597;&#20102;ChatGPT&#22312;&#23545;&#35805;&#20013;&#30340;&#35805;&#35821;&#20998;&#26512;&#28508;&#21147;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#23545;&#32447;&#24615;&#21644;&#20998;&#23618;&#35805;&#35821;&#32467;&#26500;&#30340;&#28145;&#23618;&#35821;&#20041;&#29702;&#35299;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;ChatGPT&#22312;&#35782;&#21035;&#20027;&#39064;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#29087;&#32451;&#24230;&#20294;&#22312;&#35805;&#35821;&#35299;&#26512;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;ChatGPT&#65292;&#22312;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#28982;&#32780;&#23427;&#20204;&#29702;&#35299;&#23545;&#35805;&#32467;&#26500;&#30340;&#33021;&#21147;&#20173;&#28982;&#36739;&#23569;&#34987;&#25506;&#35752;&#65292;&#36825;&#38656;&#35201;&#26356;&#39640;&#32423;&#21035;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#26597;ChatGPT&#22312;&#20004;&#20010;&#35805;&#35821;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65306;&#20027;&#39064;&#20998;&#21106;&#21644;&#35805;&#35821;&#35299;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#23545;&#35805;&#24213;&#23618;&#32447;&#24615;&#21644;&#20998;&#23618;&#35805;&#35821;&#32467;&#26500;&#30340;&#28145;&#23618;&#35821;&#20041;&#29702;&#35299;&#12290;&#20026;&#20102;&#25351;&#23548;ChatGPT&#23436;&#25104;&#36825;&#20123;&#20219;&#21153;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#20219;&#21153;&#25551;&#36848;&#12289;&#36755;&#20986;&#26684;&#24335;&#21644;&#32467;&#26500;&#21270;&#36755;&#20837;&#30340;&#25552;&#31034;&#27169;&#26495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;&#20027;&#39064;&#20998;&#21106;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#35805;&#35821;&#35299;&#26512;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#35782;&#21035;&#36890;&#29992;&#39046;&#22495;&#23545;&#35805;&#20013;&#30340;&#20027;&#39064;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#29087;&#32451;&#24230;&#65292;&#20294;&#22312;&#35805;&#35821;&#35299;&#26512;&#26041;&#38754;&#21364;&#36935;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.08391v2 Announce Type: replace  Abstract: Large language models, like ChatGPT, have shown remarkable capability in many downstream tasks, yet their ability to understand discourse structures of dialogues remains less explored, where it requires higher level capabilities of understanding and reasoning. In this paper, we aim to systematically inspect ChatGPT's performance in two discourse analysis tasks: topic segmentation and discourse parsing, focusing on its deep semantic understanding of linear and hierarchical discourse structures underlying dialogue. To instruct ChatGPT to complete these tasks, we initially craft a prompt template consisting of the task description, output format, and structured input. Then, we conduct experiments on four popular topic segmentation datasets and two discourse parsing datasets. The experimental results showcase that ChatGPT demonstrates proficiency in identifying topic structures in general-domain conversations yet struggles considerably i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#25991;&#26412;&#21435;&#22122;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#32416;&#27491;&#22122;&#22768;&#25991;&#26412;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;</title><link>https://arxiv.org/abs/1910.14080</link><description>&lt;p&gt;
&#22522;&#20110;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#25991;&#26412;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Contextual Text Denoising with Masked Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1910.14080
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#25991;&#26412;&#21435;&#22122;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#32416;&#27491;&#22122;&#22768;&#25991;&#26412;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20511;&#21161;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#22024;&#26434;&#25991;&#26412;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29616;&#25104;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#19978;&#19979;&#25991;&#25991;&#26412;&#21435;&#22122;&#31639;&#27861;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#65292;&#21487;&#20197;&#38598;&#25104;&#21040;&#20219;&#20309;NLP&#31995;&#32479;&#20013;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#37197;&#23545;&#30340;&#28165;&#27927;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#39069;&#22806;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#22122;&#22768;&#21644;&#33258;&#28982;&#22122;&#22768;&#19979;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#25152;&#25552;&#31639;&#27861;&#21487;&#20197;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#32416;&#27491;&#22122;&#22768;&#25991;&#26412;&#65292;&#24182;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#25552;&#39640;&#22024;&#26434;&#36755;&#20837;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1910.14080v2 Announce Type: replace  Abstract: Recently, with the help of deep learning models, significant advances have been made in different Natural Language Processing (NLP) tasks. Unfortunately, state-of-the-art models are vulnerable to noisy texts. We propose a new contextual text denoising algorithm based on the ready-to-use masked language model. The proposed algorithm does not require retraining of the model and can be integrated into any NLP system without additional training on paired cleaning training data. We evaluate our method under synthetic noise and natural noise and show that the proposed algorithm can use context information to correct noise text and improve the performance of noisy inputs in several downstream tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25910;&#38598;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#21517;&#20026;&#8220;Knowledge Pile&#8221;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#21892;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14624</link><description>&lt;p&gt;
CC&#26597;&#35810;&#65306;&#20174;&#20844;&#24320;&#25991;&#29486;&#20013;&#21457;&#29616;&#22823;&#35268;&#27169;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Query of CC: Unearthing Large Scale Domain-Specific Knowledge from Public Corpora. (arXiv:2401.14624v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25910;&#38598;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#21517;&#20026;&#8220;Knowledge Pile&#8221;&#30340;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25913;&#21892;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65292;&#28982;&#32780;&#29305;&#23450;&#39046;&#22495;&#30340;&#24320;&#28304;&#27169;&#22411;&#21644;&#25968;&#25454;&#20173;&#28982;&#38750;&#24120;&#31232;&#32570;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25163;&#21160;&#25351;&#23450;&#36164;&#28304;&#21644;&#25910;&#38598;&#29305;&#23450;&#39046;&#22495;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#36825;&#28040;&#32791;&#20102;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#8220;CC&#26597;&#35810;&#8221;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#23548;&#31181;&#23376;&#20449;&#24687;&#65292;&#24182;&#20174;&#20844;&#24320;&#25991;&#29486;&#20013;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#12290;&#23427;&#19981;&#20165;&#25910;&#38598;&#20102;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#30456;&#20851;&#25968;&#25454;&#65292;&#36824;&#25581;&#31034;&#20102;&#28508;&#22312;&#30340;&#25512;&#29702;&#36807;&#31243;&#25968;&#25454;&#12290;&#36890;&#36807;&#24212;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;Knowledge Pile&#8221;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#21253;&#25324;STEM&#31185;&#23398;&#21644;&#20154;&#25991;&#31185;&#23398;&#22312;&#20869;&#30340;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#8220;Knowledge Pile&#8221;&#26174;&#33879;&#25913;&#21892;&#20102;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated remarkable potential in various tasks, however, there remains a significant scarcity of open-source models and data for specific domains. Previous works have primarily focused on manually specifying resources and collecting high-quality data on specific domains, which significantly consume time and effort. To address this limitation, we propose an efficient data collection method~\textit{Query of CC} based on large language models. This method bootstraps seed information through a large language model and retrieves related data from public corpora. It not only collects knowledge-related data for specific domains but unearths the data with potential reasoning procedures. Through the application of this method, we have curated a high-quality dataset called~\textsc{Knowledge Pile}, encompassing four major domains, including stem and humanities sciences, among others. Experimental results demonstrate that~\textsc{Knowledge Pile} significantly improve
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#39537;&#21160;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#21644;&#30693;&#35782;&#22270;&#32467;&#26500;&#22686;&#24378;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#39640;LLMs&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#19978;&#65292;&#26356;&#22909;&#22320;&#22238;&#31572;&#24320;&#25918;&#24335;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.12671</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#30340;&#37325;&#35201;&#24615;&#65306;&#36890;&#36807;&#22270;&#32467;&#26500;&#21270;&#30693;&#35782;&#19978;&#19979;&#25991;&#25512;&#21160;&#24320;&#25918;&#24335;&#31572;&#26696;&#29983;&#25104;&#30340;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Context Matters: Pushing the Boundaries of Open-Ended Answer Generation with Graph-Structured Knowledge Context. (arXiv:2401.12671v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#39537;&#21160;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#21644;&#30693;&#35782;&#22270;&#32467;&#26500;&#22686;&#24378;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#39640;LLMs&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#19978;&#65292;&#26356;&#22909;&#22320;&#22238;&#31572;&#24320;&#25918;&#24335;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#26029;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#26500;&#24314;&#19978;&#19979;&#25991;&#20016;&#23500;&#12289;&#26377;&#24847;&#20041;&#30340;&#22238;&#31572;&#33267;&#20851;&#37325;&#35201;&#12290;&#30740;&#31350;&#20154;&#21592;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#24403;LLMs&#30340;&#21442;&#25968;&#36739;&#23569;&#26102;&#65292;&#23581;&#35797;&#25552;&#20379;&#21512;&#36866;&#31572;&#26696;&#32473;&#24320;&#25918;&#24335;&#38382;&#39064;&#26102;&#20250;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38556;&#30861;&#65292;&#23558;&#20808;&#36827;&#30340;&#31574;&#30053;&#19982;&#20016;&#23500;&#30340;&#22806;&#37096;&#39046;&#22495;&#30693;&#35782;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#31572;&#26696;&#30340;&#36136;&#37327;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#22522;&#20110;&#22270;&#30340;&#19978;&#19979;&#25991;&#26816;&#32034;&#19982;&#30693;&#35782;&#22270;&#32467;&#26500;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;&#31038;&#21306;&#38382;&#31572;&#24179;&#21488;&#65292;&#22914;AskUbuntu&#12289;Unix&#21644;ServerFault&#12290;&#25105;&#20204;&#23545;&#19981;&#21516;&#21442;&#25968;&#22823;&#23567;&#30340;&#21508;&#31181;LLMs&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#24320;&#25918;&#24335;&#38382;&#39064;&#30340;&#22238;&#31572;&#20013;&#30340;&#30693;&#35782;&#30830;&#23450;&#33021;&#21147;&#21644;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;GraphContextGen&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#29616;&#26377;&#26041;&#27861;&#19978;&#25345;&#32493;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the continuously advancing AI landscape, crafting context-rich and meaningful responses via Large Language Models (LLMs) is essential. Researchers are becoming more aware of the challenges that LLMs with fewer parameters encounter when trying to provide suitable answers to open-ended questions. To address these hurdles, the integration of cutting-edge strategies, augmentation of rich external domain knowledge to LLMs, offers significant improvements. This paper introduces a novel framework that combines graph-driven context retrieval in conjunction to knowledge graphs based enhancement, honing the proficiency of LLMs, especially in domain specific community question answering platforms like AskUbuntu, Unix, and ServerFault. We conduct experiments on various LLMs with different parameter sizes to evaluate their ability to ground knowledge and determine factual accuracy in answers to open-ended questions. Our methodology GraphContextGen consistently outperforms dominant text-based ret
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#21518;&#26524;&#65292;&#21457;&#29616;&#22312;&#22686;&#24378;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#20445;&#25345;&#36947;&#24503;&#23436;&#25972;&#24615;&#20043;&#38388;&#23384;&#22312;&#24726;&#35770;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#27880;&#20837;&#20934;&#30830;&#20449;&#24687;&#23545;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#24456;&#37325;&#35201;&#65292;&#20294;&#23427;&#21487;&#33021;&#30772;&#22351;&#27169;&#22411;&#30340;&#22522;&#26412;&#26694;&#26550;&#65292;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#21644;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2401.10647</link><description>&lt;p&gt;
&#25773;&#39118;&#25769;&#36215;&#39118;&#26292;&#65306;&#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models. (arXiv:2401.10647v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#21518;&#26524;&#65292;&#21457;&#29616;&#22312;&#22686;&#24378;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#20445;&#25345;&#36947;&#24503;&#23436;&#25972;&#24615;&#20043;&#38388;&#23384;&#22312;&#24726;&#35770;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#27880;&#20837;&#20934;&#30830;&#20449;&#24687;&#23545;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#24456;&#37325;&#35201;&#65292;&#20294;&#23427;&#21487;&#33021;&#30772;&#22351;&#27169;&#22411;&#30340;&#22522;&#26412;&#26694;&#26550;&#65292;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#21644;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#32418;&#38431;&#27979;&#35797;&#25110;&#36234;&#29425;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27010;&#24565;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#32534;&#36753;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#20462;&#25913;&#30340;&#22797;&#26434;&#21518;&#26524;&#65292;&#21457;&#29616;&#20102;&#22686;&#24378;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#20445;&#25345;&#20854;&#36947;&#24503;&#23436;&#25972;&#24615;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#28145;&#20837;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#24726;&#35770;&#65306;&#34429;&#28982;&#27880;&#20837;&#20934;&#30830;&#20449;&#24687;&#23545;&#20110;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23427;&#21364;&#21487;&#33021;&#30772;&#22351;&#27169;&#22411;&#30340;&#22522;&#26412;&#26694;&#26550;&#65292;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#21644;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;NicheHazardQA&#65292;&#29992;&#20110;&#30740;&#31350;&#27169;&#22411;&#22312;&#30456;&#21516;&#21644;&#36328;&#39046;&#22495;&#20013;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;&#36825;&#19968;&#26041;&#38754;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#32534;&#36753;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#23433;&#20840;&#24230;&#37327;&#21644;&#20445;&#25252;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing field of artificial intelligence, the concept of Red-Teaming or Jailbreaking large language models (LLMs) has emerged as a crucial area of study. This approach is especially significant in terms of assessing and enhancing the safety and robustness of these models. This paper investigates the intricate consequences of such modifications through model editing, uncovering a complex relationship between enhancing model accuracy and preserving its ethical integrity. Our in-depth analysis reveals a striking paradox: while injecting accurate information is crucial for model reliability, it can paradoxically destabilize the model's foundational framework, resulting in unpredictable and potentially unsafe behaviors. Additionally, we propose a benchmark dataset NicheHazardQA to investigate this unsafe behavior both within the same and cross topical domain. This aspect of our research sheds light on how the edits, impact the model's safety metrics and guardrails. Our find
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26700;&#38754;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#35268;&#21017;&#21019;&#24314;&#20102;&#19968;&#20010;&#29615;&#22659;&#65292;&#37327;&#21270;&#35780;&#20272;&#26234;&#33021;&#20307;&#31038;&#20132;&#20114;&#21160;&#30340;&#20449;&#24687;&#24615;&#21644;&#34920;&#36798;&#24615;&#65292;&#26088;&#22312;&#20811;&#26381;&#38544;&#31169;&#38382;&#39064;&#24182;&#20419;&#20351;&#26234;&#33021;&#20307;&#36827;&#34892;&#26377;&#24847;&#20041;&#12289;&#39640;&#36136;&#37327;&#30340;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.06509</link><description>&lt;p&gt;
AntEval: &#37327;&#21270;&#35780;&#20272;&#26234;&#33021;&#20307;&#31038;&#20132;&#20114;&#21160;&#30340;&#20449;&#24687;&#24615;&#21644;&#34920;&#36798;&#24615;
&lt;/p&gt;
&lt;p&gt;
AntEval: Quantitatively Evaluating Informativeness and Expressiveness of Agent Social Interactions. (arXiv:2401.06509v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26700;&#38754;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#35268;&#21017;&#21019;&#24314;&#20102;&#19968;&#20010;&#29615;&#22659;&#65292;&#37327;&#21270;&#35780;&#20272;&#26234;&#33021;&#20307;&#31038;&#20132;&#20114;&#21160;&#30340;&#20449;&#24687;&#24615;&#21644;&#34920;&#36798;&#24615;&#65292;&#26088;&#22312;&#20811;&#26381;&#38544;&#31169;&#38382;&#39064;&#24182;&#20419;&#20351;&#26234;&#33021;&#20307;&#36827;&#34892;&#26377;&#24847;&#20041;&#12289;&#39640;&#36136;&#37327;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26234;&#33021;&#20307;&#24050;&#25104;&#21151;&#22320;&#27169;&#20223;&#20102;&#21508;&#31181;&#24773;&#22659;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#20294;&#22797;&#26434;&#30340;&#12289;&#22810;&#35282;&#33394;&#31038;&#20132;&#20114;&#21160;&#22312;&#25193;&#23637;&#29615;&#22659;&#20013;&#30340;&#39046;&#22495;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#38544;&#31169;&#38382;&#39064;&#20351;&#25429;&#25417;&#21644;&#21033;&#29992;&#22797;&#26434;&#30340;&#29616;&#23454;&#29983;&#27963;&#20114;&#21160;&#21464;&#24471;&#22256;&#38590;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#32570;&#20047;&#23450;&#37327;&#35780;&#20272;&#26041;&#27861;&#38459;&#30861;&#20102;&#39640;&#36136;&#37327;&#26234;&#33021;&#20307;&#20114;&#21160;&#30340;&#36861;&#27714;&#65292;&#23548;&#33268;&#20114;&#21160;&#30340;&#20449;&#24687;&#24615;&#21644;&#34920;&#36798;&#24615;&#26377;&#38480;&#65292;&#34920;&#29616;&#20026;&#32932;&#27973;&#30340;&#38386;&#32842;&#32780;&#27809;&#26377;&#28165;&#26224;&#30340;&#24847;&#22270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26700;&#38754;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#65288;TRPG&#65289;&#30340;&#35268;&#21017;&#21019;&#24314;&#20102;&#19968;&#20010;&#26377;&#21033;&#20110;&#22797;&#26434;&#12289;&#19978;&#19979;&#25991;&#20016;&#23500;&#30340;&#20114;&#21160;&#30340;&#29615;&#22659;&#65292;&#24378;&#35843;&#20449;&#24687;&#24615;&#21644;&#34920;&#36798;&#24615;&#12290;&#36825;&#20010;&#34394;&#25311;&#29615;&#22659;&#20943;&#36731;&#20102;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#28608;&#21169;&#26234;&#33021;&#20307;&#20316;&#20026;&#28216;&#25103;&#30446;&#26631;&#30340;&#19968;&#37096;&#20998;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#12289;&#39640;&#36136;&#37327;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) based agents have successfully mimicked human behaviors in various scenarios, the realm of complex, multi-character social interactions within extended contexts remains underexplored. The challenge is compounded by privacy concerns, making it difficult to capture and utilize intricate real-life interactions. More importantly, the absence of quantitative evaluation methods hampers the pursuit of high-quality agent interactions, often leading to interactions that are limited in informativeness and expressiveness, characterized by superficial small talk without clear intentions. In this work, we leverage the rules of Tabletop Role-Playing Games (TRPG) to create an environment conducive to complex, context-rich interactions, emphasizing informativeness and expressiveness. This virtual setting alleviates privacy concerns and motivates agents to engage in meaningful, high-quality interactions as part of their in-game objectives. To assess these interactions
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#30495;&#23454;&#30340;&#39033;&#30446;&#20998;&#24067;&#12289;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#31561;&#26041;&#38754;&#26356;&#36148;&#21512;&#23454;&#38469;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.06401</link><description>&lt;p&gt;
DevEval: &#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DevEval: Evaluating Code Generation in Practical Software Projects. (arXiv:2401.06401v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#30495;&#23454;&#30340;&#39033;&#30446;&#20998;&#24067;&#12289;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#31561;&#26041;&#38754;&#26356;&#36148;&#21512;&#23454;&#38469;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#34920;&#29616;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#24050;&#32463;&#25552;&#20986;&#65292;&#20294;&#26159;&#19982;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#19981;&#19968;&#33268;&#65292;&#20363;&#22914;&#34394;&#26500;&#30340;&#31243;&#24207;&#20998;&#24067;&#65292;&#20381;&#36182;&#19981;&#36275;&#21644;&#23567;&#35268;&#27169;&#39033;&#30446;&#32972;&#26223;&#12290;&#22240;&#27492;&#65292;LLMs&#22312;&#23454;&#38469;&#39033;&#30446;&#20013;&#30340;&#33021;&#21147;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#19982;&#24320;&#21457;&#20154;&#21592;&#22312;&#23454;&#38469;&#39033;&#30446;&#20013;&#30340;&#32463;&#39564;&#30456;&#21563;&#21512;&#12290;DevEval&#36890;&#36807;&#19968;&#20010;&#20005;&#26684;&#30340;&#27969;&#31243;&#25910;&#38598;&#21040;&#20102;&#26469;&#33258;119&#20010;&#23454;&#38469;&#39033;&#30446;&#30340;2690&#20010;&#26679;&#26412;&#65292;&#28085;&#30422;10&#20010;&#39046;&#22495;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#22810;&#20010;&#32500;&#24230;&#19978;&#19982;&#23454;&#38469;&#39033;&#30446;&#30456;&#21563;&#21512;&#65292;&#20363;&#22914;&#30495;&#23454;&#30340;&#31243;&#24207;&#20998;&#24067;&#65292;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#12290;&#25105;&#20204;&#22312;DevEval&#19978;&#35780;&#20272;&#20102;&#20116;&#20010;&#27969;&#34892;&#30340;LLMs&#65288;&#20363;&#22914;gpt-4&#65292;gpt-3.5-turbo&#65292;CodeLLaMa&#21644;StarCoder&#65289;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;gpt-3.5-turbo&#30340;&#26368;&#39640;Pass@1&#21482;&#26377;42&#12290;
&lt;/p&gt;
&lt;p&gt;
How to evaluate Large Language Models (LLMs) in code generation is an open question. Many benchmarks have been proposed but are inconsistent with practical software projects, e.g., unreal program distributions, insufficient dependencies, and small-scale project contexts. Thus, the capabilities of LLMs in practical projects are still unclear. In this paper, we propose a new benchmark named DevEval, aligned with Developers' experiences in practical projects. DevEval is collected through a rigorous pipeline, containing 2,690 samples from 119 practical projects and covering 10 domains. Compared to previous benchmarks, DevEval aligns to practical projects in multiple dimensions, e.g., real program distributions, sufficient dependencies, and enough-scale project contexts. We assess five popular LLMs on DevEval (e.g., gpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo only is 42 in our experim
&lt;/p&gt;</description></item><item><title>LEGO&#26159;&#19968;&#31181;&#35821;&#35328;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#20851;&#32852;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#29702;&#35299;&#21644;&#31934;&#30830;&#30340;&#26631;&#35782;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.06071</link><description>&lt;p&gt;
LEGO: &#35821;&#35328;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#20851;&#32852;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LEGO:Language Enhanced Multi-modal Grounding Model. (arXiv:2401.06071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06071
&lt;/p&gt;
&lt;p&gt;
LEGO&#26159;&#19968;&#31181;&#35821;&#35328;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#20851;&#32852;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#29702;&#35299;&#21644;&#31934;&#30830;&#30340;&#26631;&#35782;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#27169;&#24577;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#20027;&#35201;&#24378;&#35843;&#25429;&#25417;&#27599;&#31181;&#27169;&#24577;&#20869;&#30340;&#20840;&#23616;&#20449;&#24687;&#65292;&#32780;&#24573;&#35270;&#20102;&#36328;&#27169;&#24577;&#24863;&#30693;&#23616;&#37096;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#26377;&#25928;&#29702;&#35299;&#36755;&#20837;&#25968;&#25454;&#32454;&#31890;&#24230;&#32454;&#33410;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#26356;&#32454;&#33268;&#29702;&#35299;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#22312;&#22810;&#20010;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32454;&#31890;&#24230;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#24378;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEGO&#65292;&#19968;&#31181;&#35821;&#35328;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#20851;&#32852;&#27169;&#22411;&#12290;&#38500;&#20102;&#20687;&#20854;&#20182;&#22810;&#27169;&#24577;&#27169;&#22411;&#19968;&#26679;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38656;&#35201;&#35814;&#32454;&#29702;&#35299;&#36755;&#20837;&#20869;&#30340;&#23616;&#37096;&#20449;&#24687;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#23637;&#31034;&#20102;&#31934;&#30830;&#30340;&#26631;&#35782;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models have demonstrated impressive performance across various tasks in different modalities. However, existing multi-modal models primarily emphasize capturing global information within each modality while neglecting the importance of perceiving local information across modalities. Consequently, these models lack the ability to effectively understand the fine-grained details of input data, limiting their performance in tasks that require a more nuanced understanding. To address this limitation, there is a compelling need to develop models that enable fine-grained understanding across multiple modalities, thereby enhancing their applicability to a wide range of tasks. In this paper, we propose LEGO, a language enhanced multi-modal grounding model. Beyond capturing global information like other multi-modal models, our proposed model excels at tasks demanding a detailed understanding of local information within the input. It demonstrates precise identification 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;PEFT&#26131;&#21463;&#29305;&#27931;&#20234;&#25915;&#20987;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;PETA&#65292;&#24182;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#21644;&#35302;&#21457;&#22120;&#35774;&#35745;&#20013;&#36827;&#34892;&#24191;&#27867;&#27979;&#35797;&#65292;&#21457;&#29616;PETA&#33021;&#22815;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#26410;&#21463;&#24433;&#21709;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#26377;&#25928;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.00648</link><description>&lt;p&gt;
&#26356;&#23569;&#23601;&#24847;&#21619;&#30528;&#26356;&#22810;: &#23545;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#29305;&#27931;&#20234;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Fewer is More: Trojan Attacks on Parameter-Efficient Fine-Tuning. (arXiv:2310.00648v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;PEFT&#26131;&#21463;&#29305;&#27931;&#20234;&#25915;&#20987;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#26041;&#24335;PETA&#65292;&#24182;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#21644;&#35302;&#21457;&#22120;&#35774;&#35745;&#20013;&#36827;&#34892;&#24191;&#27867;&#27979;&#35797;&#65292;&#21457;&#29616;PETA&#33021;&#22815;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#26410;&#21463;&#24433;&#21709;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#26377;&#25928;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#21487;&#20197;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#39640;&#25928;&#22320;&#36866;&#24212;&#21040;&#29305;&#23450;&#20219;&#21153;&#20013;&#12290;&#36890;&#36807;&#20165;&#24494;&#35843;&#19968;&#23567;&#37096;&#20998;&#65288;&#39069;&#22806;&#30340;&#65289;&#21442;&#25968;&#65292;PEFT&#23454;&#29616;&#20102;&#19982;&#20840;&#38754;&#24494;&#35843;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;PEFT&#30340;&#23433;&#20840;&#24615;&#24433;&#21709;&#20173;&#28982;&#40092;&#20026;&#20154;&#30693;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21021;&#27493;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;PEFT&#23545;&#29305;&#27931;&#20234;&#25915;&#20987;&#30340;&#29420;&#29305;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PETA&#65292;&#19968;&#31181;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#32771;&#34385;&#19979;&#28216;&#36866;&#24212;&#30340;&#26032;&#22411;&#25915;&#20987;&#26041;&#24335;&#65306;&#19978;&#23618;&#30446;&#26631;&#23558;&#21518;&#38376;&#23884;&#20837;PLM&#20013;&#65292;&#32780;&#19979;&#23618;&#30446;&#26631;&#27169;&#25311;PEFT&#20197;&#20445;&#30041;PLM&#30340;&#20219;&#21153;&#29305;&#23450;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#21644;&#35302;&#21457;&#22120;&#35774;&#35745;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PETA&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#21644;&#26410;&#21463;&#24433;&#21709;&#30340;&#24178;&#20928;&#20934;&#30830;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#20351;&#21463;&#23475;&#29992;&#25143;&#22312;&#20351;&#29992;&#32431;&#20928;&#25968;&#25454;&#23545;&#24102;&#26377;&#21518;&#38376;&#30340;PLM&#36827;&#34892;PEFT&#21518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient fine-tuning (PEFT) enables efficient adaptation of pre-trained language models (PLMs) to specific tasks. By tuning only a minimal set of (extra) parameters, PEFT achieves performance comparable to full fine-tuning. However, despite its prevalent use, the security implications of PEFT remain largely unexplored. In this paper, we conduct a pilot study revealing that PEFT exhibits unique vulnerability to trojan attacks. Specifically, we present PETA, a novel attack that accounts for downstream adaptation through bilevel optimization: the upper-level objective embeds the backdoor into a PLM while the lower-level objective simulates PEFT to retain the PLM's task-specific performance. With extensive evaluation across a variety of downstream tasks and trigger designs, we demonstrate PETA's effectiveness in terms of both attack success rate and unaffected clean accuracy, even after the victim user performs PEFT over the backdoored PLM using untainted data. Moreover, we empi
&lt;/p&gt;</description></item><item><title>SeaEval&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#20197;&#21450;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#34892;&#20026;&#21508;&#24322;&#65292;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#35821;&#20041;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#65292;&#20197;&#21450;&#27169;&#22411;&#22312;&#24773;&#24863;&#30456;&#20851;&#38382;&#39064;&#19978;&#30340;&#19968;&#33268;&#24615;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2309.04766</link><description>&lt;p&gt;
SeaEval&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65306;&#20174;&#36328;&#35821;&#35328;&#23545;&#40784;&#21040;&#25991;&#21270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning. (arXiv:2309.04766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04766
&lt;/p&gt;
&lt;p&gt;
SeaEval&#26159;&#19968;&#20010;&#35780;&#20272;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#20197;&#21450;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#34920;&#29616;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#34892;&#20026;&#21508;&#24322;&#65292;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#35821;&#20041;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#30340;&#22238;&#31572;&#19981;&#19968;&#33268;&#65292;&#20197;&#21450;&#27169;&#22411;&#22312;&#24773;&#24863;&#30456;&#20851;&#38382;&#39064;&#19978;&#30340;&#19968;&#33268;&#24615;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;SeaEval&#22522;&#20934;&#27979;&#35797;&#12290;&#38500;&#20102;&#34920;&#24449;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#29702;&#35299;&#21644;&#25512;&#29702;&#33258;&#28982;&#35821;&#35328;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#23427;&#20204;&#23545;&#25991;&#21270;&#23454;&#36341;&#12289;&#32454;&#24494;&#24046;&#21035;&#21644;&#20215;&#20540;&#35266;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#38500;&#20102;&#26631;&#20934;&#30340;&#20934;&#30830;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#36824;&#35843;&#26597;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#35821;&#20041;&#21644;&#22810;&#35821;&#35328;&#24615;&#32500;&#24230;&#19978;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#24320;&#28304;&#21644;&#38381;&#28304;&#27169;&#22411;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#22312;&#32463;&#20856;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#21644;&#25991;&#21270;&#29702;&#35299;&#26041;&#38754;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#37325;&#35201;&#21457;&#29616;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#32473;&#20986;&#25913;&#20889;&#25351;&#20196;&#26102;&#30340;&#34892;&#20026;&#21508;&#24322;&#65307;&#65288;2&#65289;&#35768;&#22810;&#27169;&#22411;&#20173;&#28982;&#21463;&#21040;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#65288;&#22914;&#20301;&#32622;&#20559;&#24046;&#12289;&#22823;&#22810;&#25968;&#26631;&#31614;&#20559;&#24046;&#65289;&#65307;&#65288;3&#65289;&#23545;&#20110;&#26681;&#28304;&#20110;&#20107;&#23454;&#12289;&#31185;&#23398;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#39044;&#26399;&#22312;&#35821;&#20041;&#19978;&#31561;&#20215;&#30340;&#22810;&#35821;&#35328;&#26597;&#35810;&#24212;&#35813;&#24471;&#21040;&#19968;&#33268;&#30340;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#36825;&#20123;&#26597;&#35810;&#19978;&#34920;&#29616;&#20986;&#20196;&#20154;&#24847;&#22806;&#30340;&#19981;&#19968;&#33268;&#24615;&#65307;&#65288;4&#65289;&#22810;&#35821;&#35328;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#23545;&#20110;&#24773;&#24863;&#30456;&#20851;&#30340;&#38382;&#39064;&#34920;&#29616;&#20986;&#19981;&#21516;&#31243;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SeaEval, a benchmark for multilingual foundation models. In addition to characterizing how these models understand and reason with natural language, we also investigate how well they comprehend cultural practices, nuances, and values. Alongside standard accuracy metrics, we investigate the brittleness of foundation models in the dimensions of semantics and multilinguality. Our analyses span both open-sourced and closed models, leading to empirical results across classic NLP tasks, reasoning, and cultural comprehension. Key findings indicate (1) Most models exhibit varied behavior when given paraphrased instructions. (2) Many models still suffer from exposure bias (e.g., positional bias, majority label bias). (3) For questions rooted in factual, scientific, and commonsense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent. Yet, most models surprisingly demonstrate inconsistent performance on these queries. (4) Multilingu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#65288;SDoH&#65289;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#20020;&#24202;&#25991;&#26412;&#25913;&#36827;&#20102;&#36825;&#20123;&#26497;&#26377;&#20215;&#20540;&#20294;&#24456;&#23569;&#34987;&#35760;&#24405;&#30340;&#20020;&#24202;&#25968;&#25454;&#30340;&#25552;&#21462;&#12290;&#26368;&#20339;&#27169;&#22411;&#20026;&#32463;&#36807;&#24494;&#35843;&#30340;Flan-T5 XL&#21644;Flan-T5 XXL&#65292;&#20854;&#20013;&#23567;&#22411;&#27169;&#22411;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.06354</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#35782;&#21035;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Large Language Models to Identify Social Determinants of Health in Electronic Health Records. (arXiv:2308.06354v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#25552;&#21462;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#65288;SDoH&#65289;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#20020;&#24202;&#25991;&#26412;&#25913;&#36827;&#20102;&#36825;&#20123;&#26497;&#26377;&#20215;&#20540;&#20294;&#24456;&#23569;&#34987;&#35760;&#24405;&#30340;&#20020;&#24202;&#25968;&#25454;&#30340;&#25552;&#21462;&#12290;&#26368;&#20339;&#27169;&#22411;&#20026;&#32463;&#36807;&#24494;&#35843;&#30340;Flan-T5 XL&#21644;Flan-T5 XXL&#65292;&#20854;&#20013;&#23567;&#22411;&#27169;&#22411;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#65288;SDoH&#65289;&#23545;&#24739;&#32773;&#30340;&#32467;&#26524;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#20294;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20013;&#30340;&#25910;&#38598;&#19981;&#23436;&#25972;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;EHR&#20013;&#25552;&#21462;SDoH&#30340;&#33021;&#21147;&#65292;&#24182;&#25506;&#35752;&#20102;&#21512;&#25104;&#20020;&#24202;&#25991;&#26412;&#22312;&#25913;&#36827;&#36825;&#20123;&#23569;&#35265;&#20294;&#26497;&#26377;&#20215;&#20540;&#30340;&#20020;&#24202;&#25968;&#25454;&#25552;&#21462;&#20013;&#30340;&#20316;&#29992;&#12290;&#23545;800&#20221;&#24739;&#32773;&#35760;&#24405;&#36827;&#34892;&#20102;SDoH&#31867;&#21035;&#30340;&#27880;&#37322;&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#20010;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#36824;&#23581;&#35797;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#24182;&#35780;&#20272;&#20102;&#31639;&#27861;&#20559;&#24046;&#12290;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#26159;&#32463;&#36807;&#24494;&#35843;&#30340;Flan-T5 XL&#65288;macro-F1 0.71&#65289;&#29992;&#20110;&#20219;&#20309;SDoH&#65292;&#20197;&#21450;Flan-T5 XXL&#65288;macro-F1 0.70&#65289;&#12290;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#36741;&#21161;&#24494;&#35843;&#30340;&#25928;&#30410;&#22240;&#27169;&#22411;&#26550;&#26500;&#21644;&#22823;&#23567;&#32780;&#24322;&#65292;&#22312;&#36739;&#23567;&#30340;Flan-T5&#27169;&#22411;&#65288;&#22522;&#30784;&#21644;&#22823;&#22411;&#65289;&#20013;&#34920;&#29616;&#20986;&#26368;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#65288;delta F1 +0.12&#21040;+0.23&#65289;&#12290;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social determinants of health (SDoH) have an important impact on patient outcomes but are incompletely collected from the electronic health records (EHR). This study researched the ability of large language models to extract SDoH from free text in EHRs, where they are most commonly documented, and explored the role of synthetic clinical text for improving the extraction of these scarcely documented, yet extremely valuable, clinical data. 800 patient notes were annotated for SDoH categories, and several transformer-based models were evaluated. The study also experimented with synthetic data generation and assessed for algorithmic bias. Our best-performing models were fine-tuned Flan-T5 XL (macro-F1 0.71) for any SDoH, and Flan-T5 XXL (macro-F1 0.70). The benefit of augmenting fine-tuning with synthetic data varied across model architecture and size, with smaller Flan-T5 models (base and large) showing the greatest improvements in performance (delta F1 +0.12 to +0.23). Model performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;GPT-4&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#25512;&#29702;&#65292;&#21457;&#29616;&#20854;&#19982;&#20154;&#31867;&#20043;&#38388;&#22312;&#24847;&#22270;&#24402;&#22240;&#12289;&#22240;&#26524;&#21028;&#26029;&#12289;&#27450;&#39575;&#30340;&#36947;&#24503;&#24615;&#12289;&#36947;&#24503;&#22522;&#30784;&#12289;&#36947;&#24503;&#36816;&#27668;&#23545;&#27861;&#24459;&#21028;&#26029;&#30340;&#24433;&#21709;&#12289;&#21516;&#24847;&#30340;&#27010;&#24565;&#20197;&#21450;&#35268;&#21017;&#36829;&#21453;&#21028;&#26029;&#26041;&#38754;&#23384;&#22312;&#39640;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01264</link><description>&lt;p&gt;
&#25506;&#32034;GPT-4&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#25512;&#29702;&#30340;&#24515;&#29702;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring the psychology of GPT-4's Moral and Legal Reasoning. (arXiv:2308.01264v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;GPT-4&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#25512;&#29702;&#65292;&#21457;&#29616;&#20854;&#19982;&#20154;&#31867;&#20043;&#38388;&#22312;&#24847;&#22270;&#24402;&#22240;&#12289;&#22240;&#26524;&#21028;&#26029;&#12289;&#27450;&#39575;&#30340;&#36947;&#24503;&#24615;&#12289;&#36947;&#24503;&#22522;&#30784;&#12289;&#36947;&#24503;&#36816;&#27668;&#23545;&#27861;&#24459;&#21028;&#26029;&#30340;&#24433;&#21709;&#12289;&#21516;&#24847;&#30340;&#27010;&#24565;&#20197;&#21450;&#35268;&#21017;&#36829;&#21453;&#21028;&#26029;&#26041;&#38754;&#23384;&#22312;&#39640;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#29992;&#20316;&#39640;&#24230;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#22522;&#30784;&#65292;&#33021;&#22815;&#23545;&#27861;&#24459;&#21644;&#36947;&#24503;&#38382;&#39064;&#20316;&#20986;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#22238;&#24212;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#33258;&#36523;&#20869;&#37096;&#24037;&#20316;&#30340;&#25351;&#23548;&#26159;&#19981;&#21487;&#38752;&#30340;&#65292;&#21363;&#20351;&#26159;&#23427;&#20204;&#30340;&#21019;&#24314;&#24037;&#31243;&#22242;&#38431;&#20063;&#26080;&#27861;&#35299;&#37322;&#23427;&#20204;&#22914;&#20309;&#33719;&#24471;&#24403;&#21069;&#25152;&#26377;&#33021;&#21147;&#30340;&#20855;&#20307;&#36807;&#31243;&#12290;&#26426;&#22120;&#24515;&#29702;&#23398;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#26088;&#22312;&#28145;&#20837;&#20102;&#35299;&#36825;&#20123;&#27169;&#22411;&#25317;&#26377;&#30340;&#36807;&#31243;&#21644;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36816;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#26469;&#25506;&#31350;GPT-4&#30340;&#36947;&#24503;&#21644;&#27861;&#24459;&#25512;&#29702;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;GPT-4&#19982;&#20154;&#31867;&#22312;&#24847;&#22270;&#24402;&#22240;&#12289;&#22240;&#26524;&#21028;&#26029;&#12289;&#27450;&#39575;&#30340;&#36947;&#24503;&#24615;&#12289;&#36947;&#24503;&#22522;&#30784;&#12289;&#36947;&#24503;&#36816;&#27668;&#23545;&#27861;&#24459;&#21028;&#26029;&#30340;&#24433;&#21709;&#12289;&#21516;&#24847;&#30340;&#27010;&#24565;&#20197;&#21450;&#35268;&#21017;&#36829;&#21453;&#21028;&#26029;&#26041;&#38754;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#22238;&#31572;&#20043;&#38388;&#23384;&#22312;&#36739;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have been used as the foundation of highly sophisticated artificial intelligences, capable of delivering human-like responses to probes about legal and moral issues. However, these models are unreliable guides to their own inner workings, and even the engineering teams behind their creation are unable to explain exactly how they came to develop all of the capabilities they currently have. The emerging field of machine psychology seeks to gain insight into the processes and concepts that these models possess. In this paper, we employ the methods of psychology to probe into GPT-4's moral and legal reasoning. More specifically, we investigate the similarities and differences between GPT-4 and humans when it comes to intentionality ascriptions, judgments about causation, the morality of deception, moral foundations, the impact of moral luck on legal judgments, the concept of consent, and rule violation judgments. We find high correlations between human and AI response
&lt;/p&gt;</description></item><item><title>DRAGON&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#36890;&#36807;&#35821;&#35328;&#19982;&#29992;&#25143;&#27807;&#36890;&#65292;&#20026;&#35270;&#21147;&#21463;&#25439;&#32773;&#25552;&#20379;&#23548;&#33322;&#21644;&#29615;&#22659;&#25551;&#36848;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2307.06924</link><description>&lt;p&gt;
DRAGON: &#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#24102;&#26377;&#35270;&#35273;&#35821;&#35328;&#20851;&#32852;&#30340;&#36741;&#21161;&#23548;&#33322;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding. (arXiv:2307.06924v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06924
&lt;/p&gt;
&lt;p&gt;
DRAGON&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#24182;&#36890;&#36807;&#35821;&#35328;&#19982;&#29992;&#25143;&#27807;&#36890;&#65292;&#20026;&#35270;&#21147;&#21463;&#25439;&#32773;&#25552;&#20379;&#23548;&#33322;&#21644;&#29615;&#22659;&#25551;&#36848;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#21147;&#21463;&#25439;&#32773;&#22312;&#29702;&#35299;&#21644;&#23548;&#33322;&#21608;&#22260;&#31354;&#38388;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#30446;&#21069;&#30340;&#23548;&#33322;&#25216;&#26415;&#35201;&#20040;&#21482;&#20851;&#27880;&#23548;&#33322;&#65292;&#35201;&#20040;&#25552;&#20379;&#26377;&#38480;&#30340;&#20851;&#20110;&#29615;&#22659;&#30340;&#27807;&#36890;&#12290;&#21463;&#21040;&#26368;&#36817;&#22312;&#35270;&#35273;&#35821;&#35328;&#20851;&#32852;&#21644;&#35821;&#20041;&#23548;&#33322;&#26041;&#38754;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DRAGON&#65292;&#19968;&#31181;&#30001;&#23545;&#35805;&#31995;&#32479;&#39537;&#21160;&#30340;&#23548;&#33322;&#26426;&#22120;&#20154;&#65292;&#24182;&#20855;&#26377;&#23558;&#29615;&#22659;&#19982;&#33258;&#28982;&#35821;&#35328;&#20851;&#32852;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#29702;&#35299;&#29992;&#25143;&#30340;&#25351;&#20196;&#65292;DRAGON&#33021;&#22815;&#24341;&#23548;&#29992;&#25143;&#21040;&#22320;&#22270;&#19978;&#30340;&#30446;&#26631;&#22320;&#26631;&#65292;&#25551;&#36848;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#35266;&#23519;&#22238;&#31572;&#38382;&#39064;&#12290;&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#23545;&#35805;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23558;&#29992;&#25143;&#30340;&#33258;&#30001;&#24418;&#24335;&#25551;&#36848;&#19982;&#29615;&#22659;&#20013;&#30340;&#22320;&#26631;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#21475;&#35821;&#25552;&#20379;&#35821;&#20041;&#20449;&#24687;&#32473;&#29992;&#25143;&#12290;&#25105;&#20204;&#22312;&#26085;&#24120;&#23460;&#20869;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#30450;&#30446;&#21442;&#19982;&#32773;&#30340;&#29992;&#25143;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DRAGON&#33021;&#22815;&#19982;&#29992;&#25143;&#39034;&#30021;&#22320;&#27807;&#36890;&#65292;
&lt;/p&gt;
&lt;p&gt;
Persons with visual impairments (PwVI) have difficulties understanding and navigating spaces around them. Current wayfinding technologies either focus solely on navigation or provide limited communication about the environment. Motivated by recent advances in visual-language grounding and semantic navigation, we propose DRAGON, a guiding robot powered by a dialogue system and the ability to associate the environment with natural language. By understanding the commands from the user, DRAGON is able to guide the user to the desired landmarks on the map, describe the environment, and answer questions from visual observations. Through effective utilization of dialogue, the robot can ground the user's free-form descriptions to landmarks in the environment, and give the user semantic information through spoken language. We conduct a user study with blindfolded participants in an everyday indoor environment. Our results demonstrate that DRAGON is able to communicate with the user smoothly, pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2306.17256</link><description>&lt;p&gt;
&#20197;&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#26681;&#25454;&#29992;&#25143;&#36807;&#21435;&#30340;&#34892;&#20026;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#19982;&#20854;&#20852;&#36259;&#30456;&#31526;&#30340;&#20449;&#24687;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#21382;&#21490;&#20132;&#20114;&#35760;&#24405;&#19981;&#21487;&#29992;&#26102;&#65292;&#24320;&#21457;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#23601;&#26159;&#25152;&#35859;&#30340;&#31995;&#32479;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#12290;&#27492;&#38382;&#39064;&#22312;&#21019;&#19994;&#20225;&#19994;&#25110;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#20013;&#23588;&#20026;&#31361;&#20986;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#29992;&#25143;&#25110;&#29289;&#21697;&#30340;&#20919;&#21551;&#21160;&#22330;&#26223;&#65292;&#20854;&#20013;&#31995;&#32479;&#20173;&#28982;&#36890;&#36807;&#22312;&#21516;&#19968;&#39046;&#22495;&#20013;&#30340;&#21382;&#21490;&#29992;&#25143;&#21644;&#29289;&#21697;&#20132;&#20114;&#36827;&#34892;&#35757;&#32451;&#26469;&#20026;&#26032;&#29992;&#25143;&#25110;&#29289;&#21697;&#25552;&#20379;&#25512;&#33616;&#65292;&#32780;&#26080;&#27861;&#35299;&#20915;&#25105;&#20204;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#36164;&#26009;&#21644;&#29289;&#21697;&#23646;&#24615;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems play a crucial role in helping users discover information that aligns with their interests based on their past behaviors. However, developing personalized recommendation systems becomes challenging when historical records of user-item interactions are unavailable, leading to what is known as the system cold-start recommendation problem. This issue is particularly prominent in start-up businesses or platforms with insufficient user engagement history. Previous studies focus on user or item cold-start scenarios, where systems could make recommendations for new users or items but are still trained with historical user-item interactions in the same domain, which cannot solve our problem. To bridge the gap, our research introduces an innovative and effective approach, capitalizing on the capabilities of pre-trained language models. We transform the recommendation process into sentiment analysis of natural languages containing information of user profiles and item attribu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;MultiMon&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#22810;&#27169;&#24577;&#31995;&#32479;&#20013;&#30340;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;&#25581;&#31034;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;14&#20010;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;&#27599;&#20010;&#37117;&#30001;&#25968;&#30334;&#20010;&#19981;&#21516;&#30340;&#36755;&#20837;&#32452;&#25104;&#65292;&#36825;&#20123;&#36755;&#20837;&#20250;&#23548;&#33268;&#20854;&#20182;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#22833;&#36133;&#12290;</title><link>http://arxiv.org/abs/2306.12105</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#25209;&#37327;&#29983;&#20135;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#22833;&#36133;
&lt;/p&gt;
&lt;p&gt;
Mass-Producing Failures of Multimodal Systems with Language Models. (arXiv:2306.12105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;MultiMon&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#22810;&#27169;&#24577;&#31995;&#32479;&#20013;&#30340;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;&#25581;&#31034;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;14&#20010;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;&#27599;&#20010;&#37117;&#30001;&#25968;&#30334;&#20010;&#19981;&#21516;&#30340;&#36755;&#20837;&#32452;&#25104;&#65292;&#36825;&#20123;&#36755;&#20837;&#20250;&#23548;&#33268;&#20854;&#20182;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#22833;&#36133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#21487;&#33021;&#20197;&#35780;&#20272;&#20154;&#21592;&#26410;&#26366;&#39044;&#35265;&#30340;&#26041;&#24335;&#22833;&#36133;&#12290;&#20026;&#20102;&#22312;&#37096;&#32626;&#21069;&#25214;&#21040;&#36825;&#20123;&#22833;&#36133;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MultiMon&#65292;&#36825;&#26159;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#35782;&#21035;&#31995;&#32479;&#24615;&#22833;&#36133;&#30340;&#31995;&#32479;&#65292;&#33021;&#22815;&#25552;&#20379;&#21487;&#25512;&#24191;&#30340;&#12289;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#27169;&#22411;&#22833;&#36133;&#27169;&#24335;&#30340;&#20363;&#23376;&#12290;&#20026;&#20102;&#25581;&#31034;&#31995;&#32479;&#24615;&#22833;&#36133;&#65292;MultiMon&#20174;&#35821;&#26009;&#24211;&#20013;&#25235;&#21462;&#38169;&#35823;&#21327;&#35758;&#30340;&#31034;&#20363;&#65306;&#36755;&#20837;&#20135;&#29983;&#30456;&#21516;&#30340;&#36755;&#20986;&#65292;&#20294;&#19981;&#24212;&#35813;&#22914;&#27492;&#12290;&#28982;&#21518;&#23427;&#20250;&#28608;&#27963;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-4&#65289;&#26469;&#26597;&#25214;&#31995;&#32479;&#24615;&#22833;&#36133;&#30340;&#27169;&#24335;&#24182;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#23427;&#20204;&#12290;&#25105;&#20204;&#20351;&#29992;MultiMon&#25214;&#21040;&#20102;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;14&#20010;&#31995;&#32479;&#24615;&#22833;&#36133;&#65288;&#20363;&#22914;&#8220;&#24573;&#30053;&#37327;&#35789;&#8221;&#65292;&#27599;&#20010;&#37117;&#30001;&#25968;&#30334;&#20010;&#19981;&#21516;&#30340;&#36755;&#20837;&#32452;&#25104;&#65288;&#20363;&#22914;&#8220;&#19968;&#20010;&#24102;&#26377;&#19968;&#20123;/&#35768;&#22810;&#20070;&#30340;&#20070;&#26550;&#8221;&#65289;&#12290;&#22240;&#20026;CLIP&#26159;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#22522;&#30784;&#65292;&#36825;&#20123;&#36755;&#20837;&#20250;&#23548;&#33268;Midjourney 5.1&#12289;DALL-E&#12289;VideoFusion&#31561;&#31995;&#32479;&#22833;&#36133;&#12290;MultiMon&#20063;&#21487;&#20197;&#25351;&#23548;&#38024;&#23545;&#29305;&#23450;&#29992;&#20363;&#30340;&#30456;&#20851;&#25925;&#38556;&#65292;&#20363;&#22914;&#33258;&#39550;&#36710;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployed multimodal systems can fail in ways that evaluators did not anticipate. In order to find these failures before deployment, we introduce MultiMon, a system that automatically identifies systematic failures -generalizable, natural-language descriptions of patterns of model failures. To uncover systematic failures, MultiMon scrapes a corpus for examples of erroneous agreement: inputs that produce the same output, but should not. It then prompts a language model (e.g., GPT-4) to find systematic patterns of failure and describe them in natural language. We use MultiMon to find 14 systematic failures (e.g., "ignores quantifiers") of the CLIP text-encoder, each comprising hundreds of distinct inputs (e.g., "a shelf with a few/many books"). Because CLIP is the backbone for most state-of-the-art multimodal systems, these inputs produce failures in Midjourney 5.1, DALL-E, VideoFusion, and others. MultiMon can also steer towards failures relevant to specific use cases, such as self-dri
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#25972;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#65292;&#36991;&#20813;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20154;&#20026;&#20559;&#24046;&#27880;&#20837;&#12290;&#36890;&#36807;&#20998;&#32452;&#20844;&#24179;&#24615;&#26816;&#26597;&#27169;&#22411;&#23545;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20102;&#26377;&#36259;&#30340;&#20559;&#24046;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.04735</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36719;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#29992;&#20110;&#35780;&#20272;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Soft-prompt Tuning for Large Language Models to Evaluate Bias. (arXiv:2306.04735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#25972;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#65292;&#36991;&#20813;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20154;&#20026;&#20559;&#24046;&#27880;&#20837;&#12290;&#36890;&#36807;&#20998;&#32452;&#20844;&#24179;&#24615;&#26816;&#26597;&#27169;&#22411;&#23545;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#20102;&#26377;&#36259;&#30340;&#20559;&#24046;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#21151;&#33021;&#22240;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#21363;&#21487;&#20135;&#29983;&#33391;&#22909;&#32467;&#26524;&#32780;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#20197;&#33719;&#24471;&#24341;&#23548;&#26356;&#22909;&#27169;&#22411;&#24615;&#33021;&#30340;&#26368;&#20339;&#25552;&#31034;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#25972;&#26469;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;Open Pre-trained Transformers&#65288;OPT&#65289;&#21644;Galactica&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#24046;&#12290;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#21487;&#33021;&#20559;&#21521;&#26576;&#20123;&#20154;&#32676;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#35782;&#21035;&#36825;&#20123;&#28508;&#22312;&#38382;&#39064;&#38750;&#24120;&#37325;&#35201;&#12290;&#20351;&#29992;&#36719;&#25552;&#31034;&#26469;&#35780;&#20272;&#20559;&#24046;&#32473;&#25105;&#20204;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#20248;&#21183;&#65292;&#21487;&#20197;&#36991;&#20813;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#23548;&#33268;&#30340;&#20154;&#20026;&#20559;&#24046;&#27880;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#20998;&#32452;&#20844;&#24179;&#24615;&#65288;&#20559;&#24046;&#65289;&#26816;&#26597;&#27169;&#22411;&#23545;&#19981;&#21516;&#25935;&#24863;&#23646;&#24615;&#30340;&#20559;&#35265;&#65292;&#24182;&#25214;&#21040;&#20102;&#26377;&#36259;&#30340;&#20559;&#24046;&#27169;&#24335;&#12290;&#30001;&#20110;LLMs&#24050;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#29992;&#20110;&#24037;&#19994;&#20013;&#65292;&#22240;&#27492;&#25105;&#20204;&#23545;&#20854;&#36827;&#34892;&#30340;&#20559;&#35265;&#35780;&#20272;&#20855;&#26377;&#23454;&#38469;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting large language models has gained immense popularity in recent years due to the advantage of producing good results even without the need for labelled data. However, this requires prompt tuning to get optimal prompts that lead to better model performances. In this paper, we explore the use of soft-prompt tuning on sentiment classification task to quantify the biases of large language models (LLMs) such as Open Pre-trained Transformers (OPT) and Galactica language model. Since these models are trained on real-world data that could be prone to bias toward certain groups of populations, it is important to identify these underlying issues. Using soft-prompts to evaluate bias gives us the extra advantage of avoiding the human-bias injection that can be caused by manually designed prompts. We check the model biases on different sensitive attributes using the group fairness (bias) and find interesting bias patterns. Since LLMs have been used in the industry in various applications, i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22810;&#23610;&#24230;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#30701;&#26426;&#22120;&#25991;&#26412;&#26631;&#35760;&#20026;&#8220;&#26410;&#26631;&#35760;&#8221;&#26469;&#37325;&#26032;&#34920;&#36848;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#26816;&#27979;&#24615;&#33021;&#65292;&#26377;&#25928;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.18149</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Multiscale Positive-Unlabeled Detection of AI-Generated Texts. (arXiv:2305.18149v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22810;&#23610;&#24230;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#30701;&#26426;&#22120;&#25991;&#26412;&#26631;&#35760;&#20026;&#8220;&#26410;&#26631;&#35760;&#8221;&#26469;&#37325;&#26032;&#34920;&#36848;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35268;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#20248;&#21270;&#26816;&#27979;&#24615;&#33021;&#65292;&#26377;&#25928;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#24067;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#31561;&#22312;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#25991;&#26412;&#26041;&#38754;&#20196;&#20154;&#24778;&#35766;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#34987;&#29992;&#20110;&#21046;&#36896;&#34394;&#20551;&#30340;&#23398;&#26415;&#25991;&#26412;&#12289;&#34394;&#20551;&#26032;&#38395;&#12289;&#34394;&#20551;&#25512;&#29305;&#31561;&#12290;&#20808;&#21069;&#30340;&#20316;&#21697;&#25552;&#20986;&#20102;&#26816;&#27979;&#36825;&#20123;&#22810;&#23610;&#24230;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;ML&#20998;&#31867;&#22120;&#12289;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35757;&#32451;&#19981;&#21487;&#30693;&#26041;&#27861;&#21644;&#31934;&#35843;&#30340;&#35821;&#35328;&#20998;&#31867;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20027;&#27969;&#26816;&#27979;&#22120;&#22312;&#26500;&#24314;&#26102;&#27809;&#26377;&#32771;&#34385;&#21040;&#25991;&#26412;&#38271;&#24230;&#30340;&#22240;&#32032;&#65306;&#30701;&#25991;&#26412;&#30340;&#32570;&#20047;&#20449;&#24687;&#29305;&#24449;&#65292;&#20351;&#20854;&#26356;&#38590;&#26816;&#27979;&#12290;&#38024;&#23545;&#22810;&#23610;&#24230;&#25991;&#26412;&#26816;&#27979;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#27491;&#36127;&#26679;&#26412;&#65288;MPU&#65289;&#35757;&#32451;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25215;&#35748;&#30701;&#30340;&#26426;&#22120;&#25991;&#26412;&#20855;&#26377;&#31867;&#20154;&#23646;&#24615;&#65292;&#24182;&#23558;&#25991;&#26412;&#20998;&#31867;&#37325;&#26032;&#34920;&#36848;&#20026;&#19968;&#20010;&#27491;&#36127;&#26679;&#26412;&#38382;&#39064;&#65292;&#21363;&#36890;&#36807;&#22312;&#35757;&#32451;&#26399;&#38388;&#26631;&#35760;&#36825;&#20123;&#30701;&#30340;&#26426;&#22120;&#25991;&#26412;&#20026;"unlabeled"&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#27491;&#36127;&#26679;&#26412;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may get misused for fake scholarly texts, fake news, fake tweets, et cetera. Previous works have proposed methods to detect these multiscale AI-generated texts, including simple ML classifiers, pretrained-model-based training-agnostic methods, and finetuned language classification models. However, mainstream detectors are formulated without considering the factor of corpus length: shorter corpuses are harder to detect compared with longer ones for shortage of informative features. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the challenge of multiscale text detection. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase text classification as a Positive-Unlabeled (PU) problem by marking these short machine texts as "unlabeled" during training. In this PU context, we propose the le
&lt;/p&gt;</description></item><item><title>Sophia&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#23545;&#35282;Hessian&#20316;&#20026;&#39044;&#35843;&#33410;&#22120;&#65292;&#24182;&#36827;&#34892;&#20803;&#32032;&#32423;&#21035;&#30340;&#35009;&#21098;&#25511;&#21046;&#26356;&#26032;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.14342</link><description>&lt;p&gt;
Sophia&#65306;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#30340;&#38543;&#26426;&#20108;&#38454;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training. (arXiv:2305.14342v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14342
&lt;/p&gt;
&lt;p&gt;
Sophia&#26159;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#21487;&#25193;&#23637;&#30340;&#20108;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#23545;&#35282;Hessian&#20316;&#20026;&#39044;&#35843;&#33410;&#22120;&#65292;&#24182;&#36827;&#34892;&#20803;&#32032;&#32423;&#21035;&#30340;&#35009;&#21098;&#25511;&#21046;&#26356;&#26032;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#24040;&#22823;&#25104;&#26412;&#65292;&#20248;&#21270;&#31639;&#27861;&#30340;&#24494;&#23567;&#25913;&#36827;&#23558;&#20250;&#22823;&#22823;&#38477;&#20302;&#35757;&#32451;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;Adam&#21450;&#20854;&#21464;&#31181;&#19968;&#30452;&#26159;&#26368;&#20808;&#36827;&#30340;&#65292;&#32780;&#26356;&#22797;&#26434;&#30340;&#20108;&#38454;&#65288;&#22522;&#20110;Hessian&#30340;&#65289;&#20248;&#21270;&#22120;&#24448;&#24448;&#20250;&#24102;&#26469;&#22826;&#22810;&#30340;&#27599;&#27493;&#24320;&#38144;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sophia&#65292;&#19968;&#31181;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#20108;&#38454;&#20248;&#21270;&#22120;&#65292;&#23427;&#20351;&#29992;&#36731;&#37327;&#32423;&#20272;&#35745;&#30340;&#23545;&#35282;Hessian&#20316;&#20026;&#39044;&#35843;&#33410;&#22120;&#12290;&#26356;&#26032;&#27493;&#39588;&#26159;&#26799;&#24230;&#30340;&#31227;&#21160;&#24179;&#22343;&#20540;&#38500;&#20197;&#20272;&#35745;Hessian&#30340;&#31227;&#21160;&#24179;&#22343;&#20540;&#65292;&#28982;&#21518;&#36827;&#34892;&#20803;&#32032;&#32423;&#21035;&#30340;&#35009;&#21098;&#12290;&#35009;&#21098;&#25511;&#21046;&#20102;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#26356;&#26032;&#22823;&#23567;&#65292;&#24182;&#25511;&#21046;&#20102;Hessian&#22312;&#36712;&#36857;&#19978;&#30340;&#38750;&#20984;&#24615;&#21644;&#24555;&#36895;&#21464;&#21270;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;Sophia&#21482;&#22312;&#27599;&#20960;&#27425;&#36845;&#20195;&#20013;&#20272;&#35745;&#23545;&#35282;Hessian&#65292;&#36825;&#20960;&#20046;&#27809;&#26377;&#24179;&#22343;&#27599;&#27493;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#24320;&#38144;&#12290;&#22312;&#20351;&#29992;GPT m&#36827;&#34892;&#35821;&#35328;&#24314;&#27169;&#26102;&#65292;
&lt;/p&gt;
&lt;p&gt;
Given the massive cost of language model pre-training, a non-trivial improvement of the optimization algorithm would lead to a material reduction on the time and cost of training. Adam and its variants have been state-of-the-art for years, and more sophisticated second-order (Hessian-based) optimizers often incur too much per-step overhead. In this paper, we propose Sophia, Second-order Clipped Stochastic Optimization, a simple scalable second-order optimizer that uses a light-weight estimate of the diagonal Hessian as the pre-conditioner. The update is the moving average of the gradients divided by the moving average of the estimated Hessian, followed by element-wise clipping. The clipping controls the worst-case update size and tames the negative impact of non-convexity and rapid change of Hessian along the trajectory. Sophia only estimates the diagonal Hessian every handful of iterations, which has negligible average per-step time and memory overhead. On language modeling with GPT m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21462;&#20195;&#20165;&#21463;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#20174;&#32780;&#28040;&#38500;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#38480;&#21046;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06176</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Language Models with Generative Adversarial Feedback. (arXiv:2305.06176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21462;&#20195;&#20165;&#21463;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#20174;&#32780;&#28040;&#38500;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#38480;&#21046;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#36755;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#30340;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;RLHF&#21463;&#21040;&#20154;&#31867;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#29983;&#20135;&#21147;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;: &#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#20195;&#26367;RLHF&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;&#65292;RLGAF&#21487;&#20197;&#24110;&#21161;&#23545;&#40784;LLM&#30340;&#36755;&#20986;&#65292;&#21516;&#26102;&#19981;&#20250;&#21463;&#21040;RLHF&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#20026;&#36827;&#19968;&#27493;&#33258;&#21160;&#21270;AI&#23545;&#40784;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to significantly enhance the performance of large language models (LLMs) by aligning their outputs with desired human values. However, RLHF is constrained by the expertise and productivity limitations of human evaluators. In this study, we investigate an alternative approach: Reinforcement Learning with Generative Adversarial Feedback (RLGAF) to RLHF. Our preliminary findings indicate that RLGAF can help align LLMs outputs while not suffering from the inherent restrictions of RLHF, suggesting promising avenues for further research on automating AI alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#36827;&#34892;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;&#65292;&#27979;&#35797;&#34920;&#26126;&#36825;&#26679;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#24314;&#65292;&#21363;&#20351;&#23545;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35828;&#23478;&#20063;&#21487;&#20197;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.16618</link><description>&lt;p&gt;
&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations. (arXiv:2303.16618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#36827;&#34892;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;&#65292;&#27979;&#35797;&#34920;&#26126;&#36825;&#26679;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#24314;&#65292;&#21363;&#20351;&#23545;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35828;&#23478;&#20063;&#21487;&#20197;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#20026;&#23545;&#35805;&#25935;&#24863;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#29305;&#23450;&#29305;&#24449;&#30340;&#20154;&#21592;&#21644;/&#25110;&#29305;&#23450;&#29615;&#22659;&#20013;&#30340;&#35828;&#35805;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#20016;&#23500;&#30340;&#35282;&#33394;&#27880;&#37322;&#38590;&#20197;&#24471;&#21040;&#21644;&#25104;&#21151;&#21033;&#29992;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#24182;&#25551;&#36848;&#20102;&#19968;&#32452;&#26032;&#39062;&#30340;&#25163;&#21160;&#27880;&#37322;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27969;&#34892;&#30340; Cornell &#30005;&#24433;&#23545;&#35805;&#35821;&#26009;&#24211;&#30340; 863 &#21517;&#28436;&#35762;&#32773;&#65292;&#21253;&#25324;&#29305;&#24449;&#24341;&#29992;&#21644;&#35282;&#33394;&#25551;&#36848;&#65292;&#20197;&#21450;&#36229;&#36807; 95&#65285; &#30340;&#29305;&#33394;&#30005;&#24433;&#30340;&#19968;&#32452;&#33258;&#21160;&#25552;&#21462;&#30340;&#20845;&#20010;&#20803;&#25968;&#25454;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;&#27492;&#31867;&#27880;&#37322;&#26469;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22256;&#24785;&#20943;&#23569;&#39640;&#36798; 8.5&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29978;&#33267;&#21487;&#20197;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35762;&#32773;&#65292;&#21363;&#23545;&#20110;&#27809;&#26377;&#20808;&#21069;&#22521;&#35757;&#25968;&#25454;&#30340;&#28436;&#35762;&#32773;&#65292;&#20381;&#36182;&#20110;&#35282;&#33394;&#30340;&#20154;&#21475;&#29305;&#24449;&#30340;&#32452;&#21512;&#12290;&#30001;&#20110;&#25910;&#38598;&#27492;&#31867;&#20803;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#39033;&#25104;&#26412;&#25928;&#30410;&#20998;&#26512;&#65292;&#20197;&#31361;&#20986;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
Personalisation of language models for dialogue sensitises them to better capture the speaking patterns of people of specific characteristics, and/or in specific environments. However, rich character annotations are difficult to come by and to successfully leverage. In this work, we release and describe a novel set of manual annotations for 863 speakers from the popular Cornell Movie Dialog Corpus, including features like characteristic quotes and character descriptions, and a set of six automatically extracted metadata for over 95% of the featured films. We perform extensive experiments on two corpora and show that such annotations can be effectively used to personalise language models, reducing perplexity by up to 8.5%. Our method can be applied even zero-shot for speakers for whom no prior training data is available, by relying on combinations of characters' demographic characteristics. Since collecting such metadata is costly, we also contribute a cost-benefit analysis to highlight
&lt;/p&gt;</description></item></channel></rss>