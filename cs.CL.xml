<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18168</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#25991;&#26412;&#20013;&#26082;&#21253;&#21547;&#20102;&#20107;&#23454;&#65292;&#20063;&#21253;&#21547;&#20102;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#36825;&#20123;&#30456;&#20114;&#30683;&#30462;&#30340;&#25968;&#25454;&#20013;&#36776;&#21035;&#30495;&#23454;&#19982;&#34394;&#20551;&#21527;&#65311;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24314;&#27169;&#19981;&#21516;&#20135;&#29983;&#25991;&#26412;&#30340;&#20010;&#20307;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#20551;&#35774;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#26469;&#32858;&#31867;&#30495;&#23454;&#25991;&#26412;&#65306;&#19968;&#32676;&#24456;&#21487;&#33021;&#20135;&#29983;&#30495;&#23454;&#25991;&#26412;&#24182;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#20010;&#20307;&#12290;&#20363;&#22914;&#65292;&#21487;&#20449;&#28304;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#31185;&#23398;&#26399;&#21002;&#36890;&#24120;&#20351;&#29992;&#27491;&#24335;&#30340;&#20889;&#20316;&#39118;&#26684;&#24182;&#25552;&#20986;&#19968;&#33268;&#30340;&#20027;&#24352;&#12290;&#36890;&#36807;&#24314;&#27169;&#36825;&#19968;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#27599;&#20010;&#20010;&#20307;&#29983;&#25104;&#35757;&#32451;&#25991;&#26412;&#30340;&#29305;&#23450;&#19978;&#19979;&#25991;&#20043;&#22806;&#12290;&#20363;&#22914;&#65292;&#27169;&#22411;&#21487;&#20197;&#25512;&#26029;&#20986;&#8220;&#32500;&#22522;&#30334;&#31185;&#8221;&#36825;&#20010;&#20010;&#20307;&#22312;&#8220;&#31185;&#23398;&#8221;&#29983;&#25104;&#30340;&#20027;&#39064;&#19978;&#20250;&#34920;&#29616;&#20986;&#30495;&#23454;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20849;&#20139;&#19968;&#20010;&#20154;&#35774;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20004;&#20010;&#35266;&#23519;&#32467;&#26524;&#20026;&#20154;&#35774;&#20551;&#35774;&#25552;&#20379;&#20102;&#35777;&#25454;&#65306;&#65288;1&#65289;&#25105;&#20204;&#21487;&#20197;&#25506;&#27979;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#21028;&#26029;&#30495;&#23454;&#24615;&#30340;&#33021;&#21147;&#65307;&#65288;2&#65289;&#27169;&#22411;&#21487;&#20197;&#20174;&#30456;&#20851;&#29305;&#24449;&#20013;&#25512;&#27979;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent "Wikipedia" will behave truthfully on topics that were only generated by "Science" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a mod
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2310.17940</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550;&#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Unified Segment-to-Segment Framework for Simultaneous Sequence Generation. (arXiv:2310.17940v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17940
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#26159;&#23454;&#26102;&#22330;&#26223;&#30340;&#20851;&#38190;&#20219;&#21153;&#65292;&#27604;&#22914;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#12289;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#21644;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#65292;&#20854;&#20013;&#30446;&#26631;&#24207;&#21015;&#22312;&#25509;&#25910;&#28304;&#24207;&#21015;&#30340;&#21516;&#26102;&#29983;&#25104;&#12290;&#23454;&#29616;&#39640;&#36136;&#37327;&#29983;&#25104;&#21644;&#20302;&#24310;&#36831;&#30340;&#20851;&#38190;&#22312;&#20110;&#30830;&#23450;&#29983;&#25104;&#30340;&#26368;&#20339;&#26102;&#26426;&#65292;&#36890;&#36807;&#23398;&#20064;&#28304;&#24207;&#21015;&#21644;&#30446;&#26631;&#24207;&#21015;&#20043;&#38388;&#30340;&#26144;&#23556;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#23545;&#28304;-&#30446;&#26631;&#26144;&#23556;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#33021;&#21147;&#65292;&#38459;&#30861;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22312;&#21508;&#31181;&#21516;&#26102;&#20219;&#21153;&#20013;&#30340;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#29255;&#27573;&#21040;&#29255;&#27573;&#26694;&#26550; (Seg2Seg) &#29992;&#20110;&#21516;&#26102;&#24207;&#21015;&#29983;&#25104;&#65292;&#20197;&#33258;&#36866;&#24212;&#21644;&#32479;&#19968;&#30340;&#26041;&#24335;&#23398;&#20064;&#26144;&#23556;&#12290;&#22312;&#21516;&#26102;&#29983;&#25104;&#30340;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#22312;&#31561;&#24453;&#28304;&#29255;&#27573;&#21644;&#29983;&#25104;&#30446;&#26631;&#29255;&#27573;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous sequence generation is a pivotal task for real-time scenarios, such as streaming speech recognition, simultaneous machine translation and simultaneous speech translation, where the target sequence is generated while receiving the source sequence. The crux of achieving high-quality generation with low latency lies in identifying the optimal moments for generating, accomplished by learning a mapping between the source and target sequences. However, existing methods often rely on task-specific heuristics for different sequence types, limiting the model's capacity to adaptively learn the source-target mapping and hindering the exploration of multi-task learning for various simultaneous tasks. In this paper, we propose a unified segment-to-segment framework (Seg2Seg) for simultaneous sequence generation, which learns the mapping in an adaptive and unified manner. During the process of simultaneous generation, the model alternates between waiting for a source segment and generat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;VoIP&#36890;&#20449;&#39046;&#22495;&#20013;&#25506;&#32034;&#20102;&#22768;&#23398;&#36716;&#25442;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#22768;&#23398;&#25351;&#26631;&#65292;&#25581;&#31034;&#20102;&#35821;&#38899;&#22686;&#24378;&#23545;VoIP&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.07161</link><description>&lt;p&gt;
VoIP&#24179;&#21488;&#19978;&#35821;&#38899;&#22686;&#24378;&#30340;&#24515;&#29702;&#22768;&#23398;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms. (arXiv:2310.07161v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;VoIP&#36890;&#20449;&#39046;&#22495;&#20013;&#25506;&#32034;&#20102;&#22768;&#23398;&#36716;&#25442;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#22768;&#23398;&#25351;&#26631;&#65292;&#25581;&#31034;&#20102;&#35821;&#38899;&#22686;&#24378;&#23545;VoIP&#31995;&#32479;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;VoIP&#65288;&#20114;&#32852;&#32593;&#35821;&#38899;&#20256;&#36755;&#21327;&#35758;&#65289;&#36890;&#20449;&#20013;&#65292;&#30001;&#22768;&#23398;&#36716;&#25442;&#24341;&#20837;&#30340;&#22797;&#26434;&#24615;&#38656;&#35201;&#36827;&#34892;&#20005;&#26684;&#30340;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#23545;&#19987;&#26377;&#21457;&#36865;&#31471;&#38477;&#22122;&#25928;&#26524;&#30340;&#25506;&#32034;&#65292;&#23545;Google Meets&#21644;Zoom&#31561;&#24179;&#21488;&#36827;&#34892;&#20102;&#32454;&#33268;&#35780;&#20272;&#12290;&#30740;&#31350;&#21033;&#29992;Deep Noise Suppression&#65288;DNS&#65289;2020&#25968;&#25454;&#38598;&#65292;&#30830;&#20445;&#20102;&#38024;&#23545;&#21508;&#31181;&#38477;&#22122;&#35774;&#32622;&#21644;&#25509;&#25910;&#22120;&#25509;&#21475;&#30340;&#32467;&#26500;&#21270;&#32771;&#23519;&#12290;&#36890;&#36807;&#23558;Oaxaca&#20998;&#35299;&#24341;&#20837;&#21040;&#22768;&#23398;-&#35821;&#38899;&#25200;&#21160;&#20998;&#26512;&#20013;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;&#35770;&#30340;&#21019;&#26032;&#65292;&#35813;&#20998;&#35299;&#36890;&#24120;&#26159;&#32463;&#27982;&#35745;&#37327;&#23398;&#24037;&#20855;&#65292;&#22312;&#27492;&#22788;&#37325;&#26032;&#29992;&#20110;&#20998;&#26512;VoIP&#31995;&#32479;&#20013;&#30340;&#22768;&#23398;-&#35821;&#38899;&#25200;&#21160;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30830;&#23450;&#36825;&#20123;&#36716;&#25442;&#30340;&#24433;&#21709;&#65292;&#21033;&#29992;&#24515;&#29702;&#22768;&#23398;&#25351;&#26631;&#65292;&#29305;&#21035;&#26159;PESQ&#21644;STOI&#65292;&#26469;&#25552;&#20379;&#23545;&#35821;&#38899;&#25913;&#21464;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25152;&#33719;&#24471;&#30340;&#35266;&#28857;&#31361;&#20986;&#26174;&#31034;&#20102;VoIP&#24433;&#21709;&#30340;&#22768;&#23398;&#21160;&#21147;&#23398;&#30340;&#22797;&#26434;&#26223;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
Within the ambit of VoIP (Voice over Internet Protocol) telecommunications, the complexities introduced by acoustic transformations merit rigorous analysis. This research, rooted in the exploration of proprietary sender-side denoising effects, meticulously evaluates platforms such as Google Meets and Zoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset, ensuring a structured examination tailored to various denoising settings and receiver interfaces. A methodological novelty is introduced via the Oaxaca decomposition, traditionally an econometric tool, repurposed herein to analyze acoustic-phonetic perturbations within VoIP systems. To further ground the implications of these transformations, psychoacoustic metrics, specifically PESQ and STOI, were harnessed to furnish a comprehensive understanding of speech alterations. Cumulatively, the insights garnered underscore the intricate landscape of VoIP-influenced acoustic dynamics. In addition to the primary findings, a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26080;&#27861;&#35775;&#38382;&#27169;&#22411;&#26550;&#26500;&#21644;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25805;&#32437;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLMs)&#12290;&#36890;&#36807;&#20248;&#21270;&#36890;&#29992;&#23545;&#25239;&#25552;&#31034;&#19982;&#29992;&#25143;&#26597;&#35810;&#32467;&#21512;&#65292;&#21487;&#20197;&#25200;&#20081;&#34987;&#25915;&#20987;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#23548;&#33268;&#24847;&#22806;&#21644;&#28508;&#22312;&#26377;&#23475;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25581;&#31034;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#21644;&#28431;&#27934;&#65292;&#20026;&#36127;&#36131;&#20219;&#30340;AI&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35786;&#26029;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2309.01446</link><description>&lt;p&gt;
&#24320;&#38376;&#21543;&#65281;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#40657;&#30418;&#30772;&#35299;
&lt;/p&gt;
&lt;p&gt;
Open Sesame! Universal Black Box Jailbreaking of Large Language Models. (arXiv:2309.01446v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36951;&#20256;&#31639;&#27861;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26080;&#27861;&#35775;&#38382;&#27169;&#22411;&#26550;&#26500;&#21644;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25805;&#32437;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLMs)&#12290;&#36890;&#36807;&#20248;&#21270;&#36890;&#29992;&#23545;&#25239;&#25552;&#31034;&#19982;&#29992;&#25143;&#26597;&#35810;&#32467;&#21512;&#65292;&#21487;&#20197;&#25200;&#20081;&#34987;&#25915;&#20987;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#23548;&#33268;&#24847;&#22806;&#21644;&#28508;&#22312;&#26377;&#23475;&#30340;&#36755;&#20986;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25581;&#31034;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#21644;&#28431;&#27934;&#65292;&#20026;&#36127;&#36131;&#20219;&#30340;AI&#24320;&#21457;&#25552;&#20379;&#20102;&#19968;&#31181;&#35786;&#26029;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26088;&#22312;&#25552;&#20379;&#26377;&#24110;&#21161;&#21644;&#23433;&#20840;&#30340;&#22238;&#22797;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#40784;&#25216;&#26415;&#19982;&#29992;&#25143;&#24847;&#22270;&#21644;&#31038;&#20250;&#25351;&#21335;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23545;&#40784;&#21487;&#33021;&#20250;&#34987;&#24694;&#24847;&#34892;&#20026;&#32773;&#21033;&#29992;&#65292;&#20197;&#29992;&#20110;&#24847;&#24819;&#19981;&#21040;&#30340;&#30446;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#36951;&#20256;&#31639;&#27861;&#65288;GA&#65289;&#22312;&#27169;&#22411;&#26550;&#26500;&#21644;&#21442;&#25968;&#19981;&#21487;&#35775;&#38382;&#26102;&#25805;&#32437;LLMs&#12290;GA&#25915;&#20987;&#36890;&#36807;&#20248;&#21270;&#36890;&#29992;&#23545;&#25239;&#25552;&#31034;&#19982;&#29992;&#25143;&#26597;&#35810;&#32467;&#21512;&#65292;&#25200;&#20081;&#34987;&#25915;&#20987;&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#23548;&#33268;&#24847;&#22806;&#21644;&#28508;&#22312;&#26377;&#23475;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#26041;&#27861;&#36890;&#36807;&#25581;&#31034;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#21644;&#28431;&#27934;&#65292;&#31995;&#32479;&#22320;&#25581;&#31034;&#20102;&#20854;&#21709;&#24212;&#19982;&#39044;&#26399;&#34892;&#20026;&#19981;&#31526;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#32780;&#20026;&#20851;&#20110;&#36127;&#36131;&#20219;&#30340;AI&#24320;&#21457;&#30340;&#35752;&#35770;&#25552;&#20379;&#20102;&#19968;&#31181;&#35786;&#26029;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), designed to provide helpful and safe responses, often rely on alignment techniques to align with user intent and social guidelines. Unfortunately, this alignment can be exploited by malicious actors seeking to manipulate an LLM's outputs for unintended purposes. In this paper we introduce a novel approach that employs a genetic algorithm (GA) to manipulate LLMs when model architecture and parameters are inaccessible. The GA attack works by optimizing a universal adversarial prompt that -- when combined with a user's query -- disrupts the attacked model's alignment, resulting in unintended and potentially harmful outputs. Our novel approach systematically reveals a model's limitations and vulnerabilities by uncovering instances where its responses deviate from expected behavior. Through extensive experiments we demonstrate the efficacy of our technique, thus contributing to the ongoing discussion on responsible AI development by providing a diagnostic tool 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#21644;&#38190;&#20540;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#24403;&#30456;&#20851;&#20449;&#24687;&#20301;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#24320;&#22836;&#25110;&#32467;&#23614;&#26102;&#24615;&#33021;&#26368;&#20339;&#65292;&#32780;&#24403;&#27169;&#22411;&#38656;&#35201;&#22312;&#38271;&#25991;&#26412;&#30340;&#20013;&#38388;&#35775;&#38382;&#30456;&#20851;&#20449;&#24687;&#26102;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#19987;&#38376;&#22788;&#29702;&#38271;&#25991;&#26412;&#30340;&#27169;&#22411;&#65292;&#36755;&#20837;&#25991;&#26412;&#36234;&#38271;&#24615;&#33021;&#20063;&#20250;&#22823;&#24133;&#38477;&#20302;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#36755;&#20837;&#25991;&#26412;&#30340;&#19978;&#19979;&#25991;&#25552;&#20379;&#20102;&#26032;&#30340;&#35748;&#35782;&#65292;&#24182;&#19988;&#20026;&#26410;&#26469;&#30340;&#38271;&#25991;&#26412;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.03172</link><description>&lt;p&gt;
&#36855;&#22833;&#22312;&#20013;&#38388;&#65306;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#38271;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Lost in the Middle: How Language Models Use Long Contexts. (arXiv:2307.03172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#21644;&#38190;&#20540;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#24403;&#30456;&#20851;&#20449;&#24687;&#20301;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#24320;&#22836;&#25110;&#32467;&#23614;&#26102;&#24615;&#33021;&#26368;&#20339;&#65292;&#32780;&#24403;&#27169;&#22411;&#38656;&#35201;&#22312;&#38271;&#25991;&#26412;&#30340;&#20013;&#38388;&#35775;&#38382;&#30456;&#20851;&#20449;&#24687;&#26102;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#19987;&#38376;&#22788;&#29702;&#38271;&#25991;&#26412;&#30340;&#27169;&#22411;&#65292;&#36755;&#20837;&#25991;&#26412;&#36234;&#38271;&#24615;&#33021;&#20063;&#20250;&#22823;&#24133;&#38477;&#20302;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#36755;&#20837;&#25991;&#26412;&#30340;&#19978;&#19979;&#25991;&#25552;&#20379;&#20102;&#26032;&#30340;&#35748;&#35782;&#65292;&#24182;&#19988;&#20026;&#26410;&#26469;&#30340;&#38271;&#25991;&#26412;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23558;&#38271;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#65292;&#20294;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#36739;&#38271;&#30340;&#25991;&#26412;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20004;&#20010;&#38656;&#35201;&#22312;&#36755;&#20837;&#25991;&#26412;&#20013;&#35782;&#21035;&#30456;&#20851;&#20449;&#24687;&#30340;&#20219;&#21153;&#65288;&#22810;&#25991;&#26723;&#38382;&#31572;&#21644;&#38190;&#20540;&#26816;&#32034;&#65289;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#30456;&#20851;&#20449;&#24687;&#20986;&#29616;&#22312;&#36755;&#20837;&#25991;&#26412;&#30340;&#24320;&#22836;&#25110;&#32467;&#23614;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#36890;&#24120;&#26368;&#20339;&#65307;&#32780;&#24403;&#27169;&#22411;&#38656;&#35201;&#35775;&#38382;&#38271;&#25991;&#26412;&#20013;&#30340;&#20013;&#38388;&#30456;&#20851;&#20449;&#24687;&#26102;&#65292;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#19987;&#38376;&#22788;&#29702;&#38271;&#25991;&#26412;&#30340;&#27169;&#22411;&#65292;&#24403;&#36755;&#20837;&#25991;&#26412;&#21464;&#24471;&#36234;&#26469;&#36234;&#38271;&#26102;&#65292;&#24615;&#33021;&#20063;&#20250;&#22823;&#24133;&#38477;&#20302;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20026;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#36755;&#20837;&#25991;&#26412;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#38271;&#25991;&#26412;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent language models have the ability to take long contexts as input, relatively little is known about how well the language models use longer context. We analyze language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. We find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. Furthermore, performance substantially decreases as the input context grows longer, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context models.
&lt;/p&gt;</description></item><item><title>LyricWhiz&#26159;&#19968;&#31181;&#40065;&#26834;&#12289;&#22810;&#35821;&#35328;&#12289;&#38646;&#23556;&#20987;&#30340;&#33258;&#21160;&#27468;&#35789;&#36716;&#24405;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Whisper&#20316;&#20026;"&#32819;&#26421;"&#21644;GPT-4&#20316;&#20026;"&#22823;&#33041;"&#65292;&#23427;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#27468;&#35789;&#36716;&#24405;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27468;&#35789;&#36716;&#24405;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.17103</link><description>&lt;p&gt;
LyricWhiz: &#36890;&#36807;&#21521;ChatGPT&#32819;&#35821;&#36827;&#34892;&#40065;&#26834;&#30340;&#22810;&#35821;&#35328;&#38646;&#23556;&#20987;&#27468;&#35789;&#36716;&#24405;
&lt;/p&gt;
&lt;p&gt;
LyricWhiz: Robust Multilingual Zero-shot Lyrics Transcription by Whispering to ChatGPT. (arXiv:2306.17103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17103
&lt;/p&gt;
&lt;p&gt;
LyricWhiz&#26159;&#19968;&#31181;&#40065;&#26834;&#12289;&#22810;&#35821;&#35328;&#12289;&#38646;&#23556;&#20987;&#30340;&#33258;&#21160;&#27468;&#35789;&#36716;&#24405;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Whisper&#20316;&#20026;"&#32819;&#26421;"&#21644;GPT-4&#20316;&#20026;"&#22823;&#33041;"&#65292;&#23427;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#36824;&#23454;&#29616;&#20102;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#27468;&#35789;&#36716;&#24405;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27468;&#35789;&#36716;&#24405;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LyricWhiz&#30340;&#40065;&#26834;&#12289;&#22810;&#35821;&#35328;&#12289;&#38646;&#23556;&#20987;&#30340;&#33258;&#21160;&#27468;&#35789;&#36716;&#24405;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#27468;&#35789;&#36716;&#24405;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27969;&#27966;&#22914;&#25671;&#28378;&#21644;&#37329;&#23646;&#20013;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#30340;&#20840;&#26032;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;Whisper&#65292;&#19968;&#31181;&#24369;&#30417;&#30563;&#30340;&#40065;&#26834;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#20197;&#21450;GPT-4&#65292;&#24403;&#20170;&#26368;&#24615;&#33021;&#21331;&#36234;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;Whisper&#20805;&#24403;&#8220;&#32819;&#26421;&#8221;&#65292;&#36127;&#36131;&#36716;&#24405;&#35821;&#38899;&#65292;&#32780;GPT-4&#21017;&#20316;&#20026;&#8220;&#22823;&#33041;&#8221;&#65292;&#20316;&#20026;&#19968;&#31181;&#20855;&#26377;&#24378;&#22823;&#24615;&#33021;&#30340;&#19978;&#19979;&#25991;&#36755;&#20986;&#36873;&#25321;&#21644;&#26657;&#27491;&#30340;&#27880;&#37322;&#22120;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;LyricWhiz&#22312;&#33521;&#35821;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#35789;&#38169;&#35823;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#26377;&#25928;&#22320;&#36716;&#24405;&#22810;&#31181;&#35821;&#35328;&#30340;&#27468;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;LyricWhiz&#21019;&#24314;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;CC-BY-NC-SA&#29256;&#26435;&#35768;&#21487;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27468;&#35789;&#36716;&#24405;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;MTG-Jamendo&#65292;&#24182;&#25552;&#20379;&#20102;h
&lt;/p&gt;
&lt;p&gt;
We introduce LyricWhiz, a robust, multilingual, and zero-shot automatic lyrics transcription method achieving state-of-the-art performance on various lyrics transcription datasets, even in challenging genres such as rock and metal. Our novel, training-free approach utilizes Whisper, a weakly supervised robust speech recognition model, and GPT-4, today's most performant chat-based large language model. In the proposed method, Whisper functions as the "ear" by transcribing the audio, while GPT-4 serves as the "brain," acting as an annotator with a strong performance for contextualized output selection and correction. Our experiments show that LyricWhiz significantly reduces Word Error Rate compared to existing methods in English and can effectively transcribe lyrics across multiple languages. Furthermore, we use LyricWhiz to create the first publicly available, large-scale, multilingual lyrics transcription dataset with a CC-BY-NC-SA copyright license, based on MTG-Jamendo, and offer a h
&lt;/p&gt;</description></item><item><title>BigTranslate&#26159;&#19968;&#20010;&#22522;&#20110;LLaMA&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21407;&#26377;&#30340;&#22522;&#30784;&#19978;&#36890;&#36807;&#32487;&#32493;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#23454;&#29616;&#20102;&#23545;100&#22810;&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#33021;&#21147;&#65292;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#24615;&#33021;&#25509;&#36817;&#20110;ChatGPT&#21644;&#35895;&#27468;&#32763;&#35793;&#12290;</title><link>http://arxiv.org/abs/2305.18098</link><description>&lt;p&gt;
BigTranslate&#65306;&#36890;&#36807;&#22810;&#35821;&#35328;&#32763;&#35793;&#22686;&#24378;&#36229;&#36807;100&#31181;&#35821;&#35328;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages. (arXiv:2305.18098v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18098
&lt;/p&gt;
&lt;p&gt;
BigTranslate&#26159;&#19968;&#20010;&#22522;&#20110;LLaMA&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#21407;&#26377;&#30340;&#22522;&#30784;&#19978;&#36890;&#36807;&#32487;&#32493;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#23454;&#29616;&#20102;&#23545;100&#22810;&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#32763;&#35793;&#33021;&#21147;&#65292;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#24615;&#33021;&#25509;&#36817;&#20110;ChatGPT&#21644;&#35895;&#27468;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20043;&#38388;&#23637;&#31034;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32763;&#35793;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;LLM&#65292;&#29305;&#21035;&#26159;&#24320;&#28304;&#30340;&#65292;&#27604;&#22914;BLOOM&#21644;LLaMA&#65292;&#37117;&#20197;&#33521;&#35821;&#20026;&#20027;&#23548;&#65292;&#24182;&#19988;&#21482;&#25903;&#25345;&#20960;&#21313;&#31181;&#33258;&#28982;&#35821;&#35328;&#65292;&#20351;&#24471;LLM&#22312;&#35821;&#35328;&#32763;&#35793;&#26041;&#38754;&#30340;&#28508;&#21147;&#19981;&#22826;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BigTranslate&#65292;&#23427;&#37319;&#29992;&#20102;LLaMA&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20165;&#35206;&#30422;20&#31181;&#35821;&#35328;&#65292;&#24182;&#22312;100&#22810;&#31181;&#35821;&#35328;&#19978;&#22686;&#24378;&#20102;&#20854;&#22810;&#35821;&#35328;&#32763;&#35793;&#33021;&#21147;&#12290;BigTranslate&#26159;&#24314;&#31435;&#22312;LLaMA-13B&#20043;&#19978;&#65292;&#24182;&#36890;&#36807;&#19977;&#20010;&#27493;&#39588;&#36827;&#34892;&#20248;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#20013;&#25991;&#21333;&#35821;&#25968;&#25454;&#32487;&#32493;&#35757;&#32451;LLaMA&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#35206;&#30422;102&#31181;&#33258;&#28982;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#24179;&#34892;&#25968;&#25454;&#38598;&#32487;&#32493;&#35757;&#32451;&#27169;&#22411;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#35821;&#35328;&#32763;&#35793;&#25351;&#20196;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#25351;&#23548;&#24494;&#35843;&#65292;&#24471;&#21040;&#20102;&#25105;&#20204;&#30340;BigTranslate&#27169;&#22411;&#12290;&#22810;&#35821;&#35328;&#32763;&#35793;&#30340;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;BigTranslate&#19982;ChatGPT&#21644;&#35895;&#27468;&#32763;&#35793;&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate promising translation performance among various natural languages. However, many LLMs especially the open-sourced ones, such as BLOOM and LLaMA, are English-dominant and support only dozens of natural languages, making the potential of LLMs on language translation less explored. In this work, we present BigTranslate which adapts LLaMA that covers only 20 languages and enhances it with multilingual translation capability on more than 100 languages. BigTranslate is built upon LLaMA-13B and it is optimized in three steps. First, we continue training LLaMA with massive Chinese monolingual data. Second, we continue training the model with a large-scale parallel dataset that covers 102 natural languages. Third, we instruct-tune the foundation model with multilingual translation instructions, leading to our BigTranslate model. The preliminary experiments on multilingual translation show that BigTranslate performs comparably with ChatGPT and Google Tran
&lt;/p&gt;</description></item><item><title>DoReMi&#26041;&#27861;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#35757;&#32451;&#23567;&#22411;&#20195;&#29702;&#27169;&#22411;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65292;&#20877;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#30456;&#27604;&#20351;&#29992;&#40664;&#35748;&#26435;&#37325;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;The Pile&#21644;GLaM&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;6.5%&#21644;4.7%&#30340;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#65292;&#20998;&#21035;&#20351;&#29992;2.6&#20493;&#21644;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.10429</link><description>&lt;p&gt;
DoReMi: &#20248;&#21270;&#25968;&#25454;&#28151;&#21512;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. (arXiv:2305.10429v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10429
&lt;/p&gt;
&lt;p&gt;
DoReMi&#26041;&#27861;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#35757;&#32451;&#23567;&#22411;&#20195;&#29702;&#27169;&#22411;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65292;&#20877;&#20351;&#29992;&#36825;&#20123;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#65292;&#30456;&#27604;&#20351;&#29992;&#40664;&#35748;&#26435;&#37325;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;The Pile&#21644;GLaM&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;6.5%&#21644;4.7%&#30340;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#65292;&#20998;&#21035;&#20351;&#29992;2.6&#20493;&#21644;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#22495;&#30340;&#28151;&#21512;&#27604;&#20363;&#65288;&#20363;&#22914;&#65292;&#32500;&#22522;&#30334;&#31185;&#12289;&#22270;&#20070;&#12289;&#32593;&#39029;&#25991;&#26412;&#65289;&#26497;&#22823;&#22320;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DoReMi&#30340;Domain Reweighting with Minimax Optimization&#26041;&#27861;&#65292;&#23427;&#39318;&#20808;&#20351;&#29992;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#65288;Group DRO&#65289;&#35757;&#32451;&#19968;&#20010;&#23567;&#20195;&#29702;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#22495;&#26435;&#37325;&#65288;&#28151;&#21512;&#27604;&#20363;&#65289;&#65292;&#32780;&#19981;&#38656;&#35201;&#30693;&#36947;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#22495;&#26435;&#37325;&#37325;&#26032;&#37319;&#26679;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#26356;&#22823;&#30340;&#65292;&#20840;&#23610;&#23544;&#30340;&#27169;&#22411;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;DoReMi&#22312;&#19968;&#20010;280M&#21442;&#25968;&#30340;&#20195;&#29702;&#27169;&#22411;&#19978;&#65292;&#26356;&#26377;&#25928;&#22320;&#25214;&#21040;&#35757;&#32451;&#19968;&#20010;8B&#21442;&#25968;&#27169;&#22411;&#65288;30&#20493;&#22823;&#65289;&#30340;&#22495;&#26435;&#37325;&#12290;&#22312;The Pile&#19978;&#65292;&#21363;&#20351;&#22312;&#20943;&#23567;&#19968;&#20123;&#22495;&#30340;&#27604;&#37325;&#26102;&#65292;DoReMi&#20063;&#33021;&#25552;&#39640;&#25152;&#26377;&#22495;&#30340;perplexity&#12290;&#30456;&#27604;&#20351;&#29992;The Pile&#30340;&#40664;&#35748;&#22495;&#26435;&#37325;&#35757;&#32451;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;DoReMi&#23558;&#24179;&#22343;few-shot&#19979;&#28216;&#20934;&#30830;&#24230;&#25552;&#39640;&#20102;6.5%&#65292;&#24182;&#20351;&#29992;2.6&#20493;&#30340;&#35757;&#32451;&#27493;&#39588;&#36798;&#21040;&#22522;&#32447;&#20934;&#30830;&#24230;&#12290;&#22312;GLaM&#25968;&#25454;&#38598;&#19978;&#65292;DoReMi&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;4.7%&#65288;&#27425;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65289;&#30340;few-shot&#20934;&#30830;&#24230;&#65292;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#27493;&#39588;&#19979;&#25552;&#39640;&#20102;9.0%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mixture proportions of pretraining data domains (e.g., Wikipedia, books, web text) greatly affect language model (LM) performance. In this paper, we propose Domain Reweighting with Minimax Optimization (DoReMi), which first trains a small proxy model using group distributionally robust optimization (Group DRO) over domains to produce domain weights (mixture proportions) without knowledge of downstream tasks. We then resample a dataset with these domain weights and train a larger, full-sized model. In our experiments, we use DoReMi on a 280M-parameter proxy model to find domain weights for training an 8B-parameter model (30x larger) more efficiently. On The Pile, DoReMi improves perplexity across all domains, even when it downweights a domain. DoReMi improves average few-shot downstream accuracy by 6.5% over a baseline model trained using The Pile's default domain weights and reaches the baseline accuracy with 2.6x fewer training steps. On the GLaM dataset, DoReMi, which has no know
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#30701;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#34917;&#20805;&#21477;&#23376;&#65292;&#32467;&#21512;&#23545;&#27604;&#23398;&#20064;&#21644;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#35821;&#20041;&#21305;&#37197;&#65292;&#24182;&#20351;&#29992;&#20851;&#38190;&#35789;&#36991;&#20813;&#22122;&#22768;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.03898</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21152;&#24378;&#30693;&#35782;&#30340;&#30701;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The Short Text Matching Model Enhanced with Knowledge via Contrastive Learning. (arXiv:2304.03898v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03898
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#30701;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#65292;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#34917;&#20805;&#21477;&#23376;&#65292;&#32467;&#21512;&#23545;&#27604;&#23398;&#20064;&#21644;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#35821;&#20041;&#21305;&#37197;&#65292;&#24182;&#20351;&#29992;&#20851;&#38190;&#35789;&#36991;&#20813;&#22122;&#22768;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30701;&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#22312;&#24191;&#21578;&#25628;&#32034;&#21644;&#25512;&#33616;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#30001;&#20110;&#25991;&#26412;&#38271;&#24230;&#30701;&#65292;&#35821;&#20041;&#20449;&#24687;&#21294;&#20047;&#21644;&#21333;&#35789;&#27495;&#20041;&#38382;&#39064;&#25104;&#20026;&#27492;&#31867;&#20219;&#21153;&#30340;&#38590;&#28857;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24341;&#20837;&#25991;&#26412;&#34917;&#20805;&#21477;&#23376;&#25110;&#30693;&#35782;&#24211;&#26469;&#25552;&#20379;&#38468;&#21152;&#30340;&#29305;&#24449;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#22320;&#20132;&#20114;&#21407;&#22987;&#21477;&#23376;&#21644;&#34917;&#20805;&#21477;&#23376;&#65292;&#20063;&#27809;&#26377;&#32771;&#34385;&#21040;&#22806;&#37096;&#30693;&#35782;&#24211;&#24341;&#20837;&#30340;&#22122;&#22768;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#23545;&#27604;&#23398;&#20064;&#21644;&#22806;&#37096;&#30693;&#35782;&#30340;&#30701;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#23545;&#24212;&#30340;&#34917;&#20805;&#21477;&#23376;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#25351;&#23548;&#27169;&#22411;&#33719;&#24471;&#26356;&#20855;&#35821;&#20041;&#21305;&#37197;&#24615;&#30340;&#21407;&#22987;&#21477;&#23376;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36991;&#20813;&#22122;&#22768;&#65292;&#25105;&#20204;&#20351;&#29992;&#20851;&#38190;&#35789;&#20316;&#20026;&#21407;&#22987;&#21477;&#23376;&#30340;&#20027;&#35201;&#35821;&#20041;&#36827;&#34892;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, short Text Matching tasks have been widely applied in the fields ofadvertising search and recommendation. The difficulty lies in the lack of semantic information and word ambiguity caused by the short length of the text. Previous works have introduced complement sentences or knowledge bases to provide additional feature information. However, these methods have not fully interacted between the original sentence and the complement sentence, and have not considered the noise issue that may arise from the introduction of external knowledge bases. Therefore, this paper proposes a short Text Matching model that combines contrastive learning and external knowledge. The model uses a generative model to generate corresponding complement sentences and uses the contrastive learning method to guide the model to obtain more semantically meaningful encoding of the original sentence. In addition, to avoid noise, we use keywords as the main semantics of the original sentence to retrie
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;pAbT5&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20026;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#29983;&#25104;&#37197;&#23545;&#25239;&#20307;&#38142;&#24207;&#21015;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#37197;&#23545;&#24773;&#20917;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#23454;&#39564;&#39564;&#35777;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.02748</link><description>&lt;p&gt;
&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#30340;&#37197;&#23545;&#25239;&#20307;&#38142;&#24207;&#21015;&#26465;&#20214;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Conditional Generation of Paired Antibody Chain Sequences through Encoder-Decoder Language Model. (arXiv:2301.02748v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;pAbT5&#65292;&#36825;&#26159;&#19968;&#31181;&#33021;&#22815;&#37319;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#20026;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#29983;&#25104;&#37197;&#23545;&#25239;&#20307;&#38142;&#24207;&#21015;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#37197;&#23545;&#24773;&#20917;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#23454;&#39564;&#39564;&#35777;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#22312;&#24207;&#21015;&#12289;&#32467;&#26500;&#21644;&#21151;&#33021;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#20165;&#38480;&#20110;&#21333;&#19968;&#24207;&#21015;&#30340;&#32534;&#30721;&#22120;&#25110;&#35299;&#30721;&#22120;&#32467;&#26500;&#65292;&#32780;&#35768;&#22810;&#29983;&#29289;&#23398;&#29615;&#22659;&#28041;&#21450;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;pAbT5&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;T5&#30340;&#26550;&#26500;&#23558;&#25239;&#20307;&#38142;&#37197;&#23545;&#24314;&#27169;&#20026;&#27491;&#21521;&#21644;&#21453;&#21521;&#32763;&#35793;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;pAbT5&#36890;&#36807;&#24207;&#21015;&#29983;&#25104;&#20934;&#30830;&#22320;&#21453;&#26144;&#20102;&#38142;&#30340;&#37197;&#23545;&#12290;&#25105;&#20204;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#21487;&#21464;&#38271;&#24230;&#30340;&#24207;&#21015;&#65292;&#20854;&#19979;&#19968;&#20010;&#35789;&#35821;&#30340;&#39044;&#27979;&#27010;&#29575;&#19982;&#24207;&#21015;&#27604;&#23545;&#30340;&#20301;&#32622;&#29305;&#24322;&#24615;&#35780;&#20998;&#30697;&#38453;&#19968;&#33268;&#12290;&#20687;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20854;&#20182;&#30740;&#31350;&#19968;&#26679;&#65292;pAbT5&#22312;&#23454;&#39564;&#39564;&#35777;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#39044;&#27979;&#33021;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;pAbT5&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#34507;&#30333;&#36136;&#30456;&#20114;&#20316;&#29992;&#30340;&#29983;&#25104;&#24335;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein language models (LMs) have been successful in sequence, structural and functional predictions. However, currently, protein LMs are limited to encoder- or decoder-only architectures for single sequences while many biological contexts involve protein-protein interactions. Here, we introduce pAbT5, which models antibody chain pairing as forward- and back-translations using a T5-based architecture. We show that pAbT5 accurately reflects chain pairing through sequence generation. Our protein LM generates variable-length sequences and its next-word prediction probability agrees with position-specific scoring matrix from sequence alignment. Like other works in protein LM, pAbT5 performs state-of-the-art unsupervised prediction on experimental measurements. To the best of our knowledge, pAbT5 is the first generative encoder-decoder protein LM for protein-protein interactions.
&lt;/p&gt;</description></item><item><title>Relphormer&#26159;&#19968;&#31181;&#26032;&#30340;Transformer&#21464;&#20307;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#12290;&#23427;&#24341;&#20837;&#20102;Triple2Seq&#21644;&#22686;&#24378;&#24335;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#35299;&#20915;&#22522;&#26412;Transformer&#26550;&#26500;&#22312;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2205.10852</link><description>&lt;p&gt;
Relphormer&#65306;&#20851;&#31995;&#22270;&#36716;&#25442;&#22120;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Relphormer: Relational Graph Transformer for Knowledge Graph Representations. (arXiv:2205.10852v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10852
&lt;/p&gt;
&lt;p&gt;
Relphormer&#26159;&#19968;&#31181;&#26032;&#30340;Transformer&#21464;&#20307;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#12290;&#23427;&#24341;&#20837;&#20102;Triple2Seq&#21644;&#22686;&#24378;&#24335;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#35299;&#20915;&#22522;&#26412;Transformer&#26550;&#26500;&#22312;&#25429;&#25417;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#24418;&#25366;&#25496;&#31561;&#24191;&#27867;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;remarkable&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#26412;&#30340;Transformer&#26550;&#26500;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#34920;&#31034;&#20013;&#24182;&#27809;&#26377;&#21462;&#24471;&#24456;&#22909;&#30340;&#25913;&#36827;&#65292;&#20854;&#20013;&#24179;&#31227;&#36317;&#31163;&#27169;&#22411;&#25903;&#37197;&#20102;&#36825;&#20010;&#39046;&#22495;&#12290;&#38656;&#27880;&#24847;&#30340;&#26159;&#65292;&#22522;&#26412;&#30340;Transformer&#26550;&#26500;&#38590;&#20197;&#25429;&#25417;&#21040;&#30693;&#35782;&#22270;&#35889;&#30340;&#20869;&#22312;&#24322;&#26500;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#30340;Transformer&#21464;&#20307;&#65292;&#21517;&#20026;Relphormer&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Triple2Seq&#65292;&#21487;&#20197;&#21160;&#24577;&#22320;&#37319;&#26679;&#19978;&#19979;&#25991;&#21270;&#30340;&#23376;&#22270;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;&#65292;&#20197;&#32531;&#35299;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#24335;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#23545;&#20851;&#31995;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#20445;&#25345;&#23454;&#20307;&#21644;&#20851;&#31995;&#20869;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#25513;&#34109;&#24335;&#30693;&#35782;&#24314;&#27169;&#26469;&#23454;&#29616;&#36890;&#29992;&#30340;&#30693;&#35782;&#22270;&#24418;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have achieved remarkable performance in widespread fields, including natural language processing, computer vision and graph mining. However, vanilla Transformer architectures have not yielded promising improvements in the Knowledge Graph (KG) representations, where the translational distance paradigm dominates this area. Note that vanilla Transformer architectures struggle to capture the intrinsically heterogeneous structural and semantic information of knowledge graphs. To this end, we propose a new variant of Transformer for knowledge graph representations dubbed Relphormer. Specifically, we introduce Triple2Seq which can dynamically sample contextualized sub-graph sequences as the input to alleviate the heterogeneity issue. We propose a novel structure-enhanced self-attention mechanism to encode the relational information and keep the semantic information within entities and relations. Moreover, we utilize masked knowledge modeling for general knowledge graph representa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20154;&#31867;&#30699;&#27491;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26631;&#27880;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#25968;&#25454;&#65292;&#25910;&#38598;&#32416;&#38169;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#33267;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#23558;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24230;&#25552;&#21319;&#20102;1.7&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2102.00225</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#30340;&#32416;&#38169;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning From How Humans Correct. (arXiv:2102.00225v14 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.00225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20154;&#31867;&#30699;&#27491;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26631;&#27880;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#25968;&#25454;&#65292;&#25910;&#38598;&#32416;&#38169;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#33267;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#23558;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24230;&#25552;&#21319;&#20102;1.7&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#19968;&#23450;&#25968;&#37327;&#30340;&#22122;&#22768;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25214;&#21040;&#22122;&#22768;&#25968;&#25454;&#24182;&#25163;&#21160;&#37325;&#26032;&#26631;&#27880;&#23427;&#20204;&#65292;&#21516;&#26102;&#25910;&#38598;&#32416;&#38169;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#32416;&#38169;&#20449;&#24687;&#34701;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#20154;&#31867;&#30693;&#36947;&#22914;&#20309;&#32416;&#27491;&#22122;&#22768;&#25968;&#25454;&#65292;&#22240;&#27492;&#32416;&#38169;&#20449;&#24687;&#21487;&#20197;&#27880;&#20837;&#21040;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#22312;&#33258;&#24049;&#30340;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#25163;&#21160;&#26631;&#27880;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#37325;&#26032;&#26631;&#27880;&#20102;&#25105;&#20204;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#25968;&#25454;&#65292;&#20197;&#36866;&#29992;&#20110;&#25105;&#20204;&#30340;&#24037;&#19994;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20998;&#31867;&#20934;&#30830;&#24230;&#20174;91.7%&#25552;&#21319;&#21040;92.5%&#12290;91.7%&#30340;&#20934;&#30830;&#24230;&#26159;&#22312;&#20462;&#27491;&#21518;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;&#23427;&#23558;&#22522;&#32447;&#20934;&#30830;&#24230;&#20174;83.3%&#25552;&#21319;&#21040;91.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
In industry NLP application, our manually labeled data has a certain number of noisy data. We present a simple method to find the noisy data and re-label them manually, meanwhile we collect the correction information. Then we present novel method to incorporate the human correction information into deep learning model. Human know how to correct noisy data. So the correction information can be inject into deep learning model. We do the experiment on our own text classification dataset, which is manually labeled, because we re-label the noisy data in our dataset for our industry application. The experiment result shows that our method improve the classification accuracy from 91.7% to 92.5%. The 91.7% accuracy is trained on the corrected dataset, which improve the baseline from 83.3% to 91.7%.
&lt;/p&gt;</description></item></channel></rss>