<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#21463;&#31070;&#32463;&#31995;&#32479;&#21551;&#21457;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#30340;&#31232;&#30095;&#26041;&#27861;&#65292;&#25506;&#32034;&#31867;&#20284;&#20110;&#29983;&#29289;&#32593;&#32476;&#30340;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#23545;&#21508;&#31181; NLP &#20219;&#21153;&#37117;&#34920;&#29616;&#20986;&#33394;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;-&#19981;&#21487;&#30693;&#31232;&#30095;&#24615;&#26041;&#27861;</title><link>https://arxiv.org/abs/2404.01306</link><description>&lt;p&gt;
NeuroPrune&#65306;&#19968;&#31181;&#21463;&#31070;&#32463;&#31995;&#32479;&#21551;&#21457;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25299;&#25169;&#31232;&#30095;&#35757;&#32451;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21463;&#31070;&#32463;&#31995;&#32479;&#21551;&#21457;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#30340;&#31232;&#30095;&#26041;&#27861;&#65292;&#25506;&#32034;&#31867;&#20284;&#20110;&#29983;&#29289;&#32593;&#32476;&#30340;&#26426;&#21046;&#65292;&#23637;&#31034;&#20102;&#23545;&#21508;&#31181; NLP &#20219;&#21153;&#37117;&#34920;&#29616;&#20986;&#33394;&#21644;&#39640;&#25928;&#30340;&#27169;&#22411;-&#19981;&#21487;&#30693;&#31232;&#30095;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110; Transformer &#30340;&#35821;&#35328;&#27169;&#22411;&#30001;&#20110;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#32780;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#21464;&#24471;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#26114;&#36149;&#30340;&#35757;&#32451;&#20197;&#21450;&#25512;&#29702;&#20173;&#28982;&#26159;&#23427;&#20204;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#22312;&#27169;&#22411;&#26550;&#26500;&#30340;&#21508;&#20010;&#23618;&#27425;&#24378;&#21046;&#24341;&#20837;&#31232;&#30095;&#24615;&#24050;&#34987;&#35777;&#26126;&#26377;&#21161;&#20110;&#35299;&#20915;&#25193;&#23637;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;&#65292;&#20294;&#31232;&#30095;&#24615;&#23545;&#32593;&#32476;&#25299;&#25169;&#30340;&#24433;&#21709;&#20173;&#23384;&#22312;&#26029;&#35010;&#12290;&#21463;&#22823;&#33041;&#31070;&#32463;&#32593;&#32476;&#21551;&#21457;&#65292;&#25105;&#20204;&#36890;&#36807;&#32593;&#32476;&#25299;&#25169;&#30340;&#35270;&#35282;&#25506;&#32034;&#31232;&#30095;&#24615;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#22312;&#29983;&#29289;&#32593;&#32476;&#20013;&#35266;&#23519;&#21040;&#30340;&#26426;&#21046;&#65292;&#22914;&#20248;&#20808;&#38468;&#30528;&#21644;&#20887;&#20313;&#31361;&#35302;&#20462;&#21098;&#65292;&#24182;&#23637;&#31034;&#20102;&#22522;&#20110;&#21407;&#21017;&#30340;&#12289;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#31232;&#30095;&#24615;&#26041;&#27861;&#22312;&#36328;&#36234;&#20998;&#31867;&#65288;&#22914;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65289;&#21644;&#29983;&#25104;&#65288;&#25688;&#35201;&#12289;&#26426;&#22120;&#32763;&#35793;&#65289;&#30340;&#21508;&#31181; NLP &#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#19988;&#39640;&#25928;&#65292;&#23613;&#31649; o
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01306v1 Announce Type: cross  Abstract: Transformer-based Language Models have become ubiquitous in Natural Language Processing (NLP) due to their impressive performance on various tasks. However, expensive training as well as inference remains a significant impediment to their widespread applicability. While enforcing sparsity at various levels of the model architecture has found promise in addressing scaling and efficiency issues, there remains a disconnect between how sparsity affects network topology. Inspired by brain neuronal networks, we explore sparsity approaches through the lens of network topology. Specifically, we exploit mechanisms seen in biological networks, such as preferential attachment and redundant synapse pruning, and show that principled, model-agnostic sparsity approaches are performant and efficient across diverse NLP tasks, spanning both classification (such as natural language inference) and generation (summarization, machine translation), despite o
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.19521</link><description>&lt;p&gt;
&#35299;&#37322;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20013;&#30340;&#20851;&#38190;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19521
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#25152;&#37319;&#29992;&#30340;&#26426;&#21046;&#12290;&#22312;&#38646;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#31867;&#20284;&#8220;&#27861;&#22269;&#30340;&#39318;&#37117;&#26159;&#8221;&#30340;&#25552;&#31034;&#65292;&#29305;&#23450;&#20219;&#21153;&#30340;&#27880;&#24847;&#21147;&#22836;&#20250;&#20174;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20027;&#39064;&#23454;&#20307;&#65292;&#22914;&#8220;&#27861;&#22269;&#8221;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#21518;&#32493;&#30340;MLP&#20197;&#22238;&#24518;&#25152;&#38656;&#30340;&#31572;&#26696;&#65292;&#22914;&#8220;&#24052;&#40654;&#8221;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;MLP&#30340;&#36755;&#20986;&#20998;&#35299;&#20026;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#32452;&#20214;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#36319;&#38543;&#36825;&#20123;&#29305;&#23450;&#20219;&#21153;&#22836;&#30340;MLP&#23618;&#30340;&#21151;&#33021;&#12290;&#22312;&#27531;&#24046;&#27969;&#20013;&#65292;&#23427;&#20250;&#25830;&#38500;&#25110;&#25918;&#22823;&#26469;&#33258;&#21508;&#20010;&#22836;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#23427;&#20250;&#29983;&#25104;&#19968;&#20010;&#32452;&#20214;&#65292;&#23558;&#27531;&#24046;&#27969;&#37325;&#26032;&#23450;&#21521;&#21040;&#39044;&#26399;&#31572;&#26696;&#30340;&#26041;&#21521;&#12290;&#36825;&#20123;&#38646;&#27425;&#26426;&#21046;&#20063;&#36866;&#29992;&#20110;&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#31181;&#24191;&#27867;&#23384;&#22312;&#30340;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19521v1 Announce Type: cross  Abstract: In this paper, we deeply explore the mechanisms employed by Transformer-based language models in factual recall tasks. In zero-shot scenarios, given a prompt like "The capital of France is," task-specific attention heads extract the topic entity, such as "France," from the context and pass it to subsequent MLPs to recall the required answer such as "Paris." We introduce a novel analysis method aimed at decomposing the outputs of the MLP into components understandable by humans. Through this method, we quantify the function of the MLP layer following these task-specific heads. In the residual stream, it either erases or amplifies the information originating from individual heads. Moreover, it generates a component that redirects the residual stream towards the direction of its expected answer. These zero-shot mechanisms are also employed in few-shot scenarios. Additionally, we observed a widely existent anti-overconfidence mechanism in 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#30693;&#35782;&#33976;&#39311;&#21644;&#25552;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#35299;&#20915;&#38544;&#21947;&#26816;&#27979;&#20013;&#30340;&#35821;&#35328;&#35268;&#21017;&#24212;&#29992;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#31034;&#20449;&#24687;&#21644;&#36719;&#26631;&#31614;&#20248;&#21270;&#23398;&#29983;&#27169;&#22411;&#65292;&#25552;&#39640;&#38544;&#21947;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18253</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#21644;&#30693;&#35782;&#33976;&#39311;&#30340;&#38544;&#21947;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MD-PK: Metaphor Detection via Prompt Learning and Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18253
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#30693;&#35782;&#33976;&#39311;&#21644;&#25552;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#35299;&#20915;&#38544;&#21947;&#26816;&#27979;&#20013;&#30340;&#35821;&#35328;&#35268;&#21017;&#24212;&#29992;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#25552;&#31034;&#20449;&#24687;&#21644;&#36719;&#26631;&#31614;&#20248;&#21270;&#23398;&#29983;&#27169;&#22411;&#65292;&#25552;&#39640;&#38544;&#21947;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21947;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#38543;&#22788;&#21487;&#35265;&#65292;&#20294;&#26159;&#26816;&#27979;&#23427;&#20204;&#21364;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#32463;&#24120;&#22240;&#35821;&#35328;&#35268;&#21017;&#24212;&#29992;&#19981;&#24403;&#32780;&#38590;&#20197;&#24212;&#23545;&#65292;&#24182;&#24573;&#35270;&#20102;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;&#30693;&#35782;&#33976;&#39311;&#21644;&#25552;&#31034;&#23398;&#20064;&#24341;&#20837;&#38544;&#21947;&#26816;&#27979;&#20013;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19987;&#20026;&#38544;&#21947;&#26816;&#27979;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#25552;&#31034;&#23398;&#20064;&#27169;&#26495;&#12290;&#36890;&#36807;&#23631;&#34109;&#30446;&#26631;&#35789;&#24182;&#25552;&#20379;&#30456;&#20851;&#25552;&#31034;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#23548;&#27169;&#22411;&#20934;&#30830;&#25512;&#26029;&#36825;&#20123;&#35789;&#30340;&#19978;&#19979;&#25991;&#21547;&#20041;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20943;&#36731;&#20102;&#30446;&#26631;&#35789;&#23383;&#38754;&#21547;&#20041;&#30340;&#24178;&#25200;&#65292;&#36824;&#30830;&#20445;&#20102;&#23545;&#20110;&#38544;&#21947;&#26816;&#27979;&#30340;MIP&#35821;&#35328;&#35268;&#21017;&#30340;&#27491;&#30830;&#21033;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#37197;&#22791;&#20808;&#21069;&#30693;&#35782;&#30340;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#36719;&#26631;&#31614;&#65292;&#24341;&#23548;&#23398;&#29983;&#27169;&#22411;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#36719;&#26631;&#31614;&#30340;&#24341;&#20837;&#31867;&#20284;&#20110;&#26631;&#31614;&#24179;&#28369;&#65292;&#26377;&#21161;&#20110;&#25913;&#21892;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18253v1 Announce Type: new  Abstract: Metaphors are ubiquitous in daily life, yet detecting them poses a significant challenge. Previous approaches often struggled with improper application of language rules and overlooked the issue of data sparsity. To address these challenges, we introduce knowledge distillation and prompt learning into metaphor detection. Specifically, we devise a prompt learning template tailored for the metaphor detection task. By masking target words and providing relevant prompt information, we guide the model to accurately infer the contextual meaning of these words. This approach not only mitigates the interference from the literal meaning of target words but also ensures the proper utilization of MIP language rules for metaphor detection. Moreover, we employ a teacher model equipped with prior knowledge to generate meaningful soft labels, guiding the optimization process of the student model. The inclusion of soft labels, akin to label smoothing, h
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20551;&#26032;&#38395;&#25915;&#20987;&#26041;&#27861;VLPrompt&#65292;&#36890;&#36807;&#28040;&#38500;&#23545;&#39069;&#22806;&#25968;&#25454;&#25910;&#38598;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#22659;&#19968;&#33268;&#24615;&#21644;&#21407;&#22987;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#32553;&#23567;LLM&#29983;&#25104;&#34394;&#20551;&#26032;&#38395;&#30340;&#27450;&#39575;&#21147;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.18249</link><description>&lt;p&gt;
&#25506;&#32034;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#26032;&#38395;&#30340;&#27450;&#39575;&#21147;&#65306;&#23545;&#29616;&#23454;&#19990;&#30028;&#26816;&#27979;&#25361;&#25112;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20551;&#26032;&#38395;&#25915;&#20987;&#26041;&#27861;VLPrompt&#65292;&#36890;&#36807;&#28040;&#38500;&#23545;&#39069;&#22806;&#25968;&#25454;&#25910;&#38598;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#35821;&#22659;&#19968;&#33268;&#24615;&#21644;&#21407;&#22987;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#32553;&#23567;LLM&#29983;&#25104;&#34394;&#20551;&#26032;&#38395;&#30340;&#27450;&#39575;&#21147;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#22312;&#22797;&#26434;&#39046;&#22495;&#22914;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#23588;&#20026;&#21487;&#33021;&#21019;&#36896;&#34394;&#20551;&#26032;&#38395;&#25104;&#20026;&#21487;&#33021;&#12290;&#30740;&#31350;&#31361;&#26174;&#20102;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#26032;&#38395;&#22312;&#26377;&#26080;&#20154;&#31867;&#36741;&#21161;&#30340;&#24773;&#20917;&#19979;&#30340;&#27450;&#39575;&#21147;&#24046;&#36317;&#65292;&#20294;&#23545;&#20110;&#28608;&#21169;&#25216;&#26415;&#30340;&#28508;&#21147;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#28608;&#21169;&#31574;&#30053;&#26159;&#21542;&#33021;&#26377;&#25928;&#22320;&#32553;&#23567;&#36825;&#19968;&#24046;&#36317;&#12290;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#34394;&#20551;&#26032;&#38395;&#25915;&#20987;&#38656;&#35201;&#20154;&#31867;&#24178;&#39044;&#36827;&#34892;&#20449;&#24687;&#25910;&#38598;&#65292;&#36890;&#24120;&#20250;&#24573;&#30053;&#32454;&#33410;&#24182;&#19988;&#26080;&#27861;&#20445;&#25345;&#19978;&#19979;&#25991;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#23041;&#32961;&#31574;&#30053;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26465;&#20214;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#24335;&#25552;&#31034;&#65288;VLPrompt&#65289;&#30340;&#24378;&#20551;&#26032;&#38395;&#25915;&#20987;&#26041;&#27861;&#12290;&#19982;&#24403;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;VLPrompt&#28040;&#38500;&#20102;&#23545;&#39069;&#22806;&#25968;&#25454;&#25910;&#38598;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#35821;&#22659;&#30340;&#36830;&#36143;&#24615;&#24182;&#20445;&#30041;&#20102;&#21407;&#22987;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18249v1 Announce Type: new  Abstract: Recent advancements in Large Language Models (LLMs) have enabled the creation of fake news, particularly in complex fields like healthcare. Studies highlight the gap in the deceptive power of LLM-generated fake news with and without human assistance, yet the potential of prompting techniques has not been fully explored. Thus, this work aims to determine whether prompting strategies can effectively narrow this gap. Current LLM-based fake news attacks require human intervention for information gathering and often miss details and fail to maintain context consistency. Therefore, to better understand threat tactics, we propose a strong fake news attack method called conditional Variational-autoencoder-Like Prompt (VLPrompt). Unlike current methods, VLPrompt eliminates the need for additional data collection while maintaining contextual coherence and preserving the intricacies of the original text. To propel future research on detecting VLPro
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#36807;&#31243;&#25581;&#31034;&#20102;&#29983;&#25104;&#23545;&#25239;&#25552;&#31034;&#20197;&#35823;&#23548;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#24341;&#21457;&#20102;&#23545;&#35813;&#33539;&#24335;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#25285;&#24551;&#12290;</title><link>https://arxiv.org/abs/2403.16432</link><description>&lt;p&gt;
$\textit{LinkPrompt}$: &#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#21644;&#36890;&#29992;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
$\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on Prompt-based Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16432
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#36807;&#31243;&#25581;&#31034;&#20102;&#29983;&#25104;&#23545;&#25239;&#25552;&#31034;&#20197;&#35823;&#23548;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#24341;&#21457;&#20102;&#23545;&#35813;&#33539;&#24335;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt-based learning &#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#35843;&#25972;&#21040;&#19979;&#28216;&#20219;&#21153;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#25552;&#21319;&#20102;&#24615;&#33021;&#22522;&#20934;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20248;&#21270;&#25628;&#32034;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#22266;&#23450;&#30340;&#25552;&#31034;&#27169;&#26495;&#26469;&#24494;&#35843;&#27169;&#22411;&#12290;&#36825;&#31181;&#22522;&#20110;&#25552;&#31034;&#20248;&#21270;&#36807;&#31243;&#23545;PLMs&#30340;&#23398;&#20064;&#20063;&#25581;&#31034;&#20102;&#29983;&#25104;&#23545;&#25239;&#25552;&#31034;&#20197;&#35823;&#23548;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#24341;&#21457;&#20102;&#23545;&#36825;&#19968;&#33539;&#24335;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#25285;&#24551;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#29983;&#25104;&#36890;&#29992;&#23545;&#25239;&#35302;&#21457;&#22120;&#65288;UATs&#65289;&#26469;&#25913;&#21464;&#19981;&#20165;&#30446;&#26631;PLMs&#30340;&#39044;&#27979;&#65292;&#36824;&#26377;&#23545;&#24212;Prompt-based Fine-tuning Models&#65288;PFMs&#65289;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#20316;&#21697;&#20013;&#21457;&#29616;&#30340;UATs&#36890;&#24120;&#26159;&#26080;&#27861;&#38405;&#35835;&#30340;&#20196;&#29260;&#25110;&#23383;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16432v1 Announce Type: cross  Abstract: Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks. Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization. Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm. Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm. However, UATs found in previous works are often unreadable tokens or characters a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#32454;&#31890;&#24230;&#24773;&#24863;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#25991;&#26412;&#20013;&#26816;&#27979;&#24494;&#22937;&#24773;&#24863;&#30340;&#25361;&#25112;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.06108</link><description>&lt;p&gt;
&#22312;&#21253;&#21547;&#25968;&#25454;&#22686;&#24378;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#32454;&#31890;&#24230;&#24773;&#24863;&#26816;&#27979;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#25991;&#26412;&#20013;&#26816;&#27979;&#24494;&#22937;&#24773;&#24863;&#30340;&#25361;&#25112;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;GoEmotions&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#24773;&#24863;&#26816;&#27979;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#22411;&#12289;&#25163;&#21160;&#27880;&#37322;&#30340;&#25991;&#26412;&#24773;&#24863;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35299;&#20915;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#24494;&#22937;&#24773;&#24863;&#30340;&#25361;&#25112;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#30740;&#31350;&#32467;&#26524;&#20026;&#35299;&#20915;&#25991;&#26412;&#24773;&#24863;&#26816;&#27979;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#65292;&#21253;&#25324;&#21512;&#25104;&#35813;&#39046;&#22495;&#21508;&#20010;&#25968;&#25454;&#38598;&#19978;&#26041;&#27861;&#21644;&#24615;&#33021;&#30340;&#32508;&#36848;&#35770;&#25991;&#30340;&#28508;&#22312;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06108v1 Announce Type: cross  Abstract: This paper delves into enhancing the classification performance on the GoEmotions dataset, a large, manually annotated dataset for emotion detection in text. The primary goal of this paper is to address the challenges of detecting subtle emotions in text, a complex issue in Natural Language Processing (NLP) with significant practical applications. The findings offer valuable insights into addressing the challenges of emotion detection in text and suggest directions for future research, including the potential for a survey paper that synthesizes methods and performances across various datasets in this domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SemEval-2024&#20219;&#21153;8&#30340;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#20013;&#37319;&#29992;&#30340;&#21152;&#26435;&#23618;&#24179;&#22343;RoBERTa&#25216;&#26415;&#21450;&#32467;&#26524;</title><link>https://arxiv.org/abs/2402.15873</link><description>&lt;p&gt;
SemEval-2024&#20219;&#21153;8&#65306;&#21152;&#26435;&#23618;&#24179;&#22343;RoBERTa&#29992;&#20110;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SemEval-2024 Task 8: Weighted Layer Averaging RoBERTa for Black-Box Machine-Generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SemEval-2024&#20219;&#21153;8&#30340;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#20013;&#37319;&#29992;&#30340;&#21152;&#26435;&#23618;&#24179;&#22343;RoBERTa&#25216;&#26415;&#21450;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25991;&#20214;&#21253;&#21547;&#20102;&#20316;&#32773;&#25552;&#20132;&#32473;SemEval 2024&#20219;&#21153;8&#20250;&#35758;&#35770;&#25991;&#30340;&#32454;&#33410;&#65306;&#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#40657;&#21283;&#23376;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#23376;&#20219;&#21153;A&#65288;&#21333;&#35821;&#65289;&#21644;B&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22312;&#36825;&#20221;&#25991;&#20214;&#20013;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#29992;&#20110;&#25191;&#34892;&#30456;&#21516;&#20219;&#21153;&#30340;&#25216;&#26415;&#65292;&#20197;&#21450;&#25152;&#33719;&#24471;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15873v1 Announce Type: new  Abstract: This document contains the details of the authors' submission to the proceedings of SemEval 2024's Task 8: Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection Subtask A (monolingual) and B. Detection of machine-generated text is becoming an increasingly important task, with the advent of large language models (LLMs). In this document, we lay out the techniques utilized for performing the same, along with the results obtained.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#23558;&#34920;&#26684;&#36716;&#25991;&#26412;&#26041;&#27861;&#38598;&#25104;&#21040;LLM&#38382;&#31572;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#30340;&#34920;&#26684;&#36716;&#25991;&#26412;&#26041;&#27861;&#23545;QA&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.12869</link><description>&lt;p&gt;
&#25506;&#32034;&#34920;&#26684;&#36716;&#25991;&#26412;&#26041;&#27861;&#23545;&#22686;&#24378;&#22522;&#20110;LLM&#30340;&#38382;&#31572;&#31995;&#32479;&#22312;&#39046;&#22495;&#28151;&#21512;&#25968;&#25454;&#19978;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#23558;&#34920;&#26684;&#36716;&#25991;&#26412;&#26041;&#27861;&#38598;&#25104;&#21040;LLM&#38382;&#31572;&#31995;&#32479;&#20013;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#19981;&#21516;&#30340;&#34920;&#26684;&#36716;&#25991;&#26412;&#26041;&#27861;&#23545;QA&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38382;&#31572;&#31995;&#32479;&#65288;QA&#65289;&#20013;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#36827;&#34892;&#22686;&#24378;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#39046;&#22495;&#25968;&#25454;&#36890;&#24120;&#20197;&#28151;&#21512;&#26684;&#24335;&#23384;&#22312;&#65292;&#21253;&#25324;&#25991;&#26412;&#21644;&#21322;&#32467;&#26500;&#21270;&#34920;&#26684;&#65292;&#23545;&#20449;&#24687;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#34920;&#26684;&#36716;&#25991;&#26412;&#29983;&#25104;&#26159;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#23558;&#28151;&#21512;&#25968;&#25454;&#36716;&#21270;&#20026;&#32479;&#19968;&#25991;&#26412;&#26684;&#24335;&#30340;&#35821;&#26009;&#24211;&#12290;&#23613;&#31649;&#36825;&#31181;&#25216;&#26415;&#24050;&#32463;&#34987;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#27604;&#36739;&#20998;&#26512;&#19981;&#21516;&#34920;&#26684;&#36716;&#25991;&#26412;&#26041;&#27861;&#29983;&#25104;&#30340;&#35821;&#26009;&#24211;&#23545;QA&#31995;&#32479;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20998;&#20004;&#27493;&#35299;&#20915;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#34920;&#26684;&#36716;&#25991;&#26412;&#29983;&#25104;&#21019;&#26032;&#22320;&#38598;&#25104;&#21040;&#22686;&#24378;&#22522;&#20110;LLM&#30340;QA&#31995;&#32479;&#19982;&#39046;&#22495;&#28151;&#21512;&#25968;&#25454;&#26694;&#26550;&#20013;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#22312;&#30495;&#23454;&#24037;&#19994;&#25968;&#25454;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#23545;&#20004;&#31181;&#31867;&#22411;&#30340;QA&#31995;&#32479;&#36827;&#34892;&#20102;&#27979;&#35797;&#65288;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12869v1 Announce Type: new  Abstract: Augmenting Large Language Models (LLMs) for Question Answering (QA) with domain specific data has attracted wide attention. However, domain data often exists in a hybrid format, including text and semi-structured tables, posing challenges for the seamless integration of information. Table-to-Text Generation is a promising solution by facilitating the transformation of hybrid data into a uniformly text-formatted corpus. Although this technique has been widely studied by the NLP community, there is currently no comparative analysis on how corpora generated by different table-to-text methods affect the performance of QA systems. In this paper, we address this research gap in two steps. First, we innovatively integrate table-to-text generation into the framework of enhancing LLM-based QA systems with domain hybrid data. Then, we utilize this framework in real-world industrial data to conduct extensive experiments on two types of QA systems (
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;KG&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#25506;&#32034;&#20026;LLMs&#25552;&#20379;&#30693;&#35782;&#22270;&#35889;&#30693;&#35782;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.11541</link><description>&lt;p&gt;
&#36870;&#21521;&#35748;&#30693;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27604;&#25105;&#20204;&#24819;&#35937;&#30340;&#26356;&#25797;&#38271;&#29702;&#35299;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Counter-intuitive: Large Language Models Can Better Understand Knowledge Graphs Than We Thought
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;KG&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#25506;&#32034;&#20026;LLMs&#25552;&#20379;&#30693;&#35782;&#22270;&#35889;&#30693;&#35782;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#24182;&#20943;&#23569;&#23427;&#20204;&#30340;&#24187;&#35273;&#30340;&#26041;&#27861;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30446;&#21069;&#23545;&#22914;&#20309;&#20351;LLMs&#33021;&#22815;&#21363;&#26102;&#25972;&#21512;KGs&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#25506;&#32034;&#36824;&#19981;&#36275;&#12290;&#26412;&#25991;&#37319;&#29992;&#22797;&#26434;&#38382;&#39064;&#22238;&#31572;&#65288;CQA&#65289;&#20316;&#20026;&#19968;&#39033;&#20219;&#21153;&#65292;&#35780;&#20272;LLM&#29702;&#35299;KG&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;KG&#30693;&#35782;&#27880;&#20837;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65288;&#20174;&#19977;&#20803;&#32452;&#21040;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#65289;&#65292;&#26088;&#22312;&#25506;&#32034;&#20026;LLMs&#25552;&#20379;KG&#30693;&#35782;&#30340;&#26368;&#20339;&#25552;&#31034;&#26041;&#27861;&#65292;&#20174;&#32780;&#22686;&#24378;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11541v1 Announce Type: cross  Abstract: Although the method of enhancing large language models' (LLMs') reasoning ability and reducing their hallucinations through the use of knowledge graphs (KGs) has received widespread attention, the exploration of how to enable LLMs to integrate the structured knowledge in KGs on-the-fly remains inadequate. Researchers often co-train KG embeddings and LLM parameters to equip LLMs with the ability of comprehending KG knowledge. However, this resource-hungry training paradigm significantly increases the model learning cost and is also unsuitable for non-open-source, black-box LLMs. In this paper, we employ complex question answering (CQA) as a task to assess the LLM's ability of comprehending KG knowledge. We conducted a comprehensive comparison of KG knowledge injection methods (from triples to natural language text), aiming to explore the optimal prompting method for supplying KG knowledge to LLMs, thereby enhancing their comprehension o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#24320;&#25918;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#20026;&#20363;&#65292;&#21457;&#24067;&#22312;GitHub&#21644;Hugging Face&#19978;&#20379;&#31038;&#21306;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2401.16640</link><description>&lt;p&gt;
TeenyTinyLlama&#65306;&#22522;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#35757;&#32451;&#30340;&#24320;&#28304;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese. (arXiv:2401.16640v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16640
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#24320;&#25918;&#24335;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#20026;&#20363;&#65292;&#21457;&#24067;&#22312;GitHub&#21644;Hugging Face&#19978;&#20379;&#31038;&#21306;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#30340;&#36827;&#23637;&#36824;&#19981;&#24179;&#34913;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;LLMs&#26159;&#22312;&#20687;&#33521;&#35821;&#36825;&#26679;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#35757;&#32451;&#30340;&#65292;&#20294;&#22810;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#27604;&#21333;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#31245;&#24046;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#22810;&#35821;&#35328;&#22522;&#30784;&#26377;&#26102;&#20250;&#38480;&#21046;&#23427;&#20204;&#20135;&#29983;&#30340;&#21103;&#20135;&#21697;&#65292;&#22914;&#35745;&#31639;&#38656;&#27714;&#21644;&#35768;&#21487;&#21046;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#20026;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#20351;&#29992;&#32780;&#37327;&#36523;&#23450;&#21046;&#30340;&#24320;&#25918;&#24335;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#36807;&#31243;&#12289;&#20854;&#23616;&#38480;&#24615;&#21644;&#20248;&#21183;&#12290;&#36825;&#23601;&#26159;TeenyTinyLlama&#65306;&#20004;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#25991;&#26412;&#29983;&#25104;&#30340;&#32039;&#20945;&#22411;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;GitHub&#21644;Hugging Face&#19978;&#20197;&#23485;&#26494;&#30340;Apache 2.0&#35768;&#21487;&#35777;&#21457;&#24067;&#23427;&#20204;&#65292;&#20379;&#31038;&#21306;&#20351;&#29992;&#21644;&#36827;&#19968;&#27493;&#24320;&#21457;&#12290;&#35814;&#35265;https://github.com/Nkluge-correa/TeenyTinyLlama
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly advanced natural language processing, but their progress has yet to be equal across languages. While most LLMs are trained in high-resource languages like English, multilingual models generally underperform monolingual ones. Additionally, aspects of their multilingual foundation sometimes restrict the byproducts they produce, like computational demands and licensing regimes. In this study, we document the development of open-foundation models tailored for use in low-resource settings, their limitations, and their benefits. This is the TeenyTinyLlama pair: two compact models for Brazilian Portuguese text generation. We release them under the permissive Apache 2.0 license on GitHub and Hugging Face for community use and further development. See https://github.com/Nkluge-correa/TeenyTinyLlama
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#19978;&#65292;LLMs&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#26377;&#38480;&#65292;&#20294;&#22312;&#23569;&#25968;-shot&#25552;&#31034;&#19979;&#26377;&#19968;&#23450;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.11052</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#20013;&#25366;&#25496;&#23454;&#39564;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Mining experimental data from Materials Science literature with Large Language Models. (arXiv:2401.11052v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#26448;&#26009;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25552;&#21462;&#20219;&#21153;&#19978;&#65292;LLMs&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#26377;&#38480;&#65292;&#20294;&#22312;&#23569;&#25968;-shot&#25552;&#31034;&#19979;&#26377;&#19968;&#23450;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#35780;&#20272;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5-Turbo&#12289;GPT-4&#21644;GPT-4-Turbo&#65292;&#22312;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#31185;&#23398;&#25991;&#26723;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#20998;&#26512;&#22797;&#26434;&#30340;&#26448;&#26009;&#34920;&#36798;&#24335;&#65292;&#24378;&#35843;&#21270;&#23398;&#24335;&#30340;&#26631;&#20934;&#21270;&#65292;&#20197;&#35299;&#20915;&#26448;&#26009;&#31185;&#23398;&#20449;&#24687;&#35780;&#20272;&#20013;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#20449;&#24687;&#25552;&#21462;&#30340;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#65306;&#65288;i&#65289;&#30740;&#31350;&#26448;&#26009;&#21644;&#29289;&#29702;&#24615;&#36136;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#65288;ii&#65289;&#36825;&#20123;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#12290;LLMs&#22312;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#19982;&#22522;&#20110;BERT&#26550;&#26500;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#20256;&#32479;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23545;&#20110;NER&#65292;LLMs&#22312;&#38646;-shot&#25552;&#31034;&#19979;&#26080;&#27861;&#36229;&#36234;&#22522;&#20934;&#32447;&#65292;&#24182;&#19988;&#20165;&#22312;&#23569;&#25968;-shot&#25552;&#31034;&#19979;&#26377;&#23569;&#37327;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;...
&lt;/p&gt;
&lt;p&gt;
This study is dedicated to evaluating the capabilities of advanced large language models (LLMs) such as GPT-3.5-Turbo, GPT-4, and GPT-4-Turbo in the extraction of structured information from scientific documents within the field of materials science. We introduce a novel methodology for the comparative analysis of intricate material expressions, emphasising the standardisation of chemical formulas to tackle the complexities inherent in materials science information assessment. To this end, we primarily focus on two critical tasks of information extraction: (i) a named entity recognition (NER) of studied materials and physical properties and (ii) a relation extraction (RE) between these entities. The performance of LLMs in executing these tasks is benchmarked against traditional models based on the BERT architecture and rule-based approaches. For NER, LLMs fail to outperform the baseline with zero-shot prompting and exhibit only limited improvement with few-shot prompting. However, for 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRED&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#39044;&#27979;&#12290;FRED&#21487;&#20197;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#22312;&#25552;&#20379;&#23545;&#25991;&#26412;&#27169;&#22411;&#30340;&#28145;&#20837;&#35265;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01605</link><description>&lt;p&gt;
&#23545;&#20110;&#25991;&#26412;&#39044;&#27979;&#30340;&#24544;&#23454;&#21644;&#31283;&#20581;&#30340;&#26412;&#22320;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Faithful and Robust Local Interpretability for Textual Predictions. (arXiv:2311.01605v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01605
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRED&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#39044;&#27979;&#12290;FRED&#21487;&#20197;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#22312;&#25552;&#20379;&#23545;&#25991;&#26412;&#27169;&#22411;&#30340;&#28145;&#20837;&#35265;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20851;&#38190;&#39046;&#22495;&#20013;&#24471;&#21040;&#20449;&#20219;&#21644;&#37096;&#32626;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#27169;&#22411;&#30340;&#26041;&#27861;&#36890;&#24120;&#22797;&#26434;&#65292;&#24182;&#19988;&#32570;&#20047;&#22362;&#23454;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20063;&#19981;&#33021;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;FRED&#65288;Faithful and Robust Explainer for textual Documents&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#39044;&#27979;&#12290;FRED&#21487;&#20197;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#24403;&#36825;&#20123;&#35789;&#34987;&#31227;&#38500;&#26102;&#23545;&#39044;&#27979;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#27491;&#24335;&#30340;&#23450;&#20041;&#21644;&#23545;&#21487;&#35299;&#37322;&#20998;&#31867;&#22120;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#30830;&#31435;&#20102;FRED&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;FRED&#22312;&#25552;&#20379;&#23545;&#25991;&#26412;&#27169;&#22411;&#30340;&#28145;&#20837;&#35265;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability is essential for machine learning models to be trusted and deployed in critical domains. However, existing methods for interpreting text models are often complex, lack solid mathematical foundations, and their performance is not guaranteed. In this paper, we propose FRED (Faithful and Robust Explainer for textual Documents), a novel method for interpreting predictions over text. FRED identifies key words in a document that significantly impact the prediction when removed. We establish the reliability of FRED through formal definitions and theoretical analyses on interpretable classifiers. Additionally, our empirical evaluation against state-of-the-art methods demonstrates the effectiveness of FRED in providing insights into text models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#24863;&#30693;&#30340;&#28145;&#20266;&#25991;&#26412;&#20316;&#32773;&#35782;&#21035;&#26041;&#27861;TopRoBERTa&#65292;&#36890;&#36807;&#25429;&#25417;&#28145;&#20266;&#25991;&#26412;&#20013;&#30340;&#26356;&#22810;&#35821;&#35328;&#27169;&#24335;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#20316;&#32773;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.12934</link><description>&lt;p&gt;
TopRoBERTa&#65306;&#25299;&#25169;&#24863;&#30693;&#30340;&#28145;&#20266;&#25991;&#26412;&#20316;&#32773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
TopRoBERTa: Topology-Aware Authorship Attribution of Deepfake Texts. (arXiv:2309.12934v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25299;&#25169;&#24863;&#30693;&#30340;&#28145;&#20266;&#25991;&#26412;&#20316;&#32773;&#35782;&#21035;&#26041;&#27861;TopRoBERTa&#65292;&#36890;&#36807;&#25429;&#25417;&#28145;&#20266;&#25991;&#26412;&#20013;&#30340;&#26356;&#22810;&#35821;&#35328;&#27169;&#24335;&#65292;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#20316;&#32773;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#29983;&#25104;&#24320;&#25918;&#24615;&#12289;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#20123;&#25991;&#26412;&#24456;&#38590;&#19982;&#20154;&#31867;&#20889;&#20316;&#30340;&#25991;&#26412;&#21306;&#20998;&#24320;&#26469;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#31216;&#20026;&#8220;&#28145;&#20266;&#25991;&#26412;&#8221;&#12290;&#30446;&#21069;&#65292;huggingface&#27169;&#22411;&#23384;&#20648;&#24211;&#20013;&#26377;&#36229;&#36807;11K&#20010;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#24694;&#24847;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#20351;&#29992;&#36825;&#20123;&#24320;&#28304;&#30340;LLM&#29983;&#25104;&#22823;&#35268;&#27169;&#30340;&#26377;&#23475;&#25991;&#26412;&#21644;&#34394;&#20551;&#20449;&#24687;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24076;&#26395;&#26377;&#19968;&#31181;&#35745;&#31639;&#26041;&#27861;&#33021;&#22815;&#30830;&#23450;&#32473;&#23450;&#30340;&#25991;&#26412;&#26159;&#21542;&#20026;&#28145;&#20266;&#25991;&#26412;&#65292;&#21363;&#36890;&#36807;&#22270;&#28789;&#27979;&#35797;&#65288;TT&#65289;&#26469;&#21028;&#26029;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#26356;&#19968;&#33324;&#29256;&#26412;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#22810;&#31867;&#21035;&#35774;&#32622;&#19979;&#30340;&#8220;&#20316;&#32773;&#35782;&#21035;&#65288;AA&#65289;&#8221;&#65292;&#21363;&#19981;&#20165;&#30830;&#23450;&#32473;&#23450;&#30340;&#25991;&#26412;&#26159;&#21542;&#20026;&#28145;&#20266;&#25991;&#26412;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#30830;&#23450;&#21738;&#20010;LLM&#26159;&#20316;&#32773;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TopRoBERTa&#65292;&#36890;&#36807;&#21253;&#21547;&#26356;&#22810;&#28145;&#20266;&#25991;&#26412;&#20013;&#30340;&#35821;&#35328;&#27169;&#24335;&#26469;&#25913;&#36827;&#29616;&#26377;&#30340;AA&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in Large Language Models (LLMs) have enabled the generation of open-ended high-quality texts, that are non-trivial to distinguish from human-written texts. We refer to such LLM-generated texts as \emph{deepfake texts}. There are currently over 11K text generation models in the huggingface model repo. As such, users with malicious intent can easily use these open-sourced LLMs to generate harmful texts and misinformation at scale. To mitigate this problem, a computational method to determine if a given text is a deepfake text or not is desired--i.e., Turing Test (TT). In particular, in this work, we investigate the more general version of the problem, known as \emph{Authorship Attribution (AA)}, in a multi-class setting--i.e., not only determining if a given text is a deepfake text or not but also being able to pinpoint which LLM is the author. We propose \textbf{TopRoBERTa} to improve existing AA solutions by capturing more linguistic patterns in deepfake texts by includ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#32452;&#19987;&#38376;&#38024;&#23545;&#20420;&#35821;&#30340;&#39044;&#35757;&#32451;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;&#32534;&#30721;&#22120;&#12289;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20420;&#35821;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24076;&#26395;&#33021;&#22815;&#25512;&#21160;&#20420;&#35821;&#39046;&#22495;&#30340;NLP&#30740;&#31350;&#21644;&#24037;&#19994;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.10931</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#20420;&#35821;&#30340;&#39044;&#35757;&#32451;Transformer&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;
&lt;/p&gt;
&lt;p&gt;
A Family of Pretrained Transformer Language Models for Russian. (arXiv:2309.10931v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#32452;&#19987;&#38376;&#38024;&#23545;&#20420;&#35821;&#30340;&#39044;&#35757;&#32451;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;&#32534;&#30721;&#22120;&#12289;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20420;&#35821;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24076;&#26395;&#33021;&#22815;&#25512;&#21160;&#20420;&#35821;&#39046;&#22495;&#30340;NLP&#30740;&#31350;&#21644;&#24037;&#19994;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;Transformer&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#26041;&#27861;&#21644;&#24212;&#29992;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#19987;&#38376;&#38024;&#23545;&#20420;&#35821;&#30340;&#36825;&#31181;&#27169;&#22411;&#30340;&#21457;&#23637;&#21364;&#21463;&#21040;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#32452;&#22522;&#20110;&#32534;&#30721;&#22120;&#65288;ruBERT, ruRoBERTa, ruELECTRA&#65289;&#12289;&#35299;&#30721;&#22120;&#65288;ruGPT-3&#65289;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;ruT5, FRED-T5&#65289;&#27169;&#22411;&#30340;13&#20010;&#20420;&#35821;Transformer LMs&#65292;&#20855;&#26377;&#22810;&#31181;&#23610;&#23544;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#36890;&#36807;HuggingFace&#24179;&#21488;&#36731;&#26494;&#33719;&#21462;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#21644;&#39044;&#35757;&#32451;&#30340;&#25253;&#21578;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#20420;&#35821;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#20197;&#21450;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#21457;&#24067;&#36825;&#20123;&#19987;&#38376;&#30340;Transformer LMs&#65292;&#25105;&#20204;&#24076;&#26395;&#25299;&#23485;NLP&#30740;&#31350;&#26041;&#21521;&#30340;&#33539;&#22260;&#65292;&#24182;&#20419;&#36827;&#38024;&#23545;&#20420;&#35821;&#30340;&#24037;&#19994;&#35299;&#20915;&#26041;&#26696;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, Transformer language models (LMs) represent a fundamental component of the NLP research methodologies and applications. However, the development of such models specifically for the Russian language has received little attention. This paper presents a collection of 13 Russian Transformer LMs based on the encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder (ruT5, FRED-T5) models in multiple sizes. Access to these models is readily available via the HuggingFace platform. We provide a report of the model architecture design and pretraining, and the results of evaluating their generalization abilities on Russian natural language understanding and generation datasets and benchmarks. By pretraining and releasing these specialized Transformer LMs, we hope to broaden the scope of the NLP research directions and enable the development of industrial solutions for the Russian language.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AV2Wav&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#36830;&#32493;&#33258;&#30417;&#30563;&#29305;&#24449;&#21644;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24178;&#20928;&#30340;&#35821;&#38899;&#65292;&#20811;&#26381;&#20102;&#29616;&#23454;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#19982;&#22522;&#20110;&#25513;&#34109;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22768;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#36827;&#19968;&#27493;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08030</link><description>&lt;p&gt;
AV2Wav&#65306;&#22522;&#20110;&#36830;&#32493;&#33258;&#30417;&#30563;&#29305;&#24449;&#30340;&#25193;&#25955;&#37325;&#21512;&#25104;&#25216;&#26415;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised Features for Audio-Visual Speech Enhancement. (arXiv:2309.08030v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AV2Wav&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#36830;&#32493;&#33258;&#30417;&#30563;&#29305;&#24449;&#21644;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24178;&#20928;&#30340;&#35821;&#38899;&#65292;&#20811;&#26381;&#20102;&#29616;&#23454;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#19982;&#22522;&#20110;&#25513;&#34109;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22768;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#36827;&#19968;&#27493;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#36890;&#24120;&#20351;&#29992;&#24178;&#20928;&#21644;&#22122;&#22768;&#35821;&#38899;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#20013;&#65292;&#24178;&#20928;&#30340;&#25968;&#25454;&#19981;&#22815;&#22810;&#65307;&#22823;&#22810;&#25968;&#38899;&#39057;-&#35270;&#35273;&#25968;&#25454;&#38598;&#37117;&#26159;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#25910;&#38598;&#30340;&#65292;&#21253;&#21547;&#32972;&#26223;&#22122;&#22768;&#21644;&#28151;&#21709;&#65292;&#36825;&#38459;&#30861;&#20102;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AV2Wav&#65292;&#19968;&#31181;&#22522;&#20110;&#37325;&#21512;&#25104;&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#19979;&#29983;&#25104;&#24178;&#20928;&#30340;&#35821;&#38899;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#36136;&#37327;&#20272;&#35745;&#22120;&#20174;&#38899;&#39057;-&#35270;&#35273;&#35821;&#26009;&#24211;&#20013;&#33719;&#21462;&#20960;&#20046;&#24178;&#20928;&#30340;&#35821;&#38899;&#23376;&#38598;&#65292;&#24182;&#22312;&#27492;&#23376;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#26469;&#33258;AV-HuBERT&#30340;&#36830;&#32493;&#35821;&#38899;&#34920;&#31034;&#29983;&#25104;&#22768;&#27874;&#24418;&#65292;&#20855;&#26377;&#22122;&#22768;&#40065;&#26834;&#35757;&#32451;&#12290;&#25105;&#20204;&#20351;&#29992;&#36830;&#32493;&#32780;&#19981;&#26159;&#31163;&#25955;&#34920;&#31034;&#26469;&#20445;&#30041;&#38901;&#24459;&#21644;&#35828;&#35805;&#32773;&#20449;&#24687;&#12290;&#20165;&#20165;&#36890;&#36807;&#22768;&#30721;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#23601;&#27604;&#22522;&#20110;&#25513;&#34109;&#30340;&#22522;&#32447;&#26356;&#22909;&#22320;&#25191;&#34892;&#35821;&#38899;&#22686;&#24378;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;fine-tune&#27169;&#22411;&#65292;&#20197;&#36716;&#21270;&#20026;&#22312;&#22810;&#20219;&#21153;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#32852;&#21512;&#22810;&#24103;&#22768;&#23398;&#21040;&#35821;&#38899;&#36716;&#21270;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech enhancement systems are typically trained using pairs of clean and noisy speech. In audio-visual speech enhancement (AVSE), there is not as much ground-truth clean data available; most audio-visual datasets are collected in real-world environments with background noise and reverberation, hampering the development of AVSE. In this work, we introduce AV2Wav, a resynthesis-based audio-visual speech enhancement approach that can generate clean speech despite the challenges of real-world training data. We obtain a subset of nearly clean speech from an audio-visual corpus using a neural quality estimator, and then train a diffusion model on this subset to generate waveforms conditioned on continuous speech representations from AV-HuBERT with noise-robust training. We use continuous rather than discrete representations to retain prosody and speaker information. With this vocoding task alone, the model can perform speech enhancement better than a masking-based baseline. We further fine-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#30340;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;LRNN&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LRNN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37197;&#22791;&#20102;&#19968;&#20010;&#22359;&#23545;&#35282;&#19988;&#36755;&#20837;&#30456;&#20851;&#30340;&#36716;&#31227;&#30697;&#38453;&#65292;&#24182;&#19988;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22312;&#27491;&#21017;&#35821;&#35328;&#20219;&#21153;&#19978;&#36827;&#34892;&#38271;&#24230;&#22806;&#25512;&#30340;LRNN&#12290;</title><link>http://arxiv.org/abs/2309.07412</link><description>&lt;p&gt;
&#22312;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20013;&#25512;&#36827;&#27491;&#21017;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Advancing Regular Language Reasoning in Linear Recurrent Neural Networks. (arXiv:2309.07412v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07412
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#30340;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;LRNN&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LRNN&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#37197;&#22791;&#20102;&#19968;&#20010;&#22359;&#23545;&#35282;&#19988;&#36755;&#20837;&#30456;&#20851;&#30340;&#36716;&#31227;&#30697;&#38453;&#65292;&#24182;&#19988;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22312;&#27491;&#21017;&#35821;&#35328;&#20219;&#21153;&#19978;&#36827;&#34892;&#38271;&#24230;&#22806;&#25512;&#30340;LRNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#32447;&#24615;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;LRNN&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#24314;&#27169;&#21644;&#38271;&#31243;&#24314;&#27169;&#20013;&#21462;&#24471;&#20102;&#19982;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#24555;&#36895;&#30340;&#24182;&#34892;&#35757;&#32451;&#21644;&#24658;&#23450;&#30340;&#25512;&#29702;&#25104;&#26412;&#12290;&#22312;&#23545;LRNN&#37325;&#26032;&#20135;&#29983;&#20852;&#36259;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#35757;&#32451;&#24207;&#21015;&#20013;&#30340;&#38544;&#34255;&#35268;&#21017;&#65292;&#20363;&#22914;&#27491;&#21017;&#35821;&#35328;&#30340;&#35821;&#27861;&#32467;&#26500;&#12290;&#25105;&#20204;&#23545;&#19968;&#20123;&#24050;&#26377;&#30340;LRNN&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#27491;&#21017;&#35821;&#35328;&#19978;&#23384;&#22312;&#38480;&#21046;&#12290;&#22312;&#36825;&#20010;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LRNN&#65292;&#37197;&#22791;&#20102;&#19968;&#20010;&#22359;&#23545;&#35282;&#21644;&#36755;&#20837;&#30456;&#20851;&#30340;&#36716;&#31227;&#30697;&#38453;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#26159;&#21807;&#19968;&#19968;&#20010;&#33021;&#22815;&#22312;&#27491;&#21017;&#35821;&#35328;&#20219;&#21153;&#65288;&#22914;&#27714;&#21644;&#12289;&#20598;&#25968;&#23545;&#12289;&#27169;&#36816;&#31639;&#31561;&#65289;&#19978;&#36827;&#34892;&#38271;&#24230;&#22806;&#25512;&#30340;LRNN&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent studies, linear recurrent neural networks (LRNNs) have achieved Transformer-level performance in natural language modeling and long-range modeling while offering rapid parallel training and constant inference costs. With the resurged interest in LRNNs, we study whether they can learn the hidden rules in training sequences, such as the grammatical structures of regular language. We theoretically analyze some existing LRNNs and discover their limitations on regular language. Motivated by the analysis, we propose a new LRNN equipped with a block-diagonal and input-dependent transition matrix. Experiments suggest that the proposed model is the only LRNN that can perform length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.
&lt;/p&gt;</description></item></channel></rss>