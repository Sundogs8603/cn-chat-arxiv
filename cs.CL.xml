<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#27880;&#24847;&#21147;&#20027;&#39064;&#27491;&#21017;&#21270;&#22120;&#65292;&#29992;&#20110;&#22686;&#24378;&#24773;&#24863;&#35782;&#21035;&#22120;&#22312;&#22788;&#29702;&#23545;&#35805;&#20013;&#30340;&#23616;&#37096;&#19978;&#19979;&#25991;&#26102;&#33719;&#24471;&#24773;&#24863;&#30456;&#20851;&#30340;&#20840;&#23616;&#35270;&#35282;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.12221</link><description>&lt;p&gt;
FATRER: &#29992;&#20110;&#20934;&#30830;&#21644;&#31283;&#20581;&#30340;&#20250;&#35805;&#24773;&#24863;&#35782;&#21035;&#30340;&#20840;&#27880;&#24847;&#21147;&#20027;&#39064;&#27491;&#21017;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
FATRER: Full-Attention Topic Regularizer for Accurate and Robust Conversational Emotion Recognition. (arXiv:2307.12221v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#27880;&#24847;&#21147;&#20027;&#39064;&#27491;&#21017;&#21270;&#22120;&#65292;&#29992;&#20110;&#22686;&#24378;&#24773;&#24863;&#35782;&#21035;&#22120;&#22312;&#22788;&#29702;&#23545;&#35805;&#20013;&#30340;&#23616;&#37096;&#19978;&#19979;&#25991;&#26102;&#33719;&#24471;&#24773;&#24863;&#30456;&#20851;&#30340;&#20840;&#23616;&#35270;&#35282;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#29702;&#35299;&#20250;&#35805;&#35805;&#35821;&#20013;&#24341;&#21457;&#30340;&#23545;&#35805;&#32773;&#24773;&#32490;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#26356;&#20934;&#30830;&#30340;&#24773;&#24863;&#39044;&#27979;&#65292;&#32780;&#24573;&#35270;&#20102;&#24403;&#23616;&#37096;&#19978;&#19979;&#25991;&#34987;&#23545;&#25239;&#24615;&#25915;&#20987;&#30772;&#22351;&#26102;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#20026;&#20102;&#22312;&#20445;&#35777;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#32500;&#25345;&#31283;&#20581;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#20840;&#27880;&#24847;&#21147;&#20027;&#39064;&#27491;&#21017;&#21270;&#22120;&#22686;&#24378;&#30340;&#24773;&#24863;&#35782;&#21035;&#22120;&#65292;&#22312;&#24314;&#27169;&#23545;&#35805;&#20013;&#30340;&#23616;&#37096;&#19978;&#19979;&#25991;&#26102;&#23454;&#29616;&#24773;&#24863;&#30456;&#20851;&#30340;&#20840;&#23616;&#35270;&#35282;&#12290;&#24341;&#20837;&#32852;&#21512;&#20027;&#39064;&#24314;&#27169;&#31574;&#30053;&#65292;&#20174;&#34920;&#31034;&#21644;&#25439;&#22833;&#30340;&#35282;&#24230;&#23454;&#29616;&#27491;&#21017;&#21270;&#12290;&#20026;&#20102;&#36991;&#20813;&#36807;&#24230;&#27491;&#21017;&#21270;&#65292;&#25105;&#20204;&#25918;&#24323;&#20102;&#20256;&#32479;&#20027;&#39064;&#24314;&#27169;&#20013;&#23384;&#22312;&#30340;&#20851;&#20110;&#20808;&#39564;&#20998;&#24067;&#30340;&#38480;&#21046;&#65292;&#24182;&#23436;&#20840;&#20381;&#38752;&#27880;&#24847;&#21147;&#23545;&#40784;&#36827;&#34892;&#27010;&#29575;&#36817;&#20284;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#33719;&#24471;&#20102;&#26356;&#26377;&#21033;&#30340;&#32467;&#26524;&#65292;&#24182;&#22312;&#19977;&#31181;&#31867;&#22411;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#33719;&#24471;&#20102;&#20196;&#20154;&#20449;&#26381;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper concentrates on the understanding of interlocutors' emotions evoked in conversational utterances. Previous studies in this literature mainly focus on more accurate emotional predictions, while ignoring model robustness when the local context is corrupted by adversarial attacks. To maintain robustness while ensuring accuracy, we propose an emotion recognizer augmented by a full-attention topic regularizer, which enables an emotion-related global view when modeling the local context in a conversation. A joint topic modeling strategy is introduced to implement regularization from both representation and loss perspectives. To avoid over-regularization, we drop the constraints on prior distributions that exist in traditional topic modeling and perform probabilistic approximations based entirely on attention alignment. Experiments show that our models obtain more favorable results than state-of-the-art models, and gain convincing robustness under three types of adversarial attacks
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20219;&#21153;&#65292;&#22312;&#19981;&#21516;&#20307;&#35009;&#19979;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#21147;&#65292;&#23613;&#31649;&#22312;&#21306;&#20998;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.12166</link><description>&lt;p&gt;
&#27169;&#20223;&#28216;&#25103;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#26816;&#27979;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
The Imitation Game: Detecting Human and AI-Generated Texts in the Era of Large Language Models. (arXiv:2307.12166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20219;&#21153;&#65292;&#22312;&#19981;&#21516;&#20307;&#35009;&#19979;&#36827;&#34892;&#20102;&#27604;&#36739;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#20855;&#26377;&#24456;&#39640;&#30340;&#25928;&#21147;&#65292;&#23613;&#31649;&#22312;&#21306;&#20998;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20855;&#26377;&#38761;&#26032;&#25945;&#32946;&#12289;&#30740;&#31350;&#21644;&#23454;&#36341;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#21306;&#20998;&#20154;&#31867;&#20889;&#20316;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#24050;&#32463;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#19981;&#21516;&#20307;&#35009;&#30340;&#20154;&#31867;&#20889;&#20316;&#21644;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#65306;&#35770;&#25991;&#12289;&#25925;&#20107;&#12289;&#35799;&#27468;&#21644;Python&#20195;&#30721;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20960;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#23545;&#36825;&#20123;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#25968;&#25454;&#38598;&#30340;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#39640;&#30340;&#25928;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#20998;&#31867;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#26102;&#65292;&#20219;&#21153;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#25925;&#20107;&#20889;&#20316;&#26041;&#38754;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26356;&#22797;&#26434;&#30340;&#22810;&#31867;&#21035;&#20219;&#21153;&#30456;&#27604;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65288;&#22914;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#21644;&#29305;&#23450;LLM&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential of artificial intelligence (AI)-based large language models (LLMs) holds considerable promise in revolutionizing education, research, and practice. However, distinguishing between human-written and AI-generated text has become a significant task. This paper presents a comparative study, introducing a novel dataset of human-written and LLM-generated texts in different genres: essays, stories, poetry, and Python code. We employ several machine learning models to classify the texts. Results demonstrate the efficacy of these models in discerning between human and AI-generated text, despite the dataset's limited sample size. However, the task becomes more challenging when classifying GPT-generated text, particularly in story writing. The results indicate that the models exhibit superior performance in binary classification tasks, such as distinguishing human-generated text from a specific LLM, compared to the more complex multiclass tasks that involve discerning among human-ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36716;&#24405;&#19978;&#19979;&#25991;&#20998;&#26512;&#21644;Transformer&#27169;&#22411;&#65292;&#22312;YouTube&#19978;&#35782;&#21035;&#38169;&#35823;&#20449;&#24687;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#35270;&#39057;&#20998;&#31867;&#20219;&#21153;&#36716;&#21270;&#20026;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#37319;&#29992;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#31561;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;YouTube&#19978;&#30340;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30456;&#20851;&#35270;&#39057;&#12289;&#20551;&#31185;&#23398;&#35270;&#39057;&#21644;Fake-News&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;YouTube&#19978;&#30340;&#38169;&#35823;&#20449;&#24687;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12155</link><description>&lt;p&gt;
&#36890;&#36807;&#36716;&#24405;&#19978;&#19979;&#25991;&#20998;&#26512;&#19982;Transformer&#27169;&#22411;&#22312;YouTube&#19978;&#35782;&#21035;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Identifying Misinformation on YouTube through Transcript Contextual Analysis with Transformer Models. (arXiv:2307.12155v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36716;&#24405;&#19978;&#19979;&#25991;&#20998;&#26512;&#21644;Transformer&#27169;&#22411;&#65292;&#22312;YouTube&#19978;&#35782;&#21035;&#38169;&#35823;&#20449;&#24687;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#35270;&#39057;&#20998;&#31867;&#20219;&#21153;&#36716;&#21270;&#20026;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#37319;&#29992;&#20102;&#36801;&#31227;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#31561;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;YouTube&#19978;&#30340;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30456;&#20851;&#35270;&#39057;&#12289;&#20551;&#31185;&#23398;&#35270;&#39057;&#21644;Fake-News&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;YouTube&#19978;&#30340;&#38169;&#35823;&#20449;&#24687;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
YouTube&#19978;&#30340;&#38169;&#35823;&#20449;&#24687;&#26159;&#19968;&#20010;&#37325;&#35201;&#20851;&#20999;&#65292;&#38656;&#35201;&#24378;&#22823;&#30340;&#26816;&#27979;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#20998;&#31867;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20869;&#23481;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#35270;&#39057;&#36716;&#24405;&#20013;&#33719;&#24471;&#30340;&#25991;&#26412;&#20869;&#23481;&#65292;&#23558;&#20256;&#32479;&#30340;&#35270;&#39057;&#20998;&#31867;&#20219;&#21153;&#36716;&#21270;&#20026;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22914;&#36801;&#31227;&#23398;&#20064;&#65292;&#26469;&#35299;&#20915;&#20998;&#31867;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34701;&#21512;&#20102;&#20004;&#31181;&#24418;&#24335;&#30340;&#36801;&#31227;&#23398;&#20064;&#65306;&#65288;a&#65289;&#24494;&#35843;&#22522;&#30784;&#30340;Transformer&#27169;&#22411;&#65292;&#22914;BERT&#65292;RoBERTa&#21644;ELECTRA&#65292;&#20197;&#21450;&#65288;b&#65289;&#20351;&#29992;&#21477;&#23376;&#36716;&#25442;&#22120;MPNet&#21644;RoBERTa-large&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#25105;&#20204;&#23558;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#24212;&#29992;&#20110;&#19977;&#20010;&#25968;&#25454;&#38598;&#65306;&#65288;a&#65289;&#19982;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30456;&#20851;&#30340;YouTube&#35270;&#39057;&#65292;&#65288;b&#65289;&#20851;&#20110;&#20551;&#31185;&#23398;&#30340;YouTube&#35270;&#39057;&#65292;&#20197;&#21450;&#65288;c&#65289;Fake-News&#25968;&#25454;&#38598;&#65288;&#19968;&#31995;&#21015;&#25991;&#31456;&#30340;&#38598;&#21512;&#65289;&#12290;&#21253;&#25324;Fake-News&#25968;&#25454;&#38598;&#25193;&#23637;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#35780;&#20272;&#33539;&#22260;&#65292;&#36229;&#36234;&#20102;YouTube&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation on YouTube is a significant concern, necessitating robust detection strategies. In this paper, we introduce a novel methodology for video classification, focusing on the veracity of the content. We convert the conventional video classification task into a text classification task by leveraging the textual content derived from the video transcripts. We employ advanced machine learning techniques like transfer learning to solve the classification challenge. Our approach incorporates two forms of transfer learning: (a) fine-tuning base transformer models such as BERT, RoBERTa, and ELECTRA, and (b) few-shot learning using sentence-transformers MPNet and RoBERTa-large. We apply the trained models to three datasets: (a) YouTube Vaccine-misinformation related videos, (b) YouTube Pseudoscience videos, and (c) Fake-News dataset (a collection of articles). Including the Fake-News dataset extended the evaluation of our approach beyond YouTube videos. Using these datasets, we evalua
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#29702;&#35299;&#65288;SLU&#65289;&#31995;&#32479;&#65292;&#36890;&#36807;&#34701;&#21512;&#22522;&#20110;ASR&#20551;&#35774;&#30340;&#27169;&#24577;&#32622;&#20449;&#24230;&#26469;&#22686;&#24378;&#23545;ASR&#38169;&#35823;&#30340;&#23481;&#38169;&#24615;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26377;&#25928;&#30340;&#32534;&#30721;ASR&#20551;&#35774;&#36136;&#37327;&#65292;&#25104;&#21151;&#23558;&#20854;&#25972;&#21512;&#21040;E2E SLU&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.12134</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#29702;&#35299;&#20013;&#30340;&#27169;&#24577;&#32622;&#20449;&#24230;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Modality Confidence Aware Training for Robust End-to-End Spoken Language Understanding. (arXiv:2307.12134v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12134
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#29702;&#35299;&#65288;SLU&#65289;&#31995;&#32479;&#65292;&#36890;&#36807;&#34701;&#21512;&#22522;&#20110;ASR&#20551;&#35774;&#30340;&#27169;&#24577;&#32622;&#20449;&#24230;&#26469;&#22686;&#24378;&#23545;ASR&#38169;&#35823;&#30340;&#23481;&#38169;&#24615;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26377;&#25928;&#30340;&#32534;&#30721;ASR&#20551;&#35774;&#36136;&#37327;&#65292;&#25104;&#21151;&#23558;&#20854;&#25972;&#21512;&#21040;E2E SLU&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20174;&#35821;&#38899;&#29983;&#25104;&#35821;&#20041;&#35299;&#26512;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#29702;&#35299;&#65288;SLU&#65289;&#31995;&#32479;&#21464;&#24471;&#26356;&#21152;&#26377;&#24076;&#26395;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65288;ASR&#65289;&#30340;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#27969;&#23186;&#20307;&#22330;&#26223;&#19979;&#30340;&#27969;&#27700;&#32447;SLU&#31995;&#32479;&#25928;&#26524;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#31471;&#21040;&#31471;SLU&#31995;&#32479;&#20173;&#28982;&#22312;ASR&#36716;&#20889;&#38169;&#35823;&#23548;&#33268;&#25991;&#26412;&#34920;&#31034;&#36136;&#37327;&#20302;&#26102;&#26174;&#31034;&#20986;&#24369;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;SLU&#31995;&#32479;&#65292;&#36890;&#36807;&#34701;&#21512;&#22522;&#20110;&#20272;&#35745;&#30340;ASR&#20551;&#35774;&#30340;&#27169;&#24577;&#32622;&#20449;&#24230;&#26469;&#22686;&#24378;&#23545;ASR&#38169;&#35823;&#30340;&#23481;&#38169;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65306;1&#65289;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#32534;&#30721;ASR&#20551;&#35774;&#30340;&#36136;&#37327;&#65292;2&#65289;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#23558;&#23427;&#20204;&#25972;&#21512;&#21040;&#31471;&#21040;&#31471;SLU&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;STOP&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24230;&#25913;&#36827;&#65292;&#24182;&#20998;&#20139;&#20998;&#26512;&#32467;&#26524;&#20197;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end (E2E) spoken language understanding (SLU) systems that generate a semantic parse from speech have become more promising recently. This approach uses a single model that utilizes audio and text representations from pre-trained speech recognition models (ASR), and outperforms traditional pipeline SLU systems in on-device streaming scenarios. However, E2E SLU systems still show weakness when text representation quality is low due to ASR transcription errors. To overcome this issue, we propose a novel E2E SLU system that enhances robustness to ASR errors by fusing audio and text representations based on the estimated modality confidence of ASR hypotheses. We introduce two novel techniques: 1) an effective method to encode the quality of ASR hypotheses and 2) an effective approach to integrate them into E2E SLU models. We show accuracy improvements on STOP dataset and share the analysis to demonstrate the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20027;&#39064;&#22686;&#24378;&#30340;&#35770;&#28857;&#25366;&#25496;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#30446;&#26631;&#30456;&#20851;&#23376;&#20027;&#39064;&#65292;&#24182;&#21033;&#29992;&#21477;&#32423;&#20027;&#39064;&#20449;&#24687;&#36827;&#34892;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2307.12131</link><description>&lt;p&gt;
&#20174;&#24322;&#26500;&#25968;&#25454;&#28304;&#20013;&#21487;&#35299;&#37322;&#30340;&#20027;&#39064;&#22686;&#24378;&#30340;&#35770;&#28857;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Explainable Topic-Enhanced Argument Mining from Heterogeneous Sources. (arXiv:2307.12131v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12131
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#20027;&#39064;&#22686;&#24378;&#30340;&#35770;&#28857;&#25366;&#25496;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#26469;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#30446;&#26631;&#30456;&#20851;&#23376;&#20027;&#39064;&#65292;&#24182;&#21033;&#29992;&#21477;&#32423;&#20027;&#39064;&#20449;&#24687;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#20687;&#8220;&#26680;&#33021;&#8221;&#36825;&#26679;&#30340;&#26377;&#20105;&#35758;&#30340;&#30446;&#26631;&#65292;&#35770;&#28857;&#25366;&#25496;&#26088;&#22312;&#20174;&#24322;&#26500;&#25968;&#25454;&#28304;&#20013;&#35782;&#21035;&#35770;&#35777;&#24615;&#25991;&#26412;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#27880;&#37325;&#25506;&#32034;&#26356;&#22909;&#30340;&#26041;&#27861;&#23558;&#30446;&#26631;&#30456;&#20851;&#30340;&#35821;&#20041;&#20449;&#24687;&#19982;&#35770;&#35777;&#24615;&#25991;&#26412;&#30456;&#32467;&#21512;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#32463;&#39564;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;(&#19968;)&#30446;&#26631;&#30001;&#19968;&#20010;&#35789;&#25110;&#30701;&#35821;&#34920;&#31034;&#65292;&#26080;&#27861;&#28085;&#30422;&#22810;&#26679;&#21270;&#30340;&#30446;&#26631;&#30456;&#20851;&#23376;&#20027;&#39064;&#65307;(&#20108;)&#22312;&#35770;&#35777;&#20013;&#24573;&#30053;&#20102;&#21477;&#32423;&#20027;&#39064;&#20449;&#24687;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#23545;&#20110;&#35770;&#28857;&#25366;&#25496;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35299;&#37322;&#30340;&#20027;&#39064;&#22686;&#24378;&#30340;&#35770;&#28857;&#25366;&#25496;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#30446;&#26631;&#20449;&#24687;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#20027;&#39064;&#34920;&#31034;&#36827;&#34892;&#25193;&#20805;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20854;&#28508;&#22312;&#20027;&#39064;&#20998;&#24067;&#19982;&#20854;&#35821;&#20041;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#25429;&#25417;&#35770;&#35777;&#20013;&#30340;&#21477;&#32423;&#20027;&#39064;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a controversial target such as ``nuclear energy'', argument mining aims to identify the argumentative text from heterogeneous sources. Current approaches focus on exploring better ways of integrating the target-associated semantic information with the argumentative text. Despite their empirical successes, two issues remain unsolved: (i) a target is represented by a word or a phrase, which is insufficient to cover a diverse set of target-related subtopics; (ii) the sentence-level topic information within an argument, which we believe is crucial for argument mining, is ignored. To tackle the above issues, we propose a novel explainable topic-enhanced argument mining approach. Specifically, with the use of the neural topic model and the language model, the target information is augmented by explainable topic representations. Moreover, the sentence-level topic information within the argument is captured by minimizing the distance between its latent topic distribution and its semantic
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#31245;&#36874;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.12114</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#30340;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Zero-shot and Few-shot Study of Instruction-Finetuned Large Language Models Applied to Clinical and Biomedical Tasks. (arXiv:2307.12114v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12114
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#31245;&#36874;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#27809;&#26377;&#19968;&#20010;&#27169;&#22411;&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#29305;&#23450;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#22235;&#31181;&#26368;&#20808;&#36827;&#30340;&#25351;&#23548;&#32454;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#8212;&#8212;ChatGPT&#12289;Flan-T5 UL2&#12289;Tk-Instruct&#21644;Alpaca&#8212;&#8212;&#22312;13&#20010;&#23454;&#38469;&#19990;&#30028;&#30340;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#20363;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12289;&#38382;&#31572;&#65288;QA&#65289;&#12289;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#31561;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#30340;LLM&#24320;&#22987;&#25509;&#36817;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#23545;&#20110;QA&#20219;&#21153;&#34920;&#29616;&#24471;&#29305;&#21035;&#22909;&#65292;&#21363;&#20351;&#23427;&#20204;&#20043;&#21069;&#27809;&#26377;&#35265;&#36807;&#36825;&#20123;&#20219;&#21153;&#30340;&#31034;&#20363;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#34920;&#29616;&#20302;&#20110;&#29305;&#23450;&#35757;&#32451;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#27169;&#22411;&#65288;&#22914;PubMedBERT&#65289;&#21487;&#20197;&#36798;&#21040;&#30340;&#27700;&#24179;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#27809;&#26377;&#19968;&#20010;LLM&#22312;&#25152;&#26377;&#30740;&#31350;&#20219;&#21153;&#19978;&#37117;&#32988;&#36807;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#20123;&#27169;&#22411;&#26356;&#36866;&#21512;&#20110;&#29305;&#23450;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We evaluate four state-of-the-art instruction-tuned large language models (LLMs) -- ChatGPT, Flan-T5 UL2, Tk-Instruct, and Alpaca -- on a set of 13 real-world clinical and biomedical natural language processing (NLP) tasks in English, such as named-entity recognition (NER), question-answering (QA), relation extraction (RE), etc. Our overall results demonstrate that the evaluated LLMs begin to approach performance of state-of-the-art models in zero- and few-shot scenarios for most tasks, and particularly well for the QA task, even though they have never seen examples from these tasks before. However, we observed that the classification and RE tasks perform below what can be achieved with a specifically trained model for the medical field, such as PubMedBERT. Finally, we noted that no LLM outperforms all the others on all the studied tasks, with some models being better suited for certain tasks than others.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20174;&#22806;&#37096;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#24615;&#22320;&#38598;&#25104;&#30693;&#35782;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22806;&#37096;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20363;&#23376;&#26159;ChatPDF&#12290;</title><link>http://arxiv.org/abs/2307.12057</link><description>&lt;p&gt;
&#22806;&#37096;&#25512;&#29702;&#65306;&#26397;&#30528;&#22810;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20114;&#25442;&#36741;&#21161;&#19982;&#20154;&#31867;&#21453;&#39304;&#30340;&#26041;&#21521;&#21069;&#36827;
&lt;/p&gt;
&lt;p&gt;
External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback. (arXiv:2307.12057v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20174;&#22806;&#37096;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#24615;&#22320;&#38598;&#25104;&#30693;&#35782;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22806;&#37096;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20363;&#23376;&#26159;ChatPDF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#34987;&#35748;&#20026;&#26159;&#20351;&#28023;&#39532;&#20307;&#21644;&#33041;&#31070;&#32463;&#20803;&#20869;&#20445;&#25345;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#12289;&#38543;&#21518;&#29992;&#20110;&#35299;&#20915;&#36890;&#36807;&#23398;&#20064;&#19968;&#29983;&#20013;&#36935;&#21040;&#30340;&#29616;&#23454;&#25361;&#25112;&#30340;&#20851;&#38190;&#20154;&#31867;&#33021;&#21147;&#12290;&#36890;&#36807;&#24212;&#29992;&#24050;&#33719;&#24471;&#30340;&#30693;&#35782;&#35299;&#20915;&#22797;&#26434;&#30340;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#26159;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#19968;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20687;GPT-3.5&#21644;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#20132;&#20114;&#21644;&#25512;&#29702;&#26041;&#38754;&#26174;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#30001;&#20110;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#23427;&#20204;&#26080;&#27861;&#22788;&#29702;&#24191;&#27867;&#12289;&#19981;&#26029;&#28436;&#21464;&#30340;&#30693;&#35782;&#24211;&#12290;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20174;&#22806;&#37096;&#23384;&#20648;&#24211;&#20013;&#36873;&#25321;&#24615;&#22320;&#38598;&#25104;&#30693;&#35782;&#26469;&#22686;&#24378;LLMs&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22806;&#37096;&#25512;&#29702;&#30340;&#26032;&#26041;&#27861;&#65292;&#20363;&#23376;&#26159;ChatPDF&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory is identified as a crucial human faculty that allows for the retention of visual and linguistic information within the hippocampus and neurons in the brain, which can subsequently be retrieved to address real-world challenges that arise through a lifetime of learning. The resolution of complex AI tasks through the application of acquired knowledge represents a stride toward the realization of artificial general intelligence. However, despite the prevalence of Large Language Models (LLMs) like GPT-3.5 and GPT-4 , which have displayed remarkable capabilities in language comprehension, generation, interaction, and reasoning, they are inhibited by constraints on context length that preclude the processing of extensive, continually evolving knowledge bases. This paper proposes that LLMs could be augmented through the selective integration of knowledge from external repositories, and in doing so, introduces a novel methodology for External Reasoning, exemplified by ChatPDF. Central to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#35270;&#35273;&#38382;&#39064;&#23450;&#20301;&#22238;&#31572;&#30340;&#33976;&#39311;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#33976;&#39311;&#25439;&#22833;&#65292;&#25552;&#20986;&#20102;&#21018;&#24615;-&#21487;&#22609;&#24615;&#24863;&#30693;&#33976;&#39311;&#21644;&#33258;&#26657;&#20934;&#24322;&#36136;&#33976;&#39311;&#26469;&#20445;&#30041;&#26087;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2307.12045</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#29992;&#20110;&#26426;&#22120;&#20154;&#25163;&#26415;&#35270;&#35273;&#38382;&#39064;&#23450;&#20301;&#22238;&#31572;&#30340;&#33976;&#39311;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Revisiting Distillation for Continual Learning on Visual Question Localized-Answering in Robotic Surgery. (arXiv:2307.12045v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26426;&#22120;&#20154;&#25163;&#26415;&#20013;&#35270;&#35273;&#38382;&#39064;&#23450;&#20301;&#22238;&#31572;&#30340;&#33976;&#39311;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#33976;&#39311;&#25439;&#22833;&#65292;&#25552;&#20986;&#20102;&#21018;&#24615;-&#21487;&#22609;&#24615;&#24863;&#30693;&#33976;&#39311;&#21644;&#33258;&#26657;&#20934;&#24322;&#36136;&#33976;&#39311;&#26469;&#20445;&#30041;&#26087;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#39064;&#23450;&#20301;&#22238;&#31572;&#65288;VQLA&#65289;&#31995;&#32479;&#21487;&#20197;&#20316;&#20026;&#25163;&#26415;&#25945;&#32946;&#20013;&#30340;&#30693;&#35782;&#21161;&#25163;&#12290;&#38500;&#20102;&#25552;&#20379;&#22522;&#20110;&#25991;&#26412;&#30340;&#31572;&#26696;&#22806;&#65292;VQLA&#31995;&#32479;&#36824;&#21487;&#20197;&#39640;&#20142;&#24863;&#20852;&#36259;&#30340;&#21306;&#22495;&#65292;&#20197;&#25552;&#39640;&#25163;&#26415;&#22330;&#26223;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#23398;&#20064;&#26032;&#30693;&#35782;&#26102;&#23481;&#26131;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;DNNs&#22312;&#22686;&#37327;&#31867;&#21035;&#25110;&#20219;&#21153;&#19978;&#23398;&#20064;&#26102;&#65292;&#20854;&#23545;&#26087;&#20219;&#21153;&#30340;&#24615;&#33021;&#20250;&#22823;&#24133;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#21307;&#30103;&#25968;&#25454;&#38544;&#31169;&#21644;&#35768;&#21487;&#38382;&#39064;&#65292;&#26356;&#26032;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#27169;&#22411;&#26102;&#24448;&#24448;&#38590;&#20197;&#35775;&#38382;&#26087;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38750;&#31034;&#20363;&#36830;&#32493;&#25163;&#26415;VQLA&#26694;&#26550;&#65292;&#20197;&#22312;&#39034;&#24207;&#23398;&#20064;&#33539;&#24335;&#20013;&#25506;&#32034;&#21644;&#24179;&#34913;DNNs&#30340;&#21018;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;CL&#20219;&#21153;&#20013;&#30340;&#33976;&#39311;&#25439;&#22833;&#65292;&#24182;&#25552;&#20986;&#20102;&#21018;&#24615;-&#21487;&#22609;&#24615;&#24863;&#30693;&#33976;&#39311;&#65288;RP-Dist&#65289;&#21644;&#33258;&#26657;&#20934;&#24322;&#36136;&#33976;&#39311;&#65288;SH-Dist&#65289;&#26469;&#20445;&#30041;&#26087;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
The visual-question localized-answering (VQLA) system can serve as a knowledgeable assistant in surgical education. Except for providing text-based answers, the VQLA system can highlight the interested region for better surgical scene understanding. However, deep neural networks (DNNs) suffer from catastrophic forgetting when learning new knowledge. Specifically, when DNNs learn on incremental classes or tasks, their performance on old tasks drops dramatically. Furthermore, due to medical data privacy and licensing issues, it is often difficult to access old data when updating continual learning (CL) models. Therefore, we develop a non-exemplar continual surgical VQLA framework, to explore and balance the rigidity-plasticity trade-off of DNNs in a sequential learning paradigm. We revisit the distillation loss in CL tasks, and propose rigidity-plasticity-aware distillation (RP-Dist) and self-calibrated heterogeneous distillation (SH-Dist) to preserve the old knowledge. The weight aligni
&lt;/p&gt;</description></item><item><title>Psy-LLM&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22312;&#32447;&#24515;&#29702;&#21672;&#35810;&#25552;&#20379;&#38382;&#31572;&#26381;&#21153;&#65292;&#21069;&#31471;&#24037;&#20855;&#21487;&#35753;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25552;&#20379;&#21363;&#26102;&#21709;&#24212;&#21644;&#27491;&#24565;&#27963;&#21160;&#65292;&#21516;&#26102;&#36824;&#21487;&#20316;&#20026;&#31579;&#26597;&#24037;&#20855;&#36741;&#21161;&#35782;&#21035;&#32039;&#24613;&#26696;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.11991</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#20840;&#29699;&#24515;&#29702;&#20581;&#24247;&#24515;&#29702;&#26381;&#21153;&#30340;Psy-LLM
&lt;/p&gt;
&lt;p&gt;
Psy-LLM: Scaling up Global Mental Health Psychological Services with AI-based Large Language Models. (arXiv:2307.11991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11991
&lt;/p&gt;
&lt;p&gt;
Psy-LLM&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22312;&#32447;&#24515;&#29702;&#21672;&#35810;&#25552;&#20379;&#38382;&#31572;&#26381;&#21153;&#65292;&#21069;&#31471;&#24037;&#20855;&#21487;&#35753;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25552;&#20379;&#21363;&#26102;&#21709;&#24212;&#21644;&#27491;&#24565;&#27963;&#21160;&#65292;&#21516;&#26102;&#36824;&#21487;&#20316;&#20026;&#31579;&#26597;&#24037;&#20855;&#36741;&#21161;&#35782;&#21035;&#32039;&#24613;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24515;&#29702;&#21672;&#35810;&#30340;&#38656;&#27714;&#26174;&#33879;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#20840;&#29699;COVID-19&#30340;&#29190;&#21457;&#65292;&#36825;&#21152;&#24378;&#20102;&#21450;&#26102;&#21644;&#19987;&#19994;&#30340;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#38656;&#27714;&#12290;&#22312;&#32447;&#24515;&#29702;&#21672;&#35810;&#25104;&#20026;&#24212;&#23545;&#36825;&#19968;&#38656;&#27714;&#30340;&#20027;&#35201;&#26381;&#21153;&#26041;&#24335;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Psy-LLM&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22312;&#32447;&#24515;&#29702;&#21672;&#35810;&#20013;&#30340;&#38382;&#31572;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32467;&#21512;&#20102;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;LLMs&#21644;&#20174;&#24515;&#29702;&#23398;&#23478;&#21644;&#24191;&#27867;&#25910;&#38598;&#30340;&#24515;&#29702;&#25991;&#31456;&#20013;&#33719;&#21462;&#30340;&#30495;&#23454;&#19990;&#30028;&#19987;&#19994;&#38382;&#31572;&#12290;Psy-LLM&#26694;&#26550;&#20316;&#20026;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#30340;&#21069;&#31471;&#24037;&#20855;&#65292;&#20801;&#35768;&#20182;&#20204;&#25552;&#20379;&#21363;&#26102;&#21709;&#24212;&#21644;&#27491;&#24565;&#27963;&#21160;&#26469;&#32531;&#35299;&#24739;&#32773;&#21387;&#21147;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#20316;&#20026;&#31579;&#26597;&#24037;&#20855;&#65292;&#35782;&#21035;&#38656;&#35201;&#36827;&#19968;&#27493;&#21327;&#21161;&#30340;&#32039;&#24613;&#26696;&#20363;&#12290;&#25105;&#20204;&#20351;&#29992;&#22256;&#24785;&#24230;&#31561;&#20869;&#22312;&#24230;&#37327;&#26631;&#20934;&#21644;&#22806;&#37096;&#24230;&#37327;&#26631;&#20934;&#23545;&#26694;&#26550;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demand for psychological counseling has grown significantly in recent years, particularly with the global outbreak of COVID-19, which has heightened the need for timely and professional mental health support. Online psychological counseling has emerged as the predominant mode of providing services in response to this demand. In this study, we propose the Psy-LLM framework, an AI-based system leveraging Large Language Models (LLMs) for question-answering in online psychological consultation. Our framework combines pre-trained LLMs with real-world professional Q&amp;A from psychologists and extensively crawled psychological articles. The Psy-LLM framework serves as a front-end tool for healthcare professionals, allowing them to provide immediate responses and mindfulness activities to alleviate patient stress. Additionally, it functions as a screening tool to identify urgent cases requiring further assistance. We evaluated the framework using intrinsic metrics, such as perplexity, and ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;YouTube&#35270;&#39057;&#20013;&#23398;&#20064;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#25151;&#23627;&#23548;&#35272;&#35270;&#39057;&#20013;&#30340;&#36335;&#24452;&#25351;&#20196;&#23545;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#33258;&#21160;&#26500;&#24314;&#36335;&#24452;&#25351;&#20196;&#23545;&#21644;&#20174;&#26080;&#26631;&#31614;&#35270;&#39057;&#20013;&#25552;&#21462;&#24067;&#23616;&#30693;&#35782;&#30340;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.11984</link><description>&lt;p&gt;
&#20174;YouTube&#35270;&#39057;&#20013;&#23398;&#20064;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Learning Vision-and-Language Navigation from YouTube Videos. (arXiv:2307.11984v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;YouTube&#35270;&#39057;&#20013;&#23398;&#20064;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#25151;&#23627;&#23548;&#35272;&#35270;&#39057;&#20013;&#30340;&#36335;&#24452;&#25351;&#20196;&#23545;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#27867;&#21270;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22788;&#29702;&#33258;&#21160;&#26500;&#24314;&#36335;&#24452;&#25351;&#20196;&#23545;&#21644;&#20174;&#26080;&#26631;&#31614;&#35270;&#39057;&#20013;&#25552;&#21462;&#24067;&#23616;&#30693;&#35782;&#30340;&#25361;&#25112;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#38656;&#35201;&#19968;&#20010;&#20855;&#26377;&#36523;&#20307;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#22312;&#29616;&#23454;&#30340;&#19977;&#32500;&#29615;&#22659;&#20013;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36827;&#34892;&#23548;&#33322;&#12290;&#29616;&#26377;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#26041;&#27861;&#22312;&#23567;&#35268;&#27169;&#29615;&#22659;&#25110;&#19981;&#21512;&#29702;&#30340;&#36335;&#24452;&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#38480;&#21046;&#20102;&#23545;&#26410;&#30693;&#29615;&#22659;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;YouTube&#19978;&#26377;&#22823;&#37327;&#30340;&#25151;&#23627;&#23548;&#35272;&#35270;&#39057;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#30495;&#23454;&#23548;&#35272;&#32463;&#39564;&#21644;&#24067;&#23616;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35270;&#39057;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#26041;&#38754;&#23578;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#20174;&#23548;&#35272;&#35270;&#39057;&#20013;&#33719;&#21462;&#21512;&#29702;&#30340;&#36335;&#24452;&#25351;&#20196;&#23545;&#65292;&#24182;&#22312;&#20854;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20174;&#36825;&#20123;&#35270;&#39057;&#20013;&#23398;&#20064;&#19968;&#20010;&#26234;&#33021;&#26426;&#22120;&#20154;&#20195;&#29702;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#38656;&#35201;&#35299;&#20915;&#33258;&#21160;&#26500;&#24314;&#36335;&#24452;&#25351;&#20196;&#23545;&#21644;&#20174;&#21407;&#22987;&#21644;&#26080;&#26631;&#31614;&#35270;&#39057;&#20013;&#21033;&#29992;&#30495;&#23454;&#24067;&#23616;&#30693;&#35782;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#22522;&#20110;&#29109;&#30340;&#26041;&#27861;&#26500;&#24314;&#36335;&#24452;&#36712;&#36857;&#30340;&#33410;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010; action-aware g&#27169;&#22411;&#26469;&#20174;&#35270;&#39057;&#20013;&#25552;&#21462;&#24067;&#23616;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-and-language navigation (VLN) requires an embodied agent to navigate in realistic 3D environments using natural language instructions. Existing VLN methods suffer from training on small-scale environments or unreasonable path-instruction datasets, limiting the generalization to unseen environments. There are massive house tour videos on YouTube, providing abundant real navigation experiences and layout information. However, these videos have not been explored for VLN before. In this paper, we propose to learn an agent from these videos by creating a large-scale dataset which comprises reasonable path-instruction pairs from house tour videos and pre-training the agent on it. To achieve this, we have to tackle the challenges of automatically constructing path-instruction pairs and exploiting real layout knowledge from raw and unlabeled videos. To address these, we first leverage an entropy-based method to construct the nodes of a path trajectory. Then, we propose an action-aware g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BLINDER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#26465;&#20214;&#19979;&#29366;&#24577;&#25551;&#36848;&#30340;&#20540;&#20989;&#25968;&#65292;&#33258;&#21160;&#36873;&#25321;&#31616;&#26126;&#30340;&#29366;&#24577;&#25551;&#36848;&#65292;&#20197;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#28436;&#21592;&#22312;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.11922</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#24863;&#30693;&#65306;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20026;&#35821;&#35328;&#27169;&#22411;&#28436;&#21592;&#20248;&#21270;&#29366;&#24577;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Selective Perception: Optimizing State Descriptions with Reinforcement Learning for Language Model Actors. (arXiv:2307.11922v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BLINDER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#26465;&#20214;&#19979;&#29366;&#24577;&#25551;&#36848;&#30340;&#20540;&#20989;&#25968;&#65292;&#33258;&#21160;&#36873;&#25321;&#31616;&#26126;&#30340;&#29366;&#24577;&#25551;&#36848;&#65292;&#20197;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#28436;&#21592;&#22312;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#34987;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#21644;&#28216;&#25103;&#31561;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#30340;&#28436;&#21592;&#20013;&#65292;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#35268;&#21010;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#24456;&#23569;&#25506;&#32034;&#36890;&#36807;&#35821;&#35328;&#21521;LLM&#28436;&#21592;&#25552;&#20379;&#20160;&#20040;&#29615;&#22659;&#29366;&#24577;&#20449;&#24687;&#12290;&#35814;&#23613;&#25551;&#36848;&#39640;&#32500;&#29366;&#24577;&#21487;&#33021;&#20250;&#24433;&#21709;&#24615;&#33021;&#24182;&#22686;&#21152;LLM&#28436;&#21592;&#30340;&#25512;&#29702;&#25104;&#26412;&#12290;&#20197;&#21069;&#30340;LLM&#28436;&#21592;&#36890;&#36807;&#20381;&#36182;&#25163;&#24037;&#35774;&#35745;&#30340;&#20219;&#21153;&#29305;&#23450;&#21327;&#35758;&#26469;&#30830;&#23450;&#35813;&#29366;&#24577;&#30340;&#21738;&#20123;&#29305;&#24449;&#38656;&#35201;&#36827;&#34892;&#20256;&#36882;&#65292;&#21738;&#20123;&#19981;&#38656;&#35201;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BLINDER&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20219;&#21153;&#26465;&#20214;&#19979;&#29366;&#24577;&#25551;&#36848;&#30340;&#20540;&#20989;&#25968;&#65292;&#33258;&#21160;&#36873;&#25321;&#31616;&#26126;&#30340;&#29366;&#24577;&#25551;&#36848;&#12290;&#25105;&#20204;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#39057;&#28216;&#25103;NetHack&#21644;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;BLINDER&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#20943;&#23569;&#20102;&#36755;&#20837;&#22823;&#23567;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#25552;&#39640;&#20102;&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are being applied as actors for sequential decision making tasks in domains such as robotics and games, utilizing their general world knowledge and planning abilities. However, previous work does little to explore what environment state information is provided to LLM actors via language. Exhaustively describing high-dimensional states can impair performance and raise inference costs for LLM actors. Previous LLM actors avoid the issue by relying on hand-engineered, task-specific protocols to determine which features to communicate about a state and which to leave out. In this work, we propose Brief Language INputs for DEcision-making Responses (BLINDER), a method for automatically selecting concise state descriptions by learning a value function for task-conditioned state descriptions. We evaluate BLINDER on the challenging video game NetHack and a robotic manipulation task. Our method improves task success rate, reduces input size and compute costs, and gen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#22312;&#31354;&#38388;&#35268;&#21010;&#21644;&#23548;&#33322;&#20132;&#21449;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#35299;&#26512;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25191;&#34892;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.11865</link><description>&lt;p&gt;
CARTIER: &#38754;&#21521;&#26426;&#22120;&#20154;&#25351;&#20196;&#25191;&#34892;&#30340;&#22320;&#22270;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
CARTIER: Cartographic lAnguage Reasoning Targeted at Instruction Execution for Robots. (arXiv:2307.11865v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#22312;&#31354;&#38388;&#35268;&#21010;&#21644;&#23548;&#33322;&#20132;&#21449;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#35299;&#26512;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25191;&#34892;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#31354;&#38388;&#35268;&#21010;&#21644;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#19982;&#23548;&#33322;&#20132;&#21449;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#36981;&#24490;&#30456;&#23545;&#22797;&#26434;&#30340;&#25351;&#20196;&#65292;&#36825;&#20123;&#25351;&#20196;&#26356;&#31867;&#20284;&#20110;&#33258;&#28982;&#23545;&#35805;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#26174;&#24335;&#36807;&#31243;&#25351;&#20196;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#22312;&#37027;&#20123;&#23548;&#33322;&#25351;&#20196;&#34987;&#25552;&#20379;&#20026;&#21629;&#20196;&#24335;&#25351;&#20196;&#65288;&#20363;&#22914;&#65292;&#21435;&#20912;&#31665;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#35805;&#20132;&#20114;&#20013;&#30340;&#38544;&#24335;&#25351;&#20196;&#12290;&#25105;&#20204;&#21033;&#29992;3D&#27169;&#25311;&#22120;AI2Thor&#21019;&#24314;&#22797;&#26434;&#19988;&#21487;&#37325;&#22797;&#30340;&#22330;&#26223;&#65292;&#24182;&#36890;&#36807;&#20026;40&#31181;&#23545;&#35937;&#31867;&#22411;&#28155;&#21152;&#22797;&#26434;&#30340;&#35821;&#35328;&#26597;&#35810;&#26469;&#22686;&#24378;&#23427;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;LLM&#23558;&#29992;&#25143;&#20132;&#20114;&#35299;&#37322;&#20026;&#22330;&#26223;&#20013;&#23545;&#35937;&#21015;&#34920;&#30340;&#19978;&#19979;&#25991;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#26512;&#25551;&#36848;&#24615;&#35821;&#35328;&#26597;&#35810;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work explores the capacity of large language models (LLMs) to address problems at the intersection of spatial planning and natural language interfaces for navigation.Our focus is on following relatively complex instructions that are more akin to natural conversation than traditional explicit procedural directives seen in robotics. Unlike most prior work, where navigation directives are provided as imperative commands (e.g., go to the fridge), we examine implicit directives within conversational interactions. We leverage the 3D simulator AI2Thor to create complex and repeatable scenarios at scale, and augment it by adding complex language queries for 40 object types. We demonstrate that a robot can better parse descriptive language queries than existing methods by using an LLM to interpret the user interaction in the context of a list of the objects in the scene.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#39046;&#33521;&#24179;&#21488;&#19978;&#26816;&#27979;&#34394;&#20551;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20010;&#20154;&#36164;&#26009;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#39046;&#33521;&#20010;&#20154;&#36164;&#26009;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#8220;&#37096;&#20998;&#21644;&#23376;&#37096;&#20998;&#26631;&#31614;&#23884;&#20837;&#8221;&#65288;SSTE&#65289;&#26041;&#27861;&#20197;&#22686;&#24378;&#25968;&#25454;&#30340;&#29305;&#24449;&#12290;&#30740;&#31350;&#36824;&#36890;&#36807;&#25910;&#38598;3600&#20010;&#39046;&#33521;&#20010;&#20154;&#36164;&#26009;&#24314;&#31435;&#20102;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.11864</link><description>&lt;p&gt;
&#34394;&#20551;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#39046;&#33521;&#20010;&#20154;&#36164;&#26009;&#30340;&#28508;&#22312;&#23041;&#32961;&#65306;&#26816;&#27979;&#21644;&#39044;&#38450;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
The Looming Threat of Fake and LLM-generated LinkedIn Profiles: Challenges and Opportunities for Detection and Prevention. (arXiv:2307.11864v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#39046;&#33521;&#24179;&#21488;&#19978;&#26816;&#27979;&#34394;&#20551;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20010;&#20154;&#36164;&#26009;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#39046;&#33521;&#20010;&#20154;&#36164;&#26009;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#8220;&#37096;&#20998;&#21644;&#23376;&#37096;&#20998;&#26631;&#31614;&#23884;&#20837;&#8221;&#65288;SSTE&#65289;&#26041;&#27861;&#20197;&#22686;&#24378;&#25968;&#25454;&#30340;&#29305;&#24449;&#12290;&#30740;&#31350;&#36824;&#36890;&#36807;&#25910;&#38598;3600&#20010;&#39046;&#33521;&#20010;&#20154;&#36164;&#26009;&#24314;&#31435;&#20102;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39046;&#33521;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#27880;&#20876;&#26102;&#21644;&#24314;&#31435;&#36830;&#25509;&#20043;&#21069;&#31435;&#21363;&#26816;&#27979;&#34394;&#20551;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#20010;&#20154;&#36164;&#26009;&#12290;&#26089;&#26399;&#35782;&#21035;&#34394;&#20551;&#36164;&#26009;&#23545;&#20110;&#32500;&#25252;&#24179;&#21488;&#30340;&#23436;&#25972;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#21487;&#20197;&#38450;&#27490;&#20882;&#21517;&#39030;&#26367;&#32773;&#33719;&#21462;&#21512;&#27861;&#29992;&#25143;&#30340;&#31169;&#23494;&#21644;&#25935;&#24863;&#20449;&#24687;&#65292;&#24182;&#38450;&#27490;&#20182;&#20204;&#33719;&#24471;&#22686;&#21152;&#26410;&#26469;&#38035;&#40060;&#21644;&#27450;&#35784;&#27963;&#21160;&#21487;&#20449;&#24230;&#30340;&#26426;&#20250;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#39046;&#33521;&#20010;&#20154;&#36164;&#26009;&#20013;&#25552;&#20379;&#30340;&#25991;&#26412;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#20102;&#8220;&#37096;&#20998;&#21644;&#23376;&#37096;&#20998;&#26631;&#31614;&#23884;&#20837;&#8221;&#65288;SSTE&#65289;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#21306;&#20998;&#21512;&#27861;&#36164;&#26009;&#21644;&#20882;&#21517;&#39030;&#26367;&#32773;&#25163;&#21160;&#25110;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#36164;&#26009;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#37492;&#20110;&#30446;&#21069;&#20844;&#24320;&#21487;&#29992;&#30340;&#39046;&#33521;&#25968;&#25454;&#38598;&#36739;&#23569;&#65292;&#25105;&#20204;&#20026;&#30740;&#31350;&#25910;&#38598;&#20102;3600&#20010;&#39046;&#33521;&#20010;&#20154;&#36164;&#26009;&#65292;&#24182;&#23558;&#20844;&#24320;&#21457;&#24067;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20379;&#30740;&#31350;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel method for detecting fake and Large Language Model (LLM)-generated profiles in the LinkedIn Online Social Network immediately upon registration and before establishing connections. Early fake profile identification is crucial to maintaining the platform's integrity since it prevents imposters from acquiring the private and sensitive information of legitimate users and from gaining an opportunity to increase their credibility for future phishing and scamming activities. This work uses textual information provided in LinkedIn profiles and introduces the Section and Subsection Tag Embedding (SSTE) method to enhance the discriminative characteristics of these data for distinguishing between legitimate profiles and those created by imposters manually or by using an LLM. Additionally, the dearth of a large publicly available LinkedIn dataset motivated us to collect 3600 LinkedIn profiles for our research. We will release our dataset publicly for research pur
&lt;/p&gt;</description></item><item><title>MythQA&#26159;&#19968;&#39033;&#26032;&#30340;&#22810;&#31572;&#26696;&#24320;&#25918;&#39046;&#22495;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#30683;&#30462;&#31435;&#22330;&#25366;&#25496;&#26469;&#26816;&#27979;&#22823;&#35268;&#27169;&#26597;&#35810;&#20540;&#24471;&#26816;&#26597;&#30340;&#26029;&#35328;&#12290;&#35813;&#20219;&#21153;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;522&#20010;&#22522;&#20110;&#26377;&#20105;&#35758;&#30340;&#35805;&#39064;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.11848</link><description>&lt;p&gt;
MythQA: &#22810;&#31572;&#26696;&#24320;&#25918;&#39046;&#22495;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#22823;&#35268;&#27169;&#26597;&#35810;&#20540;&#24471;&#26816;&#26597;&#30340;&#26029;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MythQA: Query-Based Large-Scale Check-Worthy Claim Detection through Multi-Answer Open-Domain Question Answering. (arXiv:2307.11848v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11848
&lt;/p&gt;
&lt;p&gt;
MythQA&#26159;&#19968;&#39033;&#26032;&#30340;&#22810;&#31572;&#26696;&#24320;&#25918;&#39046;&#22495;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#30683;&#30462;&#31435;&#22330;&#25366;&#25496;&#26469;&#26816;&#27979;&#22823;&#35268;&#27169;&#26597;&#35810;&#20540;&#24471;&#26816;&#26597;&#30340;&#26029;&#35328;&#12290;&#35813;&#20219;&#21153;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;522&#20010;&#22522;&#20110;&#26377;&#20105;&#35758;&#30340;&#35805;&#39064;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#26469;&#36827;&#34892;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#20540;&#24471;&#26816;&#26597;&#30340;&#26029;&#35328;&#26816;&#27979;&#26088;&#22312;&#21521;&#19979;&#28216;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#25110;&#20154;&#24037;&#19987;&#23478;&#25552;&#20379;&#21487;&#33021;&#30340;&#38169;&#35823;&#20449;&#24687;&#36827;&#34892;&#26816;&#26597;&#12290;&#36825;&#26159;&#21152;&#36895;&#20107;&#23454;&#26680;&#26597;&#36807;&#31243;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#35768;&#22810;&#21162;&#21147;&#24050;&#32463;&#25237;&#20837;&#21040;&#22914;&#20309;&#20174;&#39044;&#25910;&#38598;&#30340;&#23569;&#37327;&#26029;&#35328;&#20013;&#35782;&#21035;&#20540;&#24471;&#26816;&#26597;&#30340;&#26029;&#35328;&#30340;&#30740;&#31350;&#20013;&#65292;&#20294;&#22914;&#20309;&#30452;&#25509;&#20174;&#22823;&#35268;&#27169;&#20449;&#24687;&#28304;&#65288;&#22914;Twitter&#65289;&#26377;&#25928;&#26816;&#27979;&#20540;&#24471;&#26816;&#26597;&#30340;&#26029;&#35328;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MythQA&#65292;&#19968;&#39033;&#26032;&#30340;&#22810;&#31572;&#26696;&#24320;&#25918;&#39046;&#22495;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#28041;&#21450;&#29992;&#20110;&#26597;&#35810;&#20540;&#24471;&#26816;&#26597;&#30340;&#22823;&#35268;&#27169;&#26029;&#35328;&#26816;&#27979;&#30340;&#30683;&#30462;&#31435;&#22330;&#25366;&#25496;&#12290;&#36825;&#19968;&#24819;&#27861;&#30340;&#32972;&#21518;&#26159;&#65292;&#30683;&#30462;&#30340;&#26029;&#35328;&#26159;&#20540;&#24471;&#30001;&#36866;&#24403;&#30340;&#26426;&#26500;&#36827;&#34892;&#23457;&#26597;&#30340;&#38169;&#35823;&#20449;&#24687;&#30340;&#24378;&#26377;&#21147;&#25351;&#26631;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;TweetMythQA&#65292;&#19968;&#20010;&#21253;&#21547;522&#20010;&#22522;&#20110;&#26377;&#20105;&#35758;&#30340;&#35805;&#39064;&#30340;&#20107;&#23454;&#22411;&#22810;&#31572;&#26696;&#38382;&#39064;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#12290;&#27599;&#20010;&#38382;&#39064;&#37117;&#24102;&#26377;&#22810;&#20010;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Check-worthy claim detection aims at providing plausible misinformation to downstream fact-checking systems or human experts to check. This is a crucial step toward accelerating the fact-checking process. Many efforts have been put into how to identify check-worthy claims from a small scale of pre-collected claims, but how to efficiently detect check-worthy claims directly from a large-scale information source, such as Twitter, remains underexplored. To fill this gap, we introduce MythQA, a new multi-answer open-domain question answering(QA) task that involves contradictory stance mining for query-based large-scale check-worthy claim detection. The idea behind this is that contradictory claims are a strong indicator of misinformation that merits scrutiny by the appropriate authorities. To study this task, we construct TweetMythQA, an evaluation dataset containing 522 factoid multi-answer questions based on controversial topics. Each question is annotated with multiple answers. Moreover
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#24212;&#23545;&#37329;&#34701;&#31185;&#25216;&#31454;&#20105;&#21644;&#25552;&#39640;&#38134;&#34892;&#19994;&#21153;&#36816;&#33829;&#25928;&#29575;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#22810;&#27169;&#24335;&#27169;&#22411;&#29305;&#21035;&#26159;&#20808;&#36827;&#30340;&#25991;&#26723;&#20998;&#26512;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#38134;&#34892;&#27969;&#31243;&#20013;&#30340;&#28508;&#21147;&#21644;&#26426;&#20250;&#65292;&#24182;&#23637;&#31034;&#20102;LayoutXLM&#31561;&#27169;&#22411;&#22312;&#20998;&#26512;&#38134;&#34892;&#25991;&#26723;&#20013;&#30340;&#28508;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11845</link><description>&lt;p&gt;
&#38754;&#21521;&#38134;&#34892;&#27969;&#31243;&#33258;&#21160;&#21270;&#30340;&#22810;&#27169;&#24335;&#25991;&#26723;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multimodal Document Analytics for Banking Process Automation. (arXiv:2307.11845v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#24212;&#23545;&#37329;&#34701;&#31185;&#25216;&#31454;&#20105;&#21644;&#25552;&#39640;&#38134;&#34892;&#19994;&#21153;&#36816;&#33829;&#25928;&#29575;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#22810;&#27169;&#24335;&#27169;&#22411;&#29305;&#21035;&#26159;&#20808;&#36827;&#30340;&#25991;&#26723;&#20998;&#26512;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#38134;&#34892;&#27969;&#31243;&#20013;&#30340;&#28508;&#21147;&#21644;&#26426;&#20250;&#65292;&#24182;&#23637;&#31034;&#20102;LayoutXLM&#31561;&#27169;&#22411;&#22312;&#20998;&#26512;&#38134;&#34892;&#25991;&#26723;&#20013;&#30340;&#28508;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#37329;&#34701;&#31185;&#25216;&#31454;&#20105;&#30340;&#22686;&#38271;&#21644;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#30340;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#20110;&#29702;&#35299;&#22312;&#38134;&#34892;&#27969;&#31243;&#20013;&#21033;&#29992;&#22810;&#27169;&#24335;&#27169;&#22411;&#29305;&#21035;&#26159;&#20808;&#36827;&#30340;&#25991;&#26723;&#20998;&#26512;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#23545;&#22810;&#26679;&#21270;&#30340;&#38134;&#34892;&#25991;&#26723;&#39046;&#22495;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#20808;&#36827;&#30340;&#20998;&#26512;&#25216;&#26415;&#22312;&#23458;&#25143;&#19994;&#21153;&#20013;&#25552;&#39640;&#25928;&#29575;&#30340;&#26426;&#20250;&#12290;&#22522;&#20110;&#24555;&#36895;&#21457;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35832;&#22914;LayoutXLM&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#23427;&#26159;&#19968;&#31181;&#36328;&#35821;&#35328;&#12289;&#22810;&#27169;&#24335;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#38134;&#34892;&#19994;&#20013;&#21508;&#31181;&#19981;&#21516;&#30340;&#25991;&#26723;&#12290;&#35813;&#27169;&#22411;&#23545;&#24503;&#22269;&#20844;&#21496;&#30331;&#35760;&#25552;&#21462;&#30340;&#25991;&#26412;&#26631;&#35760;&#20998;&#31867;&#20855;&#26377;&#22823;&#32422;80%&#30340;F1&#24471;&#20998;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35777;&#25454;&#35777;&#23454;&#20102;&#24067;&#23616;&#20449;&#24687;&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#25972;&#21512;&#22270;&#20687;&#20449;&#24687;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to growing FinTech competition and the need for improved operational efficiency, this research focuses on understanding the potential of advanced document analytics, particularly using multimodal models, in banking processes. We perform a comprehensive analysis of the diverse banking document landscape, highlighting the opportunities for efficiency gains through automation and advanced analytics techniques in the customer business. Building on the rapidly evolving field of natural language processing (NLP), we illustrate the potential of models such as LayoutXLM, a cross-lingual, multimodal, pre-trained model, for analyzing diverse documents in the banking sector. This model performs a text token classification on German company register extracts with an overall F1 score performance of around 80\%. Our empirical evidence confirms the critical role of layout information in improving model performance and further underscores the benefits of integrating image information. Inte
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#20351;&#20854;&#20855;&#22791;&#20102;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#12290;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#26679;&#30340;&#25193;&#23637;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#23454;&#29616;&#20102;&#35821;&#38899;&#35782;&#21035;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#21487;&#20197;&#20923;&#32467;&#27169;&#22411;&#20197;&#20445;&#25345;&#20854;&#21407;&#26377;&#21151;&#33021;&#65292;&#24182;&#19988;&#25552;&#21319;&#38899;&#39057;&#32534;&#30721;&#22120;&#30340;&#35268;&#27169;&#26377;&#21161;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11795</link><description>&lt;p&gt;
&#29992;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models with Speech Recognition Abilities. (arXiv:2307.11795v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#38899;&#39057;&#32534;&#30721;&#22120;&#65292;&#20351;&#20854;&#20855;&#22791;&#20102;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#12290;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#26679;&#30340;&#25193;&#23637;&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#23454;&#29616;&#20102;&#35821;&#38899;&#35782;&#21035;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#21487;&#20197;&#20923;&#32467;&#27169;&#22411;&#20197;&#20445;&#25345;&#20854;&#21407;&#26377;&#21151;&#33021;&#65292;&#24182;&#19988;&#25552;&#21319;&#38899;&#39057;&#32534;&#30721;&#22120;&#30340;&#35268;&#27169;&#26377;&#21161;&#20110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#35777;&#26126;&#20854;&#39640;&#24230;&#28789;&#27963;&#65292;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#65292;&#22914;&#27010;&#25324;&#24615;&#25688;&#35201;&#21644;&#24320;&#25918;&#24615;&#38382;&#31572;&#12290;&#26412;&#25991;&#36890;&#36807;&#30452;&#25509;&#38468;&#21152;&#19968;&#20010;&#23567;&#22411;&#38899;&#39057;&#32534;&#30721;&#22120;&#26469;&#25193;&#23637;LLM&#30340;&#21151;&#33021;&#65292;&#20351;&#20854;&#33021;&#22815;&#25191;&#34892;&#35821;&#38899;&#35782;&#21035;&#12290;&#36890;&#36807;&#23558;&#19968;&#31995;&#21015;&#22768;&#38899;&#23884;&#20837;&#30452;&#25509;&#39044;&#32622;&#21040;&#25991;&#26412;&#20196;&#29260;&#23884;&#20837;&#20043;&#21069;&#65292;LLM&#21487;&#20197;&#36716;&#25442;&#20026;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#20854;&#25991;&#26412;&#23545;&#24212;&#29289;&#20197;&#23436;&#20840;&#30456;&#21516;&#30340;&#26041;&#24335;&#20351;&#29992;&#12290;&#22312;&#22810;&#35821;&#35328;LibriSpeech&#65288;MLS&#65289;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23558;&#19968;&#20010;conformer&#32534;&#30721;&#22120;&#34701;&#20837;&#21040;&#24320;&#28304;&#30340;LLaMA-7B&#20013;&#65292;&#20351;&#20854;&#22312;&#21333;&#19968;&#35821;&#35328;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#36229;&#36807;18%&#65292;&#24182;&#33021;&#22815;&#25191;&#34892;&#22810;&#35821;&#35328;&#35821;&#38899;&#35782;&#21035;&#65292;&#23613;&#31649;LLaMA&#30340;&#35757;&#32451;&#20027;&#35201;&#20381;&#36182;&#20110;&#33521;&#25991;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#35843;&#26597;LLM&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#21542;&#21487;&#20197;&#23436;&#20840;&#20923;&#32467;&#20197;&#20445;&#25345;&#20854;&#21407;&#26377;&#21151;&#33021;&#65292;&#20197;&#21450;&#25552;&#21319;&#38899;&#39057;&#32534;&#30721;&#22120;&#30340;&#35268;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have proven themselves highly flexible, able to solve a wide range of generative tasks, such as abstractive summarization and open-ended question answering. In this paper we extend the capabilities of LLMs by directly attaching a small audio encoder allowing it to perform speech recognition. By directly prepending a sequence of audial embeddings to the text token embeddings, the LLM can be converted to an automatic speech recognition (ASR) system, and be used in the exact same manner as its textual counterpart. Experiments on Multilingual LibriSpeech (MLS) show that incorporating a conformer encoder into the open sourced LLaMA-7B allows it to outperform monolingual baselines by 18% and perform multilingual speech recognition despite LLaMA being trained overwhelmingly on English text. Furthermore, we perform ablation studies to investigate whether the LLM can be completely frozen during training to maintain its original capabilities, scaling up the audio encoder, a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24212;&#29992;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#37327;&#23376;&#22686;&#24378;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(QLSTM)&#21487;&#20197;&#26356;&#24555;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#36719;&#20214;&#23454;&#29616;&#26041;&#38754;&#25509;&#36817;&#21476;&#20856;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11788</link><description>&lt;p&gt;
&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24212;&#29992;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Applying QNLP to sentiment analysis in finance. (arXiv:2307.11788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#37329;&#34701;&#34892;&#19994;&#20013;&#24212;&#29992;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#37327;&#23376;&#22686;&#24378;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(QLSTM)&#21487;&#20197;&#26356;&#24555;&#22320;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#36719;&#20214;&#23454;&#29616;&#26041;&#38754;&#25509;&#36817;&#21476;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#20010;&#39046;&#22495;&#65292;&#21363;&#20351;&#26159;&#26368;&#24494;&#23567;&#30340;&#36136;&#37327;&#25913;&#36827;&#20063;&#33021;&#20135;&#29983;&#24040;&#22823;&#20215;&#20540;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#37329;&#34701;&#26159;&#26089;&#26399;&#37327;&#23376;&#20248;&#21183;&#30340;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#32773;&#12290;&#22312;&#36805;&#36895;&#21457;&#23637;&#30340;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(QNLP)&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;DisCoCat&#21644;&#37327;&#23376;&#22686;&#24378;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;(QNLP)&#36825;&#20004;&#31181;&#20013;&#24515;&#26041;&#27861;&#22312;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#12290;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ChatGPT&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#21253;&#21547;1000&#22810;&#20010;&#30495;&#23454;&#21477;&#23376;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;QLSTM&#30340;&#35757;&#32451;&#36895;&#24230;&#27604;DisCoCat&#24555;&#24471;&#22810;&#65292;&#24182;&#19988;&#22312;&#21487;&#29992;&#30340;&#36719;&#20214;&#23454;&#29616;&#20013;&#20063;&#25509;&#36817;&#21476;&#20856;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an application domain where the slightest qualitative improvements can yield immense value, finance is a promising candidate for early quantum advantage. Focusing on the rapidly advancing field of Quantum Natural Language Processing (QNLP), we explore the practical applicability of the two central approaches DisCoCat and Quantum-Enhanced Long Short-Term Memory (QLSTM) to the problem of sentiment analysis in finance. Utilizing a novel ChatGPT-based data generation approach, we conduct a case study with more than 1000 realistic sentences and find that QLSTMs can be trained substantially faster than DisCoCat while also achieving close to classical results for their available software implementations.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;</title><link>http://arxiv.org/abs/2307.11787</link><description>&lt;p&gt;
LLM&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#26377;&#25152;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
LLM Cognitive Judgements Differ From Human. (arXiv:2307.11787v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11787
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35748;&#30693;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#12289;&#20225;&#19994;&#21644;&#28040;&#36153;&#32773;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#34429;&#28982;&#36825;&#31867;&#27169;&#22411;&#30340;&#35821;&#35328;&#33021;&#21147;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#23427;&#20204;&#20316;&#20026;&#35748;&#30693;&#20027;&#20307;&#30340;&#35843;&#26597;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#23545;GPT-3&#21644;ChatGPT&#22312;&#19968;&#20010;&#26469;&#33258;&#35748;&#30693;&#31185;&#23398;&#25991;&#29486;&#30340;&#26377;&#38480;&#25968;&#25454;&#24402;&#32435;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35748;&#30693;&#21028;&#26029;&#19982;&#20154;&#31867;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have lately been on the spotlight of researchers, businesses, and consumers alike. While the linguistic capabilities of such models have been studied extensively, there is growing interest in investigating them as cognitive subjects. In the present work I examine GPT-3 and ChatGPT capabilities on an limited-data inductive reasoning task from the cognitive science literature. The results suggest that these models' cognitive judgements are not human-like.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#23545;&#25239;&#23545;&#35805;&#22609;&#36896;&#26469;&#22686;&#24378;&#26234;&#33021;&#23545;&#35805;&#20195;&#29702;&#30340;&#20004;&#20010;&#27169;&#22411;&#65306;GANPG&#21644;REGS&#12290;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25913;&#36827;&#24403;&#21069;&#30340;&#33258;&#21160;&#25320;&#21495;&#31995;&#32479;&#65292;&#25552;&#39640;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11785</link><description>&lt;p&gt;
&#26234;&#33021;&#20195;&#29702;&#30340;&#23545;&#25239;&#23545;&#35805;&#22609;&#36896;
&lt;/p&gt;
&lt;p&gt;
Adversarial Conversational Shaping for Intelligent Agents. (arXiv:2307.11785v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#23545;&#25239;&#23545;&#35805;&#22609;&#36896;&#26469;&#22686;&#24378;&#26234;&#33021;&#23545;&#35805;&#20195;&#29702;&#30340;&#20004;&#20010;&#27169;&#22411;&#65306;GANPG&#21644;REGS&#12290;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#25913;&#36827;&#24403;&#21069;&#30340;&#33258;&#21160;&#25320;&#21495;&#31995;&#32479;&#65292;&#25552;&#39640;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20986;&#29616;&#20351;&#24471;&#30740;&#31350;&#30028;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#33258;&#21160;&#25320;&#21495;&#31995;&#32479;&#20173;&#28982;&#19981;&#31283;&#23450;&#19988;&#19981;&#20934;&#30830;&#65306;&#25991;&#26412;&#29983;&#25104;&#22120;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#33021;&#20250;&#36831;&#38045;&#24182;&#35823;&#35299;&#20154;&#31867;&#23545;&#35805;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23427;&#20204;&#36890;&#36807;&#23545;&#25239;&#23545;&#35805;&#22609;&#36896;&#26469;&#22686;&#24378;&#26234;&#33021;&#23545;&#35805;&#20195;&#29702;&#65306;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANPG&#65289;&#21644;&#22522;&#20110;Li&#31561;&#20154;&#25552;&#20986;&#30340;REGS&#27169;&#22411;&#30340;&#27599;&#19968;&#20195;&#29983;&#25104;&#27493;&#39588;&#37117;&#26377;&#22870;&#21169;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;REGS&#65289;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20026;&#37096;&#20998;&#21644;&#23436;&#25972;&#30340;&#29983;&#25104;&#25991;&#26412;&#24207;&#21015;&#20998;&#37197;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#35752;&#35770;&#20102;&#20351;&#29992;&#19981;&#21516;&#35757;&#32451;&#32454;&#33410;&#30340;&#24615;&#33021;&#65306;seq2seq [36]&#21644;transformers [37]&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent emergence of deep learning methods has enabled the research community to achieve state-of-the art results in several domains including natural language processing. However, the current robocall system remains unstable and inaccurate: text generator and chat-bots can be tedious and misunderstand human-like dialogue. In this work, we study the performance of two models able to enhance an intelligent conversational agent through adversarial conversational shaping: a generative adversarial network with policy gradient (GANPG) and a generative adversarial network with reward for every generation step (REGS) based on the REGS model presented in Li et al. [18] . This model is able to assign rewards to both partially and fully generated text sequences. We discuss performance with different training details : seq2seq [ 36] and transformers [37 ] in a reinforcement learning framework.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#25552;&#21462;-&#25688;&#35201;&#36724;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#34913;&#37327;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#20869;&#23481;&#30340;"&#20511;&#29992;"&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#24320;&#21457;&#30456;&#24212;&#24230;&#37327;&#26631;&#20934;&#12289;&#25968;&#25454;&#38598;&#21644;&#27880;&#37322;&#25351;&#21335;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2307.11779</link><description>&lt;p&gt;
Generate&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#21462;-&#25688;&#35201;&#36724;:&#27979;&#37327;&#20869;&#23481;"&#20511;&#29992;"
&lt;/p&gt;
&lt;p&gt;
The Extractive-Abstractive Axis: Measuring Content "Borrowing" in Generative Language Models. (arXiv:2307.11779v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#25552;&#21462;-&#25688;&#35201;&#36724;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#34913;&#37327;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#20869;&#23481;&#30340;"&#20511;&#29992;"&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#24320;&#21457;&#30456;&#24212;&#24230;&#37327;&#26631;&#20934;&#12289;&#25968;&#25454;&#38598;&#21644;&#27880;&#37322;&#25351;&#21335;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#35774;&#35745;&#20135;&#29983;&#39640;&#24230;&#25688;&#35201;&#30340;&#36755;&#20986;&#65292;&#19982;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#25552;&#21462;&#24335;&#21709;&#24212;&#24418;&#25104;&#23545;&#27604;&#12290;&#37492;&#20110;LLMs&#30340;&#36825;&#19968;&#29305;&#28857;&#21450;&#20854;&#23545;&#20869;&#23481;&#35768;&#21487;&#21644;&#24402;&#23646;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25152;&#35859;&#30340;&#25552;&#21462;-&#25688;&#35201;&#36724;&#65292;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#24378;&#35843;&#24320;&#21457;&#30456;&#24212;&#30340;&#24230;&#37327;&#26631;&#20934;&#12289;&#25968;&#25454;&#38598;&#21644;&#27880;&#37322;&#25351;&#21335;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#23558;&#35752;&#35770;&#38480;&#21046;&#22312;&#25991;&#26412;&#24418;&#24335;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative language models produce highly abstractive outputs by design, in contrast to extractive responses in search engines. Given this characteristic of LLMs and the resulting implications for content Licensing &amp; Attribution, we propose the the so-called Extractive-Abstractive axis for benchmarking generative models and highlight the need for developing corresponding metrics, datasets and annotation guidelines. We limit our discussion to the text modality.
&lt;/p&gt;</description></item><item><title>Transsion TSUP&#22242;&#38431;&#24320;&#21457;&#20102;&#19968;&#31181;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#22312;ASRU 2023 MADASR&#25361;&#25112;&#36187;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#33021;&#22815;&#36866;&#24212;&#20302;&#36164;&#28304;&#30340;&#21360;&#24230;&#35821;&#35328;&#12290;&#35813;&#31995;&#32479;&#38024;&#23545;&#25361;&#25112;&#30340;&#22235;&#20010;&#36319;&#36394;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#24182;&#37319;&#29992;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#21644;&#35299;&#30721;&#31574;&#30053;&#12290;&#23545;&#20110;&#23391;&#21152;&#25289;&#35821;&#65292;&#21462;&#24471;&#20102;&#36739;&#20302;&#30340;&#35789;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.11778</link><description>&lt;p&gt;
Transsion TSUP&#23545;ASRU 2023 MADASR&#25361;&#25112;&#36187;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Transsion TSUP's speech recognition system for ASRU 2023 MADASR Challenge. (arXiv:2307.11778v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11778
&lt;/p&gt;
&lt;p&gt;
Transsion TSUP&#22242;&#38431;&#24320;&#21457;&#20102;&#19968;&#31181;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#22312;ASRU 2023 MADASR&#25361;&#25112;&#36187;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#33021;&#22815;&#36866;&#24212;&#20302;&#36164;&#28304;&#30340;&#21360;&#24230;&#35821;&#35328;&#12290;&#35813;&#31995;&#32479;&#38024;&#23545;&#25361;&#25112;&#30340;&#22235;&#20010;&#36319;&#36394;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#24182;&#37319;&#29992;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#21644;&#35299;&#30721;&#31574;&#30053;&#12290;&#23545;&#20110;&#23391;&#21152;&#25289;&#35821;&#65292;&#21462;&#24471;&#20102;&#36739;&#20302;&#30340;&#35789;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#30001;Transsion&#35821;&#38899;&#29702;&#35299;&#22788;&#29702;&#22242;&#38431;&#65288;TSUP&#65289;&#24320;&#21457;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#29992;&#20110;ASRU 2023 MADASR&#25361;&#25112;&#36187;&#12290;&#35813;&#31995;&#32479;&#19987;&#27880;&#20110;&#36866;&#24212;&#20302;&#36164;&#28304;&#30340;&#21360;&#24230;&#35821;&#35328;&#65292;&#24182;&#28085;&#30422;&#25361;&#25112;&#30340;&#22235;&#20010;&#36319;&#36394;&#12290;&#22312;&#36319;&#36394;1&#21644;2&#20013;&#65292;&#22768;&#23398;&#27169;&#22411;&#37319;&#29992;&#20102;squeezeformer&#32534;&#30721;&#22120;&#21644;&#21452;&#21521;&#21464;&#25442;&#22120;&#35299;&#30721;&#22120;&#65292;&#37319;&#29992;&#32852;&#21512;CTC-Attention&#35757;&#32451;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#22312;TLG&#27874;&#26463;&#25628;&#32034;&#35299;&#30721;&#26399;&#38388;&#20351;&#29992;&#20102;&#22806;&#37096;&#30340;KenLM&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#36319;&#36394;3&#21644;4&#20013;&#65292;&#20351;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;IndicWhisper&#27169;&#22411;&#65292;&#24182;&#22312;&#25361;&#25112;&#25968;&#25454;&#38598;&#21644;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#20854;whisper&#27874;&#26463;&#25628;&#32034;&#35299;&#30721;&#20063;&#34987;&#20462;&#25913;&#20197;&#25903;&#25345;&#22806;&#37096;&#30340;KenLM&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#25361;&#25112;&#25152;&#25552;&#20379;&#30340;&#38468;&#21152;&#25991;&#26412;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#36319;&#36394;&#20013;&#23545;&#23391;&#21152;&#25289;&#35821;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20998;&#21035;&#20026;24.17&#65285;&#12289;24.43&#65285;&#12289;15.97&#65285;&#21644;15.97&#65285;&#65292;&#23545;&#21360;&#22320;&#35821;&#30340;WER&#20998;&#21035;&#20026;19.61&#65285;&#12289;19.54&#65285;&#12289;15.48&#65285;
&lt;/p&gt;
&lt;p&gt;
This paper presents a speech recognition system developed by the Transsion Speech Understanding Processing Team (TSUP) for the ASRU 2023 MADASR Challenge. The system focuses on adapting ASR models for low-resource Indian languages and covers all four tracks of the challenge. For tracks 1 and 2, the acoustic model utilized a squeezeformer encoder and bidirectional transformer decoder with joint CTC-Attention training loss. Additionally, an external KenLM language model was used during TLG beam search decoding. For tracks 3 and 4, pretrained IndicWhisper models were employed and finetuned on both the challenge dataset and publicly available datasets. The whisper beam search decoding was also modified to support an external KenLM language model, which enabled better utilization of the additional text provided by the challenge. The proposed method achieved word error rates (WER) of 24.17%, 24.43%, 15.97%, and 15.97% for Bengali language in the four tracks, and WER of 19.61%, 19.54%, 15.48%
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23884;&#20837;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#65292;&#23884;&#20837;&#23618;&#27425;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#21644;&#38754;&#21521;&#26102;&#38388;&#30340;&#21160;&#24577;&#23884;&#20837;&#19977;&#31181;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#22024;&#26434;&#30340;&#22823;&#25968;&#25454;&#29615;&#22659;&#20013;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#20027;&#39064;&#25552;&#21462;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.11775</link><description>&lt;p&gt;
&#25429;&#25417;&#31038;&#20132;&#23186;&#20307;&#20013;&#23458;&#25143;&#35265;&#35299;&#30340;&#20027;&#39064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Topical Approach to Capturing Customer Insight In Social Media. (arXiv:2307.11775v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23884;&#20837;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#65292;&#23884;&#20837;&#23618;&#27425;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#21644;&#38754;&#21521;&#26102;&#38388;&#30340;&#21160;&#24577;&#23884;&#20837;&#19977;&#31181;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#22312;&#22024;&#26434;&#30340;&#22823;&#25968;&#25454;&#29615;&#22659;&#20013;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#20027;&#39064;&#25552;&#21462;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#26102;&#20195;&#20026;&#20225;&#19994;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#36825;&#31181;&#32321;&#33635;&#30340;&#20449;&#24687;&#36130;&#23500;&#36229;&#20986;&#20102;&#20256;&#32479;&#33829;&#38144;&#30740;&#31350;&#30340;&#28192;&#36947;&#21644;&#26694;&#26550;&#65292;&#21253;&#25324;&#33829;&#38144;&#32452;&#21512;&#24314;&#27169;(MMM)&#12290;&#29305;&#21035;&#26159;&#65292;&#25991;&#26412;&#25968;&#25454;&#25552;&#20986;&#20102;&#35768;&#22810;&#25968;&#25454;&#20998;&#26512;&#20174;&#19994;&#20154;&#21592;&#24517;&#39035;&#24212;&#23545;&#30340;&#25361;&#25112;&#12290;&#31038;&#20132;&#23186;&#20307;&#26500;&#25104;&#20102;&#22823;&#35268;&#27169;&#12289;&#24322;&#26500;&#21644;&#22024;&#26434;&#30340;&#25991;&#26723;&#26469;&#28304;&#12290;&#24037;&#19994;&#25968;&#25454;&#37319;&#38598;&#36807;&#31243;&#21253;&#25324;&#19968;&#23450;&#37327;&#30340;ETL&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#20013;&#22122;&#22768;&#30340;&#21464;&#24322;&#24615;&#21644;&#19981;&#21516;&#26469;&#28304;&#24341;&#20837;&#30340;&#24322;&#26500;&#24615;&#32473;&#20104;&#20102;&#20020;&#26102;&#24037;&#20855;&#30340;&#38656;&#27714;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22312;&#23436;&#20840;&#26080;&#30417;&#30563;&#12289;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#25552;&#21462;&#23458;&#25143;&#35265;&#35299;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#22312;&#22024;&#26434;&#30340;&#22823;&#25968;&#25454;&#29615;&#22659;&#20013;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#20027;&#39064;&#25552;&#21462;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26694;&#26550;&#19978;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#65306;&#23884;&#20837;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#12289;&#23884;&#20837;&#23618;&#27425;&#29380;&#21033;&#20811;&#38647;&#36807;&#31243;&#21644;&#38754;&#21521;&#26102;&#38388;&#30340;&#21160;&#24577;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
The age of social media has opened new opportunities for businesses. This flourishing wealth of information is outside traditional channels and frameworks of classical marketing research, including that of Marketing Mix Modeling (MMM). Textual data, in particular, poses many challenges that data analysis practitioners must tackle. Social media constitute massive, heterogeneous, and noisy document sources. Industrial data acquisition processes include some amount of ETL. However, the variability of noise in the data and the heterogeneity induced by different sources create the need for ad-hoc tools. Put otherwise, customer insight extraction in fully unsupervised, noisy contexts is an arduous task. This research addresses the challenge of fully unsupervised topic extraction in noisy, Big Data contexts. We present three approaches we built on the Variational Autoencoder framework: the Embedded Dirichlet Process, the Embedded Hierarchical Dirichlet Process, and the time-aware Dynamic Embe
&lt;/p&gt;</description></item><item><title>AutoAlign&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25429;&#25417;&#35859;&#35789;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;TransE&#35745;&#31639;&#23454;&#20307;&#23884;&#20837;&#26469;&#23454;&#29616;&#23454;&#20307;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2307.11772</link><description>&lt;p&gt;
AutoAlign&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#33258;&#21160;&#26377;&#25928;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AutoAlign: Fully Automatic and Effective Knowledge Graph Alignment enabled by Large Language Models. (arXiv:2307.11772v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11772
&lt;/p&gt;
&lt;p&gt;
AutoAlign&#26159;&#19968;&#31181;&#20840;&#33258;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#23545;&#40784;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25429;&#25417;&#35859;&#35789;&#30456;&#20284;&#24615;&#65292;&#24182;&#20351;&#29992;TransE&#35745;&#31639;&#23454;&#20307;&#23884;&#20837;&#26469;&#23454;&#29616;&#23454;&#20307;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#38388;&#30340;&#23454;&#20307;&#23545;&#40784;&#20219;&#21153;&#26088;&#22312;&#35782;&#21035;&#20986;&#20004;&#20010;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20013;&#34920;&#31034;&#30456;&#21516;&#23454;&#20307;&#30340;&#27599;&#23545;&#23454;&#20307;&#12290;&#35768;&#22810;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#37117;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#65292;&#36825;&#26159;&#38750;&#24120;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21517;&#20026;AutoAlign&#30340;&#23436;&#20840;&#33258;&#21160;&#23545;&#40784;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#20219;&#20309;&#25163;&#24037;&#21046;&#20316;&#30340;&#31181;&#23376;&#23545;&#40784;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#35859;&#35789;&#23884;&#20837;&#65292;AutoAlign&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#35859;&#35789;&#36817;&#37051;&#22270;&#65292;&#33258;&#21160;&#25429;&#25417;&#20004;&#20010;&#30693;&#35782;&#22270;&#35889;&#20013;&#35859;&#35789;&#30340;&#30456;&#20284;&#24615;&#12290;&#23545;&#20110;&#23454;&#20307;&#23884;&#20837;&#65292;AutoAlign&#39318;&#20808;&#20351;&#29992;TransE&#29420;&#31435;&#35745;&#31639;&#27599;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#20307;&#23884;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;&#35745;&#31639;&#22522;&#20110;&#23454;&#20307;&#23646;&#24615;&#30340;&#23454;&#20307;&#30456;&#20284;&#24615;&#65292;&#23558;&#20004;&#20010;&#30693;&#35782;&#22270;&#35889;&#30340;&#23454;&#20307;&#23884;&#20837;&#31227;&#21160;&#21040;&#30456;&#21516;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#12290;&#22240;&#27492;&#65292;AutoAlign&#23454;&#29616;&#20102;&#35859;&#35789;&#23545;&#40784;&#21644;&#23454;&#20307;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of entity alignment between knowledge graphs (KGs) aims to identify every pair of entities from two different KGs that represent the same entity. Many machine learning-based methods have been proposed for this task. However, to our best knowledge, existing methods all require manually crafted seed alignments, which are expensive to obtain. In this paper, we propose the first fully automatic alignment method named AutoAlign, which does not require any manually crafted seed alignments. Specifically, for predicate embeddings, AutoAlign constructs a predicate-proximity-graph with the help of large language models to automatically capture the similarity between predicates across two KGs. For entity embeddings, AutoAlign first computes the entity embeddings of each KG independently using TransE, and then shifts the two KGs' entity embeddings into the same vector space by computing the similarity between entities based on their attributes. Thus, both predicate alignment and entity al
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#38598;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#23545;&#28385;&#24847;&#24230;&#35843;&#26597;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#35782;&#21035;&#37325;&#22797;&#30340;&#35789;&#35821;&#27169;&#24335;&#21644;&#21033;&#29992;&#24847;&#35265;&#25366;&#25496;&#26469;&#29702;&#35299;&#21442;&#19982;&#32773;&#30340;&#24847;&#35265;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#35789;&#35821;&#27169;&#24335;&#26469;&#33719;&#21462;&#26356;&#28145;&#20837;&#30340;&#24773;&#24863;&#12289;&#24847;&#35265;&#21644;&#20027;&#39064;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.11771</link><description>&lt;p&gt;
&#19968;&#31181;&#38598;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#29992;&#20110;&#28385;&#24847;&#24230;&#35843;&#26597;&#20013;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
an integrated npl approach to sentiment analysis in satisfaction surveys. (arXiv:2307.11771v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11771
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#38598;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#23545;&#28385;&#24847;&#24230;&#35843;&#26597;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65292;&#36890;&#36807;&#35782;&#21035;&#37325;&#22797;&#30340;&#35789;&#35821;&#27169;&#24335;&#21644;&#21033;&#29992;&#24847;&#35265;&#25366;&#25496;&#26469;&#29702;&#35299;&#21442;&#19982;&#32773;&#30340;&#24847;&#35265;&#65292;&#24182;&#19988;&#36890;&#36807;&#20998;&#26512;&#35789;&#35821;&#27169;&#24335;&#26469;&#33719;&#21462;&#26356;&#28145;&#20837;&#30340;&#24773;&#24863;&#12289;&#24847;&#35265;&#21644;&#20027;&#39064;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39033;&#30446;&#26088;&#22312;&#23558;&#38598;&#25104;&#26041;&#27861;&#24212;&#29992;&#20110;&#28385;&#24847;&#24230;&#35843;&#26597;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12290;&#23427;&#23558;&#30528;&#37325;&#20110;&#29702;&#35299;&#21644;&#25552;&#21462;&#35843;&#26597;&#22238;&#31572;&#20013;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#20998;&#26512;&#24773;&#24863;&#65292;&#35782;&#21035;&#37325;&#22797;&#30340;&#35789;&#35821;&#27169;&#24335;&#12290;&#23558;&#20351;&#29992;NLP&#25216;&#26415;&#26469;&#30830;&#23450;&#24773;&#24863;&#26497;&#24615;&#65292;&#23558;&#22238;&#31572;&#20998;&#31867;&#20026;&#31215;&#26497;&#12289;&#28040;&#26497;&#25110;&#20013;&#24615;&#31867;&#21035;&#65292;&#24182;&#21033;&#29992;&#24847;&#35265;&#25366;&#25496;&#26469;&#31361;&#20986;&#21442;&#19982;&#32773;&#30340;&#24847;&#35265;&#12290;&#35813;&#26041;&#27861;&#23558;&#26377;&#21161;&#20110;&#30830;&#23450;&#23545;&#21442;&#19982;&#32773;&#26368;&#30456;&#20851;&#30340;&#26041;&#38754;&#65292;&#24182;&#20102;&#35299;&#20182;&#20204;&#23545;&#36825;&#20123;&#29305;&#23450;&#26041;&#38754;&#30340;&#24847;&#35265;&#12290;&#35813;&#30740;&#31350;&#39033;&#30446;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#23558;&#26159;&#20351;&#29992;NPL&#23545;&#28385;&#24847;&#24230;&#35843;&#26597;&#22238;&#31572;&#20013;&#30340;&#35789;&#35821;&#27169;&#24335;&#36827;&#34892;&#20998;&#26512;&#12290;&#35813;&#20998;&#26512;&#23558;&#25552;&#20379;&#23545;&#22238;&#31572;&#32773;&#24773;&#24863;&#12289;&#24847;&#35265;&#20197;&#21450;&#20986;&#29616;&#30340;&#20027;&#39064;&#21644;&#36235;&#21183;&#30340;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#20174;&#35813;&#26041;&#27861;&#24471;&#21040;&#30340;&#32467;&#26524;&#21487;&#20197;&#29992;&#20110;&#30830;&#23450;&#25913;&#36827;&#30340;&#26041;&#21521;&#65292;&#20102;&#35299;&#22238;&#31572;&#32773;&#30340;&#20559;&#22909;&#65292;&#24182;&#20570;&#20986;&#25112;&#30053;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The research project aims to apply an integrated approach to natural language processing NLP to satisfaction surveys. It will focus on understanding and extracting relevant information from survey responses, analyzing feelings, and identifying recurring word patterns. NLP techniques will be used to determine emotional polarity, classify responses into positive, negative, or neutral categories, and use opinion mining to highlight participants opinions. This approach will help identify the most relevant aspects for participants and understand their opinions in relation to those specific aspects. A key component of the research project will be the analysis of word patterns in satisfaction survey responses using NPL. This analysis will provide a deeper understanding of feelings, opinions, and themes and trends present in respondents responses. The results obtained from this approach can be used to identify areas for improvement, understand respondents preferences, and make strategic decisi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#35268;&#27169;&#35745;&#31639;&#35780;&#20272;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;2D&#25991;&#26412;&#31354;&#38388;&#21270;&#30340;&#20027;&#39064;&#27169;&#22411;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#35821;&#26009;&#24211;&#20998;&#26512;&#25552;&#20379;&#20102;&#20855;&#26377;&#39640;&#36136;&#37327;&#24067;&#23616;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.11770</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35780;&#20272;&#29992;&#20110;2D&#25991;&#26412;&#31354;&#38388;&#21270;&#30340;&#20027;&#39064;&#27169;&#22411;&#21644;&#38477;&#32500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Evaluation of Topic Models and Dimensionality Reduction Methods for 2D Text Spatialization. (arXiv:2307.11770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11770
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#35268;&#27169;&#35745;&#31639;&#35780;&#20272;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29992;&#20110;2D&#25991;&#26412;&#31354;&#38388;&#21270;&#30340;&#20027;&#39064;&#27169;&#22411;&#21644;&#38477;&#32500;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#35821;&#26009;&#24211;&#20998;&#26512;&#25552;&#20379;&#20102;&#20855;&#26377;&#39640;&#36136;&#37327;&#24067;&#23616;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#26159;&#19968;&#31867;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30340;&#35821;&#20041;&#32467;&#26500;&#12290;&#19982;&#21518;&#32493;&#30340;&#38477;&#32500;&#31639;&#27861;&#19968;&#36215;&#65292;&#20027;&#39064;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#20026;&#25991;&#26412;&#35821;&#26009;&#24211;&#23548;&#20986;&#31354;&#38388;&#21270;&#30340;&#20108;&#32500;&#25955;&#28857;&#22270;&#65292;&#21453;&#26144;&#25991;&#26723;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24182;&#25903;&#25345;&#35821;&#26009;&#24211;&#20998;&#26512;&#12290;&#23613;&#31649;&#20027;&#39064;&#27169;&#22411;&#12289;&#38477;&#32500;&#31639;&#27861;&#21450;&#20854;&#24213;&#23618;&#36229;&#21442;&#25968;&#30340;&#36873;&#25321;&#23545;&#20135;&#29983;&#30340;&#24067;&#23616;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#21738;&#31181;&#29305;&#23450;&#32452;&#21512;&#33021;&#22815;&#24471;&#21040;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#24067;&#23616;&#65292;&#20934;&#30830;&#24230;&#21644;&#24863;&#30693;&#24230;&#25351;&#26631;&#26041;&#38754;&#12290;&#20026;&#20102;&#30740;&#31350;&#20027;&#39064;&#27169;&#22411;&#21644;&#38477;&#32500;&#26041;&#27861;&#22312;&#20316;&#20026;&#20108;&#32500;&#25955;&#28857;&#22270;&#30340;&#35821;&#26009;&#24211;&#31354;&#38388;&#21270;&#65288;&#25110;&#20316;&#20026;&#26223;&#35266;&#31867;&#22411;&#21487;&#35270;&#21270;&#30340;&#22522;&#30784;&#65289;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#20934;&#30340;&#22823;&#35268;&#27169;&#35745;&#31639;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21253;&#25324;&#65288;1&#65289;&#19968;&#32452;&#35821;&#26009;&#24211;&#65292;&#65288;2&#65289;&#19968;&#32452;&#24067;&#23616;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models are a class of unsupervised learning algorithms for detecting the semantic structure within a text corpus. Together with a subsequent dimensionality reduction algorithm, topic models can be used for deriving spatializations for text corpora as two-dimensional scatter plots, reflecting semantic similarity between the documents and supporting corpus analysis. Although the choice of the topic model, the dimensionality reduction, and their underlying hyperparameters significantly impact the resulting layout, it is unknown which particular combinations result in high-quality layouts with respect to accuracy and perception metrics. To investigate the effectiveness of topic models and dimensionality reduction methods for the spatialization of corpora as two-dimensional scatter plots (or basis for landscape-type visualizations), we present a large-scale, benchmark-based computational evaluation. Our evaluation consists of (1) a set of corpora, (2) a set of layout algorithms that a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#21270;&#21644;&#21322;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#36827;&#34892;&#20102;&#39046;&#22495;&#30693;&#35782;&#33976;&#39311;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#20182;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#39046;&#22495;&#26412;&#20307;&#26500;&#24314;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#20154;&#31867;&#30417;&#30563;&#21644;&#26089;&#26399;&#24178;&#39044;&#36890;&#24120;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#21644;&#36755;&#20986;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.11769</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#39046;&#22495;&#30693;&#35782;&#33976;&#39311;&#65306;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Domain Knowledge Distillation from Large Language Model: An Empirical Study in the Autonomous Driving Domain. (arXiv:2307.11769v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33258;&#21160;&#21270;&#21644;&#21322;&#33258;&#21160;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#36827;&#34892;&#20102;&#39046;&#22495;&#30693;&#35782;&#33976;&#39311;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#20182;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#39046;&#22495;&#26412;&#20307;&#26500;&#24314;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#20154;&#31867;&#30417;&#30563;&#21644;&#26089;&#26399;&#24178;&#39044;&#36890;&#24120;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#21644;&#36755;&#20986;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#31243;&#30693;&#35782;&#21270;&#65288;&#25110;&#19987;&#23478;&#65289;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#24037;&#20316;&#21644;&#39046;&#22495;&#30693;&#35782;&#12290;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20351;&#29992;&#22823;&#37327;&#36328;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#35757;&#32451;&#65292;&#22240;&#27492;&#21487;&#20197;&#33258;&#21160;&#21270;&#27492;&#31867;&#24037;&#31243;&#27969;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#24037;&#31243;&#21644;LLM ChatGPT&#30340;&#39046;&#22495;&#30693;&#35782;&#33976;&#39311;&#30340;&#23454;&#35777;&#33258;&#21160;&#21270;&#21644;&#21322;&#33258;&#21160;&#21270;&#26694;&#26550;&#65292;&#24182;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#24182;&#21576;&#29616;&#20102;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#29616;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;ChatGPT&#8220;&#32842;&#22825;&#8221;&#26469;&#26500;&#24314;&#39046;&#22495;&#30693;&#35782;&#26412;&#20307;&#35770;&#12290;&#20851;&#38190;&#21457;&#29616;&#26159;&#65292;&#34429;&#28982;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#39046;&#22495;&#26412;&#20307;&#26500;&#24314;&#26159;&#21487;&#33021;&#30340;&#65292;&#20294;&#20154;&#31867;&#30417;&#30563;&#21644;&#26089;&#26399;&#24178;&#39044;&#36890;&#24120;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#21644;&#36755;&#20986;&#36136;&#37327;&#65292;&#22240;&#20026;&#23427;&#20204;&#20943;&#23569;&#20102;&#21709;&#24212;&#38543;&#26426;&#24615;&#21644;&#34676;&#34678;&#25928;&#24212;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#32593;&#32476;&#30340;&#33976;&#39311;&#21161;&#25163;&#65292;&#20197;&#22312;&#36816;&#34892;&#26102;&#36827;&#34892;&#30417;&#30563;&#21644;&#28789;&#27963;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
Engineering knowledge-based (or expert) systems require extensive manual effort and domain knowledge. As Large Language Models (LLMs) are trained using an enormous amount of cross-domain knowledge, it becomes possible to automate such engineering processes. This paper presents an empirical automation and semi-automation framework for domain knowledge distillation using prompt engineering and the LLM ChatGPT. We assess the framework empirically in the autonomous driving domain and present our key observations. In our implementation, we construct the domain knowledge ontology by "chatting" with ChatGPT. The key finding is that while fully automated domain ontology construction is possible, human supervision and early intervention typically improve efficiency and output quality as they lessen the effects of response randomness and the butterfly effect. We, therefore, also develop a web-based distillation assistant enabling supervision and flexible intervention at runtime. We hope our find
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.11768</link><description>&lt;p&gt;
&#38382;&#39064;&#20998;&#35299;&#25552;&#39640;&#20102;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;
&lt;/p&gt;
&lt;p&gt;
Question Decomposition Improves the Faithfulness of Model-Generated Reasoning. (arXiv:2307.11768v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11768
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25191;&#34892;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#39564;&#35777;&#20854;&#34892;&#20026;&#30340;&#27491;&#30830;&#24615;&#21644;&#23433;&#20840;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#12290;&#20854;&#20013;&#19968;&#31181;&#35299;&#20915;&#26041;&#27861;&#26159;&#35201;&#27714;LLM&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#20197;&#36880;&#27493;&#25512;&#29702;&#30340;&#26041;&#24335;&#22806;&#21270;&#20854;&#25512;&#29702;&#36807;&#31243;&#65288;&#24605;&#32500;&#38142;&#65307;CoT&#65289;&#12290;&#25512;&#29702;&#36807;&#31243;&#21487;&#20197;&#35753;&#25105;&#20204;&#26816;&#26597;&#27169;&#22411;&#25191;&#34892;&#20219;&#21153;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#25152;&#38472;&#36848;&#30340;&#25512;&#29702;&#33021;&#22815;&#24544;&#23454;&#22320;&#21453;&#26144;&#27169;&#22411;&#30340;&#23454;&#38469;&#25512;&#29702;&#65292;&#32780;&#36825;&#24182;&#38750;&#24635;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#25552;&#39640;CoT&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#26469;&#29983;&#25104;&#25512;&#29702;&#12290;&#22522;&#20110;&#20998;&#35299;&#30340;&#26041;&#27861;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#25509;&#36817;CoT&#65292;&#24182;&#22312;&#20960;&#20010;&#26368;&#36817;&#25552;&#20986;&#30340;&#24230;&#37327;&#26631;&#20934;&#20013;&#25552;&#39640;&#20102;&#27169;&#22411;&#25152;&#38472;&#36848;&#25512;&#29702;&#30340;&#24544;&#23454;&#24230;&#12290;&#36890;&#36807;&#24378;&#21046;&#27169;&#22411;&#22312;&#21333;&#29420;&#30340;&#19978;&#19979;&#25991;&#20013;&#22238;&#31572;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25105;&#20204;&#22823;&#22823;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#24544;&#23454;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) perform more difficult tasks, it becomes harder to verify the correctness and safety of their behavior. One approach to help with this issue is to prompt LLMs to externalize their reasoning, e.g., by having them generate step-by-step reasoning as they answer a question (Chain-of-Thought; CoT). The reasoning may enable us to check the process that models use to perform tasks. However, this approach relies on the stated reasoning faithfully reflecting the model's actual reasoning, which is not always the case. To improve over the faithfulness of CoT reasoning, we have models generate reasoning by decomposing questions into subquestions. Decomposition-based methods achieve strong performance on question-answering tasks, sometimes approaching that of CoT while improving the faithfulness of the model's stated reasoning on several recently-proposed metrics. By forcing the model to answer simpler subquestions in separate contexts, we greatly increase the faithf
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35789;&#27719;&#25512;&#29702;&#20219;&#21153;MPC&#65292;&#36890;&#36807;&#24494;&#35843;BERT&#27169;&#22411;&#21644;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#31616;&#21270;&#26631;&#27880;&#36164;&#28304;&#30340;&#21516;&#26102;&#36798;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#19982;SentiWordNet&#30340;&#27604;&#36739;&#65292;&#36824;&#21457;&#29616;&#20102;MPC&#19982;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#20027;&#35266;&#24615;&#20998;&#31867;&#20219;&#21153;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.11767</link><description>&lt;p&gt;
&#22312;&#39640;&#25928;&#33258;&#21160;&#21270;&#30340;&#39118;&#26684;&#20013;&#35782;&#21035;&#24515;&#29702;&#24418;&#23481;&#35789;
&lt;/p&gt;
&lt;p&gt;
Recognition of Mental Adjectives in An Efficient and Automatic Style. (arXiv:2307.11767v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35789;&#27719;&#25512;&#29702;&#20219;&#21153;MPC&#65292;&#36890;&#36807;&#24494;&#35843;BERT&#27169;&#22411;&#21644;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#65292;&#22312;&#31616;&#21270;&#26631;&#27880;&#36164;&#28304;&#30340;&#21516;&#26102;&#36798;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#19982;SentiWordNet&#30340;&#27604;&#36739;&#65292;&#36824;&#21457;&#29616;&#20102;MPC&#19982;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#20027;&#35266;&#24615;&#20998;&#31867;&#20219;&#21153;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#24120;&#35782;&#25512;&#29702;&#22312;&#23398;&#26415;&#30028;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35789;&#27719;&#25512;&#29702;&#20219;&#21153;&#65292;&#24515;&#29702;&#19982;&#29289;&#29702;&#20998;&#31867;&#65288;MPC&#65289;&#65292;&#20197;&#22788;&#29702;&#24120;&#35782;&#25512;&#29702;&#12290;&#24515;&#29702;&#35789;&#35821;&#19982;&#24515;&#29702;&#27963;&#21160;&#30456;&#20851;&#65292;&#21487;&#20998;&#20026;&#20845;&#20010;&#31867;&#21035;&#65306;&#24773;&#24863;&#12289;&#38656;&#27714;&#12289;&#24863;&#30693;&#12289;&#25512;&#29702;&#12289;&#35268;&#21010;&#21644;&#20010;&#24615;&#12290;&#29289;&#29702;&#35789;&#35821;&#25551;&#36848;&#29289;&#20307;&#30340;&#29289;&#29702;&#23646;&#24615;&#65292;&#22914;&#39068;&#33394;&#12289;&#30828;&#24230;&#12289;&#36895;&#24230;&#21644;&#21487;&#22609;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;BERT&#27169;&#22411;&#23545;&#36825;&#20010;&#20219;&#21153;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#22312;&#35757;&#32451;&#26694;&#26550;&#20013;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#26469;&#20943;&#23569;&#25152;&#38656;&#30340;&#27880;&#37322;&#36164;&#28304;&#12290;&#20351;&#29992;ENTROPY&#31574;&#30053;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#20934;&#30830;&#24615;&#65292;&#20165;&#38656;&#35201;&#32422;300&#20010;&#26631;&#27880;&#30340;&#35789;&#35821;&#12290;&#25105;&#20204;&#36824;&#23558;&#32467;&#26524;&#19982;SentiWordNet&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20197;&#26816;&#26597;MPC&#19982;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#20027;&#35266;&#24615;&#20998;&#31867;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, commonsense reasoning has received more and more attention from academic community. We propose a new lexical inference task, Mental and Physical Classification (MPC), to handle commonsense reasoning in a reasoning graph. Mental words relate to mental activities, which fall into six categories: Emotion, Need, Perceiving, Reasoning, Planning and Personality. Physical words describe physical attributes of an object, like color, hardness, speed and malleability. A BERT model is fine-tuned for this task and active learning algorithm is adopted in the training framework to reduce the required annotation resources. The model using ENTROPY strategy achieves satisfactory accuracy and requires only about 300 labeled words. We also compare our result with SentiWordNet to check the difference between MPC and subjectivity classification task in sentiment analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#35780;&#20215;&#24615;&#35821;&#35328;&#34920;&#36798;&#36827;&#34892;&#19977;&#20803;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#19977;&#20803;&#20915;&#31574;&#21644;&#35780;&#20215;&#24615;&#35821;&#35328;&#34920;&#36798;&#29702;&#35770;&#20043;&#38388;&#30340;&#26032;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.11766</link><description>&lt;p&gt;
&#20351;&#29992;&#35780;&#20215;&#24615;&#35821;&#35328;&#34920;&#36798;&#36827;&#34892;&#19977;&#20803;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Three-way Decisions with Evaluative Linguistic Expressions. (arXiv:2307.11766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#35780;&#20215;&#24615;&#35821;&#35328;&#34920;&#36798;&#36827;&#34892;&#19977;&#20803;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20102;&#19977;&#20803;&#20915;&#31574;&#21644;&#35780;&#20215;&#24615;&#35821;&#35328;&#34920;&#36798;&#29702;&#35770;&#20043;&#38388;&#30340;&#26032;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#19977;&#20803;&#20915;&#31574;&#30340;&#35821;&#35328;&#35299;&#37322;&#65292;&#20854;&#20013;&#25509;&#21463;&#12289;&#25298;&#32477;&#21644;&#19981;&#25215;&#35834;&#30340;&#21306;&#22495;&#26159;&#36890;&#36807;&#20351;&#29992;&#35780;&#20215;&#24615;&#35821;&#35328;&#34920;&#36798;&#26500;&#24314;&#30340;&#65292;&#36825;&#20123;&#34920;&#36798;&#26159;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#34920;&#36798;&#65292;&#27604;&#22914;&#23567;&#12289;&#20013;&#12289;&#38750;&#24120;&#30701;&#12289;&#30456;&#24403;&#31895;&#31961;&#24378;&#28872;&#12289;&#26497;&#22909;&#31561;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#20986;&#20102;&#20004;&#20010;&#19981;&#21516;&#30740;&#31350;&#39046;&#22495;&#20043;&#38388;&#30340;&#26032;&#32852;&#31995;&#65306;&#19977;&#20803;&#20915;&#31574;&#21644;&#35780;&#20215;&#24615;&#35821;&#35328;&#34920;&#36798;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a linguistic interpretation of three-way decisions, where the regions of acceptance, rejection, and non-commitment are constructed by using the so-called evaluative linguistic expressions, which are expressions of natural language such as small, medium, very short, quite roughly strong, extremely good, etc. Our results highlight new connections between two different research areas: three-way decisions and the theory of evaluative linguistic expressions.
&lt;/p&gt;</description></item><item><title>Sensi-BERT&#26159;&#19968;&#31181;&#38754;&#21521;&#25935;&#24863;&#24230;&#39537;&#21160;&#30340;&#21442;&#25968;&#39640;&#25928;BERT&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#25935;&#24863;&#24230;&#20998;&#26512;&#21644;&#35009;&#21098;&#21442;&#25968;&#24352;&#37327;&#65292;&#21487;&#29983;&#25104;&#36866;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#39640;&#24230;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.11764</link><description>&lt;p&gt;
Sensi-BERT: &#38754;&#21521;&#25935;&#24863;&#24230;&#39537;&#21160;&#30340;&#21442;&#25968;&#39640;&#25928;BERT&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Sensi-BERT: Towards Sensitivity Driven Fine-Tuning for Parameter-Efficient BERT. (arXiv:2307.11764v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11764
&lt;/p&gt;
&lt;p&gt;
Sensi-BERT&#26159;&#19968;&#31181;&#38754;&#21521;&#25935;&#24863;&#24230;&#39537;&#21160;&#30340;&#21442;&#25968;&#39640;&#25928;BERT&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#25935;&#24863;&#24230;&#20998;&#26512;&#21644;&#35009;&#21098;&#21442;&#25968;&#24352;&#37327;&#65292;&#21487;&#29983;&#25104;&#36866;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#39640;&#24230;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#22312;&#25991;&#26412;&#20998;&#31867;&#21644;&#38382;&#31572;&#31561;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#25913;&#36827;&#34920;&#29616;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36880;&#28176;&#21463;&#21040;&#20851;&#27880;&#65292;&#21482;&#38656;&#36827;&#34892;&#24456;&#23569;&#27425;&#25968;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#20854;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#24120;&#24120;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;&#12290;&#29616;&#26377;&#30340;&#21442;&#25968;&#39640;&#25928;BERT&#27169;&#22411;&#35299;&#20915;&#26041;&#26696;&#22823;&#22810;&#20381;&#36182;&#20110;&#35745;&#31639;&#23494;&#38598;&#30340;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#24182;&#19988;&#24120;&#24120;&#20381;&#36182;&#20110;&#39069;&#22806;&#30340;&#35745;&#31639;&#23494;&#38598;&#22411;&#27169;&#22411;&#26469;&#24357;&#34917;&#24615;&#33021;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Sensi-BERT&#65292;&#19968;&#31181;&#25935;&#24863;&#24230;&#39537;&#21160;&#30340;BERT&#27169;&#22411;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#65292;&#29983;&#25104;&#36866;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#39640;&#24230;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36827;&#34892;&#25935;&#24863;&#24230;&#20998;&#26512;&#20197;&#23545;&#27599;&#20010;&#21333;&#29420;&#30340;&#21442;&#25968;&#24352;&#37327;&#36827;&#34892;&#25490;&#24207;&#65292;&#28982;&#21518;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#26681;&#25454;&#32473;&#23450;&#30340;&#21442;&#25968;&#25110;FLOPs&#39044;&#31639;&#36827;&#34892;&#30456;&#24212;&#30340;&#35009;&#21098;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;Sensi-BERT&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models have recently gained significant traction due to their improved performance on various down-stream tasks like text classification and question answering, requiring only few epochs of fine-tuning. However, their large model sizes often prohibit their applications on resource-constrained edge devices. Existing solutions of yielding parameter-efficient BERT models largely rely on compute-exhaustive training and fine-tuning. Moreover, they often rely on additional compute heavy models to mitigate the performance gap. In this paper, we present Sensi-BERT, a sensitivity driven efficient fine-tuning of BERT models that can take an off-the-shelf pre-trained BERT model and yield highly parameter-efficient models for downstream tasks. In particular, we perform sensitivity analysis to rank each individual parameter tensor, that then is used to trim them accordingly during fine-tuning for a given parameter or FLOPs budget. Our experiments show the efficacy of Sens
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#35760;&#24518;&#22686;&#24378;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20219;&#21153;&#20043;&#38388;&#24314;&#31435;&#21452;&#21521;&#20869;&#23384;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#25191;&#34892;&#25991;&#26723;&#32423;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;BioCreative V CDR&#35821;&#26009;&#24211;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11762</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#35760;&#24518;&#22686;&#24378;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Similarity-based Memory Enhanced Joint Entity and Relation Extraction. (arXiv:2307.11762v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#35760;&#24518;&#22686;&#24378;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20219;&#21153;&#20043;&#38388;&#24314;&#31435;&#21452;&#21521;&#20869;&#23384;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#26356;&#20934;&#30830;&#22320;&#25191;&#34892;&#25991;&#26723;&#32423;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;BioCreative V CDR&#35821;&#26009;&#24211;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#32423;&#32852;&#21512;&#23454;&#20307;&#21644;&#20851;&#31995;&#25277;&#21462;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20449;&#24687;&#25552;&#21462;&#38382;&#39064;&#65292;&#38656;&#35201;&#19968;&#20010;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#22312;&#20854;&#20013;&#19968;&#20010;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#25191;&#34892;&#22235;&#20010;&#23376;&#20219;&#21153;&#65306;&#25552;&#21450;&#26816;&#27979;&#65292;&#20849;&#25351;&#35299;&#26512;&#65292;&#23454;&#20307;&#20998;&#31867;&#21644;&#20851;&#31995;&#25277;&#21462;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#37319;&#29992;&#39034;&#24207;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20854;&#20013;&#20219;&#24847;&#20998;&#35299;&#23548;&#33268;&#24403;&#21069;&#20219;&#21153;&#20165;&#20381;&#36182;&#20110;&#21069;&#19968;&#20010;&#20219;&#21153;&#65292;&#24573;&#30053;&#20102;&#23427;&#20204;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#30340;&#26356;&#22797;&#26434;&#20851;&#31995;&#30340;&#21487;&#33021;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;&#21521;&#20869;&#23384;&#20381;&#36182;&#20851;&#31995;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#24182;&#26356;&#20934;&#30830;&#22320;&#25191;&#34892;&#32852;&#21512;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#22312;BioCreative V CDR&#35821;&#26009;&#24211;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-level joint entity and relation extraction is a challenging information extraction problem that requires a unified approach where a single neural network performs four sub-tasks: mention detection, coreference resolution, entity classification, and relation extraction. Existing methods often utilize a sequential multi-task learning approach, in which the arbitral decomposition causes the current task to depend only on the previous one, missing the possible existence of the more complex relationships between them. In this paper, we present a multi-task learning framework with bidirectional memory-like dependency between tasks to address those drawbacks and perform the joint problem more accurately. Our empirical studies show that the proposed approach outperforms the existing methods and achieves state-of-the-art results on the BioCreative V CDR corpus.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#22312;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#20013;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#25351;&#23548;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#34917;&#20805;&#65292;ChatGPT&#33021;&#22815;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#23569;&#33267;&#20256;&#32479;&#27169;&#22411;&#30340;1/40&#65292;&#34920;&#29616;&#20248;&#24322;&#65292;&#23588;&#20854;&#25797;&#38271;&#20943;&#23567;&#35823;&#25253;&#24182;&#25552;&#21319;&#20844;&#24179;&#24615;&#12290;&#36825;&#20026;&#26410;&#26469;&#22312;&#20854;&#20182;&#31867;&#20284;&#20219;&#21153;&#20013;&#20805;&#20998;&#21033;&#29992;ChatGPT&#30340;&#33021;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2307.11761</link><description>&lt;p&gt;
ChatGPT&#30340;&#20844;&#24179;&#24615;&#21450;&#21487;&#35299;&#37322;&#24341;&#23548;&#25552;&#31034;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Fairness of ChatGPT and the Role Of Explainable-Guided Prompts. (arXiv:2307.11761v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11761
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#22312;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#20013;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#25351;&#23548;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#34917;&#20805;&#65292;ChatGPT&#33021;&#22815;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#20351;&#29992;&#30340;&#25968;&#25454;&#37327;&#23569;&#33267;&#20256;&#32479;&#27169;&#22411;&#30340;1/40&#65292;&#34920;&#29616;&#20248;&#24322;&#65292;&#23588;&#20854;&#25797;&#38271;&#20943;&#23567;&#35823;&#25253;&#24182;&#25552;&#21319;&#20844;&#24179;&#24615;&#12290;&#36825;&#20026;&#26410;&#26469;&#22312;&#20854;&#20182;&#31867;&#20284;&#20219;&#21153;&#20013;&#20805;&#20998;&#21033;&#29992;ChatGPT&#30340;&#33021;&#21147;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20449;&#29992;&#39118;&#38505;&#35780;&#20272;&#20013;&#30340;&#28508;&#21147;&#65292;&#20855;&#20307;&#26469;&#35828;&#26159;OpenAI&#30340;GPT&#65292;&#22312;&#19968;&#20010;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;LLMs&#21463;&#21040;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#25351;&#23548;&#24182;&#34917;&#20805;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26102;&#65292;&#20854;&#34920;&#29616;&#21487;&#20197;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#23218;&#32654;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#20182;&#20204;&#21482;&#20351;&#29992;&#20102;&#23569;&#24471;&#22810;&#30340;&#25968;&#25454;-&#20165;&#20165;20&#20010;&#25968;&#25454;&#28857;&#65292;&#32780;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;800&#20010;&#25968;&#25454;&#28857;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;LLMs&#22312;&#20943;&#23567;&#35823;&#25253;&#25552;&#21319;&#20844;&#24179;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#20004;&#20010;&#26041;&#38754;&#22312;&#39118;&#38505;&#20998;&#26512;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#32467;&#26524;&#27809;&#26377;&#36229;&#36807;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#24378;&#35843;&#20102;LLMs&#22312;&#31867;&#20284;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#20026;&#26410;&#26469;&#22312;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#22880;&#23450;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our research investigates the potential of Large-scale Language Models (LLMs), specifically OpenAI's GPT, in credit risk assessment-a binary classification task. Our findings suggest that LLMs, when directed by judiciously designed prompts and supplemented with domain-specific knowledge, can parallel the performance of traditional Machine Learning (ML) models. Intriguingly, they achieve this with significantly less data-40 times less, utilizing merely 20 data points compared to the ML's 800. LLMs particularly excel in minimizing false positives and enhancing fairness, both being vital aspects of risk analysis. While our results did not surpass those of classical ML models, they underscore the potential of LLMs in analogous tasks, laying a groundwork for future explorations into harnessing the capabilities of LLMs in diverse ML tasks.
&lt;/p&gt;</description></item><item><title>EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.11760</link><description>&lt;p&gt;
EmotionPrompt: &#36890;&#36807;&#24773;&#24863;&#21050;&#28608;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#24515;&#29702;&#23398;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt: Leveraging Psychology for Large Language Models Enhancement via Emotional Stimulus. (arXiv:2307.11760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11760
&lt;/p&gt;
&lt;p&gt;
EmotionPrompt&#26159;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#25552;&#21319;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#39033;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#21516;&#26102;&#25913;&#21892;&#20102;&#20854;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25512;&#29702;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#31561;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#24182;&#34987;&#35270;&#20026;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#25552;&#31034;&#30340;&#25935;&#24863;&#24615;&#20173;&#28982;&#26159;&#20854;&#26085;&#24120;&#24212;&#29992;&#30340;&#20027;&#35201;&#29942;&#39048;&#12290;&#26412;&#25991;&#20174;&#24515;&#29702;&#23398;&#20013;&#27762;&#21462;&#28789;&#24863;&#65292;&#25552;&#20986;&#20102;EmotionPrompt&#26469;&#25506;&#32034;&#24773;&#24863;&#26234;&#33021;&#20197;&#25552;&#21319;LLMs&#30340;&#24615;&#33021;&#12290;EmotionPrompt&#22522;&#20110;&#19968;&#20010;&#38750;&#24120;&#31616;&#21333;&#26126;&#20102;&#30340;&#21407;&#21017;&#65306;&#23558;&#24773;&#24863;&#21050;&#28608;&#34701;&#20837;&#21040;&#25552;&#31034;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#30340;&#21333;&#19968;&#25552;&#31034;&#27169;&#26495;&#19978;&#65292;&#19982;&#21407;&#22987;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;Zero-shot-CoT&#30456;&#27604;&#65292;&#22312;8&#20010;&#20219;&#21153;&#19978;&#37117;&#26174;&#33879;&#20248;&#20110;&#22810;&#31181;&#27169;&#22411;&#65306;ChatGPT&#12289;Vicuna-13b&#12289;Bloom&#21644;T5&#12290;&#27492;&#22806;&#65292;&#35266;&#23519;&#21040;EmotionPrompt&#33021;&#22815;&#25552;&#39640;&#30495;&#23454;&#24615;&#21644;&#20449;&#24687;&#37327;&#12290;&#25105;&#20204;&#30456;&#20449;EmotionPrompt&#20026;&#25506;&#32034;&#36328;&#23398;&#31185;&#30693;&#35782;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved significant performance in many fields such as reasoning, language understanding, and math problem-solving, and are regarded as a crucial step to artificial general intelligence (AGI). However, the sensitivity of LLMs to prompts remains a major bottleneck for their daily adoption. In this paper, we take inspiration from psychology and propose EmotionPrompt to explore emotional intelligence to enhance the performance of LLMs. EmotionPrompt operates on a remarkably straightforward principle: the incorporation of emotional stimulus into prompts. Experimental results demonstrate that our \method, using the same single prompt templates, significantly outperforms original zero-shot prompt and Zero-shot-CoT on 8 tasks with diverse models: ChatGPT, Vicuna-13b, Bloom, and T5. Further, EmotionPrompt was observed to improve both truthfulness and informativeness. We believe that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledg
&lt;/p&gt;</description></item><item><title>CausE&#26159;&#19968;&#20010;&#37319;&#29992;&#22240;&#26524;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21644;&#23884;&#20837;&#35299;&#32544;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#24178;&#39044;&#36827;&#34892;&#31283;&#23450;&#39044;&#27979;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#23436;&#25972;&#24615;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11610</link><description>&lt;p&gt;
CausE: &#26397;&#21521;&#22240;&#26524;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
CausE: Towards Causal Knowledge Graph Embedding. (arXiv:2307.11610v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11610
&lt;/p&gt;
&lt;p&gt;
CausE&#26159;&#19968;&#20010;&#37319;&#29992;&#22240;&#26524;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#21644;&#23884;&#20837;&#35299;&#32544;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#24178;&#39044;&#36827;&#34892;&#31283;&#23450;&#39044;&#27979;&#65292;&#24182;&#22312;&#30693;&#35782;&#22270;&#35889;&#23436;&#25972;&#24615;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#30340;&#37325;&#28857;&#26159;&#23558;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#34920;&#31034;&#20026;&#36830;&#32493;&#30340;&#21521;&#37327;&#31354;&#38388;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#32570;&#22833;&#30340;&#19977;&#20803;&#32452;&#20197;&#23454;&#29616;&#30693;&#35782;&#22270;&#35889;&#23436;&#25972;&#24615;&#65288;KGC&#65289;&#12290;&#28982;&#32780;&#65292;KGE&#27169;&#22411;&#36890;&#24120;&#21482;&#26159;&#31616;&#21333;&#22320;&#23398;&#20064;&#19977;&#20803;&#32452;&#25968;&#25454;&#30340;&#32467;&#26500;&#20851;&#32852;&#65292;&#24182;&#19988;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;KG&#20013;&#65292;&#23884;&#20837;&#21487;&#33021;&#20250;&#34987;&#24494;&#19981;&#36275;&#36947;&#30340;&#27169;&#24335;&#21644;&#22122;&#22768;&#38142;&#25509;&#25152;&#35823;&#23548;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22240;&#26524;&#24615;&#21644;&#23884;&#20837;&#35299;&#32544;&#26041;&#38754;&#24314;&#31435;&#20102;KGE&#30340;&#26032;&#27169;&#24335;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;Causality-enhanced knowledge graph Embedding&#65288;CausE&#65289;&#26694;&#26550;&#12290;CausE&#20351;&#29992;&#22240;&#26524;&#24178;&#39044;&#26469;&#20272;&#35745;&#28151;&#26434;&#23884;&#20837;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#35757;&#32451;&#30446;&#26631;&#26469;&#36827;&#34892;&#31283;&#23450;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CausE&#21487;&#20197;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;KGC&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;https://github.com/zjukg/CausE&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph embedding (KGE) focuses on representing the entities and relations of a knowledge graph (KG) into the continuous vector spaces, which can be employed to predict the missing triples to achieve knowledge graph completion (KGC). However, KGE models often only briefly learn structural correlations of triple data and embeddings would be misled by the trivial patterns and noisy links in real-world KGs. To address this issue, we build the new paradigm of KGE in the context of causality and embedding disentanglement. We further propose a Causality-enhanced knowledge graph Embedding (CausE) framework. CausE employs causal intervention to estimate the causal effect of the confounder embeddings and design new training objectives to make stable predictions. Experimental results demonstrate that CausE could outperform the baseline models and achieve state-of-the-art KGC performance. We release our code in https://github.com/zjukg/CausE.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21021;&#27493;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#23454;&#30693;&#35782;&#36793;&#30028;&#65292;&#24182;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#23545;&#24320;&#25918;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#34920;&#29616;&#20986;&#33258;&#20449;&#65292;&#24182;&#19988;&#22238;&#31572;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2307.11019</link><description>&lt;p&gt;
&#29992;&#26816;&#32034;&#22686;&#24378;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#23454;&#30693;&#35782;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation. (arXiv:2307.11019v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21021;&#27493;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#23454;&#30693;&#35782;&#36793;&#30028;&#65292;&#24182;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#23545;&#24320;&#25918;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#34920;&#29616;&#20986;&#33258;&#20449;&#65292;&#24182;&#19988;&#22238;&#31572;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;QA&#65289;&#65289;&#38656;&#35201;&#22823;&#37327;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#24182;&#32463;&#24120;&#20381;&#36182;&#22806;&#37096;&#20449;&#24687;&#36827;&#34892;&#21327;&#21161;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65288;&#20363;&#22914;&#65292;ChatGPT&#65289;&#22312;&#35299;&#20915;&#21253;&#25324;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#22312;&#20869;&#30340;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;LLMs&#22312;&#24863;&#30693;&#20854;&#20107;&#23454;&#30693;&#35782;&#36793;&#30028;&#26041;&#38754;&#34920;&#29616;&#22914;&#20309;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#26102;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;LLMs&#30340;&#20107;&#23454;&#30693;&#35782;&#36793;&#30028;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#65292;&#24182;&#30740;&#31350;&#20102;&#26816;&#32034;&#22686;&#24378;&#23545;LLMs&#22312;&#24320;&#25918;&#22495;QA&#19978;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#20102;&#19977;&#20010;&#20027;&#35201;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26816;&#26597;LLMs&#30340;QA&#24615;&#33021;&#12289;&#20808;&#39564;&#21028;&#26029;&#21644;&#21518;&#39564;&#21028;&#26029;&#26469;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;LLMs&#23545;&#20110;&#33258;&#24049;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#21644;&#22238;&#31572;&#30340;&#20934;&#30830;&#24615;&#20805;&#28385;&#20102;&#33258;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge-intensive tasks (e.g., open-domain question answering (QA)) require a substantial amount of factual knowledge and often rely on external information for assistance. Recently, large language models (LLMs) (e.g., ChatGPT), have demonstrated impressive prowess in solving a wide range of tasks with world knowledge, including knowledge-intensive tasks. However, it remains unclear how well LLMs are able to perceive their factual knowledge boundaries, particularly how they behave when incorporating retrieval augmentation. In this study, we present an initial analysis of the factual knowledge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain QA. Specially, we focus on three primary research questions and analyze them by examining QA performance, priori judgement and posteriori judgement of LLMs. We show evidence that LLMs possess unwavering confidence in their capabilities to respond to questions and the accuracy of their responses. Furthermore, retrieval 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#23457;&#26597;&#38382;&#39064;&#65292;&#25351;&#20986;&#29616;&#26377;&#30340;&#35821;&#20041;&#23457;&#26597;&#26041;&#27861;&#23384;&#22312;&#29702;&#35770;&#19978;&#30340;&#38480;&#21046;&#65292;&#30001;&#20110;LLM&#30340;&#31243;&#24207;&#21270;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#35821;&#20041;&#23457;&#26597;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#19981;&#21487;&#21028;&#23450;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#26377;&#30693;&#35782;&#30340;&#25915;&#20987;&#32773;&#21487;&#20197;&#37325;&#26500;&#19981;&#21487;&#23481;&#35768;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2307.10719</link><description>&lt;p&gt;
LLM&#23457;&#26597;&#65306;&#26426;&#22120;&#23398;&#20064;&#25361;&#25112;&#36824;&#26159;&#35745;&#31639;&#26426;&#23433;&#20840;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?. (arXiv:2307.10719v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#23457;&#26597;&#38382;&#39064;&#65292;&#25351;&#20986;&#29616;&#26377;&#30340;&#35821;&#20041;&#23457;&#26597;&#26041;&#27861;&#23384;&#22312;&#29702;&#35770;&#19978;&#30340;&#38480;&#21046;&#65292;&#30001;&#20110;LLM&#30340;&#31243;&#24207;&#21270;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#35821;&#20041;&#23457;&#26597;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#19981;&#21487;&#21028;&#23450;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#26377;&#30693;&#35782;&#30340;&#25915;&#20987;&#32773;&#21487;&#20197;&#37325;&#26500;&#19981;&#21487;&#23481;&#35768;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#29702;&#35299;&#22797;&#26434;&#25351;&#20196;&#26041;&#38754;&#23637;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#25552;&#20379;&#30340;&#25351;&#20196;&#30340;&#30450;&#30446;&#36981;&#24490;&#24341;&#21457;&#20102;&#23545;&#24694;&#24847;&#20351;&#29992;&#39118;&#38505;&#30340;&#25285;&#24551;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#22914;LLM&#30340;&#27169;&#22411;&#24494;&#35843;&#25110;&#20351;&#29992;LLM&#36827;&#34892;&#36755;&#20986;&#23457;&#26597;&#65292;&#24050;&#35777;&#26126;&#26159;&#26377;&#32570;&#38519;&#30340;&#65292;&#22240;&#20026;LLM&#20173;&#28982;&#21487;&#20197;&#29983;&#25104;&#26377;&#38382;&#39064;&#30340;&#22238;&#31572;&#12290;&#24120;&#29992;&#30340;&#23457;&#26597;&#26041;&#27861;&#23558;&#36825;&#20010;&#38382;&#39064;&#35270;&#20026;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#20381;&#36182;&#20110;&#21478;&#19968;&#20010;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;LLM&#36755;&#20986;&#20013;&#30340;&#19981;&#33391;&#20869;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#36825;&#31181;&#35821;&#20041;&#23457;&#26597;&#26041;&#27861;&#30340;&#29702;&#35770;&#38480;&#21046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#35821;&#20041;&#23457;&#26597;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#19981;&#21487;&#21028;&#23450;&#30340;&#38382;&#39064;&#65292;&#31361;&#20986;&#20102;&#30001;&#20110;LLM&#30340;&#31243;&#24207;&#21270;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#32780;&#24341;&#36215;&#30340;&#23457;&#26597;&#20013;&#30340;&#22266;&#26377;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#25361;&#25112;&#19981;&#20165;&#38480;&#20110;&#35821;&#20041;&#23457;&#26597;&#65292;&#22240;&#20026;&#26377;&#30693;&#35782;&#30340;&#25915;&#20987;&#32773;&#21487;&#20197;&#37325;&#26500;&#19981;&#21487;&#23481;&#35768;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited impressive capabilities in comprehending complex instructions. However, their blind adherence to provided instructions has led to concerns regarding risks of malicious use. Existing defence mechanisms, such as model fine-tuning or output censorship using LLMs, have proven to be fallible, as LLMs can still generate problematic responses. Commonly employed censorship approaches treat the issue as a machine learning problem and rely on another LM to detect undesirable content in LLM outputs. In this paper, we present the theoretical limitations of such semantic censorship approaches. Specifically, we demonstrate that semantic censorship can be perceived as an undecidable problem, highlighting the inherent challenges in censorship that arise due to LLMs' programmatic and instruction-following capabilities. Furthermore, we argue that the challenges extend beyond semantic censorship, as knowledgeable attackers can reconstruct impermissible outputs 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2307.10490</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#22768;&#38899;&#30340;&#28389;&#29992;&#29992;&#20110;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs. (arXiv:2307.10490v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25351;&#20196;&#27880;&#20837;&#65292;&#25915;&#20987;&#32773;&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#25200;&#21160;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#65292;&#20197;&#25805;&#32437;&#27169;&#22411;&#36755;&#20986;&#29305;&#23450;&#25991;&#26412;&#21644;&#25351;&#23548;&#23545;&#35805;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#22270;&#20687;&#21644;&#22768;&#38899;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#36827;&#34892;&#38388;&#25509;&#25552;&#31034;&#21644;&#25351;&#20196;&#27880;&#20837;&#12290;&#25915;&#20987;&#32773;&#29983;&#25104;&#19982;&#25552;&#31034;&#30456;&#23545;&#24212;&#30340;&#23545;&#25239;&#25200;&#21160;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#22270;&#20687;&#25110;&#38899;&#39057;&#24405;&#38899;&#20013;&#12290;&#24403;&#29992;&#25143;&#21521;&#65288;&#26410;&#20462;&#25913;&#30340;&#33391;&#24615;&#65289;&#27169;&#22411;&#35810;&#38382;&#34987;&#25200;&#21160;&#30340;&#22270;&#20687;&#25110;&#38899;&#39057;&#26102;&#65292;&#25200;&#21160;&#20250;&#24341;&#23548;&#27169;&#22411;&#36755;&#20986;&#25915;&#20987;&#32773;&#36873;&#25321;&#30340;&#25991;&#26412;&#21644;/&#25110;&#20351;&#21518;&#32493;&#23545;&#35805;&#36981;&#24490;&#25915;&#20987;&#32773;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#29992;&#20960;&#20010;&#27010;&#24565;&#39564;&#35777;&#31034;&#20363;&#38024;&#23545;LLaVa&#21644;PandaGPT&#26469;&#35828;&#26126;&#36825;&#31181;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate how images and sounds can be used for indirect prompt and instruction injection in multi-modal LLMs. An attacker generates an adversarial perturbation corresponding to the prompt and blends it into an image or audio recording. When the user asks the (unmodified, benign) model about the perturbed image or audio, the perturbation steers the model to output the attacker-chosen text and/or make the subsequent dialog follow the attacker's instruction. We illustrate this attack with several proof-of-concept examples targeting LLaVa and PandaGPT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#20351;&#29992;&#22270;&#22686;&#24378;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#20174;&#24322;&#26500;&#22270;&#20013;&#23548;&#20986;&#30340;&#25512;&#29702;&#30693;&#35782;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#27880;&#24847;&#21147;&#12289;&#22270;&#27880;&#24847;&#21147;&#21644;&#20851;&#31995;&#31867;&#22411;&#32771;&#34385;&#65292;&#20248;&#21270;&#20102;&#23454;&#20307;&#21644;&#21333;&#35789;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#35813;&#27169;&#24335;&#19982;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#19982;LUKE&#30340;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#38598;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.10443</link><description>&lt;p&gt;
&#20351;&#29992;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#23558;&#24322;&#26500;&#22270;&#19982;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#30456;&#32467;&#21512;&#30340;&#38405;&#35835;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model. (arXiv:2307.10443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#20351;&#29992;&#22270;&#22686;&#24378;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#20174;&#24322;&#26500;&#22270;&#20013;&#23548;&#20986;&#30340;&#25512;&#29702;&#30693;&#35782;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#20840;&#23616;-&#23616;&#37096;&#27880;&#24847;&#21147;&#12289;&#22270;&#27880;&#24847;&#21147;&#21644;&#20851;&#31995;&#31867;&#22411;&#32771;&#34385;&#65292;&#20248;&#21270;&#20102;&#23454;&#20307;&#21644;&#21333;&#35789;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#35813;&#27169;&#24335;&#19982;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#19982;LUKE&#30340;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#36755;&#20837;&#24207;&#21015;&#20013;&#32570;&#23569;&#26174;&#24335;&#30693;&#35782;&#65292;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#26469;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#23427;&#21033;&#29992;&#22686;&#24378;&#22270;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23558;&#30001;&#24322;&#26500;&#22270;&#23548;&#20986;&#30340;&#25512;&#29702;&#30693;&#35782;&#25972;&#21512;&#21040;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#12290;&#25552;&#20986;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#65306;&#21333;&#35789;&#26631;&#35760;&#30340;&#20840;&#23616;-&#23616;&#37096;&#27880;&#24847;&#21147;&#65292;&#23545;&#23454;&#20307;&#26631;&#35760;&#30340;&#22270;&#27880;&#24847;&#21147;&#65292;&#23454;&#20307;&#26631;&#35760;&#23545;&#30456;&#20851;&#32852;&#30340;&#26631;&#35760;&#26174;&#31034;&#24378;&#28872;&#30340;&#27880;&#24847;&#21147;&#32780;&#23545;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#26174;&#31034;&#36739;&#24369;&#30340;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32771;&#34385;&#27599;&#20010;&#23454;&#20307;&#26631;&#35760;&#19982;&#21333;&#35789;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#31867;&#22411;&#12290;&#36825;&#26679;&#65292;&#22914;&#26524;&#23384;&#22312;&#20851;&#31995;&#65292;&#21017;&#21487;&#20197;&#20248;&#21270;&#20004;&#32773;&#20043;&#38388;&#30340;&#27880;&#24847;&#21147;&#12290;&#35813;&#27169;&#24335;&#19982;&#29305;&#27530;&#30340;&#30456;&#23545;&#20301;&#32622;&#26631;&#31614;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#19982;LUKE&#30340;&#23454;&#20307;&#24863;&#30693;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30456;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the significant progress made by transformer models in machine reading comprehension tasks, they still face limitations in handling complex reasoning tasks due to the absence of explicit knowledge in the input sequence. This paper proposes a novel attention pattern to overcome this limitation, which integrates reasoning knowledge derived from a heterogeneous graph into the transformer architecture using a graph-enhanced self-attention mechanism. The proposed attention pattern comprises three key elements: global-local attention for word tokens, graph attention for entity tokens that exhibit strong attention towards tokens connected in the graph as opposed to those unconnected, and the consideration of the type of relationship between each entity token and word token. This results in optimized attention between the two if a relationship exists. The pattern is coupled with special relative position labels, allowing it to integrate with LUKE's entity-aware self-attention mechanism
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;GPT&#36827;&#34892;&#39640;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#24182;&#32771;&#23519;&#20854;&#19982;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#26041;&#27861;&#30456;&#36739;&#20110;&#20854;&#20182;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#26377;&#25928;&#35299;&#20915;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#26816;&#27979;&#35773;&#21050;&#12290;</title><link>http://arxiv.org/abs/2307.10234</link><description>&lt;p&gt;
SentimentGPT&#65306;&#21033;&#29992;GPT&#36827;&#34892;&#39640;&#32423;&#24773;&#24863;&#20998;&#26512;&#21450;&#20854;&#19982;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning. (arXiv:2307.10234v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;GPT&#36827;&#34892;&#39640;&#32423;&#24773;&#24863;&#20998;&#26512;&#65292;&#24182;&#32771;&#23519;&#20854;&#19982;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;GPT&#26041;&#27861;&#30456;&#36739;&#20110;&#20854;&#20182;&#27169;&#22411;&#22312;&#39044;&#27979;&#24615;&#33021;&#19978;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#26377;&#25928;&#35299;&#20915;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#26816;&#27979;&#35773;&#21050;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#24773;&#24863;&#20998;&#26512;&#20013;&#21508;&#31181;&#29983;&#25104;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32771;&#23519;&#65292;&#29305;&#21035;&#26159;&#22312;SemEval 2017&#25968;&#25454;&#38598;&#30340;&#20219;&#21153;4&#20013;&#12290;&#37319;&#29992;&#20102;&#19977;&#31181;&#20027;&#35201;&#31574;&#30053;&#65306;1&#65289;&#20351;&#29992;GPT-3.5 Turbo&#36827;&#34892;&#25552;&#31034;&#24037;&#31243;&#65292;2&#65289;&#23545;GPT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;3&#65289;&#37319;&#29992;&#21019;&#26032;&#30340;&#23884;&#20837;&#20998;&#31867;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#36825;&#20123;&#31574;&#30053;&#21644;&#20010;&#21035;GPT&#27169;&#22411;&#20043;&#38388;&#30340;&#35814;&#32454;&#27604;&#36739;&#35265;&#35299;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#29420;&#29305;&#30340;&#20248;&#21183;&#21644;&#28508;&#22312;&#30340;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#23558;&#36825;&#20123;&#22522;&#20110;GPT&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#21516;&#26102;&#20195;&#12289;&#39640;&#24615;&#33021;&#30340;&#27169;&#22411;&#22312;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT&#26041;&#27861;&#22312;&#39044;&#27979;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#30456;&#36739;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;F1&#20998;&#25968;&#22686;&#21152;&#20102;22%&#20197;&#19978;&#12290;&#27492;&#22806;&#65292;&#26412;&#35770;&#25991;&#36824;&#25506;&#35752;&#20102;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#24120;&#35265;&#25361;&#25112;&#65292;&#22914;&#29702;&#35299;&#19978;&#19979;&#25991;&#21644;&#26816;&#27979;&#35773;&#21050;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;GPT&#26041;&#27861;&#30340;&#37325;&#35201;&#20215;&#20540;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents a thorough examination of various Generative Pretrained Transformer (GPT) methodologies in sentiment analysis, specifically in the context of Task 4 on the SemEval 2017 dataset. Three primary strategies are employed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2) fine-tuning GPT models, and 3) an inventive approach to embedding classification. The research yields detailed comparative insights among these strategies and individual GPT models, revealing their unique strengths and potential limitations. Additionally, the study compares these GPT-based methodologies with other contemporary, high-performing models previously used with the same dataset. The results illustrate the significant superiority of the GPT approaches in terms of predictive performance, more than 22% in F1-score compared to the state-of-the-art. Further, the paper addresses common challenges in sentiment analysis tasks, such as understanding context and detecting sarcasm. It underscores
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22810;&#31890;&#24230;&#20027;&#39064;&#20998;&#26512;&#26041;&#27861;&#65292;&#23545;&#24494;&#21338;&#35780;&#35770;&#36827;&#34892;&#35821;&#20041;&#20998;&#26512;&#65292;&#21457;&#29616;&#20851;&#20110;&#21462;&#28040;&#23130;&#23035;&#30331;&#35760;&#30340;&#29983;&#32946;&#38480;&#21046;&#30340;&#25552;&#26696;&#28041;&#21450;&#20010;&#20154;&#12289;&#31038;&#20250;&#21644;&#22269;&#23478;&#19977;&#20010;&#32500;&#24230;&#65292;&#35814;&#32454;&#35752;&#35770;&#20102;&#20010;&#20154;&#34892;&#20026;&#12289;&#31038;&#20250;&#20262;&#29702;&#21644;&#27861;&#24459;&#20197;&#21450;&#22269;&#23478;&#25919;&#31574;&#31561;&#31038;&#20250;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.10025</link><description>&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22810;&#31890;&#24230;&#20027;&#39064;&#20998;&#26512;&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#65306;&#29983;&#32946;&#25919;&#31574;&#25552;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on Fertility Proposals Using Multi-Grined Topic Analysis Methods. (arXiv:2307.10025v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#22810;&#31890;&#24230;&#20027;&#39064;&#20998;&#26512;&#26041;&#27861;&#65292;&#23545;&#24494;&#21338;&#35780;&#35770;&#36827;&#34892;&#35821;&#20041;&#20998;&#26512;&#65292;&#21457;&#29616;&#20851;&#20110;&#21462;&#28040;&#23130;&#23035;&#30331;&#35760;&#30340;&#29983;&#32946;&#38480;&#21046;&#30340;&#25552;&#26696;&#28041;&#21450;&#20010;&#20154;&#12289;&#31038;&#20250;&#21644;&#22269;&#23478;&#19977;&#20010;&#32500;&#24230;&#65292;&#35814;&#32454;&#35752;&#35770;&#20102;&#20010;&#20154;&#34892;&#20026;&#12289;&#31038;&#20250;&#20262;&#29702;&#21644;&#27861;&#24459;&#20197;&#21450;&#22269;&#23478;&#25919;&#31574;&#31561;&#31038;&#20250;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#32946;&#38382;&#39064;&#19982;&#20154;&#21475;&#23433;&#20840;&#23494;&#20999;&#30456;&#20851;&#65292;&#20013;&#22269;60&#24180;&#26469;&#39318;&#27425;&#20986;&#29616;&#20154;&#21475;&#36127;&#22686;&#38271;&#36235;&#21183;&#65292;&#29983;&#32946;&#25919;&#31574;&#30340;&#21464;&#21270;&#24341;&#36215;&#20102;&#31038;&#20250;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#25991;&#37319;&#29992;&#20849;&#29616;&#35821;&#20041;&#20998;&#26512;&#12289;&#20027;&#39064;&#20998;&#26512;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#26041;&#27861;&#65292;&#23545;&#24494;&#21338;&#35780;&#35770;&#36827;&#34892;&#22810;&#31890;&#24230;&#30340;&#35821;&#20041;&#20998;&#26512;&#12290;&#21457;&#29616;&#20851;&#20110;&#8220;&#21462;&#28040;&#23130;&#23035;&#30331;&#35760;&#30340;&#29983;&#32946;&#38480;&#21046;&#8221;&#30340;&#25552;&#26696;&#35752;&#35770;&#28041;&#21450;&#20010;&#20154;&#12289;&#31038;&#20250;&#21644;&#22269;&#23478;&#19977;&#20010;&#32500;&#24230;&#65292;&#24182;&#35814;&#32454;&#25506;&#35752;&#20102;&#20010;&#20154;&#34892;&#20026;&#12289;&#31038;&#20250;&#20262;&#29702;&#21644;&#27861;&#24459;&#20197;&#21450;&#22269;&#23478;&#25919;&#31574;&#31561;&#31038;&#20250;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fertility issues are closely related to population security, in 60 years China's population for the first time in a negative growth trend, the change of fertility policy is of great concern to the community. 2023 ``two sessions" proposal ``suggests that the country in the form of legislation, the birth of the registration of the cancellation of the marriage restriction" This topic was once a hot topic on the Internet, and ``unbundling" the relationship between birth registration and marriage has become the focus of social debate. In this paper, we adopt co-occurrence semantic analysis, topic analysis and sentiment analysis to conduct multi-granularity semantic analysis of microblog comments. It is found that the discussion on the proposal of ``removing marriage restrictions from birth registration" involves the individual, society and the state at three dimensions, and is detailed into social issues such as personal behaviour, social ethics and law, and national policy, with people's s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25968;&#23398;&#23548;&#20986;&#65292;&#20998;&#26512;&#20102;&#24494;&#35843;&#27169;&#22411;&#23545;&#26410;&#35265;&#31526;&#21495;&#21644;&#26041;&#31243;&#32467;&#26500;&#26356;&#25913;&#30340;&#25935;&#24863;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#24494;&#35843;&#30340;FLAN-T5-large&#65288;MathT5&#65289;&#22312;&#21508;&#20010;&#27979;&#35797;&#38598;&#19978;&#30340;&#32477;&#23545;&#24615;&#33021;&#20248;&#20110;GPT&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.09998</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25968;&#23398;&#23548;&#20986;
&lt;/p&gt;
&lt;p&gt;
Generating Mathematical Derivations with Large Language Models. (arXiv:2307.09998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25968;&#23398;&#23548;&#20986;&#65292;&#20998;&#26512;&#20102;&#24494;&#35843;&#27169;&#22411;&#23545;&#26410;&#35265;&#31526;&#21495;&#21644;&#26041;&#31243;&#32467;&#26500;&#26356;&#25913;&#30340;&#25935;&#24863;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#24494;&#35843;&#30340;FLAN-T5-large&#65288;MathT5&#65289;&#22312;&#21508;&#20010;&#27979;&#35797;&#38598;&#19978;&#30340;&#32477;&#23545;&#24615;&#33021;&#20248;&#20110;GPT&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#29983;&#25104;&#25968;&#23398;&#32467;&#26524;&#30340;&#23548;&#20986;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#26377;&#21487;&#33021;&#25903;&#25345;&#25968;&#23398;&#21457;&#29616;&#12290;&#26412;&#25991;&#21033;&#29992;&#31526;&#21495;&#24341;&#25806;&#22312;&#22823;&#35268;&#27169;&#19978;&#29983;&#25104;&#26041;&#31243;&#30340;&#23548;&#20986;&#65292;&#24182;&#30740;&#31350;&#20102;LLM&#22312;&#20174;&#21069;&#25552;&#20013;&#23548;&#20986;&#30446;&#26631;&#26041;&#31243;&#26102;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#23545;GPT&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23545;&#19968;&#31995;&#21015;T5&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20197;&#27604;&#36739;&#39044;&#35757;&#32451;&#31574;&#30053;&#23545;&#19987;&#38376;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;FLAN-T5-large&#65288;MathT5&#65289;&#22312;&#25152;&#26377;&#38745;&#24577;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#27979;&#35797;&#38598;&#19978;&#30340;&#32477;&#23545;&#24615;&#33021;&#20248;&#20110;GPT&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#24494;&#35843;&#27169;&#22411;&#23545;&#28041;&#21450;&#26410;&#35265;&#31526;&#21495;&#30340;&#25200;&#21160;&#65288;&#20197;&#21450;&#22312;&#36739;&#23567;&#31243;&#24230;&#19978;&#30340;&#26041;&#31243;&#32467;&#26500;&#26356;&#25913;&#65289;&#26356;&#20026;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;1.7K&#20010;&#26041;&#31243;&#21644;200&#22810;&#20010;&#23548;&#20986;&#20197;&#20984;&#26174;&#20986;LLM&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The derivation of mathematical results in specialised fields using Large Language Models (LLMs) is an emerging research direction that can help identify models' limitations, and potentially support mathematical discovery. In this paper, we leverage a symbolic engine to generate derivations of equations at scale, and investigate the capabilities of LLMs when deriving goal equations from premises. Specifically, we employ in-context learning for GPT and fine-tune a range of T5 models to compare the robustness and generalisation of pre-training strategies to specialised models. Empirical results show that fine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static and out-of-distribution test sets in terms of absolute performance. However, an in-depth analysis reveals that the fine-tuned models are more sensitive to perturbations involving unseen symbols and (to a lesser extent) changes to equation structure. In addition, we analyse 1.7K equations and over 200 derivations to hig
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08303</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models. (arXiv:2307.08303v1 [cs.IR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#26469;&#22686;&#24378;&#23494;&#38598;&#26816;&#32034;&#30340;&#26041;&#27861;&#65288;SPTAR&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#29983;&#25104;&#24369;&#26597;&#35810;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#26816;&#32034;&#65288;DR&#65289;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#36716;&#21270;&#20026;&#23494;&#38598;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#22312;&#21521;&#37327;&#31354;&#38388;&#20013;&#27979;&#37327;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;DR&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;DR&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#20174;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;MS MARCO&#65289;&#20013;&#23398;&#20064;&#65292;&#20294;&#35777;&#25454;&#34920;&#26126;&#65292;&#24182;&#38750;&#25152;&#26377;DR&#27169;&#22411;&#21644;&#39046;&#22495;&#37117;&#33021;&#21516;&#31561;&#21463;&#30410;&#20110;&#36801;&#31227;&#23398;&#20064;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#36716;&#21521;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25913;&#36827;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#30340;DR&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20013;&#37319;&#29992;&#30340;&#30828;&#25552;&#31034;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#26080;&#27861;&#20445;&#35777;&#29983;&#25104;&#30340;&#24369;&#26597;&#35810;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22686;&#24378;DR&#30340;&#36719;&#25552;&#31034;&#35843;&#20248;&#65288;SPTAR&#65289;&#65306;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992;&#36719;&#25552;&#31034;&#35843;&#20248;&#22312;&#26377;&#38480;&#30340;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#36719;&#25552;&#31034;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#25552;&#31034;&#24341;&#23548;LLMs&#20026;&#26410;&#26631;&#35760;&#30340;&#25991;&#26723;&#26631;&#35760;&#24369;&#26597;&#35810;&#65292;&#20174;&#32780;&#24471;&#21040;&#36275;&#22815;&#30340;&#24369;&#25991;&#26723;-&#26597;&#35810;&#23545;&#26469;&#35757;&#32451;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval (DR) converts queries and documents into dense embeddings and measures the similarity between queries and documents in vector space. One of the challenges in DR is the lack of domain-specific training data. While DR models can learn from large-scale public datasets like MS MARCO through transfer learning, evidence shows that not all DR models and domains can benefit from transfer learning equally. Recently, some researchers have resorted to large language models (LLMs) to improve the zero-shot and few-shot DR models. However, the hard prompts or human-written prompts utilized in these works cannot guarantee the good quality of generated weak queries. To tackle this, we propose soft prompt tuning for augmenting DR (SPTAR): For each task, we leverage soft prompt-tuning to optimize a task-specific soft prompt on limited ground truth data and then prompt the LLMs to tag unlabeled documents with weak queries, yielding enough weak document-query pairs to train task-specific d
&lt;/p&gt;</description></item><item><title>Disco-Bench&#26159;&#19968;&#20010;&#38754;&#21521;&#35821;&#35328;&#24314;&#27169;&#30340;&#35770;&#36848;&#24863;&#30693;&#35780;&#20272;&#22522;&#20934;&#65292;&#21487;&#20197;&#36328;&#22810;&#20010;NLP&#20219;&#21153;&#35780;&#20272;&#21477;&#20869;&#35770;&#36848;&#23646;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#25991;&#29486;&#39046;&#22495;&#30340;9&#20010;&#27979;&#35797;&#38598;&#21644;&#19968;&#20010;&#35786;&#26029;&#27979;&#35797;&#22871;&#20214;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#35770;&#36848;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;20&#20010;&#19981;&#21516;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2307.08074</link><description>&lt;p&gt;
Disco-Bench: &#19968;&#31181;&#38754;&#21521;&#35821;&#35328;&#24314;&#27169;&#30340;&#35770;&#36848;&#24863;&#30693;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Disco-Bench: A Discourse-Aware Evaluation Benchmark for Language Modelling. (arXiv:2307.08074v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08074
&lt;/p&gt;
&lt;p&gt;
Disco-Bench&#26159;&#19968;&#20010;&#38754;&#21521;&#35821;&#35328;&#24314;&#27169;&#30340;&#35770;&#36848;&#24863;&#30693;&#35780;&#20272;&#22522;&#20934;&#65292;&#21487;&#20197;&#36328;&#22810;&#20010;NLP&#20219;&#21153;&#35780;&#20272;&#21477;&#20869;&#35770;&#36848;&#23646;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#25991;&#29486;&#39046;&#22495;&#30340;9&#20010;&#27979;&#35797;&#38598;&#21644;&#19968;&#20010;&#35786;&#26029;&#27979;&#35797;&#22871;&#20214;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#35770;&#36848;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;20&#20010;&#19981;&#21516;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#36848;&#24314;&#27169;&#65292;&#21363;&#36229;&#36234;&#20010;&#21035;&#21477;&#23376;&#30340;&#35821;&#35328;&#29616;&#35937;&#65292;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20013;&#19968;&#20010;&#22522;&#26412;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#21477;&#38388;&#23646;&#24615;&#30340;&#35780;&#20272;&#65292;&#24573;&#35270;&#20102;&#36328;&#21477;&#23376;&#30340;&#20851;&#38190;&#35770;&#36848;&#29616;&#35937;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Disco-Bench&#65292;&#19968;&#20010;&#21487;&#20197;&#35780;&#20272;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21477;&#20869;&#35770;&#36848;&#23646;&#24615;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#29702;&#35299;&#12289;&#32763;&#35793;&#21644;&#29983;&#25104;&#12290;Disco-Bench&#21253;&#25324;&#20102;&#25991;&#29486;&#39046;&#22495;&#30340;9&#20010;&#25991;&#26723;&#32423;&#27979;&#35797;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20013;&#25991;&#21644;/&#25110;&#33521;&#25991;&#20013;&#20016;&#23500;&#30340;&#35770;&#36848;&#29616;&#35937;&#65288;&#22914;&#36830;&#36143;&#24615;&#21644;&#36830;&#36143;&#24615;&#65289;&#12290;&#20026;&#20102;&#36827;&#34892;&#35821;&#35328;&#20998;&#26512;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#22871;&#35786;&#26029;&#27979;&#35797;&#22871;&#20214;&#65292;&#21487;&#20197;&#26816;&#26597;&#30446;&#26631;&#27169;&#22411;&#26159;&#21542;&#23398;&#20064;&#21040;&#20102;&#35770;&#36848;&#30693;&#35782;&#12290;&#25105;&#20204;&#24635;&#20849;&#35780;&#20272;&#20102;20&#20010;&#22522;&#20110;Transformer&#12289;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#26550;&#26500;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#36890;&#29992;&#22411;&#12289;&#39046;&#22495;&#20869;&#21644;&#21830;&#19994;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling discourse -- the linguistic phenomena that go beyond individual sentences, is a fundamental yet challenging aspect of natural language processing (NLP). However, existing evaluation benchmarks primarily focus on the evaluation of inter-sentence properties and overlook critical discourse phenomena that cross sentences. To bridge the gap, we propose Disco-Bench, a benchmark that can evaluate intra-sentence discourse properties across a diverse set of NLP tasks, covering understanding, translation, and generation. Disco-Bench consists of 9 document-level testsets in the literature domain, which contain rich discourse phenomena (e.g. cohesion and coherence) in Chinese and/or English. For linguistic analysis, we also design a diagnostic test suite that can examine whether the target models learn discourse knowledge. We totally evaluate 20 general-, in-domain and commercial models based on Transformer, advanced pretraining architectures and large language models (LLMs). Our results 
&lt;/p&gt;</description></item><item><title>AspectCSE&#26159;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#22909;&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;3.97%&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#30340;&#23884;&#20837;&#27169;&#22411;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2307.07851</link><description>&lt;p&gt;
AspectCSE: &#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity using Contrastive Learning and Structured Knowledge. (arXiv:2307.07851v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07851
&lt;/p&gt;
&lt;p&gt;
AspectCSE&#26159;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#26041;&#38754;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#23427;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30456;&#27604;&#20043;&#21069;&#30340;&#26368;&#22909;&#32467;&#26524;&#24179;&#22343;&#25552;&#39640;&#20102;3.97%&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#30340;&#23884;&#20837;&#27169;&#22411;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#20102;&#23545;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#30340;&#31895;&#30053;&#36817;&#20284;&#65292;&#20294;&#24573;&#30053;&#20102;&#20351;&#25991;&#26412;&#30456;&#20284;&#30340;&#29305;&#23450;&#26041;&#38754;&#12290;&#30456;&#21453;&#65292;&#22522;&#20110;&#26041;&#38754;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#20102;&#22522;&#20110;&#39044;&#23450;&#20041;&#26041;&#38754;&#30340;&#25991;&#26412;&#30456;&#20284;&#24615;&#12290;&#22240;&#27492;&#65292;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#39044;&#27979;&#26356;&#21152;&#38024;&#23545;&#29305;&#23450;&#35201;&#27714;&#65292;&#24182;&#19988;&#26356;&#23481;&#26131;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AspectCSE&#65292;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#26368;&#22909;&#30340;&#32467;&#26524;&#30456;&#27604;&#65292;AspectCSE&#22312;&#22810;&#20010;&#26041;&#38754;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#24179;&#22343;&#25913;&#21892;3.97%&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20351;&#29992;Wikidata&#30693;&#35782;&#22270;&#23646;&#24615;&#26469;&#35757;&#32451;&#22810;&#26041;&#38754;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#65292;&#20854;&#20013;&#22312;&#30456;&#20284;&#24615;&#39044;&#27979;&#36807;&#31243;&#20013;&#21516;&#26102;&#32771;&#34385;&#22810;&#20010;&#29305;&#23450;&#26041;&#38754;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#26041;&#38754;&#23884;&#20837;&#22312;&#29305;&#23450;&#26041;&#38754;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#19978;&#20248;&#20110;&#21333;&#26041;&#38754;&#23884;&#20837;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23884;&#20837;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#25913;&#36827;&#23884;&#20837;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generic sentence embeddings provide a coarse-grained approximation of semantic textual similarity but ignore specific aspects that make texts similar. Conversely, aspect-based sentence embeddings provide similarities between texts based on certain predefined aspects. Thus, similarity predictions of texts are more targeted to specific requirements and more easily explainable. In this paper, we present AspectCSE, an approach for aspect-based contrastive learning of sentence embeddings. Results indicate that AspectCSE achieves an average improvement of 3.97% on information retrieval tasks across multiple aspects compared to the previous best results. We also propose using Wikidata knowledge graph properties to train models of multi-aspect sentence embeddings in which multiple specific aspects are simultaneously considered during similarity predictions. We demonstrate that multi-aspect embeddings outperform single-aspect embeddings on aspect-specific information retrieval tasks. Finally, w
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;LaunchpadGPT&#27169;&#22411;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#38899;&#20048;&#21487;&#35270;&#21270;&#35774;&#35745;&#65292;&#24182;&#23637;&#31034;&#20986;&#20248;&#20110;&#38543;&#26426;&#29983;&#25104;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#38899;&#20048;&#21487;&#35270;&#21270;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.04827</link><description>&lt;p&gt;
LaunchpadGPT: &#20197;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38899;&#20048;&#21487;&#35270;&#21270;&#35774;&#35745;&#24072;&#22312;Launchpad&#19978;
&lt;/p&gt;
&lt;p&gt;
LaunchpadGPT: Language Model as Music Visualization Designer on Launchpad. (arXiv:2307.04827v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04827
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;LaunchpadGPT&#27169;&#22411;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#38899;&#20048;&#21487;&#35270;&#21270;&#35774;&#35745;&#65292;&#24182;&#23637;&#31034;&#20986;&#20248;&#20110;&#38543;&#26426;&#29983;&#25104;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#38899;&#20048;&#21487;&#35270;&#21270;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Launchpad&#26159;&#19968;&#31181;&#20048;&#22120;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#25353;&#20142;&#30340;&#25353;&#38062;&#26469;&#21019;&#20316;&#21644;&#28436;&#22863;&#38899;&#20048;&#12290;&#20026;&#20102;&#36741;&#21161;&#21644;&#21551;&#21457;Launchpad&#28783;&#20809;&#25928;&#26524;&#30340;&#35774;&#35745;&#65292;&#24182;&#20026;&#21021;&#23398;&#32773;&#25552;&#20379;&#26356;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#27861;&#26469;&#36890;&#36807;&#36825;&#20010;&#20048;&#22120;&#21019;&#24314;&#38899;&#20048;&#21487;&#35270;&#21270;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LaunchpadGPT&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;Launchpad&#19978;&#30340;&#38899;&#20048;&#21487;&#35270;&#21270;&#35774;&#35745;&#12290;&#22522;&#20110;&#20855;&#26377;&#20986;&#33394;&#29983;&#25104;&#33021;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;LaunchpadGPT&#27169;&#22411;&#20197;&#38899;&#39057;&#38899;&#20048;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#36755;&#20986;&#20197;&#35270;&#39057;&#24418;&#24335;&#34920;&#29616;Launchpad&#28436;&#22863;&#30340;&#28783;&#20809;&#25928;&#26524;&#65288;Launchpad&#25773;&#25918;&#35270;&#39057;&#65289;&#12290;&#25105;&#20204;&#25910;&#38598;Launchpad&#28436;&#22863;&#35270;&#39057;&#24182;&#36827;&#34892;&#22788;&#29702;&#65292;&#20197;&#33719;&#21462;&#38899;&#20048;&#21644;&#30456;&#24212;&#30340;Launchpad&#28436;&#22863;&#35270;&#39057;&#24103;&#20316;&#20026;&#25552;&#31034;&#23436;&#25104;&#23545;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#38543;&#26426;&#29983;&#25104;&#26041;&#27861;&#21487;&#20197;&#21019;&#36896;&#20986;&#26356;&#22909;&#30340;&#38899;&#20048;&#21487;&#35270;&#21270;&#25928;&#26524;&#65292;&#24182;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#38899;&#20048;&#21487;&#35270;&#21270;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Launchpad is a musical instrument that allows users to create and perform music by pressing illuminated buttons. To assist and inspire the design of the Launchpad light effect, and provide a more accessible approach for beginners to create music visualization with this instrument, we proposed the LaunchpadGPT model to generate music visualization designs on Launchpad automatically. Based on the language model with excellent generation ability, our proposed LaunchpadGPT takes an audio piece of music as input and outputs the lighting effects of Launchpad-playing in the form of a video (Launchpad-playing video). We collect Launchpad-playing videos and process them to obtain music and corresponding video frame of Launchpad-playing as prompt-completion pairs, to train the language model. The experiment result shows the proposed method can create better music visualization than random generation methods and hold the potential for a broader range of music visualization applications. Our code 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21033;&#29992;&#29992;&#25143;&#35780;&#35770;&#21644;&#30456;&#20851;&#39033;&#30446;&#29305;&#24449;&#29983;&#25104;&#23545;&#27604;&#35780;&#20215;&#21477;&#23376;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#26368;&#36866;&#21512;&#30340;&#20135;&#21697;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#39033;&#30446;&#32534;&#30721;&#27169;&#22359;&#12289;&#27604;&#36739;&#29983;&#25104;&#27169;&#22359;&#21644;&#20010;&#24615;&#21270;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#20102;&#29983;&#25104;&#21477;&#23376;&#30340;&#30456;&#20851;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03691</link><description>&lt;p&gt;
&#23558;&#33529;&#26524;&#19982;&#33529;&#26524;&#36827;&#34892;&#27604;&#36739;&#65306;&#20174;&#29992;&#25143;&#35780;&#35770;&#29983;&#25104;&#32437;&#21521;&#24863;&#30693;&#30340;&#27604;&#36739;&#21477;&#23376;
&lt;/p&gt;
&lt;p&gt;
Comparing Apples to Apples: Generating Aspect-Aware Comparative Sentences from User Review. (arXiv:2307.03691v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03691
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21033;&#29992;&#29992;&#25143;&#35780;&#35770;&#21644;&#30456;&#20851;&#39033;&#30446;&#29305;&#24449;&#29983;&#25104;&#23545;&#27604;&#35780;&#20215;&#21477;&#23376;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#26368;&#36866;&#21512;&#30340;&#20135;&#21697;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#39033;&#30446;&#32534;&#30721;&#27169;&#22359;&#12289;&#27604;&#36739;&#29983;&#25104;&#27169;&#22359;&#21644;&#20010;&#24615;&#21270;&#35299;&#30721;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#20102;&#29983;&#25104;&#21477;&#23376;&#30340;&#30456;&#20851;&#24615;&#21644;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20247;&#22810;&#30456;&#20284;&#30340;&#36873;&#25321;&#20013;&#25214;&#21040;&#26368;&#20339;&#20135;&#21697;&#26159;&#38750;&#24120;&#32791;&#26102;&#30340;&#12290;&#27604;&#36739;&#21477;&#23376;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20197;&#31361;&#20986;&#30340;&#26041;&#24335;&#23545;&#27604;&#19968;&#20010;&#39033;&#30446;&#19982;&#20854;&#20182;&#39033;&#30446;&#65292;&#22312;&#27492;&#36807;&#31243;&#20013;&#24378;&#35843;&#20986;&#37325;&#35201;&#29305;&#24449;&#12290;&#22522;&#20110;&#29992;&#25143;&#23545;&#19968;&#20010;&#25110;&#22810;&#20010;&#39033;&#30446;&#30340;&#35780;&#35770;&#21450;&#30456;&#20851;&#39033;&#30446;&#29305;&#24449;&#65292;&#25105;&#20204;&#29983;&#25104;&#27604;&#36739;&#35780;&#35770;&#21477;&#23376;&#26469;&#24110;&#21161;&#29992;&#25143;&#25214;&#21040;&#26368;&#36866;&#21512;&#30340;&#20135;&#21697;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21253;&#25324;&#19977;&#20010;&#36830;&#32493;&#32452;&#20214;&#65306;&#65288;i&#65289;&#19968;&#20010;&#39033;&#30446;&#32534;&#30721;&#27169;&#22359;&#29992;&#20110;&#23545;&#39033;&#30446;&#36827;&#34892;&#32534;&#30721;&#27604;&#36739;&#65292;&#65288;ii&#65289;&#19968;&#20010;&#27604;&#36739;&#29983;&#25104;&#27169;&#22359;&#20197;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#29983;&#25104;&#27604;&#36739;&#21477;&#23376;&#65292;&#65288;iii&#65289;&#19968;&#31181;&#29992;&#20110;&#29992;&#25143;&#20010;&#24615;&#21270;&#30340;&#26032;&#22411;&#35299;&#30721;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27969;&#31243;&#33021;&#22815;&#29983;&#25104;&#27969;&#30021;&#19988;&#22810;&#26679;&#30340;&#27604;&#36739;&#21477;&#23376;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#31867;&#35780;&#20272;&#30740;&#31350;&#26469;&#39564;&#35777;&#25105;&#20204;&#29983;&#25104;&#30340;&#21477;&#23376;&#30340;&#30456;&#20851;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#29983;&#25104;&#30456;&#20851;&#19988;&#30495;&#23454;&#30340;&#27604;&#36739;&#35780;&#35770;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is time-consuming to find the best product among many similar alternatives. Comparative sentences can help to contrast one item from others in a way that highlights important features of an item that stand out. Given reviews of one or multiple items and relevant item features, we generate comparative review sentences to aid users to find the best fit. Specifically, our model consists of three successive components in a transformer: (i) an item encoding module to encode an item for comparison, (ii) a comparison generation module that generates comparative sentences in an autoregressive manner, (iii) a novel decoding method for user personalization. We show that our pipeline generates fluent and diverse comparative sentences. We run experiments on the relevance and fidelity of our generated sentences in a human evaluation study and find that our algorithm creates comparative review sentences that are relevant and truthful.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03104</link><description>&lt;p&gt;
&#20351;&#29992;&#36866;&#37197;&#22120;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Efficient Domain Adaptation of Sentence Embeddings using Adapters. (arXiv:2307.03104v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#23884;&#20837;&#20351;&#25105;&#20204;&#33021;&#22815;&#25429;&#25417;&#30701;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#22823;&#22810;&#25968;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#26159;&#38024;&#23545;&#19968;&#33324;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22240;&#27492;&#65292;&#35201;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#65292;&#24517;&#39035;&#23558;&#27169;&#22411;&#36866;&#24212;&#20110;&#35813;&#39046;&#22495;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#36890;&#24120;&#65292;&#36825;&#26159;&#36890;&#36807;&#23545;&#24863;&#20852;&#36259;&#30340;&#22495;&#23545;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#30340;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#26356;&#26032;&#20102;&#25152;&#26377;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#20351;&#35813;&#26041;&#27861;&#22312;&#36164;&#28304;&#19978;&#35201;&#27714;&#36739;&#39640;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#20026;&#27599;&#20010;&#30446;&#26631;&#39046;&#22495;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#12290;&#36825;&#20123;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#19981;&#38656;&#35201;&#24494;&#35843;&#25152;&#26377;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21482;&#35757;&#32451;&#23569;&#37327;&#30340;&#39069;&#22806;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#21487;&#20197;&#22987;&#32456;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence embeddings enable us to capture the semantic similarity of short texts. Most sentence embedding models are trained for general semantic textual similarity (STS) tasks. Therefore, to use sentence embeddings in a particular domain, the model must be adapted to it in order to achieve good results. Usually, this is done by fine-tuning the entire sentence embedding model for the domain of interest. While this approach yields state-of-the-art results, all of the model's weights are updated during fine-tuning, making this method resource-intensive. Therefore, instead of fine-tuning entire sentence embedding models for each target domain individually, we propose to train lightweight adapters. These domain-specific adapters do not require fine-tuning all underlying sentence embedding model parameters. Instead, we only train a small number of additional parameters while keeping the weights of the underlying sentence embedding model fixed. Training domain-specific adapters allows always 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#65292;&#26816;&#27979;&#21644;&#20998;&#31867;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#33647;&#29289;&#30456;&#20851;&#30149;&#20363;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.02591</link><description>&lt;p&gt;
ODD: &#19968;&#20221;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant Behavior Detection. (arXiv:2307.02591v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02591
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#65292;&#26816;&#27979;&#21644;&#20998;&#31867;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#33647;&#29289;&#30456;&#20851;&#30149;&#20363;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#65288;ORAB&#65289;&#26159;&#38450;&#27490;&#33647;&#29289;&#36807;&#37327;&#30340;&#26032;&#39118;&#38505;&#22240;&#32032;&#12290;&#20197;&#24448;&#65292;ORAB&#20027;&#35201;&#36890;&#36807;&#35843;&#26597;&#32467;&#26524;&#21644;&#33647;&#29289;&#32473;&#20104;&#30417;&#27979;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#65292;&#24182;&#19981;&#33021;&#28085;&#30422;&#25152;&#26377;&#24322;&#24120;&#34892;&#20026;&#30340;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;ORAB&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#20013;&#24191;&#27867;&#26377;&#35760;&#24405;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;ORAB&#26816;&#27979;&#12290;ODD&#26159;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;750&#22810;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#12290;ODD&#26088;&#22312;&#20174;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#20013;&#35782;&#21035;ORAB&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#20061;&#20010;&#31867;&#21035;&#65306;1&#65289;&#24050;&#30830;&#35748;&#24322;&#24120;&#34892;&#20026;&#65292;2&#65289;&#26263;&#31034;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;3&#65289;&#38463;&#29255;&#31867;&#33647;&#29289;&#65292;4&#65289;&#36866;&#24212;&#30151;&#65292;5&#65289;&#24050;&#35786;&#26029;&#30340;&#38463;&#29255;&#21046;&#21058;&#20381;&#36182;&#65292;6&#65289;&#33519;&#20108;&#27694;&#24179;&#31867;&#33647;&#29289;&#65292;7&#65289;&#33647;&#29289;&#21464;&#21270;&#65292;8&#65289;&#19982;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#30456;&#20851;&#65292;9&#65289;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opioid related aberrant behaviors (ORAB) present novel risk factors for opioid overdose. Previously, ORAB have been mainly assessed by survey results and by monitoring drug administrations. Such methods however, cannot scale up and do not cover the entire spectrum of aberrant behaviors. On the other hand, ORAB are widely documented in electronic health record notes. This paper introduces a novel biomedical natural language processing benchmark dataset named ODD, for ORAB Detection Dataset. ODD is an expert-annotated dataset comprising of more than 750 publicly available EHR notes. ODD has been designed to identify ORAB from patients' EHR notes and classify them into nine categories; 1) Confirmed Aberrant Behavior, 2) Suggested Aberrant Behavior, 3) Opioids, 4) Indication, 5) Diagnosed opioid dependency, 6) Benzodiapines, 7) Medication Changes, 8) Central Nervous System-related, and 9) Social Determinants of Health. We explored two state-of-the-art natural language processing (NLP) mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23618;&#19987;&#23478;&#29983;&#25104;SQL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#29992;&#30340;&#22810;&#20219;&#21153;&#20998;&#23618;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#30001;&#20110;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#23548;&#33268;&#29983;&#25104;&#19981;&#20934;&#30830;SQL&#35821;&#21477;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#22312;WiKSQL&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.17727</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#23618;&#19987;&#23478;&#32593;&#32476;&#30340;&#25913;&#36827;NL2SQL&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Improved NL2SQL based on Multi-layer Expert Network. (arXiv:2306.17727v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23618;&#19987;&#23478;&#29983;&#25104;SQL&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19987;&#29992;&#30340;&#22810;&#20219;&#21153;&#20998;&#23618;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#30001;&#20110;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#23548;&#33268;&#29983;&#25104;&#19981;&#20934;&#30830;SQL&#35821;&#21477;&#30340;&#38480;&#21046;&#12290;&#35813;&#26041;&#27861;&#22312;WiKSQL&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#65288;NL2SQL&#65289;&#25216;&#26415;&#29992;&#20110;&#23558;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#36716;&#25442;&#20026;&#21487;&#25191;&#34892;&#30340;SQL&#35821;&#21477;&#12290;&#36890;&#24120;&#65292;&#36890;&#36807;&#25554;&#27133;&#22635;&#20805;&#20316;&#20026;&#22810;&#20219;&#21153;&#20998;&#31867;&#26041;&#27861;&#26469;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#20998;&#31867;&#20219;&#21153;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;&#65292;&#25554;&#27133;&#22635;&#20805;&#21487;&#33021;&#23548;&#33268;&#29983;&#25104;&#19981;&#20934;&#30830;&#30340;SQL&#35821;&#21477;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#23618;&#19987;&#23478;&#29983;&#25104;SQL&#65288;MLEG-SQL&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#19987;&#29992;&#30340;&#22810;&#20219;&#21153;&#20998;&#23618;&#32593;&#32476;&#12290;&#32593;&#32476;&#30340;&#19979;&#23618;&#25552;&#21462;&#33258;&#28982;&#35821;&#35328;&#35821;&#21477;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#32780;&#19978;&#23618;&#26500;&#24314;&#19968;&#20010;&#19987;&#38376;&#30340;&#19987;&#23478;&#31995;&#32479;&#26469;&#22788;&#29702;&#29305;&#23450;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#36825;&#31181;&#20998;&#23618;&#26041;&#27861;&#20943;&#36731;&#20102;&#19981;&#21516;&#20219;&#21153;&#20914;&#31361;&#24102;&#26469;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#35813;&#26041;&#27861;&#22312;WiKSQL&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#22312;&#29983;&#25104;&#20934;&#30830;&#30340;SQL&#35821;&#21477;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Natural Language to SQL (NL2SQL) technique is used to convert natural language queries into executable SQL statements. Typically, slot-filling is employed as a classification method for multi-task cases to achieve this goal. However, slot-filling can result in inaccurate SQL statement generation due to negative migration issues arising from different classification tasks. To overcome this limitation, this study introduces a new approach called Multi-Layer Expert Generate SQL (MLEG-SQL), which utilizes a dedicated multi-task hierarchical network. The lower layer of the network extracts semantic features of natural language statements, while the upper layer builds a specialized expert system for handling specific classification tasks. This hierarchical approach mitigates performance degradation resulting from different task conflicts. The proposed method was evaluated on the WiKSQL dataset and was found to be effective in generating accurate SQL statements.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545; GPT-3.5-Turbo &#21644; GPT-4 &#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#30456;&#20851;&#29255;&#27573;&#30340;&#25903;&#25745;&#19982;&#39046;&#20808;&#31995;&#32479;&#30456;&#31454;&#20105;&#65292;&#20294;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2306.16108</link><description>&lt;p&gt;
ChatGPT &#26159;&#19968;&#20010;&#29983;&#29289;&#21307;&#23398;&#19987;&#23478;&#21527;&#65311;&#8212;&#8212;&#25506;&#32034;&#24403;&#21069; GPT &#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Biomedical Expert? -- Exploring the Zero-Shot Performance of Current GPT Models in Biomedical Tasks. (arXiv:2306.16108v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16108
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545; GPT-3.5-Turbo &#21644; GPT-4 &#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#30456;&#20851;&#29255;&#27573;&#30340;&#25903;&#25745;&#19982;&#39046;&#20808;&#31995;&#32479;&#30456;&#31454;&#20105;&#65292;&#20294;&#22312;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#22914;&#20854;&#20182;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35780;&#20272;&#20102;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; GPT-3.5-Turbo &#21644; GPT-4 &#22312; 2023 &#24180; BioASQ &#25361;&#25112;&#20013;&#30340;&#20219;&#21153;&#34920;&#29616;&#12290;&#22312;&#20219;&#21153; 11b &#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#20063;&#23601;&#26159;&#31572;&#26696;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#36890;&#36807;&#31616;&#21333;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#30456;&#20851;&#29255;&#27573;&#30340;&#25903;&#25745;&#34920;&#29616;&#20986;&#20102;&#19982;&#39046;&#20808;&#31995;&#32479;&#30456;&#31454;&#20105;&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#27809;&#26377;&#30456;&#20851;&#29255;&#27573;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20063;&#20196;&#20154;&#28385;&#24847;&#65292;&#23613;&#31649;&#27809;&#26377;&#36798;&#21040;&#26368;&#20339;&#31995;&#32479;&#30340;&#27700;&#24179;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36739;&#26087;&#19988;&#26356;&#20415;&#23452;&#30340; GPT-3.5-Turbo &#31995;&#32479;&#22312;&#22522;&#20110;&#20107;&#23454;&#21644;&#21015;&#34920;&#31572;&#26696;&#30340;&#38382;&#31572;&#29615;&#22659;&#20013;&#33021;&#22815;&#19982; GPT-4 &#30456;&#31454;&#20105;&#12290;&#22312;&#20219;&#21153; 11b &#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#20391;&#37325;&#20110;&#26816;&#32034;&#65292;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#26597;&#35810;&#25193;&#23637;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#19982;&#20854;&#20182;&#31995;&#32479;&#30456;&#27604;&#20173;&#28982;&#26377;&#25152;&#19981;&#36275;&#12290;&#37325;&#26032;&#36816;&#34892;&#36825;&#20123;&#23454;&#39564;&#25152;&#38656;&#30340;&#20195;&#30721;&#21487;&#22312; GitHub &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We assessed the performance of commercial Large Language Models (LLMs) GPT-3.5-Turbo and GPT-4 on tasks from the 2023 BioASQ challenge. In Task 11b Phase B, which is focused on answer generation, both models demonstrated competitive abilities with leading systems. Remarkably, they achieved this with simple zero-shot learning, grounded with relevant snippets. Even without relevant snippets, their performance was decent, though not on par with the best systems. Interestingly, the older and cheaper GPT-3.5-Turbo system was able to compete with GPT-4 in the grounded Q&amp;A setting on factoid and list answers. In Task 11b Phase A, focusing on retrieval, query expansion through zero-shot learning improved performance, but the models fell short compared to other systems. The code needed to rerun these experiments is available through GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2306.14096</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#23454;&#20307;&#32423;&#21035;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#26159;&#24773;&#24863;&#20998;&#26512;&#30340;&#37325;&#35201;&#23376;&#20219;&#21153;&#65292;&#30446;&#21069;&#38754;&#20020;&#30528;&#20247;&#22810;&#25361;&#25112;&#12290;&#20854;&#20013;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26469;&#33258;&#20110;&#32570;&#20047;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#37329;&#34701;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#30340;&#39640;&#36136;&#37327;&#22823;&#35268;&#27169;&#26631;&#27880;&#35821;&#26009;&#24211;&#65292;&#36825;&#38480;&#21046;&#20102;&#24320;&#21457;&#26377;&#25928;&#25991;&#26412;&#22788;&#29702;&#25216;&#26415;&#25152;&#38656;&#30340;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#35328;&#27169;&#24335;&#21305;&#37197;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#30340;&#29616;&#26377;&#24320;&#28304;LLMs&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#25105;&#20204;&#22362;&#20449;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#21160;&#30495;&#23454;&#19990;&#30028;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment analysis and currently faces numerous challenges. The primary challenge stems from the lack of high-quality and large-scale annotated corpora specifically designed for financial text sentiment analysis, which in turn limits the availability of data necessary for developing effective text processing techniques. Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which shoul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.03341</link><description>&lt;p&gt;
&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65306;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#23548;&#20986;&#30495;&#23454;&#30340;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Inference-Time Intervention: Eliciting Truthful Answers from a Language Model. (arXiv:2306.03341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#65292;&#26174;&#30528;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#12290;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#65292;ITI&#20351;LLaMA&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#20174;32.5%&#25552;&#39640;&#21040;65.1%&#12290;ITI&#26159;&#19968;&#31181;&#26368;&#23567;&#31243;&#24230;&#30340;&#24178;&#25200;&#65292;&#35745;&#31639;&#24265;&#20215;&#65292;&#19988;&#25968;&#25454;&#25928;&#29575;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#25512;&#29702;&#26102;&#38388;&#24178;&#39044;&#65288;ITI&#65289;&#25216;&#26415;&#65292;&#26088;&#22312;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30495;&#23454;&#24615;&#12290;ITI&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#27839;&#30528;&#19968;&#32452;&#26041;&#21521;&#31227;&#21160;&#27169;&#22411;&#28608;&#27963;&#65292;&#36328;&#36234;&#26377;&#38480;&#25968;&#37327;&#30340;&#27880;&#24847;&#21147;&#22836;&#12290;&#36825;&#31181;&#24178;&#39044;&#26174;&#30528;&#25552;&#39640;&#20102;LLaMA&#27169;&#22411;&#22312;TruthfulQA&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#25351;&#20196;&#24494;&#35843;&#30340;LLaMA Alpaca&#19978;&#65292;ITI&#23558;&#20854;&#30495;&#23454;&#24615;&#20174;32.5&#65285;&#25552;&#39640;&#21040;65.1&#65285;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30495;&#23454;&#24615;&#21644;&#21487;&#29992;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972;&#24178;&#39044;&#24378;&#24230;&#26469;&#24179;&#34913;&#23427;&#12290;ITI &#21462;&#24471;&#20102;&#26368;&#20302;&#31243;&#24230;&#30340;&#24178;&#25200;&#19988;&#35745;&#31639;&#24265;&#20215;&#12290;&#27492;&#22806;&#65292;&#35813;&#25216;&#26415;&#22312;&#25968;&#25454;&#25928;&#29575;&#19978;&#34920;&#29616;&#20248;&#24322;&#65306;&#34429;&#28982;&#20687;RLHF&#36825;&#26679;&#30340;&#26041;&#27861;&#38656;&#35201;&#24191;&#27867;&#27880;&#37322;&#65292;&#20294;&#26159;ITI&#20165;&#20351;&#29992;&#20102;&#20960;&#30334;&#20010;&#20363;&#23376;&#23601;&#33021;&#23450;&#20301;&#30495;&#23454;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#33021;&#20855;&#26377;&#26576;&#31181;&#20869;&#37096;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#26576;&#20107;&#26159;&#30495;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#20351;&#23427;&#20204;&#22312;&#34920;&#38754;&#19978;&#20135;&#29983;&#20102;&#34394;&#20551;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#24182;&#20026;&#20854;&#20998;&#37197;&#24773;&#24863;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#30456;&#20851;&#30340;&#23454;&#39564;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.16731</link><description>&lt;p&gt;
&#35782;&#21035;&#24773;&#24863;&#20307;&#39564;&#32773;&#20316;&#20026;&#24773;&#24863;&#20998;&#26512;&#30340;&#20808;&#20915;&#26465;&#20214;
&lt;/p&gt;
&lt;p&gt;
Emotion Experiencer Recognition as a Prerequisite for Experiencer-Specific Emotion Analysis. (arXiv:2305.16731v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#24182;&#20026;&#20854;&#20998;&#37197;&#24773;&#24863;&#30340;&#33258;&#21160;&#26041;&#27861;&#65292;&#24182;&#36827;&#34892;&#20102;&#30456;&#20851;&#30340;&#23454;&#39564;&#12290;&#35813;&#26041;&#27861;&#30340;&#23454;&#29616;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#35282;&#33394;&#26631;&#27880;&#26088;&#22312;&#25552;&#21462;&#25991;&#26412;&#20013;&#25551;&#36848;&#35841;&#32463;&#21382;&#24773;&#24863;&#12289;&#20026;&#20160;&#20040;&#20197;&#21450;&#23545;&#35841;&#30340;&#20449;&#24687;&#12290;&#36825;&#36890;&#24120;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24314;&#27169;&#20219;&#21153;&#65292;&#22914;&#26524;&#35201;&#22238;&#31572;&#30340;&#20027;&#35201;&#38382;&#39064;&#26159;&#35841;&#24863;&#21463;&#21040;&#20102;&#21738;&#31181;&#24773;&#24863;&#65292;&#36825;&#21487;&#33021;&#20250;&#36807;&#20110;&#22797;&#26434;&#12290;&#26412;&#25991;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#33258;&#21160;&#26816;&#27979;&#25991;&#26412;&#20013;&#30340;&#24773;&#24863;&#20307;&#39564;&#32773;&#24182;&#38543;&#21518;&#20026;&#20854;&#20998;&#37197;&#24773;&#24863;&#65292;&#23637;&#31034;&#20102;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#24773;&#24863;&#20307;&#39564;&#32773;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24182;&#21576;&#29616;&#20102;&#30456;&#20851;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion role labeling aims at extracting who is described in text to experience an emotion, why, and towards whom. This is often a challenging modelling task which might be overly sophisticated if the main question to answer is who feels which emotion. Recently, Troiano et al. (2022) proposed a data set that focuses on assigning emotion labels and appraisal labels to individual entities in text and Wegge et al. (2022) presented the first modelling experiments. Their experiencer-specific emotion prediction model has, however, only been evaluated on gold-annotated experiencers, due to the unavailability of an automatic experiencer detection approach. We fill this gap with the first experiments to automatically detect emotion experiencers in text and, subsequently, assign them emotions. We show that experiencer detection in text is a challenging task, with a precision of .82 and a recall of .56 (F1 =.66). Consequently, the performance of the experiencer-specific emotion detection pipeline
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31616;&#35201;&#27010;&#36848;&#20102;&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#21253;&#25324;&#25991;&#26723;&#20998;&#31867;&#21644;&#25688;&#35201;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;&#38271;&#25991;&#26412;NLP&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.16259</link><description>&lt;p&gt;
&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65306;&#29616;&#29366;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art. (arXiv:2305.16259v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31616;&#35201;&#27010;&#36848;&#20102;&#38271;&#25991;&#26412;&#30340;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#21253;&#25324;&#25991;&#26723;&#20998;&#31867;&#21644;&#25688;&#35201;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#26512;&#65292;&#21516;&#26102;&#36824;&#25506;&#35752;&#20102;&#38271;&#25991;&#26412;NLP&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#37319;&#29992;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#38271;&#25991;&#26412;&#20998;&#26512;&#30340;&#38656;&#27714;&#19982;&#30701;&#25991;&#26412;&#26377;&#24456;&#22823;&#19981;&#21516;&#65292;&#32780;&#32593;&#32476;&#19978;&#20256;&#36755;&#30340;&#25991;&#26723;&#22823;&#23567;&#19981;&#26029;&#22686;&#21152;&#65292;&#20351;&#38271;&#25991;&#26412;&#30340;&#33258;&#21160;&#29702;&#35299;&#25104;&#20026;&#19968;&#39033;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#30340;&#20004;&#20010;&#30446;&#26631;&#26159;&#65306;a&#65289;&#27010;&#36848;&#30456;&#20851;&#30340;&#31070;&#32463;&#26500;&#24314;&#27169;&#22359;&#65292;&#20316;&#20026;&#30701;&#25945;&#31243;&#65307;b&#65289;&#24635;&#32467;&#38271;&#25991;&#26412;NLP&#30340;&#29616;&#29366;&#65292;&#20027;&#35201;&#20851;&#27880;&#20004;&#20010;&#26680;&#24515;&#20219;&#21153;&#65306;&#25991;&#26723;&#20998;&#31867;&#21644;&#25991;&#26723;&#25688;&#35201;&#12290;&#24773;&#24863;&#20998;&#26512;&#20063;&#28085;&#30422;&#22312;&#20869;&#65292;&#22240;&#20026;&#23427;&#36890;&#24120;&#34987;&#35270;&#20026;&#25991;&#26723;&#20998;&#31867;&#30340;&#29305;&#20363;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#38271;&#25991;&#26412;NLP&#30456;&#20851;&#30340;&#20027;&#35201;&#25361;&#25112;&#12289;&#38382;&#39064;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#26368;&#21518;&#65292;&#20171;&#32461;&#20102;&#30456;&#20851;&#30340;&#20844;&#24320;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#20419;&#36827;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural Language Processing (NLP) during the past decade. However, the demands of long document analysis are quite different from those of shorter texts, while the ever increasing size of documents uploaded on-line renders automated understanding of long texts a critical area of research. This article has two goals: a) it overviews the relevant neural building blocks, thus serving as a short tutorial, and b) it surveys the state-of-the-art in long document NLP, mainly focusing on two central tasks: document classification and document summarization. Sentiment analysis for long texts is also covered, since it is typically treated as a particular case of document classification. Additionally, this article discusses the main challenges, issues and current solutions related to long document NLP. Finally, the relevant, publicly available, annotated datasets are presented, in order to facilitate further research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#37319;&#29992;&#22522;&#20110;&#35270;&#35273;&#24341;&#23548;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38899;&#33410;&#21457;&#29616;&#21644;&#36328;&#35821;&#35328;&#27867;&#21270;&#12290;&#20351;&#29992;&#26368;&#23567;&#21106;&#31639;&#27861;&#21644;2&#38454;&#27573;&#32858;&#31867;&#26041;&#27861;&#33258;&#21160;&#39044;&#27979;&#35821;&#38899;&#20013;&#30340;&#38899;&#33410;&#36793;&#30028;&#12290;&#22312;&#33521;&#35821;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38899;&#33410;&#20998;&#21106;&#26041;&#27861;&#65292;&#24182;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#22312;&#29233;&#27801;&#23612;&#20122;&#35821;&#19978;&#27867;&#21270;&#12290;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#20063;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2305.11435</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#24341;&#23548;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#20013;&#30340;&#38899;&#33410;&#21457;&#29616;&#21644;&#36328;&#35821;&#35328;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Mode. (arXiv:2305.11435v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#37319;&#29992;&#22522;&#20110;&#35270;&#35273;&#24341;&#23548;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#38899;&#33410;&#21457;&#29616;&#21644;&#36328;&#35821;&#35328;&#27867;&#21270;&#12290;&#20351;&#29992;&#26368;&#23567;&#21106;&#31639;&#27861;&#21644;2&#38454;&#27573;&#32858;&#31867;&#26041;&#27861;&#33258;&#21160;&#39044;&#27979;&#35821;&#38899;&#20013;&#30340;&#38899;&#33410;&#36793;&#30028;&#12290;&#22312;&#33521;&#35821;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38899;&#33410;&#20998;&#21106;&#26041;&#27861;&#65292;&#24182;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#22312;&#29233;&#27801;&#23612;&#20122;&#35821;&#19978;&#27867;&#21270;&#12290;&#22312;&#20854;&#20182;&#35821;&#35328;&#19978;&#20063;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#22522;&#20110;&#35270;&#35273;&#24341;&#23548;&#30340;&#35757;&#32451;&#30446;&#26631;&#35757;&#32451;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#26102;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#34920;&#31034;&#38899;&#33410;&#30340;&#21333;&#20803;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20960;&#20046;&#30456;&#21516;&#30340;&#27169;&#22411;&#32467;&#26500;&#65288;HuBERT&#65289;&#65292;&#22312;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#26102;&#27809;&#26377;&#34920;&#29616;&#20986;&#36825;&#31181;&#33021;&#21147;&#65292;&#36825;&#34920;&#26126;&#35270;&#35273;&#24341;&#23548;&#30446;&#26631;&#23548;&#33268;&#20102;&#36825;&#31181;&#29616;&#35937;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26368;&#23567;&#21106;&#31639;&#27861;&#33258;&#21160;&#39044;&#27979;&#35821;&#38899;&#20013;&#30340;&#38899;&#33410;&#36793;&#30028;&#65292;&#28982;&#21518;&#20351;&#29992;&#20004;&#38454;&#27573;&#32858;&#31867;&#26041;&#27861;&#23558;&#30456;&#21516;&#30340;&#38899;&#33410;&#32452;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#35757;&#32451;&#30340;&#35821;&#35328;&#65288;&#33521;&#35821;&#65289;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#38899;&#33410;&#20998;&#21106;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;&#29233;&#27801;&#23612;&#20122;&#35821;&#19978;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#36827;&#34892;&#27867;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#21516;&#30340;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;4&#31181;&#20854;&#20182;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#21333;&#35789;&#20998;&#21106;&#20219;&#21153;&#27867;&#21270;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20987;&#36133;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we show that representations capturing syllabic units emerge when training a self-supervised speech model with a visually-grounded training objective. We demonstrate that a nearly identical model architecture (HuBERT) trained with a masked language modeling loss does not exhibit this same ability, suggesting that the visual grounding objective is responsible for the emergence of this phenomenon. We propose the use of a minimum cut algorithm to automatically predict syllable boundaries in speech, followed by a 2-stage clustering method to group identical syllables together. We show that our model not only outperforms a state-of-the-art syllabic segmentation method on the language it was trained on (English), but also generalizes in a zero-shot fashion to Estonian. Finally, we show that the same model is capable of zero-shot generalization for a word segmentation task on 4 other languages from the Zerospeech Challenge, in some cases beating the previous state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#35789;&#27719;&#30693;&#35782;&#24211;&#30340;&#35789;&#20041;&#20449;&#24687;&#26469;&#35299;&#20915;&#21407;&#26469;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#20013;&#30340;&#22810;&#20041;&#35789;&#38382;&#39064;&#12290;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#21152;&#20837;&#35789;&#20041;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340; GPT-3 &#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#35789;&#20856;&#22806;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.01788</link><description>&lt;p&gt;
&#35270;&#35273;&#19982;&#23450;&#20041;&#30456;&#36935;&#65306;&#34701;&#21512;&#35789;&#20041;&#20449;&#24687;&#30340;&#26080;&#30417;&#30563;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;
&lt;/p&gt;
&lt;p&gt;
Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information. (arXiv:2305.01788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#37096;&#35789;&#27719;&#30693;&#35782;&#24211;&#30340;&#35789;&#20041;&#20449;&#24687;&#26469;&#35299;&#20915;&#21407;&#26469;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#20013;&#30340;&#22810;&#20041;&#35789;&#38382;&#39064;&#12290;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#21152;&#20837;&#35789;&#20041;&#23450;&#20041;&#65292;&#24182;&#36890;&#36807;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340; GPT-3 &#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#35789;&#20856;&#22806;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26159;&#19968;&#39033;&#20219;&#21153;&#65292;&#26088;&#22312;&#25214;&#21040;&#26368;&#20934;&#30830;&#22320;&#25551;&#36848;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#30446;&#26631;&#35789;&#27491;&#30830;&#24847;&#20041;&#30340;&#22270;&#20687;&#12290;&#20197;&#24448;&#30340;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#27169;&#22411;&#24448;&#24448;&#21463;&#21040;&#35789;&#20041;&#22810;&#20041;&#24615;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22806;&#37096;&#35789;&#27719;&#30693;&#35782;&#24211;&#30340;&#35789;&#27719;&#20449;&#24687;&#65292;&#29305;&#21035;&#26159;&#35789;&#20041;&#23450;&#20041;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#27809;&#26377;&#25552;&#20379;&#31572;&#26696;&#30340;&#35789;&#20041;&#20449;&#24687;&#26102;&#65292;&#37319;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#21152;&#20837;&#35789;&#20041;&#23450;&#20041;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25913;&#36827;&#35789;&#20856;&#22806;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;GPT-3&#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#26041;&#27861;&#26126;&#26174;&#25552;&#39640;&#20102;&#35270;&#35273;&#35789;&#20041;&#28040;&#27495;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#22312;&#35789;&#20856;&#22806;&#20363;&#23376;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#23450;&#20041;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Word Sense Disambiguation (VWSD) is a task to find the image that most accurately depicts the correct sense of the target word for the given context. Previously, image-text matching models often suffered from recognizing polysemous words. This paper introduces an unsupervised VWSD approach that uses gloss information of an external lexical knowledge-base, especially the sense definitions. Specifically, we suggest employing Bayesian inference to incorporate the sense definitions when sense information of the answer is not provided. In addition, to ameliorate the out-of-dictionary (OOD) issue, we propose a context-aware definition generation with GPT-3. Experimental results show that the VWSD performance significantly increased with our Bayesian inference-based approach. In addition, our context-aware definition generation achieved prominent performance improvement in OOD examples exhibiting better performance than the existing definition generation method. We will publish source 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;&#25968;&#23383;&#23402;&#29983;&#21644;&#24037;&#19994;&#33258;&#21160;&#21270;&#31995;&#32479;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#29983;&#20135;&#36807;&#31243;&#30340;&#26234;&#33021;&#21270;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#36890;&#36807;LLM&#20195;&#29702;&#30340;&#21327;&#35843;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#29983;&#20135;&#30340;&#33258;&#20027;&#35268;&#21010;&#21644;&#25511;&#21046;&#65292;&#33021;&#22815;&#22788;&#29702;&#26410;&#39044;&#23450;&#20041;&#30340;&#20219;&#21153;&#24182;&#35268;&#21010;&#29983;&#20135;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.14721</link><description>&lt;p&gt;
&#26397;&#33258;&#20027;&#31995;&#32479;&#36808;&#36827;&#65306;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22686;&#24378;&#30340;&#28789;&#27963;&#27169;&#22359;&#21270;&#29983;&#20135;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards autonomous system: flexible modular production system enhanced with large language model agents. (arXiv:2304.14721v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;&#25968;&#23383;&#23402;&#29983;&#21644;&#24037;&#19994;&#33258;&#21160;&#21270;&#31995;&#32479;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#29983;&#20135;&#36807;&#31243;&#30340;&#26234;&#33021;&#21270;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#36890;&#36807;LLM&#20195;&#29702;&#30340;&#21327;&#35843;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#29983;&#20135;&#30340;&#33258;&#20027;&#35268;&#21010;&#21644;&#25511;&#21046;&#65292;&#33021;&#22815;&#22788;&#29702;&#26410;&#39044;&#23450;&#20041;&#30340;&#20219;&#21153;&#24182;&#35268;&#21010;&#29983;&#20135;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#25968;&#23383;&#23402;&#29983;&#21644;&#24037;&#19994;&#33258;&#21160;&#21270;&#31995;&#32479;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#29983;&#20135;&#36807;&#31243;&#30340;&#26234;&#33021;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#24320;&#21457;&#21253;&#21547;&#29983;&#20135;&#25551;&#36848;&#20449;&#24687;&#30340;&#25968;&#23383;&#23402;&#29983;&#31995;&#32479;&#65292;&#24182;&#23558;&#33258;&#21160;&#21270;&#31995;&#32479;&#25913;&#36896;&#20026;&#25552;&#20379;&#32479;&#19968;&#25509;&#21475;&#30340;&#32454;&#31890;&#24230;&#21151;&#33021;&#25110;&#27169;&#22359;&#65292;&#20197;&#20379;&#33258;&#21160;&#21270;&#32452;&#20214;&#25110;&#27169;&#22359;&#25191;&#34892;&#12290;&#38543;&#21518;&#65292;&#35774;&#35745;LLM&#20195;&#29702;&#26469;&#35299;&#37322;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#25551;&#36848;&#24615;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;RESTful&#25509;&#21475;&#25511;&#21046;&#29289;&#29702;&#31995;&#32479;&#12290;&#36825;&#20123;LLM&#20195;&#29702;&#20316;&#20026;&#33258;&#21160;&#21270;&#31995;&#32479;&#20869;&#30340;&#26234;&#33021;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#29983;&#20135;&#30340;&#33258;&#20027;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#32473;&#23450;&#19968;&#20010;&#20219;&#21153;&#25351;&#20196;&#20316;&#20026;&#36755;&#20837;&#65292;LLM&#20195;&#29702;&#21327;&#35843;&#19968;&#31995;&#21015;&#21407;&#23376;&#21151;&#33021;&#21644;&#25216;&#33021;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#23454;&#29616;&#30340;&#21407;&#22411;&#22914;&#20309;&#22788;&#29702;&#26410;&#39044;&#23450;&#20041;&#30340;&#20219;&#21153;&#65292;&#24182;&#35745;&#21010;&#29983;&#20135;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel framework that combines large language models (LLMs), digital twins and industrial automation system to enable intelligent planning and control of production processes. Our approach involves developing a digital twin system that contains descriptive information about the production and retrofitting the automation system to offer unified interfaces of fine-granular functionalities or skills executable by automation components or modules. Subsequently, LLM-Agents are designed to interpret descriptive information in the digital twins and control the physical system through RESTful interfaces. These LLM-Agents serve as intelligent agents within an automation system, enabling autonomous planning and control of flexible production. Given a task instruction as input, the LLM-agents orchestrate a sequence of atomic functionalities and skills to accomplish the task. We demonstrate how our implemented prototype can handle un-predefined tasks, plan a production p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;BERT&#25216;&#26415;&#25506;&#31350;&#20102;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;BERT&#27169;&#22411;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;80%&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;60%&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.08649</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#30340;&#25216;&#26415;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of US Supreme Court Cases using BERT-Based Techniques. (arXiv:2304.08649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;BERT&#25216;&#26415;&#25506;&#31350;&#20102;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;BERT&#27169;&#22411;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;80%&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;60%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#65288;BERT&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65288;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#65292;&#35789;&#24615;&#65288;POS&#65289;&#26631;&#35760;&#31561;&#65289;&#19978;&#20135;&#29983;&#20102;&#26368;&#26032;&#25216;&#26415;&#65288;SOTA&#65289;&#32467;&#26524;&#12290;&#24403;&#20998;&#31867;&#38271;&#25991;&#26723;&#65288;&#20363;&#22914;&#26469;&#33258;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#30340;&#25991;&#26723;&#65289;&#26102;&#65292;&#20351;&#29992;BERT&#27169;&#22411;&#21487;&#33021;&#27604;&#36739;&#22256;&#38590;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#20960;&#31181;&#22522;&#20110;BERT&#30340;&#20998;&#31867;&#25216;&#26415;&#65292;&#29992;&#20110;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#20915;&#23450;&#25110;&#26368;&#39640;&#27861;&#38498;&#25968;&#25454;&#24211;&#65288;SCDB&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#20808;&#21069;&#30340;SOTA&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#38024;&#23545;&#38271;&#25991;&#26723;&#30340;SOTA&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;&#65288;1&#65289;&#24191;&#27867;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#20855;&#26377;15&#20010;&#31867;&#21035;&#65307;&#65288;2&#65289;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#20855;&#26377;279&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#32467;&#26524;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#20135;&#29983;80&#65285;&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#20135;&#29983;60&#65285;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models based on bidirectional encoder representations from transformers (BERT) produce state of the art (SOTA) results on many natural language processing (NLP) tasks such as named entity recognition (NER), part-of-speech (POS) tagging etc. An interesting phenomenon occurs when classifying long documents such as those from the US supreme court where BERT-based models can be considered difficult to use on a first-pass or out-of-the-box basis. In this paper, we experiment with several BERT-based classification techniques for US supreme court decisions or supreme court database (SCDB) and compare them with the previous SOTA results. We then compare our results specifically with SOTA models for long documents. We compare our results for two classification tasks: (1) a broad classification task with 15 categories and (2) a fine-grained classification task with 279 categories. Our best result produces an accuracy of 80\% on the 15 broad categories and 60\% on the fine-grained 279 categories 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;ELVIS&#65292;&#21487;&#20197;&#22686;&#24378;&#25918;&#23556;&#23398;&#25253;&#21578;&#25110; X &#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#23616;&#37096;&#24615;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#29702;&#35299;&#20301;&#32622;&#21442;&#32771;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.05303</link><description>&lt;p&gt;
ELVIS: &#21033;&#29992;&#27169;&#24577;&#20869;&#30456;&#20284;&#24615;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#30340;&#23616;&#37096;&#24615;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ELVIS: Empowering Locality of Vision Language Pre-training with Intra-modal Similarity. (arXiv:2304.05303v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;ELVIS&#65292;&#21487;&#20197;&#22686;&#24378;&#25918;&#23556;&#23398;&#25253;&#21578;&#25110; X &#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#23616;&#37096;&#24615;&#33021;&#21147;&#65292;&#25552;&#39640;&#20102;&#29702;&#35299;&#20301;&#32622;&#21442;&#32771;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#36741;&#21161;&#25918;&#23556;&#31185;&#21307;&#29983;&#38405;&#35835;&#33016;&#37096; X &#23556;&#32447;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#20854;&#38656;&#35201;&#26114;&#36149;&#30340;&#27880;&#37322;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#36825;&#38459;&#30861;&#20102;&#20854;&#24191;&#27867;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#24120;&#35268;&#29983;&#25104;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20943;&#36731;&#27880;&#37322;&#30340;&#36127;&#25285;&#21644;&#25104;&#26412;&#65292;&#36825;&#20123;&#25253;&#21578;&#20197;&#25104;&#23545;&#30340;&#24418;&#24335;&#65288;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65289;&#22823;&#37327;&#23384;&#22312;&#12290;&#27492;&#22806;&#65292;&#27491;&#22312;&#25552;&#20986;&#25193;&#23637;&#21040;&#23450;&#20301;&#24863;&#30693;VLP&#65292;&#20197;&#28385;&#36275;CAD&#22312;CXR&#30340;&#20934;&#30830;&#24322;&#24120;&#23450;&#20301;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#30001;&#23616;&#37096;&#24615;VLP&#25991;&#29486;&#25552;&#20986;&#30340;&#20844;&#24335;&#23454;&#38469;&#19978;&#23548;&#33268;&#20102;&#19979;&#28216;&#23450;&#20301;&#20219;&#21153;&#25152;&#38656;&#30340;&#31354;&#38388;&#20851;&#31995;&#30340;&#20002;&#22833;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Empowering Locality of VLP with Intra-modal Similarity&#65288;ELVIS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;VLP&#65292;&#21487;&#24863;&#30693;&#27169;&#24577;&#20869;&#37096;&#30340;&#23616;&#37096;&#24615;&#33021;&#21147;&#65292;&#20197;&#26356;&#22909;&#22320;&#20445;&#30041;&#25918;&#23556;&#23398;&#25253;&#21578;&#25110; X &#23556;&#32447;&#22270;&#20687;&#20013;&#30340;&#23616;&#37096;&#24615;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29702;&#35299;&#20301;&#32622;&#21442;&#32771;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning has shown great potential in assisting radiologists in reading chest X-ray (CXR) images, but its need for expensive annotations for improving performance prevents widespread clinical application. Visual language pre-training (VLP) can alleviate the burden and cost of annotation by leveraging routinely generated reports for radiographs, which exist in large quantities as well as in paired form (imagetext pairs). Additionally, extensions to localization-aware VLPs are being proposed to address the needs of accurate localization of abnormalities for CAD in CXR. However, we find that the formulation proposed by locality-aware VLP literatures actually leads to loss in spatial relationships required for downstream localization tasks. Therefore, we propose Empowering Locality of VLP with Intra-modal Similarity, ELVIS, a VLP aware of intra-modal locality, to better preserve the locality within radiographs or reports, which enhances the ability to comprehend location references in
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#25945;&#32946;&#20013;&#26377;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#25991;&#26412;&#20869;&#23481;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21019;&#26032;&#30340;&#23454;&#38469;&#24615;&#21644;&#20262;&#29702;&#24615;&#23384;&#22312;&#25285;&#24551;&#65292;&#38656;&#35201;&#32771;&#34385;&#25216;&#26415;&#21487;&#34892;&#24615;&#12289;&#38544;&#31169;&#12289;&#24179;&#31561;&#21644;&#21892;&#24847;&#31561;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2303.13379</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#20013;&#30340;&#23454;&#38469;&#21644;&#20262;&#29702;&#25361;&#25112;&#65306;&#19968;&#39033;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Practical and Ethical Challenges of Large Language Models in Education: A Systematic Literature Review. (arXiv:2303.13379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13379
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#25945;&#32946;&#20013;&#26377;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#25991;&#26412;&#20869;&#23481;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21019;&#26032;&#30340;&#23454;&#38469;&#24615;&#21644;&#20262;&#29702;&#24615;&#23384;&#22312;&#25285;&#24551;&#65292;&#38656;&#35201;&#32771;&#34385;&#25216;&#26415;&#21487;&#34892;&#24615;&#12289;&#38544;&#31169;&#12289;&#24179;&#31561;&#21644;&#21892;&#24847;&#31561;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24320;&#21457;&#30340;&#25945;&#32946;&#25216;&#26415;&#21019;&#26032;&#26174;&#31034;&#20986;&#33258;&#21160;&#29983;&#25104;&#21644;&#20998;&#26512;&#25991;&#26412;&#20869;&#23481;&#30340;&#28508;&#21147;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#21019;&#26032;&#26469;&#33258;&#21160;&#21270;&#21508;&#31181;&#25945;&#32946;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#29983;&#25104;&#38382;&#39064;&#12289;&#25552;&#20379;&#21453;&#39304;&#21644;&#35780;&#20998;&#65289;&#65292;&#20294;&#23545;&#36825;&#20123;&#21019;&#26032;&#30340;&#23454;&#38469;&#24615;&#21644;&#20262;&#29702;&#24615;&#23384;&#22312;&#25285;&#24551;&#12290;&#36825;&#20123;&#25285;&#24551;&#21487;&#33021;&#20250;&#38459;&#30861;&#26410;&#26469;&#30740;&#31350;&#21644;&#22312;&#30495;&#23454;&#25945;&#32946;&#29615;&#22659;&#20013;&#37319;&#29992;&#22522;&#20110;LLMs&#30340;&#21019;&#26032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;118&#31687;&#33258;2017&#24180;&#20197;&#26469;&#21457;&#34920;&#30340;&#21516;&#34892;&#35780;&#35758;&#35770;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#20197;&#30830;&#23450;&#20351;&#29992;LLMs&#33258;&#21160;&#21270;&#21644;&#25903;&#25345;&#25945;&#32946;&#20219;&#21153;&#30340;&#24403;&#21069;&#30740;&#31350;&#29366;&#24577;&#12290;&#36890;&#36807;&#35780;&#20272;&#20854;&#25216;&#26415;&#21487;&#34892;&#24615;&#12289;&#27169;&#22411;&#24615;&#33021;&#12289;&#21487;&#22797;&#21046;&#24615;&#12289;&#31995;&#32479;&#36879;&#26126;&#24230;&#12289;&#38544;&#31169;&#12289;&#24179;&#31561;&#21644;&#21892;&#24847;&#65292;&#36824;&#30830;&#23450;&#20102;LLMs&#21019;&#26032;&#30340;&#23454;&#38469;&#21644;&#20262;&#29702;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Educational technology innovations that have been developed based on large language models (LLMs) have shown the potential to automate the laborious process of generating and analysing textual content. While various innovations have been developed to automate a range of educational tasks (e.g., question generation, feedback provision, and essay grading), there are concerns regarding the practicality and ethicality of these innovations. Such concerns may hinder future research and the adoption of LLMs-based innovations in authentic educational contexts. To address this, we conducted a systematic literature review of 118 peer-reviewed papers published since 2017 to pinpoint the current state of research on using LLMs to automate and support educational tasks. The practical and ethical challenges of LLMs-based innovations were also identified by assessing their technological readiness, model performance, replicability, system transparency, privacy, equality, and beneficence. The findings 
&lt;/p&gt;</description></item><item><title>MenuCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#36741;&#21161;&#35774;&#35745;&#24072;&#65292;&#36890;&#36807;&#23545;&#35805;&#31995;&#32479;&#19982;&#35774;&#35745;&#24072;&#21327;&#20316;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#33756;&#21333;&#35774;&#35745;&#24037;&#20855;&#65292;&#21487;&#20197;&#31616;&#21270;&#33756;&#21333;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#25903;&#25345;&#38646;/&#23569;&#27425;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.04496</link><description>&lt;p&gt;
MenuCraft: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#33756;&#21333;&#31995;&#32479;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
MenuCraft: Interactive Menu System Design with Large Language Models. (arXiv:2303.04496v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04496
&lt;/p&gt;
&lt;p&gt;
MenuCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;AI&#36741;&#21161;&#35774;&#35745;&#24072;&#65292;&#36890;&#36807;&#23545;&#35805;&#31995;&#32479;&#19982;&#35774;&#35745;&#24072;&#21327;&#20316;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#33756;&#21333;&#35774;&#35745;&#24037;&#20855;&#65292;&#21487;&#20197;&#31616;&#21270;&#33756;&#21333;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#25903;&#25345;&#38646;/&#23569;&#27425;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33756;&#21333;&#31995;&#32479;&#35774;&#35745;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#35768;&#22810;&#35774;&#35745;&#36873;&#39033;&#21644;&#21508;&#31181;&#20154;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MenuCraft&#30340;AI&#36741;&#21161;&#35774;&#35745;&#24072;&#65292;&#36890;&#36807;&#35774;&#35745;&#21644;&#32454;&#21270;&#33756;&#21333;&#31995;&#32479;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#23454;&#29616;&#35774;&#35745;&#24072;&#19982;&#23545;&#35805;&#31995;&#32479;&#20043;&#38388;&#30340;&#21327;&#20316;&#12290;MenuCraft&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#35328;&#30340;&#20132;&#20114;&#24335;&#33756;&#21333;&#35774;&#35745;&#24037;&#20855;&#65292;&#31616;&#21270;&#20102;&#33756;&#21333;&#35774;&#35745;&#36807;&#31243;&#65292;&#24182;&#23454;&#29616;&#20102;&#35774;&#35745;&#36873;&#39033;&#30340;&#36731;&#26494;&#23450;&#21046;&#12290;MenuCraft&#36890;&#36807;&#23545;&#35805;&#25903;&#25345;&#21508;&#31181;&#20132;&#20114;&#26041;&#24335;&#65292;&#21487;&#20197;&#36827;&#34892;&#38646;/&#23569;&#27425;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Menu system design is a challenging task involving many design options and various human factors. For example, one crucial factor that designers need to consider is the semantic and systematic relation of menu commands. However, capturing these relations can be challenging due to limited available resources. With the advancement of neural language models, large language models can utilize their vast pre-existing knowledge in designing and refining menu systems. In this paper, we propose MenuCraft, an AI-assisted designer for menu design that enables collaboration between the designer and a dialogue system to design menus. MenuCraft offers an interactive language-based menu design tool that simplifies the menu design process and enables easy customization of design options. MenuCraft supports a variety of interactions through dialog that allows performing zero/few-shot learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;Transformer&#23398;&#20064;&#35821;&#20041;&#32467;&#26500;&#30340;&#26426;&#21046;&#24615;&#29702;&#35299;&#65292;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#23884;&#20837;&#23618;&#21644;&#33258;&#27880;&#24847;&#21147;&#23618;&#22914;&#20309;&#23545;&#35789;&#27719;&#30340;&#20849;&#29616;&#32467;&#26500;&#36827;&#34892;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2303.04245</link><description>&lt;p&gt;
Transformers&#22914;&#20309;&#23398;&#20064;&#20027;&#39064;&#32467;&#26500;&#65306;&#36208;&#21521;&#23545;&#20854;&#26426;&#21046;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding. (arXiv:2303.04245v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;Transformer&#23398;&#20064;&#35821;&#20041;&#32467;&#26500;&#30340;&#26426;&#21046;&#24615;&#29702;&#35299;&#65292;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#21644;&#23454;&#39564;&#35777;&#26126;&#20102;&#23884;&#20837;&#23618;&#21644;&#33258;&#27880;&#24847;&#21147;&#23618;&#22914;&#20309;&#23545;&#35789;&#27719;&#30340;&#20849;&#29616;&#32467;&#26500;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Transformer&#22312;&#35768;&#22810;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23545;&#20854;&#23398;&#20064;&#26426;&#21046;&#30340;&#20934;&#30830;&#29702;&#35299;&#20173;&#28982;&#23384;&#22312;&#36739;&#22823;&#30340;&#32570;&#20047;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#21253;&#25324;&#21508;&#31181;&#32467;&#26500;&#21270;&#21644;&#25512;&#29702;&#20219;&#21153;&#22312;&#20869;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#25968;&#23398;&#29702;&#35299;&#30340;&#30740;&#31350;&#20173;&#28982;&#28382;&#21518;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#20174;&#34920;&#31034;&#26041;&#38754;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#21363;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32593;&#32476;&#30340;&#22823;&#23567;/&#28145;&#24230;/&#22797;&#26434;&#24615;&#29992;&#20110;&#25191;&#34892;&#26576;&#20123;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24182;&#19981;&#33021;&#20445;&#35777;&#23398;&#20064;&#21160;&#24577;&#20250;&#25910;&#25947;&#21040;&#25152;&#25552;&#20986;&#30340;&#32467;&#26500;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#32454;&#33268;&#20837;&#24494;&#30340;&#26426;&#21046;&#29702;&#35299;&#65292;&#38416;&#26126;&#20102;Transformer&#22914;&#20309;&#23398;&#20064;&#8220;&#35821;&#20041;&#32467;&#26500;&#8221;&#65292;&#21363;&#25429;&#25417;&#35789;&#27719;&#30340;&#20849;&#29616;&#32467;&#26500;&#12290;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#21644;&#23545;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#20197;&#21450;&#30001;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#24314;&#27169;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#23884;&#20837;&#23618;&#21644;&#33258;&#27880;&#24847;&#21147;&#23618;&#22914;&#20309;&#23545;&#20027;&#39064;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the successes of transformers across many domains are indisputable, accurate understanding of the learning mechanics is still largely lacking. Their capabilities have been probed on benchmarks which include a variety of structured and reasoning tasks -- but mathematical understanding is lagging substantially behind. Recent lines of work have begun studying representational aspects of this question: that is, the size/depth/complexity of attention-based networks to perform certain tasks. However, there is no guarantee the learning dynamics will converge to the constructions proposed. In our paper, we provide fine-grained mechanistic understanding of how transformers learn "semantic structure", understood as capturing co-occurrence structure of words. Precisely, we show, through a combination of mathematical analysis and experiments on Wikipedia data and synthetic data modeled by Latent Dirichlet Allocation (LDA), that the embedding layer and the self-attention layer encode the topi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#26032;&#38395;&#25991;&#31456;&#20013;&#26816;&#27979;&#26377;&#23475;&#35758;&#31243;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#38395;&#25991;&#31456;&#27880;&#37322;&#25968;&#25454;&#38598;&#20197;&#20379;&#30740;&#31350;&#20351;&#29992;&#12290;&#30740;&#31350;&#32773;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#31995;&#32479;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#21644;&#40657;&#30418;&#27169;&#22411;&#26377;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.00102</link><description>&lt;p&gt;
&#22312;&#26032;&#38395;&#25991;&#31456;&#20013;&#26816;&#27979;&#26377;&#23475;&#35758;&#31243;
&lt;/p&gt;
&lt;p&gt;
Detecting Harmful Agendas in News Articles. (arXiv:2302.00102v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00102
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22312;&#26032;&#38395;&#25991;&#31456;&#20013;&#26816;&#27979;&#26377;&#23475;&#35758;&#31243;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#26032;&#38395;&#25991;&#31456;&#27880;&#37322;&#25968;&#25454;&#38598;&#20197;&#20379;&#30740;&#31350;&#20351;&#29992;&#12290;&#30740;&#31350;&#32773;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#31995;&#32479;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#21644;&#40657;&#30418;&#27169;&#22411;&#26377;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#19978;&#25805;&#32437;&#26032;&#38395;&#26159;&#19968;&#20010;&#26085;&#30410;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#20351;&#29992;&#33258;&#21160;&#21270;&#31995;&#32479;&#26469;&#36943;&#21046;&#20854;&#20256;&#25773;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#34429;&#28982;&#35823;&#23548;&#20449;&#24687;&#21644;&#34394;&#20551;&#20449;&#24687;&#30340;&#26816;&#27979;&#24050;&#32463;&#24471;&#21040;&#30740;&#31350;&#65292;&#20294;&#22312;&#26816;&#27979;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#26377;&#23475;&#35758;&#31243;&#36825;&#19968;&#37325;&#35201;&#25361;&#25112;&#26041;&#38754;&#32570;&#20047;&#25237;&#36164;&#65307;&#35782;&#21035;&#26377;&#23475;&#35758;&#31243;&#23545;&#20110;&#35782;&#21035;&#20855;&#26377;&#26368;&#22823;&#28508;&#22312;&#29616;&#23454;&#21361;&#23475;&#30340;&#26032;&#38395;&#36816;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23545;&#23457;&#26597;&#21046;&#24230;&#23384;&#22312;&#30495;&#23454;&#30340;&#25285;&#24551;&#65292;&#26377;&#23475;&#35758;&#31243;&#26816;&#27979;&#22120;&#24517;&#39035;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#25165;&#33021;&#21457;&#25381;&#20316;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#19968;&#20840;&#26032;&#30340;&#20219;&#21153;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#21517;&#20026;NewsAgendas&#30340;&#26032;&#38395;&#25991;&#31456;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35758;&#31243;&#35782;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#35299;&#37322;&#31995;&#32479;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#21487;&#20197;&#19982;&#40657;&#30418;&#27169;&#22411;&#20855;&#26377;&#30456;&#24403;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manipulated news online is a growing problem which necessitates the use of automated systems to curtail its spread. We argue that while misinformation and disinformation detection have been studied, there has been a lack of investment in the important open challenge of detecting harmful agendas in news articles; identifying harmful agendas is critical to flag news campaigns with the greatest potential for real world harm. Moreover, due to real concerns around censorship, harmful agenda detectors must be interpretable to be effective. In this work, we propose this new task and release a dataset, NewsAgendas, of annotated news articles for agenda identification. We show how interpretable systems can be effective on this task and demonstrate that they can perform comparably to black-box models.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectGPT&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27010;&#29575;&#26354;&#29575;&#26469;&#21028;&#26029;&#25991;&#26412;&#26159;&#21542;&#30001;&#19968;&#20010;&#32473;&#23450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#35757;&#32451;&#20998;&#31867;&#22120;&#12289;&#25910;&#38598;&#25968;&#25454;&#38598;&#25110;&#26126;&#30830;&#21152;&#27700;&#21360;&#65292;&#21482;&#20351;&#29992;&#27169;&#22411;&#35745;&#31639;&#30340;&#23545;&#25968;&#27010;&#29575;&#21644;&#21478;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38543;&#26426;&#25200;&#21160;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DetectGPT&#22312;&#27169;&#22411;&#37319;&#26679;&#26041;&#38754;&#27604;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#26356;&#20855;&#26377;&#21306;&#20998;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2301.11305</link><description>&lt;p&gt;
DetectGPT&#65306;&#20351;&#29992;&#27010;&#29575;&#26354;&#29575;&#36827;&#34892;&#38646;&#26679;&#26412;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. (arXiv:2301.11305v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectGPT&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#27010;&#29575;&#26354;&#29575;&#26469;&#21028;&#26029;&#25991;&#26412;&#26159;&#21542;&#30001;&#19968;&#20010;&#32473;&#23450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#35757;&#32451;&#20998;&#31867;&#22120;&#12289;&#25910;&#38598;&#25968;&#25454;&#38598;&#25110;&#26126;&#30830;&#21152;&#27700;&#21360;&#65292;&#21482;&#20351;&#29992;&#27169;&#22411;&#35745;&#31639;&#30340;&#23545;&#25968;&#27010;&#29575;&#21644;&#21478;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38543;&#26426;&#25200;&#21160;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;DetectGPT&#22312;&#27169;&#22411;&#37319;&#26679;&#26041;&#38754;&#27604;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#26356;&#20855;&#26377;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27969;&#30021;&#24230;&#21644;&#24191;&#27867;&#20351;&#29992;&#31361;&#26174;&#20102;&#24076;&#26395;&#26377;&#30456;&#24212;&#30340;&#24037;&#20855;&#26469;&#24110;&#21161;&#26816;&#27979;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;LLM&#27010;&#29575;&#20989;&#25968;&#32467;&#26500;&#30340;&#19968;&#20010;&#26377;&#29992;&#23646;&#24615;&#65292;&#23545;&#20110;&#36825;&#31181;&#26816;&#27979;&#38750;&#24120;&#26377;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20174;LLM&#20013;&#37319;&#26679;&#30340;&#25991;&#26412;&#20542;&#21521;&#20110;&#21344;&#25454;&#27169;&#22411;&#30340;&#23545;&#25968;&#27010;&#29575;&#20989;&#25968;&#30340;&#36127;&#26354;&#29575;&#21306;&#22495;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26354;&#29575;&#30340;&#20934;&#21017;&#65292;&#29992;&#20110;&#21028;&#26029;&#19968;&#20010;&#27573;&#33853;&#26159;&#21542;&#26159;&#30001;&#32473;&#23450;&#30340;LLM&#29983;&#25104;&#30340;&#12290;&#36825;&#31181;&#26041;&#27861;&#34987;&#31216;&#20026;DetectGPT&#65292;&#19981;&#38656;&#35201;&#35757;&#32451;&#21333;&#29420;&#30340;&#20998;&#31867;&#22120;&#65292;&#25910;&#38598;&#30495;&#23454;&#25110;&#29983;&#25104;&#27573;&#33853;&#30340;&#25968;&#25454;&#38598;&#65292;&#20063;&#19981;&#38656;&#35201;&#26126;&#30830;&#22320;&#32473;&#29983;&#25104;&#30340;&#25991;&#26412;&#21152;&#27700;&#21360;&#12290;&#23427;&#21482;&#20351;&#29992;&#25152;&#20851;&#27880;&#27169;&#22411;&#35745;&#31639;&#30340;&#23545;&#25968;&#27010;&#29575;&#21644;&#26469;&#33258;&#21478;&#19968;&#20010;&#36890;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;T5&#65289;&#30340;&#27573;&#33853;&#30340;&#38543;&#26426;&#25200;&#21160;&#12290;&#25105;&#20204;&#21457;&#29616;DetectGPT&#27604;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#26356;&#20855;&#26377;&#21306;&#20998;&#33021;&#21147;&#65292;&#29992;&#20110;&#27169;&#22411;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model samp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#26497;&#22823;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;VLPLMs&#65289;&#21019;&#20316;&#25925;&#20107;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#36890;&#36807;&#19982;SOTA&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#27604;&#36739;&#65292;&#35777;&#26126;VLPLMs&#29983;&#25104;&#30340;&#25925;&#20107;&#36136;&#37327;&#26356;&#39640;&#65292;&#24182;&#23637;&#31034;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#19982;&#20154;&#31867;&#20316;&#32773;&#30456;&#25239;&#34913;&#65292;&#23613;&#31649;&#21021;&#27493;&#35843;&#26597;&#25581;&#31034;&#20102;&#23427;&#20204;&#20542;&#21521;&#20110;&#8220;&#25220;&#34989;&#8221;&#30495;&#23454;&#30340;&#25925;&#20107;&#12290;</title><link>http://arxiv.org/abs/2301.09790</link><description>&lt;p&gt;
&#26497;&#22823;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22312;&#24456;&#23569;&#30340;&#26679;&#20363;&#19979;&#23398;&#20064;&#25925;&#20107;&#21019;&#20316;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Very Large Pretrained Language Models Learn Storytelling With A Few Examples?. (arXiv:2301.09790v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#26497;&#22823;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;VLPLMs&#65289;&#21019;&#20316;&#25925;&#20107;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#36890;&#36807;&#19982;SOTA&#27169;&#22411;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#27604;&#36739;&#65292;&#35777;&#26126;VLPLMs&#29983;&#25104;&#30340;&#25925;&#20107;&#36136;&#37327;&#26356;&#39640;&#65292;&#24182;&#23637;&#31034;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#19982;&#20154;&#31867;&#20316;&#32773;&#30456;&#25239;&#34913;&#65292;&#23613;&#31649;&#21021;&#27493;&#35843;&#26597;&#25581;&#31034;&#20102;&#23427;&#20204;&#20542;&#21521;&#20110;&#8220;&#25220;&#34989;&#8221;&#30495;&#23454;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#35821;&#27861;&#36890;&#39034;&#30340;&#21477;&#23376;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25925;&#20107;&#65292;&#20294;&#26159;&#23427;&#20204;&#38590;&#20197;&#29983;&#25104;&#36830;&#36143;&#12289;&#26377;&#24847;&#20041;&#21644;&#26377;&#36259;&#30340;&#25925;&#20107;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25925;&#20107;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#25506;&#32034;&#26356;&#39640;&#32423;&#30340;&#29305;&#24449;&#65292;&#20363;&#22914;&#24773;&#33410;&#25110;&#24120;&#35782;&#30693;&#35782;&#20197;&#25552;&#39640;&#29983;&#25104;&#25925;&#20107;&#30340;&#36136;&#37327;&#12290;&#20351;&#29992;&#26497;&#22823;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;VLPLMs&#65289;&#22914;GPT3&#30340;&#25552;&#31034;&#24335;&#23398;&#20064;&#24050;&#32463;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#20351;&#29992;&#33258;&#21160;&#21644;&#20154;&#31867;&#35780;&#20272;&#26469;&#27604;&#36739;VLPLMs&#19982;&#37027;&#20123;&#22312;&#39118;&#26684;&#12289;&#35821;&#35328;&#21644;&#38271;&#24230;&#31561;&#26041;&#38754;&#19981;&#21516;&#30340;SOTA&#27169;&#22411;&#22312;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#25925;&#20107;&#29983;&#25104;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;VLPLMs&#29983;&#25104;&#30340;&#25925;&#20107;&#36136;&#37327;&#36828;&#36828;&#39640;&#20110;&#20854;&#20182;&#25925;&#20107;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21487;&#20197;&#19982;&#20154;&#31867;&#20316;&#32773;&#30456;&#25239;&#34913;&#65292;&#23613;&#31649;&#21021;&#27493;&#35843;&#26597;&#20063;&#25581;&#31034;&#20102;&#23427;&#20204;&#20542;&#21521;&#20110;&#8220;&#25220;&#34989;&#8221;&#30495;&#23454;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
While pre-trained language models can generate individually fluent sentences for automatic story generation, they struggle to generate stories that are coherent, sensible and interesting. Current state-of-the-art (SOTA) story generation models explore using higher-level features such as plots or commonsense knowledge to improve the quality of generated stories. Prompt-based learning using very large pre-trained language models (VLPLMs) such as GPT3 has demonstrated impressive performance even across various NLP tasks. In this paper, we present an extensive study using automatic and human evaluation to compare the story generation capability of VLPLMs to those SOTA models in three different datasets where stories differ in style, register and length. Our results show that VLPLMs generate much higher quality stories than other story generation models, and to a certain extent rival human authors, although preliminary investigation also reveals that they tend to ``plagiarise'' real stories
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#38745;&#24577;&#12289;&#21160;&#24577;&#21644;&#22810;&#27169;&#24577;&#19977;&#31181;&#22270;&#31867;&#22411;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2212.05767</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31867;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#32508;&#36848;&#65306;&#38745;&#24577;&#12289;&#21160;&#24577;&#21644;&#22810;&#27169;&#24577;
&lt;/p&gt;
&lt;p&gt;
A Survey of Knowledge Graph Reasoning on Graph Types: Static, Dynamic, and Multimodal. (arXiv:2212.05767v7 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#38745;&#24577;&#12289;&#21160;&#24577;&#21644;&#22810;&#27169;&#24577;&#19977;&#31181;&#22270;&#31867;&#22411;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#65288;KGR&#65289;&#26088;&#22312;&#26681;&#25454;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#30340;&#36923;&#36753;&#35268;&#21017;&#25512;&#26029;&#20986;&#26032;&#30340;&#20107;&#23454;&#65292;&#24050;&#25104;&#20026;&#24555;&#36895;&#22686;&#38271;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#23427;&#24050;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#26497;&#22823;&#22320;&#26377;&#30410;&#65292;&#22914;&#38382;&#39064;&#22238;&#31572;&#12289;&#25512;&#33616;&#31995;&#32479;&#31561;&#12290;&#26681;&#25454;&#22270;&#31867;&#22411;&#65292;&#29616;&#26377;&#30340;KGR&#27169;&#22411;&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;&#19977;&#31867;&#65292;&#21363;&#38745;&#24577;&#27169;&#22411;&#12289;&#26102;&#24577;&#27169;&#22411;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#35813;&#39046;&#22495;&#30340;&#26089;&#26399;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#38745;&#24577;KGR&#19978;&#65292;&#32780;&#26368;&#36817;&#30340;&#24037;&#20316;&#23581;&#35797;&#21033;&#29992;&#26356;&#23454;&#38469;&#21644;&#26356;&#25509;&#36817;&#29616;&#23454;&#19990;&#30028;&#30340;&#26102;&#24577;&#21644;&#22810;&#27169;&#24577;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26080;&#32508;&#21512;&#24635;&#32467;&#21644;&#35752;&#35770;&#36825;&#19968;&#37325;&#35201;&#26041;&#21521;&#20013;&#30340;&#27169;&#22411;&#30340;&#35843;&#26597;&#35770;&#25991;&#21644;&#24320;&#28304;&#23384;&#20648;&#24211;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#38024;&#23545;&#20174;&#38745;&#24577;&#21040;&#26102;&#24577;&#20877;&#21040;&#22810;&#27169;&#24577;KG&#30340;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#39318;&#27425;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#25991;&#22522;&#20110;&#21452;&#23618;&#20998;&#31867;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph reasoning (KGR), aiming to deduce new facts from existing facts based on mined logic rules underlying knowledge graphs (KGs), has become a fast-growing research direction. It has been proven to significantly benefit the usage of KGs in many AI applications, such as question answering, recommendation systems, and etc. According to the graph types, existing KGR models can be roughly divided into three categories, i.e., static models, temporal models, and multi-modal models. Early works in this domain mainly focus on static KGR, and recent works try to leverage the temporal and multi-modal information, which are more practical and closer to real-world. However, no survey papers and open-source repositories comprehensively summarize and discuss models in this important direction. To fill the gap, we conduct a first survey for knowledge graph reasoning tracing from static to temporal and then to multi-modal KGs. Concretely, the models are reviewed based on bi-level taxonomy,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25289;&#27604;&#25991;&#23398;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#20854;&#39118;&#26684;&#26469;&#26816;&#27979;Midrash Tanhuma&#20013;&#30340;&#22833;&#33853;&#26448;&#26009;&#12290;</title><link>http://arxiv.org/abs/2211.09710</link><description>&lt;p&gt;
&#32763;&#35793;&#65306;&#21033;&#29992;&#39118;&#26684;&#20998;&#31867;&#26469;&#26816;&#27979;&#22833;&#33853;&#30340;&#12298;Midrash Tanhuma&#12299;&#26448;&#26009;
&lt;/p&gt;
&lt;p&gt;
Style Classification of Rabbinic Literature for Detection of Lost Midrash Tanhuma Material. (arXiv:2211.09710v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25289;&#27604;&#25991;&#23398;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#20854;&#39118;&#26684;&#26469;&#26816;&#27979;Midrash Tanhuma&#20013;&#30340;&#22833;&#33853;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Midrash&#38598;&#21512;&#26159;&#22797;&#26434;&#30340;&#25289;&#27604;&#25991;&#29486;&#20316;&#21697;&#65292;&#30001;&#22810;&#31181;&#35821;&#35328;&#30340;&#25991;&#26412;&#32452;&#25104;&#65292;&#32463;&#36807;&#19981;&#31283;&#23450;&#30340;&#21475;&#22836;&#21644;&#20070;&#38754;&#20256;&#36882;&#36807;&#31243;&#28436;&#21464;&#32780;&#26469;&#12290;&#30830;&#23450;&#36825;&#31181;&#21512;&#38598;&#20013;&#30340;&#19968;&#20010;&#32473;&#23450;&#27573;&#33853;&#30340;&#36215;&#28304;&#24182;&#19981;&#24635;&#26159;&#30452;&#35266;&#30340;&#65292;&#24120;&#24120;&#26159;&#23398;&#32773;&#20043;&#38388;&#30340;&#20105;&#35758;&#65292;&#28982;&#32780;&#23545;&#20110;&#23398;&#32773;&#20204;&#29702;&#35299;&#27573;&#33853;&#21450;&#20854;&#19982;&#25289;&#27604;&#25991;&#38598;&#20013;&#20854;&#20182;&#25991;&#26412;&#30340;&#20851;&#31995;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#39118;&#26684;&#30340;&#25289;&#27604;&#25991;&#23398;&#20998;&#31867;&#31995;&#32479;&#65292;&#21033;&#29992;&#26368;&#36817;&#21457;&#24067;&#30340;&#38024;&#23545;&#24076;&#20271;&#26469;&#35821;&#30340;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#22833;&#33853;&#30340;Midrash Tanhuma&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Midrash collections are complex rabbinic works that consist of text in multiple languages, which evolved through long processes of unstable oral and written transmission. Determining the origin of a given passage in such a compilation is not always straightforward and is often a matter of dispute among scholars, yet it is essential for scholars' understanding of the passage and its relationship to other texts in the rabbinic corpus.  To help solve this problem, we propose a system for classification of rabbinic literature based on its style, leveraging recently released pretrained Transformer models for Hebrew. Additionally, we demonstrate how our method can be applied to uncover lost material from Midrash Tanhuma.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;MIMIC-RRS&#65292;&#21253;&#21547;&#22810;&#20010;&#35299;&#21078;&#23398;&#21644;&#27169;&#24577;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#21644;&#20020;&#24202;&#35780;&#20272;&#65292;&#25105;&#20204;&#26088;&#22312;&#25193;&#22823;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2211.08584</link><description>&lt;p&gt;
&#25193;&#22823;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#33539;&#22260;&#65306;&#22810;&#20010;&#35299;&#21078;&#23398;&#21644;&#27169;&#24577;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Toward expanding the scope of radiology report summarization to multiple anatomies and modalities. (arXiv:2211.08584v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#23384;&#22312;&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;MIMIC-RRS&#65292;&#21253;&#21547;&#22810;&#20010;&#35299;&#21078;&#23398;&#21644;&#27169;&#24577;&#12290;&#36890;&#36807;&#22312;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#21644;&#20020;&#24202;&#35780;&#20272;&#65292;&#25105;&#20204;&#26088;&#22312;&#25193;&#22823;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;&#65288;RRS&#65289;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#32473;&#23450;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#21457;&#29616;&#37096;&#20998;&#65292;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#20010;&#27010;&#36848;&#65288;&#31216;&#20026;&#21360;&#35937;&#37096;&#20998;&#65289;&#65292;&#31361;&#20986;&#25918;&#23556;&#23398;&#30740;&#31350;&#30340;&#20851;&#38190;&#35266;&#23519;&#21644;&#32467;&#35770;&#12290;&#28982;&#32780;&#65292;RRS&#30446;&#21069;&#38754;&#20020;&#30528;&#37325;&#35201;&#30340;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#31169;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#26080;&#27861;&#37325;&#29616;&#32467;&#26524;&#24182;&#22312;&#19981;&#21516;&#31995;&#32479;&#21644;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#36827;&#34892;&#20844;&#24179;&#27604;&#36739;&#12290;&#20854;&#27425;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#20165;&#22312;&#33016;&#37096;X&#23556;&#32447;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65288;MIMIC-RRS&#65289;&#65292;&#28041;&#21450;MIMIC-III&#21644;MIMIC-CXR&#25968;&#25454;&#38598;&#30340;&#19977;&#31181;&#26032;&#30340;&#27169;&#24577;&#21644;&#19971;&#31181;&#26032;&#30340;&#35299;&#21078;&#23398;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#27169;&#22411;&#22312;MIMIC-RRS&#20013;&#30340;&#27169;&#24577;-&#35299;&#21078;&#23398;&#23545;&#20869;&#21644;&#23545;&#22806;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;RadGraph&#35780;&#20272;&#23427;&#20204;&#30340;&#20020;&#24202;&#21151;&#25928;&#65292;&#36825;&#26159;&#19968;&#20010;&#20107;&#23454;&#27491;&#30830;&#24615;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radiology report summarization (RRS) is a growing area of research. Given the Findings section of a radiology report, the goal is to generate a summary (called an Impression section) that highlights the key observations and conclusions of the radiology study. However, RRS currently faces essential limitations.First, many prior studies conduct experiments on private datasets, preventing reproduction of results and fair comparisons across different systems and solutions. Second, most prior approaches are evaluated solely on chest X-rays. To address these limitations, we propose a dataset (MIMIC-RRS) involving three new modalities and seven new anatomies based on the MIMIC-III and MIMIC-CXR datasets. We then conduct extensive experiments to evaluate the performance of models both within and across modality-anatomy pairs in MIMIC-RRS. In addition, we evaluate their clinical efficacy via RadGraph, a factual correctness metric.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#23545;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2210.10012</link><description>&lt;p&gt;
&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#24433;&#21709;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Log-linear Guardedness and its Implications. (arXiv:2210.10012v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#21450;&#20854;&#23545;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#26500;&#24314;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#21457;&#29616;&#65292;&#22312;&#20551;&#35774;&#21487;&#32447;&#24615;&#30340;&#31070;&#32463;&#34920;&#31034;&#20013;&#65292;&#20174;&#20013;&#21024;&#38500;&#21487;&#20154;&#35299;&#37322;&#30340;&#27010;&#24565;&#30340;&#26041;&#27861;&#26159;&#21487;&#34892;&#21644;&#26377;&#29992;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21024;&#38500;&#23545;&#20110;&#22522;&#20110;&#20462;&#25913;&#21518;&#34920;&#31034;&#36827;&#34892;&#35757;&#32451;&#30340;&#19979;&#28216;&#20998;&#31867;&#22120;&#34892;&#20026;&#30340;&#24433;&#21709;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#30340;&#27010;&#24565;&#65292;&#21363;&#23545;&#25163;&#26080;&#27861;&#30452;&#25509;&#20174;&#34920;&#31034;&#20013;&#39044;&#27979;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#24182;&#30740;&#31350;&#20854;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20108;&#20803;&#24773;&#20917;&#19979;&#65292;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#19979;&#28216;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#26080;&#27861;&#24674;&#22797;&#34987;&#21024;&#38500;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#22810;&#31867;&#23545;&#25968;&#32447;&#24615;&#27169;&#22411;&#65292;&#38388;&#25509;&#24674;&#22797;&#27010;&#24565;&#65292;&#36825;&#25351;&#20986;&#20102;&#23545;&#25968;&#32447;&#24615;&#20445;&#25252;&#24615;&#20316;&#20026;&#19979;&#28216;&#20559;&#24046;&#32531;&#35299;&#25216;&#26415;&#30340;&#20869;&#22312;&#23616;&#38480;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#32447;&#24615;&#21024;&#38500;&#26041;&#27861;&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#21487;&#35299;&#37322;&#31070;&#32463;&#34920;&#31034;&#19982;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#32852;&#31995;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for erasing human-interpretable concepts from neural representations that assume linearity have been found to be tractable and useful. However, the impact of this removal on the behavior of downstream classifiers trained on the modified representations is not fully understood. In this work, we formally define the notion of log-linear guardedness as the inability of an adversary to predict the concept directly from the representation, and study its implications. We show that, in the binary case, under certain assumptions, a downstream log-linear model cannot recover the erased concept. However, we demonstrate that a multiclass log-linear model \emph{can} be constructed that indirectly recovers the concept in some cases, pointing to the inherent limitations of log-linear guardedness as a downstream bias mitigation technique. These findings shed light on the theoretical limitations of linear erasure methods and highlight the need for further research on the connections between int
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#31867;&#22686;&#37327;NER&#20013;&#30340;&#38544;&#34255;&#23454;&#20307;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#23454;&#20307;&#31867;&#21035;&#21644;"O"&#36827;&#34892;&#21028;&#21035;&#24335;&#34920;&#31034;&#23398;&#20064;&#65292;&#25913;&#21892;&#20102;NER&#27169;&#22411;&#23545;&#20110;&#26032;&#26087;&#31867;&#21035;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.04676</link><description>&lt;p&gt;
&#23398;&#20064;"O"&#26377;&#21161;&#20110;&#26356;&#22810;&#30340;&#23398;&#20064;&#65306;&#35299;&#20915;&#31867;&#22686;&#37327;NER&#30340;&#38544;&#34255;&#23454;&#20307;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning "O" Helps for Learning More: Handling the Concealed Entity Problem for Class-incremental NER. (arXiv:2210.04676v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04676
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#31867;&#22686;&#37327;NER&#20013;&#30340;&#38544;&#34255;&#23454;&#20307;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#23454;&#20307;&#31867;&#21035;&#21644;"O"&#36827;&#34892;&#21028;&#21035;&#24335;&#34920;&#31034;&#23398;&#20064;&#65292;&#25913;&#21892;&#20102;NER&#27169;&#22411;&#23545;&#20110;&#26032;&#26087;&#31867;&#21035;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21629;&#21517;&#23454;&#20307;&#30340;&#31867;&#21035;&#36805;&#36895;&#22686;&#21152;&#65292;&#37096;&#32626;&#30340;NER&#27169;&#22411;&#38656;&#35201;&#19981;&#26029;&#26356;&#26032;&#20197;&#35782;&#21035;&#26356;&#22810;&#30340;&#23454;&#20307;&#31867;&#22411;&#65292;&#36825;&#23601;&#38656;&#35201;&#23545;NER&#36827;&#34892;&#31867;&#22686;&#37327;&#23398;&#20064;&#12290;&#32771;&#34385;&#21040;&#38544;&#31169;&#38382;&#39064;&#21644;&#23384;&#20648;&#32422;&#26463;&#65292;&#31867;&#22686;&#37327;NER&#30340;&#26631;&#20934;&#33539;&#24335;&#20165;&#20351;&#29992;&#24102;&#26377;&#26032;&#31867;&#21035;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#26356;&#26032;&#27169;&#22411;&#65292;&#32780;&#26469;&#33258;&#20854;&#20182;&#23454;&#20307;&#31867;&#21035;&#30340;&#23454;&#20307;&#34987;&#26631;&#35760;&#20026;"&#38750;&#23454;&#20307;"&#65288;&#25110;"O"&#65289;&#12290;&#26412;&#25991;&#23545;"&#26410;&#26631;&#35760;&#23454;&#20307;&#38382;&#39064;"&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#23427;&#23548;&#33268;"O"&#21644;&#23454;&#20307;&#20043;&#38388;&#20005;&#37325;&#28151;&#28102;&#65292;&#38477;&#20302;&#20102;&#26087;&#31867;&#21035;&#30340;&#20998;&#31867;&#33021;&#21147;&#65292;&#24182;&#38477;&#20302;&#20102;&#27169;&#22411;&#23398;&#20064;&#26032;&#31867;&#21035;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#26410;&#26631;&#35760;&#23454;&#20307;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#23398;&#20064;&#23454;&#20307;&#31867;&#21035;&#21644;"O"&#30340;&#21028;&#21035;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#20307;&#24863;&#30693;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;representation learning&#26041;&#38754;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the categories of named entities rapidly increase, the deployed NER models are required to keep updating toward recognizing more entity types, creating a demand for class-incremental learning for NER. Considering the privacy concerns and storage constraints, the standard paradigm for class-incremental NER updates the models with training data only annotated with the new classes, yet the entities from other entity classes are unlabeled, regarded as "Non-entity" (or "O"). In this work, we conduct an empirical study on the "Unlabeled Entity Problem" and find that it leads to severe confusion between "O" and entities, decreasing class discrimination of old classes and declining the model's ability to learn new classes. To solve the Unlabeled Entity Problem, we propose a novel representation learning method to learn discriminative representations for the entity classes and "O". Specifically, we propose an entity-aware contrastive learning method that adaptively detects entity clusters in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DSTEA&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#20307;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#12290;&#23427;&#36890;&#36807;&#23494;&#38598;&#35757;&#32451;&#23545;&#35805;&#35805;&#35821;&#20013;&#30340;&#20851;&#38190;&#23454;&#20307;&#65292;&#32780;&#26080;&#38656;&#22806;&#37096;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#36827;&#34892;&#39044;&#35757;&#32451;&#32780;&#19981;&#30452;&#25509;&#27880;&#20837;&#39069;&#22806;&#30340;&#30693;&#35782;&#21040;DST&#27169;&#22411;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.03858</link><description>&lt;p&gt;
DSTEA&#65306;&#36890;&#36807;&#23454;&#20307;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
DSTEA: Improving Dialogue State Tracking via Entity Adaptive Pre-training. (arXiv:2207.03858v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DSTEA&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#20307;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#12290;&#23427;&#36890;&#36807;&#23494;&#38598;&#35757;&#32451;&#23545;&#35805;&#35805;&#35821;&#20013;&#30340;&#20851;&#38190;&#23454;&#20307;&#65292;&#32780;&#26080;&#38656;&#22806;&#37096;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#24182;&#19988;&#21482;&#38656;&#35201;&#36827;&#34892;&#39044;&#35757;&#32451;&#32780;&#19981;&#30452;&#25509;&#27880;&#20837;&#39069;&#22806;&#30340;&#30693;&#35782;&#21040;DST&#27169;&#22411;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;(DST)&#23545;&#20110;&#20840;&#38754;&#35299;&#37322;&#29992;&#25143;&#21644;&#31995;&#32479;&#35805;&#35821;&#20197;&#24418;&#25104;&#39640;&#25928;&#23545;&#35805;&#31995;&#32479;&#30340;&#22522;&#30784;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36807;&#21435;&#30340;&#30740;&#31350;&#21162;&#21147;&#38598;&#20013;&#22312;&#36890;&#36807;&#25913;&#21464;&#27169;&#22411;&#32467;&#26500;&#25110;&#38598;&#25104;&#22270;&#20851;&#31995;&#31561;&#39069;&#22806;&#29305;&#24449;&#26469;&#25552;&#21319;DST&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#38656;&#35201;&#39069;&#22806;&#30340;&#39044;&#35757;&#32451;&#19982;&#22806;&#37096;&#23545;&#35805;&#35821;&#26009;&#24211;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DSTEA&#65292;&#36890;&#36807;&#23454;&#20307;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#26469;&#25913;&#36827;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65292;&#21487;&#20197;&#36890;&#36807;&#23494;&#38598;&#35757;&#32451;&#23545;&#35805;&#35805;&#35821;&#20013;&#30340;&#20851;&#38190;&#23454;&#20307;&#26469;&#22686;&#24378;&#32534;&#30721;&#22120;&#12290;DSTEA&#21033;&#29992;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#20174;&#36755;&#20837;&#23545;&#35805;&#20013;&#35782;&#21035;&#20986;&#36825;&#20123;&#20851;&#38190;&#23454;&#20307;&#65306;&#26412;&#20307;&#20449;&#24687;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;spaCy&#21644;flair library&#12290;&#38543;&#21518;&#65292;&#23427;&#37319;&#29992;&#36873;&#25321;&#24615;&#30693;&#35782;&#36974;&#32617;&#26469;&#26377;&#25928;&#35757;&#32451;&#27169;&#22411;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;DSTEA&#21482;&#38656;&#35201;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#23558;&#39069;&#22806;&#30340;&#30693;&#35782;&#30452;&#25509;&#27880;&#20837;DST&#27169;&#22411;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#20102;&#20986;&#33394;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue State Tracking (DST) is critical for comprehensively interpreting user and system utterances, thereby forming the cornerstone of efficient dialogue systems. Despite past research efforts focused on enhancing DST performance through alterations to the model structure or integrating additional features like graph relations, they often require additional pre-training with external dialogue corpora. In this study, we propose DSTEA, improving Dialogue State Tracking via Entity Adaptive pre-training, which can enhance the encoder through by intensively training key entities in dialogue utterances. DSTEA identifies these pivotal entities from input dialogues utilizing four different methods: ontology information, named-entity recognition, the spaCy, and the flair library. Subsequently, it employs selective knowledge masking to train the model effectively. Remarkably, DSTEA only requires pre-training without the direct infusion of extra knowledge into the DST model. This approach resu
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LAnoBERT&#30340;&#26080;&#35299;&#26512;&#22120;&#31995;&#32479;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20102;BERT&#27169;&#22411;&#36827;&#34892;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26080;&#20154;&#24037;&#24178;&#39044;&#30340;&#39640;&#25928;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2111.09564</link><description>&lt;p&gt;
LAnoBERT: &#22522;&#20110;BERT&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#30340;&#31995;&#32479;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LAnoBERT: System Log Anomaly Detection based on BERT Masked Language Model. (arXiv:2111.09564v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.09564
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LAnoBERT&#30340;&#26080;&#35299;&#26512;&#22120;&#31995;&#32479;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#20102;BERT&#27169;&#22411;&#36827;&#34892;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26080;&#20154;&#24037;&#24178;&#39044;&#30340;&#39640;&#25928;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#31995;&#32479;&#20013;&#29983;&#25104;&#30340;&#31995;&#32479;&#26085;&#24535;&#25351;&#30340;&#26159;&#22823;&#35268;&#27169;&#21516;&#26102;&#25910;&#38598;&#30340;&#25968;&#25454;&#65292;&#29992;&#20316;&#30830;&#23450;&#38169;&#35823;&#12289;&#20837;&#20405;&#21644;&#24322;&#24120;&#34892;&#20026;&#30340;&#22522;&#30784;&#25968;&#25454;&#12290;&#31995;&#32479;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#30340;&#30446;&#26631;&#26159;&#21450;&#26102;&#35782;&#21035;&#24322;&#24120;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20154;&#24037;&#24178;&#39044;&#65292;&#36825;&#26159;&#34892;&#19994;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#23558;&#21508;&#31181;&#24418;&#24335;&#30340;&#26085;&#24535;&#25968;&#25454;&#36716;&#25442;&#20026;&#26631;&#20934;&#21270;&#27169;&#26495;&#65292;&#20351;&#29992;&#35299;&#26512;&#22120;&#36827;&#34892;&#31639;&#27861;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#12290;&#29305;&#21035;&#26159;&#65292;&#24212;&#39044;&#20808;&#23450;&#20041;&#19982;&#29305;&#23450;&#20107;&#20214;&#23545;&#24212;&#30340;&#27169;&#26495;&#65292;&#20197;&#20415;&#25152;&#26377;&#26085;&#24535;&#25968;&#25454;&#20351;&#29992;&#35813;&#27169;&#26495;&#65292;&#20854;&#20013;&#26085;&#24535;&#20851;&#38190;&#20449;&#24687;&#21487;&#33021;&#20250;&#20002;&#22833;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LAnoBERT&#30340;&#26080;&#35299;&#26512;&#22120;&#31995;&#32479;&#26085;&#24535;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;BERT&#27169;&#22411;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;LAnoBERT&#36890;&#36807;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#36827;&#34892;&#27169;&#22411;&#23398;&#20064;&#65292;&#36827;&#32780;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The system log generated in a computer system refers to large-scale data that are collected simultaneously and used as the basic data for determining errors, intrusion and abnormal behaviors. The aim of system log anomaly detection is to promptly identify anomalies while minimizing human intervention, which is a critical problem in the industry. Previous studies performed anomaly detection through algorithms after converting various forms of log data into a standardized template using a parser. Particularly, a template corresponding to a specific event should be defined in advance for all the log data using which the information within the log key may get lost. In this study, we propose LAnoBERT, a parser free system log anomaly detection method that uses the BERT model, exhibiting excellent natural language processing performance. The proposed method, LAnoBERT, learns the model through masked language modeling, which is a BERT-based pre-training method, and proceeds with unsupervised 
&lt;/p&gt;</description></item><item><title>SparseGAN&#26159;&#19968;&#31181;&#31232;&#30095;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#29983;&#25104;&#35821;&#20041;&#21487;&#35299;&#37322;&#19988;&#31232;&#30095;&#30340;&#21477;&#23376;&#34920;&#31034;&#20316;&#20026;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26694;&#26550;&#19979;&#30340;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#32534;&#30721;&#30340;&#24605;&#24819;&#65292;SparseGAN&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#22122;&#38899;&#24182;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;SparseGAN&#22312;&#22810;&#20010;&#25991;&#26412;&#29983;&#25104;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#29305;&#21035;&#26159;&#22312;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2103.11578</link><description>&lt;p&gt;
SparseGAN: &#31232;&#30095;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SparseGAN: Sparse Generative Adversarial Network for Text Generation. (arXiv:2103.11578v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.11578
&lt;/p&gt;
&lt;p&gt;
SparseGAN&#26159;&#19968;&#31181;&#31232;&#30095;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#29983;&#25104;&#35821;&#20041;&#21487;&#35299;&#37322;&#19988;&#31232;&#30095;&#30340;&#21477;&#23376;&#34920;&#31034;&#20316;&#20026;&#36755;&#20837;&#65292;&#23454;&#29616;&#20102;&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26694;&#26550;&#19979;&#30340;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#31232;&#30095;&#32534;&#30721;&#30340;&#24605;&#24819;&#65292;SparseGAN&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#22122;&#38899;&#24182;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#24494;&#20998;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;SparseGAN&#22312;&#22810;&#20010;&#25991;&#26412;&#29983;&#25104;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#65292;&#29305;&#21035;&#26159;&#22312;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30340;&#26694;&#26550;&#19979;&#65292;&#23398;&#20064;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#19981;&#21487;&#24494;&#20998;&#12290;&#29616;&#26377;&#30340;&#35757;&#32451;&#31574;&#30053;&#35201;&#20040;&#21463;&#21040;&#19981;&#21487;&#38752;&#30340;&#26799;&#24230;&#20272;&#35745;&#30340;&#22256;&#25200;&#65292;&#35201;&#20040;&#23384;&#22312;&#19981;&#20934;&#30830;&#30340;&#21477;&#23376;&#34920;&#31034;&#12290;&#21463;&#21040;&#31232;&#30095;&#32534;&#30721;&#21407;&#29702;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SparseGAN&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#35821;&#20041;&#21487;&#35299;&#37322;&#19988;&#31232;&#30095;&#30340;&#21477;&#23376;&#34920;&#31034;&#20316;&#20026;&#37492;&#21035;&#22120;&#30340;&#36755;&#20837;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#23884;&#20837;&#30697;&#38453;&#35270;&#20026;&#19968;&#20010;&#36229;&#23436;&#22791;&#30340;&#35789;&#20856;&#65292;&#24182;&#20351;&#29992;&#26497;&#23569;&#25968;&#30340;&#36873;&#23450;&#35789;&#23884;&#20837;&#30340;&#32447;&#24615;&#32452;&#21512;&#26469;&#36924;&#36817;&#29983;&#25104;&#22120;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#36755;&#20986;&#29305;&#24449;&#34920;&#31034;&#12290;&#36890;&#36807;&#36825;&#26679;&#30340;&#35821;&#20041;&#20016;&#23500;&#34920;&#31034;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#20943;&#23569;&#19981;&#24517;&#35201;&#30340;&#22122;&#38899;&#20197;&#23454;&#29616;&#26377;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#65292;&#36824;&#21487;&#20197;&#20351;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#23436;&#20840;&#21487;&#24494;&#20998;&#12290;&#22312;&#22810;&#20010;&#25991;&#26412;&#29983;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#22312;&#24207;&#21015;&#29983;&#25104;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is still a challenging task to learn a neural text generation model under the framework of generative adversarial networks (GANs) since the entire training process is not differentiable. The existing training strategies either suffer from unreliable gradient estimations or imprecise sentence representations. Inspired by the principle of sparse coding, we propose a SparseGAN that generates semantic-interpretable, but sparse sentence representations as inputs to the discriminator. The key idea is that we treat an embedding matrix as an over-complete dictionary, and use a linear combination of very few selected word embeddings to approximate the output feature representation of the generator at each time step. With such semantic-rich representations, we not only reduce unnecessary noises for efficient adversarial training, but also make the entire training process fully differentiable. Experiments on multiple text generation datasets yield performance improvements, especially in sequen
&lt;/p&gt;</description></item><item><title>XTQA&#26159;&#38024;&#23545;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;&#25552;&#20986;&#30340;&#19968;&#31181;&#26032;&#26550;&#26500;&#65292;&#36890;&#36807;&#36328;&#21477;&#35299;&#37322;&#25552;&#20379;&#31572;&#26696;&#21644;&#35777;&#25454;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2011.12662</link><description>&lt;p&gt;
XTQA: &#25945;&#31185;&#20070;&#38382;&#31572;&#30340;&#36328;&#21477;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
XTQA: Span-Level Explanations of the Textbook Question Answering. (arXiv:2011.12662v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.12662
&lt;/p&gt;
&lt;p&gt;
XTQA&#26159;&#38024;&#23545;&#25945;&#31185;&#20070;&#38382;&#31572;&#20219;&#21153;&#25552;&#20986;&#30340;&#19968;&#31181;&#26032;&#26550;&#26500;&#65292;&#36890;&#36807;&#36328;&#21477;&#35299;&#37322;&#25552;&#20379;&#31572;&#26696;&#21644;&#35777;&#25454;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#31185;&#20070;&#38382;&#31572;&#65288;TQA&#65289;&#26159;&#19968;&#20010;&#20219;&#21153;&#65292;&#35201;&#27714;&#22312;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#25991;&#31456;&#21644;&#22270;&#34920;&#30340;&#22810;&#27169;&#24577;&#32972;&#26223;&#19979;&#65292;&#22238;&#31572;&#19968;&#20010;&#22270;&#34920;/&#38750;&#22270;&#34920;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#35299;&#37322;&#24615;&#24212;&#35813;&#23558;&#23398;&#29983;&#35270;&#20026;&#19968;&#20010;&#38656;&#35201;&#32771;&#34385;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#20174;&#31895;&#21040;&#32454;&#30340;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;TQA&#30340;&#36328;&#21477;&#35299;&#37322;&#65288;XTQA&#65289;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#20026;&#23398;&#29983;&#25552;&#20379;&#19981;&#20165;&#26159;&#31572;&#26696;&#65292;&#36824;&#21253;&#25324;&#29992;&#20110;&#36873;&#25321;&#31572;&#26696;&#30340;&#36328;&#21477;&#35777;&#25454;&#12290;&#35813;&#31639;&#27861;&#39318;&#20808;&#20351;&#29992;TF-IDF&#26041;&#27861;&#31895;&#30053;&#22320;&#36873;&#25321;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#21069;M&#20010;&#27573;&#33853;&#65292;&#28982;&#21518;&#20174;&#36825;&#20123;&#27573;&#33853;&#20013;&#30340;&#25152;&#26377;&#20505;&#36873;&#36328;&#21477;&#20013;&#31934;&#32454;&#22320;&#36873;&#25321;&#21069;K&#20010;&#35777;&#25454;&#36328;&#21477;&#65292;&#36890;&#36807;&#35745;&#31639;&#27599;&#20010;&#36328;&#21477;&#23545;&#38382;&#39064;&#30340;&#20449;&#24687;&#22686;&#30410;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;XTQA&#26174;&#33879;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/keep-smile-001/opentqa&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textbook Question Answering (TQA) is a task that one should answer a diagram/non-diagram question given a large multi-modal context consisting of abundant essays and diagrams. We argue that the explainability of this task should place students as a key aspect to be considered. To address this issue, we devise a novel architecture towards span-level eXplanations of the TQA (XTQA) based on our proposed coarse-to-fine grained algorithm, which can provide not only the answers but also the span-level evidences to choose them for students. This algorithm first coarsely chooses top $M$ paragraphs relevant to questions using the TF-IDF method, and then chooses top $K$ evidence spans finely from all candidate spans within these paragraphs by computing the information gain of each span to questions. Experimental results shows that XTQA significantly improves the state-of-the-art performance compared with baselines. The source code is available at https://github.com/keep-smile-001/opentqa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22270;&#25490;&#21517;&#27169;&#22411;RepRank&#65292;&#36890;&#36807;&#22312;&#32479;&#19968;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#35745;&#31639;&#21333;&#35789;&#12289;&#21477;&#23376;&#21644;&#21333;&#35789;&#21040;&#21477;&#23376;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#26469;&#25552;&#21462;&#22810;&#25991;&#26723;&#25688;&#35201;&#20013;&#30340;&#31361;&#20986;&#21477;&#23376;&#21644;&#20851;&#38190;&#35789;&#12290;&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21477;&#23376;&#34920;&#31034;&#20026;&#20854;&#35789;&#23884;&#20837;&#30340;&#21152;&#26435;&#21644;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21453;&#26144;&#25991;&#26723;&#20869;&#23481;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#32852;&#21512;&#25552;&#21462;&#21477;&#23376;&#21644;&#20851;&#38190;&#35789;&#30340;&#36807;&#31243;&#21487;&#20197;&#30456;&#20114;&#22686;&#24378;&#65292;&#24182;&#19988;&#24635;&#26159;&#25910;&#25947;&#21040;&#21807;&#19968;&#30340;&#35299;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#21560;&#25910;&#24335;&#38543;&#26426;&#28216;&#36208;&#30340;&#21464;&#20307;&#21644;&#22522;&#20110;&#37319;&#26679;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36991;&#20813;&#20887;&#20313;&#24182;&#22686;&#21152;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2009.07481</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#25552;&#21462;&#21477;&#23376;&#21644;&#20851;&#38190;&#35789;&#36827;&#34892;&#26080;&#30417;&#30563;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Summarization by Jointly Extracting Sentences and Keywords. (arXiv:2009.07481v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.07481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22270;&#25490;&#21517;&#27169;&#22411;RepRank&#65292;&#36890;&#36807;&#22312;&#32479;&#19968;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#35745;&#31639;&#21333;&#35789;&#12289;&#21477;&#23376;&#21644;&#21333;&#35789;&#21040;&#21477;&#23376;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#26469;&#25552;&#21462;&#22810;&#25991;&#26723;&#25688;&#35201;&#20013;&#30340;&#31361;&#20986;&#21477;&#23376;&#21644;&#20851;&#38190;&#35789;&#12290;&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21477;&#23376;&#34920;&#31034;&#20026;&#20854;&#35789;&#23884;&#20837;&#30340;&#21152;&#26435;&#21644;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21453;&#26144;&#25991;&#26723;&#20869;&#23481;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36890;&#36807;&#32852;&#21512;&#25552;&#21462;&#21477;&#23376;&#21644;&#20851;&#38190;&#35789;&#30340;&#36807;&#31243;&#21487;&#20197;&#30456;&#20114;&#22686;&#24378;&#65292;&#24182;&#19988;&#24635;&#26159;&#25910;&#25947;&#21040;&#21807;&#19968;&#30340;&#35299;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#21560;&#25910;&#24335;&#38543;&#26426;&#28216;&#36208;&#30340;&#21464;&#20307;&#21644;&#22522;&#20110;&#37319;&#26679;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36991;&#20813;&#20887;&#20313;&#24182;&#22686;&#21152;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RepRank&#65292;&#36825;&#26159;&#19968;&#20010;&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#22270;&#30340;&#25490;&#21517;&#27169;&#22411;&#65292;&#29992;&#20110;&#25552;&#21462;&#22810;&#25991;&#26723;&#25688;&#35201;&#12290;&#36890;&#36807;&#22312;&#32479;&#19968;&#30340;&#21521;&#37327;&#31354;&#38388;&#20013;&#35745;&#31639;&#23427;&#20204;&#30340;&#21521;&#37327;&#34920;&#31034;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#21487;&#20197;&#20272;&#35745;&#21333;&#35789;&#12289;&#21477;&#23376;&#21644;&#21333;&#35789;&#21040;&#21477;&#23376;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#12290;&#20026;&#20102;&#33719;&#24471;&#29702;&#24819;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21477;&#23376;&#34920;&#31034;&#20026;&#20854;&#35789;&#23884;&#20837;&#30340;&#21152;&#26435;&#21644;&#65292;&#26435;&#37325;&#38598;&#20013;&#22312;&#37027;&#20123;&#26356;&#22909;&#22320;&#21453;&#26144;&#25991;&#26723;&#20869;&#23481;&#30340;&#35789;&#19978;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#26469;&#32852;&#21512;&#25552;&#21462;&#31361;&#20986;&#30340;&#21477;&#23376;&#21644;&#20851;&#38190;&#35789;&#30340;&#36807;&#31243;&#21487;&#20197;&#30456;&#20114;&#22686;&#24378;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#36807;&#31243;&#24635;&#26159;&#25910;&#25947;&#21040;&#19968;&#20010;&#21807;&#19968;&#30340;&#35299;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#21560;&#25910;&#24335;&#38543;&#26426;&#28216;&#36208;&#30340;&#21464;&#20307;&#21644;&#30456;&#24212;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#31639;&#27861;&#65292;&#20197;&#36991;&#20813;&#25688;&#35201;&#20013;&#30340;&#20887;&#20313;&#24182;&#22686;&#21152;&#22810;&#26679;&#24615;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
We present RepRank, an unsupervised graph-based ranking model for extractive multi-document summarization in which the similarity between words, sentences, and word-to-sentence can be estimated by the distances between their vector representations in a unified vector space. In order to obtain desirable representations, we propose a self-attention based learning method that represent a sentence by the weighted sum of its word embeddings, and the weights are concentrated to those words hopefully better reflecting the content of a document. We show that salient sentences and keywords can be extracted in a joint and mutual reinforcement process using our learned representations, and prove that this process always converges to a unique solution leading to improvement in performance. A variant of absorbing random walk and the corresponding sampling-based algorithm are also described to avoid redundancy and increase diversity in the summaries. Experiment results with multiple benchmark datase
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20108;&#38454;&#25512;&#29702;&#30340;&#25351;&#20195;&#28040;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#40723;&#21169;&#20849;&#20139;&#25552;&#21450;&#20043;&#38388;&#30340;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#23616;&#25512;&#29702;&#31639;&#27861;&#26469;&#26368;&#20248;&#22320;&#32858;&#31867;&#25552;&#21450;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#24456;&#22909;&#12290;</title><link>http://arxiv.org/abs/2009.04639</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20108;&#38454;&#25512;&#29702;&#25913;&#36827;&#25351;&#20195;&#28040;&#35299;
&lt;/p&gt;
&lt;p&gt;
Improving Coreference Resolution by Leveraging Entity-Centric Features with Graph Neural Networks and Second-order Inference. (arXiv:2009.04639v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.04639
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20108;&#38454;&#25512;&#29702;&#30340;&#25351;&#20195;&#28040;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#40723;&#21169;&#20849;&#20139;&#25552;&#21450;&#20043;&#38388;&#30340;&#29305;&#24449;&#65292;&#26377;&#25928;&#22320;&#25429;&#25417;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#23616;&#25512;&#29702;&#31639;&#27861;&#26469;&#26368;&#20248;&#22320;&#32858;&#31867;&#25552;&#21450;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#24456;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20195;&#28040;&#35299;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#22914;&#20309;&#21033;&#29992;&#22312;&#25552;&#21450;&#38598;&#32676;&#19978;&#23450;&#20041;&#30340;&#23454;&#20307;&#32423;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#25552;&#21450;&#23545;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#20849;&#21516;&#25351;&#20195;&#30340;&#25552;&#21450;&#36890;&#24120;&#22312;&#25972;&#20010;&#25991;&#26412;&#20013;&#20998;&#25955;&#65292;&#36825;&#20351;&#24471;&#25972;&#21512;&#23454;&#20307;&#32423;&#29305;&#24449;&#26497;&#20854;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#20195;&#28040;&#35299;&#26041;&#27861;&#65292;&#36890;&#36807;&#40723;&#21169;&#22312;&#21487;&#33021;&#25351;&#21521;&#21516;&#19968;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#25152;&#26377;&#25552;&#21450;&#20043;&#38388;&#20849;&#20139;&#29305;&#24449;&#26469;&#25429;&#25417;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#36793;&#23558;&#25552;&#21450;&#24444;&#27492;&#38142;&#25509;&#36215;&#26469;&#65292;&#24314;&#27169;&#20004;&#20010;&#38142;&#25509;&#25552;&#21450;&#25351;&#21521;&#30456;&#21516;&#23454;&#20307;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#36825;&#26679;&#30340;&#22270;&#24418;&#24314;&#27169;&#65292;&#25552;&#21450;&#20043;&#38388;&#30340;&#29305;&#24449;&#21487;&#20197;&#36890;&#36807;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#20449;&#24687;&#20256;&#36882;&#25805;&#20316;&#36827;&#34892;&#20849;&#20139;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#38454;&#29305;&#24449;&#30340;&#20840;&#23616;&#25512;&#29702;&#31639;&#27861;&#65292;&#23558;&#25552;&#21450;&#26368;&#20248;&#22320;&#32858;&#31867;&#21040;&#19968;&#33268;&#30340;&#32452;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#32467;&#21512;&#20108;&#38454;&#29305;&#24449;&#30340;&#25928;&#26524;&#24456;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the major challenges in coreference resolution is how to make use of entity-level features defined over clusters of mentions rather than mention pairs. However, coreferent mentions usually spread far apart in an entire text, which makes it extremely difficult to incorporate entity-level features. We propose a graph neural network-based coreference resolution method that can capture the entity-centric information by encouraging the sharing of features across all mentions that probably refer to the same real-world entity. Mentions are linked to each other via the edges modeling how likely two linked mentions point to the same entity. Modeling by such graphs, the features between mentions can be shared by message passing operations in an entity-centric manner. A global inference algorithm up to second-order features is also presented to optimally cluster mentions into consistent groups. Experimental results show our graph neural network-based method combing with the second-order de
&lt;/p&gt;</description></item><item><title>Transformer&#26159;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#32593;&#32476;&#26550;&#26500;&#65292;&#23436;&#20840;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21462;&#20195;&#20102;&#22797;&#26434;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#23454;&#39564;&#35777;&#26126;Transformer&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#36136;&#37327;&#26356;&#22909;&#12289;&#24182;&#34892;&#21270;&#25928;&#26524;&#26356;&#20339;&#65292;&#19988;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#12290;&#23427;&#22312;&#33521;&#35793;&#24503;&#21644;&#33521;&#35793;&#27861;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/1706.03762</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#23601;&#26159;&#19968;&#20999;&#65288;arXiv:1706.03762v6 [cs.CL]&#24050;&#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Attention Is All You Need. (arXiv:1706.03762v6 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1706.03762
&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#32593;&#32476;&#26550;&#26500;&#65292;&#23436;&#20840;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21462;&#20195;&#20102;&#22797;&#26434;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;&#23454;&#39564;&#35777;&#26126;Transformer&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#36136;&#37327;&#26356;&#22909;&#12289;&#24182;&#34892;&#21270;&#25928;&#26524;&#26356;&#20339;&#65292;&#19988;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#12290;&#23427;&#22312;&#33521;&#35793;&#24503;&#21644;&#33521;&#35793;&#27861;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20027;&#35201;&#30340;&#24207;&#21015;&#36716;&#25442;&#27169;&#22411;&#22522;&#20110;&#22797;&#26434;&#30340;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#25110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#37197;&#32622;&#12290;&#34920;&#29616;&#26368;&#22909;&#30340;&#27169;&#22411;&#36824;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#36830;&#25509;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31616;&#21333;&#32593;&#32476;&#26550;&#26500;&#65292;Transformer&#65292;&#23436;&#20840;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#19981;&#20877;&#20351;&#29992;&#24490;&#29615;&#21644;&#21367;&#31215;&#12290;&#22312;&#20004;&#20010;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#36136;&#37327;&#19978;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#21516;&#26102;&#26356;&#26131;&#20110;&#24182;&#34892;&#21270;&#65292;&#35757;&#32451;&#26102;&#38388;&#26174;&#33879;&#20943;&#23569;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;WMT 2014&#33521;&#35793;&#24503;&#20219;&#21153;&#19978;&#36798;&#21040;28.4&#30340;BLEU&#20998;&#25968;&#65292;&#27604;&#29616;&#26377;&#26368;&#22909;&#32467;&#26524;&#65288;&#21253;&#25324;&#38598;&#25104;&#27169;&#22411;&#65289;&#25552;&#39640;&#20102;2&#20010;BLEU&#20998;&#12290;&#22312;WMT 2014&#33521;&#35793;&#27861;&#20219;&#21153;&#19978;&#65292;&#22312;8&#20010;GPU&#19978;&#35757;&#32451;&#20102;3.5&#22825;&#21518;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#33719;&#24471;&#20102;41.8&#30340;&#21333;&#27169;&#22411;&#26368;&#26032;BLEU&#20998;&#25968;&#65292;&#35757;&#32451;&#25104;&#26412;&#20165;&#20026;&#25991;&#29486;&#20013;&#26368;&#22909;&#27169;&#22411;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Transformer&#26550;&#26500;&#30340;&#20248;&#21183;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#37325;&#35201;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transforme
&lt;/p&gt;</description></item></channel></rss>