<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>Text2Cohort&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#65292;&#20943;&#23569;&#30740;&#31350;&#20154;&#21592;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23454;&#29616;&#20102;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#30340;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.07637</link><description>&lt;p&gt;
Text2Cohort: &#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#23545;&#30284;&#30151;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
Text2Cohort: Democratizing the NCI Imaging Data Commons with Natural Language Cohort Discovery. (arXiv:2305.07637v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07637
&lt;/p&gt;
&lt;p&gt;
Text2Cohort&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#65292;&#20943;&#23569;&#30740;&#31350;&#20154;&#21592;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23454;&#29616;&#20102;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#30340;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;(IDC)&#26159;&#19968;&#20010;&#22522;&#20110;&#20113;&#30340;&#25968;&#25454;&#24211;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#24320;&#25918;&#33719;&#21462;&#30340;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#21644;&#20998;&#26512;&#24037;&#20855;&#65292;&#26088;&#22312;&#20419;&#36827;&#21307;&#23398;&#25104;&#20687;&#30740;&#31350;&#20013;&#30340;&#21327;&#20316;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#25216;&#26415;&#24615;&#36136;&#65292;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#20197;&#36827;&#34892;&#38431;&#21015;&#21457;&#29616;&#21644;&#35775;&#38382;&#25104;&#20687;&#25968;&#25454;&#23545;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#26174;&#33879;&#30340;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;Text2Cohort&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#24182;&#23558;&#26597;&#35810;&#30340;&#21709;&#24212;&#36820;&#22238;&#32473;&#29992;&#25143;&#65292;&#20197;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26657;&#27491;&#20197;&#35299;&#20915;&#26597;&#35810;&#20013;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#38169;&#35823;&#65292;&#36890;&#36807;&#23558;&#38169;&#35823;&#20256;&#22238;&#27169;&#22411;&#36827;&#34892;&#35299;&#37322;&#21644;&#26657;&#27491;&#12290;&#25105;&#20204;&#23545;50&#20010;&#33258;&#28982;&#35821;&#35328;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#20102;Text2Cohort&#35780;&#20272;&#65292;&#33539;&#22260;&#20174;&#20449;&#24687;&#25552;&#21462;&#21040;&#38431;&#21015;&#21457;&#29616;&#12290;&#32467;&#26524;&#26597;&#35810;&#21644;&#36755;&#20986;&#30001;&#20004;&#20301;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#36827;&#34892;&#20102;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Imaging Data Commons (IDC) is a cloud-based database that provides researchers with open access to cancer imaging data and tools for analysis, with the goal of facilitating collaboration in medical imaging research. However, querying the IDC database for cohort discovery and access to imaging data has a significant learning curve for researchers due to its complex and technical nature. We developed Text2Cohort, a large language model (LLM) based toolkit to facilitate natural language cohort discovery by translating user input into IDC database queries through prompt engineering and returning the query's response to the user. Furthermore, autocorrection is implemented to resolve syntax and semantic errors in queries by passing the errors back to the model for interpretation and correction. We evaluate Text2Cohort on 50 natural language user inputs ranging from information extraction to cohort discovery. The resulting queries and outputs were verified by two computer scientists to me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;PALR&#30340;&#26694;&#26550;&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07622</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;LMMs&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PALR: Personalization Aware LLMs for Recommendation. (arXiv:2305.07622v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07622
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;PALR&#30340;&#26694;&#26550;&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;PALR&#65292;&#23558;&#29992;&#25143;&#30340;&#21382;&#21490;&#34892;&#20026;&#19982;LLMs&#30456;&#32467;&#21512;&#65292;&#20197;&#29983;&#25104;&#29992;&#25143;&#21916;&#27426;&#30340;&#29289;&#21697;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#29992;&#25143;/&#29289;&#21697;&#20114;&#21160;&#20316;&#20026;&#20505;&#36873;&#26816;&#32034;&#30340;&#25351;&#23548;&#65292;&#28982;&#21518;&#37319;&#29992;&#22522;&#20110;LLMs&#30340;&#25490;&#24207;&#27169;&#22411;&#29983;&#25104;&#25512;&#33616;&#29289;&#21697;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#25512;&#33616;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;PALR&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently received significant attention for their exceptional capabilities. Despite extensive efforts in developing general-purpose LLMs that can be utilized in various natural language processing (NLP) tasks, there has been less research exploring their potential in recommender systems. In this paper, we propose a novel framework, named PALR, which aiming to combine user history behaviors (such as clicks, purchases, ratings, etc.) with LLMs to generate user preferred items. Specifically, we first use user/item interactions as guidance for candidate retrieval. Then we adopt a LLM-based ranking model to generate recommended items. Unlike existing approaches that typically adopt general-purpose LLMs for zero/few-shot recommendation testing or training on small-sized language models (with less than 1 billion parameters), which cannot fully elicit LLMs' reasoning abilities and leverage rich item side parametric knowledge, we fine-tune a 7 billion parameter
&lt;/p&gt;</description></item><item><title>&#29616;&#26377;&#30340;&#24635;&#32467;&#27169;&#22411;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#21333;&#20010;&#21442;&#32771;&#30340;&#21487;&#33021;&#24615;&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#36136;&#37327;&#25351;&#26631;&#19981;&#21305;&#37197;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#19981;&#21516;&#26657;&#20934;&#38598;&#30340;&#30740;&#31350;&#65292;&#25214;&#20986;&#20102;&#26368;&#20339;&#35774;&#32622;&#30340;&#20849;&#21516;&#29305;&#28857;&#65292;&#21363;&#22312;&#26657;&#20934;&#21069;&#23558;&#27491;&#36127;&#20998;&#24320;&#65292;&#35880;&#24910;&#36873;&#25321;&#36127;&#20363;&#65292;&#22823;&#32966;&#23545;&#24453;&#27491;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.07615</link><description>&lt;p&gt;
&#26631;&#39064;&#65306;&#26657;&#20934;&#38598;&#30340;&#26399;&#26395;&#29305;&#28857;&#26159;&#20160;&#20040;&#65311;&#30830;&#23450;&#38271;&#31687;&#31185;&#23398;&#25688;&#35201;&#30340;&#30456;&#20851;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
What are the Desired Characteristics of Calibration Sets? Identifying Correlates on Long Form Scientific Summarization. (arXiv:2305.07615v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07615
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#24635;&#32467;&#27169;&#22411;&#30001;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#21333;&#20010;&#21442;&#32771;&#30340;&#21487;&#33021;&#24615;&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#36136;&#37327;&#25351;&#26631;&#19981;&#21305;&#37197;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#19981;&#21516;&#26657;&#20934;&#38598;&#30340;&#30740;&#31350;&#65292;&#25214;&#20986;&#20102;&#26368;&#20339;&#35774;&#32622;&#30340;&#20849;&#21516;&#29305;&#28857;&#65292;&#21363;&#22312;&#26657;&#20934;&#21069;&#23558;&#27491;&#36127;&#20998;&#24320;&#65292;&#35880;&#24910;&#36873;&#25321;&#36127;&#20363;&#65292;&#22823;&#32966;&#23545;&#24453;&#27491;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25688;&#35201;&#65306;&#24635;&#32467;&#27169;&#22411;&#36890;&#24120;&#20250;&#29983;&#25104;&#19982;&#36136;&#37327;&#25351;&#26631;&#19981;&#21305;&#37197;&#30340;&#25991;&#26412;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#35757;&#32451;&#26159;&#20026;&#20102;&#26368;&#22823;&#21270;&#21333;&#20010;&#21442;&#32771;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#21152;&#20837;&#20102;&#19968;&#20010;&#26657;&#20934;&#27493;&#39588;&#65292;&#35753;&#27169;&#22411;&#26292;&#38706;&#22312;&#23427;&#33258;&#24049;&#30340;&#25490;&#21517;&#36755;&#20986;&#20013;&#65292;&#20197;&#25552;&#39640;&#30456;&#20851;&#24615;&#25110;&#25913;&#36827;&#24544;&#23454;&#24230;&#12290;&#34429;&#28982;&#36825;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#24456;&#22810;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#22914;&#20309;&#29983;&#25104;&#21644;&#20248;&#21270;&#36825;&#20123;&#38598;&#21512;&#19978;&#12290;&#20851;&#20110;&#20026;&#20160;&#20040;&#19968;&#31181;&#35774;&#32622;&#27604;&#21478;&#19968;&#31181;&#26356;&#26377;&#25928;&#65292;&#25105;&#20204;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#26377;&#25928;&#38598;&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#23545;&#20110;&#27599;&#20010;&#35757;&#32451;&#23454;&#20363;&#65292;&#25105;&#20204;&#24418;&#25104;&#20102;&#19968;&#20010;&#24222;&#22823;&#12289;&#22810;&#26679;&#21270;&#30340;&#20505;&#36873;&#20154;&#27744;&#65292;&#24182;&#31995;&#32479;&#22320;&#21464;&#21270;&#20102;&#29992;&#20110;&#26657;&#20934;&#24494;&#35843;&#30340;&#23376;&#38598;&#12290;&#27599;&#31181;&#36873;&#25321;&#31574;&#30053;&#37117;&#38024;&#23545;&#38598;&#21512;&#30340;&#19981;&#21516;&#26041;&#38754;&#36827;&#34892;&#65292;&#20363;&#22914;&#35789;&#27719;&#22810;&#26679;&#24615;&#25110;&#27491;&#36127;&#20043;&#38388;&#30340;&#24046;&#36317;&#22823;&#23567;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#31185;&#23398;&#38271;&#31687;&#25688;&#35201;&#25968;&#25454;&#38598;&#19978;&#65288;&#28085;&#30422;&#29983;&#29289;&#21307;&#23398;&#12289;&#20020;&#24202;&#21644;COVID-19&#39046;&#22495;&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#20339;&#35774;&#32622;&#30340;&#20849;&#21516;&#29305;&#28857;&#26159;&#22312;&#26657;&#20934;&#21069;&#23558;&#27491;&#36127;&#20998;&#24320;&#65292;&#20026;&#36127;&#20363;&#36873;&#25321;&#35880;&#24910;&#32780;&#22823;&#32966;&#22320;&#23545;&#24453;&#27491;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarization models often generate text that is poorly calibrated to quality metrics because they are trained to maximize the likelihood of a single reference (MLE). To address this, recent work has added a calibration step, which exposes a model to its own ranked outputs to improve relevance or, in a separate line of work, contrasts positive and negative sets to improve faithfulness. While effective, much of this work has focused on how to generate and optimize these sets. Less is known about why one setup is more effective than another. In this work, we uncover the underlying characteristics of effective sets. For each training instance, we form a large, diverse pool of candidates and systematically vary the subsets used for calibration fine-tuning. Each selection strategy targets distinct aspects of the sets, such as lexical diversity or the size of the gap between positive and negatives. On three diverse scientific long-form summarization datasets (spanning biomedical, clinical, a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21542;&#23450;&#22312;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24433;&#21709;&#65292;&#26500;&#24314;&#20102;&#22522;&#20934;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#22823;&#22810;&#25968;&#37117;&#27809;&#26377;&#32771;&#34385;&#21542;&#23450;&#65292;&#32780;&#20132;&#21449;&#32534;&#30721;&#22120;&#26159;&#30446;&#21069;&#34920;&#29616;&#26368;&#22909;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.07614</link><description>&lt;p&gt;
NevIR: &#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#21542;&#23450;
&lt;/p&gt;
&lt;p&gt;
NevIR: Negation in Neural Information Retrieval. (arXiv:2305.07614v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21542;&#23450;&#22312;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24433;&#21709;&#65292;&#26500;&#24314;&#20102;&#22522;&#20934;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#22823;&#22810;&#25968;&#37117;&#27809;&#26377;&#32771;&#34385;&#21542;&#23450;&#65292;&#32780;&#20132;&#21449;&#32534;&#30721;&#22120;&#26159;&#30446;&#21069;&#34920;&#29616;&#26368;&#22909;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21542;&#23450;&#26159;&#19968;&#31181;&#24120;&#35265;&#32780;&#26085;&#24120;&#21270;&#30340;&#29616;&#35937;&#65292;&#20063;&#19968;&#30452;&#26159;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#20010;&#24369;&#28857;&#12290;&#34429;&#28982;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#37319;&#29992;&#20102;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29616;&#20195;&#21270;&#26550;&#26500;&#30340;&#20027;&#24178;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#30740;&#31350;&#28145;&#20837;&#20102;&#35299;&#21542;&#23450;&#23545;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20934;&#26469;&#30740;&#31350;&#36825;&#20010;&#20027;&#39064;&#65306;&#35201;&#27714;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#23545;&#20165;&#20165;&#22240;&#20026;&#26159;&#21542;&#23450;&#32780;&#19981;&#21516;&#30340;&#20004;&#20010;&#25991;&#26723;&#36827;&#34892;&#25490;&#21517;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32467;&#26524;&#26681;&#25454;&#19981;&#21516;&#30340;&#20449;&#24687;&#26816;&#32034;&#26550;&#26500;&#32780;&#26377;&#24456;&#22823;&#24046;&#24322;&#65306;&#20132;&#21449;&#32534;&#30721;&#22120;&#34920;&#29616;&#26368;&#22909;&#65292;&#21518;&#26399;&#20132;&#20114;&#27169;&#22411;&#27425;&#20043;&#65292;&#32780;&#21452;&#32534;&#30721;&#22120;&#21644;&#31232;&#30095;&#31070;&#32463;&#26550;&#26500;&#25490;&#21517;&#26368;&#21518;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#37117;&#27809;&#26377;&#32771;&#34385;&#21542;&#23450;&#65292;&#34920;&#29616;&#19982;&#38543;&#26426;&#25490;&#21517;&#30456;&#20284;&#25110;&#26356;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23613;&#31649;&#22312;&#19968;&#20010;&#21253;&#21547;&#21542;&#23450;&#23545;&#29031;&#25991;&#26723;&#30340;&#25968;&#25454;&#38598;&#19978;&#32487;&#32493;&#24494;&#35843;&#26126;&#26174;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65288;&#27169;&#22411;&#22823;&#23567;&#20063;&#26159;&#22914;&#27492;&#65289;&#65292;&#20294;&#26159;&#26426;&#22120;&#21644;&#20154;&#20043;&#38388;&#20173;&#23384;&#22312;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Negation is a common everyday phenomena and has been a consistent area of weakness for language models (LMs). Although the Information Retrieval (IR) community has adopted LMs as the backbone of modern IR architectures, there has been little to no research in understanding how negation impacts neural IR. We therefore construct a straightforward benchmark on this theme: asking IR models to rank two documents that differ only by negation. We show that the results vary widely according to the type of IR architecture: cross-encoders perform best, followed by late-interaction models, and in last place are bi-encoder and sparse neural architectures. We find that most current information retrieval models do not consider negation, performing similarly or worse than randomly ranking. We show that although the obvious approach of continued fine-tuning on a dataset of contrastive documents containing negations increases performance (as does model size), there is still a large gap between machine 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#23450;&#20041;&#12289;&#21457;&#23637;&#21644;&#25361;&#25112;&#65292;&#35752;&#35770;&#20102;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#26500;&#24314;&#26356;&#22909;&#24615;&#33021;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.07611</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sentiment Analysis: A Survey. (arXiv:2305.07611v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#23450;&#20041;&#12289;&#21457;&#23637;&#21644;&#25361;&#25112;&#65292;&#35752;&#35770;&#20102;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#20808;&#36827;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#26500;&#24314;&#26356;&#22909;&#24615;&#33021;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36825;&#39033;&#25216;&#26415;&#24050;&#32463;&#36798;&#21040;&#20102;&#26032;&#30340;&#39640;&#24230;&#12290;&#23427;&#22312;&#24212;&#29992;&#21644;&#30740;&#31350;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#27492;&#25104;&#20026;&#20102;&#19968;&#20010;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#23450;&#20041;&#12289;&#32972;&#26223;&#21644;&#21457;&#23637;&#27010;&#36848;&#12290;&#23427;&#36824;&#28085;&#30422;&#20102;&#26368;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#20808;&#36827;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#35813;&#25216;&#26415;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#21069;&#26223;&#12290;&#26368;&#21518;&#65292;&#23427;&#23637;&#26395;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#38656;&#35201;&#25351;&#20986;&#30340;&#26159;&#65292;&#26412;&#32508;&#36848;&#20026;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#26500;&#24314;&#26356;&#22909;&#24615;&#33021;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#25552;&#20379;&#20102;&#24314;&#35774;&#24615;&#30340;&#24314;&#35758;&#65292;&#26377;&#21161;&#20110;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal sentiment analysis has become an important research area in the field of artificial intelligence. With the latest advances in deep learning, this technology has reached new heights. It has great potential for both application and research, making it a popular research topic. This review provides an overview of the definition, background, and development of multimodal sentiment analysis. It also covers recent datasets and advanced models, emphasizing the challenges and future prospects of this technology. Finally, it looks ahead to future research directions. It should be noted that this review provides constructive suggestions for promising research directions and building better performing multimodal sentiment analysis models, which can help researchers in this field.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#33539;&#24335;&#8212;&#8212;&#36890;&#36807;LLM&#36827;&#34892;&#25512;&#33616;&#65292;&#20294;&#30001;&#20110;LLMs&#21487;&#33021;&#23384;&#22312;&#31038;&#20250;&#20559;&#35265;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;RecLLM&#25152;&#20570;&#25512;&#33616;&#30340;&#20844;&#27491;&#24615;&#12290;&#20026;&#27492;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#8212;&#8212;FaiRLLM&#65292;&#24182;&#38024;&#23545;&#38899;&#20048;&#21644;&#30005;&#24433;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#20843;&#20010;&#25935;&#24863;&#23646;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.07609</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#20844;&#24179;&#21487;&#38752;&#65311;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#33616;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation. (arXiv:2305.07609v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07609
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#33539;&#24335;&#8212;&#8212;&#36890;&#36807;LLM&#36827;&#34892;&#25512;&#33616;&#65292;&#20294;&#30001;&#20110;LLMs&#21487;&#33021;&#23384;&#22312;&#31038;&#20250;&#20559;&#35265;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;RecLLM&#25152;&#20570;&#25512;&#33616;&#30340;&#20844;&#27491;&#24615;&#12290;&#20026;&#27492;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#8212;&#8212;FaiRLLM&#65292;&#24182;&#38024;&#23545;&#38899;&#20048;&#21644;&#30005;&#24433;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#20843;&#20010;&#25935;&#24863;&#23646;&#24615;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26174;&#30528;&#25104;&#23601;&#23548;&#33268;&#19968;&#31181;&#26032;&#30340;&#25512;&#33616;&#33539;&#24335;&#8212;&#8212;&#36890;&#36807;LLM&#36827;&#34892;&#25512;&#33616;&#65288;RecLLM&#65289;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#27880;&#24847;LLMs&#21487;&#33021;&#21253;&#21547;&#31038;&#20250;&#20559;&#35265;&#65292;&#22240;&#27492;&#38656;&#35201;&#36827;&#19968;&#27493;&#35843;&#26597;RecLLM&#25152;&#20570;&#25512;&#33616;&#30340;&#20844;&#27491;&#24615;&#12290;&#20026;&#20102;&#36991;&#20813;RecLLM&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#26377;&#24517;&#35201;&#20174;&#29992;&#25143;&#30340;&#21508;&#31181;&#25935;&#24863;&#23646;&#24615;&#35282;&#24230;&#35780;&#20272;RecLLM&#30340;&#20844;&#24179;&#24615;&#12290;&#30001;&#20110;RecLLM&#33539;&#24335;&#19982;&#20256;&#32479;&#25512;&#33616;&#33539;&#24335;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#22240;&#27492;&#30452;&#25509;&#20351;&#29992;&#20256;&#32479;&#25512;&#33616;&#30340;&#20844;&#24179;&#24615;&#22522;&#20934;&#26159;&#26377;&#38382;&#39064;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#31216;&#20026;&#8220;&#36890;&#36807;LLM&#30340;&#25512;&#33616;&#30340;&#20844;&#24179;&#24615;&#8221;&#65288;FaiRLLM&#65289;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#31934;&#24515;&#35774;&#35745;&#30340;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20004;&#20010;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#20843;&#20010;&#25935;&#24863;&#23646;&#24615;&#65306;&#38899;&#20048;&#21644;&#30005;&#24433;&#12290;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#30340;FaiRLLM&#22522;&#20934;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable achievements of Large Language Models (LLMs) have led to the emergence of a novel recommendation paradigm -- Recommendation via LLM (RecLLM). Nevertheless, it is important to note that LLMs may contain social prejudices, and therefore, the fairness of recommendations made by RecLLM requires further investigation. To avoid the potential risks of RecLLM, it is imperative to evaluate the fairness of RecLLM with respect to various sensitive attributes on the user side. Due to the differences between the RecLLM paradigm and the traditional recommendation paradigm, it is problematic to directly use the fairness benchmark of traditional recommendation. To address the dilemma, we propose a novel benchmark called Fairness of Recommendation via LLM (FaiRLLM). This benchmark comprises carefully crafted metrics and a dataset that accounts for eight sensitive attributes1 in two recommendation scenarios: music and movies. By utilizing our FaiRLLM benchmark, we conducted an evaluation 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#26102;&#65292;&#36890;&#36807;&#25490;&#32451;&#21644;&#39044;&#26399;&#26469;&#35760;&#24518;&#26377;&#20851;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#24212;&#29992;&#33258;&#30417;&#30563;&#26426;&#21046;&#65292;&#36890;&#36807;&#26680;&#25351;&#20195;&#20449;&#24687;&#30340;&#23631;&#34109;&#24314;&#27169;&#20219;&#21153;&#35757;&#32451;&#65292;&#25104;&#21151;&#36890;&#36807;&#30701;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#22823;&#22411;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.07565</link><description>&lt;p&gt;
&#19968;&#31181;&#25903;&#25345;&#26680;&#25351;&#20195;&#20449;&#24687;&#30340;&#38382;&#31572;&#27969;&#24335;&#25968;&#25454;&#35760;&#24518;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of Coreference Information. (arXiv:2305.07565v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07565
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#27169;&#22411;&#65292;&#22312;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#26102;&#65292;&#36890;&#36807;&#25490;&#32451;&#21644;&#39044;&#26399;&#26469;&#35760;&#24518;&#26377;&#20851;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#24212;&#29992;&#33258;&#30417;&#30563;&#26426;&#21046;&#65292;&#36890;&#36807;&#26680;&#25351;&#20195;&#20449;&#24687;&#30340;&#23631;&#34109;&#24314;&#27169;&#20219;&#21153;&#35757;&#32451;&#65292;&#25104;&#21151;&#36890;&#36807;&#30701;&#24207;&#21015;&#25968;&#25454;&#38598;&#21644;&#22823;&#22411;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38382;&#31572;&#26041;&#27861;&#24448;&#24448;&#20551;&#35774;&#36755;&#20837;&#20869;&#23481;&#65288;&#22914;&#25991;&#20214;&#25110;&#35270;&#39057;&#65289;&#24635;&#26159;&#21487;&#35775;&#38382;&#30340;&#65292;&#20197;&#35299;&#20915;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#35760;&#24518;&#32593;&#32476;&#34987;&#24341;&#20837;&#26469;&#27169;&#20223;&#20154;&#31867;&#36880;&#27493;&#29702;&#35299;&#21644;&#21387;&#32553;&#20449;&#24687;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#21482;&#23398;&#20064;&#22914;&#20309;&#36890;&#36807;&#25972;&#20010;&#32593;&#32476;&#21453;&#21521;&#20256;&#25773;&#38169;&#35823;&#26469;&#32500;&#25252;&#20869;&#23384;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#20855;&#26377;&#25552;&#39640;&#35760;&#24518;&#23481;&#37327;&#30340;&#26377;&#25928;&#26426;&#21046;&#65292;&#20363;&#22914;&#25490;&#32451;&#21644;&#39044;&#26399;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#27169;&#22411;&#65292;&#36890;&#36807;&#25490;&#32451;&#21644;&#39044;&#26399;&#26469;&#22788;&#29702;&#36755;&#20837;&#20197;&#35760;&#24518;&#26377;&#20851;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#22312;&#35757;&#32451;&#26399;&#38388;&#36890;&#36807;&#38024;&#23545;&#26680;&#25351;&#20195;&#20449;&#24687;&#30340;&#23631;&#34109;&#24314;&#27169;&#20219;&#21153;&#36827;&#34892;&#33258;&#30417;&#30563;&#24212;&#29992;&#12290;&#25105;&#20204;&#22312;&#30701;&#24207;&#21015;&#65288;bAbI&#65289;&#25968;&#25454;&#38598;&#20197;&#21450;&#22823;&#22411;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing question answering methods often assume that the input content (e.g., documents or videos) is always accessible to solve the task. Alternatively, memory networks were introduced to mimic the human process of incremental comprehension and compression of the information in a fixed-capacity memory. However, these models only learn how to maintain memory by backpropagating errors in the answers through the entire network. Instead, it has been suggested that humans have effective mechanisms to boost their memorization capacities, such as rehearsal and anticipation. Drawing inspiration from these, we propose a memory model that performs rehearsal and anticipation while processing inputs to memorize important information for solving question answering tasks from streaming data. The proposed mechanisms are applied self-supervised during training through masked modeling tasks focused on coreference information. We validate our model on a short-sequence (bAbI) dataset as well as large-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#37327;&#21270;&#27979;&#37327;&#32454;&#31890;&#24230;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#25506;&#31350;&#20102;&#22235;&#20010;&#31454;&#20105;&#30340;V&amp;L&#27169;&#22411;&#22312;&#22235;&#20010;&#31934;&#32454;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;X-VLM&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24182;&#24378;&#35843;&#20102;&#26032;&#39062;&#30340;&#25439;&#22833;&#21644;&#20016;&#23500;&#30340;&#25968;&#25454;&#28304;&#23545;&#20110;&#23398;&#20064;&#32454;&#31890;&#24230;&#25216;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#37096;&#20998;&#27169;&#22411;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#23384;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#28508;&#22312;&#20998;&#31163;&#12290;</title><link>http://arxiv.org/abs/2305.07558</link><description>&lt;p&gt;
&#22312;&#32454;&#31890;&#24230;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#36827;&#23637;&#37327;&#21270;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Measuring Progress in Fine-grained Vision-and-Language Understanding. (arXiv:2305.07558v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#37327;&#21270;&#27979;&#37327;&#32454;&#31890;&#24230;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#25506;&#31350;&#20102;&#22235;&#20010;&#31454;&#20105;&#30340;V&amp;L&#27169;&#22411;&#22312;&#22235;&#20010;&#31934;&#32454;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;X-VLM&#27169;&#22411;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24182;&#24378;&#35843;&#20102;&#26032;&#39062;&#30340;&#25439;&#22833;&#21644;&#20016;&#23500;&#30340;&#25968;&#25454;&#28304;&#23545;&#20110;&#23398;&#20064;&#32454;&#31890;&#24230;&#25216;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#25581;&#31034;&#20102;&#37096;&#20998;&#27169;&#22411;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#23384;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#28508;&#22312;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21033;&#29992;&#26469;&#33258;Web&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#32463;&#20419;&#36827;&#20102;&#35270;&#35273;&#35821;&#35328;&#65288;V&#65286;L&#65289;&#20219;&#21153;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#39044;&#35757;&#32451;&#27169;&#22411;&#32570;&#20047;&#8220;&#32454;&#31890;&#24230;&#8221;&#29702;&#35299;&#65292;&#20363;&#22914;&#22312;&#22270;&#20687;&#20013;&#35782;&#21035;&#20851;&#31995;&#12289;&#21160;&#35789;&#21644;&#25968;&#23383;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#24320;&#21457;&#26032;&#30340;&#22522;&#20934;&#25110;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#31181;&#33021;&#21147;&#30340;&#20852;&#36259;&#22686;&#21152;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20102;&#35299;&#21644;&#37327;&#21270;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#22312;&#22235;&#20010;&#31934;&#32454;&#30340;&#22522;&#20934;&#20219;&#21153;&#19978;&#30740;&#31350;&#20102;&#22235;&#20010;&#31454;&#20105;&#30340;V&#65286;L&#27169;&#22411;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;X-VLM&#65288;Zeng&#31561;&#20154;&#65292;2022&#65289;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#65292;&#24182;&#19988;&#27169;&#22411;&#21019;&#26032;&#21487;&#20197;&#27604;&#25193;&#23637;Web&#25968;&#25454;&#23545;&#34920;&#29616;&#20135;&#29983;&#26356;&#22823;&#30340;&#24433;&#21709;&#65292;&#26377;&#26102;&#29978;&#33267;&#20250;&#38477;&#20302;&#34920;&#29616;&#12290;&#36890;&#36807;&#23545;X-VLM&#30340;&#28145;&#20837;&#30740;&#31350;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#26032;&#39062;&#30340;&#25439;&#22833;&#21644;&#20016;&#23500;&#30340;&#25968;&#25454;&#28304;&#23545;&#23398;&#20064;&#32454;&#31890;&#24230;&#25216;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#35757;&#32451;&#21160;&#24577;&#65292;&#21457;&#29616;&#23545;&#20110;&#26576;&#20123;&#20219;&#21153;&#65288;&#20363;&#22914;&#21160;&#35789;&#39044;&#27979;&#65289;&#65292;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#23398;&#20064;&#24863;&#30693;&#35270;&#35273;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#28508;&#22312;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
While pretraining on large-scale image-text data from the Web has facilitated rapid progress on many vision-and-language (V&amp;L) tasks, recent work has demonstrated that pretrained models lack "fine-grained" understanding, such as the ability to recognise relationships, verbs, and numbers in images. This has resulted in an increased interest in the community to either develop new benchmarks or models for such capabilities. To better understand and quantify progress in this direction, we investigate four competitive V&amp;L models on four fine-grained benchmarks. Through our analysis, we find that X-VLM (Zeng et al., 2022) consistently outperforms other baselines, and that modelling innovations can impact performance more than scaling Web data, which even degrades performance sometimes. Through a deeper investigation of X-VLM, we highlight the importance of both novel losses and rich data sources for learning fine-grained skills. Finally, we inspect training dynamics, and discover that for so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#38754;&#21521;&#27861;&#24459;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#21457;&#24067;&#20102;&#19968;&#20010;&#36328;&#22269;&#33521;&#35821;&#27861;&#24459;&#25991;&#38598;&#21644;&#19968;&#20010;&#27861;&#24459;&#30693;&#35782;&#25506;&#38024;&#22522;&#20934;&#65292;&#21457;&#29616;&#25506;&#38024;&#24615;&#33021;&#19982;&#19978;&#28216;&#24615;&#33021;&#24378;&#30456;&#20851;&#65292;&#19979;&#28216;&#24615;&#33021;&#20027;&#35201;&#30001;&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#20808;&#21069;&#30340;&#27861;&#24459;&#30693;&#35782;&#39537;&#21160;&#12290;</title><link>http://arxiv.org/abs/2305.07507</link><description>&lt;p&gt;
LeXFiles&#21644;LegalLAMA&#65306;&#20419;&#36827;&#33521;&#35821;&#36328;&#22269;&#27861;&#24459;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
LeXFiles and LegalLAMA: Facilitating English Multinational Legal Language Model Development. (arXiv:2305.07507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#38754;&#21521;&#27861;&#24459;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#21457;&#24067;&#20102;&#19968;&#20010;&#36328;&#22269;&#33521;&#35821;&#27861;&#24459;&#25991;&#38598;&#21644;&#19968;&#20010;&#27861;&#24459;&#30693;&#35782;&#25506;&#38024;&#22522;&#20934;&#65292;&#21457;&#29616;&#25506;&#38024;&#24615;&#33021;&#19982;&#19978;&#28216;&#24615;&#33021;&#24378;&#30456;&#20851;&#65292;&#19979;&#28216;&#24615;&#33021;&#20027;&#35201;&#30001;&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#20808;&#21069;&#30340;&#27861;&#24459;&#30693;&#35782;&#39537;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#38754;&#21521;&#27861;&#24459;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#32771;&#23519;&#20102;&#23427;&#20204;&#30340;&#21407;&#22987;&#30446;&#26631;&#12289;&#33719;&#21462;&#30340;&#30693;&#35782;&#21644;&#27861;&#24459;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#23558;&#20854;&#23450;&#20041;&#20026;&#19978;&#28216;&#12289;&#25506;&#38024;&#21644;&#19979;&#28216;&#24615;&#33021;&#12290;&#25105;&#20204;&#19981;&#20165;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#36824;&#23558;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20316;&#20026;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#32500;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#36328;&#22269;&#33521;&#35821;&#27861;&#24459;&#25991;&#38598;&#65288;LeXFiles&#65289;&#21644;&#19968;&#20010;&#27861;&#24459;&#30693;&#35782;&#25506;&#38024;&#22522;&#20934;&#65288;LegalLAMA&#65289;&#65292;&#20197;&#20419;&#36827;&#27861;&#24459;&#23548;&#21521;PLMs&#30340;&#35757;&#32451;&#21644;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#20004;&#20010;&#22312;LeXFiles&#19978;&#35757;&#32451;&#30340;&#26032;&#30340;&#27861;&#24459;PLMs&#65292;&#24182;&#22312;LegalLAMA&#21644;LexGLUE&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#30456;&#20851;&#27861;&#24459;&#20027;&#39064;&#20013;&#65292;&#25506;&#38024;&#24615;&#33021;&#19982;&#19978;&#28216;&#24615;&#33021;&#24378;&#30456;&#20851;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19979;&#28216;&#24615;&#33021;&#20027;&#35201;&#30001;&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#20808;&#21069;&#30340;&#27861;&#24459;&#30693;&#35782;&#39537;&#21160;&#65292;&#21487;&#20197;&#36890;&#36807;&#19978;&#28216;&#21644;&#25506;&#38024;&#24615;&#33021;&#26469;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we conduct a detailed analysis on the performance of legal-oriented pre-trained language models (PLMs). We examine the interplay between their original objective, acquired knowledge, and legal language understanding capacities which we define as the upstream, probing, and downstream performance, respectively. We consider not only the models' size but also the pre-training corpora used as important dimensions in our study. To this end, we release a multinational English legal corpus (LeXFiles) and a legal knowledge probing benchmark (LegalLAMA) to facilitate training and detailed analysis of legal-oriented PLMs. We release two new legal PLMs trained on LeXFiles and evaluate them alongside others on LegalLAMA and LexGLUE. We find that probing performance strongly correlates with upstream performance in related legal topics. On the other hand, downstream performance is mainly driven by the model's size and prior legal knowledge which can be estimated by upstream and probing 
&lt;/p&gt;</description></item><item><title>&#36866;&#37197;&#22120;&#34429;&#28982;&#26159;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20294;&#26159;&#22312;&#35757;&#32451;/&#37096;&#32626;&#25928;&#29575;&#21644;&#21487;&#32500;&#25252;&#24615;/&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#24182;&#27809;&#26377;&#23454;&#29616;PEFT&#30340;&#20248;&#21183;&#65292;&#30456;&#23545;&#30340;&#65292;&#21487;&#20197;&#20351;&#29992;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22914;&#22810;&#20219;&#21153;&#35757;&#32451;&#26469;&#23454;&#29616;&#21487;&#32500;&#25252;&#24615;/&#21487;&#25193;&#23637;&#24615;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.07491</link><description>&lt;p&gt;
&#36866;&#37197;&#22120;&#25928;&#29575;&#30340;&#20840;&#38754;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Analysis of Adapter Efficiency. (arXiv:2305.07491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07491
&lt;/p&gt;
&lt;p&gt;
&#36866;&#37197;&#22120;&#34429;&#28982;&#26159;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20294;&#26159;&#22312;&#35757;&#32451;/&#37096;&#32626;&#25928;&#29575;&#21644;&#21487;&#32500;&#25252;&#24615;/&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#24182;&#27809;&#26377;&#23454;&#29616;PEFT&#30340;&#20248;&#21183;&#65292;&#30456;&#23545;&#30340;&#65292;&#21487;&#20197;&#20351;&#29992;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#22914;&#22810;&#20219;&#21153;&#35757;&#32451;&#26469;&#23454;&#29616;&#21487;&#32500;&#25252;&#24615;/&#21487;&#25193;&#23637;&#24615;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#37197;&#22120;&#34987;&#23450;&#20301;&#20026;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21482;&#38656;&#21521;&#27169;&#22411;&#28155;&#21152;&#26368;&#23569;&#37327;&#30340;&#21442;&#25968;&#21363;&#21487;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;PEFT&#22312;&#35757;&#32451;/&#37096;&#32626;&#25928;&#29575;&#21644;&#21487;&#32500;&#25252;&#24615;/&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#36866;&#37197;&#22120;&#24182;&#26410;&#24471;&#21040;&#36275;&#22815;&#30340;&#20998;&#26512;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#36866;&#37197;&#22120;&#12289;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#21253;&#25324;&#30417;&#30563;&#21644;&#36328;&#35821;&#35328;&#38646;-shot&#35774;&#32622;&#65292;&#25105;&#20204;&#28165;&#26970;&#22320;&#34920;&#26126;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#20013;&#65292;&#36866;&#37197;&#22120;&#30340;&#21442;&#25968;&#25928;&#29575;&#19981;&#31561;&#21516;&#20110;&#19982;&#23436;&#20840;&#24494;&#35843;&#27169;&#22411;&#30456;&#27604;&#30340;&#25928;&#29575;&#25552;&#39640;&#12290;&#26356;&#26126;&#30830;&#22320;&#35828;&#65292;&#36866;&#37197;&#22120;&#30340;&#35757;&#32451;&#25104;&#26412;&#30456;&#23545;&#36739;&#39640;&#65292;&#24182;&#19988;&#20855;&#26377;&#31245;&#39640;&#30340;&#37096;&#32626;&#24310;&#36831;&#12290;&#27492;&#22806;&#65292;&#21487;&#20197;&#36890;&#36807;&#20840;&#38754;&#24494;&#35843;&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#31561;&#31616;&#21333;&#26041;&#27861;&#23454;&#29616;&#36866;&#37197;&#22120;&#30340;&#21487;&#32500;&#25252;&#24615;/&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#65292;&#36825;&#20063;&#25552;&#20379;&#20102;&#30456;&#23545;&#36739;&#24555;&#30340;&#22521;&#35757;&#26102;&#38388;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24314;&#35758;&#23545;&#20110;&#20013;&#31561;&#22823;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20351;&#29992;&#20256;&#32479;&#30340;&#20840;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#65292;&#36991;&#20813;&#20351;&#29992;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapters have been positioned as a parameter-efficient fine-tuning (PEFT) approach, whereby a minimal number of parameters are added to the model and fine-tuned. However, adapters have not been sufficiently analyzed to understand if PEFT translates to benefits in training/deployment efficiency and maintainability/extensibility. Through extensive experiments on many adapters, tasks, and languages in supervised and cross-lingual zero-shot settings, we clearly show that for Natural Language Understanding (NLU) tasks, the parameter efficiency in adapters does not translate to efficiency gains compared to full fine-tuning of models. More precisely, adapters are relatively expensive to train and have slightly higher deployment latency. Furthermore, the maintainability/extensibility benefits of adapters can be achieved with simpler approaches like multi-task training via full fine-tuning, which also provide relatively faster training times. We, therefore, recommend that for moderately sized m
&lt;/p&gt;</description></item><item><title>ArtGPT-4&#26159;&#19968;&#31181;&#22522;&#20110;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;MiniGPT-4&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#35299;&#20915;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#20986;&#20855;&#22791;&#33391;&#22909;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.07490</link><description>&lt;p&gt;
ArtGPT-4: &#22522;&#20110;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;MiniGPT-4&#27169;&#22411;&#30340;&#33402;&#26415;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4. (arXiv:2305.07490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07490
&lt;/p&gt;
&lt;p&gt;
ArtGPT-4&#26159;&#19968;&#31181;&#22522;&#20110;&#36866;&#37197;&#22120;&#22686;&#24378;&#30340;MiniGPT-4&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#35299;&#20915;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#20986;&#20855;&#22791;&#33391;&#22909;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#27604;&#22914;ChatGPT&#21644;GPT-4&#31561;&#27169;&#22411;&#22312;&#22810;&#31181;&#35821;&#35328;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#23545;&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#32780;&#25214;&#21040;&#19982;&#27169;&#22411;&#35268;&#27169;&#21305;&#37197;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#20063;&#24456;&#22256;&#38590;&#12290;&#24494;&#35843;&#21644;&#20351;&#29992;&#26032;&#26041;&#27861;&#35757;&#32451;&#21442;&#25968;&#36739;&#23569;&#30340;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;MiniGPT-4&#27169;&#22411;&#20415;&#26159;&#20854;&#20013;&#20043;&#19968;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#36816;&#29992;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#38761;&#26032;&#24615;&#30340;&#22521;&#35757;&#31574;&#30053;&#23454;&#29616;&#20102;&#19982;GPT-4&#30456;&#24403;&#30340;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#35813;&#27169;&#22411;&#22312;&#22270;&#20687;&#29702;&#35299;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33402;&#26415;&#22270;&#29255;&#26041;&#38754;&#12290;ArtGPT-4&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#26088;&#22312;&#24212;&#23545;&#36825;&#20123;&#23616;&#38480;&#12290;ArtGPT-4&#20351;&#29992;Tesla A100&#35774;&#22791;&#23545;&#22270;&#20687;-&#25991;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#65292;&#20165;&#29992;&#20102;&#32422;200GB&#30340;&#25968;&#25454;&#65292;&#22312;2&#23567;&#26102;&#20869;&#23601;&#33021;&#23637;&#31034;&#20986;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large language models (LLMs) have made significant progress in natural language processing (NLP), with models like ChatGPT and GPT-4 achieving impressive capabilities in various linguistic tasks. However, training models on such a large scale is challenging, and finding datasets that match the model's scale is often difficult. Fine-tuning and training models with fewer parameters using novel methods have emerged as promising approaches to overcome these challenges. One such model is MiniGPT-4, which achieves comparable vision-language understanding to GPT-4 by leveraging novel pre-training models and innovative training strategies. However, the model still faces some challenges in image understanding, particularly in artistic pictures. A novel multimodal model called ArtGPT-4 has been proposed to address these limitations. ArtGPT-4 was trained on image-text pairs using a Tesla A100 device in just 2 hours, using only about 200 GB of data. The model can depict images wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#34920;&#26684;&#19982;&#25991;&#26412;&#28151;&#21512;&#25968;&#20540;&#25512;&#29702;&#30340;&#20840;&#38754;&#39044;&#35757;&#32451;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21464;&#37327;&#23436;&#25972;&#24615;&#25490;&#24207;&#12289;&#21464;&#37327;&#36816;&#31639;&#39044;&#27979;&#21644;&#21464;&#37327;&#20851;&#38190;&#35789;&#23631;&#34109;&#31561;&#20219;&#21153;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#26377;&#29992;&#30340;&#21464;&#37327;&#21644;&#30830;&#23450;&#23376;&#31243;&#24207;&#26469;&#28304;&#30340;&#20851;&#38190;&#35777;&#25454;&#65292;&#23454;&#39564;&#32467;&#26524;&#36229;&#36807;&#20102;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.07475</link><description>&lt;p&gt;
&#38754;&#21521;&#34920;&#26684;&#19982;&#25991;&#26412;&#28151;&#21512;&#25968;&#20540;&#25512;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#20840;&#38754;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Solution Program Centric Pretraining for Table-and-Text Hybrid Numerical Reasoning. (arXiv:2305.07475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#34920;&#26684;&#19982;&#25991;&#26412;&#28151;&#21512;&#25968;&#20540;&#25512;&#29702;&#30340;&#20840;&#38754;&#39044;&#35757;&#32451;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21464;&#37327;&#23436;&#25972;&#24615;&#25490;&#24207;&#12289;&#21464;&#37327;&#36816;&#31639;&#39044;&#27979;&#21644;&#21464;&#37327;&#20851;&#38190;&#35789;&#23631;&#34109;&#31561;&#20219;&#21153;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#26377;&#29992;&#30340;&#21464;&#37327;&#21644;&#30830;&#23450;&#23376;&#31243;&#24207;&#26469;&#28304;&#30340;&#20851;&#38190;&#35777;&#25454;&#65292;&#23454;&#39564;&#32467;&#26524;&#36229;&#36807;&#20102;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#37329;&#34701;&#25253;&#21578;&#31561;&#34920;&#26684;&#19982;&#25991;&#26412;&#28151;&#21512;&#30340;&#35821;&#22659;&#20013;&#30340;&#25968;&#20540;&#25512;&#29702;&#65292;&#23384;&#22312;&#22122;&#22768;&#21644;&#26080;&#20851;&#21464;&#37327;&#20173;&#28982;&#26159;&#29616;&#23454;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#24212;&#29992;&#30340;&#38590;&#28857;&#12290;&#32780;&#31895;&#31961;&#30340;&#25972;&#20010;&#35299;&#20915;&#26041;&#26696;&#31243;&#24207;&#30340;&#30417;&#30563;&#38459;&#30861;&#20102;&#27169;&#22411;&#23398;&#20064;&#28508;&#22312;&#30340;&#25968;&#20540;&#25512;&#29702;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#26082;&#28041;&#21450;&#21040;&#25972;&#20010;&#31243;&#24207;&#20063;&#28041;&#21450;&#21040;&#23376;&#31243;&#24207;&#32423;&#21035;&#30340;&#21464;&#37327;&#23436;&#25972;&#24615;&#25490;&#24207;&#12289;&#21464;&#37327;&#36816;&#31639;&#39044;&#27979;&#21644;&#21464;&#37327;&#20851;&#38190;&#35789;&#23631;&#34109;&#12290;&#36825;&#20123;&#20219;&#21153;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#26377;&#29992;&#30340;&#21464;&#37327;&#12289;&#23558;&#30417;&#30563;&#20998;&#35299;&#20026;&#32454;&#31890;&#24230;&#30340;&#21333;&#20010;&#36816;&#31639;&#31526;&#39044;&#27979;&#21644;&#30830;&#23450;&#23376;&#31243;&#24207;&#26469;&#28304;&#30340;&#20851;&#38190;&#35777;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#36229;&#36807;&#20102;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerical reasoning over table-and-text hybrid passages, such as financial reports, poses significant challenges and has numerous potential applications. Noise and irrelevant variables in the model input have been a hindrance to its performance. Additionally, coarse-grained supervision of the whole solution program has impeded the model's ability to learn the underlying numerical reasoning process. In this paper, we propose three pretraining tasks that operate at both the whole program and sub-program level: Variable Integrity Ranking, which guides the model to focus on useful variables; Variable Operator Prediction, which decomposes the supervision into fine-grained single operator prediction; and Variable Keyphrase Masking, which encourages the model to identify key evidence that sub-programs are derived from. Experimental results demonstrate the effectiveness of our proposed methods, surpassing transformer-based model baselines.
&lt;/p&gt;</description></item><item><title>BactInt&#26159;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#32454;&#33740;&#38388;&#30456;&#20114;&#20316;&#29992;&#24182;&#25366;&#25496;&#29305;&#23450;&#32454;&#33740;&#32676;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20844;&#24320;&#21487;&#29992;&#30340;BactInt&#35821;&#26009;&#24211;&#26631;&#27880;&#20102;1200&#31687;PubMed&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.07468</link><description>&lt;p&gt;
BactInt:&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#21644;&#19968;&#20010;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#32454;&#33740;&#38388;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
BactInt: A domain driven transfer learning approach and a corpus for extracting inter-bacterial interactions from biomedical text. (arXiv:2305.07468v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07468
&lt;/p&gt;
&lt;p&gt;
BactInt&#26159;&#19968;&#31181;&#38754;&#21521;&#39046;&#22495;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#32454;&#33740;&#38388;&#30456;&#20114;&#20316;&#29992;&#24182;&#25366;&#25496;&#29305;&#23450;&#32454;&#33740;&#32676;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20844;&#24320;&#21487;&#29992;&#30340;BactInt&#35821;&#26009;&#24211;&#26631;&#27880;&#20102;1200&#31687;PubMed&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#23398;&#39046;&#22495;&#20013;&#19981;&#21516;&#31867;&#22411;&#24494;&#29983;&#29289;&#22312;&#29983;&#29289;&#23398;&#31354;&#38388;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#20123;&#24494;&#29983;&#29289;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26159;&#24494;&#29983;&#29289;&#32676;&#33853;&#32467;&#26500;&#30340;&#22522;&#26412;&#26500;&#24314;&#21333;&#20803;&#12290;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#30340;&#35777;&#25454;&#21487;&#20316;&#20026;&#39044;&#27979;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#30340;&#21487;&#38752;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#38405;&#35835;&#28023;&#37327;&#19988;&#19981;&#26029;&#22686;&#38271;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#26159;&#19968;&#39033;&#32791;&#26102;&#24182;&#20196;&#20154;&#26395;&#32780;&#29983;&#30031;&#30340;&#24037;&#20316;&#12290;&#36825;&#23601;&#24517;&#28982;&#38656;&#35201;&#24320;&#21457;&#33258;&#21160;&#21270;&#26041;&#27861;&#26469;&#20934;&#30830;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25152;&#25253;&#36947;&#30340;&#32454;&#33740;&#20851;&#31995;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#33258;&#21160;&#25552;&#21462;&#24494;&#29983;&#29289;&#30456;&#20114;&#20316;&#29992;&#65288;&#29305;&#21035;&#26159;&#32454;&#33740;&#20043;&#38388;&#65289;&#30340;&#26041;&#27861;&#20197;&#21450;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#26469;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#19968;&#20010;&#31649;&#36947;&#65292;&#29992;&#20110;&#25366;&#25496;&#29305;&#23450;&#32454;&#33740;&#32676;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;Bacterial Interaction (BactInt)&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;1200&#31687;PubMed&#25688;&#35201;&#65292;&#27880;&#37322;&#26377;&#32454;&#33740;&#38388;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
The community of different types of microbes present in a biological niche plays a very important role in functioning of the system. The crosstalk or interactions among the different microbes contributes to the building blocks of such microbial community structures. Evidence reported in biomedical text serves as a reliable source for predicting such interactions. However, going through the vast and ever-increasing volume of biomedical literature is an intimidating and time consuming process. This necessitates development of automated methods capable of accurately extracting bacterial relations reported in biomedical literature. In this paper, we introduce a method for automated extraction of microbial interactions (specifically between bacteria) from biomedical literature along with ways of using transfer learning to improve its accuracy. We also describe a pipeline using which relations among specific bacteria groups can be mined. Additionally, we introduce the first publicly availabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36879;&#26126;&#12289;&#26080;&#30417;&#30563;&#30340;&#35789;&#32423;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25200;&#21160;&#30340;&#36755;&#20837;&#28304;&#21477;&#23376;&#19978;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#36755;&#20986;&#26469;&#24037;&#20316;&#65292;&#24182;&#21487;&#20197;&#35780;&#20272;&#20219;&#20309;&#31867;&#22411;&#30340;&#40657;&#30418;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#20316;&#20026;&#21453;&#39304;&#20449;&#21495;&#65292;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#25913;&#36827;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.07457</link><description>&lt;p&gt;
&#22522;&#20110;&#25668;&#21160;&#30340;&#36136;&#37327;&#20272;&#35745;&#65306;&#19968;&#31181;&#36879;&#26126;&#12289;&#26080;&#30417;&#30563;&#30340;&#35789;&#32423;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#40657;&#30418;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Perturbation-based QE: An Explainable, Unsupervised Word-level Quality Estimation Method for Blackbox Machine Translation. (arXiv:2305.07457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36879;&#26126;&#12289;&#26080;&#30417;&#30563;&#30340;&#35789;&#32423;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25200;&#21160;&#30340;&#36755;&#20837;&#28304;&#21477;&#23376;&#19978;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#36755;&#20986;&#26469;&#24037;&#20316;&#65292;&#24182;&#21487;&#20197;&#35780;&#20272;&#20219;&#20309;&#31867;&#22411;&#30340;&#40657;&#30418;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;&#20351;&#29992;&#35813;&#26041;&#27861;&#20316;&#20026;&#21453;&#39304;&#20449;&#21495;&#65292;&#21487;&#20197;&#22312;&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#25913;&#36827;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#20272;&#35745;&#65288;QE&#65289;&#26159;&#39044;&#27979;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#31995;&#32479;&#36755;&#20986;&#36136;&#37327;&#30340;&#20219;&#21153;&#65292;&#19981;&#20351;&#29992;&#20219;&#20309;&#40644;&#37329;&#26631;&#20934;&#32763;&#35793;&#21442;&#32771;&#12290;&#30446;&#21069;&#30340;QE&#27169;&#22411;&#26159;&#30417;&#30563;&#30340;&#65306;&#23427;&#20204;&#38656;&#35201;&#23545;&#19968;&#20123;&#25968;&#25454;&#38598;&#19978;&#26576;&#20123;MT&#31995;&#32479;&#36755;&#20986;&#36827;&#34892;&#20154;&#31867;&#26631;&#27880;&#36136;&#37327;&#26469;&#36827;&#34892;&#22521;&#35757;&#65292;&#20351;&#23427;&#20204;&#19982;&#22495;&#30456;&#20851;&#21644;MT&#31995;&#32479;&#30456;&#20851;&#12290;&#26377;&#30740;&#31350;&#23545;&#26080;&#30417;&#30563;&#30340;QE&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#38656;&#35201;&#29627;&#29827;&#30418;&#35775;&#38382;MT&#31995;&#32479;&#65292;&#25110;&#32773;&#20351;&#29992;&#24182;&#34892;MT&#25968;&#25454;&#26469;&#29983;&#25104;&#21512;&#25104;&#38169;&#35823;&#20197;&#35757;&#32451;QE&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25668;&#21160;&#30340;QE&#26041;&#27861;-&#19968;&#31181;&#35789;&#32423;&#36136;&#37327;&#20272;&#35745;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#25200;&#21160;&#30340;&#36755;&#20837;&#28304;&#21477;&#23376;&#19978;MT&#31995;&#32479;&#36755;&#20986;&#26469;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26080;&#30417;&#30563;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#65292;&#24182;&#21487;&#20197;&#35780;&#20272;&#20219;&#20309;&#31867;&#22411;&#30340;&#40657;&#30418;MT&#31995;&#32479;&#65292;&#21253;&#25324;&#30446;&#21069;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#20855;&#26377;&#19981;&#36879;&#26126;&#20869;&#37096;&#36807;&#31243;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#23545;&#20110;&#27809;&#26377;&#26631;&#35760;QE&#25968;&#25454;&#30340;&#35821;&#35328;&#26041;&#21521;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#30456;&#20284;&#25110;&#26356;&#22909;&#30340;&#38646;-shot&#30417;&#30563;&#31995;&#32479;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#25668;&#21160;&#30340;QE&#20316;&#20026;&#21453;&#39304;&#20449;&#21495;&#65292;&#22312;&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#20013;&#25913;&#36827;MT&#31995;&#32479;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quality Estimation (QE) is the task of predicting the quality of Machine Translation (MT) system output, without using any gold-standard translation references. State-of-the-art QE models are supervised: they require human-labeled quality of some MT system output on some datasets for training, making them domain-dependent and MT-system-dependent. There has been research on unsupervised QE, which requires glass-box access to the MT systems, or parallel MT data to generate synthetic errors for training QE models. In this paper, we present Perturbation-based QE - a word-level Quality Estimation approach that works simply by analyzing MT system output on perturbed input source sentences. Our approach is unsupervised, explainable, and can evaluate any type of blackbox MT systems, including the currently prominent large language models (LLMs) with opaque internal processes. For language directions with no labeled QE data, our approach has similar or better performance than the zero-shot supe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32423;&#32852;&#30340;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#65292;&#20351;&#29992;&#23436;&#20840;&#38750;&#37197;&#23545;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#26080;&#30417;&#30563;&#31995;&#32479;&#12290;&#36890;&#36807;&#37319;&#29992;&#21435;&#22122;&#21453;&#21521;&#32763;&#35793;&#25216;&#26415;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#25152;&#26377;&#19977;&#20010;&#32763;&#35793;&#26041;&#21521;&#30340;BLEU&#24471;&#20998;0.7-0.9&#12290;</title><link>http://arxiv.org/abs/2305.07455</link><description>&lt;p&gt;
&#29992;&#21435;&#22122;&#21453;&#21521;&#32763;&#35793;&#25552;&#39640;&#32423;&#32852;&#38750;&#30417;&#30563;&#35821;&#38899;&#32763;&#35793;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation. (arXiv:2305.07455v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07455
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32423;&#32852;&#30340;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#65292;&#20351;&#29992;&#23436;&#20840;&#38750;&#37197;&#23545;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#26080;&#30417;&#30563;&#31995;&#32479;&#12290;&#36890;&#36807;&#37319;&#29992;&#21435;&#22122;&#21453;&#21521;&#32763;&#35793;&#25216;&#26415;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#25152;&#26377;&#19977;&#20010;&#32763;&#35793;&#26041;&#21521;&#30340;BLEU&#24471;&#20998;0.7-0.9&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#37117;&#20005;&#37325;&#20381;&#36182;&#20110;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#32780;&#35328;&#65292;&#25910;&#38598;&#36825;&#31181;&#25968;&#25454;&#38750;&#24120;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32423;&#32852;&#30340;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#37197;&#23545;&#25968;&#25454;&#12290;&#25105;&#20204;&#20351;&#29992;&#23436;&#20840;&#38750;&#37197;&#23545;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#31995;&#32479;&#65292;&#24182;&#22312; CoVoST 2 &#21644; CVSS &#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#35821;&#35328;&#23545;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20854;&#20182;&#19968;&#20123;&#26089;&#26399;&#30417;&#30563;&#26041;&#27861;&#30456;&#24403;&#12290;&#23613;&#31649;&#32423;&#32852;&#31995;&#32479;&#24635;&#26159;&#23384;&#22312;&#20005;&#37325;&#30340;&#35823;&#24046;&#20256;&#25773;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#22122;&#21453;&#21521;&#32763;&#35793;&#65288;DBT&#65289;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#20581;&#22766;&#30340;&#26080;&#30417;&#30563;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;UNMT&#65289;&#31995;&#32479;&#12290;DBT&#25104;&#21151;&#25552;&#39640;&#20102;&#25152;&#26377;&#19977;&#20010;&#32763;&#35793;&#26041;&#21521;&#30340; BLEU &#24471;&#20998;0.7-0.9&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31616;&#21270;&#20102;&#32423;&#32852;&#31995;&#32479;&#30340;&#27969;&#31243;&#65292;&#20197;&#38477;&#20302;&#25512;&#29702;&#24310;&#36831;&#65292;&#24182;&#23545;&#25105;&#20204;&#24037;&#20316;&#30340;&#27599;&#20010;&#37096;&#20998;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#36824;&#22312;EST&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most of the speech translation models heavily rely on parallel data, which is hard to collect especially for low-resource languages. To tackle this issue, we propose to build a cascaded speech translation system without leveraging any kind of paired data. We use fully unpaired data to train our unsupervised systems and evaluate our results on CoVoST 2 and CVSS. The results show that our work is comparable with some other early supervised methods in some language pairs. While cascaded systems always suffer from severe error propagation problems, we proposed denoising back-translation (DBT), a novel approach to building robust unsupervised neural machine translation (UNMT). DBT successfully increases the BLEU score by 0.7--0.9 in all three translation directions. Moreover, we simplified the pipeline of our cascaded system to reduce inference latency and conducted a comprehensive analysis of every part of our work. We also demonstrate our unsupervised speech translation results on the est
&lt;/p&gt;</description></item><item><title>QVoice&#26159;&#19968;&#31181;&#38463;&#25289;&#20271;&#35821;&#21457;&#38899;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#65292;&#26088;&#22312;&#24110;&#21161;&#38750;&#27597;&#35821;&#32773;&#25552;&#39640;&#21457;&#38899;&#25216;&#33021;&#65292;&#21516;&#26102;&#24110;&#21161;&#27597;&#35821;&#32773;&#36991;&#20813;&#22320;&#21306;&#26041;&#35328;&#23545;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#21457;&#38899;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.07445</link><description>&lt;p&gt;
QVoice: &#38463;&#25289;&#20271;&#35821;&#35821;&#38899;&#21457;&#38899;&#23398;&#20064;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
QVoice: Arabic Speech Pronunciation Learning Application. (arXiv:2305.07445v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07445
&lt;/p&gt;
&lt;p&gt;
QVoice&#26159;&#19968;&#31181;&#38463;&#25289;&#20271;&#35821;&#21457;&#38899;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#65292;&#26088;&#22312;&#24110;&#21161;&#38750;&#27597;&#35821;&#32773;&#25552;&#39640;&#21457;&#38899;&#25216;&#33021;&#65292;&#21516;&#26102;&#24110;&#21161;&#27597;&#35821;&#32773;&#36991;&#20813;&#22320;&#21306;&#26041;&#35328;&#23545;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#21457;&#38899;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#38463;&#25289;&#20271;&#35821;&#21457;&#38899;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;QVoice&#65292;&#20854;&#37197;&#22791;&#20102;&#31471;&#21040;&#31471;&#30340;&#21457;&#38899;&#38169;&#35823;&#26816;&#27979;&#21644;&#21453;&#39304;&#29983;&#25104;&#27169;&#22359;&#12290;&#35813;&#24212;&#29992;&#31243;&#24207;&#26088;&#22312;&#25903;&#25345;&#38750;&#27597;&#35821;&#38463;&#25289;&#20271;&#35821;&#30340;&#20154;&#25552;&#39640;&#20854;&#21457;&#38899;&#25216;&#33021;&#65292;&#24182;&#24110;&#21161;&#38463;&#25289;&#20271;&#35821;&#27597;&#35821;&#32773;&#36991;&#20813;&#22320;&#21306;&#26041;&#35328;&#23545;&#20854;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#65288;MSA&#65289;&#21457;&#38899;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;QVoice&#37319;&#29992;&#21508;&#31181;&#23398;&#20064;&#25552;&#31034;&#65292;&#24110;&#21161;&#23398;&#20064;&#32773;&#29702;&#35299;&#21547;&#20041;&#65292;&#19982;&#20854;&#23545;&#33521;&#35821;&#35821;&#35328;&#30340;&#29616;&#26377;&#30693;&#35782;&#24314;&#31435;&#32852;&#31995;&#65292;&#24182;&#25552;&#20379;&#35814;&#32454;&#30340;&#21457;&#38899;&#32416;&#27491;&#21453;&#39304;&#65292;&#20197;&#21450;&#23637;&#31034;&#21333;&#35789;&#29992;&#27861;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#12290;QVoice&#20013;&#30340;&#23398;&#20064;&#25552;&#31034;&#28085;&#30422;&#20102;&#21508;&#31181;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#65292;&#22914;&#30701;&#35821;/&#21333;&#35789;&#21450;&#20854;&#32763;&#35793;&#30340;&#21487;&#35270;&#21270;&#65292;&#20197;&#21450;&#38899;&#26631;&#21644;&#38899;&#35793;&#12290;QVoice&#25552;&#20379;&#23383;&#31526;&#32423;&#21035;&#30340;&#21457;&#38899;&#21453;&#39304;&#65292;&#24182;&#23545;&#21333;&#35789;&#32423;&#21035;&#30340;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel Arabic pronunciation learning application QVoice, powered with end-to-end mispronunciation detection and feedback generator module. The application is designed to support non-native Arabic speakers in enhancing their pronunciation skills, while also helping native speakers mitigate any potential influence from regional dialects on their Modern Standard Arabic (MSA) pronunciation. QVoice employs various learning cues to aid learners in comprehending meaning, drawing connections with their existing knowledge of English language, and offers detailed feedback for pronunciation correction, along with contextual examples showcasing word usage. The learning cues featured in QVoice encompass a wide range of meaningful information, such as visualizations of phrases/words and their translations, as well as phonetic transcriptions and transliterations. QVoice provides pronunciation feedback at the character level and assesses performance at the word level.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;IS-CSE&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#20363;&#24179;&#28369;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#65292;&#20197;&#24179;&#28369;&#23884;&#20837;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26631;&#20934;&#30340;STS&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.07424</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#30340;&#23454;&#20363;&#24179;&#28369;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Instance Smoothed Contrastive Learning for Unsupervised Sentence Embedding. (arXiv:2305.07424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;IS-CSE&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#20363;&#24179;&#28369;&#23545;&#27604;&#23398;&#20064;&#26469;&#23398;&#20064;&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#65292;&#20197;&#24179;&#28369;&#23884;&#20837;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26631;&#20934;&#30340;STS&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22914;unsup-SimCSE&#65292;&#22312;&#23398;&#20064;&#26080;&#30417;&#30563;&#21477;&#23376;&#23884;&#20837;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#29992;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#27599;&#20010;&#23884;&#20837;&#20165;&#26469;&#33258;&#20110;&#19968;&#20010;&#21477;&#23376;&#23454;&#20363;&#65292;&#25105;&#20204;&#31216;&#36825;&#20123;&#23884;&#20837;&#20026;&#23454;&#20363;&#32423;&#23884;&#20837;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#23884;&#20837;&#34987;&#35270;&#20026;&#26159;&#19968;&#31867;&#29420;&#29305;&#30340;&#31867;&#65292;&#36825;&#21487;&#33021;&#20250;&#25439;&#23475;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;IS-CSE&#65288;&#23454;&#20363;&#24179;&#28369;&#23545;&#27604;&#21477;&#23376;&#23884;&#20837;&#65289;&#26469;&#24179;&#28369;&#29305;&#24449;&#31354;&#38388;&#20013;&#23884;&#20837;&#30340;&#36793;&#30028;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26681;&#25454;&#35821;&#20041;&#30456;&#20284;&#24615;&#20174;&#21160;&#24577;&#20869;&#23384;&#32531;&#20914;&#21306;&#20013;&#26816;&#32034;&#23884;&#20837;&#20197;&#33719;&#24471;&#27491;&#23884;&#20837;&#32452;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#33258;&#27880;&#24847;&#21147;&#25805;&#20316;&#23545;&#32452;&#20013;&#30340;&#23884;&#20837;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#29983;&#25104;&#24179;&#28369;&#23454;&#20363;&#23884;&#20837;&#20197;&#36827;&#34892;&#36827;&#19968;&#27493;&#20998;&#26512;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#22343;78.30&#65285;&#65292;79.47&#65285;&#65292;77.73&#65285;&#21644;79.42&#65285;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning-based methods, such as unsup-SimCSE, have achieved state-of-the-art (SOTA) performances in learning unsupervised sentence embeddings. However, in previous studies, each embedding used for contrastive learning only derived from one sentence instance, and we call these embeddings instance-level embeddings. In other words, each embedding is regarded as a unique class of its own, whichmay hurt the generalization performance. In this study, we propose IS-CSE (instance smoothing contrastive sentence embedding) to smooth the boundaries of embeddings in the feature space. Specifically, we retrieve embeddings from a dynamic memory buffer according to the semantic similarity to get a positive embedding group. Then embeddings in the group are aggregated by a self-attention operation to produce a smoothed instance embedding for further analysis. We evaluate our method on standard semantic text similarity (STS) tasks and achieve an average of 78.30%, 79.47%, 77.73%, and 79.42% 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#27169;&#22411;&#21163;&#25345;&#25915;&#20987;&#30340;&#33539;&#22260;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ditto&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#19981;&#21516;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#21163;&#25345;&#20026;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#25915;&#20987;&#30340;&#25104;&#21151;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07406</link><description>&lt;p&gt;
&#20004;&#21512;&#19968;&#65306;&#19968;&#31181;&#38024;&#23545;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#30340;&#27169;&#22411;&#21163;&#25345;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Two-in-One: A Model Hijacking Attack Against Text Generation Models. (arXiv:2305.07406v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#27169;&#22411;&#21163;&#25345;&#25915;&#20987;&#30340;&#33539;&#22260;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ditto&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#19981;&#21516;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#21163;&#25345;&#20026;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#25915;&#20987;&#30340;&#25104;&#21151;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20174;&#20154;&#33080;&#35782;&#21035;&#21040;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#25104;&#21151;&#20063;&#20276;&#38543;&#30528;&#21508;&#31181;&#25915;&#20987;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#65292;&#21363;&#27169;&#22411;&#21163;&#25345;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#25552;&#39640;&#20102;&#38382;&#36131;&#21644;&#23492;&#29983;&#35745;&#31639;&#30340;&#39118;&#38505;&#12290;&#20294;&#26159;&#65292;&#35813;&#25915;&#20987;&#20165;&#38598;&#20013;&#20110;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#27492;&#25915;&#20987;&#30340;&#33539;&#22260;&#25193;&#22823;&#21040;&#21253;&#25324;&#25991;&#26412;&#29983;&#25104;&#21644;&#20998;&#31867;&#27169;&#22411;&#65292;&#20174;&#32780;&#23637;&#31034;&#20854;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21163;&#25345;&#25915;&#20987;&#8212;&#8212;Ditto&#65292;&#23427;&#21487;&#20197;&#23558;&#19981;&#21516;&#30340;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#21163;&#25345;&#20026;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#65292;&#20363;&#22914;&#35821;&#35328;&#32763;&#35793;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#35821;&#35328;&#24314;&#27169;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#25991;&#26412;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;&#22914;SST-2&#12289;TweetEval&#12289;AGnews&#12289;QNLI&#21644;IMDB&#65289;&#26469;&#35780;&#20272;&#25105;&#20204;&#25915;&#20987;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;Ditto&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25104;&#21151;&#22320;&#21163;&#25345;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has progressed significantly in various applications ranging from face recognition to text generation. However, its success has been accompanied by different attacks. Recently a new attack has been proposed which raises both accountability and parasitic computing risks, namely the model hijacking attack. Nevertheless, this attack has only focused on image classification tasks. In this work, we broaden the scope of this attack to include text generation and classification models, hence showing its broader applicability. More concretely, we propose a new model hijacking attack, Ditto, that can hijack different text classification tasks into multiple generation ones, e.g., language translation, text summarization, and language modeling. We use a range of text benchmark datasets such as SST-2, TweetEval, AGnews, QNLI, and IMDB to evaluate the performance of our attacks. Our results show that by using Ditto, an adversary can successfully hijack text generation models withou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;InteR&#65292;&#36890;&#36807;&#25628;&#32034;&#24341;&#25806;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#20419;&#36827;&#30693;&#35782;&#31934;&#28860;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07402</link><description>&lt;p&gt;
&#25628;&#32034;&#24341;&#25806;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38388;&#30340;&#20132;&#20114;&#20248;&#21270;&#30693;&#35782;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Knowledge Refinement via Interaction Between Search Engines and Large Language Models. (arXiv:2305.07402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;InteR&#65292;&#36890;&#36807;&#25628;&#32034;&#24341;&#25806;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#20419;&#36827;&#30693;&#35782;&#31934;&#28860;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#22312;&#20174;&#22823;&#37327;&#25968;&#25454;&#20013;&#23450;&#20301;&#30456;&#20851;&#36164;&#28304;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#65292;&#20854;&#24212;&#29992;&#24050;&#20174;&#20256;&#32479;&#30693;&#35782;&#24211;&#21457;&#23637;&#33267;&#29616;&#20195;&#25628;&#32034;&#24341;&#25806;&#65288;SEs&#65289;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#36827;&#19968;&#27493;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#19982;&#25628;&#32034;&#31995;&#32479;&#20132;&#20114;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20102;&#35813;&#39046;&#22495;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;LLMs&#21644;SEs&#30340;&#20248;&#32570;&#28857;&#65292;&#24378;&#35843;&#23427;&#20204;&#22312;&#29702;&#35299;&#29992;&#25143;&#26597;&#35810;&#21644;&#26816;&#32034;&#26368;&#26032;&#20449;&#24687;&#26041;&#38754;&#30340;&#21508;&#33258;&#20248;&#21183;&#12290;&#20026;&#20102;&#21033;&#29992;&#20004;&#31181;&#33539;&#20363;&#30340;&#20248;&#21183;&#24182;&#36991;&#20813;&#20854;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InteR&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;SEs&#21644;LLMs&#20043;&#38388;&#30340;&#20132;&#20114;&#20419;&#36827;&#30693;&#35782;&#31934;&#28860;&#30340;&#26032;&#26694;&#26550;&#12290; InteR&#20351;SEs&#33021;&#22815;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#25688;&#35201;&#26469;&#35843;&#25972;&#26597;&#35810;&#65292;&#21516;&#26102;&#20351;LLMs&#33021;&#22815;&#20351;&#29992;SE&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#26469;&#22686;&#24378;&#25552;&#31034;&#12290;&#36825;&#31181;&#36845;&#20195;&#30340;&#31934;&#28860;&#36807;&#31243;&#22686;&#24378;&#20102;SEs&#21644;LLMs&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#20934;&#30830;&#30340;&#26816;&#32034;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information retrieval (IR) plays a crucial role in locating relevant resources from vast amounts of data, and its applications have evolved from traditional knowledge bases to modern search engines (SEs). The emergence of large language models (LLMs) has further revolutionized the field by enabling users to interact with search systems in natural language. In this paper, we explore the advantages and disadvantages of LLMs and SEs, highlighting their respective strengths in understanding user-issued queries and retrieving up-to-date information. To leverage the benefits of both paradigms while circumventing their limitations, we propose InteR, a novel framework that facilitates knowledge refinement through interaction between SEs and LLMs. InteR allows SEs to refine knowledge in query using LLM-generated summaries and enables LLMs to enhance prompts using SE-retrieved documents. This iterative refinement process augments the inputs of SEs and LLMs, leading to more accurate retrieval. Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#22495;&#38750;&#33521;&#35821;&#35821;&#35328;&#23545;&#35805;&#31995;&#32479;&#20013;&#23569;&#37327;&#25968;&#25454;&#19979;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#20845;&#31181;&#35821;&#35328;&#19978;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07393</link><description>&lt;p&gt;
&#38024;&#23545;&#24320;&#25918;&#22495;&#23545;&#35805;&#29983;&#25104;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#20013;&#38477;&#20302;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Prompt Learning to Mitigate Catastrophic Forgetting in Cross-lingual Transfer for Open-domain Dialogue Generation. (arXiv:2305.07393v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#24320;&#25918;&#22495;&#38750;&#33521;&#35821;&#35821;&#35328;&#23545;&#35805;&#31995;&#32479;&#20013;&#23569;&#37327;&#25968;&#25454;&#19979;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#20845;&#31181;&#35821;&#35328;&#19978;&#30340;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#23545;&#35805;&#31995;&#32479;&#19968;&#30452;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#31532;&#19968;&#27425;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#26377;&#38480;&#25968;&#25454;&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#29983;&#25104;&#20013;&#30740;&#31350;&#20102;&#23569;&#26679;&#26412;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#65288;FS-XLT&#65289;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#12290;&#22312;&#21021;&#27493;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;FS-XLT&#21644;MTL&#20013;&#25152;&#26377;&#30340;6&#31181;&#35821;&#35328;&#20013;&#37117;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22266;&#23450;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#35843;&#21442;&#21644;&#25105;&#20204;&#25163;&#24037;&#21046;&#20316;&#30340;&#25552;&#31034;&#35821;&#26469;&#24357;&#21512;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#20445;&#25345;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;mPLM&#65289;&#22312;FS-XLT&#21644;MTL&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#12290;&#22312;&#25152;&#26377;6&#31181;&#35821;&#35328;&#19978;&#30340;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#32467;&#26524;&#37117;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; https://github.com/JeremyLeiLiu/XLinguDial &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue systems for non-English languages have long been under-explored. In this paper, we take the first step to investigate few-shot cross-lingual transfer learning (FS-XLT) and multitask learning (MTL) in the context of open-domain dialogue generation for non-English languages with limited data. We observed catastrophic forgetting in both FS-XLT and MTL for all 6 languages in our preliminary experiments. To mitigate the issue, we propose a simple yet effective prompt learning approach that can preserve the multilinguality of multilingual pre-trained language model (mPLM) in FS-XLT and MTL by bridging the gap between pre-training and fine-tuning with Fixed-prompt LM Tuning and our hand-crafted prompts. Experimental results on all 6 languages in terms of both automatic and human evaluations demonstrate the effectiveness of our approach. Our code is available at https://github.com/JeremyLeiLiu/XLinguDial.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#23545;L2&#33521;&#35821;&#35821;&#38899;&#21464;&#20307;&#30340;&#25935;&#24863;&#24230;&#65292;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#29575;&#21644;&#25163;&#21160;&#32416;&#27491;&#30340;&#25928;&#29575;&#65292;&#20026;&#20919;&#38376;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2305.07389</link><description>&lt;p&gt;
&#25506;&#31350;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#23545;L2&#33521;&#35821;&#35821;&#38899;&#21464;&#20307;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating the Sensitivity of Automatic Speech Recognition Systems to Phonetic Variation in L2 Englishes. (arXiv:2305.07389v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#32034;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#23545;L2&#33521;&#35821;&#35821;&#38899;&#21464;&#20307;&#30340;&#25935;&#24863;&#24230;&#65292;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#29575;&#21644;&#25163;&#21160;&#32416;&#27491;&#30340;&#25928;&#29575;&#65292;&#20026;&#20919;&#38376;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#22312;&#33021;&#35782;&#21035;&#19982;&#20854;&#35757;&#32451;&#25968;&#25454;&#30456;&#20284;&#30340;&#35821;&#38899;&#26102;&#34920;&#29616;&#26368;&#20339;&#12290;&#22240;&#27492;&#65292;&#22320;&#26041;&#26041;&#35328;&#12289;&#23569;&#25968;&#26063;&#35028;&#20351;&#29992;&#33521;&#35821;&#21644;&#20302;&#36164;&#28304;&#35821;&#31181;&#31561;&#36739;&#20026;&#20919;&#38376;&#30340;&#35821;&#35328;&#65292;&#19982;&#27604;&#36739;&#27491;&#32479;&#12289;&#20027;&#27969;&#25110;&#26631;&#20934;&#30340;&#35821;&#35328;&#30456;&#27604;&#20986;&#29616;&#26356;&#39640;&#30340;&#35782;&#21035;&#38169;&#35823;&#29575;&#12290;&#36825;&#25104;&#20026;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#32435;&#20837;&#22823;&#35268;&#27169;&#35821;&#35328;&#23398;&#30740;&#31350;&#30340;&#26631;&#27880;&#36807;&#31243;&#30340;&#38556;&#30861;&#65292;&#22240;&#20026;&#25163;&#21160;&#32416;&#27491;&#38169;&#35823;&#30340;&#33258;&#21160;&#21270;&#25991;&#26412;&#19982;&#25163;&#21160;&#36716;&#24405;&#21516;&#26679;&#38656;&#35201;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#34892;&#20026;&#26377;&#26356;&#28145;&#23618;&#27425;&#30340;&#29702;&#35299;&#65292;&#26080;&#35770;&#26159;&#20174;&#35821;&#38899;&#25216;&#26415;&#35282;&#24230;&#20986;&#21457;&#65292;&#36824;&#26159;&#20174;&#26631;&#27880;&#35282;&#24230;&#20986;&#21457;&#65292;&#37117;&#26377;&#30410;&#20110;&#25552;&#39640;&#35782;&#21035;&#20934;&#30830;&#24230;&#25110;&#32773;&#20026;&#25163;&#21160;&#32416;&#27491;&#25552;&#20379;&#32447;&#32034;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#25506;&#31350;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#20197;&#21457;&#29616;&#20854;&#22914;&#20309;&#22788;&#29702;&#19968;&#20010;&#36739;&#23567;&#35821;&#31181;&#20013;&#30340;&#35821;&#38899;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic Speech Recognition (ASR) systems exhibit the best performance on speech that is similar to that on which it was trained. As such, underrepresented varieties including regional dialects, minority-speakers, and low-resource languages, see much higher word error rates (WERs) than those varieties seen as 'prestigious', 'mainstream', or 'standard'. This can act as a barrier to incorporating ASR technology into the annotation process for large-scale linguistic research since the manual correction of the erroneous automated transcripts can be just as time and resource consuming as manual transcriptions. A deeper understanding of the behaviour of an ASR system is thus beneficial from a speech technology standpoint, in terms of improving ASR accuracy, and from an annotation standpoint, where knowing the likely errors made by an ASR system can aid in this manual correction. This work demonstrates a method of probing an ASR system to discover how it handles phonetic variation across a n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#65292;&#20294;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#22240;&#26524;&#24187;&#35273;&#38382;&#39064;&#65292;&#23545;&#20110;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.07375</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#26029;&#22120;&#21527;&#65311;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation. (arXiv:2305.07375v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#65292;&#20294;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#22240;&#26524;&#24187;&#35273;&#38382;&#39064;&#65292;&#23545;&#20110;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#23545;&#20110;&#20247;&#22810;NLP&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;ChatGPT&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#20852;&#33021;&#21147;&#65292;&#20294;ChatGPT&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#22914;&#20309;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ChatGPT&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#20294;&#26159;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#12290;&#27492;&#22806;&#65292;ChatGPT&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#20005;&#37325;&#30340;&#24187;&#35273;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#20013;&#22240;&#26524;&#20851;&#31995;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#30340;&#25253;&#21578;&#20559;&#35265;&#65292;&#20197;&#21450;ChatGPT&#30340;&#21319;&#32423;&#36807;&#31243;&#65292;&#22914;RLHF&#12290;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;COT&#65289;&#25216;&#26415;&#26041;&#38754;&#65292;&#21487;&#33021;&#20250;&#36827;&#19968;&#27493;&#21152;&#21095;&#36825;&#31181;&#22240;&#26524;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#23545;&#20110;&#22312;&#25552;&#31034;&#20013;&#34920;&#36798;&#22240;&#26524;&#27010;&#24565;&#30340;&#35789;&#35821;&#38750;&#24120;&#25935;&#24863;&#65292;&#24182;&#19988;&#23553;&#38381;&#25552;&#31034;&#27604;&#24320;&#25918;&#25552;&#31034;&#34920;&#29616;&#26356;&#22909;&#12290;&#23545;&#20110;&#21477;&#23376;&#20013;&#30340;&#20107;&#20214;&#65292;ChatGPT&#25797;&#38271;&#25429;&#25417;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT's causal reasoning capabilities. Experiments show that ChatGPT is not a good causal reasoner, but a good causal interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes, such as RLHF. The In-Context Learning (ICL) and Chain-of-Though (COT) techniques can further exacerbate such causal hallucination. Additionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts. For events in sentences, ChatGPT excels at capturing explicit caus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#20132;&#20114;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#30452;&#25509;&#32534;&#36753;&#19968;&#27493;&#27493;&#35299;&#37322;&#38169;&#35823;SQL&#20197;&#20462;&#22797;SQL&#38169;&#35823;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#25552;&#39640;&#20102;31.6&#65285;&#30340;&#25191;&#34892;&#20934;&#30830;&#24615;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#24110;&#21161;&#29992;&#25143;&#20197;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#20449;&#24515;&#35299;&#20915;&#20102;&#26356;&#22810;&#30340;SQL&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.07372</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#32534;&#36753;&#30340;&#36880;&#27493;&#35299;&#37322;&#23454;&#29616;&#20132;&#20114;&#24335;&#25991;&#26412;&#36716;SQL
&lt;/p&gt;
&lt;p&gt;
Interactive Text-to-SQL Generation via Editable Step-by-Step Explanations. (arXiv:2305.07372v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07372
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#20132;&#20114;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#30452;&#25509;&#32534;&#36753;&#19968;&#27493;&#27493;&#35299;&#37322;&#38169;&#35823;SQL&#20197;&#20462;&#22797;SQL&#38169;&#35823;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#25552;&#39640;&#20102;31.6&#65285;&#30340;&#25191;&#34892;&#20934;&#30830;&#24615;&#12290;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#24110;&#21161;&#29992;&#25143;&#20197;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#20449;&#24515;&#35299;&#20915;&#20102;&#26356;&#22810;&#30340;SQL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25968;&#25454;&#24211;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#38750;&#19987;&#23478;&#24456;&#38590;&#23436;&#20840;&#37322;&#25918;&#20851;&#31995;&#25968;&#25454;&#24211;&#30340;&#20998;&#26512;&#33021;&#21147;&#65292;&#22240;&#20026;&#20182;&#20204;&#19981;&#29087;&#24713;SQL&#31561;&#25968;&#25454;&#24211;&#35821;&#35328;&#12290;&#35768;&#22810;&#25216;&#26415;&#24050;&#34987;&#25552;&#20986;&#33258;&#28982;&#35821;&#35328;&#33258;&#21160;&#29983;&#25104;SQL&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#20197;&#19979;&#20004;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#23545;&#20110;&#22797;&#26434;&#26597;&#35810;&#23427;&#20204;&#20173;&#20250;&#29359;&#24456;&#22810;&#38169;&#35823;&#65292;&#65288;2&#65289;&#23427;&#20204;&#19981;&#25552;&#20379;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#24335;&#65292;&#35753;&#38750;&#19987;&#23478;&#29992;&#25143;&#39564;&#35777;&#21644;&#25913;&#36827;&#19981;&#27491;&#30830;&#30340;&#26597;&#35810;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20132;&#20114;&#26426;&#21046;&#65292;&#20801;&#35768;&#29992;&#25143;&#30452;&#25509;&#32534;&#36753;&#19968;&#27493;&#27493;&#35299;&#37322;&#38169;&#35823;SQL&#20197;&#20462;&#22797;SQL&#38169;&#35823;&#12290;&#22312;Spider&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33267;&#23569;&#27604;&#19977;&#31181;&#26368;&#20808;&#36827;&#26041;&#27861;&#22312;&#25191;&#34892;&#20934;&#30830;&#24615;&#26041;&#38754;&#25552;&#39640;&#20102;31.6&#65285;&#12290;24&#21517;&#21442;&#19982;&#32773;&#30340;&#29992;&#25143;&#30740;&#31350;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24110;&#21161;&#29992;&#25143;&#20197;&#26356;&#23569;&#30340;&#26102;&#38388;&#21644;&#26356;&#39640;&#30340;&#20449;&#24515;&#35299;&#20915;&#20102;&#26356;&#22810;&#30340;SQL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relational databases play an important role in this Big Data era. However, it is challenging for non-experts to fully unleash the analytical power of relational databases, since they are not familiar with database languages such as SQL. Many techniques have been proposed to automatically generate SQL from natural language, but they suffer from two issues: (1) they still make many mistakes, particularly for complex queries, and (2) they do not provide a flexible way for non-expert users to validate and refine the incorrect queries. To address these issues, we introduce a new interaction mechanism that allows users directly edit a step-by-step explanation of an incorrect SQL to fix SQL errors. Experiments on the Spider benchmark show that our approach outperforms three SOTA approaches by at least 31.6% in terms of execution accuracy. A user study with 24 participants further shows that our approach helped users solve significantly more SQL tasks with less time and higher confidence, demo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22825;&#22478;&#25991;&#36763;&#36842;&#36716;&#25442;&#20026;&#27874;&#26031;-&#38463;&#25289;&#20271;&#25991;&#36763;&#36842;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#28151;&#21512;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#21644;&#27010;&#29575;&#27169;&#22411;&#65292;&#31995;&#32479;&#21462;&#24471;&#20102;99.64&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.07365</link><description>&lt;p&gt;
&#23454;&#29616;&#20174;&#22825;&#22478;&#25991;&#36763;&#36842;&#21040;&#27874;&#26031;-&#38463;&#25289;&#20271;&#25991;&#36763;&#36842;&#30340;&#38899;&#35793;
&lt;/p&gt;
&lt;p&gt;
Towards Transliteration between Sindhi Scripts from Devanagari to Perso-Arabic. (arXiv:2305.07365v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22825;&#22478;&#25991;&#36763;&#36842;&#36716;&#25442;&#20026;&#27874;&#26031;-&#38463;&#25289;&#20271;&#25991;&#36763;&#36842;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#28151;&#21512;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#21644;&#27010;&#29575;&#27169;&#22411;&#65292;&#31995;&#32479;&#21462;&#24471;&#20102;99.64&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#22825;&#22478;&#25991;&#36763;&#36842;&#36716;&#25442;&#20026;&#27874;&#26031;-&#38463;&#25289;&#20271;&#25991;&#36763;&#36842;&#30340;&#36716;&#25442;&#25216;&#26415;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#20854;&#20013;&#37096;&#20998;&#25991;&#26412;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#36827;&#34892;&#36716;&#25442;&#65292;&#22914;&#26524;&#26377;&#27495;&#20041;&#65292;&#21017;&#20351;&#29992;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#35299;&#20915;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#31995;&#32479;&#24635;&#20307;&#20934;&#30830;&#29575;&#36798;&#21040;99.64&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we have shown a script conversion (transliteration) technique that converts Sindhi text in the Devanagari script to the Perso-Arabic script. We showed this by incorporating a hybrid approach where some part of the text is converted using a rule base and in case an ambiguity arises then a probabilistic model is used to resolve the same. Using this approach, the system achieved an overall accuracy of 99.64%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21629;&#21517;&#23454;&#20307;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#36827;&#34892;&#32763;&#35793;/&#38899;&#35793;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#33021;&#22815;&#27491;&#30830;&#32763;&#35793;&#22823;&#22810;&#25968;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#20934;&#30830;&#29575;&#39640;&#36798;99.52&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.07360</link><description>&lt;p&gt;
&#36890;&#36807;&#36866;&#24403;&#32763;&#35793;&#21629;&#21517;&#23454;&#20307;&#26469;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Improving the Quality of Neural Machine Translation Through Proper Translation of Name Entities. (arXiv:2305.07360v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21629;&#21517;&#23454;&#20307;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#36827;&#34892;&#32763;&#35793;/&#38899;&#35793;&#65292;&#20197;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#33021;&#22815;&#27491;&#30830;&#32763;&#35793;&#22823;&#22810;&#25968;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#20934;&#30830;&#29575;&#39640;&#36798;99.52&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#21629;&#21517;&#23454;&#20307;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#36827;&#34892;&#32763;&#35793;/&#38899;&#35793;&#26469;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;&#23545;&#20110;&#35780;&#20272;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#21363;&#20154;&#21517;&#12289;&#22320;&#21517;&#21644;&#32452;&#32455;&#21517;&#12290;&#31995;&#32479;&#33021;&#22815;&#27491;&#30830;&#22320;&#32763;&#35793;&#22823;&#22810;&#25968;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#23545;&#20110;&#20154;&#21517;&#65292;&#20934;&#30830;&#29575;&#20026;99.86&#65285;&#65292;&#23545;&#20110;&#22320;&#21517;&#65292;&#20934;&#30830;&#29575;&#20026;99.63&#65285;&#65292;&#23545;&#20110;&#32452;&#32455;&#21517;&#65292;&#20934;&#30830;&#29575;&#20026;99.05&#65285;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#31995;&#32479;&#30340;&#20934;&#30830;&#29575;&#20026;99.52&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we have shown a method of improving the quality of neural machine translation by translating/transliterating name entities as a preprocessing step. Through experiments we have shown the performance gain of our system. For evaluation we considered three types of name entities viz person names, location names and organization names. The system was able to correctly translate mostly all the name entities. For person names the accuracy was 99.86%, for location names the accuracy was 99.63% and for organization names the accuracy was 99.05%. Overall, the accuracy of the system was 99.52%
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;X-adapter&#25554;&#25300;&#24335;&#27169;&#22359;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#39640;&#25928;&#22320;&#21521;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#27880;&#20837;&#35270;&#35273;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.07358</link><description>&lt;p&gt;
&#21033;&#29992;&#36328;&#27169;&#24577;&#36866;&#37197;&#22120;&#21521;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#27880;&#20837;&#22810;&#21151;&#33021;&#39640;&#25928;&#30340;&#35270;&#35273;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Towards Versatile and Efficient Visual Knowledge Injection into Pre-trained Language Models with Cross-Modal Adapters. (arXiv:2305.07358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;X-adapter&#25554;&#25300;&#24335;&#27169;&#22359;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#39640;&#25928;&#22320;&#21521;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#27880;&#20837;&#35270;&#35273;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#22810;&#27169;&#24577;&#30693;&#35782;&#23398;&#20064;&#35821;&#35328;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20165;&#25903;&#25345;&#25991;&#26412;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25554;&#25300;&#24335;&#27169;&#22359;X-adapter&#65292;&#23427;&#33021;&#22815;&#26681;&#25454;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#23545;&#40784;&#35270;&#35273;&#21644;&#25991;&#26412;&#30693;&#35782;&#65292;&#28789;&#27963;&#39640;&#25928;&#22320;&#21521;PLMs&#27880;&#20837;&#35270;&#35273;&#30693;&#35782;&#12290; X-adapter&#21253;&#21547;&#20004;&#20010;&#23376;&#27169;&#22359;V-expert&#21644;T-expert&#65292;&#21487;&#20197;&#26681;&#25454;&#19979;&#28216;&#20219;&#21153;&#28608;&#27963;&#19981;&#21516;&#30340;&#23376;&#27169;&#22359;&#65292;&#26469;&#34701;&#21512;VLMs&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans learn language via multi-modal knowledge. However, due to the text-only pre-training scheme, most existing pre-trained language models (PLMs) are hindered from the multi-modal information.  To inject visual knowledge into PLMs, existing methods incorporate either the text or image encoder of vision-language models (VLMs) to encode the visual information and update all the original parameters of PLMs for knowledge fusion.  In this paper, we propose a new plug-and-play module, X-adapter, to flexibly leverage the aligned visual and textual knowledge learned in pre-trained VLMs and efficiently inject them into PLMs.  Specifically, we insert X-adapters into PLMs, and only the added parameters are updated during adaptation.  To fully exploit the potential in VLMs, X-adapters consist of two sub-modules, V-expert and T-expert, to fuse VLMs' image and text representations, respectively.  We can opt for activating different sub-modules depending on the downstream tasks.  Experimental resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZARA&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#36890;&#36807;&#23558;&#21512;&#29702;&#24615;&#21028;&#26029;&#38382;&#39064;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26469;&#33258;&#21160;&#26500;&#24314;&#20266;&#24179;&#34892;&#25968;&#25454;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#33258;&#25105;&#35299;&#37322;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ZARA&#22312;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#36136;&#37327;&#19978;&#37117;&#34920;&#29616;&#20986;SOTA&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.07355</link><description>&lt;p&gt;
ZARA&#65306;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#33258;&#29702;&#24615;
&lt;/p&gt;
&lt;p&gt;
ZARA: Improving Few-Shot Self-Rationalization for Small Language Models. (arXiv:2305.07355v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZARA&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#36890;&#36807;&#23558;&#21512;&#29702;&#24615;&#21028;&#26029;&#38382;&#39064;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26469;&#33258;&#21160;&#26500;&#24314;&#20266;&#24179;&#34892;&#25968;&#25454;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#33258;&#25105;&#35299;&#37322;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ZARA&#22312;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#36136;&#37327;&#19978;&#37117;&#34920;&#29616;&#20986;SOTA&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#29983;&#25104;&#32456;&#31471;&#20219;&#21153;&#31572;&#26696;&#21644;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#30340;&#35821;&#35328;&#27169;&#22411;&#34987;&#31216;&#20026;&#33258;&#25105;&#35299;&#37322;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26377;&#29702;&#25454;&#30340;&#20363;&#23376;&#26469;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#29616;&#20102;&#23569;&#26679;&#26412;&#33258;&#25105;&#35299;&#37322;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25165;&#33021;&#21463;&#30410;&#20110;&#35299;&#37322;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#24456;&#38590;&#34987;&#33719;&#24471;&#12290;&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#35299;&#37322;&#26469;&#25552;&#39640;&#23569;&#26679;&#26412;&#33258;&#25105;&#35299;&#37322;&#23545;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#39318;&#20808;&#37325;&#26032;&#25506;&#35752;&#20102;&#35299;&#37322;&#21644;&#31572;&#26696;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#21463;&#21040;&#20154;&#31867;&#22914;&#20309;&#35780;&#20272;&#35299;&#37322;&#30340;&#38544;&#21547;&#24605;&#32771;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;ZARA&#65292;&#21363;&#29702;&#24615;&#31572;&#26696;&#23545;&#30340;&#38646;&#26679;&#26412;&#22686;&#24378;&#65292;&#36890;&#36807;&#23558;&#21512;&#29702;&#24615;&#21028;&#26029;&#38382;&#39064;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26469;&#33258;&#21160;&#26500;&#24314;&#20266;&#24179;&#34892;&#25968;&#25454;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;FEB&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ZARA&#22312;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#36136;&#37327;&#19978;&#37117;&#21462;&#24471;&#20102;SOTA&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) that jointly generate end-task answers as well as free-text rationales are known as self-rationalization models. Recent works demonstrate great performance gain for self-rationalization by few-shot prompting LMs with rationale-augmented exemplars. However, the ability to benefit from explanations only emerges with large-scale LMs, which have poor accessibility. In this work, we explore the less-studied setting of leveraging explanations for small LMs to improve few-shot self-rationalization. We first revisit the relationship between rationales and answers. Inspired by the implicit mental process of how human beings assess explanations, we present a novel approach, Zero-shot Augmentation of Rationale-Answer pairs (ZARA), to automatically construct pseudo-parallel data for self-training by reducing the problem of plausibility judgement to natural language inference. Experimental results show ZARA achieves SOTA performance on the FEB benchmark, for both the task accu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#27169;&#22411;&#30340;&#32534;&#31243;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#12290;&#25512;&#20986;&#30340;M&#35821;&#35328;&#23558;&#27169;&#22411;&#20316;&#20026;&#22522;&#26412;&#30340;&#35745;&#31639;&#21333;&#20301;&#65292;&#21152;&#24378;&#20102;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#20851;&#38190;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#36825;&#31181;&#21019;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#23558;&#24443;&#24213;&#25913;&#21464;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.07341</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#32534;&#31243;&#65306;&#20026;&#28145;&#24230;&#23398;&#20064;&#26102;&#20195;&#37325;&#26032;&#23450;&#20041;&#31243;&#24207;&#30340;&#22522;&#26412;&#21333;&#20301;
&lt;/p&gt;
&lt;p&gt;
Model-based Programming: Redefining the Atomic Unit of Programming for the Deep Learning Era. (arXiv:2305.07341v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#27169;&#22411;&#30340;&#32534;&#31243;&#65292;&#26088;&#22312;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#36807;&#31243;&#20013;&#30340;&#38382;&#39064;&#12290;&#25512;&#20986;&#30340;M&#35821;&#35328;&#23558;&#27169;&#22411;&#20316;&#20026;&#22522;&#26412;&#30340;&#35745;&#31639;&#21333;&#20301;&#65292;&#21152;&#24378;&#20102;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#20851;&#38190;&#20219;&#21153;&#30340;&#25928;&#29575;&#65292;&#36825;&#31181;&#21019;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#23558;&#24443;&#24213;&#25913;&#21464;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#27169;&#22411;&#30340;&#32534;&#31243;&#65292;&#26088;&#22312;&#35299;&#20915;&#23558;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#21151;&#65292;&#20294;&#23558;&#23427;&#20204;&#37096;&#32626;&#21040;&#23454;&#38469;&#19994;&#21153;&#22330;&#26223;&#20013;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#22914;&#22797;&#26434;&#30340;&#27169;&#22411;&#35757;&#32451;&#12289;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#20197;&#21450;&#19982;&#29616;&#26377;&#32534;&#31243;&#35821;&#35328;&#30340;&#38598;&#25104;&#38382;&#39064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#27169;&#22411;&#30340;&#32534;&#31243;&#8221;&#27010;&#24565;&#65292;&#24182;&#25512;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#31243;&#35821;&#35328;&#8212;&#8212;M&#35821;&#35328;&#65292;&#35813;&#35821;&#35328;&#38024;&#23545;&#39044;&#26399;&#30340;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#32534;&#31243;&#33539;&#24335;&#32780;&#35774;&#35745;&#12290;M&#35821;&#35328;&#23558;&#27169;&#22411;&#35270;&#20026;&#22522;&#26412;&#30340;&#35745;&#31639;&#21333;&#20301;&#65292;&#20351;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#26356;&#19987;&#27880;&#20110;&#20851;&#38190;&#20219;&#21153;&#65292;&#22914;&#27169;&#22411;&#21152;&#36733;&#12289;&#24494;&#35843;&#12289;&#35780;&#20272;&#21644;&#37096;&#32626;&#65292;&#20174;&#32780;&#22686;&#24378;&#21019;&#24314;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#31243;&#24207;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#21019;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#23558;&#24443;&#24213;&#25913;&#21464;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces and explores a new programming paradigm, Model-based Programming, designed to address the challenges inherent in applying deep learning models to real-world applications. Despite recent significant successes of deep learning models across a range of tasks, their deployment in real business scenarios remains fraught with difficulties, such as complex model training, large computational resource requirements, and integration issues with existing programming languages. To ameliorate these challenges, we propose the concept of 'Model-based Programming' and present a novel programming language - M Language, tailored to a prospective model-centered programming paradigm. M Language treats models as basic computational units, enabling developers to concentrate more on crucial tasks such as model loading, fine-tuning, evaluation, and deployment, thereby enhancing the efficiency of creating deep learning applications. We posit that this innovative programming paradigm will 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#35268;&#33539;&#21270;&#26041;&#27861;CrossConST&#65292;&#29992;&#20110;&#25913;&#36827;&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CrossConST&#21487;&#20197;&#32553;&#23567;&#35821;&#35328;&#20043;&#38388;&#30340;&#34920;&#31034;&#24046;&#36317;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#32763;&#35793;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07310</link><description>&lt;p&gt;
&#21033;&#29992;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#35268;&#33539;&#21270;&#25913;&#36827;&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Improving Zero-shot Multilingual Neural Machine Translation by Leveraging Cross-lingual Consistency Regularization. (arXiv:2305.07310v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#35268;&#33539;&#21270;&#26041;&#27861;CrossConST&#65292;&#29992;&#20110;&#25913;&#36827;&#38646;&#26679;&#26412;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CrossConST&#21487;&#20197;&#32553;&#23567;&#35821;&#35328;&#20043;&#38388;&#30340;&#34920;&#31034;&#24046;&#36317;&#65292;&#25552;&#39640;&#38646;&#26679;&#26412;&#32763;&#35793;&#30340;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#26377;&#24456;&#24378;&#30340;&#38646;&#26679;&#26412;&#32763;&#35793;&#33021;&#21147;&#65292;&#33021;&#22815;&#22312;&#26410;&#32463;&#36807;&#35757;&#32451;&#30340;&#35821;&#35328;&#23545;&#20043;&#38388;&#36827;&#34892;&#30452;&#25509;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#20174;&#26377;&#30417;&#30563;&#26041;&#21521;&#21040;&#38646;&#26679;&#26412;&#26041;&#21521;&#30340;&#33391;&#22909;&#36716;&#31227;&#24615;&#33021;&#65292;&#38656;&#35201;&#35753;&#22810;&#35821;&#35328;NMT&#27169;&#22411;&#23398;&#20064;&#21040;&#36328;&#19981;&#21516;&#35821;&#35328;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#35268;&#33539;&#21270;CrossConST&#65292;&#20197;&#24357;&#21512;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#34920;&#31034;&#24046;&#36317;&#65292;&#24182;&#25552;&#39640;&#38646;&#26679;&#26412;&#32763;&#35793;&#24615;&#33021;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;CrossConST&#38544;&#21547;&#22320;&#26368;&#22823;&#21270;&#20102;&#38646;&#26679;&#26412;&#32763;&#35793;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#22312;&#20302;&#36164;&#28304;&#21644;&#39640;&#36164;&#28304;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31283;&#23450;&#30340;&#25552;&#21319;&#12290;&#23454;&#39564;&#20998;&#26512;&#36824;&#35777;&#26126;&#65292;CrossConST&#21487;&#20197;&#32553;&#23567;&#21477;&#23376;&#34920;&#31034;&#24046;&#36317;&#24182;&#26356;&#22909;&#22320;&#23545;&#40784;&#34920;&#31034;&#31354;&#38388;&#12290;&#37492;&#20110;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;CrossConST&#21487;&#20197;&#26041;&#20415;&#22320;&#24212;&#29992;&#20110;&#20854;&#20182;&#22810;&#35821;&#35328;NMT&#27169;&#22411;&#20013;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#38646;&#26679;&#26412;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The multilingual neural machine translation (NMT) model has a promising capability of zero-shot translation, where it could directly translate between language pairs unseen during training. For good transfer performance from supervised directions to zero-shot directions, the multilingual NMT model is expected to learn universal representations across different languages. This paper introduces a cross-lingual consistency regularization, CrossConST, to bridge the representation gap among different languages and boost zero-shot translation performance. The theoretical analysis shows that CrossConST implicitly maximizes the probability distribution for zero-shot translation, and the experimental results on both low-resource and high-resource benchmarks show that CrossConST consistently improves the translation performance. The experimental analysis also proves that CrossConST could close the sentence representation gap and better align the representation space. Given the universality and s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.07303</link><description>&lt;p&gt;
&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Multi-Relational Hyperbolic Word Embeddings from Natural Language Definitions. (arXiv:2305.07303v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#22810;&#20851;&#31995;&#21452;&#26354;&#35789;&#21521;&#37327;&#30340;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#30001;&#23450;&#20041;&#25152;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#20998;&#24067;&#20449;&#24687;&#30340;&#31070;&#32463;&#35789;&#21521;&#37327;&#19968;&#30452;&#20197;&#26469;&#37117;&#33021;&#20026;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#26377;&#29992;&#30340;&#21547;&#20041;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#38590;&#20197;&#35299;&#37322;&#21644;&#25511;&#21046;&#30340;&#34920;&#31034;&#12290;&#30456;&#21453;&#65292;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20855;&#26377;&#36882;&#24402;&#30340;&#65292;&#33258;&#35828;&#26126;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#21487;&#20197;&#25903;&#25345;&#33021;&#22815;&#20445;&#30041;&#21521;&#37327;&#31354;&#38388;&#20013;&#26174;&#24335;&#27010;&#24565;&#20851;&#31995;&#21644;&#32422;&#26463;&#30340;&#26032;&#22411;&#34920;&#31034;&#23398;&#20064;&#33539; paradigm&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#12289;&#22810;&#20851;&#31995;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#26144;&#23556;&#23450;&#20041;&#21644;&#23450;&#20041;&#26415;&#35821;&#21450;&#20854;&#30456;&#24212;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#20165;&#20174;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20013;&#23398;&#20064;&#35789;&#21521;&#37327;&#12290;&#36890;&#36807;&#33258;&#21160;&#20174;&#23450;&#20041;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#32763;&#35793;&#30446;&#26631;&#35268;&#33539;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26694;&#26550;&#19987;&#38376;&#35774;&#23450;&#20026;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#25429;&#33719;&#30001;&#23450;&#20041;&#24341;&#36215;&#30340;&#20998;&#23618;&#21644;&#22810;&#20998;&#36776;&#29575;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural-based word embeddings using solely distributional information have consistently produced useful meaning representations for downstream tasks. However, existing approaches often result in representations that are hard to interpret and control. Natural language definitions, on the other side, possess a recursive, self-explanatory semantic structure that can support novel representation learning paradigms able to preserve explicit conceptual relations and constraints in the vector space.  This paper proposes a neuro-symbolic, multi-relational framework to learn word embeddings exclusively from natural language definitions by jointly mapping defined and defining terms along with their corresponding semantic relations. By automatically extracting the relations from definitions corpora and formalising the learning problem via a translational objective, we specialise the framework in hyperbolic space to capture the hierarchical and multi-resolution structure induced by the definitions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31867;&#22686;&#37327;&#35774;&#32622;&#19979;&#30340;&#25345;&#32493;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#34920;&#31034;&#20559;&#35265;&#65292;&#20174;&#20449;&#24687;&#29942;&#39048;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#21033;&#29992;&#26356;&#22810;&#31867;&#30456;&#20851;&#20449;&#24687;&#28040;&#38500;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#21363;RepCL&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#33021;&#26377;&#25928;&#22320;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07289</link><description>&lt;p&gt;
RepCL: &#25506;&#32034;&#26377;&#25928;&#30340;&#34920;&#31034;&#26041;&#27861;&#20197;&#36827;&#34892;&#25345;&#32493;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
RepCL: Exploring Effective Representation for Continual Text Classification. (arXiv:2305.07289v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31867;&#22686;&#37327;&#35774;&#32622;&#19979;&#30340;&#25345;&#32493;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#34920;&#31034;&#20559;&#35265;&#65292;&#20174;&#20449;&#24687;&#29942;&#39048;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#21033;&#29992;&#26356;&#22810;&#31867;&#30456;&#20851;&#20449;&#24687;&#28040;&#38500;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#21363;RepCL&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#33021;&#26377;&#25928;&#22320;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#24182;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#19981;&#26029;&#23398;&#20064;&#26032;&#30693;&#35782;&#65292;&#21516;&#26102;&#36991;&#20813;&#24536;&#35760;&#26087;&#20219;&#21153;&#36896;&#25104;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#26412;&#25991;&#30740;&#31350;&#30340;&#26159;&#31867;&#22686;&#37327;&#35774;&#32622;&#19979;&#30340;&#25345;&#32493;&#25991;&#26412;&#20998;&#31867;&#12290;&#26368;&#36817;&#30340;&#25345;&#32493;&#23398;&#20064;&#30740;&#31350;&#21457;&#29616;&#65292;&#20026;&#19968;&#39033;&#20219;&#21153;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#26041;&#27861;&#21487;&#33021;&#23545;&#20854;&#20182;&#20219;&#21153;&#19981;&#36215;&#20316;&#29992;&#65292;&#21363;&#34920;&#31034;&#20559;&#35265;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#27425;&#20174;&#20449;&#24687;&#29942;&#39048;&#30340;&#35282;&#24230;&#27491;&#24335;&#20998;&#26512;&#20102;&#34920;&#31034;&#20559;&#24046;&#65292;&#24182;&#24314;&#35758;&#21033;&#29992;&#26356;&#22810;&#31867;&#30456;&#20851;&#20449;&#24687;&#26469;&#28040;&#38500;&#20559;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#25918;&#30340;&#25345;&#32493;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;RepCL&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#21644;&#29983;&#25104;&#34920;&#31034;&#23398;&#20064;&#30446;&#26631;&#26469;&#25429;&#33719;&#26356;&#22810;&#30340;&#31867;&#30456;&#20851;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;RepCL&#24341;&#20837;&#20102;&#23545;&#25239;&#24335;&#22238;&#25918;&#31574;&#30053;&#20197;&#28040;&#38500;&#22238;&#25918;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;RepCL&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) aims to constantly learn new knowledge over time while avoiding catastrophic forgetting on old tasks. In this work, we focus on continual text classification under the class-incremental setting. Recent CL studies find that the representations learned in one task may not be effective for other tasks, namely representation bias problem. For the first time we formally analyze representation bias from an information bottleneck perspective and suggest that exploiting representations with more class-relevant information could alleviate the bias. To this end, we propose a novel replay-based continual text classification method, RepCL. Our approach utilizes contrastive and generative representation learning objectives to capture more class-relevant features. In addition, RepCL introduces an adversarial replay strategy to alleviate the overfitting problem of replay. Experiments demonstrate that RepCL effectively alleviates forgetting and achieves state-of-the-art perform
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20107;&#20214;&#27169;&#24335;&#35825;&#23548;&#33539;&#24335;&#65292;&#36890;&#36807;&#20174;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#25910;&#21106;&#30693;&#35782;&#26469;&#33258;&#21160;&#35825;&#23548;&#39640;&#36136;&#37327;&#21644;&#39640;&#35206;&#30422;&#33539;&#22260;&#30340;&#20107;&#20214;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.07280</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#20107;&#20214;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Harvesting Event Schemas from Large Language Models. (arXiv:2305.07280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20107;&#20214;&#27169;&#24335;&#35825;&#23548;&#33539;&#24335;&#65292;&#36890;&#36807;&#20174;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#25910;&#21106;&#30693;&#35782;&#26469;&#33258;&#21160;&#35825;&#23548;&#39640;&#36136;&#37327;&#21644;&#39640;&#35206;&#30422;&#33539;&#22260;&#30340;&#20107;&#20214;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#27169;&#24335;&#25552;&#20379;&#20102;&#19968;&#31181;&#27010;&#24565;&#24615;&#12289;&#32467;&#26500;&#24615;&#21644;&#24418;&#24335;&#21270;&#30340;&#35821;&#35328;&#65292;&#29992;&#20110;&#34920;&#31034;&#20107;&#20214;&#21644;&#23545;&#19990;&#30028;&#20107;&#20214;&#30693;&#35782;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30495;&#23454;&#19990;&#30028;&#20107;&#20214;&#30340;&#24320;&#25918;&#24615;&#12289;&#20107;&#20214;&#34920;&#36798;&#24418;&#24335;&#30340;&#22810;&#26679;&#24615;&#20197;&#21450;&#20107;&#20214;&#30693;&#35782;&#30340;&#31232;&#30095;&#24615;&#65292;&#33258;&#21160;&#35825;&#23548;&#39640;&#36136;&#37327;&#21644;&#39640;&#35206;&#30422;&#33539;&#22260;&#30340;&#20107;&#20214;&#27169;&#24335;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20107;&#20214;&#27169;&#24335;&#35825;&#23548;&#33539;&#24335;&#8212;&#8212;&#20174;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#36890;&#36807;&#20174;PLM&#20013;&#21457;&#29616;&#12289;&#27010;&#24565;&#21270;&#21644;&#32467;&#26500;&#21270;&#20107;&#20214;&#27169;&#24335;&#26469;&#26377;&#25928;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#12290;&#24182;&#35774;&#35745;&#20102;&#20107;&#20214;&#27169;&#24335;&#25910;&#21106;&#26426;&#65288;ESHer&#65289;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#29983;&#25104;&#30340;&#27010;&#24565;&#21270;&#12289;&#32622;&#20449;&#24230;&#24863;&#30693;&#30340;&#26550;&#26500;&#21270;&#20197;&#21450;&#22522;&#20110;&#22270;&#24418;&#30340;&#27169;&#24335;&#32858;&#21512;&#26469;&#33258;&#21160;&#35825;&#23548;&#39640;&#36136;&#37327;&#30340;&#20107;&#20214;&#27169;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ESHer&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#35825;&#23548;&#39640;&#36136;&#37327;&#21644;&#39640;&#35206;&#30422;&#33539;&#22260;&#30340;&#20107;&#20214;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event schema provides a conceptual, structural and formal language to represent events and model the world event knowledge. Unfortunately, it is challenging to automatically induce high-quality and high-coverage event schemas due to the open nature of real-world events, the diversity of event expressions, and the sparsity of event knowledge. In this paper, we propose a new paradigm for event schema induction -- knowledge harvesting from large-scale pre-trained language models, which can effectively resolve the above challenges by discovering, conceptualizing and structuralizing event schemas from PLMs. And an Event Schema Harvester (ESHer) is designed to automatically induce high-quality event schemas via in-context generation-based conceptualization, confidence-aware schema structuralization and graph-based schema aggregation. Empirical results show that ESHer can induce high-quality and high-coverage event schemas on varying domains.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#22871;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;GPRL&#65292;&#20351;&#29992;&#39640;&#26031;&#20808;&#39564;&#35843;&#25972;&#23884;&#22871;&#36793;&#30028;&#26631;&#35760;&#30340;&#36755;&#20986;&#27010;&#29575;&#20998;&#24067;&#65292;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#23454;&#20307;&#19977;&#20803;&#32452;&#65292;&#26080;&#38656;&#32771;&#34385;&#37329;&#26631;&#31614;&#20013;&#30340;&#23454;&#20307;&#39034;&#24207;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20248;&#20110;&#20197;&#21069;&#30340;&#23884;&#22871;NER&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.07266</link><description>&lt;p&gt;
&#39640;&#26031;&#20808;&#39564;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#23884;&#22871;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Gaussian Prior Reinforcement Learning for Nested Named Entity Recognition. (arXiv:2305.07266v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07266
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#22871;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;GPRL&#65292;&#20351;&#29992;&#39640;&#26031;&#20808;&#39564;&#35843;&#25972;&#23884;&#22871;&#36793;&#30028;&#26631;&#35760;&#30340;&#36755;&#20986;&#27010;&#29575;&#20998;&#24067;&#65292;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#23454;&#20307;&#19977;&#20803;&#32452;&#65292;&#26080;&#38656;&#32771;&#34385;&#37329;&#26631;&#31614;&#20013;&#30340;&#23454;&#20307;&#39034;&#24207;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20248;&#20110;&#20197;&#21069;&#30340;&#23884;&#22871;NER&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#32463;&#36807;&#24191;&#27867;&#30740;&#31350;&#30340;&#20219;&#21153;&#65292;&#26368;&#36817;&#65292;&#23884;&#22871;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24341;&#36215;&#20102;&#26356;&#22810;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#30340;&#23454;&#29992;&#24615;&#21644;&#38590;&#24230;&#12290;&#29616;&#26377;&#30340;&#23884;&#22871;NER&#20316;&#21697;&#24573;&#30053;&#20102;&#23884;&#22871;&#23454;&#20307;&#30340;&#35782;&#21035;&#39034;&#24207;&#21644;&#36793;&#30028;&#20301;&#32622;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;seq2seq&#27169;&#22411;GPRL&#65292;&#23558;&#23884;&#22871;NER&#20219;&#21153;&#24418;&#25104;&#19968;&#20010;&#23454;&#20307;&#19977;&#20803;&#32452;&#24207;&#21015;&#29983;&#25104;&#36807;&#31243;&#12290;GPRL&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#23454;&#20307;&#19977;&#20803;&#32452;&#65292;&#23558;&#37329;&#26631;&#31614;&#20013;&#30340;&#23454;&#20307;&#39034;&#24207;&#35299;&#32806;&#65292;&#24182;&#26399;&#26395;&#36890;&#36807;&#35797;&#38169;&#23398;&#20064;&#21512;&#29702;&#30340;&#23454;&#20307;&#35782;&#21035;&#39034;&#24207;&#12290;&#22522;&#20110;&#23884;&#22871;&#23454;&#20307;&#36793;&#30028;&#36317;&#31163;&#30340;&#32479;&#35745;&#65292;GPRL&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#26031;&#20808;&#39564;&#65292;&#20195;&#34920;&#23884;&#22871;&#23454;&#20307;&#20043;&#38388;&#30340;&#36793;&#30028;&#36317;&#31163;&#20998;&#24067;&#65292;&#24182;&#35843;&#25972;&#23884;&#22871;&#36793;&#30028;&#26631;&#35760;&#30340;&#36755;&#20986;&#27010;&#29575;&#20998;&#24067;&#12290;&#22312;&#19977;&#20010;&#23884;&#22871;NER&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;GPRL&#20248;&#20110;&#20197;&#21069;&#30340;&#23884;&#22871;NER&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) is a well and widely studied task in natural language processing. Recently, the nested NER has attracted more attention since its practicality and difficulty. Existing works for nested NER ignore the recognition order and boundary position relation of nested entities. To address these issues, we propose a novel seq2seq model named GPRL, which formulates the nested NER task as an entity triplet sequence generation process. GPRL adopts the reinforcement learning method to generate entity triplets decoupling the entity order in gold labels and expects to learn a reasonable recognition order of entities via trial and error. Based on statistics of boundary distance for nested entities, GPRL designs a Gaussian prior to represent the boundary distance distribution between nested entities and adjust the output probability distribution of nested boundary tokens. Experiments on three nested NER datasets demonstrate that GPRL outperforms previous nested NER models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#24212;&#29992;&#20110;&#35821;&#38899;&#21512;&#25104;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#36798;&#24615;&#24378;&#12289;&#22810;&#35821;&#38899;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;TorToise&#12290;</title><link>http://arxiv.org/abs/2305.07243</link><description>&lt;p&gt;
&#36890;&#36807;&#32553;&#25918;&#23454;&#29616;&#26356;&#22909;&#30340;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Better speech synthesis through scaling. (arXiv:2305.07243v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07243
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#24212;&#29992;&#20110;&#35821;&#38899;&#21512;&#25104;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#36798;&#24615;&#24378;&#12289;&#22810;&#35821;&#38899;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;TorToise&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#22312;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#21644;DDPM&#30340;&#24212;&#29992;&#19979;&#24471;&#21040;&#20102;&#38761;&#21629;&#24615;&#31361;&#30772;&#12290;&#36825;&#20123;&#26041;&#27861;&#23558;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#24314;&#27169;&#20026;&#36880;&#27493;&#27010;&#29575;&#36807;&#31243;&#65292;&#24182;&#21033;&#29992;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#25968;&#25454;&#26469;&#23398;&#20064;&#22270;&#20687;&#20998;&#24067;&#12290;&#26412;&#25991;&#23558;&#36825;&#31181;&#24615;&#33021;&#25552;&#21319;&#26041;&#27861;&#36816;&#29992;&#21040;&#35821;&#38899;&#21512;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TorToise&#30340;&#34920;&#36798;&#24615;&#12289;&#22810;&#35821;&#38899;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31995;&#32479;&#12290;&#25152;&#26377;&#27169;&#22411;&#20195;&#30721;&#21644;&#35757;&#32451;&#26435;&#37325;&#22343;&#24050;&#24320;&#28304;&#65292;&#23384;&#25918;&#20110;https://github.com/neonbjb/tortoise-tts&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the field of image generation has been revolutionized by the application of autoregressive transformers and DDPMs. These approaches model the process of image generation as a step-wise probabilistic processes and leverage large amounts of compute and data to learn the image distribution. This methodology of improving performance need not be confined to images. This paper describes a way to apply advances in the image generative domain to speech synthesis. The result is TorToise -- an expressive, multi-voice text-to-speech system.  All model code and trained weights have been open-sourced at https://github.com/neonbjb/tortoise-tts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#20445;&#38505;&#38382;&#31572;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#20445;&#38505;&#25919;&#31574;&#25163;&#20876;&#20013;&#25552;&#21462;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#22686;&#24378;LLMs&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07230</link><description>&lt;p&gt;
&#24403;&#36229;&#32423;&#35821;&#35328;&#27169;&#22411;&#19981;&#36275;&#20197;&#28385;&#36275;&#19994;&#21153;&#38656;&#27714;&#65306;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
When Giant Language Brains Just Aren't Enough! Domain Pizzazz with Knowledge Sparkle Dust. (arXiv:2305.07230v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#20445;&#38505;&#38382;&#31572;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#20445;&#38505;&#25919;&#31574;&#25163;&#20876;&#20013;&#25552;&#21462;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#26469;&#22686;&#24378;LLMs&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#26174;&#33879;&#22320;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#36827;&#23637;&#65292;GPT&#27169;&#22411;&#22788;&#20110;&#39046;&#20808;&#22320;&#20301;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20196;&#20154;&#24778;&#21497;&#65292;&#20294;&#23558;LLMs&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#19994;&#21153;&#22330;&#26223;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#65292;&#24182;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#20998;&#26512;&#65292;&#26088;&#22312;&#24357;&#21512;&#23558;LLMs&#36866;&#24212;&#20110;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#30340;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36873;&#25321;&#20445;&#38505;&#38382;&#31572;&#65288;QA&#65289;&#20219;&#21153;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#25512;&#29702;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#35813;&#20219;&#21153;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#20381;&#36182;&#20110;&#20174;&#20445;&#38505;&#25919;&#31574;&#25163;&#20876;&#20013;&#25552;&#21462;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#20351;LLMs&#33021;&#22815;&#29702;&#35299;&#20445;&#38505;&#30340;&#26032;&#27010;&#24565;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#12290;&#23454;&#38469;QA&#23545;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;&#25919;&#31574;&#25163;&#20876;&#20013;&#25552;&#21462;&#30340;&#30693;&#35782;&#26174;&#33879;&#25552;&#39640;&#20102;GPT-3.5&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;50.4&#65285;&#12290;&#20998;&#26512;&#36824;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#20844;&#24320;&#35780;&#20272;&#26631;&#20934;&#21487;&#33021;&#19981;&#36275;&#20197;&#35780;&#20272;LLMs&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have significantly advanced the field of natural language processing, with GPT models at the forefront. While their remarkable performance spans a range of tasks, adapting LLMs for real-world business scenarios still poses challenges warranting further investigation. This paper presents an empirical analysis aimed at bridging the gap in adapting LLMs to practical use cases. To do that, we select the question answering (QA) task of insurance as a case study due to its challenge of reasoning. Based on the task we design a new model relied on LLMs which are empowered by domain-specific knowledge extracted from insurance policy rulebooks. The domain-specific knowledge helps LLMs to understand new concepts of insurance for domain adaptation. Preliminary results on real QA pairs show that knowledge enhancement from policy rulebooks significantly improves the reasoning ability of GPT-3.5 of 50.4% in terms of accuracy. The analysis also indicates that existing publ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#27169;&#22411;&#65292;&#33021;&#22815;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#38750;&#23545;&#31216;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;&#22312;&#20004;&#20010;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35782;&#21035;&#24433;&#21709;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#29305;&#24449;&#20132;&#20114;&#24402;&#22240;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07224</link><description>&lt;p&gt;
&#38754;&#21521;&#27169;&#22411;&#39044;&#27979;&#35299;&#37322;&#30340;&#38750;&#23545;&#31216;&#29305;&#24449;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
Asymmetric feature interaction for interpreting model predictions. (arXiv:2305.07224v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#27169;&#22411;&#65292;&#33021;&#22815;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#38750;&#23545;&#31216;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;&#22312;&#20004;&#20010;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35782;&#21035;&#24433;&#21709;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#29305;&#24449;&#20132;&#20114;&#24402;&#22240;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#27169;&#25311;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#26524;&#12290;&#20808;&#21069;&#26377;&#20851;&#29305;&#24449;&#20132;&#20114;&#24402;&#22240;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#31216;&#20132;&#20114;&#30340;&#30740;&#31350;&#19978;&#65292;&#23427;&#21482;&#33021;&#35299;&#37322;&#21333;&#20010;&#35789;&#27719;&#32452;&#21512;&#21518;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#38468;&#21152;&#24433;&#21709;&#65292;&#32780;&#26080;&#27861;&#25429;&#25417;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#30340;&#38750;&#23545;&#31216;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#23545;&#31216;&#29305;&#24449;&#20132;&#20114;&#35299;&#37322;&#27169;&#22411;&#65292;&#26088;&#22312;&#25506;&#32034;&#28145;&#24230;&#31070;&#32463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25512;&#29702;&#20013;&#30340;&#38750;&#23545;&#31216;&#39640;&#38454;&#29305;&#24449;&#20132;&#20114;&#12290;&#36890;&#36807;&#34920;&#31034;&#25105;&#20204;&#30340;&#35299;&#37322;&#20026;&#19968;&#20010;&#26377;&#21521;&#20132;&#20114;&#22270;&#65292;&#25105;&#20204;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#22270;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#33021;&#22815;&#21457;&#29616;&#38750;&#23545;&#31216;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#20004;&#20010;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35782;&#21035;&#24433;&#21709;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#29305;&#24449;&#20132;&#20114;&#24402;&#22240;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In natural language processing (NLP), deep neural networks (DNNs) could model complex interactions between context and have achieved impressive results on a range of NLP tasks. Prior works on feature interaction attribution mainly focus on studying symmetric interaction that only explains the additional influence of a set of words in combination, which fails to capture asymmetric influence that contributes to model prediction. In this work, we propose an asymmetric feature interaction attribution explanation model that aims to explore asymmetric higher-order feature interactions in the inference of deep neural NLP models. By representing our explanation with an directed interaction graph, we experimentally demonstrate interpretability of the graph to discover asymmetric feature interactions. Experimental results on two sentiment classification datasets show the superiority of our model against the state-of-the-art feature interaction attribution methods in identifying influential featu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#21644;&#25512;&#26029;&#26694;&#26550;OneCAD&#65292;&#36890;&#36807;Mask-Image-Modeling(MIM)&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#35299;&#20915;&#20102;&#24403;&#21069;&#26550;&#26500;(&#22914;ViTs&#21644;CNNs)&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31181;&#21487;&#20197;&#36866;&#29992;&#20110;&#25152;&#26377;&#22270;&#20687;&#25968;&#25454;&#38598;&#19988;&#19982;&#31867;&#21035;&#25968;&#26080;&#20851;&#30340;DNN&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.07167</link><description>&lt;p&gt;
OneCAD: &#20351;&#29992;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#21333;&#20998;&#31867;&#22120;&#36866;&#29992;&#20110;&#25152;&#26377;&#22270;&#20687;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OneCAD: One Classifier for All image Datasets using multimodal learning. (arXiv:2305.07167v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#21644;&#25512;&#26029;&#26694;&#26550;OneCAD&#65292;&#36890;&#36807;Mask-Image-Modeling(MIM)&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#35299;&#20915;&#20102;&#24403;&#21069;&#26550;&#26500;(&#22914;ViTs&#21644;CNNs)&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#31181;&#21487;&#20197;&#36866;&#29992;&#20110;&#25152;&#26377;&#22270;&#20687;&#25968;&#25454;&#38598;&#19988;&#19982;&#31867;&#21035;&#25968;&#26080;&#20851;&#30340;DNN&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;(ViTs)&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#12290;&#36825;&#20123;&#27169;&#22411;&#26550;&#26500;&#20381;&#36182;&#20110;&#23427;&#25152;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#20013;&#31867;&#21035;&#25968;&#30340;&#25968;&#37327;&#12290;&#31867;&#21035;&#25968;&#30340;&#20219;&#20309;&#25913;&#21464;&#37117;&#20250;&#23548;&#33268;&#27169;&#22411;&#26550;&#26500;&#30340;&#25913;&#21464;(&#37096;&#20998;&#25110;&#20840;&#37096;)&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#33021;&#21019;&#24314;&#19968;&#20010;&#19982;&#31867;&#21035;&#25968;&#26080;&#20851;&#30340;&#27169;&#22411;&#26550;&#26500;&#65311;&#36825;&#26679;&#21487;&#20197;&#20351;&#27169;&#22411;&#26550;&#26500;&#19982;&#20854;&#25152;&#35757;&#32451;&#30340;&#25968;&#25454;&#38598;&#26080;&#20851;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#24403;&#21069;&#26550;&#26500;(ViTs&#21644;CNNs)&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35757;&#32451;&#21644;&#25512;&#26029;&#26694;&#26550;- OneCAD(&#36866;&#29992;&#20110;&#25152;&#26377;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#21333;&#20998;&#31867;&#22120;)&#65292;&#20197;&#23454;&#29616;&#25509;&#36817;&#19982;&#31867;&#21035;&#25968;&#26080;&#20851;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#22810;&#27169;&#24577;&#23398;&#20064;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#30340;Mask-Image-Modeling(MIM)&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#19982;&#31867;&#21035;&#25968;&#26080;&#20851;&#30340;DNN&#27169;&#22411;&#26550;&#26500;&#30340;&#24037;&#20316;&#12290;&#21021;&#27493;&#32467;&#26524;&#24050;&#22312;&#33258;&#28982;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Transformers (ViTs) and Convolutional neural networks (CNNs) are widely used Deep Neural Networks (DNNs) for classification task. These model architectures are dependent on the number of classes in the dataset it was trained on. Any change in number of classes leads to change (partial or full) in the model's architecture. This work addresses the question: Is it possible to create a number-of-class-agnostic model architecture?. This allows model's architecture to be independent of the dataset it is trained on. This work highlights the issues with the current architectures (ViTs and CNNs). Also, proposes a training and inference framework OneCAD (One Classifier for All image Datasets) to achieve close-to number-of-class-agnostic transformer model. To best of our knowledge this is the first work to use Mask-Image-Modeling (MIM) with multimodal learning for classification task to create a DNN model architecture agnostic to the number of classes. Preliminary results are shown on natu
&lt;/p&gt;</description></item><item><title>&#25506;&#35752;&#20102;&#22235;&#31181;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#24847;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#21253;&#25324;&#39046;&#22495;&#36866;&#24212;&#12289;&#25968;&#25454;&#22686;&#24378;&#12289;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#24847;&#22270;&#20998;&#31867;&#20197;&#21450;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#24615;&#33021;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.07157</link><description>&lt;p&gt;
&#25506;&#32034;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#25216;&#26415;&#29992;&#20110;&#24847;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring Zero and Few-shot Techniques for Intent Classification. (arXiv:2305.07157v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07157
&lt;/p&gt;
&lt;p&gt;
&#25506;&#35752;&#20102;&#22235;&#31181;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#24847;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#21253;&#25324;&#39046;&#22495;&#36866;&#24212;&#12289;&#25968;&#25454;&#22686;&#24378;&#12289;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#24847;&#22270;&#20998;&#31867;&#20197;&#21450;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#37117;&#26159;&#26377;&#25928;&#30340;&#12290;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#24615;&#33021;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#25552;&#20379;&#32773;&#36890;&#24120;&#38656;&#35201;&#25193;&#23637;&#21040;&#25968;&#21315;&#20010;&#24847;&#22270;&#20998;&#31867;&#27169;&#22411;&#65292;&#20854;&#20013;&#26032;&#23458;&#25143;&#32463;&#24120;&#38754;&#20020;&#20919;&#21551;&#21160;&#38382;&#39064;&#12290;&#22312;&#25317;&#26377;&#36825;&#20040;&#22810;&#23458;&#25143;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#65292;&#20250;&#23545;&#23384;&#20648;&#31354;&#38388;&#26045;&#21152;&#38480;&#21046;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#38646;&#26679;&#26412;&#21644;&#23567;&#26679;&#26412;&#24847;&#22270;&#20998;&#31867;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21463;&#21040;&#20302;&#36164;&#28304;&#38480;&#21046;&#30340;&#21046;&#32422;&#65306;1&#65289;&#39046;&#22495;&#36866;&#24212;&#65292;2&#65289;&#25968;&#25454;&#22686;&#24378;&#65292;3&#65289;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38646;&#26679;&#26412;&#24847;&#22270;&#20998;&#31867;&#65292;&#20197;&#21450;4&#65289;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#37117;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#31243;&#24230;&#19981;&#21516;&#12290;&#20351;&#29992;Flan-T5&#65288;Chang et al&#65292;2022&#65289;&#22312;T-few&#37197;&#26041;&#65288;Liu et al&#65292;2022&#65289;&#19978;&#36827;&#34892;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#65292;&#21363;&#20351;&#27599;&#20010;&#24847;&#22270;&#21482;&#26377;&#19968;&#20010;&#26679;&#26412;&#65292;&#24615;&#33021;&#20063;&#26368;&#20339;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;&#24847;&#22270;&#25551;&#36848;&#25552;&#31034;LLM&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational NLU providers often need to scale to thousands of intent-classification models where new customers often face the cold-start problem. Scaling to so many customers puts a constraint on storage space as well. In this paper, we explore four different zero and few-shot intent classification approaches with this low-resource constraint: 1) domain adaptation, 2) data augmentation, 3) zero-shot intent classification using descriptions large language models (LLMs), and 4) parameter-efficient fine-tuning of instruction-finetuned language models. Our results show that all these approaches are effective to different degrees in low-resource settings. Parameter-efficient fine-tuning using T-few recipe (Liu et al., 2022) on Flan-T5 (Chang et al., 2022) yields the best performance even with just one sample per intent. We also show that the zero-shot method of prompting LLMs using intent descriptions
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20135;&#29983;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#30001;&#23545;&#20154;&#31867;&#26159;&#21542;&#26377;&#29992;&#65292;&#21457;&#29616;&#29616;&#26377;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;&#36828;&#20302;&#20110;&#29702;&#24819;&#29366;&#24577;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#20272;&#35745;&#29702;&#30001;&#22312;&#22238;&#31572;&#32473;&#23450;&#38382;&#39064;&#20013;&#30340;&#26377;&#29992;&#24615;&#26469;&#25552;&#39640;&#26426;&#22120;&#29983;&#25104;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.07095</link><description>&lt;p&gt;
&#26426;&#22120;&#29702;&#30001;&#23545;&#20154;&#31867;&#26159;&#21542;&#26377;&#29992;&#65311;&#35780;&#20272;&#21644;&#25552;&#39640;&#33258;&#28982;&#25991;&#26412;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;
&lt;/p&gt;
&lt;p&gt;
Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-Text Rationales. (arXiv:2305.07095v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07095
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20135;&#29983;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#30001;&#23545;&#20154;&#31867;&#26159;&#21542;&#26377;&#29992;&#65292;&#21457;&#29616;&#29616;&#26377;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;&#36828;&#20302;&#20110;&#29702;&#24819;&#29366;&#24577;&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;&#20272;&#35745;&#29702;&#30001;&#22312;&#22238;&#31572;&#32473;&#23450;&#38382;&#39064;&#20013;&#30340;&#26377;&#29992;&#24615;&#26469;&#25552;&#39640;&#26426;&#22120;&#29983;&#25104;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#26174;&#30528;&#20986;&#29616;&#33021;&#21147;&#20013;&#65292;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#26159;&#20854;&#20013;&#20043;&#19968;&#65307;&#36229;&#36807;&#26576;&#20010;&#35268;&#27169;&#21518;&#65292;&#22823;&#22411;LMs&#33021;&#22815;&#29983;&#25104;&#30475;&#20284;&#26377;&#29992;&#30340;&#29702;&#30001;&#65292;&#36827;&#32780;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#23427;&#20204;&#22312;&#39046;&#23548;&#27036;&#19978;&#30340;&#34920;&#29616;&#12290;&#36825;&#31181;&#29616;&#35937;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#26426;&#22120;&#29983;&#25104;&#30340;&#29702;&#30001;&#26159;&#21542;&#20063;&#33021;&#23545;&#20154;&#31867;&#26377;&#29992;&#65292;&#29305;&#21035;&#26159;&#24403;&#26222;&#36890;&#20154;&#23581;&#35797;&#26681;&#25454;&#36825;&#20123;&#26426;&#22120;&#29702;&#30001;&#22238;&#31572;&#38382;&#39064;&#26102;&#65311;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#26377;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;&#36828;&#26410;&#20196;&#20154;&#28385;&#24847;&#65292;&#24182;&#19988;&#26114;&#36149;&#30340;&#20154;&#31867;&#30740;&#31350;&#25165;&#33021;&#20272;&#35745;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#22914;&#29983;&#25104;&#29702;&#30001;LM&#30340;&#20219;&#21153;&#34920;&#29616;&#25110;&#29983;&#25104;&#29702;&#30001;&#19982;&#40644;&#37329;&#29702;&#30001;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#19981;&#33021;&#24456;&#22909;&#22320;&#34920;&#26126;&#23427;&#20204;&#30340;&#20154;&#31867;&#25928;&#29992;&#12290;&#34429;&#28982;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#29702;&#30001;&#30340;&#26576;&#20123;&#23646;&#24615;&#65292;&#22914;&#31616;&#27905;&#24615;&#21644;&#26032;&#39062;&#24615;&#65292;&#19982;&#23427;&#20204;&#30340;&#20154;&#31867;&#25928;&#29992;&#26377;&#20851;&#65292;&#20294;&#22312;&#27809;&#26377;&#20154;&#31867;&#21442;&#19982;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#23427;&#20204;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20272;&#35745;&#29702;&#30001;&#22312;&#22238;&#31572;&#32473;&#23450;&#38382;&#39064;&#20013;&#30340;&#26377;&#29992;&#24615;&#26469;&#25552;&#39640;&#26426;&#22120;&#29983;&#25104;&#29702;&#30001;&#30340;&#20154;&#31867;&#25928;&#29992;&#65292;&#20174;&#32780;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond a certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a question: can machine generated rationales also be useful for humans, especially when lay humans try to answer questions based on those machine rationales? We observe that human utility of existing rationales is far from satisfactory, and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales, or similarity between generated and gold rationales are not good indicators of their human utility. While we observe that certain properties of rationales like conciseness and novelty are correlated with their human utility, estimating them without human involvement is challenging. We show that, by estimating a rationale's helpfulness in ans
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32784;&#22122;&#22768;&#30340;&#23545;&#27604;&#26694;&#26550; NaCl &#26469;&#22312;&#22024;&#26434;&#26631;&#31614;&#19979;&#23398;&#20064;&#36880;&#27493;&#21463;&#25439;&#30340;&#20851;&#31995;&#65292;&#30456;&#27604;&#20110;&#30452;&#25509;&#20002;&#24323;&#25110;&#37325;&#26032;&#26631;&#35760;&#22122;&#22768;&#65292;&#36890;&#36807;&#25915;&#20987;&#29305;&#24449;&#31354;&#38388;&#20351;&#20854;&#36866;&#24212;&#22024;&#26434;&#26631;&#31614;&#26159;&#26356;&#22909;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.07085</link><description>&lt;p&gt;
&#24341;&#23548;&#22122;&#22768;&#25915;&#20987;&#22686;&#24378;&#23545;&#27604;&#23398;&#20064;&#65306;&#26397;&#21521;&#37326;&#22806;&#25345;&#32493;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Enhancing Contrastive Learning with Noise-Guided Attack: Towards Continual Relation Extraction in the Wild. (arXiv:2305.07085v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32784;&#22122;&#22768;&#30340;&#23545;&#27604;&#26694;&#26550; NaCl &#26469;&#22312;&#22024;&#26434;&#26631;&#31614;&#19979;&#23398;&#20064;&#36880;&#27493;&#21463;&#25439;&#30340;&#20851;&#31995;&#65292;&#30456;&#27604;&#20110;&#30452;&#25509;&#20002;&#24323;&#25110;&#37325;&#26032;&#26631;&#35760;&#22122;&#22768;&#65292;&#36890;&#36807;&#25915;&#20987;&#29305;&#24449;&#31354;&#38388;&#20351;&#20854;&#36866;&#24212;&#22024;&#26434;&#26631;&#31614;&#26159;&#26356;&#22909;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#19981;&#26029;&#20851;&#31995;&#25552;&#21462;&#8221;&#30340;&#21407;&#21017;&#26159;&#36866;&#24212;&#26032;&#20852;&#30340;&#20851;&#31995;&#32780;&#20445;&#30041;&#26087;&#26377;&#30340;&#30693;&#35782;&#12290;&#24403;&#21069;&#30340;&#25345;&#32493;&#20851;&#31995;&#25552;&#21462;&#24037;&#20316;&#25104;&#21151;&#22320;&#20445;&#30041;&#20102;&#26087;&#26377;&#30340;&#30693;&#35782;&#65292;&#20294;&#22312;&#38754;&#23545;&#20986;&#29616;&#27745;&#26579;&#25968;&#25454;&#27969;&#26102;&#24448;&#24448;&#20250;&#22833;&#36133;&#12290;&#26412;&#25991;&#35748;&#20026;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#20381;&#36182;&#19968;&#31181;&#20154;&#24037;&#20551;&#35774;&#65292;&#21363;&#25968;&#25454;&#27969;&#27809;&#26377;&#27880;&#37322;&#38169;&#35823;&#65292;&#36825;&#38480;&#21046;&#20102;&#25345;&#32493;&#20851;&#31995;&#25552;&#21462;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21457;&#23637;&#12290;&#32771;&#34385;&#21040;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#22024;&#26434;&#30340;&#26631;&#31614;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#23454;&#29992;&#30340;&#23398;&#20064;&#22330;&#26223;&#65292;&#31216;&#20026;&#8220;&#22122;&#22768;&#25345;&#32493;&#20851;&#31995;&#25552;&#21462;&#8221;&#12290;&#22312;&#36825;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32784;&#22122;&#22768;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#21517;&#20026; &#8220;NACl&#8221;&#65292;&#29992;&#20110;&#23398;&#20064;&#36880;&#27493;&#21463;&#25439;&#30340;&#20851;&#31995;&#12290;&#19982;&#30452;&#25509;&#20002;&#24323;&#22122;&#22768;&#25110;&#26080;&#27861;&#35775;&#38382;&#30340;&#22122;&#22768;&#37325;&#26032;&#26631;&#35760;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#25915;&#20987;&#26469;&#20462;&#25913;&#29305;&#24449;&#31354;&#38388;&#20197;&#21305;&#37197;&#32473;&#23450;&#30340;&#22024;&#26434;&#26631;&#31614;&#26159;&#26356;&#22909;&#30340;&#20016;&#23500;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The principle of continual relation extraction~(CRE) involves adapting to emerging novel relations while preserving od knowledge. While current endeavors in CRE succeed in preserving old knowledge, they tend to fail when exposed to contaminated data streams. We assume this is attributed to their reliance on an artificial hypothesis that the data stream has no annotation errors, which hinders real-world applications for CRE. Considering the ubiquity of noisy labels in real-world datasets, in this paper, we formalize a more practical learning scenario, termed as \textit{noisy-CRE}. Building upon this challenging setting, we develop a noise-resistant contrastive framework named as \textbf{N}oise-guided \textbf{a}ttack in \textbf{C}ontrative \textbf{L}earning~(NaCL) to learn incremental corrupted relations. Compared to direct noise discarding or inaccessible noise relabeling, we present modifying the feature space to match the given noisy labels via attacking can better enrich contrastive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;CTC&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#65292;&#26469;&#35782;&#21035;&#21476;&#20848;&#32463;&#30340;&#26391;&#35829;&#12290;&#37319;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.07034</link><description>&lt;p&gt;
&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#30340;&#21476;&#20848;&#32463;&#26391;&#35829;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Quran Recitation Recognition using End-to-End Deep Learning. (arXiv:2305.07034v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;CTC&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#65292;&#26469;&#35782;&#21035;&#21476;&#20848;&#32463;&#30340;&#26391;&#35829;&#12290;&#37319;&#29992;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21476;&#20848;&#32463;&#26159;&#20234;&#26031;&#20848;&#25945;&#30340;&#22307;&#20070;&#65292;&#20854;&#26391;&#35829;&#26159;&#35813;&#23447;&#25945;&#20449;&#20208;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#12290;&#30001;&#20110;&#21476;&#20848;&#32463;&#30340;&#29420;&#29305;&#35268;&#21017;&#19981;&#36866;&#29992;&#20110;&#27491;&#24120;&#30340;&#28436;&#35762;&#65292;&#25152;&#20197;&#33258;&#21160;&#35782;&#21035;&#21476;&#20848;&#32463;&#30340;&#26391;&#35829;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#27492;&#20043;&#21069;&#65292;&#24050;&#36827;&#34892;&#20102;&#35768;&#22810;&#30740;&#31350;&#65292;&#20294;&#20197;&#24448;&#30340;&#30740;&#31350;&#23558;&#26391;&#35829;&#38169;&#35823;&#26816;&#27979;&#35270;&#20026;&#20998;&#31867;&#20219;&#21153;&#25110;&#20351;&#29992;&#20256;&#32479;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#21476;&#20848;&#32463;&#30340;&#26391;&#35829;&#12290;&#35813;&#27169;&#22411;&#26159;&#19968;&#20010;CNN-Bidirectional GRU&#32534;&#30721;&#22120;&#65292;&#20351;&#29992;CTC&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#21644;&#22522;&#20110;&#23383;&#31526;&#30340;&#35299;&#30721;&#22120;&#65292;&#21363;&#27874;&#26463;&#25628;&#32034;&#35299;&#30721;&#22120;&#12290;&#27492;&#22806;&#65292;&#25152;&#26377;&#20197;&#24448;&#30340;&#30740;&#31350;&#37117;&#26159;&#22312;&#30001;&#30701;&#33410;&#21644;&#20960;&#31456;&#21476;&#20848;&#32463;&#32452;&#25104;&#30340;&#23567;&#22411;&#31169;&#20154;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#12290;&#30001;&#20110;&#20351;&#29992;&#31169;&#20154;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#27809;&#26377;&#36827;&#34892;&#20219;&#20309;&#27604;&#36739;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#36817;&#21457;&#24067;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;Ar-DAD&#65289;&#20316;&#20026;&#23454;&#39564;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Quran is the holy scripture of Islam, and its recitation is an important aspect of the religion. Recognizing the recitation of the Holy Quran automatically is a challenging task due to its unique rules that are not applied in normal speaking speeches. A lot of research has been done in this domain, but previous works have detected recitation errors as a classification task or used traditional automatic speech recognition (ASR). In this paper, we proposed a novel end-to-end deep learning model for recognizing the recitation of the Holy Quran. The proposed model is a CNN-Bidirectional GRU encoder that uses CTC as an objective function, and a character-based decoder which is a beam search decoder. Moreover, all previous works were done on small private datasets consisting of short verses and a few chapters of the Holy Quran. As a result of using private datasets, no comparisons were done. To overcome this issue, we used a public dataset that has recently been published (Ar-DAD) and co
&lt;/p&gt;</description></item><item><title>Musketeer&#26159;&#19968;&#31181;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#20219;&#21153;&#35299;&#37322;&#25552;&#31034;&#65288;TEP&#65289;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#25972;&#21512;&#24322;&#26500;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#22343;&#21248;</title><link>http://arxiv.org/abs/2305.07019</link><description>&lt;p&gt;
Musketeer&#65288;&#19968;&#20154;&#20043;&#21147;&#65292;&#19975;&#20154;&#20043;&#21147;&#65289;&#65306;&#20855;&#26377;&#20219;&#21153;&#35299;&#37322;&#25552;&#31034;&#30340;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts. (arXiv:2305.07019v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07019
&lt;/p&gt;
&lt;p&gt;
Musketeer&#26159;&#19968;&#31181;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#20219;&#21153;&#35299;&#37322;&#25552;&#31034;&#65288;TEP&#65289;&#26426;&#21046;&#65292;&#33021;&#22815;&#26377;&#25928;&#25972;&#21512;&#24322;&#26500;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#22343;&#21248;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#21442;&#25968;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#65288;&#19975;&#20154;&#20043;&#21147;&#65289;&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#20043;&#38388;&#23436;&#20840;&#20849;&#20139;&#65288;&#19968;&#20154;&#20043;&#21147;&#65289;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#21517;&#20026;Musketeer&#30340;&#21333;&#19968;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a sequence-to-sequence vision-language model whose parameters are jointly trained on all tasks (all for one) and fully shared among multiple tasks (one for all), resulting in a single model which we named Musketeer. The integration of knowledge across heterogeneous tasks is enabled by a novel feature called Task Explanation Prompt (TEP). TEP reduces interference among tasks, allowing the model to focus on their shared structure. With a single model, Musketeer achieves results comparable to or better than strong baselines trained on single tasks, almost uniformly across multiple tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;LSF-Models&#65289;&#22914;ChatGPT&#21644;DALLE-E&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#65288;PHM&#65289;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22823;&#37327;&#25968;&#25454;&#21644;&#36229;&#22823;&#27169;&#22411;&#33539;&#24335;&#65292;&#25104;&#20026;AI-2.0&#30340;&#26032;&#26102;&#20195;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;</title><link>http://arxiv.org/abs/2305.06472</link><description>&lt;p&gt;
ChatGPT&#24335;&#30340;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;&#19982;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps. (arXiv:2305.06472v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06472
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;LSF-Models&#65289;&#22914;ChatGPT&#21644;DALLE-E&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#22312;&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#65288;PHM&#65289;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22823;&#37327;&#25968;&#25454;&#21644;&#36229;&#22823;&#27169;&#22411;&#33539;&#24335;&#65292;&#25104;&#20026;AI-2.0&#30340;&#26032;&#26102;&#20195;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#19982;&#20581;&#24247;&#31649;&#29702;&#25216;&#26415;&#22312;&#24037;&#19994;&#29983;&#20135;&#21644;&#35774;&#22791;&#32500;&#25252;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#36890;&#36807;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;PHM&#25216;&#26415;&#35782;&#21035;&#21644;&#39044;&#27979;&#35774;&#22791;&#25925;&#38556;&#21644;&#25439;&#22351;&#12290;&#29616;&#22312;&#65292;&#22522;&#20110;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#65288;LSF-Models&#65289;&#22914;ChatGPT&#21644;DALLE-E&#30340;AI&#25216;&#26415;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#12289;&#22810;&#20219;&#21153;&#12289;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#36229;&#22823;&#27169;&#22411;&#33539;&#24335;&#65292;&#25104;&#20026;AI-2.0&#30340;&#26032;&#26102;&#20195;&#30340;&#26631;&#24535;&#20043;&#19968;&#12290;&#36825;&#31181;&#25216;&#26415;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#24037;&#19994;&#39046;&#22495;&#65292;&#22914;&#38081;&#36335;&#12289;&#33021;&#28304;&#21644;&#33322;&#31354;&#31561;&#65292;&#20197;&#25552;&#39640;&#35774;&#22791;&#30340;&#26381;&#21153;&#23551;&#21629;&#21644;&#21487;&#38752;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#29983;&#20135;&#25104;&#26412;&#21644;&#20572;&#26426;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prognostics and health management (PHM) technology plays a critical role in industrial production and equipment maintenance by identifying and predicting possible equipment failures and damages, thereby allowing necessary maintenance measures to be taken to enhance equipment service life and reliability while reducing production costs and downtime. In recent years, PHM technology based on artificial intelligence (AI) has made remarkable achievements in the context of the industrial IoT and big data, and it is widely used in various industries, such as railway, energy, and aviation, for condition monitoring, fault prediction, and health management. The emergence of large-scale foundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of AI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved from a research paradigm of single-modal, single-task, and limited-data to a multi-modal, multi-task, massive data, and super-large model paradigm. ChatGPT r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SUR-adapter&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#39044;&#20808;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#20415;&#22312;&#29983;&#25104;&#22270;&#29255;&#26102;&#20351;&#29992;&#31616;&#30701;&#30340;&#21465;&#36848;&#25552;&#31034;&#12290;&#20316;&#32773;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SURD&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.05189</link><description>&lt;p&gt;
SUR-adapter&#65306;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#25991;&#26412;-&#22270;&#20687;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models. (arXiv:2305.05189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SUR-adapter&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#39044;&#20808;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#20415;&#22312;&#29983;&#25104;&#22270;&#29255;&#26102;&#20351;&#29992;&#31616;&#30701;&#30340;&#21465;&#36848;&#25552;&#31034;&#12290;&#20316;&#32773;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SURD&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#30446;&#21069;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#21644;&#20869;&#23481;&#20016;&#23500;&#24230;&#30340;&#22270;&#20687;&#12290;&#20294;&#26159;&#65292;&#24403;&#36755;&#20837;&#30340;&#25552;&#31034;&#20026;&#31616;&#30701;&#30340;&#21465;&#36848;&#26102;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#35821;&#20041;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#65292;&#23548;&#33268;&#22270;&#20687;&#29983;&#25104;&#30340;&#36136;&#37327;&#36739;&#20302;&#12290;&#20026;&#20102;&#25552;&#39640;&#21465;&#36848;&#25552;&#31034;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;Semantic Understanding&#21644;Reasoning adapter&#65288;SUR-adapter&#65289;&#65292;&#29992;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#21644;&#27880;&#37322;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SURD&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;57,000&#20010;&#35821;&#20041;&#20462;&#27491;&#30340;&#22810;&#27169;&#24577;&#26679;&#26412;&#12290;&#27599;&#20010;&#26679;&#26412;&#37117;&#21253;&#21547;&#19968;&#20010;&#31616;&#21333;&#30340;&#21465;&#36848;&#25552;&#31034;&#65292;&#19968;&#20010;&#22797;&#26434;&#30340;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;&#25552;&#31034;&#21644;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#21465;&#36848;&#25552;&#31034;&#30340;&#35821;&#20041;&#34920;&#31034;&#19982;&#22797;&#26434;&#25552;&#31034;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#23558;&#20854;&#36716;&#31227;&#33267;&#25105;&#20204;&#30340;SUR-adapter&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models, which have emerged to become popular text-to-image generation models, can produce high-quality and content-rich images guided by textual prompts. However, there are limitations to semantic understanding and commonsense reasoning in existing models when the input prompts are concise narrative, resulting in low-quality image generation. To improve the capacities for narrative prompts, we propose a simple-yet-effective parameter-efficient fine-tuning approach called the Semantic Understanding and Reasoning adapter (SUR-adapter) for pre-trained diffusion models. To reach this goal, we first collect and annotate a new dataset SURD which consists of more than 57,000 semantically corrected multi-modal samples. Each sample contains a simple narrative prompt, a complex keyword-based prompt, and a high-quality image. Then, we align the semantic representation of narrative prompts to the complex prompts and transfer knowledge of large language models (LLMs) to our SUR-adapter vi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#35843;&#33410;&#38376;&#25511;Transformer&#26550;&#26500;&#65292;&#36890;&#36807;&#20056;&#27861;&#25928;&#24212;&#23454;&#29616;&#20102;&#31070;&#32463;&#35843;&#33410;&#65292;&#22312;SuperGLUE&#22522;&#20934;&#39564;&#35777;&#38598;&#19978;&#34920;&#29616;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.03232</link><description>&lt;p&gt;
&#31070;&#32463;&#35843;&#33410;&#38376;&#25511;Transformer
&lt;/p&gt;
&lt;p&gt;
Neuromodulation Gated Transformer. (arXiv:2305.03232v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#35843;&#33410;&#38376;&#25511;Transformer&#26550;&#26500;&#65292;&#36890;&#36807;&#20056;&#27861;&#25928;&#24212;&#23454;&#29616;&#20102;&#31070;&#32463;&#35843;&#33410;&#65292;&#22312;SuperGLUE&#22522;&#20934;&#39564;&#35777;&#38598;&#19978;&#34920;&#29616;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#21363;&#31070;&#32463;&#35843;&#33410;&#38376;&#25511;Transformer&#65288;NGT&#65289;&#65292;&#23427;&#36890;&#36807;&#19968;&#31181;&#20056;&#27861;&#25928;&#24212;&#65292;&#23454;&#29616;&#20102;Transformer&#20013;&#30340;&#31070;&#32463;&#35843;&#33410;&#30340;&#31616;&#21333;&#23454;&#29616;&#12290;&#25105;&#20204;&#23558;&#20854;&#19982;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20854;&#22312;SuperGLUE&#22522;&#20934;&#39564;&#35777;&#38598;&#19978;&#36798;&#21040;&#26368;&#20339;&#24179;&#22343;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel architecture, the Neuromodulation Gated Transformer (NGT), which is a simple implementation of neuromodulation in transformers via a multiplicative effect. We compare it to baselines and show that it results in the best average performance on the SuperGLUE benchmark validation sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#21477;&#23376;&#32423;&#21035;&#30340;&#26102;&#38388;&#12289;&#22240;&#26524;&#21644;&#35821;&#31687;&#20851;&#31995;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#20854;&#22312;&#26816;&#27979;&#21644;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35782;&#21035;&#26102;&#38388;&#39034;&#24207;&#26041;&#38754;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14827</link><description>&lt;p&gt;
ChatGPT&#22312;&#21477;&#23376;&#32423;&#20851;&#31995;&#19978;&#30340;&#35780;&#20272;&#65306;&#37325;&#28857;&#20851;&#27880;&#26102;&#38388;&#12289;&#22240;&#26524;&#21644;&#35821;&#31687;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations. (arXiv:2304.14827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#21477;&#23376;&#32423;&#21035;&#30340;&#26102;&#38388;&#12289;&#22240;&#26524;&#21644;&#35821;&#31687;&#20851;&#31995;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#20854;&#22312;&#26816;&#27979;&#21644;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35782;&#21035;&#26102;&#38388;&#39034;&#24207;&#26041;&#38754;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23450;&#37327;&#35780;&#20272;ChatGPT&#65292;&#22312;&#26102;&#38388;&#20851;&#31995;&#12289;&#22240;&#26524;&#20851;&#31995;&#21644;&#35821;&#31687;&#20851;&#31995;&#31561;&#21477;&#38388;&#20851;&#31995;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#21040;ChatGPT&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#25105;&#20204;&#22312;13&#20010;&#25968;&#25454;&#38598;&#30340;&#25972;&#20010;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#26102;&#38388;&#21644;&#22240;&#26524;&#20851;&#31995;&#12289;&#22522;&#20110;PDTB2.0&#21644;&#22522;&#20110;&#23545;&#35805;&#30340;&#35821;&#31687;&#20851;&#31995;&#65292;&#20197;&#21450;&#20851;&#20110;&#23545;&#35805;&#29702;&#35299;&#30340;&#19979;&#28216;&#24212;&#29992;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#31181;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#30340;&#23450;&#21046;&#25552;&#31034;&#27169;&#26495;&#65292;&#21253;&#25324;&#38646;-shot&#25552;&#31034;&#27169;&#26495;&#12289;&#38646;-shot&#25552;&#31034;&#24037;&#31243;&#65288;PE&#65289;&#27169;&#26495;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#25552;&#31034;&#27169;&#26495;&#65292;&#20026;&#25152;&#26377;&#27969;&#34892;&#30340;&#21477;&#23545;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#24314;&#31435;&#20102;&#21021;&#22987;&#22522;&#20934;&#20998;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#22312;&#26816;&#27979;&#21644;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#21487;&#33021;&#19981;&#25797;&#38271;&#35782;&#21035;&#21477;&#23376;&#38388;&#30340;&#26102;&#38388;&#39034;&#24207;&#12290;ICL&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#25552;&#39640;&#19968;&#20123;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#27169;&#22411;&#25913;&#36827;&#21644;&#26377;&#25928;&#25552;&#31034;&#27169;&#26495;&#30340;&#35774;&#35745;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to quantitatively evaluate the performance of ChatGPT, an interactive large language model, on inter-sentential relations such as temporal relations, causal relations, and discourse relations. Given ChatGPT's promising performance across various tasks, we conduct extensive evaluations on the whole test sets of 13 datasets, including temporal and causal relations, PDTB2.0-based and dialogue-based discourse relations, and downstream applications on discourse understanding. To achieve reliable results, we adopt three tailored prompt templates for each task, including the zero-shot prompt template, zero-shot prompt engineering (PE) template, and in-context learning (ICL) prompt template, to establish the initial baseline scores for all popular sentence-pair relation classification tasks for the first time. We find that ChatGPT exhibits strong performance in detecting and reasoning about causal relations, while it may not be proficient in identifying the temporal order betwe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GPT-NER&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65288;NER&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#23558;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#36716;&#21270;&#20026;&#29983;&#25104;&#20219;&#21153;&#65292;&#23558;LLM&#33021;&#22815;&#23481;&#26131;&#22320;&#36866;&#24212;NER&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;LLMs&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#39564;&#35777;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#35810;&#38382;&#33258;&#36523;&#26469;&#30830;&#23450;&#25552;&#21462;&#30340;&#23454;&#20307;&#26159;&#21542;&#23646;&#20110;&#23454;&#38469;&#23384;&#22312;&#30340;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2304.10428</link><description>&lt;p&gt;
GPT-NER&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
GPT-NER: Named Entity Recognition via Large Language Models. (arXiv:2304.10428v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GPT-NER&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65288;NER&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#23558;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#36716;&#21270;&#20026;&#29983;&#25104;&#20219;&#21153;&#65292;&#23558;LLM&#33021;&#22815;&#23481;&#26131;&#22320;&#36866;&#24212;NER&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;LLMs&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#20316;&#32773;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#39564;&#35777;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#35810;&#38382;&#33258;&#36523;&#26469;&#30830;&#23450;&#25552;&#21462;&#30340;&#23454;&#20307;&#26159;&#21542;&#23646;&#20110;&#23454;&#38469;&#23384;&#22312;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#24050;&#32463;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;NER&#24615;&#33021;&#20173;&#28982;&#26126;&#26174;&#20302;&#20110;&#30417;&#30563;&#22522;&#32447;&#12290;&#36825;&#26159;&#30001;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;LLMs&#20043;&#38388;&#30340;&#24046;&#36317;&#65306;&#21069;&#32773;&#22312;&#26412;&#36136;&#19978;&#26159;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#65292;&#21518;&#32773;&#26159;&#19968;&#31181;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPT-NER&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290; GPT-NER&#36890;&#36807;&#23558;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#36716;&#25442;&#20026;&#29983;&#25104;&#20219;&#21153;&#26469;&#24357;&#21512;&#24046;&#36317;&#65292;LLMs&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#12290;&#20363;&#22914;&#65292;&#23558;&#22312;&#36755;&#20837;&#25991;&#26412;&#8220;&#21733;&#20262;&#24067;&#26159;&#19968;&#24231;&#22478;&#24066;&#8221;&#20013;&#26597;&#25214;&#20301;&#32622;&#23454;&#20307;&#30340;&#20219;&#21153;&#36716;&#25442;&#20026;&#29983;&#25104;&#25991;&#26412;&#24207;&#21015;&#8220;@@&#21733;&#20262;&#24067;##&#26159;&#19968;&#24231;&#22478;&#24066;&#8221;&#65292;&#20854;&#20013;&#29305;&#27530;&#26631;&#35760;@@##&#26631;&#35760;&#35201;&#25552;&#21462;&#30340;&#23454;&#20307;&#12290;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;LLMs&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#21363;LLMs&#26377;&#24456;&#24378;&#30340;&#20542;&#21521;&#23558;&#31354;&#36755;&#20837;&#36807;&#24230;&#33258;&#20449;&#22320;&#26631;&#35760;&#20026;&#23454;&#20307;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#25105;&#39564;&#35777;&#31574;&#30053;&#65292;&#36890;&#36807;&#25552;&#31034;LLMs&#35810;&#38382;&#33258;&#36523;&#26469;&#30830;&#23450;&#25552;&#21462;&#30340;&#23454;&#20307;&#26159;&#21542;&#23646;&#20110;&#23454;&#38469;&#23384;&#22312;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model.  In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text "Columbus is a city" is transformed to generate the text sequence "@@Columbus## is a city", where special tokens @@## marks the entity to extract. To efficiently address the "hallucination" issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a lab
&lt;/p&gt;</description></item><item><title>PLUE&#26159;&#19968;&#20010;&#22810;&#20219;&#21153;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#33521;&#25991;&#38544;&#31169;&#25919;&#31574;&#35821;&#35328;&#29702;&#35299;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#30740;&#31350;&#32773;&#36824;&#25910;&#38598;&#20102;&#22823;&#37327;&#30340;&#38544;&#31169;&#25919;&#31574;&#35821;&#26009;&#24211;&#65292;&#20197;&#25903;&#25345;&#38544;&#31169;&#25919;&#31574;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10011</link><description>&lt;p&gt;
PLUE&#65306;&#33521;&#25991;&#38544;&#31169;&#25919;&#31574;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English. (arXiv:2212.10011v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10011
&lt;/p&gt;
&lt;p&gt;
PLUE&#26159;&#19968;&#20010;&#22810;&#20219;&#21153;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#33521;&#25991;&#38544;&#31169;&#25919;&#31574;&#35821;&#35328;&#29702;&#35299;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#30740;&#31350;&#32773;&#36824;&#25910;&#38598;&#20102;&#22823;&#37327;&#30340;&#38544;&#31169;&#25919;&#31574;&#35821;&#26009;&#24211;&#65292;&#20197;&#25903;&#25345;&#38544;&#31169;&#25919;&#31574;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#25919;&#31574;&#21521;&#20010;&#20154;&#25552;&#20379;&#20854;&#26435;&#21033;&#20197;&#21450;&#22914;&#20309;&#22788;&#29702;&#19982;&#20043;&#30456;&#20851;&#30340;&#20010;&#20154;&#20449;&#24687;&#30340;&#20449;&#24687;&#12290;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;(NLU)&#25216;&#26415;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#29702;&#35299;&#38271;&#19988;&#22797;&#26434;&#30340;&#25991;&#26723;&#20013;&#25551;&#36848;&#30340;&#38544;&#31169;&#23454;&#36341;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NLU&#25216;&#26415;&#22312;&#22788;&#29702;&#35821;&#35328;&#26102;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#65292;&#20165;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#38544;&#31169;&#23454;&#36341;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38544;&#31169;&#25919;&#31574;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;(PLUE)&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#20219;&#21153;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#38544;&#31169;&#25919;&#31574;&#35821;&#35328;&#29702;&#35299;&#12290;&#25105;&#20204;&#36824;&#25910;&#38598;&#20102;&#22823;&#37327;&#30340;&#38544;&#31169;&#25919;&#31574;&#35821;&#26009;&#24211;&#65292;&#20197;&#25903;&#25345;&#38544;&#31169;&#25919;&#31574;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#25910;&#38598;&#30340;&#35821;&#26009;&#24211;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy policies provide individuals with information about their rights and how their personal information is handled. Natural language understanding (NLU) technologies can support individuals and practitioners to understand better privacy practices described in lengthy and complex documents. However, existing efforts that use NLU technologies are limited by processing the language in a way exclusive to a single task focusing on certain privacy practices. To this end, we introduce the Privacy Policy Language Understanding Evaluation (PLUE) benchmark, a multi-task benchmark for evaluating the privacy policy language understanding across various tasks. We also collect a large corpus of privacy policies to enable privacy policy domain-specific language model pre-training. We evaluate several generic pre-trained language models and continue pre-training them on the collected corpus. We demonstrate that domain-specific continual pre-training offers performance improvements across all tasks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RHO ($\rho$)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#38142;&#25509;&#23454;&#20307;&#21644;&#20851;&#31995;&#35859;&#35789;&#30340;&#34920;&#31034;&#26469;&#20943;&#23569;&#23545;&#35805;&#31995;&#32479;&#20013;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#25552;&#39640;&#20102;&#23545;&#35805;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#23376;&#22270;&#28459;&#27493;&#30340;&#22238;&#22797;&#37325;&#26032;&#25490;&#24207;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2212.01588</link><description>&lt;p&gt;
&#12298;RHO ($\rho$)&#65306;&#21033;&#29992;&#30693;&#35782;&#38142;&#25509;&#20943;&#23569;&#24320;&#25918;&#22495;&#23545;&#35805;&#20013;&#30340;&#24187;&#35273;&#12299;
&lt;/p&gt;
&lt;p&gt;
RHO ($\rho$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding. (arXiv:2212.01588v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RHO ($\rho$)&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#38142;&#25509;&#23454;&#20307;&#21644;&#20851;&#31995;&#35859;&#35789;&#30340;&#34920;&#31034;&#26469;&#20943;&#23569;&#23545;&#35805;&#31995;&#32479;&#20013;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#25552;&#39640;&#20102;&#23545;&#35805;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#23376;&#22270;&#28459;&#27493;&#30340;&#22238;&#22797;&#37325;&#26032;&#25490;&#24207;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#21487;&#20197;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#24211;&#29983;&#25104;&#27969;&#30021;&#19988;&#20449;&#24687;&#20016;&#23500;&#30340;&#22238;&#22797;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#24335;&#30340;&#22238;&#22797;&#65292;&#24182;&#20005;&#37325;&#24433;&#21709;&#24212;&#29992;&#12290;&#22806;&#37096;&#30693;&#35782;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#24322;&#26500;&#24615;&#25361;&#25112;&#20102;&#34920;&#24449;&#23398;&#20064;&#21644;&#28304;&#38598;&#25104;&#65292;&#36827;&#19968;&#27493;&#23548;&#33268;&#19981;&#24544;&#23454;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#24182;&#29983;&#25104;&#26356;&#24544;&#23454;&#30340;&#22238;&#22797;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#38142;&#25509;&#23454;&#20307;&#21644;&#20851;&#31995;&#35859;&#35789;&#30340;&#34920;&#31034;&#26469;&#20943;&#23569;&#24187;&#35273;&#30340;&#26041;&#27861;&#65292;&#21363;RHO ($\rho$)&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;(1)&#26412;&#22320;&#30693;&#35782;&#22522;&#30784;&#65292;&#23558;&#25991;&#26412;&#23884;&#20837;&#19982;&#23545;&#24212;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30456;&#32467;&#21512;&#65307;&#20197;&#21450;(2)&#20840;&#23616;&#30693;&#35782;&#22522;&#30784;&#65292;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#20351;RHO&#20855;&#26377;&#22810;&#27425;&#36339;&#25512;&#29702;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#23376;&#22270;&#28459;&#27493;&#30340;&#22238;&#22797;&#37325;&#26032;&#25490;&#24207;&#25216;&#26415;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#23545;&#35805;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between external knowledge and dialogue context challenges representation learning and source integration, and further contributes to unfaithfulness. To handle this challenge and generate more faithful responses, this paper presents RHO ($\rho$) utilizing the representations of linked entities and relation predicates from a knowledge graph (KG). We propose (1) local knowledge grounding to combine textual embeddings with the corresponding KG embeddings; and (2) global knowledge grounding to equip RHO with multi-hop reasoning abilities via the attention mechanism. In addition, we devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning. Experimental re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21019;&#24314;&#21517;&#20026;&#26041;&#27861;&#30340;&#32479;&#19968;&#22522;&#20934;&#30340;&#23581;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;13&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;OOD&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;21&#20010;&#24120;&#29992;&#30340;PLMs&#19978;&#23545;8&#20010;&#32463;&#20856;NLP&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2211.08073</link><description>&lt;p&gt;
GLUE-X: &#20174;ODD&#26222;&#36866;&#24615;&#35282;&#24230;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective. (arXiv:2211.08073v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21019;&#24314;&#21517;&#20026;&#26041;&#27861;&#30340;&#32479;&#19968;&#22522;&#20934;&#30340;&#23581;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;13&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;OOD&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;21&#20010;&#24120;&#29992;&#30340;PLMs&#19978;&#23545;8&#20010;&#32463;&#20856;NLP&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24050;&#30693;&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;NLP&#20219;&#21153;&#20013;&#30340;ODD&#26222;&#36866;&#24615;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21019;&#24314;&#21517;&#20026;&#26041;&#27861;&#30340;&#32479;&#19968;&#22522;&#20934;&#30340;&#23581;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#65292;&#24378;&#35843;OOD&#40065;&#26834;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#22914;&#20309;&#34913;&#37327;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#22914;&#20309;&#25913;&#21892;&#27169;&#22411;&#30340;&#35265;&#35299;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;13&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;OOD&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;21&#20010;&#24120;&#29992;&#30340;PLMs&#65288;&#21253;&#25324;GPT-3&#21644;GPT-3.5&#65289;&#19978;&#23545;8&#20010;&#32463;&#20856;NLP&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#30830;&#35748;&#20102;&#22312;&#25152;&#26377;&#35774;&#32622;&#19979;&#65292;&#19982;ID&#20934;&#30830;&#24230;&#30456;&#27604;&#65292;&#23384;&#22312;&#26174;&#30528;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#38656;&#35201;&#25913;&#21892;NLP&#20219;&#21153;&#20013;&#30340;OOD&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named \method for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22823;&#37327;&#26032;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#20174;&#32780;&#22686;&#24378;&#21407;&#22987;&#35757;&#32451;&#38598;&#65292;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.03044</link><description>&lt;p&gt;
&#20197;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#22120;&#20026;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#30340;&#22686;&#24378;&#23398;&#20064;&#23569;&#26679;&#26412;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tuning Language Models as Training Data Generators for Augmentation-Enhanced Few-Shot Learning. (arXiv:2211.03044v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03044
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22823;&#37327;&#26032;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#20174;&#32780;&#22686;&#24378;&#21407;&#22987;&#35757;&#32451;&#38598;&#65292;&#25552;&#39640;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#24778;&#20154;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#65306;&#23427;&#20204;&#21487;&#20197;&#22312;&#20197;&#25552;&#31034;&#24418;&#24335;&#34920;&#36798;&#30340;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#19978;&#24494;&#35843;&#21518;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#20016;&#23500;&#30340;&#20219;&#21153;&#29305;&#23450;&#27880;&#37322;&#12290;&#23613;&#31649;&#26377;&#30528;&#24456;&#26377;&#21069;&#36884;&#30340;&#34920;&#29616;&#65292;&#20294;&#22823;&#22810;&#25968;&#20165;&#20174;&#23569;&#37327;&#35757;&#32451;&#38598;&#23398;&#20064;&#30340;&#29616;&#26377;&#23569;&#26679;&#26412;&#26041;&#27861;&#20173;&#28982;&#27604;&#38750;&#24179;&#20961;&#30340;&#20840;&#30417;&#30563;&#35757;&#32451;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20351;&#29992;PLMs&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#65306;&#25105;&#20204;&#39318;&#20808;&#35843;&#25972;&#33258;&#22238;&#24402;PLM&#65292;&#28982;&#21518;&#20351;&#29992;&#23427;&#20316;&#20026;&#29983;&#25104;&#22120;&#65292;&#21512;&#25104;&#22823;&#37327;&#26032;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#20197;&#22686;&#24378;&#21407;&#22987;&#35757;&#32451;&#38598;&#12290;&#20026;&#20102;&#40723;&#21169;&#29983;&#25104;&#22120;&#20135;&#29983;&#20855;&#26377;&#26631;&#31614;&#21306;&#20998;&#33021;&#21147;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#36890;&#36807;&#21152;&#26435;&#26368;&#22823;&#20284;&#28982;&#24230;&#37327;&#35757;&#32451;&#23427;&#65292;&#22312;&#20854;&#20013;&#27599;&#20010;&#20196;&#29260;&#30340;&#26435;&#37325;&#22522;&#20110;&#19968;&#20010;&#21306;&#20998;&#24615;&#20803;&#23398;&#20064;&#30446;&#26631;&#33258;&#21160;&#35843;&#25972;&#12290;&#28982;&#21518;&#21487;&#20197;&#22312;&#22686;&#21152;&#21518;&#30340;&#35757;&#32451;&#38598;&#19978;&#24494;&#35843;&#20998;&#31867;PLM&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have revealed the intriguing few-shot learning ability of pretrained language models (PLMs): They can quickly adapt to a new task when fine-tuned on a small amount of labeled data formulated as prompts, without requiring abundant task-specific annotations. Despite their promising performance, most existing few-shot approaches that only learn from the small training set still underperform fully supervised training by nontrivial margins. In this work, we study few-shot learning with PLMs from a different perspective: We first tune an autoregressive PLM on the few-shot samples and then use it as a generator to synthesize a large amount of novel training samples which augment the original training set. To encourage the generator to produce label-discriminative samples, we train it via weighted maximum likelihood where the weight of each token is automatically adjusted based on a discriminative meta-learning objective. A classification PLM can then be fine-tuned on both the f
&lt;/p&gt;</description></item></channel></rss>