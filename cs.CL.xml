<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2404.00297</link><description>&lt;p&gt;
TRABSA&#65306;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;BiLSTM&#21644;Twitter-RoBERTa&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based BiLSTM and Twitter-RoBERTa
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00297
&lt;/p&gt;
&lt;p&gt;
TRABSA&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;transformer&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#21033;&#29992;RoBERTa&#22312;&#22823;&#37327;&#25512;&#29305;&#19978;&#35757;&#32451;&#65292;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#30340;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#23545;&#20110;&#29702;&#35299;&#20844;&#20247;&#33286;&#35770;&#21644;&#28040;&#36153;&#32773;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#27169;&#22411;&#38754;&#20020;&#30528;&#35821;&#35328;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TRABSA&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#22522;&#20110;transformer&#30340;&#26550;&#26500;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;BiLSTM&#32593;&#32476;&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#21033;&#29992;&#22312;124M&#26465;&#25512;&#25991;&#19978;&#35757;&#32451;&#30340;RoBERTa&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#24773;&#24863;&#20998;&#26512;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24046;&#36317;&#65292;&#30830;&#20445;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#23558;&#26469;&#33258;32&#20010;&#22269;&#23478;&#21644;&#32654;&#22269;&#21508;&#24030;&#30340;&#25512;&#25991;&#19982;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20845;&#31181;&#35789;&#23884;&#20837;&#25216;&#26415;&#21644;&#19977;&#31181;&#22522;&#20110;&#35789;&#20856;&#30340;&#26631;&#27880;&#25216;&#26415;&#65292;&#24182;&#36873;&#25321;&#20102;&#26368;&#20339;&#25216;&#26415;&#20197;&#23454;&#29616;&#26368;&#20339;&#24773;&#24863;&#20998;&#26512;&#25928;&#26524;&#12290;TRABSA&#20197;94%&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#33879;&#30340;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#24471;&#20998;&#22686;&#30410;&#65292;&#32988;&#36807;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26174;&#31034;&#20102;&#19968;&#33268;&#30340;&#20248;&#36234;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;SHAP&#21644;LIME&#20998;&#26512;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22686;&#24378;&#20102;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00297v1 Announce Type: new  Abstract: Sentiment analysis is crucial for understanding public opinion and consumer behavior. Existing models face challenges with linguistic diversity, generalizability, and explainability. We propose TRABSA, a hybrid framework integrating transformer-based architectures, attention mechanisms, and BiLSTM networks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge gaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy. Augmenting datasets with tweets from 32 countries and US states, we compare six word-embedding techniques and three lexicon-based labeling techniques, selecting the best for optimal sentiment analysis. TRABSA outperforms traditional ML and deep learning models with 94% accuracy and significant precision, recall, and F1-score gains. Evaluation across diverse datasets demonstrates consistent superiority and generalizability. SHAP and LIME analyses enhance interpretability, improving confidence i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.19521</link><description>&lt;p&gt;
&#35299;&#37322;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20013;&#30340;&#20851;&#38190;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19521
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#20837;&#30740;&#31350;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#30340;&#26426;&#21046;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#38646;/&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#20219;&#21153;&#22836;&#12289;MLP&#23618;&#21644;&#27531;&#24046;&#27969;&#30340;&#21151;&#33021;&#65292;&#20197;&#21450;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;Transformer-based&#35821;&#35328;&#27169;&#22411;&#22312;&#20107;&#23454;&#22238;&#24518;&#20219;&#21153;&#20013;&#25152;&#37319;&#29992;&#30340;&#26426;&#21046;&#12290;&#22312;&#38646;&#27425;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#31867;&#20284;&#8220;&#27861;&#22269;&#30340;&#39318;&#37117;&#26159;&#8221;&#30340;&#25552;&#31034;&#65292;&#29305;&#23450;&#20219;&#21153;&#30340;&#27880;&#24847;&#21147;&#22836;&#20250;&#20174;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20027;&#39064;&#23454;&#20307;&#65292;&#22914;&#8220;&#27861;&#22269;&#8221;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#21518;&#32493;&#30340;MLP&#20197;&#22238;&#24518;&#25152;&#38656;&#30340;&#31572;&#26696;&#65292;&#22914;&#8220;&#24052;&#40654;&#8221;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;MLP&#30340;&#36755;&#20986;&#20998;&#35299;&#20026;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#32452;&#20214;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#36319;&#38543;&#36825;&#20123;&#29305;&#23450;&#20219;&#21153;&#22836;&#30340;MLP&#23618;&#30340;&#21151;&#33021;&#12290;&#22312;&#27531;&#24046;&#27969;&#20013;&#65292;&#23427;&#20250;&#25830;&#38500;&#25110;&#25918;&#22823;&#26469;&#33258;&#21508;&#20010;&#22836;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#23427;&#20250;&#29983;&#25104;&#19968;&#20010;&#32452;&#20214;&#65292;&#23558;&#27531;&#24046;&#27969;&#37325;&#26032;&#23450;&#21521;&#21040;&#39044;&#26399;&#31572;&#26696;&#30340;&#26041;&#21521;&#12290;&#36825;&#20123;&#38646;&#27425;&#26426;&#21046;&#20063;&#36866;&#29992;&#20110;&#23569;&#27425;&#26679;&#26412;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19968;&#31181;&#24191;&#27867;&#23384;&#22312;&#30340;&#25239;&#36807;&#24230;&#33258;&#20449;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19521v1 Announce Type: cross  Abstract: In this paper, we deeply explore the mechanisms employed by Transformer-based language models in factual recall tasks. In zero-shot scenarios, given a prompt like "The capital of France is," task-specific attention heads extract the topic entity, such as "France," from the context and pass it to subsequent MLPs to recall the required answer such as "Paris." We introduce a novel analysis method aimed at decomposing the outputs of the MLP into components understandable by humans. Through this method, we quantify the function of the MLP layer following these task-specific heads. In the residual stream, it either erases or amplifies the information originating from individual heads. Moreover, it generates a component that redirects the residual stream towards the direction of its expected answer. These zero-shot mechanisms are also employed in few-shot scenarios. Additionally, we observed a widely existent anti-overconfidence mechanism in 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;FEEL&#65292;&#29992;&#20110;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38750;&#20154;&#24037;&#26041;&#27861;&#22312;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#37319;&#29992;&#20102;&#27010;&#29575;&#20998;&#24067;&#26041;&#27861;&#21644;&#38598;&#25104;&#23398;&#20064;&#20197;&#33719;&#24471;&#26356;&#31283;&#23450;&#21644;&#20840;&#38754;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15699</link><description>&lt;p&gt;
FEEL&#65306;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FEEL: A Framework for Evaluating Emotional Support Capability with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15699
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;FEEL&#65292;&#29992;&#20110;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38750;&#20154;&#24037;&#26041;&#27861;&#22312;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#37319;&#29992;&#20102;&#27010;&#29575;&#20998;&#24067;&#26041;&#27861;&#21644;&#38598;&#25104;&#23398;&#20064;&#20197;&#33719;&#24471;&#26356;&#31283;&#23450;&#21644;&#20840;&#38754;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#65288;ESC&#65289;&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#23545;&#35805;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24110;&#21161;&#29992;&#25143;&#32531;&#35299;&#24773;&#24863;&#21387;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#24773;&#24863;&#20998;&#26512;&#20013;&#28041;&#21450;&#22266;&#26377;&#20027;&#35266;&#24615;&#65292;&#24403;&#21069;&#38750;&#20154;&#24037;&#26041;&#27861;&#22312;&#26377;&#25928;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#20123;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#23384;&#22312;&#24456;&#20302;&#30340;&#30456;&#20851;&#24615;&#12290;&#21516;&#26102;&#65292;&#25163;&#21160;&#35780;&#20272;&#26041;&#27861;&#23558;&#23548;&#33268;&#24456;&#39640;&#30340;&#25104;&#26412;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#22411;&#27169;&#22411;FEEL&#65288;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#30340;&#26694;&#26550;&#65289;&#65292;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35780;&#20272;&#32773;&#26469;&#35780;&#20272;&#24773;&#24863;&#25903;&#25345;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#21608;&#23494;&#32771;&#34385;ESC&#30340;&#21508;&#31181;&#35780;&#20272;&#26041;&#38754;&#65292;&#24212;&#29992;&#26356;&#20840;&#38754;&#21644;&#20934;&#30830;&#30340;ESC&#35780;&#20272;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#23427;&#37319;&#29992;&#27010;&#29575;&#20998;&#24067;&#26041;&#27861;&#20197;&#33719;&#24471;&#26356;&#31283;&#23450;&#30340;&#32467;&#26524;&#65292;&#24182;&#38598;&#25104;&#20102;&#38598;&#25104;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15699v1 Announce Type: new  Abstract: Emotional Support Conversation (ESC) is a typical dialogue that can effec-tively assist the user in mitigating emotional pressures. However, owing to the inherent subjectivity involved in analyzing emotions, current non-artificial methodologies face challenges in effectively appraising the emo-tional support capability. These metrics exhibit a low correlation with human judgments. Concurrently, manual evaluation methods extremely will cause high costs. To solve these problems, we propose a novel model FEEL (Framework for Evaluating Emotional Support Capability with Large Lan-guage Models), employing Large Language Models (LLMs) as evaluators to assess emotional support capabilities. The model meticulously considers var-ious evaluative aspects of ESC to apply a more comprehensive and accurate evaluation method for ESC. Additionally, it employs a probability distribu-tion approach for a more stable result and integrates an ensemble learnin
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#19982;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#21644;&#32034;&#36180;&#25968;&#25454;&#22238;&#31572;&#27969;&#34892;&#30149;&#23398;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#23454;&#34892;&#19994;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.09226</link><description>&lt;p&gt;
&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#22120;&#29992;&#20110;&#27969;&#34892;&#30149;&#23398;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Retrieval augmented text-to-SQL generation for epidemiological question answering using electronic health records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09226
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#19982;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#21644;&#32034;&#36180;&#25968;&#25454;&#22238;&#31572;&#27969;&#34892;&#30149;&#23398;&#38382;&#39064;&#65292;&#24182;&#22312;&#29616;&#23454;&#34892;&#19994;&#20013;&#26174;&#31034;&#20986;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#21644;&#32034;&#36180;&#25968;&#25454;&#26159;&#21453;&#26144;&#24739;&#32773;&#20581;&#24247;&#29366;&#20917;&#21644;&#21307;&#30103;&#21033;&#29992;&#24773;&#20917;&#30340;&#20016;&#23500;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#26469;&#28304;&#12290;&#26597;&#35810;&#36825;&#20123;&#25968;&#25454;&#24211;&#20197;&#22238;&#31572;&#27969;&#34892;&#30149;&#23398;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#22312;&#20110;&#21307;&#23398;&#26415;&#35821;&#30340;&#22797;&#26434;&#24615;&#21644;&#23545;&#22797;&#26434;SQL&#26597;&#35810;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#65292;&#23558;&#25991;&#26412;&#21040;SQL&#29983;&#25104;&#19982;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#29992;EHR&#21644;&#32034;&#36180;&#25968;&#25454;&#22238;&#31572;&#27969;&#34892;&#30149;&#23398;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23558;&#21307;&#23398;&#32534;&#30721;&#27493;&#39588;&#25972;&#21512;&#21040;&#25991;&#26412;&#21040;SQL&#36807;&#31243;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#31616;&#21333;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#23578;&#19981;&#36275;&#20197;&#26080;&#30417;&#30563;&#20351;&#29992;&#65292;&#20294;RAG&#20026;&#25913;&#36827;&#23427;&#20204;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#65292;&#22914;&#22312;&#19968;&#20010;&#29616;&#23454;&#30340;&#34892;&#19994;&#29615;&#22659;&#20013;&#25152;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09226v1 Announce Type: new  Abstract: Electronic health records (EHR) and claims data are rich sources of real-world data that reflect patient health status and healthcare utilization. Querying these databases to answer epidemiological questions is challenging due to the intricacy of medical terminology and the need for complex SQL queries. Here, we introduce an end-to-end methodology that combines text-to-SQL generation with retrieval augmented generation (RAG) to answer epidemiological questions using EHR and claims data. We show that our approach, which integrates a medical coding step into the text-to-SQL process, significantly improves the performance over simple prompting. Our findings indicate that although current language models are not yet sufficiently accurate for unsupervised use, RAG offers a promising direction for improving their capabilities, as shown in a realistic industry setting.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#65292;&#21253;&#25324;&#26816;&#27979;&#22330;&#26223;&#36793;&#30028;&#12289;&#37325;&#26032;&#25490;&#21015;&#22330;&#26223;&#12289;&#23558;&#35270;&#35273;&#20449;&#24687;&#36716;&#25442;&#20026;&#25991;&#26412;&#12289;&#24635;&#32467;&#23545;&#35805;&#20197;&#21450;&#23558;&#22330;&#26223;&#25688;&#35201;&#34701;&#21512;&#30340;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#34913;&#37327;&#25688;&#35201;&#36136;&#37327;&#30340;&#35780;&#20215;&#25351;&#26631;PREFS&#12290;</title><link>https://arxiv.org/abs/2403.03823</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Modular Approach for Multimodal Summarization of TV Shows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03823
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#65292;&#21253;&#25324;&#26816;&#27979;&#22330;&#26223;&#36793;&#30028;&#12289;&#37325;&#26032;&#25490;&#21015;&#22330;&#26223;&#12289;&#23558;&#35270;&#35273;&#20449;&#24687;&#36716;&#25442;&#20026;&#25991;&#26412;&#12289;&#24635;&#32467;&#23545;&#35805;&#20197;&#21450;&#23558;&#22330;&#26223;&#25688;&#35201;&#34701;&#21512;&#30340;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#34913;&#37327;&#25688;&#35201;&#36136;&#37327;&#30340;&#35780;&#20215;&#25351;&#26631;PREFS&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#30005;&#35270;&#33410;&#30446;&#25688;&#35201;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#21040;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#30340;&#20851;&#38190;&#39046;&#22495;&#65306;&#22797;&#26434;&#25512;&#29702;&#12289;&#22810;&#27169;&#24577;&#21644;&#38271;&#31687;&#21465;&#20107;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#20854;&#20013;&#21508;&#20010;&#32452;&#20214;&#25191;&#34892;&#19987;&#38376;&#30340;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#35748;&#20026;&#19982;&#31471;&#21040;&#31471;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22359;&#28041;&#21450;&#26816;&#27979;&#22330;&#26223;&#36793;&#30028;&#65292;&#37325;&#26032;&#25490;&#21015;&#22330;&#26223;&#20197;&#23613;&#37327;&#20943;&#23569;&#19981;&#21516;&#20107;&#20214;&#20043;&#38388;&#30340;&#20999;&#25442;&#27425;&#25968;&#65292;&#23558;&#35270;&#35273;&#20449;&#24687;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#24635;&#32467;&#27599;&#20010;&#22330;&#26223;&#20013;&#30340;&#23545;&#35805;&#65292;&#24182;&#23558;&#22330;&#26223;&#25688;&#35201;&#34701;&#21512;&#25104;&#25972;&#38598;&#30340;&#26368;&#32456;&#25688;&#35201;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;PREFS&#65288;&#25688;&#35201;&#20107;&#23454;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#35780;&#20272;&#65289;&#65292;&#29992;&#20110;&#34913;&#37327;&#29983;&#25104;&#25688;&#35201;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#65292;&#25105;&#20204;&#23558;&#20854;&#20998;&#35299;&#20026;&#21407;&#23376;&#20107;&#23454;&#12290;&#22312;&#26368;&#36817;&#21457;&#24067;&#30340;SummScreen3D&#25968;&#25454;&#38598;Papalampidi&#21644;Lapata&#65288;2023&#65289;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03823v1 Announce Type: new  Abstract: In this paper we address the task of summarizing television shows, which touches key areas in AI research: complex reasoning, multiple modalities, and long narratives. We present a modular approach where separate components perform specialized sub-tasks which we argue affords greater flexibility compared to end-to-end methods. Our modules involve detecting scene boundaries, reordering scenes so as to minimize the number of cuts between different events, converting visual information to text, summarizing the dialogue in each scene, and fusing the scene summaries into a final summary for the entire episode. We also present a new metric, PREFS (\textbf{P}recision and \textbf{R}ecall \textbf{E}valuation of Summary \textbf{F}act\textbf{s}), to measure both precision and recall of generated summaries, which we decompose into atomic facts. Tested on the recently released SummScreen3D dataset Papalampidi and Lapata (2023), our method produces hi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;&#26041;&#27861;CDD&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#26041;&#27861;TED&#65292;&#20197;&#24212;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#27745;&#26579;&#21644;&#21487;&#20449;&#35780;&#20272;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15938</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#25110;&#35760;&#24518;&#65306;&#25968;&#25454;&#27745;&#26579;&#19982;&#21487;&#20449;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Generalization or Memorization: Data Contamination and Trustworthy Evaluation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15938
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;&#26041;&#27861;CDD&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#26041;&#27861;TED&#65292;&#20197;&#24212;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#27745;&#26579;&#21644;&#21487;&#20449;&#35780;&#20272;&#26041;&#38754;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#33021;&#21147;&#30340;&#35828;&#27861;&#36890;&#24120;&#26159;&#36890;&#36807;&#22312;&#24320;&#25918;&#33719;&#21462;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#26469;&#25903;&#25345;&#30340;&#12290;&#32771;&#34385;&#21040;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24222;&#22823;&#35268;&#27169;&#21644;&#24191;&#27867;&#26469;&#28304;&#65292;&#23427;&#21487;&#33021;&#26126;&#30830;&#25110;&#38544;&#21547;&#22320;&#21253;&#21547;&#27979;&#35797;&#25968;&#25454;&#65292;&#23548;&#33268;LLMs&#26356;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#36879;&#26126;&#24615;&#12289;&#27169;&#22411;&#30340;&#40657;&#30418;&#35775;&#38382;&#20197;&#21450;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#23545;&#20110;LLMs&#26469;&#35828;&#26816;&#27979;&#21644;&#20943;&#36731;&#25968;&#25454;&#27745;&#26579;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CDD&#65292;&#21363;&#36890;&#36807;LLMs&#36755;&#20986;&#20998;&#24067;&#36827;&#34892;&#27745;&#26579;&#26816;&#27979;&#30340;CDD&#12290;CDD&#20165;&#38656;&#35201;&#37319;&#26679;&#25991;&#26412;&#26469;&#26816;&#27979;&#25968;&#25454;&#27745;&#26579;&#65292;&#36890;&#36807;&#35782;&#21035;LLMs&#36755;&#20986;&#20998;&#24067;&#30340;&#23792;&#20540;&#26469;&#36827;&#34892;&#26816;&#27979;&#12290;&#20026;&#20102;&#20943;&#36731;&#35780;&#20272;&#20013;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;TED&#65306;&#22522;&#20110;LLMs&#36755;&#20986;&#20462;&#27491;&#30340;&#21487;&#20449;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15938v1 Announce Type: cross  Abstract: Recent statements about the impressive capabilities of large language models (LLMs) are usually supported by evaluating on open-access benchmarks. Considering the vast size and wide-ranging sources of LLMs' training data, it could explicitly or implicitly include test data, leading to LLMs being more susceptible to data contamination. However, due to the opacity of training data, the black-box access of models, and the rapid growth of synthetic training data, detecting and mitigating data contamination for LLMs faces significant challenges. In this paper, we propose CDD, which stands for Contamination Detection via output Distribution for LLMs. CDD necessitates only the sampled texts to detect data contamination, by identifying the peakedness of LLM's output distribution. To mitigate the impact of data contamination in evaluation, we also present TED: Trustworthy Evaluation via output Distribution, based on the correction of LLM's outp
&lt;/p&gt;</description></item><item><title>GenTranslate&#26159;&#19968;&#20010;&#26032;&#30340;&#32763;&#35793;&#20219;&#21153;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#21487;&#20197;&#20174;N-best&#21015;&#34920;&#20013;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06894</link><description>&lt;p&gt;
GenTranslate: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#29983;&#25104;&#30340;&#22810;&#35821;&#35328;&#35821;&#38899;&#21644;&#26426;&#22120;&#32763;&#35793;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06894
&lt;/p&gt;
&lt;p&gt;
GenTranslate&#26159;&#19968;&#20010;&#26032;&#30340;&#32763;&#35793;&#20219;&#21153;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20016;&#23500;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#65292;&#21487;&#20197;&#20174;N-best&#21015;&#34920;&#20013;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#36890;&#36807;&#20943;&#23569;&#34920;&#31034;&#35823;&#24046;&#21644;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#65292;&#25512;&#21160;&#20102;&#22810;&#35821;&#35328;&#35821;&#38899;&#21644;&#26426;&#22120;&#32763;&#35793;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#32763;&#35793;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#26463;&#25628;&#32034;&#35299;&#30721;&#21644;&#21069;k&#20010;&#20551;&#35774;&#36873;&#25321;&#36827;&#34892;&#25512;&#29702;&#12290;&#36825;&#20123;&#25216;&#26415;&#24448;&#24448;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#22810;&#26679;&#21270;&#30340;N-best&#20551;&#35774;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#38656;&#35201;&#21333;&#20010;&#39640;&#36136;&#37327;&#36755;&#20986;&#24207;&#21015;&#30340;&#32763;&#35793;&#20219;&#21153;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32763;&#35793;&#20219;&#21153;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;&#8220;GenTranslate&#8221;&#65292;&#23427;&#22522;&#20110;LLMs&#26469;&#20174;N-best&#21015;&#34920;&#20013;&#29983;&#25104;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#21033;&#29992;LLMs&#20016;&#23500;&#30340;&#35821;&#35328;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26032;&#27169;&#22411;&#21487;&#20197;&#23558;N-best&#20505;&#36873;&#20154;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#25972;&#21512;&#36215;&#26469;&#65292;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25903;&#25345;LLM&#30340;&#24494;&#35843;&#65292;&#25105;&#20204;&#26500;&#24314;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;HypoTransla&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely "GenTranslate", which builds upon LLMs to generate better results from the diverse translation versions in N-best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the rich information in N-best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTransla
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30450;&#28857;&#22312;&#20110;&#20854;&#23545;&#36229;&#21465;&#20107;&#35821;&#35328;&#20449;&#24687;&#30340;&#24573;&#35270;&#65292;&#30740;&#31350;&#25552;&#20986;&#32771;&#34385;&#27169;&#22411;&#22914;&#20309;&#24863;&#30693;&#35821;&#35328;&#20449;&#24687;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;&#20854;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2306.06794</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30450;&#28857;&#65306;&#36229;&#21465;&#20107;&#35821;&#35328;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
A blind spot for large language models: Supradiegetic linguistic information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.06794
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30450;&#28857;&#22312;&#20110;&#20854;&#23545;&#36229;&#21465;&#20107;&#35821;&#35328;&#20449;&#24687;&#30340;&#24573;&#35270;&#65292;&#30740;&#31350;&#25552;&#20986;&#32771;&#34385;&#27169;&#22411;&#22914;&#20309;&#24863;&#30693;&#35821;&#35328;&#20449;&#24687;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21453;&#26144;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#28145;&#21051;&#21464;&#38761;&#65292;&#23454;&#29616;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#29978;&#33267;&#20196;&#20154;&#38663;&#24778;&#30340;&#31867;&#20154;&#35821;&#35328;&#27969;&#21033;&#24230;&#12290;&#23427;&#20204;&#30446;&#21069;&#21644;&#28508;&#22312;&#30340;&#33021;&#21147;&#33539;&#22260;&#26159;&#19968;&#20010;&#31215;&#26497;&#25506;&#35752;&#30340;&#39046;&#22495;&#65292;&#32477;&#38750;&#20165;&#38480;&#20110;&#31185;&#30740;&#20154;&#21592;&#12290;&#20154;&#20204;&#36890;&#24120;&#23558;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#26694;&#23450;&#20026;&#8220;&#25991;&#26412;&#8221;&#29978;&#33267;&#8220;&#35821;&#35328;&#8221;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#35821;&#35328;&#23398;&#12289;&#20307;&#29616;&#35748;&#30693;&#12289;&#35748;&#30693;&#31185;&#23398;&#12289;&#25968;&#23398;&#21644;&#21382;&#21490;&#31561;&#39046;&#22495;&#30340;&#24605;&#24819;&#65292;&#20180;&#32454;&#23457;&#35270;&#36825;&#19968;&#26694;&#26550;&#30340;&#32454;&#33410;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#32771;&#34385;&#20687;ChatGPT&#36825;&#26679;&#30340;LLM&#26159;&#20160;&#20040;&#24863;&#35273;&#65292;&#27491;&#22914;&#32435;&#26684;&#23572;&#21487;&#33021;&#20250;&#35828;&#30340;&#37027;&#26679;&#65292;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;&#20854;&#25972;&#20307;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#65292;&#20854;&#25509;&#21463;&#30340;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#21487;&#20197;&#34987;&#26377;&#30410;&#22320;&#37325;&#26032;&#26500;&#24605;&#20026;&#23545;&#35821;&#35328;&#20013;&#32534;&#30721;&#30340;&#21465;&#20107;&#20449;&#24687;&#30340;&#25509;&#35302;&#65292;&#20854;&#32570;&#38519;&#21487;&#20197;&#34987;&#37325;&#26032;&#26500;&#24605;&#20026;&#23545;&#36825;&#20123;&#20449;&#24687;&#30340;&#26080;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.06794v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) like ChatGPT reflect profound changes in the field of Artificial Intelligence, achieving a linguistic fluency that is impressively, even shockingly, human-like. The extent of their current and potential capabilities is an active area of investigation by no means limited to scientific researchers. It is common for people to frame the training data for LLMs as "text" or even "language". We examine the details of this framing using ideas from several areas, including linguistics, embodied cognition, cognitive science, mathematics, and history. We propose that considering what it is like to be an LLM like ChatGPT, as Nagel might have put it, can help us gain insight into its capabilities in general, and in particular, that its exposure to linguistic training data can be productively reframed as exposure to the diegetic information encoded in language, and its deficits can be reframed as ignorance of ext
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#21518;&#26524;&#65292;&#21457;&#29616;&#22312;&#22686;&#24378;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#20445;&#25345;&#36947;&#24503;&#23436;&#25972;&#24615;&#20043;&#38388;&#23384;&#22312;&#24726;&#35770;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#27880;&#20837;&#20934;&#30830;&#20449;&#24687;&#23545;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#24456;&#37325;&#35201;&#65292;&#20294;&#23427;&#21487;&#33021;&#30772;&#22351;&#27169;&#22411;&#30340;&#22522;&#26412;&#26694;&#26550;&#65292;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#21644;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2401.10647</link><description>&lt;p&gt;
&#25773;&#39118;&#25769;&#36215;&#39118;&#26292;&#65306;&#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language Models. (arXiv:2401.10647v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#32534;&#36753;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#26434;&#21518;&#26524;&#65292;&#21457;&#29616;&#22312;&#22686;&#24378;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#20445;&#25345;&#36947;&#24503;&#23436;&#25972;&#24615;&#20043;&#38388;&#23384;&#22312;&#24726;&#35770;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#27880;&#20837;&#20934;&#30830;&#20449;&#24687;&#23545;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#24456;&#37325;&#35201;&#65292;&#20294;&#23427;&#21487;&#33021;&#30772;&#22351;&#27169;&#22411;&#30340;&#22522;&#26412;&#26694;&#26550;&#65292;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#21644;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#32418;&#38431;&#27979;&#35797;&#25110;&#36234;&#29425;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#27010;&#24565;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#36890;&#36807;&#23545;&#27169;&#22411;&#36827;&#34892;&#32534;&#36753;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#20462;&#25913;&#30340;&#22797;&#26434;&#21518;&#26524;&#65292;&#21457;&#29616;&#20102;&#22686;&#24378;&#27169;&#22411;&#20934;&#30830;&#24615;&#19982;&#20445;&#25345;&#20854;&#36947;&#24503;&#23436;&#25972;&#24615;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#28145;&#20837;&#20998;&#26512;&#25581;&#31034;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#24726;&#35770;&#65306;&#34429;&#28982;&#27880;&#20837;&#20934;&#30830;&#20449;&#24687;&#23545;&#20110;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23427;&#21364;&#21487;&#33021;&#30772;&#22351;&#27169;&#22411;&#30340;&#22522;&#26412;&#26694;&#26550;&#65292;&#23548;&#33268;&#19981;&#21487;&#39044;&#27979;&#21644;&#28508;&#22312;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;NicheHazardQA&#65292;&#29992;&#20110;&#30740;&#31350;&#27169;&#22411;&#22312;&#30456;&#21516;&#21644;&#36328;&#39046;&#22495;&#20013;&#30340;&#19981;&#23433;&#20840;&#34892;&#20026;&#12290;&#36825;&#19968;&#26041;&#38754;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#32534;&#36753;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#23433;&#20840;&#24230;&#37327;&#21644;&#20445;&#25252;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing field of artificial intelligence, the concept of Red-Teaming or Jailbreaking large language models (LLMs) has emerged as a crucial area of study. This approach is especially significant in terms of assessing and enhancing the safety and robustness of these models. This paper investigates the intricate consequences of such modifications through model editing, uncovering a complex relationship between enhancing model accuracy and preserving its ethical integrity. Our in-depth analysis reveals a striking paradox: while injecting accurate information is crucial for model reliability, it can paradoxically destabilize the model's foundational framework, resulting in unpredictable and potentially unsafe behaviors. Additionally, we propose a benchmark dataset NicheHazardQA to investigate this unsafe behavior both within the same and cross topical domain. This aspect of our research sheds light on how the edits, impact the model's safety metrics and guardrails. Our find
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20197;&#23562;&#37325;&#20026;&#35270;&#35282;&#35780;&#20272;&#19982;&#35821;&#35328;&#20195;&#29702;&#30340;&#20132;&#20114;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#20851;&#27880;&#20851;&#31995;&#21644;&#24773;&#22659;&#22240;&#32032;&#30340;&#20262;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;LLM&#25216;&#26415;&#34920;&#29616;&#24471;&#8220;&#22909;&#8221;</title><link>http://arxiv.org/abs/2401.09082</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#8220;&#22909;&#8221;&#30340;&#31038;&#20132;&#34892;&#20026;&#32773;&#65311;&#20197;&#23562;&#37325;&#20026;&#35270;&#35282;&#35780;&#20272;&#19982;&#35821;&#35328;&#20195;&#29702;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
What makes for a 'good' social actor? Using respect as a lens to evaluate interactions with language agents. (arXiv:2401.09082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20197;&#23562;&#37325;&#20026;&#35270;&#35282;&#35780;&#20272;&#19982;&#35821;&#35328;&#20195;&#29702;&#30340;&#20132;&#20114;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#20851;&#27880;&#20851;&#31995;&#21644;&#24773;&#22659;&#22240;&#32032;&#30340;&#20262;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;LLM&#25216;&#26415;&#34920;&#29616;&#24471;&#8220;&#22909;&#8221;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#35805;&#20195;&#29702;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22914;&#20309;&#30830;&#20445;&#23427;&#20204;&#30340;&#34892;&#20026;&#36947;&#24503;&#21644;&#36866;&#24403;&#24615;&#24050;&#32463;&#24341;&#36215;&#20102;&#32039;&#24613;&#20851;&#27880;&#12290;&#20174;&#8220;HHH&#8221;&#26631;&#20934;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20027;&#35201;&#20307;&#29616;&#22312;&#35753;&#36755;&#20986;&#26356;&#26377;&#24110;&#21161;&#21644;&#35802;&#23454;&#65292;&#24182;&#36991;&#20813;&#26377;&#23475;&#65288;&#26377;&#20559;&#35265;&#12289;&#26377;&#27602;&#25110;&#19981;&#20934;&#30830;&#65289;&#30340;&#38472;&#36848;&#12290;&#34429;&#28982;&#36825;&#31181;&#35821;&#20041;&#28966;&#28857;&#23545;&#20110;&#23558;LLM&#20195;&#29702;&#35270;&#20026;&#32431;&#31929;&#30340;&#20449;&#24687;&#23186;&#20171;&#26159;&#26377;&#29992;&#30340;&#65292;&#20294;&#23427;&#26410;&#33021;&#32771;&#34385;&#21040;&#22312;&#19981;&#21516;&#31038;&#20132;&#24773;&#22659;&#20013;&#65292;&#21516;&#26679;&#30340;&#35805;&#35821;&#21487;&#33021;&#20250;&#26174;&#24471;&#26356;&#25110;&#32773;&#26356;&#23569;&#20882;&#29359;&#25110;&#19981;&#24471;&#20307;&#30340;&#23454;&#38469;&#22240;&#32032;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#21152;&#20851;&#27880;&#20851;&#31995;&#21644;&#24773;&#22659;&#22240;&#32032;&#30340;&#20262;&#29702;&#26041;&#27861;&#65292;&#25506;&#35752;&#20316;&#20026;&#31038;&#20132;&#34892;&#20026;&#32773;&#30340;&#31995;&#32479;&#22914;&#20309;&#22312;&#20132;&#20114;&#20013;&#20197;&#23562;&#37325;&#30340;&#26041;&#24335;&#23545;&#24453;&#20010;&#20307;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#39044;&#35265;&#20102;&#22312;&#24773;&#22659;&#20132;&#20114;&#23618;&#38754;&#19978;&#19968;&#31995;&#21015;&#23578;&#26410;&#34987;&#25506;&#32034;&#30340;&#39118;&#38505;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#29992;&#24314;&#35758;&#65292;&#20197;&#24110;&#21161;LLM&#25216;&#26415;&#34920;&#29616;&#24471;&#8220;&#22909;&#8221;
&lt;/p&gt;
&lt;p&gt;
With the growing popularity of dialogue agents based on large language models (LLMs), urgent attention has been drawn to finding ways to ensure their behaviour is ethical and appropriate. These are largely interpreted in terms of the 'HHH' criteria: making outputs more helpful and honest, and avoiding harmful (biased, toxic, or inaccurate) statements. Whilst this semantic focus is useful from the perspective of viewing LLM agents as mere mediums for information, it fails to account for pragmatic factors that can make the same utterance seem more or less offensive or tactless in different social situations. We propose an approach to ethics that is more centred on relational and situational factors, exploring what it means for a system, as a social actor, to treat an individual respectfully in a (series of) interaction(s). Our work anticipates a set of largely unexplored risks at the level of situated interaction, and offers practical suggestions to help LLM technologies behave as 'good'
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31639;&#27861;&#34920;&#31034;&#38598;&#25104;&#21040;&#31639;&#27861;&#36873;&#25321;&#20013;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#24403;&#21069;&#31639;&#27861;&#36873;&#25321;&#25216;&#26415;&#23545;&#31639;&#27861;&#29305;&#24449;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2311.13184</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#31639;&#27861;&#36873;&#25321;&#65306;&#26397;&#30528;&#20840;&#38754;&#31639;&#27861;&#34920;&#31034;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Large Language Model-Enhanced Algorithm Selection: Towards Comprehensive Algorithm Representation. (arXiv:2311.13184v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.13184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#31639;&#27861;&#34920;&#31034;&#38598;&#25104;&#21040;&#31639;&#27861;&#36873;&#25321;&#20013;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#24403;&#21069;&#31639;&#27861;&#36873;&#25321;&#25216;&#26415;&#23545;&#31639;&#27861;&#29305;&#24449;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#36873;&#25321;&#26088;&#22312;&#22312;&#25191;&#34892;&#20043;&#21069;&#35782;&#21035;&#35299;&#20915;&#29305;&#23450;&#38382;&#39064;&#30340;&#26368;&#21512;&#36866;&#31639;&#27861;&#65292;&#24050;&#25104;&#20026;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#24403;&#21069;&#20027;&#27969;&#30340;&#31639;&#27861;&#36873;&#25321;&#25216;&#26415;&#20027;&#35201;&#20381;&#36182;&#20110;&#21508;&#31181;&#38382;&#39064;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#27599;&#20010;&#31639;&#27861;&#30340;&#24615;&#33021;&#20316;&#20026;&#30417;&#30563;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#31639;&#27861;&#29305;&#24449;&#30340;&#32771;&#34385;&#23384;&#22312;&#37325;&#35201;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#36825;&#20027;&#35201;&#24402;&#22240;&#20110;&#31639;&#27861;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#20351;&#24471;&#22312;&#19981;&#21516;&#31181;&#31867;&#30340;&#31639;&#27861;&#20013;&#25214;&#21040;&#19968;&#31181;&#26222;&#36866;&#26377;&#25928;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24573;&#35270;&#20102;&#36825;&#19968;&#26041;&#38754;&#26080;&#30097;&#20250;&#24433;&#21709;&#31639;&#27861;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#38388;&#25509;&#38656;&#35201;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#31354;&#30333;&#65292;&#21363;&#23558;&#31639;&#27861;&#34920;&#31034;&#38598;&#25104;&#21040;&#31639;&#27861;&#36873;&#25321;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithm selection aims to identify the most suitable algorithm for solving a specific problem before execution, which has become a critical process of the AutoML. Current mainstream algorithm selection techniques rely heavily on feature representations of various problems and employ the performance of each algorithm as supervised information. However, there is a significant research gap concerning the consideration of algorithm features. This gap is primarily attributed to the inherent complexity of algorithms, making it particularly challenging to find a universally effective feature extraction method that is applicable across a diverse range of algorithms. Unfortunately, neglecting this aspect undoubtedly impacts the accuracy of algorithm selection and indirectly necessitates an increased volume of problem data for training purposes. This paper takes a significant stride towards addressing this gap by proposing an approach that integrates algorithm representation into the algorithm
&lt;/p&gt;</description></item><item><title>MagicBrush&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25351;&#23548;&#30495;&#23454;&#22270;&#20687;&#30340;&#32534;&#36753;&#12290;&#23427;&#21253;&#25324;&#36229;&#36807;10K&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;InstructPix2Pix&#21487;&#20197;&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.10012</link><description>&lt;p&gt;
MagicBrush: &#20154;&#24037;&#26631;&#27880;&#30340;&#29992;&#20110;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing. (arXiv:2306.10012v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10012
&lt;/p&gt;
&lt;p&gt;
MagicBrush&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25351;&#23548;&#30495;&#23454;&#22270;&#20687;&#30340;&#32534;&#36753;&#12290;&#23427;&#21253;&#25324;&#36229;&#36807;10K&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;InstructPix2Pix&#21487;&#20197;&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#32534;&#36753;&#20174;&#20010;&#20154;&#20351;&#29992;&#21040;&#19987;&#19994;&#24212;&#29992;&#65288;&#22914;Photoshop&#65289;&#24191;&#27867;&#38656;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#26159;&#38646;&#26679;&#26412;&#65292;&#35201;&#20040;&#26159;&#22312;&#33258;&#21160;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21547;&#26377;&#22823;&#37327;&#30340;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#20173;&#38656;&#35201;&#22823;&#37327;&#30340;&#25163;&#21160;&#35843;&#25972;&#25165;&#33021;&#20135;&#29983;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MagicBrush&#65292;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25351;&#23548;&#30495;&#23454;&#22270;&#20687;&#30340;&#32534;&#36753;&#65292;&#21253;&#25324;&#21333;&#20010;&#25805;&#20316;&#12289;&#22810;&#20010;&#25805;&#20316;&#12289;&#25552;&#20379;&#25513;&#30721;&#21644;&#19981;&#25552;&#20379;&#25513;&#30721;&#31561;&#19981;&#21516;&#22330;&#26223;&#12290;MagicBrush&#21253;&#25324;&#36229;&#36807;10K&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#65288;&#28304;&#22270;&#20687;&#65292;&#25351;&#20196;&#65292;&#30446;&#26631;&#22270;&#20687;&#65289;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;MagicBrush&#19978;&#24494;&#35843;InstructPix2Pix&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20351;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-guided image editing is widely needed in daily life, ranging from personal use to professional applications such as Photoshop. However, existing methods are either zero-shot or trained on an automatically synthesized dataset, which contains a high volume of noise. Thus, they still require lots of manual tuning to produce desirable outcomes in practice. To address this issue, we introduce MagicBrush (https://osu-nlp-group.github.io/MagicBrush/), the first large-scale, manually annotated dataset for instruction-guided real image editing that covers diverse scenarios: single-turn, multi-turn, mask-provided, and mask-free editing. MagicBrush comprises over 10K manually annotated triples (source image, instruction, target image), which supports trainining large-scale text-guided image editing models. We fine-tune InstructPix2Pix on MagicBrush and show that the new model can produce much better images according to human evaluation. We further conduct extensive experiments to evaluate cu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#26469;&#20943;&#23569;&#38750;&#27954;&#21475;&#38899;&#20020;&#24202;ASR&#35757;&#32451;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36229;&#36807;&#29616;&#26377;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#24182;&#25552;&#39640;&#20302;&#36164;&#28304;&#21475;&#38899;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.02105</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;ASR&#27169;&#22411;&#20197;&#24212;&#23545;&#20302;&#36164;&#28304;&#20020;&#24202;&#35821;&#38899;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Adapting Pretrained ASR Models to Low-resource Clinical Speech using Epistemic Uncertainty-based Data Selection. (arXiv:2306.02105v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#26469;&#20943;&#23569;&#38750;&#27954;&#21475;&#38899;&#20020;&#24202;ASR&#35757;&#32451;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36229;&#36807;&#29616;&#26377;&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#24182;&#25552;&#39640;&#20302;&#36164;&#28304;&#21475;&#38899;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;ASR&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#23545;&#20110;&#38750;&#27954;&#21475;&#38899;&#30340;&#20020;&#24202;ASR&#30340;&#30740;&#31350;&#36824;&#19981;&#22815;&#12290;&#22312;&#36825;&#19968;&#39046;&#22495;&#26500;&#24314;&#24378;&#22823;&#30340;ASR&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#65292;&#29992;&#20110;&#21508;&#31181;&#35821;&#35328;&#21644;&#24418;&#24577;&#20016;&#23500;&#30340;&#21475;&#38899;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#30340;&#21019;&#24314;&#25104;&#26412;&#36739;&#39640;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22522;&#20110;&#20449;&#24687;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#26469;&#20943;&#23569;&#27880;&#37322;&#36153;&#29992;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23558;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#25105;&#20204;&#30340;&#33258;&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#21487;&#20197;&#36229;&#36807;&#20351;&#29992;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;ASR&#27169;&#22411;&#24314;&#31435;&#30340;&#20960;&#20010;&#22522;&#20934;&#32467;&#26524;&#65292;&#21516;&#26102;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#65292;&#20174;&#32780;&#38477;&#20302;&#27880;&#37322;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#25913;&#21892;&#20102;&#20302;&#36164;&#28304;&#21475;&#38899;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#33021;&#21147;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38750;&#27954;&#20020;&#24202;ASR&#30340;&#32972;&#26223;&#19979;&#26500;&#24314;&#27867;&#21270;&#22411;ASR&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#65292;&#32780;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#20027;&#35201;&#26159;&#31232;&#32570;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
While there has been significant progress in ASR, African-accented clinical ASR has been understudied due to a lack of training datasets. Building robust ASR systems in this domain requires large amounts of annotated or labeled data, for a wide variety of linguistically and morphologically rich accents, which are expensive to create. Our study aims to address this problem by reducing annotation expenses through informative uncertainty-based data selection. We show that incorporating epistemic uncertainty into our adaptation rounds outperforms several baseline results, established using state-of-the-art (SOTA) ASR models, while reducing the required amount of labeled data, and hence reducing annotation costs. Our approach also improves out-of-distribution generalization for very low-resource accents, demonstrating the viability of our approach for building generalizable ASR models in the context of accented African clinical ASR, where training datasets are predominantly scarce.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25670;&#33073;&#26426;&#22120;&#32763;&#35793;&#20013;&#21477;&#23376;&#32423;&#33539;&#24335;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22788;&#29702;&#19977;&#20010;&#38556;&#30861;&#26469;&#23454;&#29616;&#65306;&#20351;&#29992;&#36275;&#22815;&#22823;&#30340;&#26631;&#20934;Transformer&#26550;&#26500;&#12289;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#23558;&#25991;&#26723;&#32423;&#20449;&#24687;&#36716;&#21270;&#20026;&#36866;&#21512;&#35757;&#32451;&#30340;&#24418;&#24335;&#12289;&#22522;&#20110;&#33258;&#21160;&#25991;&#26723;&#20998;&#31867;&#30340;&#35780;&#20272;&#21327;&#35758;&#26469;&#26377;&#25928;&#22320;&#35782;&#21035;&#25991;&#26723;&#32423;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;&#20004;&#20010;&#38750;&#24120;&#19981;&#21516;&#30340;&#25991;&#26723;&#32423;&#32763;&#35793;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#27492;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#24378;&#22823;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.12959</link><description>&lt;p&gt;
&#36867;&#31163;&#26426;&#22120;&#32763;&#35793;&#20013;&#21477;&#23376;&#32423;&#33539;&#24335;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Escaping the sentence-level paradigm in machine translation. (arXiv:2304.12959v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25670;&#33073;&#26426;&#22120;&#32763;&#35793;&#20013;&#21477;&#23376;&#32423;&#33539;&#24335;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22788;&#29702;&#19977;&#20010;&#38556;&#30861;&#26469;&#23454;&#29616;&#65306;&#20351;&#29992;&#36275;&#22815;&#22823;&#30340;&#26631;&#20934;Transformer&#26550;&#26500;&#12289;&#24341;&#20837;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25216;&#26415;&#26469;&#23558;&#25991;&#26723;&#32423;&#20449;&#24687;&#36716;&#21270;&#20026;&#36866;&#21512;&#35757;&#32451;&#30340;&#24418;&#24335;&#12289;&#22522;&#20110;&#33258;&#21160;&#25991;&#26723;&#20998;&#31867;&#30340;&#35780;&#20272;&#21327;&#35758;&#26469;&#26377;&#25928;&#22320;&#35782;&#21035;&#25991;&#26723;&#32423;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;&#20004;&#20010;&#38750;&#24120;&#19981;&#21516;&#30340;&#25991;&#26723;&#32423;&#32763;&#35793;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#27492;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;Transformer&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#24378;&#22823;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#25991;&#26723;&#35821;&#22659;&#23545;&#20110;&#35299;&#20915;&#19968;&#31995;&#21015;&#32763;&#35793;&#27169;&#31946;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20107;&#23454;&#19978;&#65292;&#25991;&#26723;&#35774;&#32622;&#20960;&#20046;&#26159;&#25152;&#26377;&#32763;&#35793;&#30340;&#33258;&#28982;&#35774;&#32622;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#32763;&#35793;&#65288;&#21253;&#25324;&#30740;&#31350;&#21644;&#29983;&#20135;&#65289;&#22312;&#20960;&#21313;&#24180;&#21069;&#30340;&#21477;&#23376;&#32423;&#32763;&#35793;&#33539;&#24335;&#20013;&#20173;&#28982;&#20572;&#28382;&#19981;&#21069;&#65292;&#36825;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#30001;&#20110;&#26469;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31454;&#20105;&#21387;&#21147;&#65292;&#36825;&#20123;&#27169;&#22411;&#22825;&#29983;&#23601;&#26159;&#22522;&#20110;&#25991;&#26723;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25670;&#33073;&#36825;&#31181;&#22256;&#22659;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#19977;&#20010;&#38556;&#30861;&#65306;&#25105;&#20204;&#24212;&#35813;&#20351;&#29992;&#20160;&#20040;&#26550;&#26500;&#65311;&#25105;&#20204;&#20174;&#21738;&#37324;&#33719;&#21462;&#35757;&#32451;&#23427;&#20204;&#30340;&#25991;&#26723;&#32423;&#20449;&#24687;&#65311;&#20197;&#21450;&#25105;&#20204;&#22914;&#20309;&#30693;&#36947;&#23427;&#20204;&#26159;&#21542;&#36275;&#22815;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
It is well-known that document context is vital for resolving a range of translation ambiguities, and in fact the document setting is the most natural setting for nearly all translation. It is therefore unfortunate that machine translation -- both research and production -- largely remains stuck in a decades-old sentence-level translation paradigm. It is also an increasingly glaring problem in light of competitive pressure from large language models, which are natively document-based. Much work in document-context machine translation exists, but for various reasons has been unable to catch hold. This paper suggests a path out of this rut by addressing three impediments at once: what architectures should we use? where do we get document-level information for training them? and how do we know whether they are any good? In contrast to work on specialized architectures, we show that the standard Transformer architecture is sufficient, provided it has enough capacity. Next, we address the t
&lt;/p&gt;</description></item></channel></rss>