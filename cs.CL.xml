<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#22810;&#26679;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#24212;&#29992;&#36827;&#23637;&#32531;&#24930;&#65292;LLMs&#36824;&#27809;&#26377;&#30495;&#27491;&#24443;&#24213;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.10070</link><description>&lt;p&gt;
ChatGPT&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health. (arXiv:2306.10070v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#22810;&#26679;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#24212;&#29992;&#36827;&#23637;&#32531;&#24930;&#65292;LLMs&#36824;&#27809;&#26377;&#30495;&#27491;&#24443;&#24213;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#20844;&#20247;&#21644;&#39046;&#22495;&#19987;&#23478;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#20135;&#29983;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#22810;&#26679;&#24212;&#29992;&#65292;&#20855;&#20307;&#25506;&#35752;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#12289;&#38382;&#31572;&#12289;&#21307;&#30103;&#25991;&#26412;&#25688;&#35201;&#12289;&#20449;&#24687;&#25277;&#21462;&#21644;&#21307;&#23398;&#25945;&#32946;&#31561;&#39046;&#22495;&#65292;&#24182;&#30740;&#31350;LLMs&#26159;&#21542;&#20855;&#26377;&#30495;&#27491;&#30340;&#36716;&#22411;&#21147;&#37327;&#20197;&#24443;&#24213;&#25913;&#21464;&#36825;&#20123;&#20219;&#21153;&#25110;&#32773;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29420;&#29305;&#22797;&#26434;&#24615;&#26159;&#21542;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#25991;&#29486;&#35843;&#30740;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#23545;&#20110;&#20854;&#20182;&#24212;&#29992;&#65292;&#36827;&#23637;&#36824;&#27604;&#36739;&#32531;&#24930;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;LLMs&#36824;&#27809;&#26377;&#24443;&#24213;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has drawn considerable attention from both the general public and domain experts with its remarkable text generation capabilities. This has subsequently led to the emergence of diverse applications in the field of biomedicine and health. In this work, we examine the diverse applications of large language models (LLMs), such as ChatGPT, in biomedicine and health. Specifically we explore the areas of biomedical information retrieval, question answering, medical text summarization, information extraction, and medical education, and investigate whether LLMs possess the transformative power to revolutionize these tasks or whether the distinct complexities of biomedical domain presents unique challenges. Following an extensive literature survey, we find that significant advances have been made in the field of text generation tasks, surpassing the previous state-of-the-art methods. For other applications, the advances have been modest. Overall, LLMs have not yet revolutionized the bio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#28436;&#31034;&#22914;&#20309;&#21033;&#29992;&#29616;&#26377;&#26041;&#27861;&#21644;&#36719;&#20214;&#24037;&#20855;&#32467;&#21512;&#23884;&#20837;&#25216;&#26415;&#35774;&#35745;&#38754;&#21521;&#31185;&#23398;&#39046;&#22495;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#35813;&#26426;&#22120;&#20154;&#33021;&#22815;&#22788;&#29702;&#31185;&#23398;&#25991;&#29486;&#65292;&#25552;&#20379;&#29305;&#23450;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#22312;&#21021;&#27493;&#30740;&#31350;&#36741;&#21161;&#30693;&#35782;&#26041;&#38754;&#20026;&#29289;&#29702;&#31185;&#23398;&#23478;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2306.10067</link><description>&lt;p&gt;
&#21033;&#29992;&#23884;&#20837;&#25216;&#26415;&#35774;&#35745;&#38754;&#21521;&#31185;&#23398;&#39046;&#22495;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Domain-specific ChatBots for Science using Embeddings. (arXiv:2306.10067v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#28436;&#31034;&#22914;&#20309;&#21033;&#29992;&#29616;&#26377;&#26041;&#27861;&#21644;&#36719;&#20214;&#24037;&#20855;&#32467;&#21512;&#23884;&#20837;&#25216;&#26415;&#35774;&#35745;&#38754;&#21521;&#31185;&#23398;&#39046;&#22495;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#35813;&#26426;&#22120;&#20154;&#33021;&#22815;&#22788;&#29702;&#31185;&#23398;&#25991;&#29486;&#65292;&#25552;&#20379;&#29305;&#23450;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#22312;&#21021;&#27493;&#30740;&#31350;&#36741;&#21161;&#30693;&#35782;&#26041;&#38754;&#20026;&#29289;&#29702;&#31185;&#23398;&#23478;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#25104;&#20026;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#33021;&#22788;&#29702;&#22810;&#31181;&#20219;&#21153;&#12290;&#32463;&#35843;&#25972;&#30340;&#36825;&#20123;&#31995;&#32479;&#24050;&#34987;&#36716;&#21270;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#33021;&#22238;&#31572;&#29992;&#25143;&#23545;&#24191;&#27867;&#35805;&#39064;&#30340;&#26597;&#35810;&#65292;&#25552;&#20379;&#20016;&#23500;&#30340;&#20449;&#24687;&#21644;&#21019;&#24847;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#22312;&#33258;&#28982;&#31185;&#23398;&#39046;&#22495;&#30340;&#30693;&#35782;&#20173;&#19981;&#23436;&#25972;&#65292;&#24182;&#19988;&#38754;&#20020;&#20005;&#26684;&#38656;&#27714;&#21644;&#26469;&#28304;&#26631;&#20934;&#65292;&#22240;&#27492;&#20854;&#22312;&#29289;&#29702;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#20173;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#36731;&#26494;&#22320;&#23558;&#29616;&#26377;&#26041;&#27861;&#21644;&#36719;&#20214;&#24037;&#20855;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#35813;&#31995;&#32479;&#33021;&#25509;&#21463;&#29616;&#26377;&#26684;&#24335;&#30340;&#31185;&#23398;&#25991;&#29486;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#26597;&#25214;&#26469;&#20026;LLM&#25552;&#20379;&#29305;&#23450;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#20415;&#22312;&#25776;&#20889;&#22238;&#31572;&#26102;&#20351;&#29992;&#12290;&#25105;&#20204;&#21516;&#26679;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#22270;&#20687;&#23884;&#20837;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#36328;&#20986;&#29256;&#29289;&#22270;&#29255;&#30340;&#25628;&#32034;&#21644;&#26816;&#32034;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25552;&#20379;&#21021;&#27493;&#30740;&#31350;&#36741;&#21161;&#30693;&#35782;&#26041;&#38754;&#65292;LLM&#24050;&#32463;&#36866;&#29992;&#20110;&#29289;&#29702;&#31185;&#23398;&#23478;&#30340;&#20351;&#29992;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#30340;&#24320;&#21457;&#21487;&#20197;&#25193;&#23637;&#36825;&#20123;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as powerful machine-learning systems capable of handling a myriad of tasks. Tuned versions of these systems have been turned into chatbots that can respond to user queries on a vast diversity of topics, providing informative and creative replies. However, their application to physical science research remains limited owing to their incomplete knowledge in these areas, contrasted with the needs of rigor and sourcing in science domains. Here, we demonstrate how existing methods and software tools can be easily combined to yield a domain-specific chatbot. The system ingests scientific documents in existing formats, and uses text embedding lookup to provide the LLM with domain-specific contextual information when composing its reply. We similarly demonstrate that existing image embedding methods can be used for search and retrieval across publication figures. These results confirm that LLMs are already suitable for use by physical scientists in acc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#32467;&#26500;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#19981;&#26159;&#21333;&#19968;&#33021;&#21147;&#65292;&#32780;&#26159;&#30001;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#26680;&#24515;&#35821;&#35328;&#24314;&#27169;&#31561;&#19977;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#32032;&#32452;&#25104;&#65292;&#24182;&#19988;&#36825;&#19977;&#20010;&#33021;&#21147;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#20013;&#30340;&#22823;&#37096;&#20998;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.10062</link><description>&lt;p&gt;
&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Revealing the structure of language model capabilities. (arXiv:2306.10062v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#32467;&#26500;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#19981;&#26159;&#21333;&#19968;&#33021;&#21147;&#65292;&#32780;&#26159;&#30001;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#26680;&#24515;&#35821;&#35328;&#24314;&#27169;&#31561;&#19977;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#32032;&#32452;&#25104;&#65292;&#24182;&#19988;&#36825;&#19977;&#20010;&#33021;&#21147;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#20013;&#30340;&#22823;&#37096;&#20998;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#29702;&#35770;&#29702;&#35299;&#23545;&#20110;&#25105;&#20204;&#39044;&#27979;&#21644;&#35299;&#37322;&#36825;&#20123;&#31995;&#32479;&#30340;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#21508;&#31181;LLMs&#30340;&#20010;&#20307;&#24046;&#24322;&#27169;&#24335;&#20013;&#25552;&#21462;&#28508;&#22312;&#33021;&#21147;&#26469;&#35843;&#26597;LLMs&#33021;&#21147;&#30340;&#32467;&#26500;&#12290;&#20351;&#29992;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#22240;&#23376;&#20998;&#26512;&#30340;&#32452;&#21512;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26469;&#33258;29&#20010;&#19981;&#21516;LLMs&#30340;27&#31181;&#35748;&#30693;&#20219;&#21153;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLMs&#33021;&#21147;&#24182;&#38750;&#21333;&#19968;&#30340;&#65292;&#30456;&#21453;&#65292;&#23427;&#20204;&#26356;&#22909;&#22320;&#30001;&#19977;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#32032;&#35299;&#37322;&#65292;&#20998;&#21035;&#20195;&#34920;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#26680;&#24515;&#35821;&#35328;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#19977;&#20010;&#22240;&#32032;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#20013;&#30340;&#39640;&#27604;&#20363;&#26041;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#19981;&#21516;LLMs&#33021;&#21147;&#30340;&#19968;&#33268;&#32467;&#26500;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#33021;&#21147;&#30340;&#22810;&#26041;&#38754;&#24615;&#36136;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#36825;&#19977;&#20010;&#21151;&#33021;&#19982;&#27169;&#22411;&#23646;&#24615;&#20855;&#26377;&#19981;&#21516;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building a theoretical understanding of the capabilities of large language models (LLMs) is vital for our ability to predict and explain the behavior of these systems. Here, we investigate the structure of LLM capabilities by extracting latent capabilities from patterns of individual differences across a varied population of LLMs. Using a combination of Bayesian and frequentist factor analysis, we analyzed data from 29 different LLMs across 27 cognitive tasks. We found evidence that LLM capabilities are not monolithic. Instead, they are better explained by three well-delineated factors that represent reasoning, comprehension and core language modeling. Moreover, we found that these three factors can explain a high proportion of the variance in model performance. These results reveal a consistent structure in the capabilities of different LLMs and demonstrate the multifaceted nature of these capabilities. We also found that the three abilities show different relationships to model prope
&lt;/p&gt;</description></item><item><title>EM-Network&#26159;&#19968;&#31181;&#33258;&#25105;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#35861;&#25351;&#23548;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30446;&#26631;&#20449;&#24687;&#36827;&#34892;&#30417;&#30563;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#65292;&#24182;&#22312;&#35821;&#38899;&#35782;&#21035;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.10058</link><description>&lt;p&gt;
EM-Network: &#29992;&#20110;&#24207;&#21015;&#23398;&#20064;&#30340;&#33258;&#36523;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EM-Network: Oracle Guided Self-distillation for Sequence Learning. (arXiv:2306.10058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10058
&lt;/p&gt;
&lt;p&gt;
EM-Network&#26159;&#19968;&#31181;&#33258;&#25105;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#35861;&#25351;&#23548;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#30446;&#26631;&#20449;&#24687;&#36827;&#34892;&#30417;&#30563;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#65292;&#24182;&#22312;&#35821;&#38899;&#35782;&#21035;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;EM-Network&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#26377;&#25928;&#21033;&#29992;&#30446;&#26631;&#20449;&#24687;&#36827;&#34892;&#30417;&#30563;&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;seq2seq&#65289;&#23398;&#20064;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#26159;&#22312;&#26469;&#33258;&#30446;&#26631;&#24207;&#21015;&#30340;&#8220;&#31070;&#35861;&#25351;&#23548;&#8221;&#19979;&#35757;&#32451;&#30340;&#12290;&#30001;&#20110;&#31070;&#35861;&#25351;&#23548;&#32039;&#20945;&#22320;&#34920;&#31034;&#20102;&#30446;&#26631;&#26041;&#38754;&#30340;&#19978;&#19979;&#25991;&#65292;&#21487;&#20197;&#24110;&#21161;&#24207;&#21015;&#27169;&#22411;&#35299;&#20915;&#20219;&#21153;&#65292;&#22240;&#27492;&#19982;&#20165;&#20351;&#29992;&#28304;&#36755;&#20837;&#30456;&#27604;&#65292;EM-Network&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#20351;&#24207;&#21015;&#27169;&#22411;&#32487;&#25215;EM-Network&#30340;&#26377;&#21069;&#36884;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#33976;&#39311;&#31574;&#30053;&#65292;&#21407;&#22987;&#24207;&#21015;&#27169;&#22411;&#21487;&#20197;&#22312;&#19968;&#20010;&#38454;&#27573;&#20013;&#20174;EM-Network&#30340;&#30693;&#35782;&#20013;&#33719;&#30410;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;seq2seq&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#32508;&#21512;&#23454;&#39564;&#65306;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#21644;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;AED&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EM-Network&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce EM-Network, a novel self-distillation approach that effectively leverages target information for supervised sequence-to-sequence (seq2seq) learning. In contrast to conventional methods, it is trained with oracle guidance, which is derived from the target sequence. Since the oracle guidance compactly represents the target-side context that can assist the sequence model in solving the task, the EM-Network achieves a better prediction compared to using only the source input. To allow the sequence model to inherit the promising capability of the EM-Network, we propose a new self-distillation strategy, where the original sequence model can benefit from the knowledge of the EM-Network in a one-stage manner. We conduct comprehensive experiments on two types of seq2seq models: connectionist temporal classification (CTC) for speech recognition and attention-based encoder-decoder (AED) for machine translation. Experimental results demonstrate that the EM-Network significantly advanc
&lt;/p&gt;</description></item><item><title>GUR&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#23558;&#35821;&#35328;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#32467;&#21512;&#22312;&#21333;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#65292;&#36890;&#36807;&#20174;&#21407;&#22987;&#26080;&#26631;&#31614;&#25991;&#26723;&#20013;&#36873;&#25321;&#30456;&#20284;&#30340;&#25991;&#26412;&#23545;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#21363;&#21487;&#20316;&#20026;&#26816;&#32034;&#22120;&#36229;&#36807;&#20854;&#20182;&#39044;&#35757;&#32451;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.10056</link><description>&lt;p&gt;
&#20026;&#20102;&#34920;&#31034;&#32780;&#29983;&#25104;&#8212;&#8212;&#19968;&#31181;&#32467;&#21512;&#23545;&#27604;&#23398;&#20064;&#30340;&#35821;&#35328;&#39044;&#35757;&#32451;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Generate to Understand for Representation. (arXiv:2306.10056v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10056
&lt;/p&gt;
&lt;p&gt;
GUR&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#23558;&#35821;&#35328;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#32467;&#21512;&#22312;&#21333;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#65292;&#36890;&#36807;&#20174;&#21407;&#22987;&#26080;&#26631;&#31614;&#25991;&#26723;&#20013;&#36873;&#25321;&#30456;&#20284;&#30340;&#25991;&#26412;&#23545;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#26080;&#38656;&#20219;&#20309;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#21363;&#21487;&#20316;&#20026;&#26816;&#32034;&#22120;&#36229;&#36807;&#20854;&#20182;&#39044;&#35757;&#32451;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#28044;&#29616;&#20102;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#21644;&#25991;&#26412;&#34920;&#31034;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#19978;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#36827;&#34892;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36825;&#23548;&#33268;&#20102;&#39640;&#26114;&#30340;GPU&#20351;&#29992;&#21644;&#21171;&#21160;&#21147;&#25104;&#26412;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;GUR&#65306;&#19968;&#31181;&#23558;&#35821;&#35328;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#32467;&#21512;&#22312;&#21333;&#20010;&#35757;&#32451;&#27493;&#39588;&#20013;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#20174;&#21407;&#22987;&#30340;&#26080;&#26631;&#31614;&#25991;&#26723;&#20013;&#22522;&#20110;&#26368;&#38271;&#20844;&#20849;&#23376;&#23383;&#31526;&#20018;&#65288;LCS&#65289;&#36873;&#25321;&#30456;&#20284;&#30340;&#25991;&#26412;&#23545;&#65292;&#24182;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#21644;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GUR&#27169;&#22411;&#22312;&#27809;&#26377;&#20219;&#20309;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20316;&#20026;&#26816;&#32034;&#22120;&#36229;&#36807;&#20102;&#25152;&#26377;&#20854;&#20182;&#39044;&#35757;&#32451;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, a significant number of high-quality pretrained models have emerged, greatly impacting Natural Language Understanding (NLU), Natural Language Generation (NLG), and Text Representation tasks. Traditionally, these models are pretrained on custom domain corpora and finetuned for specific tasks, resulting in high costs related to GPU usage and labor. Unfortunately, recent trends in language modeling have shifted towards enhancing performance through scaling, further exacerbating the associated costs.  Introducing GUR: a pretraining framework that combines language modeling and contrastive learning objectives in a single training step. We select similar text pairs based on their Longest Common Substring (LCS) from raw unlabeled documents and train the model using masked language modeling and unsupervised contrastive learning. The resulting model, GUR, achieves impressive results without any labeled training data, outperforming all other pretrained baselines as a retriever a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#23558;&#23454;&#20307;&#38142;&#25509;&#21040;&#30693;&#35782;&#24211;&#20013;&#30340;&#36890;&#29992;&#31995;&#32479;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#38142;&#25509;&#29305;&#23450;&#39046;&#22495;&#30340;&#23454;&#20307;&#65292;&#29305;&#21035;&#26159; COVID-19 &#30456;&#20851;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#23884;&#20837;&#24335;&#23454;&#20307;&#12290;&#36890;&#36807;&#21033;&#29992;&#34920;&#26684;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#25972;&#20307;&#23454;&#20307;&#38142;&#25509;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10044</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#31185;&#23398;&#25991;&#29486;&#20013;&#34920;&#26684;&#23454;&#20307;&#38142;&#25509;&#30340;&#23454;&#29992;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Practical Entity Linking System for Tables in Scientific Literature. (arXiv:2306.10044v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#23558;&#23454;&#20307;&#38142;&#25509;&#21040;&#30693;&#35782;&#24211;&#20013;&#30340;&#36890;&#29992;&#31995;&#32479;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#38142;&#25509;&#29305;&#23450;&#39046;&#22495;&#30340;&#23454;&#20307;&#65292;&#29305;&#21035;&#26159; COVID-19 &#30456;&#20851;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#23884;&#20837;&#24335;&#23454;&#20307;&#12290;&#36890;&#36807;&#21033;&#29992;&#34920;&#26684;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#25972;&#20307;&#23454;&#20307;&#38142;&#25509;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#26159;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#37325;&#35201;&#27493;&#39588;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#22238;&#31572;&#21253;&#25324;&#20174;&#36825;&#20123;&#25991;&#26723;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#22312;&#20869;&#30340;&#39640;&#32423;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#23558;&#23454;&#20307;&#38142;&#25509;&#21040;&#32500;&#22522;&#25968;&#25454;&#30693;&#35782;&#24211;&#20013;&#30340;&#39033;&#12290;&#23427;&#25551;&#36848;&#20102;&#22914;&#20309;&#36866;&#24212;&#35813;&#31995;&#32479;&#20197;&#38142;&#25509;&#39046;&#22495;&#29305;&#23450;&#30340;&#23454;&#20307;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#26469;&#33258;COVID-19&#30456;&#20851;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#23884;&#20837;&#24335;&#23454;&#20307;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#31995;&#32479;&#30340;&#31163;&#32447;&#23454;&#20363;&#30340;&#35774;&#32622;&#65292;&#20351;&#25105;&#20204;&#30340;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#26356;&#21152;&#21487;&#34892;&#12290;&#20316;&#20026;&#25512;&#26029;&#31185;&#23398;&#34920;&#26684;&#30340;&#35821;&#20041;&#21547;&#20041;&#30340;&#26356;&#24191;&#27867;&#26041;&#27861;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#21033;&#29992;&#34920;&#26684;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#26469;&#25552;&#39640;&#25972;&#20307;&#23454;&#20307;&#38142;&#25509;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity linking is an important step towards constructing knowledge graphs that facilitate advanced question answering over scientific documents, including the retrieval of relevant information included in tables within these documents. This paper introduces a general-purpose system for linking entities to items in the Wikidata knowledge base. It describes how we adapt this system for linking domain-specific entities, especially for those entities embedded within tables drawn from COVID-19-related scientific literature. We describe the setup of an efficient offline instance of the system that enables our entity-linking approach to be more feasible in practice. As part of a broader approach to infer the semantic meaning of scientific tables, we leverage the structural and semantic characteristics of the tables to improve overall entity linking performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37197;&#23545;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23558;&#26041;&#38754;-&#24847;&#35265;&#37197;&#23545;&#30693;&#35782;&#27880;&#20837;&#21040;Aspect Sentiment Triplet Extraction&#65288;ASTE&#65289;&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#20102;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10042</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;Aspect Sentiment Triplet Extraction&#30340;&#37197;&#23545;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Pairing Enhancement Approach for Aspect Sentiment Triplet Extraction. (arXiv:2306.10042v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37197;&#23545;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23558;&#26041;&#38754;-&#24847;&#35265;&#37197;&#23545;&#30693;&#35782;&#27880;&#20837;&#21040;Aspect Sentiment Triplet Extraction&#65288;ASTE&#65289;&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#20102;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Aspect Sentiment Triplet Extraction&#65288;ASTE&#65289;&#26088;&#22312;&#20174;&#35780;&#35770;&#25991;&#26412;&#20013;&#25552;&#21462;&#19968;&#20010;&#26041;&#38754;&#26415;&#35821;&#12289;&#19968;&#20010;&#24847;&#35265;&#26415;&#35821;&#21644;&#23427;&#20204;&#30456;&#24212;&#30340;&#24773;&#24863;&#26497;&#24615;&#30340;&#19977;&#20803;&#32452;&#12290;&#30001;&#20110;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#21644;&#21333;&#20010;&#21477;&#23376;&#20013;&#23384;&#22312;&#22810;&#20010;&#26041;&#38754;&#26415;&#35821;&#21644;&#24847;&#35265;&#26415;&#35821;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#32463;&#24120;&#20250;&#28151;&#28102;&#25551;&#36848;&#23427;&#30340;&#26041;&#38754;&#26415;&#35821;&#21644;&#24847;&#35265;&#26415;&#35821;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37197;&#23545;&#22686;&#24378;&#26041;&#27861;&#65292;&#23427;&#22312;&#35757;&#32451;&#38454;&#27573;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#65292;&#23558;&#26041;&#38754;-&#24847;&#35265;&#37197;&#23545;&#30693;&#35782;&#27880;&#20837;&#21040;&#19977;&#20803;&#32452;&#25552;&#21462;&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20960;&#31181;&#30456;&#20851;&#32463;&#20856;&#21644;&#26368;&#20808;&#36827;&#30340;&#19977;&#20803;&#32452;&#25552;&#21462;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;ASTE&#25968;&#25454;&#38598;&#65288;&#21363;14lap&#65292;14res&#65292;15res&#21644;16res&#65289;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#27492;&#22806;&#65292;&#28040;&#34701;&#30740;&#31350;&#36827;&#34892;&#20998;&#26512;&#24182;&#39564;&#35777;&#20102;&#23545;&#27604;&#23398;&#20064;&#30456;&#27604;&#20854;&#20182;&#37197;&#23545;&#22686;&#24378;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect Sentiment Triplet Extraction (ASTE) aims to extract the triplet of an aspect term, an opinion term, and their corresponding sentiment polarity from the review texts. Due to the complexity of language and the existence of multiple aspect terms and opinion terms in a single sentence, current models often confuse the connections between an aspect term and the opinion term describing it. To address this issue, we propose a pairing enhancement approach for ASTE, which incorporates contrastive learning during the training stage to inject aspect-opinion pairing knowledge into the triplet extraction model. Experimental results demonstrate that our approach performs well on four ASTE datasets (i.e., 14lap, 14res, 15res and 16res) compared to several related classical and state-of-the-art triplet extraction methods. Moreover, ablation studies conduct an analysis and verify the advantage of contrastive learning over other pairing enhancement approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.09896</link><description>&lt;p&gt;
&#25581;&#31192; GPT &#33258;&#25105;&#20462;&#22797;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#32534;&#31243;&#20219;&#21153;&#19978;&#20173;&#38754;&#20020;&#22256;&#38590;&#12290;&#33258;&#25105;&#20462;&#22797;&#8212;&#8212;&#21363;&#27169;&#22411;&#35843;&#35797;&#24182;&#20462;&#22797;&#33258;&#24049;&#30340;&#20195;&#30721;&#8212;&#8212;&#26368;&#36817;&#25104;&#20026;&#25552;&#39640;&#24615;&#33021;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#33258;&#25105;&#20462;&#22797;&#22914;&#20309;&#26377;&#25928;&#22320;&#21457;&#25381;&#20316;&#29992;&#30340;&#30740;&#31350;&#36824;&#38750;&#24120;&#26377;&#38480;&#12290;&#26377;&#20154;&#20250;&#24819;&#30693;&#36947;&#65292;&#24403;&#21516;&#19968;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#26102;&#65292;&#27169;&#22411;&#31350;&#31455;&#33021;&#21542;&#25552;&#20379;&#20934;&#30830;&#30340;&#21453;&#39304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#22810;&#31181;&#32534;&#30721;&#25361;&#25112;&#32452;&#25104;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#31574;&#30053; pass@t&#65292;&#35813;&#31574;&#30053;&#34913;&#37327;&#20102;&#20219;&#21153;&#36890;&#36807;&#29575;&#19982;&#20174;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#24635;&#26631;&#35760;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20165;&#22522;&#20110;&#25277;&#26679;&#30340;&#26041;&#27861;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;&#36890;&#36807;&#36825;&#31181;&#35780;&#20272;&#31574;&#30053;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#24433;&#21709;&#33258;&#25105;&#20462;&#22797;&#34920;&#29616;&#30340;&#20960;&#20010;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36755;&#20837;&#22122;&#22768;&#36739;&#23569;&#19988;&#27169;&#22411;&#23545;&#21021;&#22987;&#36755;&#20986;&#19981;&#22826;&#33258;&#20449;&#30340;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#65292;&#33258;&#25105;&#20462;&#22797;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#21453;&#39304;&#26469;&#22686;&#24378; GPT &#27169;&#22411;&#30340;&#33258;&#25105;&#20462;&#22797;&#33021;&#21147;&#65292;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable aptitude in code generation but still struggle on challenging programming tasks. Self-repair -in which the model debugs and fixes mistakes in its own code -- has recently become a popular way to boost performance in these settings. However, only very limited studies on how and when self-repair works effectively exist in the literature, and one might wonder to what extent a model is really capable of providing accurate feedback on why the code is wrong when that code was generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's ability to perform self-repair on APPS, a challenging dataset consisting of diverse coding challenges. To do so, we first establish a new evaluation strategy dubbed pass@t that measures the pass rate of the tasks against the total number of tokens sampled from the model, enabling a fair comparison to purely sampling-based approaches. With this evaluation strategy, we find that the effectiveness
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#26159;&#33258;&#21160;&#26631;&#27880;&#30340;SRED$^{\rm FM}$&#21644;&#20154;&#24037;&#20462;&#35746;&#30340;RED$^{\rm FM}$&#12290;SRED$^{\rm FM}$&#28085;&#30422;&#20102;18&#31181;&#35821;&#35328;&#12289;400&#31181;&#20851;&#31995;&#31867;&#22411;&#12289;13&#31181;&#23454;&#20307;&#31867;&#22411;&#65292;&#24635;&#20849;&#36229;&#36807;4000&#19975;&#20010;&#19977;&#20803;&#32452;&#23454;&#20363;&#65307;RED$^{\rm FM}$&#26159;RED&#30340;&#31934;&#31616;&#29256;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26032;&#25968;&#25454;&#38598;&#33021;&#26377;&#25928;&#29992;&#20110;&#24314;&#31435;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.09802</link><description>&lt;p&gt;
RED$^{\rm FM}$&#65306;&#19968;&#20010;&#32463;&#36807;&#28388;&#27874;&#21644;&#22810;&#35821;&#35328;&#22788;&#29702;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RED$^{\rm FM}$: a Filtered and Multilingual Relation Extraction Dataset. (arXiv:2306.09802v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#26159;&#33258;&#21160;&#26631;&#27880;&#30340;SRED$^{\rm FM}$&#21644;&#20154;&#24037;&#20462;&#35746;&#30340;RED$^{\rm FM}$&#12290;SRED$^{\rm FM}$&#28085;&#30422;&#20102;18&#31181;&#35821;&#35328;&#12289;400&#31181;&#20851;&#31995;&#31867;&#22411;&#12289;13&#31181;&#23454;&#20307;&#31867;&#22411;&#65292;&#24635;&#20849;&#36229;&#36807;4000&#19975;&#20010;&#19977;&#20803;&#32452;&#23454;&#20363;&#65307;RED$^{\rm FM}$&#26159;RED&#30340;&#31934;&#31616;&#29256;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26032;&#25968;&#25454;&#38598;&#33021;&#26377;&#25928;&#29992;&#20110;&#24314;&#31435;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#26088;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#33719;&#21462;&#20851;&#31995;&#20107;&#23454;&#65292;&#24357;&#21512;&#33258;&#28982;&#35821;&#35328;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#24448;&#24448;&#20381;&#36182;&#20110;&#23567;&#22411;&#25968;&#25454;&#38598;&#65292;&#23545;&#20110;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#20851;&#31995;&#31867;&#22411;&#35206;&#30422;&#29575;&#20063;&#36739;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#36164;&#28304;&#65292;&#21487;&#29992;&#20110;&#22521;&#35757;&#21644;&#35780;&#20272;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#12290;&#20854;&#19968;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#8212;&#8212;SRED$^{\rm FM}$&#65292;&#28085;&#30422;&#20102;18&#31181;&#35821;&#35328;&#12289;400&#31181;&#20851;&#31995;&#31867;&#22411;&#12289;13&#31181;&#23454;&#20307;&#31867;&#22411;&#65292;&#24635;&#20849;&#36229;&#36807;4000&#19975;&#20010;&#19977;&#20803;&#32452;&#23454;&#20363;&#12290;&#20854;&#20108;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#20154;&#24037;&#20462;&#35746;&#30340;&#12289;&#38024;&#23545;&#19971;&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#8212;&#8212;RED$^{\rm FM}$&#65292;&#21487;&#29992;&#20110;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#20123;&#26032;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;mREBEL&#36827;&#34892;&#23454;&#39564;&#65292;&#21253;&#25324;&#23454;&#20307;&#31867;&#22411;&#22312;&#20869;&#30340;&#19977;&#20803;&#32452;&#34987;&#25277;&#21462;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation Extraction (RE) is a task that identifies relationships between entities in a text, enabling the acquisition of relational facts and bridging the gap between natural language and structured knowledge. However, current RE models often rely on small datasets with low coverage of relation types, particularly when working with languages other than English. In this paper, we address the above issue and provide two new resources that enable the training and evaluation of multilingual RE systems. First, we present SRED$^{\rm FM}$, an automatically annotated dataset covering 18 languages, 400 relation types, 13 entity types, totaling more than 40 million triplet instances. Second, we propose RED$^{\rm FM}$, a smaller, human-revised dataset for seven languages that allows for the evaluation of multilingual RE systems. To demonstrate the utility of these novel datasets, we experiment with the first end-to-end multilingual RE model, mREBEL, that extracts triplets, including entity types,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;DiPlomat&#65292;&#29992;&#20110;&#35780;&#27979;&#26426;&#22120;&#30340;&#24773;&#22659;&#25512;&#29702;&#21644;&#23545;&#35805;&#29702;&#35299;&#33021;&#21147;&#12290;&#19982;&#20808;&#21069;&#24037;&#20316;&#30456;&#27604;&#65292;DiPlomat&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#23454;&#29616;&#19968;&#33324;&#30340;&#35821;&#29992;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36890;&#36807;&#21033;&#29992;&#20122;&#39532;&#36874;&#26426;&#26800;&#22303;&#32819;&#20854;&#35821;&#65288;AMT&#65289;&#26469;&#21019;&#24314;&#65292;&#20849;&#26377;4,177&#20010;&#22810;&#36718;&#23545;&#35805;&#12290;&#19982;&#25968;&#25454;&#38598;&#19968;&#36215;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20219;&#21153;&#65306;&#35821;&#29992;&#35782;&#21035;&#21644;&#25512;&#29702;&#65288;PIR&#65289;&#21644;&#20250;&#35805;&#38382;&#31572;&#65288;CQA&#65289;&#12290;</title><link>http://arxiv.org/abs/2306.09030</link><description>&lt;p&gt;
DiPlomat: &#29992;&#20110;&#24773;&#22659;&#35821;&#29992;&#25512;&#29702;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DiPlomat: A Dialogue Dataset for Situated Pragmatic Reasoning. (arXiv:2306.09030v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;DiPlomat&#65292;&#29992;&#20110;&#35780;&#27979;&#26426;&#22120;&#30340;&#24773;&#22659;&#25512;&#29702;&#21644;&#23545;&#35805;&#29702;&#35299;&#33021;&#21147;&#12290;&#19982;&#20808;&#21069;&#24037;&#20316;&#30456;&#27604;&#65292;DiPlomat&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#23454;&#29616;&#19968;&#33324;&#30340;&#35821;&#29992;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36890;&#36807;&#21033;&#29992;&#20122;&#39532;&#36874;&#26426;&#26800;&#22303;&#32819;&#20854;&#35821;&#65288;AMT&#65289;&#26469;&#21019;&#24314;&#65292;&#20849;&#26377;4,177&#20010;&#22810;&#36718;&#23545;&#35805;&#12290;&#19982;&#25968;&#25454;&#38598;&#19968;&#36215;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20219;&#21153;&#65306;&#35821;&#29992;&#35782;&#21035;&#21644;&#25512;&#29702;&#65288;PIR&#65289;&#21644;&#20250;&#35805;&#38382;&#31572;&#65288;CQA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#29992;&#25512;&#29702;&#22312;&#30772;&#35299;&#23454;&#38469;&#23545;&#35805;&#20013;&#32463;&#24120;&#20986;&#29616;&#30340;&#38544;&#21547;&#21547;&#20041;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#19988;&#23545;&#20110;&#21457;&#23637;&#20132;&#38469;&#31038;&#20250;&#20195;&#29702;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#8212;&#8212;DiPlomat&#65292;&#26088;&#22312;&#23545;&#26426;&#22120;&#30340;&#24773;&#22659;&#25512;&#29702;&#21644;&#23545;&#35805;&#29702;&#35299;&#33021;&#21147;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#19982;&#23558;&#19981;&#21516;&#30340;&#27604;&#21947;&#34920;&#36798;&#65288;&#20363;&#22914;&#27604;&#21947;&#12289;&#35773;&#21050;&#65289;&#35270;&#20026;&#21333;&#29420;&#20219;&#21153;&#30340;&#20808;&#21069;&#24037;&#20316;&#30456;&#27604;&#65292;DiPlomat&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#23454;&#29616;&#19968;&#33324;&#30340;&#35821;&#29992;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36890;&#36807;&#21033;&#29992;&#20122;&#39532;&#36874;&#26426;&#26800;&#22303;&#32819;&#20854;&#35821;&#65288;AMT&#65289;&#26469;&#21019;&#24314;&#65292;&#20849;&#26377;4,177&#20010;&#22810;&#36718;&#23545;&#35805;&#12290;&#19982;&#25968;&#25454;&#38598;&#19968;&#36215;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20219;&#21153;&#65306;&#35821;&#29992;&#35782;&#21035;&#21644;&#25512;&#29702;&#65288;PIR&#65289;&#21644;&#20250;&#35805;&#38382;&#31572;&#65288;CQA&#65289;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#65288;SOTA&#65289;&#31070;&#32463;&#26550;&#26500;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#25581;&#31034;&#20102;&#20960;&#20010;&#37325;&#35201;&#21457;&#29616;&#65306;1&#65289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pragmatic reasoning plays a pivotal role in deciphering implicit meanings that frequently arise in real-life conversations and is essential for the development of communicative social agents. In this paper, we introduce a novel challenge, DiPlomat, aiming at benchmarking machines' capabilities on pragmatic reasoning and situated conversational understanding. Compared with previous works that treat different figurative expressions (e.g. metaphor, sarcasm) as individual tasks, DiPlomat provides a cohesive framework towards general pragmatic understanding. Our dataset is created through the utilization of Amazon Mechanical Turk ( AMT ), resulting in a total of 4, 177 multi-turn dialogues. In conjunction with the dataset, we propose two tasks, Pragmatic Identification and Reasoning (PIR) and Conversational Question Answering (CQA). Experimental results with state-of-the-art (SOTA) neural architectures reveal several significant findings: 1) large language models ( LLMs) exhibit poor perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21069;&#30651;&#24615;&#30340;&#32479;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#36335;&#32447;&#22270;&#65292;&#36890;&#36807;&#19977;&#20010;&#26694;&#26550;&#65306;&#22686;&#24378;KGs&#30340;LLMs&#65292;&#30693;&#35782;&#22686;&#24378;KGs&#21644;LLMs&#19982;KGs&#30340;&#32852;&#21512;&#25512;&#29702;&#65292;&#32508;&#21512;&#21033;&#29992;&#20004;&#32773;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.08302</link><description>&lt;p&gt;
&#32479;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;: &#19968;&#26465;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Unifying Large Language Models and Knowledge Graphs: A Roadmap. (arXiv:2306.08302v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21069;&#30651;&#24615;&#30340;&#32479;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#36335;&#32447;&#22270;&#65292;&#36890;&#36807;&#19977;&#20010;&#26694;&#26550;&#65306;&#22686;&#24378;KGs&#30340;LLMs&#65292;&#30693;&#35782;&#22686;&#24378;KGs&#21644;LLMs&#19982;KGs&#30340;&#32852;&#21512;&#25512;&#29702;&#65292;&#32508;&#21512;&#21033;&#29992;&#20004;&#32773;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;GPT4&#27491;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#25472;&#36215;&#26032;&#30340;&#28909;&#28526;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#31361;&#29616;&#33021;&#21147;&#21644;&#19968;&#33324;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLM&#26159;&#40657;&#30418;&#27169;&#22411;&#65292;&#24448;&#24448;&#19981;&#33021;&#25429;&#25417;&#21644;&#33719;&#21462;&#23454;&#38469;&#30693;&#35782;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#21326;&#26222;&#21017;&#26159;&#26126;&#30830;&#23384;&#20648;&#20016;&#23500;&#23454;&#38469;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#27169;&#22411;&#12290;KGs&#21487;&#20197;&#36890;&#36807;&#20026;&#25512;&#29702;&#21644;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#22806;&#37096;&#30693;&#35782;&#26469;&#22686;&#24378;LLMs&#12290;&#21516;&#26102;&#65292;KGs&#30340;&#26500;&#24314;&#22256;&#38590;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#28436;&#21270;&#65292;&#36825;&#25361;&#25112;&#20102;&#29616;&#26377;&#30340;KGs&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#20107;&#23454;&#24182;&#34920;&#31034;&#26410;&#35265;&#36807;&#30340;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#32479;&#19968;LLMs&#21644;KGs&#24182;&#21516;&#26102;&#21033;&#29992;&#23427;&#20204;&#30340;&#20248;&#28857;&#26159;&#26377;&#30410;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21069;&#30651;&#24615;&#30340;&#32479;&#19968;LLMs&#21644;KGs&#30340;&#36335;&#32447;&#22270;&#12290;&#25105;&#20204;&#30340;&#36335;&#32447;&#22270;&#21253;&#25324;&#19977;&#20010;&#19968;&#33324;&#26694;&#26550;&#65292;&#21363;1&#65289;&#22686;&#24378;KGs&#30340;LLMs&#65292;&#23427;&#20204;&#23558;&#30693;&#35782;&#34920;&#31034;&#20026;LM&#30340;&#19968;&#37096;&#20998;&#65292;&#20174;&#32780;&#33021;&#22815;&#25429;&#25417;&#20016;&#23500;&#30340;&#23454;&#20307;&#20851;&#31995;&#65292;2&#65289;&#30693;&#35782;&#22686;&#24378;KGs&#65292;&#23427;&#20204;&#23558;LLMs&#29992;&#20316;&#30693;&#35782;&#34920;&#31034;&#23398;&#20064;&#30340;&#20248;&#31168;&#24037;&#20855;&#65292;3&#65289;LLMs&#19982;KGs&#30340;&#32852;&#21512;&#25512;&#29702;&#65292;&#20854;&#20013;LLMs&#21644;KGs&#30456;&#20114;&#22686;&#24378;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorpo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#23458;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;&#32452;&#21512;&#27169;&#22411;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#12289;&#21521;&#37327;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#24050;&#32463;&#38598;&#25104;&#24182;&#36827;&#19968;&#27493;&#21457;&#23637;&#65292;&#20197;&#26356;&#22909;&#22320;&#28385;&#36275;&#39640;&#25928;&#20449;&#24687;&#25552;&#21462;&#12289;&#25552;&#21462;&#20449;&#24687;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#29992;&#25143;&#38656;&#27714;&#30340;&#35201;&#27714;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26412;&#31995;&#32479;&#21487;&#20197;&#27604;&#29616;&#26377;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#23383;&#25552;&#21462;&#35299;&#20915;&#26041;&#26696;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07786</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20113;&#30340;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#26377;&#25928;&#20174;&#23458;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Cloud-based Machine Learning Pipeline for the Efficient Extraction of Insights from Customer Reviews. (arXiv:2306.07786v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#23458;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;&#32452;&#21512;&#27169;&#22411;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#12289;&#21521;&#37327;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#24050;&#32463;&#38598;&#25104;&#24182;&#36827;&#19968;&#27493;&#21457;&#23637;&#65292;&#20197;&#26356;&#22909;&#22320;&#28385;&#36275;&#39640;&#25928;&#20449;&#24687;&#25552;&#21462;&#12289;&#25552;&#21462;&#20449;&#24687;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#29992;&#25143;&#38656;&#27714;&#30340;&#35201;&#27714;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26412;&#31995;&#32479;&#21487;&#20197;&#27604;&#29616;&#26377;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#23383;&#25552;&#21462;&#35299;&#20915;&#26041;&#26696;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25928;&#29575;&#26377;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#29305;&#23450;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38598;&#25104;&#21040;&#31649;&#36947;&#20013;&#65292;&#20174;&#23458;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#35265;&#35299;&#12290;&#23545;&#20110;&#20027;&#39064;&#24314;&#27169;&#65292;&#25105;&#20204;&#30340;&#32452;&#21512;&#27169;&#22411;&#20351;&#29992;&#20102;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#12289;&#22522;&#20110;&#21521;&#37327;&#23884;&#20837;&#30340;&#20851;&#38190;&#23383;&#25552;&#21462;&#21644;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20803;&#32032;&#24050;&#32463;&#38598;&#25104;&#24182;&#36827;&#19968;&#27493;&#21457;&#23637;&#65292;&#20197;&#26356;&#22909;&#22320;&#28385;&#36275;&#39640;&#25928;&#20449;&#24687;&#25552;&#21462;&#12289;&#25552;&#21462;&#20449;&#24687;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#29992;&#25143;&#38656;&#27714;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#27604;&#36825;&#20010;&#20219;&#21153;&#29616;&#26377;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#23383;&#25552;&#21462;&#35299;&#20915;&#26041;&#26696;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#23458;&#25143;&#35780;&#35770;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#24182;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The efficiency of natural language processing has improved dramatically with the advent of machine learning models, particularly neural network-based solutions. However, some tasks are still challenging, especially when considering specific domains. In this paper, we present a cloud-based system that can extract insights from customer reviews using machine learning methods integrated into a pipeline. For topic modeling, our composite model uses transformer-based neural networks designed for natural language processing, vector embedding-based keyword extraction, and clustering. The elements of our model have been integrated and further developed to meet better the requirements of efficient information extraction, topic modeling of the extracted information, and user needs. Furthermore, our system can achieve better results than this task's existing topic modeling and keyword extraction solutions. Our approach is validated and compared with other state-of-the-art methods using publicly a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#25991;&#26412;&#20869;&#37096;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#20154;&#31867;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#22312;&#20869;&#37096;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.04723</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#20869;&#37096;&#32500;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. (arXiv:2306.04723v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#25991;&#26412;&#20869;&#37096;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#20154;&#31867;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#22312;&#20869;&#37096;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#25552;&#39640;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#20351;&#24471;&#24456;&#38590;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#31867;&#25991;&#26412;&#30340;&#19981;&#21464;&#23646;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#25991;&#26412;&#30340;&#19981;&#21464;&#29305;&#24449;&#65292;&#21363;&#32473;&#23450;&#25991;&#26412;&#26679;&#26412;&#23884;&#20837;&#38598;&#21512;&#19979;&#30340;&#27969;&#24418;&#30340;&#20869;&#37096;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#27969;&#30021;&#25991;&#26412;&#30340;&#24179;&#22343;&#20869;&#37096;&#32500;&#24230;&#22312;&#20960;&#20010;&#22522;&#20110;&#23383;&#27597;&#30340;&#35821;&#35328;&#20013;&#32422;&#20026; $9$&#65292;&#32780;&#20013;&#25991;&#32422;&#20026; $7$&#65292;&#32780;&#27599;&#31181;&#35821;&#35328;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#24179;&#22343;&#20869;&#37096;&#32500;&#24230;&#36739;&#20302;&#65292;&#24046;&#32422; $1.5$&#65292;&#24182;&#19988;&#26377;&#26126;&#26174;&#30340;&#32479;&#35745;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over text domains and various proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant of human texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings of a given text sample. We show that the average intrinsic dimensionality of fluent texts in natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\approx 1.5$ lower, with a clear statistical separation between
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#20010;&#21517;&#20026;SpeechGen&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#33410;&#65292;&#35299;&#38145;&#20102;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#30452;&#25509;&#36866;&#24212;&#36830;&#32493;&#35821;&#38899;&#21040;&#31163;&#25955;&#26631;&#35760;&#30340;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#38899;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02207</link><description>&lt;p&gt;
SpeechGen: &#21033;&#29992;&#25552;&#31034;&#35299;&#38145;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts. (arXiv:2306.02207v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#20010;&#21517;&#20026;SpeechGen&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#33410;&#65292;&#35299;&#38145;&#20102;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#30452;&#25509;&#36866;&#24212;&#36830;&#32493;&#35821;&#38899;&#21040;&#31163;&#25955;&#26631;&#35760;&#30340;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#38899;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#20013;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;ChatGPT&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#23558;&#36830;&#32493;&#35821;&#38899;&#30452;&#25509;&#36866;&#24212;&#20110;&#22788;&#29702;&#31163;&#25955;&#26631;&#35760;&#30340;LLM&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#36825;&#22952;&#30861;&#20102;LLM&#22312;&#35821;&#38899;&#29983;&#25104;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#39640;&#32423;&#35821;&#38899;LM&#20204;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#35821;&#38899;&#20449;&#21495;&#25152;&#21253;&#21547;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#21253;&#25324;&#35828;&#35805;&#32773;&#21644;&#24773;&#24863;&#31561;&#65292;&#36825;&#20123;&#20449;&#24687;&#20165;&#36890;&#36807;&#25991;&#26412;&#25968;&#25454;&#26080;&#27861;&#33719;&#21462;&#12290;&#22312;&#19968;&#20123;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#35843;&#25972;&#24050;&#32463;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#21442;&#25968;&#25928;&#29575;&#21644;&#31454;&#20105;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;&#20294;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#25552;&#31034;&#33021;&#22815;&#26377;&#25928;&#22320;&#28608;&#21457;&#35821;&#38899;LM&#30340;&#29983;&#25104;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#20808;&#39537;&#24615;&#30740;&#31350;&#65292;&#35813;&#30740;&#31350;&#22312;&#31216;&#20026;SpeechGen&#30340;&#32479;&#19968;&#26694;&#26550;&#20013;&#20351;&#29992;&#25552;&#31034;&#35843;&#33410;&#26469;&#21050;&#28608;&#35821;&#38899;LM&#36827;&#34892;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#20855;&#26377;&#32422;10M&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unif
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#21453;&#21521;&#20256;&#25773;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#8212;&#8212;&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65288;BT-Cell&#65289;&#65292;&#29992;&#20110;&#25193;&#23637;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#23545;&#28508;&#22312;&#32467;&#26500;&#30340;&#24863;&#30693;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#26463;&#25628;&#32034;&#20013;&#30828;&#21069;k&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26799;&#24230;&#20449;&#21495;&#20256;&#36882;&#12290;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#65292;BT-Cell&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#22810;&#20010;&#20855;&#26377;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.19999</link><description>&lt;p&gt;
&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65306;&#19968;&#31181;&#25903;&#25345;&#21453;&#21521;&#20256;&#25773;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Beam Tree Recursive Cells. (arXiv:2305.19999v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#21453;&#21521;&#20256;&#25773;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#8212;&#8212;&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65288;BT-Cell&#65289;&#65292;&#29992;&#20110;&#25193;&#23637;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#23545;&#28508;&#22312;&#32467;&#26500;&#30340;&#24863;&#30693;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#26463;&#25628;&#32034;&#20013;&#30828;&#21069;k&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26799;&#24230;&#20449;&#21495;&#20256;&#36882;&#12290;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#65292;BT-Cell&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#22810;&#20010;&#20855;&#26377;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65288;BT-Cell&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25193;&#23637;&#25903;&#25345;&#20351;&#29992;&#26463;&#25628;&#32034;&#36827;&#34892;&#28508;&#22312;&#32467;&#26500;&#24863;&#30693;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RvNN&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25552;&#20986;&#22312;&#26463;&#25628;&#32034;&#20013;&#23545;&#30828;&#24615;&#21069;k&#31639;&#23376;&#30340;&#25918;&#26494;&#26469;&#25193;&#23637;&#27492;&#26694;&#26550;&#65292;&#20197;&#26356;&#22909;&#22320;&#20256;&#36882;&#26799;&#24230;&#20449;&#21495;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#19981;&#21516;&#20195;&#34920;&#24615;&#20998;&#24067;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BT-Cell&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20307;&#29616;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#65288;&#22914;ListOps&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#19978;&#36798;&#21040;&#20102;&#20960;&#20046;&#23436;&#32654;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#19982;&#20854;&#20182;&#22522;&#20110;RvNN&#30340;&#27169;&#22411;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;ListOps&#20013;&#30830;&#23450;&#20102;&#31070;&#32463;&#27169;&#22411;&#22312;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#21442;&#25968;&#25968;&#37327;&#19978;&#30340;&#26410;&#30693;&#22833;&#25928;&#26696;&#20363;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/JRC1995/BeamTreeRecursiveCells&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Beam Tree Recursive Cell (BT-Cell) - a backpropagation-friendly framework to extend Recursive Neural Networks (RvNNs) with beam search for latent structure induction. We further extend this framework by proposing a relaxation of the hard top-k operators in beam search for better propagation of gradient signals. We evaluate our proposed models in different out-of-distribution splits in both synthetic and realistic data. Our experiments show that BTCell achieves near-perfect performance on several challenging structure-sensitive synthetic tasks like ListOps and logical inference while maintaining comparable performance in realistic data against other RvNN-based models. Additionally, we identify a previously unknown failure case for neural models in generalization to unseen number of arguments in ListOps. The code is available at: https://github.com/JRC1995/BeamTreeRecursiveCells.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-3&#29983;&#25104;&#30340;&#38024;&#23545;&#20167;&#24680;&#20869;&#23481;&#30340;&#35299;&#37322;&#26159;&#21542;&#20934;&#30830;&#21644;&#26377;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3&#29983;&#25104;&#30340;&#35299;&#37322;&#26222;&#36941;&#23384;&#22312;&#36807;&#20110;&#27169;&#31946;&#12289;&#32858;&#28966;&#19981;&#24403;&#31561;&#32570;&#28857;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#20167;&#24680;&#35328;&#35770;&#29983;&#25104;&#30340;&#35299;&#37322;&#36136;&#37327;&#24046;&#24322;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17680</link><description>&lt;p&gt;
&#35780;&#20272;GPT-3&#29983;&#25104;&#30340;&#20167;&#24680;&#20869;&#23481;&#23457;&#26680;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-3 Generated Explanations for Hateful Content Moderation. (arXiv:2305.17680v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-3&#29983;&#25104;&#30340;&#38024;&#23545;&#20167;&#24680;&#20869;&#23481;&#30340;&#35299;&#37322;&#26159;&#21542;&#20934;&#30830;&#21644;&#26377;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3&#29983;&#25104;&#30340;&#35299;&#37322;&#26222;&#36941;&#23384;&#22312;&#36807;&#20110;&#27169;&#31946;&#12289;&#32858;&#28966;&#19981;&#24403;&#31561;&#32570;&#28857;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#20167;&#24680;&#35328;&#35770;&#29983;&#25104;&#30340;&#35299;&#37322;&#36136;&#37327;&#24046;&#24322;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;Fine-tune&#25110;&#25552;&#31034;&#29983;&#25104;&#20167;&#24680;&#35328;&#35770;&#30340;&#35299;&#37322;&#12290;&#23613;&#31649;&#36825;&#20010;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20294;&#36825;&#20123;&#29983;&#25104;&#35299;&#37322;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#22312;&#38480;&#21046;&#20173;&#28982;&#19981;&#20026;&#20154;&#20204;&#25152;&#20102;&#35299;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#65292;&#30001;LLMs&#29983;&#25104;&#30340;&#36825;&#20123;&#35299;&#37322;&#21487;&#33021;&#20250;&#23548;&#33268;&#29992;&#25143;&#21644;&#20869;&#23481;&#23457;&#26680;&#21592;&#23545;&#26631;&#35760;&#20869;&#23481;&#26412;&#36136;&#20570;&#20986;&#38169;&#35823;&#21028;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#26469;&#26816;&#26597;&#20167;&#24680;&#35328;&#35770;&#35299;&#37322;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#35843;&#26597;&#26469;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;GPT-3&#19978;&#36755;&#20837;&#20167;&#24680;&#21644;&#38750;&#20167;&#24680;&#20869;&#23481;&#65292;&#21457;&#29616;&#21463;&#35843;&#26597;&#32773;&#22312;&#20154;&#24037;&#23457;&#26680;GPT&#29983;&#25104;&#30340;&#35299;&#37322;&#26102;&#65292;&#23558;&#20167;&#24680;&#35328;&#35770;&#35299;&#37322;&#35780;&#20215;&#20026;&#19981;&#22815;&#20934;&#30830;&#21644;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has focused on using large language models (LLMs) to generate explanations for hate speech through fine-tuning or prompting. Despite the growing interest in this area, these generated explanations' effectiveness and potential limitations remain poorly understood. A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators. For instance, an LLM-generated explanation might inaccurately convince a content moderator that a benign piece of content is hateful. In light of this, we propose an analytical framework for examining hate speech explanations and conducted an extensive survey on evaluating such explanations. Specifically, we prompted GPT-3 to generate explanations for both hateful and non-hateful content, and a survey was conducted with 2,400 unique respondents to evaluate the generated explanations. Our findings reveal that (1) human evaluators rated the GPT-gene
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#21360;&#24230;22&#31181;&#23448;&#26041;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#30340;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;IndicTrans2&#31995;&#32479;&#65292;&#23427;&#22312;&#22810;&#20010;&#35821;&#35328;&#23545;&#19978;&#34920;&#29616;&#26368;&#20808;&#36827;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21360;&#24230;&#35821;&#35328;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#33050;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.16307</link><description>&lt;p&gt;
IndicTrans2: &#20026;&#21360;&#24230;&#25152;&#26377;22&#31181;&#23448;&#26041;&#35821;&#35328;&#26500;&#24314;&#39640;&#36136;&#37327;&#21487;&#35775;&#38382;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IndicTrans2: Towards High-Quality and Accessible Machine Translation Models for all 22 Scheduled Indian Languages. (arXiv:2305.16307v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#21360;&#24230;22&#31181;&#23448;&#26041;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#30340;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;IndicTrans2&#31995;&#32479;&#65292;&#23427;&#22312;&#22810;&#20010;&#35821;&#35328;&#23545;&#19978;&#34920;&#29616;&#26368;&#20808;&#36827;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#21360;&#24230;&#35821;&#35328;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#33050;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21360;&#24230;&#26377;&#30528;&#20016;&#23500;&#30340;&#35821;&#35328;&#26223;&#35266;&#65292;&#21253;&#25324;&#22235;&#20010;&#20027;&#35201;&#35821;&#31995;&#30340;&#35821;&#35328;&#65292;&#36229;&#36807;&#21313;&#20159;&#20154;&#21475;&#20351;&#29992;&#12290;&#26412;&#31687;&#35770;&#25991;&#32858;&#28966;&#20110;&#21360;&#24230;&#23466;&#27861;&#21015;&#20986;&#30340;22&#31181;&#35821;&#35328;&#65292;&#34987;&#31216;&#20026;&#8220;&#23448;&#26041;&#35821;&#35328;&#8221;&#12290;&#37492;&#20110;&#35821;&#35328;&#22810;&#26679;&#24615;&#65292;&#39640;&#36136;&#37327;&#21644;&#21487;&#35775;&#38382;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#21360;&#24230;&#36825;&#26679;&#30340;&#22269;&#23478;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#27492;&#20043;&#21069;&#65292;&#32570;&#23569;&#65288;i&#65289;&#28085;&#30422;&#25152;&#26377;22&#31181;&#35821;&#35328;&#30340;&#24179;&#34892;&#35757;&#32451;&#25968;&#25454;&#65292;&#65288;ii&#65289;&#35206;&#30422;&#36825;&#20123;&#35821;&#35328;&#24182;&#21253;&#21547;&#21360;&#24230;&#30456;&#20851;&#20869;&#23481;&#30340;&#20581;&#22766;&#22522;&#20934;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#25903;&#25345;&#21360;&#24230;&#25152;&#26377;22&#31181;&#23448;&#26041;&#35821;&#35328;&#30340;&#29616;&#26377;&#32763;&#35793;&#27169;&#22411;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24182;&#19987;&#27880;&#20110;&#21551;&#29992;22&#31181;&#21360;&#24230;&#23448;&#26041;&#35821;&#35328;&#30340;&#24191;&#27867;&#12289;&#26131;&#20110;&#20351;&#29992;&#21644;&#24320;&#25918;&#24335;&#35775;&#38382;&#22909;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#25152;&#38656;&#30340;&#32570;&#22833;&#37096;&#20998;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#25913;&#36827;&#20851;&#38190;&#39046;&#22495;&#65306;&#31574;&#21010;&#21644;&#21019;&#24314;&#26356;&#22823;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12289;&#21019;&#24314;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#22522;&#20934;&#12289;&#36328;&#36234;&#25152;&#26377;22&#31181;&#23448;&#26041;&#35821;&#35328;&#36827;&#34892;&#32763;&#35793;&#65292;&#24182;&#26500;&#24314;&#39640;&#36136;&#37327;&#19988;&#21487;&#35775;&#38382;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#31995;&#32479;IndicTrans2&#22312;&#22810;&#20010;&#35821;&#35328;&#23545;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20248;&#20110;&#29616;&#26377;&#30340;&#22312;22&#31181;&#23448;&#26041;&#35821;&#35328;&#19979;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#20123;&#35821;&#35328;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#33050;&#26412;&#65292;&#20351;&#24471;&#30740;&#31350;&#20154;&#21592;&#26356;&#23481;&#26131;&#22320;&#35780;&#20215;&#21644;&#25552;&#39640;&#20851;&#20110;&#21360;&#24230;&#35821;&#35328;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
India has a rich linguistic landscape with languages from 4 major language families spoken by over a billion people. 22 of these languages are listed in the Constitution of India (referred to as scheduled languages) are the focus of this work. Given the linguistic diversity, high-quality and accessible Machine Translation (MT) systems are essential in a country like India. Prior to this work, there was (i) no parallel training data spanning all the 22 languages, (ii) no robust benchmarks covering all these languages and containing content relevant to India, and (iii) no existing translation models which support all the 22 scheduled languages of India. In this work, we aim to address this gap by focusing on the missing pieces required for enabling wide, easy, and open access to good machine translation systems for all 22 scheduled Indian languages. We identify four key areas of improvement: curating and creating larger training datasets, creating diverse and high-quality benchmarks, tra
&lt;/p&gt;</description></item><item><title>PromptNER&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31639;&#27861;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#28508;&#22312;&#23454;&#20307;&#21015;&#34920;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#21644;&#36328;&#39046;&#22495;NER&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15444</link><description>&lt;p&gt;
PromptNER: &#22522;&#20110;&#25552;&#31034;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
PromptNER: Prompting For Named Entity Recognition. (arXiv:2305.15444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15444
&lt;/p&gt;
&lt;p&gt;
PromptNER&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31639;&#27861;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#28508;&#22312;&#23454;&#20307;&#21015;&#34920;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#21644;&#36328;&#39046;&#22495;NER&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#36234;&#26469;&#36234;&#22810;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#29616;&#22312;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29616;&#25104;&#26041;&#27861;&#65292;&#20026;&#21508;&#31181;&#32463;&#20856;&#30340;NLP&#38382;&#39064;&#25552;&#20379;&#20102;&#23569;&#37327;&#26679;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#30528;&#20196;&#20154;&#26399;&#24453;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#22522;&#20110;LLM&#30340;&#23569;&#26679;&#26412;&#26041;&#27861;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26041;&#38754;&#20173;&#36828;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#31471;&#21040;&#31471;&#32467;&#26500;&#29702;&#35299;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#22312;&#26631;&#20934;&#26631;&#35760;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PromptNER&#65292;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#23569;&#26679;&#26412;&#21644;&#36328;&#39046;&#22495;NER&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;&#20026;&#20102;&#36866;&#24212;&#20219;&#20309;&#26032;&#30340;NER&#20219;&#21153;&#65292;PromptNER&#38656;&#35201;&#25552;&#20379;&#19968;&#32452;&#23454;&#20307;&#23450;&#20041;&#65292;&#38500;&#22522;&#26412;&#30340;&#23569;&#26679;&#26412;&#26679;&#20363;&#20197;&#22806;&#12290;&#32473;&#23450;&#36755;&#20837;&#21477;&#23376;&#65292;PromptNER&#25552;&#31034;LLM&#29983;&#25104;&#19968;&#20010;&#28508;&#22312;&#23454;&#20307;&#21015;&#34920;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#35299;&#37322;&#65292;&#35777;&#26126;&#23427;&#20204;&#19982;&#25552;&#20379;&#30340;&#23454;&#20307;&#31867;&#22411;&#23450;&#20041;&#30340;&#20860;&#23481;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;PromptNER&#22312;&#23569;&#26679;&#26412;NER&#20219;&#21153;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;WikiAnn&#25968;&#25454;&#38598;&#19978;&#20026;&#36328;&#39046;&#22495;NER&#35774;&#23450;&#20102;&#26032;&#30340;SOTA&#12290;
&lt;/p&gt;
&lt;p&gt;
In a surprising turn, Large Language Models (LLMs) together with a growing arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches providing few-shot solutions to myriad classic NLP problems. However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to any new NER task PromptNER requires a set of entity definitions in addition to the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. Remarkably, PromptNER achieves state-of-the-art performance on few-sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#30005;&#23376;&#21830;&#21153;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#19987;&#19994;&#39046;&#22495;&#20013;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#29305;&#23450;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#25506;&#32034;&#29992;&#20110;&#39044;&#27979;&#23545;&#25991;&#26723;&#30340;&#26597;&#35810;&#20998;&#32423;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#23581;&#35797;&#20351;&#29992;&#26080;&#30417;&#30563;&#32858;&#31867;&#25216;&#26415;&#36827;&#19968;&#27493;&#25913;&#36827;&#23545;&#25968;&#25454;&#20013;&#30456;&#20851;&#24615;&#27169;&#24335;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.11944</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#30456;&#20851;&#24615;&#39044;&#27979;&#30340;&#21512;&#25104;&#26597;&#35810;&#29983;&#25104;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Viability of Synthetic Query Generation for Relevance Prediction. (arXiv:2305.11944v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#30005;&#23376;&#21830;&#21153;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#19987;&#19994;&#39046;&#22495;&#20013;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#29305;&#23450;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#25506;&#32034;&#29992;&#20110;&#39044;&#27979;&#23545;&#25991;&#26723;&#30340;&#26597;&#35810;&#20998;&#32423;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#23581;&#35797;&#20351;&#29992;&#26080;&#30417;&#30563;&#32858;&#31867;&#25216;&#26415;&#36827;&#19968;&#27493;&#25913;&#36827;&#23545;&#25968;&#25454;&#20013;&#30456;&#20851;&#24615;&#27169;&#24335;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;-&#25991;&#26723;&#30456;&#20851;&#24615;&#39044;&#27979;&#26159;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#65288;&#39044;&#20808;&#35757;&#32451;&#30340;&#65289;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#19987;&#19994;&#39046;&#22495;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21463;&#21040;&#39046;&#22495;&#20869;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#21294;&#20047;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#21033;&#29992;&#36825;&#20123;&#24378;&#22823;&#30340;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29305;&#23450;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#25506;&#32034;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25110;&#29992;&#20110;&#38382;&#31572;&#21644;&#20108;&#20803;&#65288;&#26159;/&#21542;&#65289;&#30456;&#20851;&#24615;&#39044;&#27979;&#30340;&#26597;&#35810;&#29983;&#25104;&#65288;QGen&#65289;, &#20854;&#20013;&#20363;&#22914;&#65292;QGen&#27169;&#22411;&#32473;&#20986;&#19968;&#20010;&#25991;&#26723;&#65292;&#24182;&#35757;&#32451;&#29983;&#25104;&#19968;&#20010;&#19982;&#35813;&#25991;&#26723;&#30456;&#20851;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#23545;&#30456;&#20851;&#24615;&#26377;&#19968;&#20010;&#26356;&#32454;&#31890;&#24230;&#30340;&#27010;&#24565;&#65292;&#32780;&#19981;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#26159;/&#21542;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;QGen&#26041;&#27861;&#23454;&#29616;&#32454;&#24494;&#30340;&#30456;&#20851;&#24615;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#21512;&#25104;&#26597;&#35810;&#26469;&#39044;&#27979;&#23545;&#25991;&#26723;&#30340;&#26597;&#35810;&#20998;&#32423;&#30456;&#20851;&#24615;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#32034;&#20351;&#29992;&#26080;&#30417;&#30563;&#32858;&#31867;&#25216;&#26415;&#36827;&#19968;&#27493;&#25913;&#36827;&#23545;&#25968;&#25454;&#20013;&#30456;&#20851;&#24615;&#27169;&#24335;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query-document relevance prediction is a critical problem in Information Retrieval systems. This problem has increasingly been tackled using (pretrained) transformer-based models which are finetuned using large collections of labeled data. However, in specialized domains such as e-commerce and healthcare, the viability of this approach is limited by the dearth of large in-domain data. To address this paucity, recent methods leverage these powerful models to generate high-quality task and domain-specific synthetic data. Prior work has largely explored synthetic data generation or query generation (QGen) for Question-Answering (QA) and binary (yes/no) relevance prediction, where for instance, the QGen models are given a document, and trained to generate a query relevant to that document. However in many problems, we have a more fine-grained notion of relevance than a simple yes/no label. Thus, in this work, we conduct a detailed study into how QGen approaches can be leveraged for nuanced
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#26469;&#26377;&#25928;&#35268;&#36991;&#29616;&#26377;&#30340;&#25991;&#26412;&#26816;&#27979;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10847</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#24341;&#23548;&#26469;&#35268;&#36991;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#26469;&#26377;&#25928;&#35268;&#36991;&#29616;&#26377;&#30340;&#25991;&#26412;&#26816;&#27979;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21253;&#25324;&#35770;&#25991;&#20889;&#20316;&#21644;&#38382;&#31572;&#31561;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24517;&#39035;&#35299;&#20915;&#36825;&#20123;&#27169;&#22411;&#28508;&#22312;&#30340;&#35823;&#29992;&#38382;&#39064;&#65292;&#21542;&#21017;&#21487;&#33021;&#23548;&#33268;&#25220;&#34989;&#21644;&#22403;&#22334;&#20449;&#24687;&#31561;&#19981;&#33391;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26367;&#25442;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#20248;&#21270;&#26041;&#27861;&#65288;SICO&#65289;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#36825;&#31181;&#25552;&#31034;&#35821;&#12290;&#22312;&#19977;&#20010;&#29616;&#23454;&#20219;&#21153;&#20013;&#65292;LLMs&#21487;&#33021;&#34987;&#35823;&#29992;&#65292;&#22312;SICO&#30340;&#24110;&#21161;&#19979;&#65292;ChatGPT&#25104;&#21151;&#22320;&#35268;&#36991;&#20102;&#20845;&#39033;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#65292;&#24179;&#22343;&#23548;&#33268;0.54&#30340;AUC&#19979;&#38477;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#34920;&#29616;&#29978;&#33267;&#27604;&#38543;&#26426;&#20998;&#31867;&#22120;&#36824;&#35201;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#22362;&#23450;&#22320;&#25581;&#31034;&#20102;&#29616;&#26377;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional performance in a variety of tasks, including essay writing and question answering. However, it is crucial to address the potential misuse of these models, which can lead to detrimental outcomes such as plagiarism and spamming. Recently, several detectors have been proposed, including fine-tuned classifiers and various statistical methods. In this study, we reveal that with the aid of carefully crafted prompts, LLMs can effectively evade these detection systems. We propose a novel Substitution-based In-Context example Optimization method (SICO) to automatically generate such prompts. On three real-world tasks where LLMs can be misused, SICO successfully enables ChatGPT to evade six existing detectors, causing a significant 0.54 AUC drop on average. Surprisingly, in most cases these detectors perform even worse than random classifiers. These results firmly reveal the vulnerability of existing detectors. Finally, the strong perfor
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#21160;&#21518;&#32534;&#36753;&#31995;&#32479;&#26080;&#27861;&#22788;&#29702;&#39640;&#36136;&#37327;&#26426;&#22120;&#32763;&#35793;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#32473;&#23450;MT&#36827;&#34892;&#23545;&#31216;&#33258;&#25105;&#20851;&#27880;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#33258;&#21160;&#21518;&#32534;&#36753;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.10557</link><description>&lt;p&gt;
"&#33258;&#21160;&#21518;&#32534;&#36753;&#39640;&#36136;&#37327;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#21477;&#27861;&#23545;&#31216;&#24615;"
&lt;/p&gt;
&lt;p&gt;
Bring More Attention to Syntactic Symmetry for Automatic Postediting of High-Quality Machine Translations. (arXiv:2305.10557v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10557
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#33258;&#21160;&#21518;&#32534;&#36753;&#31995;&#32479;&#26080;&#27861;&#22788;&#29702;&#39640;&#36136;&#37327;&#26426;&#22120;&#32763;&#35793;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23545;&#32473;&#23450;MT&#36827;&#34892;&#23545;&#31216;&#33258;&#25105;&#20851;&#27880;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#33258;&#21160;&#21518;&#32534;&#36753;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21518;&#32534;&#36753;&#65288;APE&#65289;&#26159;&#29992;&#20110;&#25913;&#36827;&#32473;&#23450;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#30340;&#33258;&#21160;&#21270;&#36807;&#31243;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#22312;&#26377;&#20016;&#23500;&#25968;&#25454;&#36164;&#28304;&#30340;&#35821;&#35328;&#23545;&#65288;&#33521;&#35821;-&#24503;&#35821;&#65289;&#65292;&#29616;&#26377;&#30340;APE&#31995;&#32479;&#20063;&#19981;&#25797;&#38271;&#22788;&#29702;&#39640;&#36136;&#37327;&#30340;MT&#65306;&#32473;&#23450;&#30340;MT&#36136;&#37327;&#36234;&#39640;&#65292;&#20915;&#23450;&#21738;&#20123;&#37096;&#20998;&#38656;&#35201;&#32534;&#36753;&#20197;&#21450;&#22914;&#20309;&#20462;&#22797;&#36825;&#20123;&#38169;&#35823;&#23601;&#36234;&#22256;&#38590;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#21487;&#33021;&#30340;&#26041;&#27861;&#26159;&#23558;&#26356;&#28145;&#20837;&#30340;&#30446;&#26631;&#35821;&#35328;&#30693;&#35782;&#27880;&#20837;&#27169;&#22411;&#20013;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#35821;&#35328;&#23398;&#21160;&#26426;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#22686;&#24378;APE&#27169;&#22411;&#23545;&#30446;&#26631;&#35821;&#35328;&#30340;&#29702;&#35299;&#65306;&#36890;&#36807;&#19968;&#20010;&#40723;&#21169;&#23545;&#32473;&#23450;MT&#36827;&#34892;&#23545;&#31216;&#33258;&#25105;&#20851;&#27880;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#23545;&#23454;&#39564;&#32467;&#26524;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#39640;&#39640;&#36136;&#37327;MT&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#26550;&#26500;&#30340;APE&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic postediting (APE) is an automated process to refine a given machine translation (MT). Recent findings present that existing APE systems are not good at handling high-quality MTs even for a language pair with abundant data resources, English$\unicode{x2013}$German: the better the given MT is, the harder it is to decide what parts to edit and how to fix these errors. One possible solution to this problem is to instill deeper knowledge about the target language into the model. Thus, we propose a linguistically motivated method of regularization that is expected to enhance APE models' understanding of the target language: a loss function that encourages symmetric self-attention on the given MT. Our analysis of experimental results demonstrates that the proposed method helps improving the state-of-the-art architecture's APE quality for high-quality MTs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#25991;&#26412;&#30456;&#20851;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#34429;&#28982;&#22312;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23545;&#21333;&#20010;&#23383;&#31526;&#24418;&#29366;&#30340;&#24863;&#30693;&#26377;&#38480;&#65292;&#23545;&#22270;&#20687;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#26816;&#27979;&#33021;&#21147;&#20063;&#19981;&#36275;&#65292;&#19981;&#33021;&#19982;&#20256;&#32479;&#39046;&#22495;&#29305;&#23450;&#26041;&#27861;&#30456;&#21305;&#37197;&#65292;&#24182;&#20173;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#23427;&#20204;&#22312;OCR&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.07895</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;OCR&#30340;&#38544;&#31192;&#20043;&#35868;
&lt;/p&gt;
&lt;p&gt;
On the Hidden Mystery of OCR in Large Multimodal Models. (arXiv:2305.07895v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#25991;&#26412;&#30456;&#20851;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#34429;&#28982;&#22312;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23545;&#21333;&#20010;&#23383;&#31526;&#24418;&#29366;&#30340;&#24863;&#30693;&#26377;&#38480;&#65292;&#23545;&#22270;&#20687;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#26816;&#27979;&#33021;&#21147;&#20063;&#19981;&#36275;&#65292;&#19981;&#33021;&#19982;&#20256;&#32479;&#39046;&#22495;&#29305;&#23450;&#26041;&#27861;&#30456;&#21305;&#37197;&#65292;&#24182;&#20173;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#23427;&#20204;&#22312;OCR&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22823;&#22411;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#20013;&#25198;&#28436;&#30528;&#25903;&#37197;&#24615;&#30340;&#35282;&#33394;&#12290;&#20851;&#20110;&#23427;&#20204;&#22312;&#25991;&#26412;&#30456;&#20851;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#26377;&#25928;&#24615;&#30340;&#25506;&#32034;&#20173;&#19981;&#22815;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#25991;&#26412;&#35782;&#21035;&#12289;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#35273;&#38382;&#31572;&#21644;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#20248;&#21155;&#21183;&#65292;&#23427;&#20204;&#20027;&#35201;&#20381;&#36182;&#20110;&#35821;&#20041;&#29702;&#35299;&#26469;&#35782;&#21035;&#21333;&#35789;&#65292;&#24182;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#23545;&#21333;&#20010;&#23383;&#31526;&#24418;&#29366;&#30340;&#24863;&#30693;&#12290;&#23427;&#20204;&#23545;&#25991;&#26412;&#38271;&#24230;&#28448;&#19981;&#20851;&#24515;&#65292;&#22312;&#26816;&#27979;&#22270;&#20687;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#26041;&#38754;&#20855;&#26377;&#26377;&#38480;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#24403;&#21069;&#26368;&#24378;&#22823;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20063;&#26080;&#27861;&#19982;&#20256;&#32479;&#25991;&#26412;&#20219;&#21153;&#30340;&#39046;&#22495;&#29305;&#23450;&#26041;&#27861;&#30456;&#21305;&#37197;&#65292;&#24182;&#22312;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#20013;&#38754;&#20020;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#30340;&#22522;&#32447;&#32467;&#26524;&#25581;&#31034;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;OCR&#30340;&#38544;&#31192;&#20043;&#35868;&#65292;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large models have recently played a dominant role in natural language processing and multimodal vision-language learning. It remains less explored about their efficacy in text-related visual tasks. We conducted a comprehensive study of existing publicly available multimodal models, evaluating their performance in text recognition, text-based visual question answering, and key information extraction. Our findings reveal strengths and weaknesses in these models, which primarily rely on semantic understanding for word recognition and exhibit inferior perception of individual character shapes. They also display indifference towards text length and have limited capabilities in detecting fine-grained features in images. Consequently, these results demonstrate that even the current most powerful large multimodal models cannot match domain-specific methods in traditional text tasks and face greater challenges in more complex tasks. Most importantly, the baseline results showcased in this study
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110; Transformer &#30340; Albertina PT-* &#27169;&#22411;&#36827;&#34892;&#20102;&#33889;&#33796;&#29273;&#35821;&#30340;&#31070;&#32463;&#32534;&#30721;&#65292;&#21019;&#26032;&#24615;&#22320;&#25552;&#21319;&#20102;&#35813;&#35821;&#35328;&#22312;&#25968;&#23383;&#26102;&#20195;&#30340;&#25216;&#26415;&#20934;&#22791;&#27700;&#24179;&#65292;&#23588;&#20854;&#26159;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#21644;&#24052;&#35199;&#30340;&#32654;&#27954;&#33889;&#33796;&#29273;&#35821;&#20004;&#20010;&#21464;&#31181;&#12290;</title><link>http://arxiv.org/abs/2305.06721</link><description>&lt;p&gt;
&#22522;&#20110; Transformer Albertina PT-* &#25552;&#21319;&#33889;&#33796;&#29273;&#35821;&#30340;&#31070;&#32463;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Advancing Neural Encoding of Portuguese with Transformer Albertina PT-*. (arXiv:2305.06721v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110; Transformer &#30340; Albertina PT-* &#27169;&#22411;&#36827;&#34892;&#20102;&#33889;&#33796;&#29273;&#35821;&#30340;&#31070;&#32463;&#32534;&#30721;&#65292;&#21019;&#26032;&#24615;&#22320;&#25552;&#21319;&#20102;&#35813;&#35821;&#35328;&#22312;&#25968;&#23383;&#26102;&#20195;&#30340;&#25216;&#26415;&#20934;&#22791;&#27700;&#24179;&#65292;&#23588;&#20854;&#26159;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#21644;&#24052;&#35199;&#30340;&#32654;&#27954;&#33889;&#33796;&#29273;&#35821;&#20004;&#20010;&#21464;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25512;&#36827;&#33889;&#33796;&#29273;&#35821;&#65288;PT&#65289;&#30340;&#31070;&#32463;&#32534;&#30721;&#65292;&#20026;&#35813;&#35821;&#35328;&#22312;&#25968;&#23383;&#26102;&#20195;&#30340;&#25216;&#26415;&#20934;&#22791;&#25171;&#19979;&#22522;&#30784;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110; Transformer &#30340; Albertina PT-* &#22522;&#30784;&#27169;&#22411;&#65292;&#20026;&#20854;&#20004;&#20010;&#21464;&#31181;&#65288;&#33889;&#33796;&#29273;&#30340;&#27431;&#27954;&#33889;&#33796;&#29273;&#35821;&#65288;PT-PT&#65289;&#21644;&#24052;&#35199;&#30340;&#32654;&#27954;&#33889;&#33796;&#29273;&#35821;&#65288;PT-BR&#65289;&#65289;&#30340;&#31070;&#32463;&#32534;&#30721;&#21019;&#19979;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#24378;&#22823;&#30340;&#27169;&#22411;&#20316;&#20026;&#36215;&#28857;&#65292;&#21363;DeBERTa&#65292;&#24182;&#20351;&#29992;&#25105;&#20204;&#25910;&#38598;&#30340;PT-PT&#25968;&#25454;&#38598;&#21644;brWaC&#35821;&#26009;&#24211;&#23545;&#20854;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#36866;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#30340;&#33879;&#21517;&#19979;&#28216;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#26469;&#35780;&#20272;Albertina&#21644;&#31454;&#20105;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290; Albertina PT-PT&#21644;PT-BR&#29256;&#26412;&#22343;&#21487;&#20813;&#36153;&#20998;&#21457;&#65292;&#24182;&#22312;&#26368;&#23485;&#26494;&#30340;&#35768;&#21487;&#20197;&#19979;&#36816;&#34892;&#20110;&#28040;&#36153;&#32423;&#30828;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
To advance the neural encoding of Portuguese (PT), and a fortiori the technological preparation of this language for the digital age, we developed a Transformer-based foundation model that sets a new state of the art in this respect for two of its variants, namely European Portuguese from Portugal (PT-PT) and American Portuguese from Brazil (PT-BR).  To develop this encoder, which we named Albertina PT-*, a strong model was used as a starting point, DeBERTa, and its pre-training was done over data sets of Portuguese, namely over a data set we gathered for PT-PT and over the brWaC corpus for PT-BR. The performance of Albertina and competing models was assessed by evaluating them on prominent downstream language processing tasks adapted for Portuguese.  Both Albertina PT-PT and PT-BR versions are distributed free of charge and under the most permissive license possible and can be run on consumer-grade hardware, thus seeking to contribute to the advancement of research and innovation in l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;N-back&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30456;&#20284;&#65292;&#36825;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2305.03731</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Assessing Working Memory Capacity of ChatGPT. (arXiv:2305.03731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;N-back&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30456;&#20284;&#65292;&#36825;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#35760;&#24518;&#26159;&#20154;&#31867;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#23427;&#20316;&#20026;&#20449;&#24687;&#20020;&#26102;&#23384;&#20648;&#21644;&#25805;&#20316;&#30340;&#24037;&#20316;&#31354;&#38388;&#12290;&#26412;&#25991;&#36890;&#36807;&#26816;&#26597;ChatGPT&#22312;N-back&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35843;&#26597;&#20102;&#36825;&#19968;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#24037;&#20316;&#35760;&#24518;&#23545;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#24615;&#65292;&#25509;&#30528;&#20171;&#32461;&#20102;&#35780;&#20272;ChatGPT&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#22312;&#35328;&#35821;&#21644;&#31354;&#38388;N- back&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#25991;&#29486;&#25253;&#36947;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24403;&#21069;&#36827;&#23637;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#65292;&#24182;&#20026;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29702;&#35299;&#20154;&#31867;&#24037;&#20316;&#35760;&#24518;&#30340;&#26410;&#26469;&#21162;&#21147;&#25552;&#20379;&#20102;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Working memory is a critical aspect of both human intelligence and artificial intelligence (AI), serving as a workspace for the temporary storage and manipulation of information. This paper investigates working memory capacity of ChatGPT, a state-of-the-art language model, by examining its performance on N-back tasks. We begin by discussing the importance of working memory to humans and AI, followed by the methods employed to assess working memory capacity of ChatGPT. Our study compares behavioral performance of ChatGPT on verbal and spatial N-back tasks to that of human participants reported in the literature, revealing notable similarities. Our findings offer crucial insights into the current progress in designing AI systems with human-level cognitive abilities and hold promise for informing future endeavors aimed at enhancing AI working memory and understanding human working memory through AI models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#28857;&#30740;&#31350;&#20102;&#36731;&#37327;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#20020;&#24202;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#22312;RRS&#31034;&#20363;&#19978;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#23454;&#29616;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25918;&#23556;&#24615;&#25253;&#21578;&#25688;&#35201;&#65288;RRS&#65289;&#20219;&#21153;&#12290;&#24182;&#19988;&#35813;&#26041;&#27861;&#20165;&#24494;&#35843;&#27169;&#22411;&#30340;0.32&#65285;&#30340;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#34920;&#29616;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#39046;&#22495;&#36866;&#24212;&#22312;RRS&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#24320;&#21457;&#26356;&#22909;&#30340;&#25918;&#23556;&#24615;&#25253;&#21578;&#25688;&#35201;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.01146</link><description>&lt;p&gt;
RadAdapt&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#21270;&#39046;&#22495;&#33258;&#36866;&#24212;&#23454;&#29616;&#25918;&#23556;&#23398;&#25253;&#21578;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
RadAdapt: Radiology Report Summarization via Lightweight Domain Adaptation of Large Language Models. (arXiv:2305.01146v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#28857;&#30740;&#31350;&#20102;&#36731;&#37327;&#21270;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#20020;&#24202;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#22312;RRS&#31034;&#20363;&#19978;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#23454;&#29616;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25918;&#23556;&#24615;&#25253;&#21578;&#25688;&#35201;&#65288;RRS&#65289;&#20219;&#21153;&#12290;&#24182;&#19988;&#35813;&#26041;&#27861;&#20165;&#24494;&#35843;&#27169;&#22411;&#30340;0.32&#65285;&#30340;&#21442;&#25968;&#65292;&#25552;&#39640;&#20102;&#34920;&#29616;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#39046;&#22495;&#36866;&#24212;&#22312;RRS&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#24320;&#21457;&#26356;&#22909;&#30340;&#25918;&#23556;&#24615;&#25253;&#21578;&#25688;&#35201;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#36731;&#37327;&#32423;&#31574;&#30053;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#65288;&#33258;&#28982;&#35821;&#35328;&#65292;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#65292;&#20020;&#24202;&#25991;&#26412;&#65289;&#21644;&#25552;&#31034;&#65288;&#38646;-shot&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#25110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;&#21069;&#32512;&#24494;&#35843;&#65292;LoRA&#65289;&#65292;&#26469;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#25918;&#23556;&#24615;&#25253;&#21578;&#25688;&#35201;&#65288;RRS&#65289;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#22823;&#31243;&#24230;&#22320;&#36866;&#24212;&#20219;&#21153;&#30340;&#26041;&#27861;&#26159;&#65292;&#36890;&#36807;&#22312;&#20020;&#24202;&#25991;&#26412;&#19978;&#39044;&#20808;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;RRS&#31034;&#20363;&#19978;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#26041;&#27861;&#20165;&#24494;&#35843;&#27169;&#22411;&#30340;0.32&#65285;&#30340;&#21442;&#25968;&#65292;&#19982;&#31471;&#23545;&#31471;&#24494;&#35843;&#65288;100&#65285;&#30340;&#21442;&#25968;&#65289;&#24418;&#25104;&#23545;&#27604;&#12290;&#27492;&#22806;&#65292;&#22312;&#30740;&#31350;&#19978;&#19979;&#25991;&#31034;&#20363;&#21644;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#35757;&#32451;&#30340;&#24433;&#21709;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25918;&#23556;&#31185;&#21307;&#24072;&#35835;&#32773;&#30740;&#31350;&#21644;&#23450;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#39046;&#22495;&#36866;&#24212;&#22312;RRS&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#24320;&#21457;&#26356;&#22909;&#30340;&#25918;&#23556;&#24615;&#25253;&#21578;&#25688;&#35201;&#27169;&#22411;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We systematically investigate lightweight strategies to adapt large language models (LLMs) for the task of radiology report summarization (RRS). Specifically, we focus on domain adaptation via pretraining (on natural language, biomedical text, and clinical text) and via prompting (zero-shot, in-context learning) or parameter-efficient fine-tuning (prefix tuning, LoRA). Our results on the MIMIC-III dataset consistently demonstrate best performance by maximally adapting to the task via pretraining on clinical text and parameter-efficient fine-tuning on RRS examples. Importantly, this method fine-tunes a mere 0.32% of parameters throughout the model, in contrast to end-to-end fine-tuning (100% of parameters). Additionally, we study the effect of in-context examples and out-of-distribution (OOD) training before concluding with a radiologist reader study and qualitative analysis. Our findings highlight the importance of domain adaptation in RRS and provide valuable insights toward developin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#25143;&#27169;&#25311;&#22120;&#26469;&#35757;&#32451;&#21644;&#27979;&#35797;&#20027;&#21160;&#23545;&#35805;&#31574;&#30053;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#25506;&#32034;&#21644;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#34920;&#29616;&#30340;&#36866;&#24403;&#20027;&#21160;&#31574;&#30053;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2304.11913</link><description>&lt;p&gt;
&#24320;&#21457;&#19968;&#31181;&#20449;&#20219;&#24863;&#24863;&#30693;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#29992;&#20110;&#32479;&#35745;&#23398;&#30340;&#20027;&#21160;&#24335;&#23545;&#35805;&#24314;&#27169;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;
&lt;/p&gt;
&lt;p&gt;
Development of a Trust-Aware User Simulator for Statistical Proactive Dialog Modeling in Human-AI Teams. (arXiv:2304.11913v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#25143;&#27169;&#25311;&#22120;&#26469;&#35757;&#32451;&#21644;&#27979;&#35797;&#20027;&#21160;&#23545;&#35805;&#31574;&#30053;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#25506;&#32034;&#21644;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#34920;&#29616;&#30340;&#36866;&#24403;&#20027;&#21160;&#31574;&#30053;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#27010;&#24565;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#23454;&#29616;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#38431;&#21451;&#20043;&#38388;&#30340;&#26377;&#25928;&#21327;&#20316;&#65292;&#20027;&#21160;&#24615;&#23545;&#20110;&#32039;&#23494;&#21327;&#35843;&#21644;&#26377;&#25928;&#27807;&#36890;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20026;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#35774;&#35745;&#36866;&#24403;&#30340;&#20027;&#21160;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20027;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#24320;&#21457;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20027;&#21160;&#24335;&#23545;&#35805;&#31574;&#30053;&#12290;&#35813;&#27169;&#25311;&#22120;&#20197;&#26377;&#20851;&#20027;&#21160;&#24335;&#23545;&#35805;&#21450;&#20854;&#23545;&#29992;&#25143;&#20449;&#20219;&#24230;&#30340;&#30693;&#35782;&#20026;&#22522;&#30784;&#65292;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#21644;&#20010;&#20154;&#20449;&#24687;&#65292;&#21253;&#25324;&#31038;&#20250;&#20154;&#21475;&#29305;&#24449;&#21644;&#20154;&#26684;&#29305;&#24449;&#12290;&#23545;&#27604;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#25311;&#26041;&#27861;&#65292;&#22522;&#20110;&#20219;&#21153;&#27493;&#39588;&#30340;&#26041;&#27861;&#30001;&#20110;&#21152;&#24378;&#20102;&#39034;&#24207;&#20381;&#36182;&#27169;&#22411;&#65292;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24635;&#20307;&#32467;&#26524;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#25506;&#32034;&#21644;&#35780;&#20272;&#22312;&#23545;&#35805;&#28216;&#25103;&#29615;&#22659;&#20013;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#34920;&#29616;&#30340;&#36866;&#24403;&#20027;&#21160;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The concept of a Human-AI team has gained increasing attention in recent years. For effective collaboration between humans and AI teammates, proactivity is crucial for close coordination and effective communication. However, the design of adequate proactivity for AI-based systems to support humans is still an open question and a challenging topic. In this paper, we present the development of a corpus-based user simulator for training and testing proactive dialog policies. The simulator incorporates informed knowledge about proactive dialog and its effect on user trust and simulates user behavior and personal information, including socio-demographic features and personality traits. Two different simulation approaches were compared, and a task-step-based approach yielded better overall results due to enhanced modeling of sequential dependencies. This research presents a promising avenue for exploring and evaluating appropriate proactive strategies in a dialog game setting for improving H
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;k-NN&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#33021;&#22815;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.09058</link><description>&lt;p&gt;
&#37325;&#35775;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;k-NN
&lt;/p&gt;
&lt;p&gt;
Revisiting k-NN for Pre-trained Language Models. (arXiv:2304.09058v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;k-NN&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#33021;&#22815;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20316;&#20026;&#21442;&#25968;&#21270;&#30340;&#24613;&#20999;&#23398;&#20064;&#22120;&#65292;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24403;&#21069;&#33539;&#24335;&#30340;&#23454;&#38469;&#36873;&#25321;&#12290;&#19982;&#27492;&#24418;&#25104;&#23545;&#27604;&#30340;&#26159;&#65292;k-&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#20998;&#31867;&#22120;&#20316;&#20026;&#24310;&#36831;&#23398;&#20064;&#27169;&#22411;&#65292;&#20542;&#21521;&#20110;&#20943;&#36731;&#36807;&#25311;&#21512;&#21644;&#23396;&#31435;&#22122;&#22768;&#12290;&#26412;&#25991;&#20013;&#25105;&#20204;&#37325;&#35775;&#20102;k-NN&#20998;&#31867;&#22120;&#65292;&#20197;&#22686;&#24378;&#22522;&#20110;PLMs&#30340;&#20998;&#31867;&#22120;&#12290;&#20174;&#26041;&#27861;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#25991;&#26412;&#34920;&#31034;&#30340;PLMs&#22312;&#20004;&#20010;&#27493;&#39588;&#20013;&#37319;&#29992;k-NN&#65306;&#65288;1&#65289;&#21033;&#29992;k-NN&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#26469;&#26657;&#20934;&#35757;&#32451;&#36807;&#31243;&#65288;2&#65289;&#32447;&#24615;&#25554;&#20540;k-NN&#39044;&#27979;&#30340;&#27010;&#29575;&#20998;&#24067;&#21644;PLMs&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26680;&#24515;&#26159;&#23454;&#29616;&#20102;k-NN&#26657;&#20934;&#35757;&#32451;&#65292;&#23558;&#39044;&#27979;&#32467;&#26524;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#20013;&#26131;&#20110;&#21644;&#38590;&#20197;&#23398;&#20064;&#30340;&#31034;&#20363;&#30340;&#25351;&#26631;&#12290;&#20174;&#24212;&#29992;&#22330;&#26223;&#22810;&#26679;&#24615;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#24494;&#35843;&#12289;&#25552;&#31034;&#24494;&#35843;&#33539;&#24335;&#21644;&#38646;&#26679;&#26412;&#20219;&#21153;&#35774;&#32622;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;k-NN&#21487;&#20197;&#22312;&#25152;&#26377;&#21463;&#21040;&#26816;&#26597;&#30340;&#35774;&#32622;&#20013;&#25345;&#32493;&#25552;&#39640;PLMs&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#21463;&#21040;&#32771;&#34385;&#30340;&#35774;&#32622;&#20013;&#36305;&#36194;&#20102;&#22522;&#20110;&#26222;&#36890;PLMs&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models (PLMs), as parametric-based eager learners, have become the de-facto choice for current paradigms of Natural Language Processing (NLP). In contrast, k-Nearest-Neighbor (k-NN) classifiers, as the lazy learning paradigm, tend to mitigate over-fitting and isolated noise. In this paper, we revisit k-NN classifiers for augmenting the PLMs-based classifiers. From the methodological level, we propose to adopt k-NN with textual representations of PLMs in two steps: (1) Utilize k-NN as prior knowledge to calibrate the training process. (2) Linearly interpolate the probability distribution predicted by k-NN with that of the PLMs' classifier. At the heart of our approach is the implementation of k-NN-calibrated training, which treats predicted results as indicators for easy versus hard examples during the training process. From the perspective of the diversity of application scenarios, we conduct extensive experiments on fine-tuning, prompt-tuning paradigms and zero-sh
&lt;/p&gt;</description></item><item><title>ASL Citizen&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#29420;&#31435;&#25163;&#35821;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#25163;&#35821;&#23383;&#20856;&#26816;&#32034;&#65292;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#22312;&#24230;&#37327;&#26631;&#20934;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#20363;&#22914;&#22312;&#35757;&#32451;&#25110;&#39564;&#35777;&#20013;&#26410;&#20986;&#29616;&#30340;&#29992;&#25143;&#30340;&#35270;&#39057;&#19978;&#65292;&#23454;&#29616;&#20102;62&#65285;&#30340;&#20934;&#30830;&#24615;&#21644;90&#65285;&#30340;&#21069;10&#39033;&#26816;&#32034;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.05934</link><description>&lt;p&gt;
ASL Citizen: &#19968;&#20010;&#25512;&#36827;&#29420;&#31435;&#25163;&#35821;&#35782;&#21035;&#30340;&#31038;&#21306;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ASL Citizen: A Community-Sourced Dataset for Advancing Isolated Sign Language Recognition. (arXiv:2304.05934v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05934
&lt;/p&gt;
&lt;p&gt;
ASL Citizen&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#29420;&#31435;&#25163;&#35821;&#35782;&#21035;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#25163;&#35821;&#23383;&#20856;&#26816;&#32034;&#65292;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#22312;&#24230;&#37327;&#26631;&#20934;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#20363;&#22914;&#22312;&#35757;&#32451;&#25110;&#39564;&#35777;&#20013;&#26410;&#20986;&#29616;&#30340;&#29992;&#25143;&#30340;&#35270;&#39057;&#19978;&#65292;&#23454;&#29616;&#20102;62&#65285;&#30340;&#20934;&#30830;&#24615;&#21644;90&#65285;&#30340;&#21069;10&#39033;&#26816;&#32034;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#34987;&#20840;&#29699;&#32422;7000&#19975;&#32843;&#20581;&#20154;&#22763;&#29992;&#20316;&#20027;&#35201;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20132;&#27969;&#25216;&#26415;&#36816;&#20316;&#22312;&#21475;&#22836;&#21644;&#20070;&#38754;&#35821;&#35328;&#20013;&#65292;&#23548;&#33268;&#33719;&#21462;&#20449;&#24687;&#23384;&#22312;&#19981;&#20844;&#24179;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;ASL Citizen&#65292;&#23427;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#29420;&#31435;&#25163;&#35821;&#35782;&#21035; (ISLR) &#25968;&#25454;&#38598;&#65292;&#32463;&#36807;&#21516;&#24847;&#25910;&#38598;&#65292;&#21253;&#25324;52&#20010;&#25163;&#35821;&#32773;&#22312;&#21508;&#31181;&#29615;&#22659;&#20013;&#25293;&#25668;&#30340;2,731&#20010;&#19981;&#21516;&#25163;&#21183;&#30340;83,912&#20010;&#35270;&#39057;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#36825;&#20010;&#25968;&#25454;&#38598;&#29992;&#20110;&#32654;&#22269;&#25163;&#35821; (ASL) &#30340;&#25163;&#35821;&#23383;&#20856;&#26816;&#32034;&#65292;&#29992;&#25143;&#36890;&#36807;&#33258;&#24049;&#30340;&#32593;&#32476;&#25668;&#20687;&#22836;&#28436;&#31034;&#25163;&#35821;&#65292;&#20174;&#23383;&#20856;&#20013;&#26816;&#32034;&#30456;&#21305;&#37197;&#30340;&#25163;&#35821;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#23545;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#19982;&#23383;&#20856;&#26816;&#32034;&#30456;&#20851;&#30340;&#24230;&#37327;&#26631;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20363;&#22914;&#22312;&#35757;&#32451;&#25110;&#39564;&#35777;&#20013;&#26410;&#20986;&#29616;&#30340;&#29992;&#25143;&#30340;&#35270;&#39057;&#19978;&#65292;&#23454;&#29616;&#20102;62&#65285;&#30340;&#20934;&#30830;&#24615;&#21644;90&#65285;&#30340;&#21069;10&#39033;&#26816;&#32034;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sign languages are used as a primary language by approximately 70 million D/deaf people world-wide. However, most communication technologies operate in spoken and written languages, creating inequities in access. To help tackle this problem, we release ASL Citizen, the largest Isolated Sign Language Recognition (ISLR) dataset to date, collected with consent and containing 83,912 videos for 2,731 distinct signs filmed by 52 signers in a variety of environments. We propose that this dataset be used for sign language dictionary retrieval for American Sign Language (ASL), where a user demonstrates a sign to their own webcam with the aim of retrieving matching signs from a dictionary. We show that training supervised machine learning classifiers with our dataset greatly advances the state-of-the-art on metrics relevant for dictionary retrieval, achieving, for instance, 62% accuracy and a recall-at-10 of 90%, evaluated entirely on videos of users who are not present in the training or valida
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.04370</link><description>&lt;p&gt;
OpenAGI&#65306;&#24403;LLM&#36935;&#21040;&#39046;&#22495;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04370
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#23558;&#22522;&#26412;&#25216;&#33021;&#32452;&#21512;&#25104;&#22797;&#26434;&#25216;&#33021;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21516;&#26679;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#26029;&#35328;&#65292;&#38500;&#20102;&#24320;&#21457;&#22823;&#22411;&#32508;&#21512;&#26234;&#33021;&#27169;&#22411;&#22806;&#65292;&#23558;&#19981;&#21516;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#21516;&#26679;&#20851;&#38190;&#65292;&#20197;&#22312;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#26234;&#33021;&#30340;&#36861;&#27714;&#20013;&#20351;&#20854;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#35777;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#36873;&#25321;&#12289;&#32508;&#21512;&#21644;&#25191;&#34892;&#22806;&#37096;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#30340;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OpenAGI&#30340;&#24320;&#28304;AGI&#30740;&#31350;&#24179;&#21488;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#25552;&#20379;&#22797;&#26434;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#65292;&#24182;&#37197;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21508;&#31181;&#21487;&#25193;&#23637;&#27169;&#22411;&#12290;OpenAGI&#23558;&#22797;&#26434;&#20219;&#21153;&#38416;&#37322;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#26088;&#22312;&#20419;&#36827;&#39046;&#22495;&#19987;&#23478;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#32531;&#35299;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#30340;&#20302;&#25928;&#29575;&#65292;&#21253;&#25324;&#21160;&#24577;&#38376;&#25511;&#12289;&#19987;&#23478;&#32531;&#20914;&#21644;&#19987;&#23478;&#36127;&#36733;&#24179;&#34913;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25191;&#34892;&#26102;&#38388;&#21644;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.06182</link><description>&lt;p&gt;
&#36808;&#21521;MoE&#37096;&#32626;&#65306;&#32531;&#35299;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#25512;&#29702;&#20013;&#30340;&#20302;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference. (arXiv:2303.06182v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#32531;&#35299;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#30340;&#20302;&#25928;&#29575;&#65292;&#21253;&#25324;&#21160;&#24577;&#38376;&#25511;&#12289;&#19987;&#23478;&#32531;&#20914;&#21644;&#19987;&#23478;&#36127;&#36733;&#24179;&#34913;&#12290;&#36825;&#20123;&#25216;&#26415;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25191;&#34892;&#26102;&#38388;&#21644;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes three optimization techniques to mitigate inefficiencies in Mixture-of-Experts (MoE) models during inference, including dynamic gating, expert buffering, and expert load balancing. These techniques can significantly improve execution time and reduce memory usage.
&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#27169;&#22411;&#26368;&#36817;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#24191;&#27867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#20204;&#22312;&#35757;&#32451;&#26399;&#38388;&#26377;&#25928;&#22320;&#25193;&#23637;&#20102;&#27169;&#22411;&#23481;&#37327;&#65292;&#21516;&#26102;&#22686;&#21152;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#23567;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#22797;&#26434;&#30340;&#36890;&#20449;&#27169;&#24335;&#65292;&#37096;&#32626;&#36825;&#26679;&#30340;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;MoE&#24037;&#20316;&#36127;&#36733;&#30340;&#29305;&#24449;&#21270;&#65292;&#21363;&#35821;&#35328;&#24314;&#27169;&#65288;LM&#65289;&#21644;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#20204;&#22312;&#37096;&#32626;&#26102;&#30340;&#20302;&#25928;&#29575;&#26469;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#32531;&#35299;&#20302;&#25928;&#29575;&#30340;&#26469;&#28304;&#65292;&#21363;&#65288;1&#65289;&#21160;&#24577;&#38376;&#25511;&#65292;&#65288;2&#65289;&#19987;&#23478;&#32531;&#20914;&#21644;&#65288;3&#65289;&#19987;&#23478;&#36127;&#36733;&#24179;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21160;&#24577;&#38376;&#25511;&#21487;&#20197;&#20351;LM&#30340;&#25191;&#34892;&#26102;&#38388;&#25552;&#39640;1.25-4&#20493;&#65292;MT&#32534;&#30721;&#22120;&#25552;&#39640;2-5&#20493;&#65292;MT&#35299;&#30721;&#22120;&#25552;&#39640;1.09-1.5&#20493;&#12290;&#23427;&#36824;&#21487;&#20197;&#23558;LM&#30340;&#20869;&#23384;&#20351;&#29992;&#20943;&#23569;&#39640;&#36798;1.36&#20493;&#65292;MT&#30340;&#20869;&#23384;&#20351;&#29992;&#20943;&#23569;&#39640;&#36798;1.1&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Experts (MoE) models have recently gained steam in achieving the state-of-the-art performance in a wide range of tasks in computer vision and natural language processing. They effectively expand the model capacity while incurring a minimal increase in computation cost during training. However, deploying such models for inference is difficult due to their large model size and complex communication pattern. In this work, we provide a characterization of two MoE workloads, namely Language Modeling (LM) and Machine Translation (MT) and identify their sources of inefficiencies at deployment.  We propose three optimization techniques to mitigate sources of inefficiencies, namely (1) Dynamic gating, (2) Expert Buffering, and (3) Expert load balancing. We show that dynamic gating improves execution time by 1.25-4$\times$ for LM, 2-5$\times$ for MT Encoder and 1.09-1.5$\times$ for MT Decoder. It also reduces memory usage by up to 1.36$\times$ for LM and up to 1.1$\times$ for MT. We f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21435;&#20559;&#24046;&#26041;&#27861;&#8212;&#8212;DAM&#65292;&#23427;&#37319;&#29992;AdapterFusion&#27010;&#24565;&#65292;&#23558;&#20559;&#24046;&#20462;&#27491;&#21151;&#33021;&#23553;&#35013;&#21040;&#29420;&#31435;&#30340;&#36866;&#37197;&#22120;&#20013;&#65292;&#22312;&#19981;&#24433;&#21709;&#26680;&#24515;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#25353;&#38656;&#30340;&#21435;&#20559;&#24046;&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#27169;&#22411;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.06321</link><description>&lt;p&gt;
&#36890;&#36807;AdapterFusion&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient Modularised Bias Mitigation via AdapterFusion. (arXiv:2302.06321v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21435;&#20559;&#24046;&#26041;&#27861;&#8212;&#8212;DAM&#65292;&#23427;&#37319;&#29992;AdapterFusion&#27010;&#24565;&#65292;&#23558;&#20559;&#24046;&#20462;&#27491;&#21151;&#33021;&#23553;&#35013;&#21040;&#29420;&#31435;&#30340;&#36866;&#37197;&#22120;&#20013;&#65292;&#22312;&#19981;&#24433;&#21709;&#26680;&#24515;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#25353;&#38656;&#30340;&#21435;&#20559;&#24046;&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#27169;&#22411;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#23558;&#36825;&#20123;&#20559;&#35265;&#24102;&#32473;&#19979;&#28216;&#20219;&#21153;&#12290;&#24403;&#21069;&#30340;&#20869;&#37096;&#22788;&#29702;&#20559;&#24046;&#20462;&#27491;&#26041;&#27861;&#65288;&#22914;&#23545;&#25239;&#35757;&#32451;&#65289;&#36890;&#36807;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#26469;&#26045;&#21152;&#21435;&#20559;&#24046;&#65292;&#20174;&#32780;&#23558;&#27169;&#22411;&#36716;&#31227;&#21040;&#26032;&#30340;&#12289;&#19981;&#21487;&#36870;&#30340;&#21435;&#20559;&#24046;&#29366;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20986;&#20102;&#29420;&#31435;&#30340;&#21435;&#20559;&#24046;&#21151;&#33021;&#65292;&#19982;&#27169;&#22411;&#20998;&#31163;&#65292;&#21487;&#20197;&#25353;&#38656;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#21516;&#26102;&#20445;&#25345;&#26680;&#24515;&#27169;&#22411;&#19981;&#21464;&#12290;&#20511;&#37492;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;AdapterFusion&#27010;&#24565;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DAM&#65288;&#20351;&#29992;&#36866;&#37197;&#22120;&#27169;&#22359;&#36827;&#34892;&#21435;&#20559;&#24046;&#65289;&#8212;&#8212;&#19968;&#31181;&#21435;&#20559;&#24046;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#20219;&#24847;&#20559;&#24046;&#20462;&#27491;&#21151;&#33021;&#23553;&#35013;&#21040;&#29420;&#31435;&#30340;&#36866;&#37197;&#22120;&#20013;&#65292;&#28982;&#21518;&#25353;&#38656;&#23558;&#23427;&#20204;&#28155;&#21152;&#21040;&#27169;&#22411;&#20013;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#31181;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20445;&#25252;&#23646;&#24615;&#20026;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DAM&#25913;&#36827;&#25110;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models contain societal biases and carry along these biases to downstream tasks. Current in-processing bias mitigation approaches (like adversarial training) impose debiasing by updating a model's parameters, effectively transferring the model to a new, irreversible debiased state. In this work, we propose a novel approach to develop stand-alone debiasing functionalities separate from the model, which can be integrated into the model on-demand, while keeping the core model untouched. Drawing from the concept of AdapterFusion in multi-task learning, we introduce DAM (Debiasing with Adapter Modules) - a debiasing approach to first encapsulate arbitrary bias mitigation functionalities into separate adapters, and then add them to the model on-demand in order to deliver fairness qualities. We conduct a large set of experiments on three classification tasks with gender, race, and age as protected attributes. Our results show that DAM improves or maintains the effec
&lt;/p&gt;</description></item><item><title>MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#28216;&#25103;&#20851;&#21345;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#24335;&#30340;&#12289;&#21487;&#25511;&#21046;&#30340;&#20851;&#21345;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2302.05981</link><description>&lt;p&gt;
MarioGPT: &#36890;&#36807;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#24335;&#25991;&#26412;&#20851;&#21345;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MarioGPT: Open-Ended Text2Level Generation through Large Language Models. (arXiv:2302.05981v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05981
&lt;/p&gt;
&lt;p&gt;
MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#28216;&#25103;&#20851;&#21345;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#24335;&#30340;&#12289;&#21487;&#25511;&#21046;&#30340;&#20851;&#21345;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#22797;&#26434;&#25968;&#19968;&#33268;&#30340;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#26041;&#27861;&#29983;&#25104;&#21453;&#26144;&#29305;&#23450;&#24847;&#22270;&#21644;&#38480;&#21046;&#30340;&#26377;&#24847;&#20041;&#20869;&#23481;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#31639;&#27861;&#32570;&#20047;&#20197;&#24320;&#25918;&#24335;&#26041;&#24335;&#29983;&#25104;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#37117;&#34920;&#29616;&#20986;&#20102;&#38750;&#24120;&#39640;&#30340;&#25928;&#29575;&#12290;&#36825;&#20123;&#35757;&#32451;&#26377;&#32032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#24494;&#35843;&#65292;&#37325;&#22797;&#20351;&#29992;&#20449;&#24687;&#24182;&#21152;&#36895;&#26032;&#20219;&#21153;&#30340;&#22521;&#35757;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MarioGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;GPT2&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#22522;&#20110;&#29943;&#30742;&#30340;&#28216;&#25103;&#20851;&#21345;&#65292;&#25105;&#20204;&#20197;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#30340;&#20851;&#21345;&#20026;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MarioGPT&#19981;&#20165;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#30340;&#28216;&#25103;&#20851;&#21345;&#65292;&#32780;&#19988;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#25511;&#21046;&#20851;&#21345;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;PCG&#25216;&#26415;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#20851;&#21345;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural Content Generation (PCG) algorithms provide a technique to generate complex and diverse environments in an automated way. However, while generating content with PCG methods is often straightforward, generating meaningful content that reflects specific intentions and constraints remains challenging. Furthermore, many PCG algorithms lack the ability to generate content in an open-ended manner. Recently, Large Language Models (LLMs) have shown to be incredibly effective in many diverse domains. These trained LLMs can be fine-tuned, re-using information and accelerating training for new tasks. In this work, we introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game levels, in our case Super Mario Bros levels. We show that MarioGPT can not only generate diverse levels, but can be text-prompted for controllable level generation, addressing one of the key challenges of current PCG techniques. As far as we know, MarioGPT is the first text-to-level model. We a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;CEIL&#65288;Compositional Exemplars for In-context Learning&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#20915;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#27169;&#22411;&#22788;&#29702;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.05698</link><description>&lt;p&gt;
&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#32452;&#21512;&#33539;&#20363;
&lt;/p&gt;
&lt;p&gt;
Compositional Exemplars for In-context Learning. (arXiv:2302.05698v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05698
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;CEIL&#65288;Compositional Exemplars for In-context Learning&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#20915;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#27169;&#22411;&#22788;&#29702;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20854;&#20013;&#27169;&#22411;&#36890;&#36807;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#20316;&#20026;&#28436;&#31034;&#65292;&#22312;&#19981;&#36827;&#34892;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#25191;&#34892;&#30475;&#19981;&#35265;&#30340;&#20219;&#21153;&#12290;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#39640;&#24230;&#21463;&#21040;&#25152;&#36873;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#36136;&#37327;&#25152;&#25903;&#37197;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#36873;&#25321;&#26041;&#27861;&#22522;&#26412;&#19978;&#26159;&#22522;&#20110;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#65292;&#23548;&#33268;&#24615;&#33021;&#27425;&#20248;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#24418;&#24335;&#21270;&#20026;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;CEIL&#65288;Compositional Exemplars for In-context Learning&#65289;&#65292;&#23427;&#36890;&#36807;&#20915;&#23450;&#24615;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#23545;&#25152;&#32473;&#36755;&#20837;&#21644;&#19978;&#19979;&#25991;&#31034;&#20363;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#33719;&#24471;&#26469;&#33258;LM&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;7&#20010;&#19981;&#21516;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;12&#20010;&#20998;&#31867;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;CEIL&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#12289;&#37322;&#20041;&#26816;&#27979;&#12289;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pretrained language models (LMs) have shown impressive In-Context Learning (ICL) ability, where the model learns to do an unseen task via a prompt consisting of input-output examples as the demonstration, without any parameter updates. The performance of ICL is highly dominated by the quality of the selected in-context examples. However, previous selection methods are mostly based on simple heuristics, leading to sub-optimal performance. In this work, we formulate in-context example selection as a subset selection problem. We propose CEIL (Compositional Exemplars for In-context Learning), which is instantiated by Determinantal Point Processes (DPPs) to model the interaction between the given input and in-context examples, and optimized through a carefully-designed contrastive learning objective to obtain preference from LMs. We validate CEIL on 12 classification and generation datasets from 7 distinct NLP tasks, including sentiment analysis, paraphrase detection, natural language
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#21644;&#30456;&#26426;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20132;&#21449;&#27169;&#24577;&#20851;&#32852;&#25552;&#21462;&#30456;&#26426;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#24471;&#21040;&#30340;&#29305;&#24449;&#25104;&#21151;&#23454;&#29616;&#25340;&#25509;&#22270;&#20687;&#21306;&#22495;&#30340;"&#38646;&#26679;&#26412;"&#23450;&#20301;&#12290;</title><link>http://arxiv.org/abs/2301.04647</link><description>&lt;p&gt;
EXIF&#20316;&#20026;&#19968;&#31181;&#35821;&#35328;&#65306;&#23398;&#20064;&#22270;&#20687;&#19982;&#30456;&#26426;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20132;&#21449;&#27169;&#24577;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata. (arXiv:2301.04647v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#21644;&#30456;&#26426;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20132;&#21449;&#27169;&#24577;&#20851;&#32852;&#25552;&#21462;&#30456;&#26426;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#24471;&#21040;&#30340;&#29305;&#24449;&#25104;&#21151;&#23454;&#29616;&#25340;&#25509;&#22270;&#20687;&#21306;&#22495;&#30340;"&#38646;&#26679;&#26412;"&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#35270;&#35273;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#21462;&#19982;&#25152;&#35760;&#24405;&#30340;&#29031;&#29255;&#30456;&#20851;&#30340;&#30456;&#26426;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;&#22359;&#21644;&#33258;&#21160;&#25554;&#20837;&#21040;&#22270;&#20687;&#25991;&#20214;&#20013;&#30340;EXIF&#20803;&#25968;&#25454;&#20043;&#38388;&#35757;&#32451;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#23558;&#20803;&#25968;&#25454;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#28982;&#21518;&#20351;&#29992;transformer&#36827;&#34892;&#22788;&#29702;&#26469;&#34920;&#31034;&#27492;&#20803;&#25968;&#25454;&#12290;&#25105;&#20204;&#23398;&#20064;&#30340;&#29305;&#24449;&#22312;&#19979;&#28216;&#22270;&#20687;&#21462;&#35777;&#21644;&#26657;&#20934;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#33258;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#29305;&#24449;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#36890;&#36807;&#23545;&#22270;&#20687;&#20869;&#25152;&#26377;&#22359;&#30340;&#35270;&#35273;&#23884;&#20837;&#36827;&#34892;&#32858;&#31867;&#26469;&#23454;&#29616;"&#38646;&#26679;&#26412;"&#30340;&#25340;&#25509;&#22270;&#20687;&#21306;&#22495;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
We learn a visual representation that captures information about the camera that recorded a given photo. To do this, we train a multimodal embedding between image patches and the EXIF metadata that cameras automatically insert into image files. Our model represents this metadata by simply converting it to text and then processing it with a transformer. The features that we learn significantly outperform other self-supervised and supervised features on downstream image forensics and calibration tasks. In particular, we successfully localize spliced image regions "zero shot" by clustering the visual embeddings for all of the patches within an image.
&lt;/p&gt;</description></item><item><title>MULTI3NLU++&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#12289;&#22810;&#24847;&#22270;&#12289;&#22810;&#22495;&#25968;&#25454;&#38598;&#12290;&#20854;&#20013;&#21253;&#21547;&#25163;&#21160;&#32763;&#35793;&#30340;&#22810;&#31181;&#39640;&#12289;&#20013;&#12289;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;&#20004;&#20010;&#39046;&#22495;&#12290;&#35813;&#25968;&#25454;&#38598;&#33021;&#22815;&#24110;&#21161;&#34913;&#37327;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#29616;&#23454;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10455</link><description>&lt;p&gt;
MULTI3NLU++&#65306;&#19968;&#31181;&#29992;&#20110;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#24847;&#22270;&#12289;&#22810;&#22495;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MULTI3NLU++: A Multilingual, Multi-Intent, Multi-Domain Dataset for Natural Language Understanding in Task-Oriented Dialogue. (arXiv:2212.10455v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10455
&lt;/p&gt;
&lt;p&gt;
MULTI3NLU++&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#12289;&#22810;&#24847;&#22270;&#12289;&#22810;&#22495;&#25968;&#25454;&#38598;&#12290;&#20854;&#20013;&#21253;&#21547;&#25163;&#21160;&#32763;&#35793;&#30340;&#22810;&#31181;&#39640;&#12289;&#20013;&#12289;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;&#20004;&#20010;&#39046;&#22495;&#12290;&#35813;&#25968;&#25454;&#38598;&#33021;&#22815;&#24110;&#21161;&#34913;&#37327;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#29616;&#23454;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#24050;&#32463;&#34987;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#35768;&#22810;&#34892;&#19994;&#20013;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25552;&#20379;&#26356;&#26377;&#25928;&#30340;&#23458;&#25143;&#25903;&#25345;&#12290;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#26159;&#20026;&#21333;&#20010;&#39046;&#22495;&#25110;&#35821;&#35328;&#26500;&#24314;&#30340;&#65292;&#24182;&#19988;&#22312;&#36825;&#20123;&#39046;&#22495;&#20043;&#22806;&#30340;&#25512;&#24191;&#33021;&#21147;&#24456;&#24046;&#12290;&#20026;&#20102;&#21516;&#26102;&#25903;&#25345;&#36328;&#22810;&#31181;&#35821;&#35328;&#21644;&#39046;&#22495;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#24037;&#20316;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102; MULTI3NLU++&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#12289;&#22810;&#24847;&#22270;&#12289;&#22810;&#22495;&#25968;&#25454;&#38598;&#12290;MULTI3NLU++ &#23558;&#20165;&#38480;&#20110;&#33521;&#35821;&#30340; NLU++ &#25968;&#25454;&#38598;&#25193;&#23637;&#21040;&#22810;&#31181;&#39640;&#12289;&#20013;&#12289;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#35199;&#29677;&#29273;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#22303;&#32819;&#20854;&#35821;&#21644;&#38463;&#22982;&#21704;&#25289;&#35821;&#65289;&#20197;&#21450;&#20004;&#20010;&#39046;&#22495;&#65288;&#38134;&#34892;&#21644;&#37202;&#24215;&#65289;&#20013;&#30340;&#25163;&#21160;&#32763;&#35793;&#12290;&#30001;&#20110; MULTI3NLU++ &#20855;&#26377;&#22810;&#24847;&#22270;&#30340;&#23646;&#24615;&#65292;&#22240;&#27492;&#23427;&#20195;&#34920;&#20102;&#22797;&#26434;&#21644;&#33258;&#28982;&#30340;&#29992;&#25143;&#30446;&#26631;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#19990;&#30028;&#35821;&#35328;&#20013;&#30340;&#21508;&#31181;&#39046;&#22495;&#20013;&#34913;&#37327; TOD &#31995;&#32479;&#30340;&#29616;&#23454;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992; MULTI3NLU++ &#26469;&#22522;&#20934;&#27979;&#35797;&#29992;&#20110;&#24847;&#22270;&#26816;&#27979;&#21644;&#27133;&#20301;&#26631;&#35760;&#30340; NLU &#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogue (TOD) systems have been widely deployed in many industries as they deliver more efficient customer support. These systems are typically constructed for a single domain or language and do not generalise well beyond this. To support work on Natural Language Understanding (NLU) in TOD across multiple languages and domains simultaneously, we constructed MULTI3NLU++, a multilingual, multi-intent, multi-domain dataset. MULTI3NLU++ extends the English only NLU++ dataset to include manual translations into a range of high, medium, and low resource languages (Spanish, Marathi, Turkish and Amharic), in two domains (BANKING and HOTELS). Because of its multi-intent property, MULTI3NLU++ represents complex and natural user goals, and therefore allows us to measure the realistic performance of TOD systems in a varied set of the world's languages. We use MULTI3NLU++ to benchmark state-of-the-art multilingual models for the NLU tasks of intent detection and slot labelling for TO
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#22312;&#22823;&#22411;&#24179;&#21488;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#21457;&#29616;&#26576;&#20123;&#24230;&#37327;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#34920;&#29616;&#19981;&#20339;&#19988;&#20854;&#26377;&#29992;&#24615;&#19982;&#19979;&#28216;&#20219;&#21153;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2212.10297</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#30340;&#22806;&#22312;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Extrinsic Evaluation of Machine Translation Metrics. (arXiv:2212.10297v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10297
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#22312;&#22823;&#22411;&#24179;&#21488;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#21457;&#29616;&#26576;&#20123;&#24230;&#37327;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#34920;&#29616;&#19981;&#20339;&#19988;&#20854;&#26377;&#29992;&#24615;&#19982;&#19979;&#28216;&#20219;&#21153;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#36890;&#24120;&#29992;&#20110;&#22312;&#36739;&#22823;&#30340;&#27979;&#35797;&#38598;&#19978;&#27604;&#36739;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#32763;&#35793;&#36136;&#37327;&#65288;&#31995;&#32479;&#32423;&#35780;&#20272;&#65289;&#65292;&#20294;&#26159;&#65292;&#21477;&#23376;&#32423;&#21035;&#19978;&#33258;&#21160;&#24230;&#37327;&#26159;&#21542;&#33021;&#21487;&#38752;&#22320;&#21306;&#20998;&#22909;&#32763;&#35793;&#21644;&#24046;&#32763;&#35793;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#20855;&#26377;&#19979;&#28216;&#20219;&#21153;&#30340;&#22823;&#22411;&#24179;&#21488;&#19978;&#25918;&#32622;&#26426;&#22120;&#32763;&#35793;&#32452;&#20214;&#20197;&#26816;&#27979;&#20854;&#25104;&#21151;&#30340;MT&#24230;&#37327;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#36328;&#35821;&#35328;&#19979;&#28216;&#20219;&#21153;&#65288;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65292;&#38382;&#39064;&#22238;&#31572;&#21644;&#35821;&#20041;&#35299;&#26512;&#65289;&#19978;&#35780;&#20272;&#20102;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;MT&#37327;&#24230;&#65288;chrF&#65292;COMET&#65292;BERTScore&#31561;&#65289;&#30340;&#20998;&#27573;&#24615;&#33021;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#20165;&#33021;&#35775;&#38382;&#21333;&#35821;&#31181;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#25105;&#20204;&#35745;&#31639;&#22312;Translate-Test&#35774;&#32622;&#19979;&#65292;&#24230;&#37327;&#39044;&#27979;&#22909;/&#22351;&#32763;&#35793;&#33021;&#21147;&#19982;&#26368;&#32456;&#20219;&#21153;&#25104;&#21151;/&#22833;&#36133;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;&#26576;&#20123;&#24230;&#37327;&#22312;&#31995;&#32479;&#32423;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20998;&#27573;&#35780;&#20272;&#20013;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;&#27492;&#22806;&#65292;&#26576;&#20123;&#24230;&#37327;&#30340;&#26377;&#29992;&#24615;&#21462;&#20915;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic machine translation (MT) metrics are widely used to distinguish the translation qualities of machine translation systems across relatively large test sets (system-level evaluation). However, it is unclear if automatic metrics are reliable at distinguishing good translations from bad translations at the sentence level (segment-level evaluation). In this paper, we investigate how useful MT metrics are at detecting the success of a machine translation component when placed in a larger platform with a downstream task. We evaluate the segment-level performance of the most widely used MT metrics (chrF, COMET, BERTScore, etc.) on three downstream cross-lingual tasks (dialogue state tracking, question answering, and semantic parsing). For each task, we only have access to a monolingual task-specific model. We calculate the correlation between the metric's ability to predict a good/bad translation with the success/failure on the final task for the Translate-Test setup. Our experiments
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MASTER&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#29942;&#39048;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#32479;&#19968;&#21508;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#19968;&#20010;&#27169;&#22411;&#20013;&#12290;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#21516;&#31561;&#27169;&#22411;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#36164;&#28304;&#30340;&#26368;&#20808;&#36827;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#65292;MASTER&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2212.07841</link><description>&lt;p&gt;
MASTER:&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#30340;&#29942;&#39048;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#27604;&#23494;&#38598;&#22411;&#26816;&#32034;&#22120;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
MASTER: Multi-task Pre-trained Bottlenecked Masked Autoencoders are Better Dense Retrievers. (arXiv:2212.07841v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MASTER&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#29942;&#39048;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#32479;&#19968;&#21508;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#24182;&#23558;&#20854;&#38598;&#25104;&#21040;&#19968;&#20010;&#27169;&#22411;&#20013;&#12290;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#21516;&#31561;&#27169;&#22411;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#36164;&#28304;&#30340;&#26368;&#20808;&#36827;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#65292;MASTER&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#23494;&#38598;&#26816;&#32034;&#26041;&#27861;&#20013;&#65292;&#39044;&#35757;&#32451;&#30340;Transformer&#65288;&#22914;BERT&#65289;&#36890;&#24120;&#29992;&#20110;&#21442;&#25968;&#21021;&#22987;&#21270;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#27491;&#22312;&#25506;&#32034;&#26356;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#23494;&#38598;&#21521;&#37327;&#30340;&#36136;&#37327;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#20219;&#21153;&#65292;&#20294;&#23427;&#20204;&#19981;&#21516;&#30340;&#36755;&#20837;&#26684;&#24335;&#21644;&#23398;&#20064;&#30446;&#26631;&#20351;&#23427;&#20204;&#38590;&#20197;&#34987;&#25972;&#21512;&#36215;&#26469;&#20849;&#21516;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#23558;&#21508;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#32479;&#19968;&#25104;&#29942;&#39048;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#65292;&#23558;&#23427;&#20204;&#25972;&#21512;&#21040;&#19968;&#20010;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#21517;&#20026;MASTER&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MASTER&#21033;&#29992;&#20849;&#20139;&#32534;&#30721;&#22120;&#22810;&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#21487;&#20197;&#26500;&#36896;&#34920;&#31034;&#29942;&#39048;&#65292;&#23558;&#36328;&#21508;&#31181;&#20219;&#21153;&#30340;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#21387;&#32553;&#25104;&#23494;&#38598;&#21521;&#37327;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25972;&#21512;&#20102;&#19977;&#31181;&#20195;&#34920;&#24615;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65306;&#30772;&#25439;&#27573;&#33853;&#24674;&#22797;&#12289;&#30456;&#20851;&#27573;&#33853;&#24674;&#22797;&#21644;PLMs&#36755;&#20986;&#24674;&#22797;&#65292;&#20197;&#21516;&#26102;&#36827;&#34892;&#36716;&#24405;&#12289;&#32034;&#24341;&#21644;QA&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MASTER&#22312;&#30456;&#21516;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#36164;&#28304;&#19979;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#23494;&#38598;&#26816;&#32034;&#27169;&#22411;&#65292;&#34920;&#26126;&#20102;&#23558;&#21508;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#20197;&#32479;&#19968;&#30340;&#26684;&#24335;&#38598;&#25104;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Transformers (\eg BERT) have been commonly used in existing dense retrieval methods for parameter initialization, and recent studies are exploring more effective pre-training tasks for further improving the quality of dense vectors. Although various novel and effective tasks have been proposed, their different input formats and learning objectives make them hard to be integrated for jointly improving the model performance. In this work, we aim to unify a variety of pre-training tasks into the bottlenecked masked autoencoder manner, and integrate them into a multi-task pre-trained model, namely MASTER. Concretely, MASTER utilizes a shared-encoder multi-decoder architecture that can construct a representation bottleneck to compress the abundant semantic information across tasks into dense vectors. Based on it, we integrate three types of representative pre-training tasks: corrupted passages recovering, related passages recovering and PLMs outputs recovering, to characterize t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MoNET&#65292;&#36890;&#36807;&#22122;&#22768;&#22686;&#24378;&#35757;&#32451;&#35299;&#20915;&#20102;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20013;&#30340;&#29366;&#24577;&#24815;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#20462;&#27491;&#27133;&#20540;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.05503</link><description>&lt;p&gt;
MoNET&#65306;&#36890;&#36807;&#22122;&#22768;&#22686;&#24378;&#30340;&#35757;&#32451;&#35299;&#20915;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20013;&#30340;&#29366;&#24577;&#24815;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
MoNET: Tackle State Momentum via Noise-Enhanced Training for Dialogue State Tracking. (arXiv:2211.05503v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MoNET&#65292;&#36890;&#36807;&#22122;&#22768;&#22686;&#24378;&#35757;&#32451;&#35299;&#20915;&#20102;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#20013;&#30340;&#29366;&#24577;&#24815;&#24615;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#20462;&#27491;&#27133;&#20540;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#26088;&#22312;&#23558;&#23545;&#35805;&#21382;&#21490;&#36716;&#25442;&#20026;&#21253;&#21547;&#27133;-&#20540;&#23545;&#30340;&#23545;&#35805;&#29366;&#24577;&#12290;&#20316;&#20026;&#25152;&#26377;&#21382;&#21490;&#20449;&#24687;&#30340;&#32467;&#26500;&#21270;&#27010;&#25324;&#20449;&#24687;&#65292;&#36890;&#24120;&#37319;&#29992;&#19978;&#19968;&#36718;&#30340;&#23545;&#35805;&#29366;&#24577;&#20316;&#20026;DST&#27169;&#22411;&#39044;&#27979;&#24403;&#21069;&#29366;&#24577;&#30340;&#36755;&#20837;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#20445;&#25345;&#39044;&#27979;&#30340;&#27133;&#20540;&#19981;&#21464;&#65292;&#36825;&#22312;&#26412;&#25991;&#20013;&#34987;&#23450;&#20041;&#20026;&#29366;&#24577;&#24815;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MoNET&#65292;&#22312;&#22122;&#22768;&#22686;&#24378;&#35757;&#32451;&#30340;&#24110;&#21161;&#19979;&#35299;&#20915;&#29366;&#24577;&#24815;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#26367;&#25442;&#19968;&#20123;&#27133;&#20540;&#26469;&#22122;&#22768;&#21270;&#27599;&#36718;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19978;&#19968;&#20010;&#29366;&#24577;&#12290;&#28982;&#21518;&#65292;&#23558;&#22122;&#22768;&#21270;&#30340;&#19978;&#19968;&#20010;&#29366;&#24577;&#20316;&#20026;&#36755;&#20837;&#65292;&#39044;&#27979;&#24403;&#21069;&#29366;&#24577;&#65292;&#20174;&#32780;&#25913;&#21892;&#27169;&#22411;&#26356;&#26032;&#21644;&#20462;&#27491;&#27133;&#20540;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23545;&#27604;&#19978;&#19979;&#25991;&#21305;&#37197;&#26694;&#26550;&#26469;&#32553;&#23567;...
&lt;/p&gt;
&lt;p&gt;
Dialogue state tracking (DST) aims to convert the dialogue history into dialogue states which consist of slot-value pairs. As condensed structural information memorizing all history information, the dialogue state in the last turn is typically adopted as the input for predicting the current state by DST models. However, these models tend to keep the predicted slot values unchanged, which is defined as state momentum in this paper. Specifically, the models struggle to update slot values that need to be changed and correct wrongly predicted slot values in the last turn. To this end, we propose MoNET to tackle state momentum via noise-enhanced training. First, the previous state of each turn in the training data is noised via replacing some of its slot values. Then, the noised previous state is used as the input to learn to predict the current state, improving the model's ability to update and correct slot values. Furthermore, a contrastive context matching framework is designed to narrow
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#31038;&#20250;&#20559;&#35265;&#22522;&#20934;&#20013;&#25968;&#25454;&#38598;&#26500;&#24314;&#20559;&#35265;&#21487;&#33021;&#23545;&#32467;&#26524;&#36896;&#25104;&#20102;&#37325;&#35201;&#24433;&#21709;&#65292;&#38656;&#35201;&#26356;&#20005;&#35880;&#30340;&#31038;&#20250;&#20559;&#35265;&#24230;&#37327;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.10040</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#20559;&#35265;&#65306;&#31038;&#20250;&#20559;&#35265;&#22522;&#20934;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Tail Wagging the Dog: Dataset Construction Biases of Social Bias Benchmarks. (arXiv:2210.10040v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10040
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#31038;&#20250;&#20559;&#35265;&#22522;&#20934;&#20013;&#25968;&#25454;&#38598;&#26500;&#24314;&#20559;&#35265;&#21487;&#33021;&#23545;&#32467;&#26524;&#36896;&#25104;&#20102;&#37325;&#35201;&#24433;&#21709;&#65292;&#38656;&#35201;&#26356;&#20005;&#35880;&#30340;&#31038;&#20250;&#20559;&#35265;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#21487;&#38752;&#22320;&#30456;&#20449;&#20174;&#31038;&#20250;&#20559;&#35265;&#22522;&#20934;&#24471;&#21040;&#30340;&#20998;&#25968;&#26159;&#23545;&#32473;&#23450;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#31038;&#20250;&#20559;&#35265;&#30340;&#24544;&#23454;&#25351;&#26631;&#65311;&#26412;&#25991;&#36890;&#36807;&#23558;&#31038;&#20250;&#20559;&#35265;&#19982;&#26469;&#28304;&#20110;&#25968;&#25454;&#38598;&#26500;&#24314;&#36807;&#31243;&#20013;&#30340;&#38750;&#31038;&#20250;&#20559;&#35265;&#36827;&#34892;&#23545;&#27604;&#30740;&#31350;&#36825;&#19968;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26681;&#25454;&#26080;&#23475;&#30340;&#20462;&#25913;&#65288;&#22914;&#37322;&#20041;&#25110;&#38543;&#26426;&#25277;&#26679;&#65289;&#23454;&#38469;&#27169;&#25311;&#20102;&#32473;&#23450;&#22522;&#20934;&#30340;&#21508;&#31181;&#26367;&#20195;&#32467;&#26500;&#65292;&#36825;&#20123;&#20462;&#25913;&#20445;&#25345;&#20854;&#31038;&#20250;&#20559;&#35265;&#30340;&#26412;&#36136;&#12290;&#22312;&#20004;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#31038;&#20250;&#20559;&#35265;&#22522;&#20934;&#65288;Winogender&#21644;BiasNLI&#65289;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#27973;&#26174;&#30340;&#20462;&#25913;&#23545;&#21508;&#31181;&#27169;&#22411;&#20013;&#23548;&#33268;&#30340;&#20559;&#35265;&#31243;&#24230;&#20135;&#29983;&#20102;&#24778;&#20154;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20123;&#20196;&#20154;&#19981;&#23433;&#30340;&#35266;&#23519;&#32467;&#26524;&#33021;&#22815;&#28608;&#21457;&#26356;&#20005;&#35880;&#30340;&#31038;&#20250;&#20559;&#35265;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
How reliably can we trust the scores obtained from social bias benchmarks as faithful indicators of problematic social biases in a given language model? In this work, we study this question by contrasting social biases with non-social biases stemming from choices made during dataset construction that might not even be discernible to the human eye. To do so, we empirically simulate various alternative constructions for a given benchmark based on innocuous modifications (such as paraphrasing or random-sampling) that maintain the essence of their social bias. On two well-known social bias benchmarks (Winogender and BiasNLI) we observe that these shallow modifications have a surprising effect on the resulting degree of bias across various models. We hope these troubling observations motivate more robust measures of social biases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23454;&#39564;&#35777;&#25454;&#30340;&#25991;&#26412;&#34920;&#24449;&#21644;&#35821;&#20041;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#32032;&#26364;&#21704;&#39039;&#36317;&#31163;&#21521;&#37327;&#29305;&#24449;&#35782;&#21035;&#25991;&#26412;-&#20551;&#35774;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;F1&#20998;&#25968;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2210.09723</link><description>&lt;p&gt;
&#24102;&#26377;&#23454;&#35777;&#25991;&#26412;&#34920;&#24449;&#30340;&#35821;&#20041;&#29305;&#24449;&#30340;&#25991;&#26412;&#34164;&#28085;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Textual Entailment Recognition with Semantic Features from Empirical Text Representation. (arXiv:2210.09723v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23454;&#39564;&#35777;&#25454;&#30340;&#25991;&#26412;&#34920;&#24449;&#21644;&#35821;&#20041;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#32032;&#26364;&#21704;&#39039;&#36317;&#31163;&#21521;&#37327;&#29305;&#24449;&#35782;&#21035;&#25991;&#26412;-&#20551;&#35774;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;F1&#20998;&#25968;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#34164;&#28085;&#35782;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#22522;&#26412;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#22312;&#33258;&#21160;&#35782;&#21035;&#25991;&#26412;&#34164;&#21547;&#20043;&#21069;&#65292;&#29702;&#35299;&#21477;&#23376;&#30340;&#21547;&#20041;&#26159;&#24517;&#35201;&#30340;&#21069;&#25552;&#12290;&#22914;&#26524;&#21069;&#25552;&#20026;&#30495;&#65292;&#21017;&#25991;&#26412;&#34164;&#28085;&#20551;&#35774;&#20063;&#20026;&#30495;&#12290;&#32463;&#20856;&#30340;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#26469;&#33258;&#35789;&#23884;&#20837;&#30340;&#27599;&#20010;&#21333;&#35789;&#30340;&#29305;&#24449;&#20540;&#26469;&#34920;&#31034;&#21477;&#23376;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#25991;&#26412;&#21644;&#20551;&#35774;&#20043;&#38388;&#30340;&#34164;&#21547;&#20851;&#31995;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#35777;&#22522;&#20110;&#38408;&#20540;&#30340;&#35821;&#20041;&#25991;&#26412;&#34920;&#24449;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#22522;&#20110;&#20803;&#32032;&#30340;&#26364;&#21704;&#39039;&#36317;&#31163;&#21521;&#37327;&#29305;&#24449;&#65292;&#21487;&#20197;&#35782;&#21035;&#25991;&#26412;-&#20551;&#35774;&#23545;&#20043;&#38388;&#30340;&#35821;&#20041;&#34164;&#28085;&#20851;&#31995;&#12290;&#25105;&#20204;&#23545;&#22522;&#20934;&#34164;&#28085;&#20998;&#31867;(SICK-RTE)&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20960;&#39033;&#23454;&#39564;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#35757;&#32451;&#20102;&#20960;&#20010;&#26426;&#22120;&#23398;&#20064;(ML)&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#24615;&#33021;&#19982;&#32463;&#20856;&#30340;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;F1&#20998;&#25968;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#32463;&#20856;&#27169;&#22411;&#65292;&#24182;&#22312;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual entailment recognition is one of the basic natural language understanding(NLU) tasks. Understanding the meaning of sentences is a prerequisite before applying any natural language processing(NLP) techniques to automatically recognize the textual entailment. A text entails a hypothesis if and only if the true value of the hypothesis follows the text. Classical approaches generally utilize the feature value of each word from word embedding to represent the sentences. In this paper, we propose a novel approach to identifying the textual entailment relationship between text and hypothesis, thereby introducing a new semantic feature focusing on empirical threshold-based semantic text representation. We employ an element-wise Manhattan distance vector-based feature that can identify the semantic entailment relationship between the text-hypothesis pair. We carried out several experiments on a benchmark entailment classification(SICK-RTE) dataset. We train several machine learning(ML) 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#65292;&#23558;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.08964</link><description>&lt;p&gt;
PromptCast&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#65292;&#23558;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#12290;&#22312;&#36825;&#31181;&#26032;&#30340;&#20219;&#21153;&#20013;&#65292;&#23558;&#21407;&#26469;&#30340;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#39044;&#27979;&#30340;&#30446;&#30340;&#12290;&#20026;&#20102;&#25903;&#25345;&#21644;&#20419;&#36827;&#36825;&#20010;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65288;PISA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#36947;&#24503;&#27169;&#20223;&#33021;&#21147;&#65292;&#20250;&#26681;&#25454;&#25919;&#27835;&#36523;&#20221;&#29983;&#25104;&#21453;&#26144;&#30456;&#24212;&#36947;&#24503;&#20559;&#35265;&#30340;&#25991;&#26412;&#12290;</title><link>http://arxiv.org/abs/2209.12106</link><description>&lt;p&gt;
&#36947;&#24503;&#27169;&#20223;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36866;&#24212;&#25919;&#27835;&#36523;&#20221;&#30340;&#36947;&#24503;&#36777;&#25252;
&lt;/p&gt;
&lt;p&gt;
Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity. (arXiv:2209.12106v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#36947;&#24503;&#27169;&#20223;&#33021;&#21147;&#65292;&#20250;&#26681;&#25454;&#25919;&#27835;&#36523;&#20221;&#29983;&#25104;&#21453;&#26144;&#30456;&#24212;&#36947;&#24503;&#20559;&#35265;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#29983;&#25104;&#27969;&#30021;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#24778;&#20154;&#33021;&#21147;&#65292;&#20294;&#20063;&#20542;&#21521;&#20110;&#37325;&#22797;&#19981;&#33391;&#31038;&#20250;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;LLMs&#26159;&#21542;&#20250;&#22797;&#21046;&#19982;&#32654;&#22269;&#25919;&#27835;&#22242;&#20307;&#30456;&#20851;&#30340;&#36947;&#24503;&#20559;&#35265;&#65292;&#21363;&#25152;&#36848;&#30340;&#26356;&#24191;&#27867;&#30340;&#36947;&#24503;&#27169;&#20223;&#33021;&#21147;&#12290;&#35813;&#20551;&#35774;&#22312;&#22522;&#20110;Transformer&#30340;LLMs&#23478;&#26063;&#20013;&#30340;GPT-3 / 3.5&#21644;OPT&#20013;&#24471;&#21040;&#20102;&#25506;&#35752;&#12290;&#20351;&#29992;&#36947;&#24503;&#22522;&#30784;&#29702;&#35770;&#24037;&#20855;&#65292;&#34920;&#26126;&#36825;&#20123;LLMs&#30830;&#23454;&#26159;&#36947;&#24503;&#27169;&#20223;&#32773;&#12290;&#24403;&#20197;&#33258;&#30001;&#20027;&#20041;&#25110;&#20445;&#23432;&#20027;&#20041;&#25919;&#27835;&#36523;&#20221;&#20026;&#25552;&#31034;&#26102;&#65292;&#27169;&#22411;&#20250;&#29983;&#25104;&#21453;&#26144;&#30456;&#24212;&#36947;&#24503;&#20559;&#35265;&#30340;&#25991;&#26412;&#12290;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#36947;&#24503;&#27169;&#20223;&#19982;&#27169;&#22411;&#22823;&#23567;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#20154;&#31867;&#21644;LLM&#36947;&#24503;&#29992;&#35821;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive capabilities in generating fluent text, as well as tendencies to reproduce undesirable social biases. This study investigates whether LLMs reproduce the moral biases associated with political groups in the United States, an instance of a broader capability herein termed moral mimicry. This hypothesis is explored in the GPT-3/3.5 and OPT families of Transformer-based LLMs. Using tools from Moral Foundations Theory, it is shown that these LLMs are indeed moral mimics. When prompted with a liberal or conservative political identity, the models generate text reflecting corresponding moral biases. This study also explores the relationship between moral mimicry and model size, and similarity between human and LLM moral word use.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#24211;&#26500;&#24314;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#22810;&#31181;&#25552;&#31034;&#25216;&#26415;&#65292;&#25163;&#21160;&#25552;&#31034;&#31574;&#30053;&#30340;&#32534;&#21046;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#24517;&#39035;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#32473;&#20986;&#19981;&#21516;&#38271;&#24230;&#30340;&#31572;&#26696;&#38598;&#65292;&#29305;&#21035;&#26159;&#21253;&#25324;&#31354;&#31572;&#26696;&#38598;&#12290;&#23454;&#20307;&#21035;&#21517;&#23383;&#20856;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2208.11057</link><description>&lt;p&gt;
&#25552;&#31034;&#20316;&#20026;&#25506;&#27979;&#22120;&#65306;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#24211;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Prompting as Probing: Using Language Models for Knowledge Base Construction. (arXiv:2208.11057v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#24211;&#26500;&#24314;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#22810;&#31181;&#25552;&#31034;&#25216;&#26415;&#65292;&#25163;&#21160;&#25552;&#31034;&#31574;&#30053;&#30340;&#32534;&#21046;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#24517;&#39035;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#32473;&#20986;&#19981;&#21516;&#38271;&#24230;&#30340;&#31572;&#26696;&#38598;&#65292;&#29305;&#21035;&#26159;&#21253;&#25324;&#31354;&#31572;&#26696;&#38598;&#12290;&#23454;&#20307;&#21035;&#21517;&#23383;&#20856;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#20013;&#37117;&#24456;&#26377;&#29992;&#65292;&#20363;&#22914;&#25688;&#35201;&#12289;&#32763;&#35793;&#12289;&#38382;&#31572;&#21644;&#25991;&#26412;&#20998;&#31867;&#12290;&#30001;&#20110;&#23427;&#20204;&#21487;&#20197;&#23384;&#20648;&#22823;&#37327;&#20449;&#24687;&#65292;&#22240;&#27492;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ProP&#65288;&#25552;&#31034;&#20316;&#20026;&#25506;&#27979;&#22120;&#65289;&#65292;&#23427;&#21033;&#29992;OpenAI&#22312;2020&#24180;&#25552;&#20986;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-3&#26469;&#25191;&#34892;&#30693;&#35782;&#24211;&#26500;&#24314;&#20219;&#21153;&#12290;ProP&#37319;&#29992;&#22810;&#27493;&#39588;&#26041;&#27861;&#65292;&#32467;&#21512;&#21508;&#31181;&#25552;&#31034;&#25216;&#26415;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25163;&#21160;&#25552;&#31034;&#31574;&#30053;&#30340;&#32534;&#21046;&#33267;&#20851;&#37325;&#35201;&#65307;&#24517;&#39035;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#32473;&#20986;&#19981;&#21516;&#38271;&#24230;&#30340;&#31572;&#26696;&#38598;&#65292;&#29305;&#21035;&#26159;&#21253;&#25324;&#31354;&#31572;&#26696;&#38598;&#65307;&#30495;/&#20551;&#38382;&#39064;&#26159;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#24314;&#35758;&#30340;&#20934;&#30830;&#24615;&#30340;&#26377;&#29992;&#26041;&#27861;&#65307;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23567;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#22240;&#32032;&#65307;&#23454;&#20307;&#21035;&#21517;&#23383;&#20856;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have proven to be useful in various downstream applications, such as summarisation, translation, question answering and text classification. LMs are becoming increasingly important tools in Artificial Intelligence, because of the vast quantity of information they can store. In this work, we present ProP (Prompting as Probing), which utilizes GPT-3, a large Language Model originally proposed by OpenAI in 2020, to perform the task of Knowledge Base Construction (KBC). ProP implements a multi-step approach that combines a variety of prompting techniques to achieve this. Our results show that manual prompt curation is essential, that the LM must be encouraged to give answer sets of variable lengths, in particular including empty answer sets, that true/false questions are a useful device to increase precision on suggestions generated by the LM, that the size of the LM is a crucial factor, and that a dictionary of entity aliases improves the LM score. Our evaluation stu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Transformer&#22312;U.S Patent Phrase to Phrase Matching Dataset&#19978;&#36827;&#34892;&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#25928;&#29575;&#65292;&#36798;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.11716</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#35821;&#26009;&#24211;&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#30340;&#35748;&#30693;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach. (arXiv:2207.11716v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Transformer&#22312;U.S Patent Phrase to Phrase Matching Dataset&#19978;&#36827;&#34892;&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#25928;&#29575;&#65292;&#36798;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#21644;&#24314;&#27169;&#26159;&#24403;&#20170;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35768;&#22810;&#20808;&#39537;&#24212;&#29992;&#20013;&#22522;&#26412;&#35748;&#21487;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#39034;&#24207;&#27169;&#24335;&#35782;&#21035;&#30340;&#24863;&#30693;&#65292;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#65288;&#22914;RNN&#21644;LSTM&#65289;&#22312;&#35821;&#20041;&#30456;&#20284;&#24230;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23427;&#20204;&#26080;&#27861;&#20197;&#38750;&#39034;&#24207;&#26041;&#24335;&#22788;&#29702;&#20449;&#24687;&#65292;&#22240;&#27492;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#34987;&#35748;&#20026;&#25928;&#29575;&#20302;&#19979;&#65292;&#20174;&#32780;&#23548;&#33268;&#19978;&#19979;&#25991;&#25552;&#21462;&#19981;&#24403;&#12290;Transformer&#22240;&#20854;&#38750;&#39034;&#24207;&#25968;&#25454;&#22788;&#29702;&#21644;&#33258;&#25105;&#20851;&#27880;&#31561;&#20248;&#21183;&#32780;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#12290;&#26412;&#25991;&#20351;&#29992;&#20256;&#32479;&#21644;&#22522;&#20110;transformer&#30340;&#25216;&#26415;&#23545;&#32654;&#22269;&#19987;&#21033;&#30701;&#35821;&#36827;&#34892;&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#21644;&#24314;&#27169;&#12290;&#25105;&#20204;&#23545;&#22235;&#31181;&#19981;&#21516;&#29256;&#26412;&#30340;&#35299;&#30721;&#22686;&#24378;BERT-DeBERTa&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;K&#25240;&#20132;&#21449;&#39564;&#35777;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity analysis and modeling is a fundamentally acclaimed task in many pioneering applications of natural language processing today. Owing to the sensation of sequential pattern recognition, many neural networks like RNNs and LSTMs have achieved satisfactory results in semantic similarity modeling. However, these solutions are considered inefficient due to their inability to process information in a non-sequential manner, thus leading to the improper extraction of context. Transformers function as the state-of-the-art architecture due to their advantages like non-sequential data processing and self-attention. In this paper, we perform semantic similarity analysis and modeling on the U.S Patent Phrase to Phrase Matching Dataset using both traditional and transformer-based techniques. We experiment upon four different variants of the Decoding Enhanced BERT - DeBERTa and enhance its performance by performing K-Fold Cross-Validation. The experimental results demonstrate our me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#27010;&#24565;&#21435;&#38500;&#12290;&#23545;&#20110;&#29616;&#26377;&#30340;&#21518;&#26399;&#21644;&#23545;&#25239;&#24615;&#26041;&#27861;&#65292;&#26412;&#25991;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#20854;&#20381;&#36182;&#30340;&#25506;&#27979;&#20998;&#31867;&#22120;&#21487;&#33021;&#20351;&#29992;&#38750;&#27010;&#24565;&#29305;&#24449;&#65292;&#23548;&#33268;&#26080;&#27861;&#23436;&#20840;&#21435;&#38500;&#19981;&#38656;&#35201;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#23398;&#20064;&#20174;&#27169;&#22411;&#30340;&#34920;&#31034;&#20013;&#21435;&#38500;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21518;&#26399;&#21644;&#23545;&#25239;&#24615;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2207.04153</link><description>&lt;p&gt;
&#25506;&#27979;&#20998;&#31867;&#22120;&#23545;&#20110;&#27010;&#24565;&#21435;&#38500;&#21644;&#26816;&#27979;&#19981;&#21487;&#38752;
&lt;/p&gt;
&lt;p&gt;
Probing Classifiers are Unreliable for Concept Removal and Detection. (arXiv:2207.04153v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#30340;&#19981;&#33391;&#27010;&#24565;&#21435;&#38500;&#12290;&#23545;&#20110;&#29616;&#26377;&#30340;&#21518;&#26399;&#21644;&#23545;&#25239;&#24615;&#26041;&#27861;&#65292;&#26412;&#25991;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#20854;&#20381;&#36182;&#30340;&#25506;&#27979;&#20998;&#31867;&#22120;&#21487;&#33021;&#20351;&#29992;&#38750;&#27010;&#24565;&#29305;&#24449;&#65292;&#23548;&#33268;&#26080;&#27861;&#23436;&#20840;&#21435;&#38500;&#19981;&#38656;&#35201;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#23398;&#20064;&#20174;&#27169;&#22411;&#30340;&#34920;&#31034;&#20013;&#21435;&#38500;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21518;&#26399;&#21644;&#23545;&#25239;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#34987;&#21457;&#29616;&#22312;&#20854;&#34920;&#31034;&#20013;&#32534;&#30721;&#20102;&#19981;&#33391;&#30340;&#35821;&#35328;&#25110;&#25935;&#24863;&#27010;&#24565;&#65292;&#31227;&#38500;&#36825;&#20123;&#27010;&#24565;&#26159;&#19981;&#23481;&#26131;&#30340;&#65292;&#22240;&#20026;&#27010;&#24565;&#12289;&#25991;&#26412;&#36755;&#20837;&#21644;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21518;&#26399;&#21644;&#23545;&#25239;&#24615;&#26041;&#27861;&#26469;&#20174;&#27169;&#22411;&#30340;&#34920;&#31034;&#20013;&#21435;&#38500;&#36825;&#20123;&#19981;&#38656;&#35201;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#26159;&#36866;&#24471;&#20854;&#21453;&#30340;&#65306;&#23427;&#20204;&#19981;&#33021;&#23436;&#20840;&#21435;&#38500;&#27010;&#24565;&#65292;&#32780;&#22312;&#26368;&#31967;&#31957;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#30772;&#22351;&#25152;&#26377;&#20219;&#21153;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#21407;&#22240;&#26159;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#25506;&#27979;&#20998;&#31867;&#22120;&#20316;&#20026;&#27010;&#24565;&#30340;&#20195;&#29702;&#12290;&#21363;&#20351;&#22312;&#27010;&#24565;&#30456;&#20851;&#29305;&#24449;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#23601;&#21487;&#20197;&#25552;&#20379;100%&#20934;&#30830;&#24615;&#30340;&#26368;&#26377;&#21033;&#26465;&#20214;&#19979;&#23398;&#20064;&#25506;&#27979;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#35777;&#26126;&#25506;&#27979;&#20998;&#31867;&#22120;&#24456;&#21487;&#33021;&#20250;&#20351;&#29992;&#38750;&#27010;&#24565;&#29305;&#24449;&#65292;&#22240;&#27492;&#21518;&#26399;&#25110;&#23545;&#25239;&#24615;&#22788;&#29702;&#26041;&#27861;&#23558;&#19981;&#33021;&#23436;&#20840;&#21435;&#38500;&#19981;&#38656;&#35201;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#35757;&#32451;&#27491;&#21017;&#21270;&#39033;&#26469;&#30452;&#25509;&#23398;&#20064;&#20174;&#27169;&#22411;&#34920;&#31034;&#20013;&#21435;&#38500;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#21435;&#38500;&#27010;&#24565;&#30340;&#21516;&#26102;&#20445;&#30041;&#20219;&#21153;&#30456;&#20851;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21518;&#26399;&#21644;&#23545;&#25239;&#24615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network models trained on text data have been found to encode undesirable linguistic or sensitive concepts in their representation. Removing such concepts is non-trivial because of a complex relationship between the concept, text input, and the learnt representation. Recent work has proposed post-hoc and adversarial methods to remove such unwanted concepts from a model's representation. Through an extensive theoretical and empirical analysis, we show that these methods can be counter-productive: they are unable to remove the concepts entirely, and in the worst case may end up destroying all task-relevant features. The reason is the methods' reliance on a probing classifier as a proxy for the concept. Even under the most favorable conditions for learning a probing classifier when a concept's relevant features in representation space alone can provide 100% accuracy, we prove that a probing classifier is likely to use non-concept features and thus post-hoc or adversarial methods wi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#23545;2017&#24180;&#33267;2020&#24180;&#38388;&#20445;&#21152;&#21033;&#20122;&#12289;&#25463;&#20811;&#12289;&#27861;&#22269;&#12289;&#26031;&#27931;&#25991;&#23612;&#20122;&#12289;&#35199;&#29677;&#29273;&#21644;&#33521;&#22269;&#20845;&#20010;&#22269;&#23478;&#30340;&#35758;&#20250;&#35760;&#24405;&#36827;&#34892;&#20102;&#32852;&#21512;&#21644;&#27604;&#36739;&#20998;&#26512;&#12290;&#32467;&#26524;&#26174;&#31034;&#20986;&#20102;&#36825;&#20123;&#22269;&#23478;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#20849;&#21516;&#28857;&#12290;</title><link>http://arxiv.org/abs/2207.01054</link><description>&lt;p&gt;
&#22810;&#26041;&#20301;&#22810;&#35821;&#35328;&#19982;&#36328;&#35821;&#35328;&#30340;&#35758;&#20250;&#28436;&#35762;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multi-aspect Multilingual and Cross-lingual Parliamentary Speech Analysis. (arXiv:2207.01054v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.01054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#65292;&#23545;2017&#24180;&#33267;2020&#24180;&#38388;&#20445;&#21152;&#21033;&#20122;&#12289;&#25463;&#20811;&#12289;&#27861;&#22269;&#12289;&#26031;&#27931;&#25991;&#23612;&#20122;&#12289;&#35199;&#29677;&#29273;&#21644;&#33521;&#22269;&#20845;&#20010;&#22269;&#23478;&#30340;&#35758;&#20250;&#35760;&#24405;&#36827;&#34892;&#20102;&#32852;&#21512;&#21644;&#27604;&#36739;&#20998;&#26512;&#12290;&#32467;&#26524;&#26174;&#31034;&#20986;&#20102;&#36825;&#20123;&#22269;&#23478;&#20043;&#38388;&#30340;&#24046;&#24322;&#21644;&#20849;&#21516;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35758;&#20250;&#21644;&#31435;&#27861;&#36777;&#35770;&#35760;&#24405;&#25552;&#20379;&#20102;&#26377;&#20851;&#36873;&#23450;&#25919;&#27835;&#23478;&#24847;&#35265;&#12289;&#31435;&#22330;&#21644;&#25919;&#31574;&#20559;&#22909;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#23427;&#20204;&#23545;&#25919;&#27835;&#21644;&#31038;&#20250;&#31185;&#23398;&#20197;&#21450;&#35821;&#35328;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#38750;&#24120;&#26377;&#36259;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#30740;&#31350;&#30740;&#31350;&#20102;&#20010;&#21035;&#30340;&#35758;&#20250;&#65292;&#20294;&#25105;&#20204;&#23558;&#20808;&#36827;&#30340;NLP&#26041;&#27861;&#24212;&#29992;&#20110;&#23545;2017&#24180;&#33267;2020&#24180;&#26399;&#38388;&#20845;&#20010;&#22269;&#23478;&#35758;&#20250;&#65288;&#20445;&#21152;&#21033;&#20122;&#12289;&#25463;&#20811;&#12289;&#27861;&#22269;&#12289;&#26031;&#27931;&#25991;&#23612;&#20122;&#12289;&#35199;&#29677;&#29273;&#21644;&#33521;&#22269;&#65289;&#36827;&#34892;&#32852;&#21512;&#21644;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;ParlaMint&#25968;&#25454;&#38598;&#25910;&#38598;&#30340;&#35760;&#24405;&#20013;&#30340;&#24773;&#24863;&#21644;&#24773;&#32490;&#65292;&#24182;&#35780;&#20272;&#26159;&#21542;&#21487;&#20197;&#20174;&#28436;&#35762;&#20013;&#26816;&#27979;&#21040;&#21457;&#35328;&#32773;&#30340;&#24180;&#40836;&#12289;&#24615;&#21035;&#21644;&#25919;&#27835;&#20542;&#21521;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#20998;&#26512;&#22269;&#23478;&#20043;&#38388;&#26377;&#19968;&#20123;&#20849;&#21516;&#28857;&#21644;&#35768;&#22810;&#20196;&#20154;&#24778;&#35766;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parliamentary and legislative debate transcripts provide informative insight into elected politicians' opinions, positions, and policy preferences. They are interesting for political and social sciences as well as linguistics and natural language processing (NLP) research. While existing research studied individual parliaments, we apply advanced NLP methods to a joint and comparative analysis of six national parliaments (Bulgarian, Czech, French, Slovene, Spanish, and United Kingdom) between 2017 and 2020. We analyze emotions and sentiment in the transcripts from the ParlaMint dataset collection and assess if the age, gender, and political orientation of speakers can be detected from their speeches. The results show some commonalities and many surprising differences among the analyzed countries.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;AMR&#22270;&#23545;&#40784;&#22120;&#65292;&#37319;&#29992;&#29616;&#20195;Transformer&#35299;&#26512;&#22120;&#32534;&#30721;&#23545;&#40784;&#20449;&#24687;&#65292;&#36991;&#20813;&#20351;&#29992;&#33521;&#35821;&#29305;&#23450;&#35268;&#21017;&#25110;EM&#31639;&#27861;&#65292;&#21516;&#26102;&#25552;&#20986;&#19968;&#31181;&#24341;&#23548;&#30417;&#30563;&#26041;&#27861;&#24182;&#22312;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2206.07587</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;AMR Aligner: &#37325;&#28857;&#20851;&#27880;&#20132;&#21449;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual AMR Aligner: Paying Attention to Cross-Attention. (arXiv:2206.07587v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;AMR&#22270;&#23545;&#40784;&#22120;&#65292;&#37319;&#29992;&#29616;&#20195;Transformer&#35299;&#26512;&#22120;&#32534;&#30721;&#23545;&#40784;&#20449;&#24687;&#65292;&#36991;&#20813;&#20351;&#29992;&#33521;&#35821;&#29305;&#23450;&#35268;&#21017;&#25110;EM&#31639;&#27861;&#65292;&#21516;&#26102;&#25552;&#20986;&#19968;&#31181;&#24341;&#23548;&#30417;&#30563;&#26041;&#27861;&#24182;&#22312;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;AMR&#22270;&#23545;&#40784;&#22120;&#65292;&#21487;&#20197;&#36328;&#36234;&#22810;&#31181;&#35821;&#35328;&#20197;&#25193;&#23637;&#20854;&#35268;&#27169;&#65292;&#22240;&#27492;&#33021;&#22815;&#23545;&#19981;&#21516;&#35821;&#35328;&#30340;&#21477;&#23376;&#20013;&#30340;&#21333;&#20803;&#21644;&#36328;&#24230;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#29616;&#20195;&#35299;&#26512;&#22120;&#65292;&#22312;&#20854;&#20132;&#21449;&#27880;&#24847;&#21147;&#26435;&#37325;&#20013;&#22266;&#26377;&#22320;&#32534;&#30721;&#23545;&#40784;&#20449;&#24687;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#22312;&#35299;&#26512;&#36807;&#31243;&#20013;&#25552;&#21462;&#27492;&#20449;&#24687;&#12290;&#36825;&#28040;&#38500;&#20102;&#20197;&#21069;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#33521;&#35821;&#29305;&#23450;&#35268;&#21017;&#25110;&#26399;&#26395;&#26368;&#22823;&#21270;(EM)&#31639;&#27861;&#30340;&#38656;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#40784;&#30340;&#24341;&#23548;&#30417;&#30563;&#26041;&#27861;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#25105;&#20204;&#30340;&#23545;&#40784;&#22120;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;AMR&#23545;&#40784;&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#23545;&#40784;&#22120;&#36328;&#22810;&#31181;&#35821;&#35328;&#33719;&#24471;&#36825;&#20123;&#32467;&#26524;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312; \href{https://www.github.com/Babelscape/AMR-alignment}{github.com/Babelscape/AMR-alignment} &#19978;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel aligner for Abstract Meaning Representation (AMR) graphs that can scale cross-lingually, and is thus capable of aligning units and spans in sentences of different languages. Our approach leverages modern Transformer-based parsers, which inherently encode alignment information in their cross-attention weights, allowing us to extract this information during parsing. This eliminates the need for English-specific rules or the Expectation Maximization (EM) algorithm that have been used in previous approaches. In addition, we propose a guided supervised method using alignment to further enhance the performance of our aligner. We achieve state-of-the-art results in the benchmarks for AMR alignment and demonstrate our aligner's ability to obtain them across multiple languages. Our code will be available at \href{https://www.github.com/Babelscape/AMR-alignment}{github.com/Babelscape/AMR-alignment}.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;-&#35821;&#38899;&#32852;&#21512;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#21518;&#23454;&#29616;&#20102;&#33258;&#21160;&#35789;&#35821;&#20998;&#21106;&#21644;&#32858;&#31867;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2203.15081</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#38899;&#33258;&#30417;&#30563;&#27169;&#22411;&#20013;&#30340;&#35789;&#35821;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Word Discovery in Visually Grounded, Self-Supervised Speech Models. (arXiv:2203.15081v5 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15081
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;-&#35821;&#38899;&#32852;&#21512;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#21518;&#23454;&#29616;&#20102;&#33258;&#21160;&#35789;&#35821;&#20998;&#21106;&#21644;&#32858;&#31867;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;-&#35821;&#38899;&#32852;&#21512;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#35789;&#35821;&#30340;&#33258;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#65292;&#24182;&#22312; Buckeye &#35789;&#20998;&#21106;&#21644; ZeroSpeech &#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#19982;&#24403;&#21069;&#24050;&#21457;&#34920;&#30340;&#26041;&#27861;&#30456;&#24403;&#30340;&#29978;&#33267;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#33021;&#21147;&#24182;&#26410;&#20986;&#29616;&#22312;&#22522;&#26412;&#30340; HuBERT &#21644; wav2vec2.0 &#27169;&#22411;&#20013;&#65292;&#35270;&#35273;&#32852;&#32467;&#20219;&#21153;&#26159;&#25105;&#20204;&#35266;&#23519;&#21040;&#30340;&#35789;&#35821;&#21457;&#29616;&#33021;&#21147;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for visually-grounded spoken term discovery. After training either a HuBERT or wav2vec2.0 model to associate spoken captions with natural images, we show that powerful word segmentation and clustering capability emerges within the model's self-attention heads. Our experiments reveal that this ability is not present to nearly the same extent in the base HuBERT and wav2vec2.0 models, suggesting that the visual grounding task is a crucial component of the word discovery capability we observe. We also evaluate our method on the Buckeye word segmentation and ZeroSpeech spoken term discovery tasks, where we perform on par with or better than currently published methods on several metrics. Code and model weights are available at https://github.com/jasonppy/word-discovery.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;&#31995;&#32479;&#23454;&#29992;&#32452;&#20214;&#65292;&#21487;&#33258;&#21160;&#26816;&#27979;&#21644;&#20462;&#27491;&#29992;&#25143;&#21457;&#20986;&#30340;&#35831;&#27714;&#20449;&#24687;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#20462;&#27491;&#20449;&#24687;&#36827;&#34892;&#25552;&#21462;&#23545;&#65292;&#20197;&#23454;&#29616;&#23398;&#20064;&#21644;&#36991;&#20813;&#37325;&#22797;&#24320;&#21457;&#30340;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31181;&#35821;&#24207;&#21015;&#26631;&#31614;&#12289;&#24207;&#21015;&#21040;&#24207;&#21015;&#21644;&#24207;&#21015;&#20998;&#31867;&#31561;&#24773;&#24418;&#12290;</title><link>http://arxiv.org/abs/2004.04243</link><description>&lt;p&gt;
&#35831;&#27714;&#23545;&#35805;&#30340;&#38169;&#35823;&#32416;&#27491;&#21644;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Error correction and extraction in request dialogs. (arXiv:2004.04243v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2004.04243
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;&#31995;&#32479;&#23454;&#29992;&#32452;&#20214;&#65292;&#21487;&#33258;&#21160;&#26816;&#27979;&#21644;&#20462;&#27491;&#29992;&#25143;&#21457;&#20986;&#30340;&#35831;&#27714;&#20449;&#24687;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#20462;&#27491;&#20449;&#24687;&#36827;&#34892;&#25552;&#21462;&#23545;&#65292;&#20197;&#23454;&#29616;&#23398;&#20064;&#21644;&#36991;&#20813;&#37325;&#22797;&#24320;&#21457;&#30340;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22810;&#31181;&#35821;&#24207;&#21015;&#26631;&#31614;&#12289;&#24207;&#21015;&#21040;&#24207;&#21015;&#21644;&#24207;&#21015;&#20998;&#31867;&#31561;&#24773;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;&#31995;&#32479;&#23454;&#29992;&#32452;&#20214;&#65292;&#21487;&#20197;&#33719;&#21462;&#29992;&#25143;&#30340;&#26368;&#21518;&#20004;&#20010;&#35805;&#35821;&#65292;&#24182;&#26816;&#27979;&#26368;&#21518;&#19968;&#21477;&#35805;&#26159;&#21542;&#26159;&#23545;&#31532;&#20108;&#21477;&#35805;&#30340;&#38169;&#35823;&#32416;&#27491;&#12290;&#22914;&#26524;&#26159;&#65292;&#21017;&#26681;&#25454;&#26368;&#21518;&#19968;&#21477;&#35805;&#20013;&#30340;&#38169;&#35823;&#32416;&#27491;&#26469;&#32416;&#27491;&#31532;&#20108;&#21477;&#35805;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#32452;&#20214;&#36755;&#20986;&#20102;&#34987;&#20462;&#22797;&#21644;&#20462;&#22797;&#20307;&#30340;&#25552;&#21462;&#23545;&#12290;&#36825;&#20010;&#32452;&#20214;&#25552;&#20379;&#20102;&#20004;&#20010;&#20248;&#28857;&#65292;&#19968;&#26159;&#23398;&#20064;&#32416;&#27491;&#30340;&#27010;&#24565;&#20197;&#36991;&#20813;&#20026;&#27599;&#20010;&#26032;&#22495;&#25910;&#38598;&#32416;&#27491;&#65292;&#20108;&#26159;&#25552;&#21462;&#34987;&#20462;&#22797;&#21644;&#20462;&#22797;&#23545;&#65292;&#20174;&#32780;&#25552;&#20379;&#23398;&#20064;&#30340;&#21487;&#33021;&#24615;&#12290;&#23545;&#20110;&#38169;&#35823;&#32416;&#27491;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#26631;&#31614;&#21644;&#20004;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#26041;&#27861;&#12290;&#23545;&#20110;&#38169;&#35823;&#32416;&#27491;&#26816;&#27979;&#65292;&#36825;&#19977;&#31181;&#38169;&#35823;&#32416;&#27491;&#26041;&#27861;&#20063;&#21487;&#20197;&#34987;&#29992;&#26469;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#24207;&#21015;&#20998;&#31867;&#26041;&#27861;&#12290;&#19968;&#20010;&#38169;&#35823;&#32416;&#27491;&#26816;&#27979;&#21644;&#19968;&#20010;&#38169;&#35823;&#32416;&#27491;&#26041;&#27861;&#21487;&#20197;&#32452;&#21512;&#25104;&#19968;&#20010;&#27969;&#27700;&#32447;&#65292;&#25110;&#32773;&#38169;&#35823;&#32416;&#27491;&#26041;&#27861;&#21487;&#20197;&#34987;&#20998;&#21035;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a dialog system utility component that gets the two last utterances of a user and can detect whether the last utterance is an error correction of the second last utterance. If yes, it corrects the second last utterance according to the error correction in the last utterance. In addition, the proposed component outputs the extracted pairs of reparandum and repair entity. This component offers two advantages, learning the concept of corrections to avoid collecting corrections for every new domain and extracting reparandum and repair pairs, which offers the possibility to learn out of it.  For the error correction one sequence labeling and two sequence to sequence approaches are presented. For the error correction detection these three error correction approaches can also be used and in addition, we present a sequence classification approach. One error correction detection and one error correction approach can be combined to a pipeline or the error correction approaches can be 
&lt;/p&gt;</description></item></channel></rss>