<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#21644;&#22768;&#26126;&#24615;&#20219;&#21153;&#35268;&#33539;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.09656</link><description>&lt;p&gt;
&#22768;&#26126;&#25552;&#31034;&#19979;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Satisfiability-Aided Language Models Using Declarative Prompting. (arXiv:2305.09656v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#21644;&#22768;&#26126;&#24615;&#20219;&#21153;&#35268;&#33539;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#22768;&#26126;&#24615;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#21033;&#29992;&#19968;&#20010;&#29616;&#25104;&#30340;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#24471;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#20248;&#28857;&#65306;&#31532;&#19968;&#65292;&#22768;&#26126;&#24615;&#35268;&#33539;&#27604;&#25512;&#29702;&#27493;&#39588;&#26356;&#25509;&#36817;&#38382;&#39064;&#25551;&#36848;&#65292;&#22240;&#27492;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35299;&#26512;&#23427;&#65307;&#31532;&#20108;&#65292;&#36890;&#36807;&#23558;&#23454;&#38469;&#25512;&#29702;&#20219;&#21153;&#22996;&#25176;&#32473;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20445;&#35777;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has combined chain-of-thought prompting in large language models (LLMs) with programmatic representations to perform effective and transparent reasoning. While such an approach works very well for tasks that only require forward reasoning (e.g., straightforward arithmetic), it is less effective for constraint solving tasks that require more sophisticated planning and search. In this paper, we propose a new satisfiability-aided language modeling approach for improving the reasoning capabilities of LLMs. We use an LLM to generate a declarative task specification rather than an imperative program and leverage an off-the-shelf automated theorem prover to derive the final answer. This approach has two key advantages. The declarative specification is closer to the problem description than the reasoning steps are, so the LLM can parse it more accurately. Furthermore, by offloading the actual reasoning task to an automated theorem prover, our approach can guarantee the correctness o
&lt;/p&gt;</description></item><item><title>&#35821;&#38899;&#32763;&#35793;(ST)&#26159;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;&#30340;&#33391;&#22909;&#25163;&#27573;&#12290;&#36890;&#36807;&#24341;&#20837;ST&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21333;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#22330;&#26223;&#19979;&#34920;&#29616;&#22343;&#22909;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09652</link><description>&lt;p&gt;
&#35299;&#37322;&#22120;&#29702;&#35299;&#24744;&#30340;&#24847;&#24605;: &#30001;&#35821;&#38899;&#32763;&#35793;&#21327;&#21161;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
The Interpreter Understands Your Meaning: End-to-end Spoken Language Understanding Aided by Speech Translation. (arXiv:2305.09652v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09652
&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#32763;&#35793;(ST)&#26159;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;&#30340;&#33391;&#22909;&#25163;&#27573;&#12290;&#36890;&#36807;&#24341;&#20837;ST&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21333;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#22330;&#26223;&#19979;&#34920;&#29616;&#22343;&#22909;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24403;&#21069;&#22312;&#25991;&#26412;&#21644;&#35821;&#38899;&#19978;&#26377;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#22810;&#35821;&#35328;&#24773;&#20917;&#19979;&#12290;&#26426;&#22120;&#32763;&#35793;&#24050;&#34987;&#30830;&#23450;&#20026;&#24378;&#22823;&#30340;&#25991;&#26412;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#22240;&#20026;&#23427;&#20351;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#36755;&#20837;&#35821;&#21477;&#30340;&#39640;&#32423;&#35821;&#20041;&#21644;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#36825;&#23545;&#20110;&#22788;&#29702;&#26356;&#20302;&#32423;&#21035;&#30340;&#22768;&#23398;&#24103;&#30340;&#35821;&#38899;&#27169;&#22411;&#38750;&#24120;&#26377;&#29992;&#12290;&#26412;&#25991;&#29305;&#21035;&#38024;&#23545;&#36328;&#35821;&#35328;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;&#35821;&#38899;&#32763;&#35793;(ST)&#26159;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#31471;&#21040;&#31471;&#21475;&#35821;&#29702;&#35299;&#30340;&#33391;&#22909;&#25163;&#27573;&#65292;&#26080;&#35770;&#26159;&#22312;&#21333;&#35821;&#35328;&#22330;&#26223;&#36824;&#26159;&#36328;&#35821;&#35328;&#22330;&#26223;&#19979;&#12290;&#36890;&#36807;&#24341;&#20837;ST&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20351;&#29992;SLURP&#12289;MINDS-14&#21644;NMSQA&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#21333;&#35821;&#35328;&#21644;&#22810;&#35821;&#35328;&#24847;&#22270;&#20998;&#31867;&#20197;&#21450;&#21475;&#35821;&#38382;&#31572;&#26102;&#65292;&#30456;&#27604;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#22343;&#20855;&#26377;&#26356;&#39640;&#24615;&#33021;&#12290;&#20026;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#20004;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#26469;&#33258;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end spoken language understanding (SLU) remains elusive even with current large pretrained language models on text and speech, especially in multilingual cases. Machine translation has been established as a powerful pretraining objective on text as it enables the model to capture high-level semantics of the input utterance and associations between different languages, which is desired for speech models that work on lower-level acoustic frames. Motivated particularly by the task of cross-lingual SLU, we demonstrate that the task of speech translation (ST) is a good means of pretraining speech models for end-to-end SLU on both monolingual and cross-lingual scenarios.  By introducing ST, our models give higher performance over current baselines on monolingual and multilingual intent classification as well as spoken question answering using SLURP, MINDS-14, and NMSQA benchmarks. To verify the effectiveness of our methods, we also release two new benchmark datasets from both syntheti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#25351;&#23548;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#31216;&#20026;LGTM&#65292;&#20854;&#21033;&#29992;&#33976;&#39311;&#25928;&#24212;&#36873;&#25321;&#26679;&#26412;&#20197;&#22686;&#24378;&#23398;&#29983;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;10&#20010;&#24120;&#35265;&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09651</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#25351;&#23548;&#26377;&#21161;&#20110;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation. (arXiv:2305.09651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#25351;&#23548;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#31216;&#20026;LGTM&#65292;&#20854;&#21033;&#29992;&#33976;&#39311;&#25928;&#24212;&#36873;&#25321;&#26679;&#26412;&#20197;&#22686;&#24378;&#23398;&#29983;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;10&#20010;&#24120;&#35265;&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30740;&#31350;&#34920;&#26126;&#65292;&#33021;&#21147;&#36229;&#32676;&#30340;&#25945;&#24072;&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#35753;&#23398;&#29983;&#27700;&#24179;&#24471;&#21040;&#25552;&#21319;&#65292;&#36825;&#20984;&#26174;&#20102;&#24403;&#21069;&#25945;&#24072;&#22521;&#35757;&#23454;&#36341;&#21644;&#26377;&#25928;&#30693;&#35782;&#20256;&#25480;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#25945;&#24072;&#22521;&#35757;&#36807;&#31243;&#30340;&#25351;&#23548;&#25928;&#26524;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#33976;&#39311;&#25928;&#24212;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#23545;&#23398;&#29983;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#22909;&#25945;&#24072;&#24456;&#37325;&#35201;&#65288;LGTM&#65289;&#30340;&#26377;&#25928;&#35757;&#32451;&#25216;&#26415;&#65292;&#20197;&#23558;&#33976;&#39311;&#25928;&#24212;&#32435;&#20837;&#25945;&#24072;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#21487;&#33021;&#25552;&#21319;&#23398;&#29983;&#27867;&#21270;&#33021;&#21147;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#30340;LGTM&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;10&#20010;&#24120;&#35265;&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student's generalization ability. In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher's learning process. By prioritizing samples that are likely to enhance the student's generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#21644;&#22238;&#28335;&#25512;&#29702;&#26041;&#38754;&#20248;&#31168;&#30340;&#25104;&#26524;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#26041;&#38754;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.09620</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#35843;&#26597;&#65306;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys. (arXiv:2305.09620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#21644;&#22238;&#28335;&#25512;&#29702;&#26041;&#38754;&#20248;&#31168;&#30340;&#25104;&#26524;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#26041;&#38754;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#35266;&#28857;&#39044;&#27979;&#20013;&#65292;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#65292;&#22238;&#28335;&#25512;&#29702;&#21644;&#38646;&#27425;&#39044;&#27979;&#19977;&#20010;&#19981;&#21516;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#23558;&#35843;&#26597;&#38382;&#39064;&#12289;&#20010;&#20154;&#20449;&#24565;&#21644;&#26102;&#38388;&#32972;&#26223;&#30340;&#31070;&#32463;&#23884;&#20837;&#24341;&#20837;&#21040;&#35266;&#28857;&#39044;&#27979;&#30340;&#20010;&#24615;&#21270;LLMs&#20013;&#12290;&#22312;1972&#24180;&#21040;2021&#24180;&#30340;&#8220;&#24120;&#35268;&#31038;&#20250;&#35843;&#26597;&#8221;&#20013;&#65292;&#25105;&#20204;&#20174;68,846&#21517;&#32654;&#22269;&#20154;&#20013;&#33719;&#24471;&#20102;3,110&#20010;&#20108;&#36827;&#21046;&#35266;&#28857;&#65292;&#22312;Alpaca-7b&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#26524;&#65292;&#22312;&#32570;&#22833;&#25968;&#25454;&#25554;&#20540;&#65288;AUC=0.87&#65292;&#20844;&#24320;&#35266;&#28857;&#39044;&#27979;&#20026;$\rho$=0.99&#65289;&#21644;&#22238;&#28335;&#25512;&#29702;&#65288;AUC=0.86&#65292;$\rho$=0.98&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#26174;&#33879;&#30340;&#39044;&#27979;&#33021;&#21147;&#33021;&#22815;&#20197;&#39640;&#32622;&#20449;&#24230;&#22635;&#34917;&#32570;&#22833;&#30340;&#36235;&#21183;&#65292;&#24182;&#26631;&#26126;&#20844;&#20247;&#24577;&#24230;&#20309;&#26102;&#21457;&#29983;&#21464;&#21270;&#65292;&#22914;&#21516;&#24615;&#23130;&#23035;&#30340;&#33719;&#21462;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#21463;&#21040;&#38480;&#21046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we use large language models (LLMs) to augment surveys? This paper investigates three distinct applications of LLMs fine-tuned by nationally representative surveys for opinion prediction -- missing data imputation, retrodiction, and zero-shot prediction. We present a new methodological framework that incorporates neural embeddings of survey questions, individual beliefs, and temporal contexts to personalize LLMs in opinion prediction. Among 3,110 binarized opinions from 68,846 Americans in the General Social Survey from 1972 to 2021, our best models based on Alpaca-7b excels in missing data imputation (AUC = 0.87 for personal opinion prediction and $\rho$ = 0.99 for public opinion prediction) and retrodiction (AUC = 0.86, $\rho$ = 0.98). These remarkable prediction capabilities allow us to fill in missing trends with high confidence and pinpoint when public attitudes changed, such as the rising support for same-sex marriage. However, the models show limited performance in a zer
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Med-PaLM2&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#30784;LLM&#25913;&#36827;&#12289;&#21307;&#23398;&#39046;&#22495;&#24494;&#35843;&#21644;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#29992;&#26032;&#39062;&#30340;&#38598;&#25104;&#31934;&#28860;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;MedQA&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;86.5%&#30340;&#21307;&#23398;&#38382;&#31572;&#20934;&#30830;&#29575;&#65292;&#36808;&#21521;&#21307;&#23398;&#19987;&#23478;&#32423;&#21035;&#30340;&#38382;&#31572;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.09617</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#38382;&#31572;&#20013;&#30340;&#24212;&#29992;&#65306;&#36808;&#21521;&#21307;&#23398;&#19987;&#23478;&#32423;&#21035;&#30340;&#38382;&#31572;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Towards Expert-Level Medical Question Answering with Large Language Models. (arXiv:2305.09617v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Med-PaLM2&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#30784;LLM&#25913;&#36827;&#12289;&#21307;&#23398;&#39046;&#22495;&#24494;&#35843;&#21644;&#25552;&#31034;&#31574;&#30053;&#65292;&#24182;&#29992;&#26032;&#39062;&#30340;&#38598;&#25104;&#31934;&#28860;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22312;MedQA&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;86.5%&#30340;&#21307;&#23398;&#38382;&#31572;&#20934;&#30830;&#29575;&#65292;&#36808;&#21521;&#21307;&#23398;&#19987;&#23478;&#32423;&#21035;&#30340;&#38382;&#31572;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#35832;&#22914;&#22260;&#26827;&#21644;&#34507;&#30333;&#36136;&#25240;&#21472;&#31561;&#8220;&#23439;&#20255;&#25361;&#25112;&#8221;&#26041;&#38754;&#21462;&#24471;&#20102;&#37324;&#31243;&#30865;&#24335;&#30340;&#36827;&#23637;&#12290;&#20294;&#22238;&#31572;&#21307;&#23398;&#38382;&#39064;&#24182;&#20687;&#21307;&#29983;&#19968;&#26679;&#36827;&#34892;&#25512;&#29702;&#34987;&#35748;&#20026;&#20063;&#26159;&#19968;&#31181;&#23439;&#20255;&#25361;&#25112;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65307;Med-PaLM&#26159;&#31532;&#19968;&#20010;&#22312;MedQA&#25968;&#25454;&#38598;&#19978;&#20197;67.2&#65285;&#30340;&#20998;&#25968;&#36229;&#36807;&#32654;&#22269;&#21307;&#30103;&#25191;&#19994;&#32771;&#35797;&#65288;USMLE&#65289;&#26679;&#24335;&#38382;&#39064;&#30340;&#8220;&#21450;&#26684;&#8221;&#20998;&#25968;&#30340;&#27169;&#22411;&#12290; &#28982;&#32780;&#65292;&#23545;&#27604;&#27169;&#22411;&#31572;&#26696;&#21644;&#21307;&#29983;&#31572;&#26696;&#65292;&#36825;&#39033;&#21644;&#20854;&#20182;&#20808;&#21069;&#24037;&#20316;&#34920;&#26126;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Med-PaLM2&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#30784;LLM&#25913;&#36827;&#65288;PaLM2&#65289;&#12289;&#21307;&#23398;&#39046;&#22495;&#24494;&#35843;&#21644;&#25552;&#31034;&#31574;&#30053;&#65288;&#21253;&#25324;&#26032;&#39062;&#30340;&#38598;&#25104;&#31934;&#28860;&#26041;&#27861;&#65289;&#26469;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#12290;&#22312;MedQA&#25968;&#25454;&#38598;&#19978;&#65292;Med-PaLM2&#30340;&#24471;&#20998;&#21487;&#36798;86.5&#65285;&#65292;&#27604;Med-PaLM&#25552;&#39640;&#20102;&#36229;&#36807;11&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent artificial intelligence (AI) systems have reached milestones in "grand challenges" ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge.  Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a "passing" score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach.  Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25351;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#20869;&#32622;&#25628;&#32034;&#24341;&#25806;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20123;&#19978;&#19979;&#25991;&#28436;&#31034;&#30452;&#25509;&#29983;&#25104;Web URLs&#65292;&#22312;&#25991;&#26723;&#26816;&#32034;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.09612</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20869;&#32622;&#30340;&#33258;&#22238;&#24402;&#25628;&#32034;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Built-in Autoregressive Search Engines. (arXiv:2305.09612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25351;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#20869;&#32622;&#25628;&#32034;&#24341;&#25806;&#65292;&#36890;&#36807;&#25552;&#20379;&#19968;&#20123;&#19978;&#19979;&#25991;&#28436;&#31034;&#30452;&#25509;&#29983;&#25104;Web URLs&#65292;&#22312;&#25991;&#26723;&#26816;&#32034;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#26816;&#32034;&#26159;&#26631;&#20934;&#32593;&#32476;&#25628;&#32034;&#24341;&#25806;&#30340;&#20851;&#38190;&#38454;&#27573;&#12290;&#29616;&#26377;&#30340;&#21452;&#32534;&#30721;&#22120;&#23494;&#38598;&#26816;&#32034;&#22120;&#29420;&#31435;&#22320;&#33719;&#21462;&#38382;&#39064;&#21644;&#25991;&#26723;&#30340;&#34920;&#31034;&#65292;&#21482;&#20801;&#35768;&#23427;&#20204;&#20043;&#38388;&#30340;&#27973;&#23618;&#20132;&#20114;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#26368;&#36817;&#30340;&#33258;&#22238;&#24402;&#25628;&#32034;&#24341;&#25806;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#20505;&#36873;&#27744;&#20013;&#30456;&#20851;&#25991;&#26723;&#30340;&#26631;&#35782;&#31526;&#26469;&#26367;&#25442;&#21452;&#32534;&#30721;&#22120;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33258;&#22238;&#24402;&#25628;&#32034;&#24341;&#25806;&#30340;&#35757;&#32451;&#25104;&#26412;&#38543;&#30528;&#20505;&#36873;&#25991;&#26723;&#25968;&#37327;&#30340;&#22686;&#21152;&#32780;&#24613;&#21095;&#19978;&#21319;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#20197;&#36981;&#24490;&#20154;&#31867;&#25351;&#31034;&#30452;&#25509;&#29983;&#25104;&#25991;&#26723;&#26816;&#32034;&#30340;URL&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#24403;&#25552;&#20379;&#19968;&#20123;{Query-URL}&#23545;&#20316;&#20026;&#19978;&#19979;&#25991;&#28436;&#31034;&#26102;&#65292;LLMs&#21487;&#20197;&#29983;&#25104;Web URL&#65292;&#20854;&#20013;&#36817;90&#65285;&#30340;&#30456;&#24212;&#25991;&#26723;&#21253;&#21547;&#24320;&#25918;&#22495;&#38382;&#39064;&#30340;&#27491;&#30830;&#31572;&#26696;&#12290;&#36825;&#26679;&#65292;LLMs&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#20869;&#32622;&#25628;&#32034;&#24341;&#25806;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#26126;&#30830;&#35757;&#32451;&#20197;&#26144;&#23556;&#38382;&#39064;&#21644;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document retrieval is a key stage of standard Web search engines. Existing dual-encoder dense retrievers obtain representations for questions and documents independently, allowing for only shallow interactions between them. To overcome this limitation, recent autoregressive search engines replace the dual-encoder architecture by directly generating identifiers for relevant documents in the candidate pool. However, the training cost of such autoregressive search engines rises sharply as the number of candidate documents increases. In this paper, we find that large language models (LLMs) can follow human instructions to directly generate URLs for document retrieval.  Surprisingly, when providing a few {Query-URL} pairs as in-context demonstrations, LLMs can generate Web URLs where nearly 90\% of the corresponding documents contain correct answers to open-domain questions. In this way, LLMs can be thought of as built-in search engines, since they have not been explicitly trained to map qu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21435;&#22122;&#32467;&#26500;&#21040;&#25991;&#26412;&#22686;&#24378;&#26694;&#26550;&#65288;DAEE&#65289;&#65292;&#36890;&#36807;&#22522;&#20110;&#30693;&#35782;&#30340;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#38468;&#21152;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36845;&#20195;&#22320;&#36873;&#25321;&#26377;&#25928;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#20197;&#35299;&#20915;&#20107;&#20214;&#25552;&#21462;&#20013;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09598</link><description>&lt;p&gt;
&#29992;&#21435;&#22122;&#32467;&#26500;&#21040;&#25991;&#26412;&#22686;&#24378;&#25216;&#26415;&#25552;&#21319;&#20107;&#20214;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Boosting Event Extraction with Denoised Structure-to-Text Augmentation. (arXiv:2305.09598v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21435;&#22122;&#32467;&#26500;&#21040;&#25991;&#26412;&#22686;&#24378;&#26694;&#26550;&#65288;DAEE&#65289;&#65292;&#36890;&#36807;&#22522;&#20110;&#30693;&#35782;&#30340;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#38468;&#21152;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36845;&#20195;&#22320;&#36873;&#25321;&#26377;&#25928;&#30340;&#25968;&#25454;&#23376;&#38598;&#65292;&#20197;&#35299;&#20915;&#20107;&#20214;&#25552;&#21462;&#20013;&#26631;&#27880;&#25968;&#25454;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25552;&#21462;&#26088;&#22312;&#20174;&#25991;&#26412;&#20013;&#35782;&#21035;&#39044;&#23450;&#20041;&#30340;&#20107;&#20214;&#35302;&#21457;&#22120;&#21644;&#21442;&#25968;&#65292;&#20294;&#36825;&#31181;&#20219;&#21153;&#24120;&#24120;&#21463;&#21046;&#20110;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#26368;&#36817;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#32463;&#24120;&#22240;&#35821;&#27861;&#19981;&#27491;&#30830;&#12289;&#32467;&#26500;&#19981;&#21305;&#37197;&#21644;&#35821;&#20041;&#28418;&#31227;&#31561;&#38382;&#39064;&#32780;&#23548;&#33268;&#24615;&#33021;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20107;&#20214;&#25552;&#21462;&#30340;&#21435;&#22122;&#32467;&#26500;&#21040;&#25991;&#26412;&#22686;&#24378;&#26694;&#26550;&#65292;&#31216;&#20026;DAEE&#65292;&#36890;&#36807;&#22522;&#20110;&#30693;&#35782;&#30340;&#32467;&#26500;&#21040;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#38468;&#21152;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#28145;&#23618;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36845;&#20195;&#22320;&#36873;&#25321;&#29983;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#23376;&#38598;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#26356;&#20855;&#22810;&#26679;&#24615;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event extraction aims to recognize pre-defined event triggers and arguments from texts, which suffer from the lack of high-quality annotations. In most NLP applications, involving a large scale of synthetic training data is a practical and effective approach to alleviate the problem of data scarcity. However, when applying to the task of event extraction, recent data augmentation methods often neglect the problem of grammatical incorrectness, structure misalignment, and semantic drifting, leading to unsatisfactory performances. In order to solve these problems, we propose a denoised structure-to-text augmentation framework for event extraction DAEE, which generates additional training data through the knowledge-based structure-to-text generation model and selects the effective subset from the generated data iteratively with a deep reinforcement learning agent. Experimental results on several datasets demonstrate that the proposed method generates more diverse text representations for e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;UOR&#65292;&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;&#35302;&#21457;&#22120;&#24182;&#23398;&#20064;&#36890;&#29992;&#36755;&#20986;&#34920;&#31034;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;99.3&#65285;&#65292;&#33021;&#22815;&#23545;&#22810;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#19979;&#28216;&#20219;&#21153;&#23454;&#26045;&#25915;&#20987;&#65292;&#19988;&#21487;&#31361;&#30772;&#26368;&#26032;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09574</link><description>&lt;p&gt;
UOR&#65306;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
UOR: Universal Backdoor Attacks on Pre-trained Language Models. (arXiv:2305.09574v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;UOR&#65292;&#21487;&#20197;&#33258;&#21160;&#36873;&#25321;&#35302;&#21457;&#22120;&#24182;&#23398;&#20064;&#36890;&#29992;&#36755;&#20986;&#34920;&#31034;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;99.3&#65285;&#65292;&#33021;&#22815;&#23545;&#22810;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#19979;&#28216;&#20219;&#21153;&#23454;&#26045;&#25915;&#20987;&#65292;&#19988;&#21487;&#31361;&#30772;&#26368;&#26032;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#26893;&#20837;&#21518;&#38376;&#21487;&#20197;&#20256;&#36882;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#36825;&#23545;&#23433;&#20840;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21518;&#38376;&#25915;&#20987;&#22823;&#37117;&#26159;&#38750;&#30446;&#26631;&#21644;&#29305;&#23450;&#20219;&#21153;&#30340;&#12290;&#24456;&#23569;&#26377;&#38024;&#23545;&#30446;&#26631;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#24615;&#30340;&#26041;&#27861;&#20351;&#29992;&#25163;&#21160;&#39044;&#23450;&#20041;&#30340;&#35302;&#21457;&#22120;&#21644;&#36755;&#20986;&#34920;&#31034;&#65292;&#36825;&#20351;&#24471;&#25915;&#20987;&#25928;&#26524;&#19981;&#22815;&#24378;&#22823;&#21644;&#26222;&#36866;&#12290;&#26412;&#25991;&#39318;&#20808;&#24635;&#32467;&#20102;&#19968;&#20010;&#26356;&#20855;&#23041;&#32961;&#24615;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21518;&#38376;&#25915;&#20987;&#24212;&#28385;&#36275;&#30340;&#35201;&#27714;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;UOR&#65292;&#36890;&#36807;&#23558;&#25163;&#21160;&#36873;&#25321;&#21464;&#25104;&#33258;&#21160;&#20248;&#21270;&#65292;&#25171;&#30772;&#20102;&#20197;&#24448;&#26041;&#27861;&#30340;&#29942;&#39048;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#34987;&#27745;&#26579;&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#21508;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#35302;&#21457;&#22120;&#30340;&#26356;&#21152;&#22343;&#21248;&#21644;&#36890;&#29992;&#36755;&#20986;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#26799;&#24230;&#25628;&#32034;&#36873;&#21462;&#36866;&#24403;&#30340;&#35302;&#21457;&#35789;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#35789;&#27719;&#34920;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;UOR&#21487;&#20197;&#22312;&#21508;&#31181;PLMs&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#23454;&#29616;&#39640;&#21518;&#38376;&#25104;&#21151;&#29575;&#65288;&#39640;&#36798;99.3&#65285;&#65289;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;UOR&#36824;&#21487;&#20197;&#31361;&#30772;&#23545;&#25239;&#21518;&#38376;&#25915;&#20987;&#30340;&#26368;&#26032;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoors implanted in pre-trained language models (PLMs) can be transferred to various downstream tasks, which exposes a severe security threat. However, most existing backdoor attacks against PLMs are un-targeted and task-specific. Few targeted and task-agnostic methods use manually pre-defined triggers and output representations, which prevent the attacks from being more effective and general. In this paper, we first summarize the requirements that a more threatening backdoor attack against PLMs should satisfy, and then propose a new backdoor attack method called UOR, which breaks the bottleneck of the previous approach by turning manual selection into automatic optimization. Specifically, we define poisoned supervised contrastive learning which can automatically learn the more uniform and universal output representations of triggers for various PLMs. Moreover, we use gradient search to select appropriate trigger words which can be adaptive to different PLMs and vocabularies. Experi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33322;&#31354;&#39046;&#22495;&#30340;&#21477;&#23376;&#21464;&#25442;&#22120;&#35843;&#25972;&#26041;&#27861;&#65292;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;TSDAE&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#28982;&#21518;&#22312;&#23569;&#37327;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#33322;&#31354;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09556</link><description>&lt;p&gt;
&#38024;&#23545;&#33322;&#31354;&#39046;&#22495;&#36827;&#34892;&#21477;&#23376;&#21464;&#25442;&#22120;&#30340;&#36866;&#24212;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adapting Sentence Transformers for the Aviation Domain. (arXiv:2305.09556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33322;&#31354;&#39046;&#22495;&#30340;&#21477;&#23376;&#21464;&#25442;&#22120;&#35843;&#25972;&#26041;&#27861;&#65292;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20351;&#29992;TSDAE&#27169;&#22411;&#36827;&#34892;&#25913;&#36827;&#65292;&#28982;&#21518;&#22312;&#23569;&#37327;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#22312;&#33322;&#31354;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#26377;&#25928;&#30340;&#21477;&#23376;&#34920;&#31034;&#23545;&#20110;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#35821;&#20041;&#25628;&#32034;&#12289;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#21644;&#32858;&#31867;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#20010;&#29992;&#20110;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#30340;&#21464;&#24418;&#22120;&#27169;&#22411;&#65292;&#20294;&#26159;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#20855;&#26377;&#21807;&#19968;&#29305;&#24449;&#30340;&#19987;&#19994;&#39046;&#22495;&#26102;&#65292;&#22914;&#33322;&#31354;&#39046;&#22495;&#65292;&#21487;&#33021;&#26080;&#27861;&#21457;&#25381;&#26368;&#20339;&#24615;&#33021;&#65292;&#22240;&#20026;&#33322;&#31354;&#39046;&#22495;&#21253;&#21547;&#29305;&#27530;&#26415;&#35821;&#12289;&#32553;&#20889;&#35789;&#21644;&#38750;&#20256;&#32479;&#35821;&#27861;&#31561;&#39046;&#22495;&#29305;&#26377;&#29305;&#28857;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#20351;&#24471;&#38590;&#20197;&#19987;&#38376;&#35757;&#32451;&#33322;&#31354;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33322;&#31354;&#39046;&#22495;&#35843;&#25972;&#21477;&#23376;&#21464;&#25442;&#22120;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#36807;&#31243;&#65292;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#20351;&#29992;&#21547;&#33322;&#31354;&#25991;&#26412;&#25968;&#25454;&#30340;&#21464;&#24418;&#22120;&#21644;&#24207;&#21015;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;(TSDAE)&#20316;&#20026;&#36755;&#20837;&#26469;&#25552;&#39640;&#21021;&#22987;&#27169;&#22411;&#24615;&#33021;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#27880;&#37322;&#30340;&#33322;&#31354;&#25968;&#25454;&#38598;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20219;&#21153;&#26469;&#24494;&#35843;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#22312;&#20960;&#20010;&#19982;&#33322;&#31354;&#30456;&#20851;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#21464;&#25442;&#27169;&#22411;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning effective sentence representations is crucial for many Natural Language Processing (NLP) tasks, including semantic search, semantic textual similarity (STS), and clustering. While multiple transformer models have been developed for sentence embedding learning, these models may not perform optimally when dealing with specialized domains like aviation, which has unique characteristics such as technical jargon, abbreviations, and unconventional grammar. Furthermore, the absence of labeled datasets makes it difficult to train models specifically for the aviation domain. To address these challenges, we propose a novel approach for adapting sentence transformers for the aviation domain. Our method is a two-stage process consisting of pre-training followed by fine-tuning. During pre-training, we use Transformers and Sequential Denoising AutoEncoder (TSDAE) with aviation text data as input to improve the initial model performance. Subsequently, we fine-tune our models using a Natural 
&lt;/p&gt;</description></item><item><title>&#8220;Life of PII&#8221;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#28102;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;PII&#36716;&#21270;&#20026;&#20154;&#36896;PII&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#28151;&#28102;&#25991;&#26723;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26723;&#30340;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09550</link><description>&lt;p&gt;
PII&#30340;&#29983;&#21629;--&#19968;&#31181;PII&#28151;&#28102;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Life of PII -- A PII Obfuscation Transformer. (arXiv:2305.09550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09550
&lt;/p&gt;
&lt;p&gt;
&#8220;Life of PII&#8221;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#28102;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;PII&#36716;&#21270;&#20026;&#20154;&#36896;PII&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#28151;&#28102;&#25991;&#26723;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26723;&#30340;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#26381;&#21153;&#30340;&#19990;&#30028;&#20013;&#65292;&#20445;&#25252;&#25935;&#24863;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#25968;&#25454;&#25200;&#21160;&#25216;&#26415;&#26469;&#20943;&#23569;(&#25935;&#24863;)&#20010;&#20154;&#36523;&#20221;&#35782;&#21035;&#20449;&#24687;(PII)&#25968;&#25454;&#30340;&#36807;&#24230;&#23454;&#29992;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;&#25968;&#25454;&#25200;&#21160;&#26041;&#27861;&#32463;&#24120;&#23548;&#33268;&#26174;&#30528;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;PII&#30340;&#29983;&#21629;&#8221;--&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#28102;&#21464;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;PII&#36716;&#21270;&#20026;&#20154;&#36896;PII&#21516;&#26102;&#23613;&#21487;&#33021;&#22320;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12289;&#24847;&#22270;&#21644;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20010;API&#26469;&#19982;&#32473;&#23450;&#30340;&#25991;&#26723;&#36827;&#34892;&#25509;&#21475;&#65292;&#19968;&#20010;&#22522;&#20110;&#37197;&#32622;&#30340;&#28151;&#28102;&#22120;&#21644;&#19968;&#20010;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#21644;LLMs&#20013;&#34920;&#29616;&#20986;&#39640;&#30340;&#19978;&#19979;&#25991;&#20445;&#23384;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#23398;&#20064;&#20102;&#21407;&#22987;PII&#21644;&#20854;&#36716;&#25442;&#21518;&#30340;&#20154;&#36896;PII&#23545;&#24212;&#30340;&#26144;&#23556;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#26377;&#36873;&#25321;&#22320;&#28151;&#28102;&#25991;&#26723;&#20013;&#30340;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#25991;&#26723;&#30340;&#32479;&#35745;&#21644;&#35821;&#20041;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protecting sensitive information is crucial in today's world of Large Language Models (LLMs) and data-driven services. One common method used to preserve privacy is by using data perturbation techniques to reduce overreaching utility of (sensitive) Personal Identifiable Information (PII) data while maintaining its statistical and semantic properties. Data perturbation methods often result in significant information loss, making them impractical for use. In this paper, we propose 'Life of PII', a novel Obfuscation Transformer framework for transforming PII into faux-PII while preserving the original information, intent, and context as much as possible. Our approach includes an API to interface with the given document, a configuration-based obfuscator, and a model based on the Transformer architecture, which has shown high context preservation and performance in natural language processing tasks and LLMs.  Our Transformer-based approach learns mapping between the original PII and its tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#26032;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20154;&#20204;&#22914;&#20309;&#23558;&#36523;&#20221;&#26631;&#31614;&#24212;&#29992;&#20110;&#33258;&#24049;&#21644;&#20182;&#20154;&#20197;&#21450;&#37327;&#21270;&#31361;&#20986;&#30340;&#31038;&#20250;&#32500;&#24230;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#21051;&#26495;&#21360;&#35937;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09548</link><description>&lt;p&gt;
&#20351;&#29992;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#26469;&#34913;&#37327;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Measuring Stereotypes using Entity-Centric Data. (arXiv:2305.09548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#26032;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20154;&#20204;&#22914;&#20309;&#23558;&#36523;&#20221;&#26631;&#31614;&#24212;&#29992;&#20110;&#33258;&#24049;&#21644;&#20182;&#20154;&#20197;&#21450;&#37327;&#21270;&#31361;&#20986;&#30340;&#31038;&#20250;&#32500;&#24230;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#21051;&#26495;&#21360;&#35937;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21051;&#26495;&#21360;&#35937;&#24433;&#21709;&#25105;&#20204;&#22914;&#20309;&#23637;&#31034;&#33258;&#24049;&#21644;&#20182;&#20154;&#65292;&#20174;&#32780;&#24433;&#21709;&#25105;&#20204;&#30340;&#34892;&#20026;&#12290;&#22240;&#27492;&#65292;&#34913;&#37327;&#21051;&#26495;&#21360;&#35937;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#20998;&#24067;&#35821;&#20041;&#27169;&#22411;&#65288;DSM&#65289;&#65288;&#22914;BERT&#65289;&#20013;&#23884;&#20837;&#30340;&#25237;&#24433;&#26469;&#36827;&#34892;&#36825;&#20123;&#27979;&#37327;&#12290;&#28982;&#32780;&#65292;DSMs&#25429;&#25417;&#21040;&#30340;&#35748;&#30693;&#32852;&#24819;&#19981;&#19968;&#23450;&#19982;&#21051;&#26495;&#21360;&#35937;&#30340;&#20154;&#38469;&#24615;&#36136;&#30456;&#20851;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19977;&#31181;&#26032;&#30340;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20174;Twitter&#21644;Wikipedia&#20256;&#35760;&#20013;&#23398;&#20064;&#21051;&#26495;&#21360;&#35937;&#12290;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#30701;&#35821;&#24212;&#29992;&#20110;&#21516;&#19968;&#20010;&#20154;&#30340;&#20107;&#23454;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#25193;&#22823;&#20102;&#23398;&#20064;&#32852;&#24819;&#30340;&#20154;&#26412;&#36523;&#20013;&#24515;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#39044;&#27979;&#20154;&#20204;&#22914;&#20309;&#23558;&#36523;&#20221;&#26631;&#31614;&#24212;&#29992;&#20110;&#33258;&#24049;&#21644;&#20182;&#20154;&#20197;&#21450;&#37327;&#21270;&#31361;&#20986;&#30340;&#31038;&#20250;&#32500;&#24230;&#65288;&#22914;&#24615;&#21035;&#65289;&#30340;&#21051;&#26495;&#21360;&#35937;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#23545;&#26410;&#26469;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#38382;&#39064;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stereotypes inform how we present ourselves and others, and in turn how we behave. They are thus important to measure. Recent work has used projections of embeddings from Distributional Semantic Models (DSMs), such as BERT, to perform these measurements. However, DSMs capture cognitive associations that are not necessarily relevant to the interpersonal nature of stereotyping. Here, we propose and evaluate three novel, entity-centric methods for learning stereotypes from Twitter and Wikipedia biographies. Models are trained by leveraging the fact that multiple phrases are applied to the same person, magnifying the person-centric nature of the learned associations. We show that these models outperform existing approaches to stereotype measurement with respect to 1) predicting which identities people apply to themselves and others, and 2) quantifying stereotypes on salient social dimensions (e.g. gender). Via a case study, we also show the utility of these models for future questions in c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;MetaSRL ++&#65292;&#23427;&#26159;&#19968;&#31181;&#32479;&#19968;&#12289;&#35821;&#35328;&#21644;&#24418;&#24335;&#26080;&#20851;&#30340;&#24314;&#27169;&#26041;&#26696;&#65292;&#22522;&#20110;&#35821;&#20041;&#22270;&#65292;&#29992;&#20110;&#24314;&#27169;&#26356;&#28145;&#23618;&#30340;&#35821;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.09534</link><description>&lt;p&gt;
MetaSRL++&#65306;&#29992;&#20110;&#24314;&#27169;&#26356;&#28145;&#23618;&#35821;&#20041;&#30340;&#32479;&#19968;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
MetaSRL++: A Uniform Scheme for Modelling Deeper Semantics. (arXiv:2305.09534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MetaSRL ++&#65292;&#23427;&#26159;&#19968;&#31181;&#32479;&#19968;&#12289;&#35821;&#35328;&#21644;&#24418;&#24335;&#26080;&#20851;&#30340;&#24314;&#27169;&#26041;&#26696;&#65292;&#22522;&#20110;&#35821;&#20041;&#22270;&#65292;&#29992;&#20110;&#24314;&#27169;&#26356;&#28145;&#23618;&#30340;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#34429;&#28982;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20294;&#20173;&#32570;&#20047;&#19968;&#31181;&#20849;&#21516;&#30340;&#28145;&#23618;&#35821;&#20041;&#34920;&#31034;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#20041;&#22270;&#30340;&#32479;&#19968;&#12289;&#35821;&#35328;&#21644;&#24418;&#24335;&#26080;&#20851;&#30340;&#24314;&#27169;&#26041;&#26696;MetaSRL ++&#20316;&#20026;&#23454;&#29616;&#36825;&#26679;&#30340;&#26041;&#26696;&#30340;&#19968;&#31181;&#26041;&#27861;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#23450;&#20041;&#36825;&#20123;&#22270;&#20013;&#20351;&#29992;&#30340;&#27010;&#24565;&#21644;&#23454;&#20307;&#26041;&#27861;&#12290;&#26412;&#25991;&#30340;&#36755;&#20986;&#26377;&#20004;&#37096;&#20998;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#20855;&#20307;&#31034;&#20363;&#35828;&#26126;MetaSRL ++&#65307;&#20854;&#27425;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#23427;&#19982;&#35813;&#39046;&#22495;&#29616;&#26377;&#24037;&#20316;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite enormous progress in Natural Language Processing (NLP), our field is still lacking a common deep semantic representation scheme. As a result, the problem of meaning and understanding is typically sidestepped through more simple, approximative methods. This paper argues that in order to arrive at such a scheme, we also need a common modelling scheme. It therefore introduces MetaSRL++, a uniform, language- and modality-independent modelling scheme based on Semantic Graphs, as a step towards a common representation scheme; as well as a method for defining the concepts and entities that are used in these graphs. Our output is twofold. First, we illustrate MetaSRL++ through concrete examples. Secondly, we discuss how it relates to existing work in the field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65288;AR-Diffusion&#65289;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#21160;&#24577;&#25968;&#37327;&#30340;&#38477;&#22122;&#27493;&#39588;&#65292;&#30830;&#20445;&#24038;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#24433;&#21709;&#21491;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2305.09515</link><description>&lt;p&gt;
AR-Diffusion&#65306;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation. (arXiv:2305.09515v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65288;AR-Diffusion&#65289;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#21160;&#24577;&#25968;&#37327;&#30340;&#38477;&#22122;&#27493;&#39588;&#65292;&#30830;&#20445;&#24038;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#24433;&#21709;&#21491;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#22312;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#36825;&#31181;&#25104;&#21151;&#24050;&#32463;&#25193;&#23637;&#21040;&#20102;&#36890;&#36807;&#21516;&#26102;&#29983;&#25104;&#24207;&#21015;&#20013;&#30340;&#25152;&#26377;&#26631;&#35760;&#26469;&#23454;&#29616;&#25991;&#26412;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#30456;&#23545;&#20110;&#22270;&#20687;&#20855;&#26377;&#26356;&#20026;&#26126;&#26174;&#30340;&#24207;&#21015;&#20381;&#36182;&#24615;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#37117;&#26159;&#20351;&#29992;&#33258;&#24038;&#21521;&#21491;&#30340;&#33258;&#22238;&#24402;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22266;&#26377;&#30340;&#24207;&#21015;&#29305;&#24449;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#22238;&#24402;&#25193;&#25955;&#65288;AR-Diffusion&#65289;&#27169;&#22411;&#12290;AR-Diffusion&#30830;&#20445;&#21491;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#21462;&#20915;&#20110;&#24038;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#65292;&#36825;&#31181;&#26426;&#21046;&#26159;&#36890;&#36807;&#37319;&#29992;&#21160;&#24577;&#25968;&#37327;&#30340;&#38477;&#22122;&#27493;&#39588;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#20123;&#27493;&#39588;&#26681;&#25454;&#26631;&#35760;&#20301;&#32622;&#32780;&#21464;&#21270;&#12290;&#36825;&#23548;&#33268;&#24038;&#20391;&#30340;&#26631;&#35760;&#32463;&#21382;&#30340;&#38477;&#22122;&#27493;&#39588;&#27604;&#21491;&#20391;&#30340;&#26631;&#35760;&#23569;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#33021;&#22815;&#26356;&#26089;&#22320;&#29983;&#25104;&#24182;&#38543;&#21518;&#24433;&#21709;&#21491;&#20391;&#26631;&#35760;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have gained significant attention in the realm of image generation due to their exceptional performance. Their success has been recently expanded to text generation via generating all tokens within a sequence concurrently. However, natural language exhibits a far more pronounced sequential dependency in comparison to images, and the majority of existing language models are trained utilizing a left-to-right auto-regressive approach. To account for the inherent sequential characteristic of natural language, we introduce Auto-Regressive Diffusion (AR-Diffusion). AR-Diffusion ensures that the generation of tokens on the right depends on the generated ones on the left, a mechanism achieved through employing a dynamic number of denoising steps that vary based on token position. This results in tokens on the left undergoing fewer denoising steps than those on the right, thereby enabling them to generate earlier and subsequently influence the generation of tokens on the right.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#26041;&#38754;&#22522;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#21452;&#21521;&#29983;&#25104;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20174;&#26631;&#31614;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#26631;&#31614;&#20004;&#20010;&#26041;&#21521;&#19978;&#35757;&#32451;&#20102;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#32479;&#19968;&#30340;&#26684;&#24335;&#36716;&#25442;&#21644;&#22122;&#22768;&#26631;&#31614;&#30340;&#33258;&#28982;&#35821;&#21477;&#29983;&#25104;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09509</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#26041;&#38754;&#22522;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#21452;&#21521;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Generative Framework for Cross-domain Aspect-based Sentiment Analysis. (arXiv:2305.09509v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36328;&#39046;&#22495;&#26041;&#38754;&#22522;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340;&#21452;&#21521;&#29983;&#25104;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20174;&#26631;&#31614;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#26631;&#31614;&#20004;&#20010;&#26041;&#21521;&#19978;&#35757;&#32451;&#20102;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#32479;&#19968;&#30340;&#26684;&#24335;&#36716;&#25442;&#21644;&#22122;&#22768;&#26631;&#31614;&#30340;&#33258;&#28982;&#35821;&#21477;&#29983;&#25104;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#26041;&#38754;&#22522;&#20110;&#24773;&#24863;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#20174;&#28304;&#39046;&#22495;&#36716;&#31227;&#30693;&#35782;&#26469;&#22312;&#30446;&#26631;&#39046;&#22495;&#19978;&#25191;&#34892;&#21508;&#31181;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;&#30001;&#20110;&#26631;&#35760;&#25968;&#25454;&#21482;&#23384;&#22312;&#20110;&#28304;&#22495;&#20013;&#65292;&#22240;&#27492;&#26399;&#26395;&#27169;&#22411;&#33021;&#22815;&#24357;&#21512;&#22495;&#38388;&#24046;&#36317;&#20197;&#22788;&#29702;&#36328;&#39046;&#22495;&#26041;&#38754;&#22522;&#20110;&#24773;&#24863;&#20998;&#26512;&#12290;&#34429;&#28982;&#22495;&#36866;&#24212;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23427;&#20204;&#22823;&#22810;&#22522;&#20110;&#21028;&#21035;&#24335;&#27169;&#22411;&#65292;&#24182;&#19988;&#38656;&#35201;&#38024;&#23545;&#19981;&#21516;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#36827;&#34892;&#29305;&#23450;&#30340;&#35774;&#35745;&#12290;&#20026;&#20102;&#25552;&#20379;&#26356;&#36890;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#21452;&#21521;&#29983;&#25104;&#26694;&#26550;&#26469;&#22788;&#29702;&#21508;&#31181;&#36328;&#39046;&#22495;&#26041;&#38754;&#22522;&#20110;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#25991;&#26412;&#21040;&#26631;&#31614;&#21644;&#26631;&#31614;&#21040;&#25991;&#26412;&#20004;&#20010;&#26041;&#21521;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#12290;&#21069;&#32773;&#23558;&#27599;&#20010;&#20219;&#21153;&#36716;&#25442;&#20026;&#32479;&#19968;&#30340;&#26684;&#24335;&#20197;&#23398;&#20064;&#22495;&#19981;&#21487;&#30693;&#29305;&#24449;&#65292;&#21518;&#32773;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#29983;&#25104;&#33258;&#28982;&#35821;&#21477;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#20174;&#32780;&#21487;&#20197;&#35757;&#32451;&#20986;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#30740;&#31350;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21644;&#36890;&#29992;&#24615;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#33391;&#22909;&#30340;&#36890;&#29992;&#24615;&#30340;&#21516;&#26102;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain aspect-based sentiment analysis (ABSA) aims to perform various fine-grained sentiment analysis tasks on a target domain by transferring knowledge from a source domain. Since labeled data only exists in the source domain, a model is expected to bridge the domain gap for tackling cross-domain ABSA. Though domain adaptation methods have proven to be effective, most of them are based on a discriminative model, which needs to be specifically designed for different ABSA tasks. To offer a more general solution, we propose a unified bidirectional generative framework to tackle various cross-domain ABSA tasks. Specifically, our framework trains a generative model in both text-to-label and label-to-text directions. The former transforms each task into a unified format to learn domain-agnostic features, and the latter generates natural sentences from noisy labels for data augmentation, with which a more accurate model can be trained. To investigate the effectiveness and generality of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#36827;&#31243;&#25551;&#36848;&#26041;&#27861;&#65292;&#20351;&#29992;&#27169;&#31946;&#26102;&#38388;&#21407;&#22411;&#20307;&#23454;&#29616;&#23450;&#37327;&#25551;&#36848;&#12290;&#27169;&#22411;&#33021;&#25552;&#21462;&#36807;&#31243;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#21487;&#24212;&#29992;&#20110;&#24515;&#33039;&#30149;&#39046;&#22495;&#31561;&#20854;&#20182;&#22797;&#26434;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.09506</link><description>&lt;p&gt;
&#27169;&#31946;&#26102;&#38388;&#21407;&#22411;&#20307;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#36827;&#31243;&#30340;&#23450;&#37327;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Fuzzy Temporal Protoforms for the Quantitative Description of Processes in Natural Language. (arXiv:2305.09506v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#36827;&#31243;&#25551;&#36848;&#26041;&#27861;&#65292;&#20351;&#29992;&#27169;&#31946;&#26102;&#38388;&#21407;&#22411;&#20307;&#23454;&#29616;&#23450;&#37327;&#25551;&#36848;&#12290;&#27169;&#22411;&#33021;&#25552;&#21462;&#36807;&#31243;&#30340;&#30456;&#20851;&#20449;&#24687;&#65292;&#24182;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#21487;&#24212;&#29992;&#20110;&#24515;&#33039;&#30149;&#39046;&#22495;&#31561;&#20854;&#20182;&#22797;&#26434;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#33258;&#28982;&#35821;&#35328;&#36807;&#31243;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#25551;&#36848;&#30340;&#26694;&#26550;&#20013;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#27169;&#31946;&#26102;&#38388;&#21407;&#22411;&#20307;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#36807;&#31243;&#21644;&#23646;&#24615;&#30340;&#26102;&#38388;&#21644;&#22240;&#26524;&#20449;&#24687;&#65292;&#23450;&#37327;&#25551;&#36848;&#20102;&#36807;&#31243;&#23551;&#21629;&#20869;&#30340;&#23646;&#24615;&#65292;&#24182;&#22238;&#24518;&#20102;&#20107;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#26102;&#38388;&#36317;&#31163;&#65292;&#31561;&#31561;&#20854;&#20182;&#29305;&#24449;&#12290;&#36890;&#36807;&#22312;&#24120;&#35268;&#25968;&#25454;&#21040;&#25991;&#26412;&#26550;&#26500;&#20013;&#38598;&#25104;&#36807;&#31243;&#25366;&#25496;&#25216;&#26415;&#21644;&#27169;&#31946;&#38598;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#20174;&#36807;&#31243;&#20013;&#25552;&#21462;&#30456;&#20851;&#30340;&#23450;&#37327;&#21644;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#29992;&#28041;&#21450;&#19981;&#30830;&#23450;&#26415;&#35821;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#25551;&#36848;&#23427;&#12290;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#24515;&#33039;&#30149;&#39046;&#22495;&#30340;&#23454;&#38469;&#29992;&#20363;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20026;&#21521;&#39046;&#22495;&#19987;&#23478;&#25552;&#20379;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a series of fuzzy temporal protoforms in the framework of the automatic generation of quantitative and qualitative natural language descriptions of processes. The model includes temporal and causal information from processes and attributes, quantifies attributes in time during the process life-span and recalls causal relations and temporal distances between events, among other features. Through integrating process mining techniques and fuzzy sets within the usual Data-to-Text architecture, our framework is able to extract relevant quantitative temporal as well as structural information from a process and describe it in natural language involving uncertain terms. A real use-case in the cardiology domain is presented, showing the potential of our model for providing natural language explanations addressed to domain experts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;MPI-rical&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#20195;&#30721;&#29255;&#27573;&#36827;&#34892;&#35757;&#32451;&#23454;&#29616;&#33258;&#21160;&#21270;MPI&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#24182;&#34892;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09438</link><description>&lt;p&gt;
MPI-rical&#65306;&#22522;&#20110;Transformer&#30340;&#25968;&#25454;&#39537;&#21160;MPI&#20998;&#24067;&#24335;&#24182;&#34892;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
MPI-rical: Data-Driven MPI Distributed Parallelism Assistance with Transformers. (arXiv:2305.09438v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;MPI-rical&#65292;&#36890;&#36807;&#23545;&#22823;&#37327;&#20195;&#30721;&#29255;&#27573;&#36827;&#34892;&#35757;&#32451;&#23454;&#29616;&#33258;&#21160;&#21270;MPI&#20195;&#30721;&#29983;&#25104;&#65292;&#20351;&#24182;&#34892;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#20013;&#65292;&#23558;&#20018;&#34892;&#20195;&#30721;&#33258;&#21160;&#24182;&#34892;&#21270;&#20197;&#25903;&#25345;&#20849;&#20139;&#20869;&#23384;&#21644;&#20998;&#24067;&#24335;&#20869;&#23384;&#31995;&#32479;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#35768;&#22810;&#23581;&#35797;&#23558;&#20018;&#34892;&#20195;&#30721;&#36716;&#25442;&#20026;&#20849;&#20139;&#20869;&#23384;&#29615;&#22659;&#30340;&#24182;&#34892;&#20195;&#30721;&#65288;&#36890;&#24120;&#20351;&#29992;OpenMP&#65289;&#65292;&#20294;&#27809;&#26377;&#20219;&#20309;&#19968;&#39033;&#23581;&#35797;&#25104;&#21151;&#23558;&#20854;&#36716;&#21270;&#20026;&#20998;&#24067;&#24335;&#20869;&#23384;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MPI-rical&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;Transformer&#27169;&#22411;&#23545;&#22823;&#32422;25,000&#20010;&#20018;&#34892;&#20195;&#30721;&#29255;&#27573;&#21450;&#20854;&#23545;&#24212;&#30340;&#24182;&#34892;MPI&#20195;&#30721;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#65288;MPICodeCorpus&#65289;&#30340;50,000&#22810;&#20010;&#20195;&#30721;&#29255;&#27573;&#20013;&#29983;&#25104;&#33258;&#21160;&#21270;MPI&#20195;&#30721;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20018;&#34892;&#20195;&#30721;&#36716;&#25442;&#20026;&#22522;&#20110;MPI&#30340;&#24182;&#34892;&#20195;&#30721;&#32763;&#35793;&#38382;&#39064;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#38382;&#39064;&#65292;&#24182;&#21046;&#23450;&#20004;&#20010;&#30740;&#31350;&#30446;&#26631;&#65306;&#20195;&#30721;&#34917;&#20840;&#65292;&#21363;&#22312;&#32473;&#23450;&#28304;&#20195;&#30721;&#20013;&#30340;&#26576;&#20010;&#20301;&#32622;&#65292;&#39044;&#27979;&#35813;&#20301;&#32622;&#30340;MPI&#20989;&#25968;&#65307;&#20195;&#30721;&#32763;&#35793;&#65292;&#21363;&#39044;&#27979;&#19968;&#20010;MPI&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic source-to-source parallelization of serial code for shared and distributed memory systems is a challenging task in high-performance computing. While many attempts were made to translate serial code into parallel code for a shared memory environment (usually using OpenMP), none has managed to do so for a distributed memory environment. In this paper, we propose a novel approach, called MPI-rical, for automated MPI code generation using a transformer-based model trained on approximately 25,000 serial code snippets and their corresponding parallelized MPI code out of more than 50,000 code snippets in our corpus (MPICodeCorpus). To evaluate the performance of the model, we first break down the serial code to MPI-based parallel code translation problem into two sub-problems and develop two research objectives: code completion defined as given a location in the source code, predict the MPI function for that location, and code translation defined as predicting an MPI function as wel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#21517;&#20026;RECENT&#30340;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#65292;&#20316;&#32773;&#22312;TACRED&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#25104;&#26524;74.8&#30340;F1&#20998;&#25968;&#65292;&#20294;&#22312;&#26356;&#27491;&#38169;&#35823;&#21644;&#37325;&#26032;&#35780;&#20272;&#21518;&#20854;F1&#20998;&#25968;&#20026;65.16&#12290;</title><link>http://arxiv.org/abs/2305.09410</link><description>&lt;p&gt;
&#20851;&#20110;F1&#20998;&#25968;&#30340;&#35752;&#35770;&#8212;&#8212;&#20197;&#26368;&#36817;&#30340;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
About Evaluation of F1 Score for RECENT Relation Extraction System. (arXiv:2305.09410v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#19968;&#20010;&#21517;&#20026;RECENT&#30340;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#65292;&#20316;&#32773;&#22312;TACRED&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20043;&#21069;&#30340;&#26368;&#26032;&#25104;&#26524;74.8&#30340;F1&#20998;&#25968;&#65292;&#20294;&#22312;&#26356;&#27491;&#38169;&#35823;&#21644;&#37325;&#26032;&#35780;&#20272;&#21518;&#20854;F1&#20998;&#25968;&#20026;65.16&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;Shengfei Lyu&#21644;Huanhuan Chen&#22312;Findings of the Association for Computational Linguistics&#65306;ACL-IJCNLP 2021&#19978;&#21457;&#34920;&#30340;&#35770;&#25991;&#8220;Relation Classification with Entity Type Restriction&#8221;&#20013;&#20351;&#29992;&#30340;F1&#20998;&#25968;&#35780;&#20272;&#26041;&#27861;&#12290;&#20316;&#32773;&#21019;&#24314;&#30340;&#31995;&#32479;&#21517;&#20026;RECENT&#65292;&#22768;&#31216;&#20854;&#22312;TACRED&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#65288;&#24403;&#26102;&#30340;&#65289;&#26368;&#26032;&#30340;75.2&#65288;&#20043;&#21069;&#20026;74.8&#65289;&#30340;&#26368;&#26032;&#25104;&#26524;&#65292;&#20294;&#22312;&#26356;&#27491;&#38169;&#35823;&#21644;&#37325;&#26032;&#35780;&#20272;&#21518;&#26368;&#32456;&#32467;&#26524;&#20026;65.16&#12290;
&lt;/p&gt;
&lt;p&gt;
This document contains a discussion of the F1 score evaluation used in the article 'Relation Classification with Entity Type Restriction' by Shengfei Lyu, Huanhuan Chen published on Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. The authors created a system named RECENT and claim it achieves (then) a new state-of-the-art result 75.2 (previous 74.8) on the TACRED dataset, while after correcting errors and reevaluation the final result is 65.16
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;Bard AI&#27169;&#22411;&#22312;&#29983;&#25104;Java&#20195;&#30721;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#34920;&#29616;&#26356;&#22909;&#65292;&#20026;AI&#36741;&#21161;&#20195;&#30721;&#29983;&#25104;&#24037;&#20855;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#28508;&#22312;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2305.09402</link><description>&lt;p&gt;
GPT-3.5&#21644;Bard AI&#27169;&#22411;&#22312;Java&#20989;&#25968;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#26041;&#38754;&#30340;&#21021;&#27493;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Preliminary Analysis on the Code Generation Capabilities of GPT-3.5 and Bard AI Models for Java Functions. (arXiv:2305.09402v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;GPT-3.5&#21644;Bard AI&#27169;&#22411;&#22312;&#29983;&#25104;Java&#20195;&#30721;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-3.5&#34920;&#29616;&#26356;&#22909;&#65292;&#20026;AI&#36741;&#21161;&#20195;&#30721;&#29983;&#25104;&#24037;&#20855;&#30340;&#24320;&#21457;&#25552;&#20379;&#20102;&#28508;&#22312;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20004;&#20010;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;GPT-3.5&#21644;Bard&#22312;&#32473;&#23450;&#20989;&#25968;&#25551;&#36848;&#26102;&#29983;&#25104;Java&#20195;&#30721;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20174;CodingBat.com&#33719;&#21462;&#20102;&#36825;&#20123;&#25551;&#36848;&#65292;&#24182;&#22522;&#20110;&#24179;&#21488;&#33258;&#24049;&#30340;&#27979;&#35797;&#29992;&#20363;&#27604;&#36739;&#20102;&#20004;&#20010;&#27169;&#22411;&#29983;&#25104;&#30340;Java&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;&#27169;&#22411;&#30340;&#33021;&#21147;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;GPT-3.5&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24615;&#33021;&#65292;&#23545;&#22823;&#32422;90.6&#65285;&#30340;&#20989;&#25968;&#25551;&#36848;&#29983;&#25104;&#27491;&#30830;&#30340;&#20195;&#30721;&#65292;&#32780;Bard&#20165;&#20026;53.1&#65285;&#30340;&#20989;&#25968;&#29983;&#25104;&#20102;&#27491;&#30830;&#30340;&#20195;&#30721;&#12290;&#23613;&#31649;&#20004;&#20010;&#27169;&#22411;&#37117;&#26377;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#20294;&#36825;&#20123;&#21457;&#29616;&#20026;&#26356;&#20808;&#36827;&#30340;AI&#36741;&#21161;&#20195;&#30721;&#29983;&#25104;&#24037;&#20855;&#30340;&#24320;&#21457;&#21644;&#25913;&#36827;&#25552;&#20379;&#20102;&#28508;&#22312;&#36884;&#24452;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;AI&#22312;&#33258;&#21160;&#21270;&#21644;&#25903;&#25345;&#36719;&#20214;&#24320;&#21457;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20294;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper evaluates the capability of two state-of-the-art artificial intelligence (AI) models, GPT-3.5 and Bard, in generating Java code given a function description. We sourced the descriptions from CodingBat.com, a popular online platform that provides practice problems to learn programming. We compared the Java code generated by both models based on correctness, verified through the platform's own test cases. The results indicate clear differences in the capabilities of the two models. GPT-3.5 demonstrated superior performance, generating correct code for approximately 90.6% of the function descriptions, whereas Bard produced correct code for 53.1% of the functions. While both models exhibited strengths and weaknesses, these findings suggest potential avenues for the development and refinement of more advanced AI-assisted code generation tools. The study underlines the potential of AI in automating and supporting aspects of software development, although further research is requir
&lt;/p&gt;</description></item><item><title>GIFT&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#22810;&#26041;&#23545;&#35805;&#29702;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#22235;&#31181;&#31867;&#22411;&#30340;&#36793;&#32536;&#23558;&#22270;&#24863;&#30693;&#20449;&#24687;&#38598;&#25104;&#21040;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#25913;&#36827;&#20102;&#21407;&#22987;&#30340;&#39034;&#24207;&#25991;&#26412;&#22788;&#29702;&#30340;PLM&#12290;</title><link>http://arxiv.org/abs/2305.09360</link><description>&lt;p&gt;
GIFT: &#22522;&#20110;&#22270;&#24863;&#30693;&#24494;&#35843;&#30340;&#22810;&#26041;&#23545;&#35805;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding. (arXiv:2305.09360v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09360
&lt;/p&gt;
&lt;p&gt;
GIFT&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#22810;&#26041;&#23545;&#35805;&#29702;&#35299;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#22235;&#31181;&#31867;&#22411;&#30340;&#36793;&#32536;&#23558;&#22270;&#24863;&#30693;&#20449;&#24687;&#38598;&#25104;&#21040;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#65292;&#25913;&#36827;&#20102;&#21407;&#22987;&#30340;&#39034;&#24207;&#25991;&#26412;&#22788;&#29702;&#30340;PLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#35841;&#19982;&#35841;&#22312;&#22810;&#26041;&#23545;&#35805;&#20013;&#35828;&#20102;&#20160;&#20040;&#30340;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#24456;&#22810;&#30740;&#31350;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#26041;&#23545;&#35805;&#29702;&#35299;&#26041;&#27861;&#36890;&#24120;&#23558;&#35828;&#35805;&#32773;&#21644;&#35805;&#35821;&#23884;&#20837;&#21040;&#39034;&#24207;&#20449;&#24687;&#27969;&#20013;&#65292;&#25110;&#20165;&#21033;&#29992;&#22810;&#26041;&#23545;&#35805;&#20013;&#22266;&#26377;&#22270;&#32467;&#26500;&#30340;&#34920;&#23618;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24863;&#30693;&#24494;&#35843;&#65288;GIFT&#65289;&#30340;&#21363;&#25554;&#21363;&#29992;&#36731;&#37327;&#32423;&#26041;&#27861;&#65292;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#36890;&#29992;&#22810;&#26041;&#23545;&#35805;&#29702;&#35299;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;&#26222;&#36890;Transformer&#20013;&#65292;&#35805;&#35821;&#20043;&#38388;&#30340;&#20840;&#31561;&#36830;&#25509;&#20250;&#24573;&#30053;&#19968;&#20010;&#35805;&#35821;&#23545;&#21478;&#19968;&#20010;&#35805;&#35821;&#30340;&#31232;&#30095;&#20294;&#26377;&#21306;&#21035;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#21306;&#20998;&#35805;&#35821;&#20043;&#38388;&#30340;&#19981;&#21516;&#20851;&#31995;&#65292;&#35774;&#35745;&#20102;&#22235;&#31181;&#31867;&#22411;&#30340;&#36793;&#32536;&#20197;&#23558;&#22270;&#24863;&#30693;&#20449;&#21495;&#38598;&#25104;&#21040;&#27880;&#24847;&#26426;&#21046;&#20013;&#65292;&#20197;&#25913;&#36827;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#39034;&#24207;&#25991;&#26412;&#30340;PLMs&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;GIFT&#23454;&#29616;&#21040;&#19977;&#20010;PLMs&#24182;&#23545;&#20854;&#36827;&#34892;&#27979;&#35797;&#26469;&#35780;&#20272;GIFT&#12290;
&lt;/p&gt;
&lt;p&gt;
Addressing the issues of who saying what to whom in multi-party conversations (MPCs) has recently attracted a lot of research attention. However, existing methods on MPC understanding typically embed interlocutors and utterances into sequential information flows, or utilize only the superficial of inherent graph structures in MPCs. To this end, we present a plug-and-play and lightweight method named graph-induced fine-tuning (GIFT) which can adapt various Transformer-based pre-trained language models (PLMs) for universal MPC understanding. In detail, the full and equivalent connections among utterances in regular Transformer ignore the sparse but distinctive dependency of an utterance on another in MPCs. To distinguish different relationships between utterances, four types of edges are designed to integrate graph-induced signals into attention mechanisms to refine PLMs originally designed for processing sequential texts. We evaluate GIFT by implementing it into three PLMs, and test the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27493;&#39588;&#25552;&#31034;&#23398;&#20064;&#27169;&#22411;&#65288;MsPrompt&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#20107;&#20214;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27424;&#37319;&#26679;&#12289;&#22810;&#32423;&#25552;&#31034;&#20197;&#21450;&#21407;&#22411;&#27169;&#22359;&#26469;&#35299;&#20915;&#19978;&#19979;&#25991;&#32469;&#36807;&#21644;&#22686;&#24378;&#20107;&#20214;&#35821;&#20041;&#21644;&#28508;&#22312;&#20808;&#39564;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.09335</link><description>&lt;p&gt;
MsPrompt: &#22810;&#27493;&#39588;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#21435;&#20559;&#23569;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MsPrompt: Multi-step Prompt Learning for Debiasing Few-shot Event Detection. (arXiv:2305.09335v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27493;&#39588;&#25552;&#31034;&#23398;&#20064;&#27169;&#22411;&#65288;MsPrompt&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#20107;&#20214;&#26816;&#27979;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#27424;&#37319;&#26679;&#12289;&#22810;&#32423;&#25552;&#31034;&#20197;&#21450;&#21407;&#22411;&#27169;&#22359;&#26469;&#35299;&#20915;&#19978;&#19979;&#25991;&#32469;&#36807;&#21644;&#22686;&#24378;&#20107;&#20214;&#35821;&#20041;&#21644;&#28508;&#22312;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#26816;&#27979;&#65288;ED&#65289;&#26088;&#22312;&#35782;&#21035;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#30340;&#20851;&#38190;&#35302;&#21457;&#35789;&#24182;&#30456;&#24212;&#22320;&#39044;&#27979;&#20107;&#20214;&#31867;&#22411;&#12290;&#20256;&#32479;&#30340; ED &#27169;&#22411;&#36807;&#20110;&#20381;&#36182;&#25968;&#25454;&#65292;&#38590;&#20197;&#36866;&#24212;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#35302;&#21457;&#20559;&#35265;&#20250;&#23548;&#33268; ED &#25968;&#25454;&#38598;&#23384;&#22312;&#19978;&#19979;&#25991;&#32469;&#36807;&#21644;&#27531;&#32570;&#27867;&#21270;&#31561;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#30495;&#27491;&#30340;&#23569;&#26679;&#26412;&#33539;&#24335;&#26469;&#36866;&#24212;&#20302;&#36164;&#28304;&#22330;&#26223;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27493;&#39588;&#25552;&#31034;&#23398;&#20064;&#27169;&#22411;&#65288;MsPrompt&#65289;&#29992;&#20110;&#21435;&#20559;&#23569;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#65292;&#21253;&#25324;&#20197;&#19979;&#19977;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#19968;&#20010;&#27424;&#37319;&#26679;&#27169;&#22359;&#65292; &#20855;&#26377;&#22810;&#32423;&#25552;&#31034;&#27169;&#22359;&#21644;&#21407;&#22411;&#27169;&#22359;&#65292;&#20197;&#35299;&#20915;&#35821;&#22659;&#32469;&#36807;&#38382;&#39064;&#21644;&#22686;&#24378;&#20107;&#20214;&#35821;&#20041;&#21644;&#28508;&#22312;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event detection (ED) is aimed to identify the key trigger words in unstructured text and predict the event types accordingly. Traditional ED models are too data-hungry to accommodate real applications with scarce labeled data. Besides, typical ED models are facing the context-bypassing and disabled generalization issues caused by the trigger bias stemming from ED datasets. Therefore, we focus on the true few-shot paradigm to satisfy the low-resource scenarios. In particular, we propose a multi-step prompt learning model (MsPrompt) for debiasing few-shot event detection, that consists of the following three components: an under-sampling module targeting to construct a novel training set that accommodates the true few-shot setting, a multi-step prompt module equipped with a knowledge-enhanced ontology to leverage the event semantics and latent prior knowledge in the PLMs sufficiently for tackling the context-bypassing problem, and a prototypical module compensating for the weakness of cl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#22810;&#27169;&#24577;&#35270;&#35273;&#29702;&#35299;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#31163;&#35821;&#20041;&#20449;&#24687;&#25552;&#39640;&#23545;&#22270;&#20687;&#30340;&#29702;&#35299;&#65292;&#26088;&#22312;&#25512;&#36827;&#22270;&#20687;&#35782;&#21035;&#21644;&#29702;&#35299;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.09333</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#22810;&#27169;&#24577;&#35270;&#35273;&#29702;&#35299;&#25216;&#26415;&#23545;&#22270;&#20687;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#20998;&#31163;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Visual Understanding with Prompts for Semantic Information Disentanglement of Image. (arXiv:2305.09333v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#22810;&#27169;&#24577;&#35270;&#35273;&#29702;&#35299;&#25216;&#26415;&#65292;&#36890;&#36807;&#20998;&#31163;&#35821;&#20041;&#20449;&#24687;&#25552;&#39640;&#23545;&#22270;&#20687;&#30340;&#29702;&#35299;&#65292;&#26088;&#22312;&#25512;&#36827;&#22270;&#20687;&#35782;&#21035;&#21644;&#29702;&#35299;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#26469;&#36827;&#34892;&#22810;&#27169;&#24577;&#35270;&#35273;&#29702;&#35299;&#25216;&#26415;&#21487;&#21033;&#29992;&#21508;&#31181;&#35270;&#35273;&#21450;&#25991;&#26412;&#32447;&#32034;&#65292;&#25552;&#39640;&#23545;&#22270;&#20687;&#30340;&#35821;&#20041;&#29702;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#32467;&#21512;&#20102;&#35270;&#35273;&#21644;&#35821;&#35328;&#22788;&#29702;&#65292;&#33021;&#22815;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#21644;&#35782;&#21035;&#22270;&#20687;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#25216;&#26415;&#65292;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#20851;&#27880;&#22270;&#20687;&#30340;&#26576;&#20123;&#29305;&#24449;&#65292;&#20197;&#25552;&#21462;&#19979;&#28216;&#20219;&#21153;&#25152;&#38656;&#30340;&#26377;&#29992;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#22810;&#27169;&#24577;&#29702;&#35299;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#26356;&#24378;&#22823;&#30340;&#22270;&#20687;&#34920;&#31034;&#26469;&#25913;&#21892;&#21333;&#27169;&#24577;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#32467;&#21512;&#26159;&#25512;&#36827;&#22270;&#20687;&#35782;&#21035;&#21644;&#29702;&#35299;&#39046;&#22495;&#30340;&#26377;&#26395;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#25991;&#23558;&#23581;&#35797;&#19968;&#20123;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26356;&#22909;&#22320;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal visual understanding of images with prompts involves using various visual and textual cues to enhance the semantic understanding of images. This approach combines both vision and language processing to generate more accurate predictions and recognition of images. By utilizing prompt-based techniques, models can learn to focus on certain features of an image to extract useful information for downstream tasks. Additionally, multi-modal understanding can improve upon single modality models by providing more robust representations of images. Overall, the combination of visual and textual information is a promising area of research for advancing image recognition and understanding. In this paper we will try an amount of prompt design methods and propose a new method for better extraction of semantic information
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;BoW&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#25991;&#26723;&#30340;&#20027;&#39064;&#20998;&#24067;&#65292;&#24182;&#30452;&#25509;&#20174;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#20013;&#25512;&#26029;&#20986;&#25991;&#26723;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20165;&#20381;&#36182;BoW&#34920;&#31034;&#21644;&#20854;&#20182;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09329</link><description>&lt;p&gt;
BERTTM: &#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#21521;&#37327;&#36827;&#34892;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
BERTTM: Leveraging Contextualized Word Embeddings from Pre-trained Language Models for Neural Topic Modeling. (arXiv:2305.09329v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;BoW&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#25991;&#26723;&#30340;&#20027;&#39064;&#20998;&#24067;&#65292;&#24182;&#30452;&#25509;&#20174;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#20013;&#25512;&#26029;&#20986;&#25991;&#26723;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20165;&#20381;&#36182;BoW&#34920;&#31034;&#21644;&#20854;&#20182;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20027;&#39064;&#24314;&#27169;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#25198;&#28436;&#30528;&#26085;&#30410;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20027;&#39064;&#27169;&#22411;&#20173;&#28982;&#20381;&#36182;&#20110;&#35789;&#34955;&#65288;BoW&#65289;&#20449;&#24687;&#65292;&#26080;&#35770;&#26159;&#20316;&#20026;&#35757;&#32451;&#36755;&#20837;&#36824;&#26159;&#35757;&#32451;&#30446;&#26631;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#25429;&#25417;&#25991;&#26723;&#20013;&#30340;&#21333;&#35789;&#39034;&#24207;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#23548;&#33268;&#23427;&#20204;&#22312;&#22788;&#29702;&#26032;&#25991;&#26723;&#20013;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#21333;&#35789;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#21521;&#37327;&#22312;&#35789;&#20041;&#28040;&#27495;&#30340;&#33021;&#21147;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;OOV&#21333;&#35789;&#26102;&#26159;&#26377;&#25928;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;BoW&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#25991;&#26723;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#20174;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#20013;&#25512;&#26029;&#20986;&#25991;&#26723;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20165;&#20381;&#36182;BoW&#34920;&#31034;&#21644;&#20854;&#20182;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of neural topic models in recent years, topic modelling is playing an increasingly important role in natural language understanding. However, most existing topic models still rely on bag-of-words (BoW) information, either as training input or training target. This limits their ability to capture word order information in documents and causes them to suffer from the out-of-vocabulary (OOV) issue, i.e. they cannot handle unobserved words in new documents. Contextualized word embeddings from pre-trained language models show superiority in the ability of word sense disambiguation and prove to be effective in dealing with OOV words. In this work, we developed a novel neural topic model combining contextualized word embeddings from the pre-trained language model BERT. The model can infer the topic distribution of a document without using any BoW information. In addition, the model can infer the topic distribution of each word in a document directly from the contextualize
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#21152;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#38271;&#31687;&#31185;&#25216;&#35770;&#25991;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#12290;&#36890;&#36807;&#26500;&#24314;&#25991;&#26412;&#20849;&#29616;&#22270;&#24182;&#32467;&#21512;&#22270;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#21270;&#30340;PLM&#23884;&#20837;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#38271;&#31687;&#25991;&#26723;&#65292;&#22686;&#24378;PLMs&#24615;&#33021;&#27604;&#29616;&#26377;&#25216;&#26415;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#21152;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.09316</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#23884;&#20837;&#22686;&#24378;&#20174;&#38271;&#31687;&#31185;&#25216;&#35770;&#25991;&#20013;&#25552;&#21462;&#20851;&#38190;&#35789;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Keyphrase Extraction from Long Scientific Documents using Graph Embeddings. (arXiv:2305.09316v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#21152;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#38271;&#31687;&#31185;&#25216;&#35770;&#25991;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#12290;&#36890;&#36807;&#26500;&#24314;&#25991;&#26412;&#20849;&#29616;&#22270;&#24182;&#32467;&#21512;&#22270;&#34920;&#31034;&#21644;&#19978;&#19979;&#25991;&#21270;&#30340;PLM&#23884;&#20837;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#38271;&#31687;&#25991;&#26723;&#65292;&#22686;&#24378;PLMs&#24615;&#33021;&#27604;&#29616;&#26377;&#25216;&#26415;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#21152;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#34920;&#31034;&#24378;&#21270;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23545;&#38271;&#31687;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#22270;&#23884;&#20837;&#22686;&#24378;PLM&#25552;&#20379;&#26356;&#20840;&#38754;&#30340;&#35821;&#20041;&#29702;&#35299;&#25991;&#26723;&#20013;&#30340;&#21333;&#35789;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#31687;&#25991;&#26723;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#25991;&#26412;&#30340;&#20849;&#29616;&#22270;&#65292;&#24182;&#20351;&#29992;&#22312;&#36793;&#39044;&#27979;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26469;&#23884;&#20837;&#23427;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#22686;&#24378;&#30340;&#24207;&#21015;&#26631;&#35760;&#26550;&#26500;&#65292;&#23427;&#23558;&#19978;&#19979;&#25991;&#21270;&#30340;PLM&#23884;&#20837;&#19982;&#22270;&#34920;&#31034;&#30456;&#32467;&#21512;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#22686;&#24378;PLM&#19982;&#22270;&#23884;&#20837;&#27604;&#29616;&#26377;&#25216;&#26415;&#30340;&#27169;&#22411;&#22312;&#38271;&#25991;&#26723;&#19978;&#34920;&#29616;&#26356;&#20986;&#33394;&#65292;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#20013;&#37117;&#26174;&#30528;&#25552;&#39640;F1&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;GNN&#34920;&#31034;&#20316;&#20026;&#25913;&#36827;PLM&#24615;&#33021;&#30340;&#19968;&#31181;&#34917;&#20805;&#26041;&#27861;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigate using graph neural network (GNN) representations to enhance contextualized representations of pre-trained language models (PLMs) for keyphrase extraction from lengthy documents. We show that augmenting a PLM with graph embeddings provides a more comprehensive semantic understanding of words in a document, particularly for long documents. We construct a co-occurrence graph of the text and embed it using a graph convolutional network (GCN) trained on the task of edge prediction. We propose a graph-enhanced sequence tagging architecture that augments contextualized PLM embeddings with graph representations. Evaluating on benchmark datasets, we demonstrate that enhancing PLMs with graph embeddings outperforms state-of-the-art models on long documents, showing significant improvements in F1 scores across all the datasets. Our study highlights the potential of GNN representations as a complementary approach to improve PLM performance for keyphrase extraction fro
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HybRank&#30340;&#28151;&#21512;&#19982;&#21327;&#20316;&#30340;&#27573;&#33853;&#20877;&#25490;&#24207;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#28216;&#26816;&#32034;&#22120;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#23454;&#29616;&#27573;&#33853;&#21327;&#20316;&#65292;&#20877;&#21033;&#29992;&#31232;&#30095;&#21644;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#35789;&#27719;&#21644;&#35821;&#20041;&#23646;&#24615;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#21253;&#25324;&#20808;&#21069;&#34987;&#37325;&#26032;&#25490;&#24207;&#30340;&#27573;&#33853;&#21015;&#34920;&#22312;&#20869;&#30340;&#20219;&#24847;&#27573;&#33853;&#21015;&#34920;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#24615;&#33021;&#31283;&#23450;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.09313</link><description>&lt;p&gt;
&#28151;&#21512;&#19982;&#21327;&#20316;&#30340;&#27573;&#33853;&#20877;&#25490;&#24207;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hybrid and Collaborative Passage Reranking. (arXiv:2305.09313v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09313
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HybRank&#30340;&#28151;&#21512;&#19982;&#21327;&#20316;&#30340;&#27573;&#33853;&#20877;&#25490;&#24207;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19978;&#28216;&#26816;&#32034;&#22120;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#23454;&#29616;&#27573;&#33853;&#21327;&#20316;&#65292;&#20877;&#21033;&#29992;&#31232;&#30095;&#21644;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#35789;&#27719;&#21644;&#35821;&#20041;&#23646;&#24615;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#21253;&#25324;&#20808;&#21069;&#34987;&#37325;&#26032;&#25490;&#24207;&#30340;&#27573;&#33853;&#21015;&#34920;&#22312;&#20869;&#30340;&#20219;&#24847;&#27573;&#33853;&#21015;&#34920;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#24615;&#33021;&#31283;&#23450;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27573;&#33853;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#21021;&#22987;&#26816;&#32034;&#32467;&#26524;&#21487;&#33021;&#19981;&#23613;&#22914;&#20154;&#24847;&#65292;&#38656;&#35201;&#36890;&#36807;&#37325;&#26032;&#25490;&#24207;&#26041;&#26696;&#36827;&#34892;&#25913;&#21892;&#12290;&#29616;&#26377;&#30340;&#27573;&#33853;&#37325;&#26032;&#25490;&#24207;&#26041;&#26696;&#20027;&#35201;&#38598;&#20013;&#20110;&#20016;&#23500;&#26597;&#35810;&#21644;&#27599;&#20010;&#27573;&#33853;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#24573;&#30053;&#20102;&#22312;&#21021;&#22987;&#26816;&#32034;&#21015;&#34920;&#20013;&#25490;&#21517;&#38752;&#21069;&#30340;&#22810;&#20010;&#27573;&#33853;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#31995;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#19982;&#21327;&#20316;&#30340;&#27573;&#33853;&#20877;&#25490;&#24207;&#26041;&#27861;&#65288;HybRank&#65289;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#19978;&#28216;&#26816;&#32034;&#22120;&#30340;&#30456;&#20284;&#24615;&#24230;&#37327;&#36827;&#34892;&#27573;&#33853;&#21327;&#20316;&#65292;&#24182;&#32467;&#21512;&#31232;&#30095;&#21644;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#35789;&#27719;&#21644;&#35821;&#20041;&#23646;&#24615;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#29616;&#25104;&#30340;&#26816;&#32034;&#22120;&#29305;&#24449;&#65292;HybRank&#26159;&#19968;&#20010;&#25554;&#20214;&#20877;&#25490;&#24207;&#22120;&#65292;&#33021;&#22815;&#22686;&#24378;&#21253;&#25324;&#20808;&#21069;&#37325;&#26032;&#25490;&#24207;&#30340;&#27573;&#33853;&#21015;&#34920;&#22312;&#20869;&#30340;&#20219;&#24847;&#27573;&#33853;&#21015;&#34920;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#27604;&#26222;&#36941;&#30340;&#26816;&#32034;&#21644;&#20877;&#25490;&#24207;&#26041;&#27861;&#24615;&#33021;&#31283;&#23450;&#30340;&#25552;&#21319;&#65292;&#24182;&#39564;&#35777;&#20102;HybRank&#30340;&#26680;&#24515;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In passage retrieval system, the initial passage retrieval results may be unsatisfactory, which can be refined by a reranking scheme. Existing solutions to passage reranking focus on enriching the interaction between query and each passage separately, neglecting the context among the top-ranked passages in the initial retrieval list. To tackle this problem, we propose a Hybrid and Collaborative Passage Reranking (HybRank) method, which leverages the substantial similarity measurements of upstream retrievers for passage collaboration and incorporates the lexical and semantic properties of sparse and dense retrievers for reranking. Besides, built on off-the-shelf retriever features, HybRank is a plug-in reranker capable of enhancing arbitrary passage lists including previously reranked ones. Extensive experiments demonstrate the stable improvements of performance over prevalent retrieval and reranking methods, and verify the effectiveness of the core components of HybRank.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#38646;&#26679;&#26412;&#32763;&#35793;&#20013;&#65292;&#20351;&#29992;&#27531;&#24046;&#36830;&#25509;&#21518;&#30340;Transformer&#35774;&#32622;&#30340;&#23618;&#24402;&#19968;&#21270;&#65288;PostNorm&#65289;&#22987;&#32456;&#20248;&#20110;&#24102;&#26377;&#23618;&#24402;&#19968;&#21270;&#30340;PreNorm&#65292;&#26368;&#39640;&#21487;&#25552;&#39640;12.3 BLEU&#20998;&#12290;</title><link>http://arxiv.org/abs/2305.09312</link><description>&lt;p&gt;
&#25506;&#31350;&#23618;&#24402;&#19968;&#21270;&#22312;&#38646;&#26679;&#26412;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring the Impact of Layer Normalization for Zero-shot Neural Machine Translation. (arXiv:2305.09312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09312
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#38646;&#26679;&#26412;&#32763;&#35793;&#20013;&#65292;&#20351;&#29992;&#27531;&#24046;&#36830;&#25509;&#21518;&#30340;Transformer&#35774;&#32622;&#30340;&#23618;&#24402;&#19968;&#21270;&#65288;PostNorm&#65289;&#22987;&#32456;&#20248;&#20110;&#24102;&#26377;&#23618;&#24402;&#19968;&#21270;&#30340;PreNorm&#65292;&#26368;&#39640;&#21487;&#25552;&#39640;12.3 BLEU&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23618;&#24402;&#19968;&#21270;&#65288;LayerNorm&#65289;&#23545;&#38646;&#26679;&#26412;&#32763;&#35793;&#65288;ZST&#65289;&#30340;&#24433;&#21709;&#12290;&#26368;&#36817;&#30340;ZST&#30740;&#31350;&#36890;&#24120;&#20351;&#29992;Transformer&#26550;&#26500;&#20316;&#20026;&#20027;&#24178;&#65292;&#24182;&#23558;&#23618;&#30340;&#36755;&#20837;&#35774;&#32622;&#20026;&#24102;&#26377;LayerNorm&#30340;PreNorm&#12290;&#28982;&#32780;&#65292;&#24464;&#31561;&#20154;&#65288;2019&#65289;&#25581;&#31034;&#20102;PreNorm&#23384;&#22312;&#36807;&#24230;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#20551;&#35774;PreNorm&#21487;&#33021;&#20250;&#36807;&#24230;&#25311;&#21512;&#30417;&#30563;&#26041;&#21521;&#65292;&#22240;&#27492;&#22312;ZST&#20013;&#20855;&#26377;&#20302;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;OPUS&#12289;IWSLT&#21644;Europarl&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;54&#20010;ZST&#26041;&#21521;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#27531;&#24046;&#36830;&#25509;&#21518;&#20351;&#29992;&#21407;&#22987;&#30340;Transformer&#35774;&#32622;LayerNorm&#65288;PostNorm&#65289;&#30340;&#34920;&#29616;&#22987;&#32456;&#20248;&#20110;PreNorm&#36798;12.3 BLEU&#20998;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;PreNorm&#21644;PostNorm&#20043;&#38388;&#30340;&#31163;&#38774;&#29575;&#21644;&#32467;&#26500;&#21464;&#21270;&#30340;&#24046;&#24322;&#65292;&#30740;&#31350;&#20102;&#24615;&#33021;&#24046;&#24322;&#12290;&#36825;&#39033;&#30740;&#31350;&#24378;&#35843;&#20102;&#23545;ZST&#30340;LayerNorm&#35774;&#32622;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the impact of layer normalization (LayerNorm) on zero-shot translation (ZST). Recent efforts for ZST often utilize the Transformer architecture as the backbone, with LayerNorm at the input of layers (PreNorm) set as the default. However, Xu et al. (2019) has revealed that PreNorm carries the risk of overfitting the training data. Based on this, we hypothesize that PreNorm may overfit supervised directions and thus have low generalizability for ZST. Through experiments on OPUS, IWSLT, and Europarl datasets for 54 ZST directions, we demonstrate that the original Transformer setting of LayerNorm after residual connections (PostNorm) consistently outperforms PreNorm by up to 12.3 BLEU points. We then study the performance disparities by analyzing the differences in off-target rates and structural variations between PreNorm and PostNorm. This study highlights the need for careful consideration of the LayerNorm setting for ZST.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35789;&#27719;&#31232;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#20316;&#20026;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#30828;&#27491;&#20363;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.09287</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#35789;&#27719;&#31232;&#37322;&#20316;&#20026;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AdversarialWord Dilution as Text Data Augmentation in Low-Resource Regime. (arXiv:2305.09287v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35789;&#27719;&#31232;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#20316;&#20026;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#30828;&#27491;&#20363;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#22686;&#24378;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#20013;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#29983;&#25104;&#30828;&#27491;&#20363;&#29992;&#20316;&#25968;&#25454;&#22686;&#24378;&#30340;&#26377;&#25928;&#26041;&#27861;&#21364;&#20173;&#26377;&#24453;&#25506;&#32034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35789;&#27719;&#31232;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#30828;&#27491;&#20363;&#20316;&#20026;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35757;&#32451;&#20302;&#36164;&#28304;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is widely used in text classification, especially in the low-resource regime where a few examples for each class are available during training. Despite the success, generating data augmentations as hard positive examples that may increase their effectiveness is under-explored. This paper proposes an Adversarial Word Dilution (AWD) method that can generate hard positive examples as text data augmentations to train the low-resource text classification model efficiently. Our idea of augmenting the text data is to dilute the embedding of strong positive words by weighted mixing with unknown-word embedding, making the augmented inputs hard to be recognized as positive by the classification model. We adversarially learn the dilution weights through a constrained min-max optimization process with the guidance of the labels. Empirical studies on three benchmark datasets show that AWD can generate more effective data augmentations and outperform the state-of-the-art text data 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36861;&#28335;&#20102;&#36807;&#21435;500&#24180;&#31181;&#26063;&#20027;&#20041;&#12289;&#24615;&#21035;&#27495;&#35270;&#21644;&#21516;&#24615;&#24651;&#24656;&#24807;&#30151;&#65292;&#35748;&#20026;NLP&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#28304;&#33258;&#31038;&#20250;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#19982;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.09281</link><description>&lt;p&gt;
&#20174;Jim&#20195;&#30721;&#30340;&#35282;&#24230;&#25506;&#31350;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20559;&#35265;&#30340;&#36215;&#28304;
&lt;/p&gt;
&lt;p&gt;
On the Origins of Bias in NLP through the Lens of the Jim Code. (arXiv:2305.09281v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36861;&#28335;&#20102;&#36807;&#21435;500&#24180;&#31181;&#26063;&#20027;&#20041;&#12289;&#24615;&#21035;&#27495;&#35270;&#21644;&#21516;&#24615;&#24651;&#24656;&#24807;&#30151;&#65292;&#35748;&#20026;NLP&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#28304;&#33258;&#31038;&#20250;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#27861;&#19982;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36861;&#28335;&#36807;&#21435;500&#24180;&#30340;&#31181;&#26063;&#20027;&#20041;&#12289;&#24615;&#21035;&#27495;&#35270;&#21644;&#21516;&#24615;&#24651;&#24656;&#24807;&#30151;&#65292;&#23558;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#36861;&#28335;&#21040;&#23427;&#20204;&#30340;&#36215;&#28304;&#12290;&#25105;&#20204;&#20174;&#20851;&#38190;&#31181;&#26063;&#29702;&#35770;&#12289;&#24615;&#21035;&#30740;&#31350;&#12289;&#25968;&#25454;&#20262;&#29702;&#21644;&#25968;&#23383;&#20154;&#25991;&#30740;&#31350;&#30340;&#25991;&#29486;&#20013;&#24635;&#32467;&#20102;&#20559;&#35265;&#22312;NLP&#27169;&#22411;&#20013;&#30340;&#36215;&#28304;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;NLP&#20013;&#20559;&#35265;&#30340;&#26681;&#26412;&#21407;&#22240;&#26159;&#31038;&#20250;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#35299;&#20915;NLP&#20013;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#30340;&#21807;&#19968;&#26041;&#27861;&#26159;&#35299;&#20915;&#23548;&#33268;&#36825;&#20123;&#38382;&#39064;&#30340;&#31038;&#20250;&#38382;&#39064;&#65292;&#24182;&#23558;&#31038;&#20250;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#23478;&#32435;&#20837;&#20943;&#36731;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#21162;&#21147;&#20013;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#21487;&#23454;&#26045;&#30340;&#24314;&#35758;&#65292;&#20379;NLP&#30740;&#31350;&#30028;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we trace the biases in current natural language processing (NLP) models back to their origins in racism, sexism, and homophobia over the last 500 years. We review literature from critical race theory, gender studies, data ethics, and digital humanities studies, and summarize the origins of bias in NLP models from these social science perspective. We show how the causes of the biases in the NLP pipeline are rooted in social issues. Finally, we argue that the only way to fix the bias and unfairness in NLP is by addressing the social problems that caused them in the first place and by incorporating social sciences and social scientists in efforts to mitigate bias in NLP models. We provide actionable recommendations for the NLP research community to do so.
&lt;/p&gt;</description></item><item><title>ContrastNet&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#21306;&#20998;&#24615;&#34920;&#31034;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#25289;&#36817;&#30456;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#24182;&#25512;&#36828;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#34920;&#31034;&#26469;&#23398;&#20064;&#21306;&#20998;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.09269</link><description>&lt;p&gt;
ContrastNet&#65306;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ContrastNet: A Contrastive Learning Framework for Few-Shot Text Classification. (arXiv:2305.09269v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09269
&lt;/p&gt;
&lt;p&gt;
ContrastNet&#26159;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#21306;&#20998;&#24615;&#34920;&#31034;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#36890;&#36807;&#25289;&#36817;&#30456;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#24182;&#25512;&#36828;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#34920;&#31034;&#26469;&#23398;&#20064;&#21306;&#20998;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20803;&#23398;&#20064;&#33539;&#24335;&#25512;&#21160;&#20102;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#65292;&#26088;&#22312;&#36890;&#36807;&#21517;&#20026;episodes&#30340;&#23567;&#20219;&#21153;&#65292;&#23558;&#30693;&#35782;&#20174;&#28304;&#31867;&#21035;&#36716;&#31227;&#21040;&#30446;&#26631;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#26500;&#24314;&#20803;&#23398;&#20064;&#22120;&#30340;&#26041;&#27861;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#23398;&#20064;&#30456;&#20284;&#31867;&#21035;&#20043;&#38388;&#30340;&#21306;&#20998;&#24615;&#25991;&#26412;&#34920;&#31034;&#65292;&#21487;&#33021;&#23548;&#33268;&#26631;&#31614;&#39044;&#27979;&#26102;&#30340;&#30683;&#30462;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23569;&#37327;&#35757;&#32451;&#31034;&#20363;&#65292;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#20219;&#21153;&#23618;&#21644;&#23454;&#20363;&#23618;&#36807;&#25311;&#21512;&#38382;&#39064;&#20063;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#35299;&#20915;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ContrastNet&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#21306;&#20998;&#24615;&#34920;&#31034;&#21644;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;ContrastNet&#23398;&#20064;&#23558;&#23646;&#20110;&#21516;&#19968;&#31867;&#21035;&#30340;&#25991;&#26412;&#34920;&#31034;&#25289;&#36817;&#65292;&#24182;&#23558;&#23646;&#20110;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#26412;&#34920;&#31034;&#25512;&#36828;&#65292;&#21516;&#26102;&#24341;&#20837;&#26080;&#30417;&#30563;&#30340;&#23545;&#27604;&#23398;&#20064;&#20197;&#20419;&#36827;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot text classification has recently been promoted by the meta-learning paradigm which aims to identify target classes with knowledge transferred from source classes with sets of small tasks named episodes. Despite their success, existing works building their meta-learner based on Prototypical Networks are unsatisfactory in learning discriminative text representations between similar classes, which may lead to contradictions during label prediction. In addition, the tasklevel and instance-level overfitting problems in few-shot text classification caused by a few training examples are not sufficiently tackled. In this work, we propose a contrastive learning framework named ContrastNet to tackle both discriminative representation and overfitting problems in few-shot text classification. ContrastNet learns to pull closer text representations belonging to the same class and push away text representations belonging to different classes, while simultaneously introducing unsupervised con
&lt;/p&gt;</description></item><item><title>HyHTM&#26159;&#19968;&#31181;&#22522;&#20110;&#21452;&#26354;&#20960;&#20309;&#30340;&#23618;&#27425;&#20027;&#39064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#21452;&#26354;&#20960;&#20309;&#20013;&#30340;&#23618;&#27425;&#20449;&#24687;&#32435;&#20837;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#26174;&#24335;&#22320;&#24314;&#27169;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;HyHTM&#26356;&#22909;&#22320;&#20851;&#27880;&#20027;&#39064;&#20043;&#38388;&#30340;&#29238;&#23376;&#20851;&#31995;&#65292;&#24182;&#20135;&#29983;&#20102;&#36830;&#36143;&#30340;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#12290;&#21516;&#26102;&#65292;HyHTM&#30340;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#20869;&#23384;&#21344;&#29992;&#26356;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.09258</link><description>&lt;p&gt;
HyHTM: &#22522;&#20110;&#21452;&#26354;&#20960;&#20309;&#30340;&#23618;&#27425;&#20027;&#39064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HyHTM: Hyperbolic Geometry based Hierarchical Topic Models. (arXiv:2305.09258v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09258
&lt;/p&gt;
&lt;p&gt;
HyHTM&#26159;&#19968;&#31181;&#22522;&#20110;&#21452;&#26354;&#20960;&#20309;&#30340;&#23618;&#27425;&#20027;&#39064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#21452;&#26354;&#20960;&#20309;&#20013;&#30340;&#23618;&#27425;&#20449;&#24687;&#32435;&#20837;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#26174;&#24335;&#22320;&#24314;&#27169;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;HyHTM&#26356;&#22909;&#22320;&#20851;&#27880;&#20027;&#39064;&#20043;&#38388;&#30340;&#29238;&#23376;&#20851;&#31995;&#65292;&#24182;&#20135;&#29983;&#20102;&#36830;&#36143;&#30340;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#12290;&#21516;&#26102;&#65292;HyHTM&#30340;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#20869;&#23384;&#21344;&#29992;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#20027;&#39064;&#27169;&#22411;&#23545;&#20110;&#21457;&#29616;&#25991;&#26723;&#38598;&#21512;&#20013;&#30340;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#23618;&#27425;&#20027;&#39064;&#27169;&#22411;&#24120;&#24120;&#20135;&#29983;&#20302;&#23618;&#27425;&#20027;&#39064;&#19982;&#20854;&#39640;&#23618;&#27425;&#20027;&#39064;&#26080;&#20851;&#19988;&#19981;&#22815;&#20855;&#20307;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; HyHTM &#30340;&#21452;&#26354;&#20960;&#20309;&#23618;&#27425;&#20027;&#39064;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#21452;&#26354;&#20960;&#20309;&#20013;&#30340;&#23618;&#27425;&#20449;&#24687;&#32435;&#20837;&#20027;&#39064;&#27169;&#22411;&#20013;&#65292;&#26174;&#24335;&#22320;&#24314;&#27169;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20123;&#38480;&#21046;&#12290;&#19982;&#22235;&#20010;&#22522;&#32447;&#20570;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HyHTM &#21487;&#20197;&#26356;&#22909;&#22320;&#20851;&#27880;&#20027;&#39064;&#20043;&#38388;&#29238;&#23376;&#20851;&#31995;&#12290;HyHTM &#20135;&#29983;&#36830;&#36143;&#30340;&#20027;&#39064;&#23618;&#27425;&#32467;&#26500;&#65292;&#20174;&#36890;&#29992;&#30340;&#39640;&#23618;&#27425;&#20027;&#39064;&#21040;&#20855;&#20307;&#30340;&#20302;&#23618;&#27425;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#35745;&#31639;&#36895;&#24230;&#26356;&#24555;&#65292;&#20869;&#23384;&#21344;&#29992;&#26356;&#23567;&#12290;&#25105;&#20204;&#24050;&#32463;&#20844;&#24320;&#20102;&#31639;&#27861;&#30340;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Topic Models (HTMs) are useful for discovering topic hierarchies in a collection of documents. However, traditional HTMs often produce hierarchies where lowerlevel topics are unrelated and not specific enough to their higher-level topics. Additionally, these methods can be computationally expensive. We present HyHTM - a Hyperbolic geometry based Hierarchical Topic Models - that addresses these limitations by incorporating hierarchical information from hyperbolic geometry to explicitly model hierarchies in topic models. Experimental results with four baselines show that HyHTM can better attend to parent-child relationships among topics. HyHTM produces coherent topic hierarchies that specialise in granularity from generic higher-level topics to specific lowerlevel topics. Further, our model is significantly faster and leaves a much smaller memory footprint than our best-performing baseline.We have made the source code for our algorithm publicly accessible.
&lt;/p&gt;</description></item><item><title>xPQA&#26159;&#19968;&#20010;&#25903;&#25345;12&#31181;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#20135;&#21697;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20339;&#33521;&#25991;&#20505;&#36873;&#20154;&#24182;&#29983;&#25104;&#33258;&#28982;&#30340;&#20854;&#20182;&#35821;&#35328;&#31572;&#26696;&#65292;&#23454;&#29616;&#20102;&#22810;&#35821;&#35328;&#39038;&#23458;&#25903;&#25345;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#22495;&#20869;&#25968;&#25454;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#65292;&#24182;&#19988;&#34429;&#28982;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24456;&#26377;&#24076;&#26395;&#65292;&#20294;&#36816;&#34892;&#26102;&#32763;&#35793;&#20173;&#28982;&#24517;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.09249</link><description>&lt;p&gt;
xPQA&#65306;&#36328;12&#31181;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#20135;&#21697;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
xPQA: Cross-Lingual Product Question Answering across 12 Languages. (arXiv:2305.09249v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09249
&lt;/p&gt;
&lt;p&gt;
xPQA&#26159;&#19968;&#20010;&#25903;&#25345;12&#31181;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#20135;&#21697;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#36873;&#25321;&#26368;&#20339;&#33521;&#25991;&#20505;&#36873;&#20154;&#24182;&#29983;&#25104;&#33258;&#28982;&#30340;&#20854;&#20182;&#35821;&#35328;&#31572;&#26696;&#65292;&#23454;&#29616;&#20102;&#22810;&#35821;&#35328;&#39038;&#23458;&#25903;&#25345;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#22495;&#20869;&#25968;&#25454;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#65292;&#24182;&#19988;&#34429;&#28982;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24456;&#26377;&#24076;&#26395;&#65292;&#20294;&#36816;&#34892;&#26102;&#32763;&#35793;&#20173;&#28982;&#24517;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#20013;&#65292;&#20135;&#21697;&#38382;&#31572;&#31995;&#32479;&#26159;&#25552;&#20379;&#39038;&#23458;&#38382;&#39064;&#22238;&#31572;&#30340;&#20851;&#38190;&#65292;&#20197;&#24110;&#21161;&#20182;&#20204;&#22312;&#36141;&#29289;&#26102;&#23545;&#20135;&#21697;&#36827;&#34892;&#20102;&#35299;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20135;&#21697;&#38382;&#31572;&#31995;&#32479;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#65292;&#20294;&#23454;&#38469;&#19978;&#38656;&#35201;&#25903;&#25345;&#22810;&#31181;&#35821;&#35328;&#30340;&#39038;&#23458;&#65292;&#24182;&#21033;&#29992;&#33521;&#25991;&#29256;&#20135;&#21697;&#20449;&#24687;&#36827;&#34892;&#22238;&#31572;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#23454;&#38469;&#30340;&#24037;&#19994;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;xPQA&#65292;&#19968;&#20010;&#36328;9&#20010;&#20998;&#25903;&#30340;12&#31181;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#26631;&#27880;&#36328;&#35821;&#35328;&#20135;&#21697;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#24182;&#25253;&#21578;&#20102;(1)&#20505;&#36873;&#31572;&#26696;&#30340;&#25490;&#21517;&#65292;&#20197;&#36873;&#25321;&#26368;&#20339;&#30340;&#33521;&#25991;&#20505;&#36873;&#31572;&#26696;&#26469;&#22238;&#31572;&#38750;&#33521;&#35821;&#38382;&#39064;;&#21644;(2)&#22238;&#31572;&#29983;&#25104;&#65292;&#20197;&#22522;&#20110;&#36873;&#25321;&#30340;&#33521;&#25991;&#20505;&#36873;&#31572;&#26696;&#29983;&#25104;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#30340;&#38750;&#33521;&#35821;&#22238;&#31572;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#28041;&#21450;&#26426;&#22120;&#32763;&#35793;&#12289;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#31561;&#19981;&#21516;&#26041;&#27861;&#65292;&#21253;&#25324;&#25110;&#25490;&#38500;xPQA&#35757;&#32451;&#25968;&#25454;&#30340;&#36816;&#34892;&#26102;&#25110;&#31163;&#32447;&#12290;&#25105;&#20204;&#21457;&#29616;(1)&#22495;&#20869;&#25968;&#25454;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#22240;&#20026;&#22312;&#20854;&#20182;&#39046;&#22495;&#35757;&#32451;&#30340;&#36328;&#35821;&#35328;&#25490;&#21517;&#22120;&#24615;&#33021;&#19979;&#38477;&#65307;(2)&#34429;&#28982;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24456;&#26377;&#24076;&#26395;&#65292;&#20294;&#36816;&#34892;&#26102;&#32763;&#35793;&#20173;&#28982;&#24517;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Product Question Answering (PQA) systems are key in e-commerce applications to provide responses to customers' questions as they shop for products. While existing work on PQA focuses mainly on English, in practice there is need to support multiple customer languages while leveraging product information available in English. To study this practical industrial task, we present xPQA, a large-scale annotated cross-lingual PQA dataset in 12 languages across 9 branches, and report results in (1) candidate ranking, to select the best English candidate containing the information to answer a non-English question; and (2) answer generation, to generate a natural-sounding non-English answer based on the selected English candidate. We evaluate various approaches involving machine translation at runtime or offline, leveraging multilingual pre-trained LMs, and including or excluding xPQA training data. We find that (1) In-domain data is essential as cross-lingual rankers trained on other domains per
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#21482;&#38656;&#20351;&#29992;0.5%&#25968;&#25454;&#20415;&#21487;&#20197;&#36827;&#34892;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25351;&#20196;&#35843;&#25972;&#65292;&#24182;&#19988;&#19981;&#24433;&#21709;&#24615;&#33021;&#34920;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#33410;&#32422;&#22521;&#35757;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2305.09246</link><description>&lt;p&gt;
&#25110;&#35768;&#21482;&#38656;&#35201;0.5&#65285;&#30340;&#25968;&#25454;&#65306;&#20302;&#25968;&#25454;&#37327;&#35757;&#32451;&#25351;&#20196;&#35843;&#25972;&#21021;&#27493;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning. (arXiv:2305.09246v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#21482;&#38656;&#20351;&#29992;0.5%&#25968;&#25454;&#20415;&#21487;&#20197;&#36827;&#34892;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25351;&#20196;&#35843;&#25972;&#65292;&#24182;&#19988;&#19981;&#24433;&#21709;&#24615;&#33021;&#34920;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#33410;&#32422;&#22521;&#35757;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#36827;&#34892;&#20102;&#21021;&#27493;&#25506;&#32034;&#65292;&#20197;&#38477;&#20302;LLM&#25351;&#20196;&#35843;&#25972;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#65292;&#20174;&#32780;&#20943;&#23569;&#22521;&#35757;&#25104;&#26412;&#65292;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#12290; &#30740;&#31350;&#21457;&#29616;&#65292;LLM&#25351;&#20196;&#35843;&#25972;&#30340;&#25968;&#25454;&#21487;&#20197;&#38477;&#33267;0.5&#65285;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;Low Training Data Instruction Tuning&#65288;LTD Instruction Tuning&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning for large language models (LLMs) has gained attention from researchers due to its ability to unlock the potential of LLMs in following instructions. While instruction tuning offers advantages for facilitating the adaptation of large language models (LLMs) to downstream tasks as a fine-tuning approach, training models with tens of millions or even billions of parameters on large amounts of data results in unaffordable computational costs. To address this, we focus on reducing the data used in LLM instruction tuning to decrease training costs and improve data efficiency, dubbed as Low Training Data Instruction Tuning (LTD Instruction Tuning). Specifically, this paper conducts a preliminary exploration into reducing the data used in LLM training and identifies several observations regarding task specialization for LLM training, such as the optimization of performance for a specific task, the number of instruction types required for instruction tuning, and the amount of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#23558;&#22810;&#35821;&#35328;&#25688;&#35201;&#21644;&#36328;&#35821;&#35328;&#25688;&#35201;&#32479;&#19968;&#21040;&#26356;&#36890;&#29992;&#30340;&#22810;&#23545;&#22810;&#25688;&#35201;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;M2MS&#27169;&#22411;&#8220;Pisces&#8221;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#20219;&#20309;&#35821;&#35328;&#30340;&#25991;&#26723;&#24182;&#29983;&#25104;&#25688;&#35201;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Pisces&#22312;&#38646;-shot&#26041;&#21521;&#19978;&#34920;&#29616;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.09220</link><description>&lt;p&gt;
&#36208;&#21521;&#32479;&#19968;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Towards Unifying Multi-Lingual and Cross-Lingual Summarization. (arXiv:2305.09220v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09220
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23558;&#22810;&#35821;&#35328;&#25688;&#35201;&#21644;&#36328;&#35821;&#35328;&#25688;&#35201;&#32479;&#19968;&#21040;&#26356;&#36890;&#29992;&#30340;&#22810;&#23545;&#22810;&#25688;&#35201;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;M2MS&#27169;&#22411;&#8220;Pisces&#8221;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#20219;&#20309;&#35821;&#35328;&#30340;&#25991;&#26723;&#24182;&#29983;&#25104;&#25688;&#35201;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Pisces&#22312;&#38646;-shot&#26041;&#21521;&#19978;&#34920;&#29616;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36866;&#24212;&#22810;&#35821;&#35328;&#30340;&#19990;&#30028;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#22810;&#35821;&#35328;&#25688;&#35201;&#65288;MLS&#65289;&#21644;&#36328;&#35821;&#35328;&#25688;&#35201;&#65288;CLS&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#20010;&#20219;&#21153;&#22240;&#20026;&#23450;&#20041;&#30340;&#19981;&#21516;&#32780;&#34987;&#20998;&#21035;&#30740;&#31350;&#65292;&#38480;&#21046;&#20102;&#20004;&#32773;&#30340;&#20860;&#23481;&#24615;&#21644;&#31995;&#32479;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#23558;MLS&#21644;CLS&#32479;&#19968;&#21040;&#26356;&#36890;&#29992;&#30340;&#35774;&#32622;&#20013;&#65292;&#21363;&#22810;&#23545;&#22810;&#25688;&#35201;&#65288;M2MS&#65289;&#65292;&#20854;&#20013;&#21333;&#20010;&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#20219;&#20309;&#35821;&#35328;&#30340;&#25991;&#26723;&#24182;&#29983;&#25104;&#23427;&#20204;&#30340;&#25688;&#35201;&#65292;&#20063;&#21487;&#20197;&#29992;&#20219;&#20309;&#35821;&#35328;&#12290;&#20316;&#20026;&#36890;&#21521;M2MS&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#36827;&#34892;&#21021;&#27493;&#30740;&#31350;&#65292;&#34920;&#26126;M2MS&#21487;&#20197;&#26356;&#22909;&#22320;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#20256;&#36882;&#20219;&#21153;&#30693;&#35782;&#65292;&#32780;&#19981;&#26159;MLS&#21644;CLS&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Pisces&#65292;&#36825;&#26159;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;M2MS&#27169;&#22411;&#65292;&#36890;&#36807;&#19977;&#38454;&#27573;&#30340;&#39044;&#20808;&#35757;&#32451;&#23398;&#20064;&#35821;&#35328;&#24314;&#27169;&#12289;&#36328;&#35821;&#35328;&#21644;&#25688;&#35201;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;Pisces&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#65292;&#29305;&#21035;&#26159;&#22312;&#38646;-shot&#26041;&#21521;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
To adapt text summarization to the multilingual world, previous work proposes multi-lingual summarization (MLS) and cross-lingual summarization (CLS). However, these two tasks have been studied separately due to the different definitions, which limits the compatible and systematic research on both of them. In this paper, we aim to unify MLS and CLS into a more general setting, i.e., many-to-many summarization (M2MS), where a single model could process documents in any language and generate their summaries also in any language. As the first step towards M2MS, we conduct preliminary studies to show that M2MS can better transfer task knowledge across different languages than MLS and CLS. Furthermore, we propose Pisces, a pre-trained M2MS model that learns language modeling, cross-lingual ability and summarization ability via three-stage pre-training. Experimental results indicate that our Pisces significantly outperforms the state-of-the-art baselines, especially in the zero-shot directio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#20026;&#19981;&#21516;&#35821;&#35328;&#30340;&#21457;&#35328;&#20154;&#26500;&#24314;&#35821;&#38899;&#23545;&#35805;&#32763;&#35793;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;SpeechBSD&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#22522;&#20934;&#23454;&#39564;&#12290;&#20316;&#32773;&#25351;&#20986;&#19978;&#19979;&#25991;&#26159;&#35813;&#20219;&#21153;&#20013;&#38656;&#35201;&#32771;&#34385;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#12290;&#26368;&#32456;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35813;&#20219;&#21153;&#20013;&#21452;&#35821;&#19978;&#19979;&#25991;&#27604;&#21333;&#35821;&#19978;&#19979;&#25991;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.09210</link><description>&lt;p&gt;
&#21521;&#19981;&#21516;&#35821;&#35328;&#21457;&#35328;&#20154;&#26017;&#26059;&#30340;&#35821;&#38899;&#23545;&#35805;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Towards Speech Dialogue Translation Mediating Speakers of Different Languages. (arXiv:2305.09210v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#20026;&#19981;&#21516;&#35821;&#35328;&#30340;&#21457;&#35328;&#20154;&#26500;&#24314;&#35821;&#38899;&#23545;&#35805;&#32763;&#35793;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;SpeechBSD&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#22522;&#20934;&#23454;&#39564;&#12290;&#20316;&#32773;&#25351;&#20986;&#19978;&#19979;&#25991;&#26159;&#35813;&#20219;&#21153;&#20013;&#38656;&#35201;&#32771;&#34385;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#12290;&#26368;&#32456;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35813;&#20219;&#21153;&#20013;&#21452;&#35821;&#19978;&#19979;&#25991;&#27604;&#21333;&#35821;&#19978;&#19979;&#25991;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26159;&#20026;&#19981;&#21516;&#35821;&#35328;&#30340;&#21457;&#35328;&#20154;&#26500;&#24314;&#35821;&#38899;&#23545;&#35805;&#32763;&#35793;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;SpeechBSD&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20102;&#22522;&#20934;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#19978;&#19979;&#25991;&#26159;&#36825;&#39033;&#20219;&#21153;&#20013;&#38656;&#35201;&#35299;&#20915;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#65292;&#21363;&#21333;&#35821;&#19978;&#19979;&#25991;&#21644;&#21452;&#35821;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#20351;&#29992;Whisper&#21644;mBART&#36827;&#34892;&#32423;&#32852;&#24335;&#35821;&#38899;&#32763;&#35793;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#21452;&#35821;&#19978;&#19979;&#25991;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new task, speech dialogue translation mediating speakers of different languages. We construct the SpeechBSD dataset for the task and conduct baseline experiments. Furthermore, we consider context to be an important aspect that needs to be addressed in this task and propose two ways of utilizing context, namely monolingual context and bilingual context. We conduct cascaded speech translation experiments using Whisper and mBART, and show that bilingual context performs better in our settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26435;&#37325;&#33707;&#27604;&#20044;&#26031;&#20998;&#25968;&#20316;&#20026;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#24402;&#22240;&#26694;&#26550;&#65292;&#21487;&#20197;&#28085;&#30422;&#24456;&#22810;&#19981;&#21516;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#21253;&#25324;&#29305;&#24449;&#20132;&#20114;&#65292;&#35299;&#20915;&#20102;&#26041;&#27861;&#32321;&#34893;&#21644;&#19981;&#21487;&#27604;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#26041;&#27861;&#30340;&#21521;&#37327;&#31354;&#38388;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#21644;&#35299;&#37322;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20854;&#22810;&#21151;&#33021;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09204</link><description>&lt;p&gt;
&#26435;&#37325;&#33707;&#27604;&#20044;&#26031;&#20998;&#25968;&#65306;&#19968;&#20010;&#29305;&#24449;&#24402;&#22240;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Weighted M\"obius Score: A Unified Framework for Feature Attribution. (arXiv:2305.09204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26435;&#37325;&#33707;&#27604;&#20044;&#26031;&#20998;&#25968;&#20316;&#20026;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#24402;&#22240;&#26694;&#26550;&#65292;&#21487;&#20197;&#28085;&#30422;&#24456;&#22810;&#19981;&#21516;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#21253;&#25324;&#29305;&#24449;&#20132;&#20114;&#65292;&#35299;&#20915;&#20102;&#26041;&#27861;&#32321;&#34893;&#21644;&#19981;&#21487;&#27604;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#30740;&#31350;&#26041;&#27861;&#30340;&#21521;&#37327;&#31354;&#38388;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#21644;&#35299;&#37322;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20854;&#22810;&#21151;&#33021;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#27599;&#20010;&#29305;&#24449;&#23545;&#39044;&#27979;&#30340;&#24433;&#21709;&#26469;&#35299;&#37322;&#40657;&#30418;&#27169;&#22411;&#39044;&#27979;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#23558;&#29305;&#24449;&#24402;&#22240;&#25193;&#23637;&#21040;&#22810;&#20010;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#32479;&#19968;&#30340;&#26694;&#26550;&#23548;&#33268;&#26041;&#27861;&#30340;&#22823;&#37327;&#32321;&#34893;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#19981;&#33021;&#30452;&#25509;&#27604;&#36739;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#30340;&#24402;&#22240;&#26694;&#26550;&#8212;&#8212;&#26435;&#37325;&#33707;&#27604;&#20044;&#26031;&#20998;&#25968;&#65292;&#24182;&#26174;&#31034;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#38024;&#23545;&#21333;&#20010;&#29305;&#24449;&#21644;&#29305;&#24449;&#20132;&#20114;&#30340;&#24402;&#22240;&#26041;&#27861;&#26159;&#29305;&#20363;&#65292;&#36824;&#39564;&#35777;&#20102;&#19968;&#20123;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#30740;&#31350;&#24402;&#22240;&#26041;&#27861;&#30340;&#21521;&#37327;&#31354;&#38388;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#26631;&#20934;&#32447;&#24615;&#20195;&#25968;&#24037;&#20855;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#25552;&#20379;&#35299;&#37322;&#65292;&#21253;&#25324;&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#21644;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#36825;&#20123;&#24402;&#22240;&#26041;&#27861;&#24212;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#29305;&#24449;&#20132;&#20114;&#26469;&#23454;&#35777;&#20102;&#26694;&#26550;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attribution aims to explain the reasoning behind a black-box model's prediction by identifying the impact of each feature on the prediction. Recent work has extended feature attribution to interactions between multiple features. However, the lack of a unified framework has led to a proliferation of methods that are often not directly comparable. This paper introduces a parameterized attribution framework -- the Weighted M\"obius Score -- and (i) shows that many different attribution methods for both individual features and feature interactions are special cases and (ii) identifies some new methods. By studying the vector space of attribution methods, our framework utilizes standard linear algebra tools and provides interpretations in various fields, including cooperative game theory and causal mediation analysis. We empirically demonstrate the framework's versatility and effectiveness by applying these attribution methods to feature interactions in sentiment analysis and chain-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#23398;&#38590;&#23398;&#30340;&#20449;&#24687;&#25277;&#21462;&#23398;&#20064;&#26694;&#26550;&#65292;&#20998;&#20026;&#20837;&#38376;&#12289;&#22256;&#38590;&#21644;&#20027;&#38454;&#27573;&#65292;&#27169;&#20223;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#65292;&#36890;&#36807;&#20998;&#38454;&#27573;&#23398;&#20064;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.09193</link><description>&lt;p&gt;
&#20449;&#24687;&#25277;&#21462;&#30340;&#26131;&#23398;&#38590;&#23398;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Easy-to-Hard Learning for Information Extraction. (arXiv:2305.09193v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26131;&#23398;&#38590;&#23398;&#30340;&#20449;&#24687;&#25277;&#21462;&#23398;&#20064;&#26694;&#26550;&#65292;&#20998;&#20026;&#20837;&#38376;&#12289;&#22256;&#38590;&#21644;&#20027;&#38454;&#27573;&#65292;&#27169;&#20223;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#65292;&#36890;&#36807;&#20998;&#38454;&#27573;&#23398;&#20064;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#25277;&#21462;&#26159;&#25351;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#20986;&#21629;&#21517;&#23454;&#20307;&#12289;&#23454;&#20307;&#20851;&#31995;&#21644;&#20107;&#20214;&#31561;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#20223;&#20154;&#31867;&#23398;&#20064;&#36807;&#31243;&#30340;&#26131;&#23398;&#38590;&#23398;&#26694;&#26550;&#65292;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65306;&#20837;&#38376;&#38454;&#27573;&#12289;&#22256;&#38590;&#38454;&#27573;&#21644;&#20027;&#38454;&#27573;&#12290;&#36890;&#36807;&#20998;&#38454;&#27573;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20419;&#36827;&#20102;&#27169;&#22411;&#33719;&#24471;&#26356;&#24191;&#27867;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#25552;&#39640;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#22235;&#20010;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information extraction (IE) systems aim to automatically extract structured information, such as named entities, relations between entities, and events, from unstructured texts. While most existing work addresses a particular IE task, universally modeling various IE tasks with one model has achieved great success recently. Despite their success, they employ a one-stage learning strategy, i.e., directly learning to extract the target structure given the input text, which contradicts the human learning process. In this paper, we propose a unified easy-to-hard learning framework consisting of three stages, i.e., the easy stage, the hard stage, and the main stage, for IE by mimicking the human learning process. By breaking down the learning process into multiple stages, our framework facilitates the model to acquire general IE task knowledge and improve its generalization ability. Extensive experiments across four IE tasks demonstrate the effectiveness of our framework. We achieve new stat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20013;&#38388;&#24207;&#21015;&#26469;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#39046;&#22495;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20840;&#25490;&#21015;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35793;&#30721;&#31639;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09154</link><description>&lt;p&gt;
&#36845;&#20195;&#32763;&#35793;&#65306;&#20351;&#29992;&#20013;&#38388;&#24207;&#21015;&#26469;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#39046;&#22495;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Progressive Translation: Improving Domain Robustness of Neural Machine Translation with Intermediate Sequences. (arXiv:2305.09154v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09154
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20013;&#38388;&#24207;&#21015;&#26469;&#25552;&#39640;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#39046;&#22495;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20840;&#25490;&#21015;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35793;&#30721;&#31639;&#27861;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20013;&#38388;&#30417;&#30563;&#20449;&#21495;&#26377;&#21161;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#26159;&#21542;&#23384;&#22312;&#26377;&#21161;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#30340;&#20013;&#38388;&#20449;&#21495;&#12290;&#20511;&#37492;&#32479;&#35745;&#26426;&#22120;&#32763;&#35793;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20013;&#38388;&#20449;&#21495;&#65292;&#21363;&#20174;&#8220;&#28304;&#35821;&#35328;&#8221;&#32467;&#26500;&#21040;&#8220;&#30446;&#26631;&#35821;&#35328;&#8221;&#32467;&#26500;&#30340;&#20013;&#38388;&#24207;&#21015;&#12290;&#36825;&#31181;&#20013;&#38388;&#24207;&#21015;&#24341;&#20837;&#20102;&#19968;&#20010;&#24402;&#32435;&#20559;&#32622;&#65292;&#21453;&#26144;&#20102;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#30340;&#32763;&#35793;&#21407;&#21017;&#65292;&#21487;&#20197;&#20943;&#23569;&#26377;&#23475;&#20110;&#36328;&#39046;&#22495;&#27867;&#21270;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20840;&#25490;&#21015;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#20197;&#20943;&#36731;&#30001;&#26292;&#38706;&#20559;&#24046;&#24341;&#36215;&#30340;&#20013;&#38388;&#24207;&#21015;&#23545;&#30446;&#26631;&#30340;&#34394;&#20551;&#22240;&#26524;&#20851;&#31995;&#12290;&#20351;&#29992;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35793;&#30721;&#31639;&#27861;&#20174;&#25152;&#26377;&#25490;&#21015;&#20013;&#36873;&#25321;&#26368;&#20339;&#20505;&#36873;&#32763;&#35793;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#24341;&#20837;&#30340;&#20013;&#38388;&#20449;&#21495;&#26174;&#33879;&#25552;&#39640;&#20102;NMT&#27169;&#22411;&#36328;&#19981;&#21516;&#39046;&#22495;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20943;&#23567;&#20102;&#20869;&#37096;&#21644;&#22806;&#37096;&#32763;&#35793;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous studies show that intermediate supervision signals benefit various Natural Language Processing tasks. However, it is not clear whether there exist intermediate signals that benefit Neural Machine Translation (NMT). Borrowing techniques from Statistical Machine Translation, we propose intermediate signals which are intermediate sequences from the "source-like" structure to the "target-like" structure. Such intermediate sequences introduce an inductive bias that reflects a domain-agnostic principle of translation, which reduces spurious correlations that are harmful to out-of-domain generalisation. Furthermore, we introduce a full-permutation multi-task learning to alleviate the spurious causal relations from intermediate sequences to the target, which results from exposure bias. The Minimum Bayes Risk decoding algorithm is used to pick the best candidate translation from all permutations to further improve the performance. Experiments show that the introduced intermediate signa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#23545;&#40784;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#65292;&#23427;&#32467;&#21512;&#20102;&#21477;&#23376;&#32423;&#21035;&#21644;&#26631;&#35760;&#32423;&#21035;&#30340;&#23545;&#40784;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#34920;&#31034;&#32763;&#35793;&#23398;&#20064;&#20219;&#21153;&#65292;&#20174;&#32780;&#23558;&#32763;&#35793;&#20449;&#24687;&#23884;&#20837;&#26631;&#35760;&#34920;&#31034;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.09148</link><description>&lt;p&gt;
&#21452;&#37325;&#23545;&#40784;&#39044;&#35757;&#32451;&#29992;&#20110;&#36328;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Dual-Alignment Pre-training for Cross-lingual Sentence Embedding. (arXiv:2305.09148v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09148
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#23545;&#40784;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#65292;&#23427;&#32467;&#21512;&#20102;&#21477;&#23376;&#32423;&#21035;&#21644;&#26631;&#35760;&#32423;&#21035;&#30340;&#23545;&#40784;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#34920;&#31034;&#32763;&#35793;&#23398;&#20064;&#20219;&#21153;&#65292;&#20174;&#32780;&#23558;&#32763;&#35793;&#20449;&#24687;&#23884;&#20837;&#26631;&#35760;&#34920;&#31034;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#21477;&#23376;&#32423;&#21035;&#32763;&#35793;&#25490;&#21517;&#20219;&#21153;&#35757;&#32451;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#26159;&#36328;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22810;&#35821;&#35328;&#22330;&#26223;&#20013;&#65292;&#26631;&#35760;&#32423;&#21035;&#30340;&#23545;&#40784;&#20063;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20294;&#27492;&#21069;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#36825;&#19968;&#38382;&#39064;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#37325;&#23545;&#40784;&#39044;&#35757;&#32451;&#65288;DAP&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#35821;&#35328;&#21477;&#23376;&#23884;&#20837;&#65292;&#23427;&#32467;&#21512;&#20102;&#21477;&#23376;&#32423;&#21035;&#21644;&#26631;&#35760;&#32423;&#21035;&#30340;&#23545;&#40784;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#32763;&#35793;&#23398;&#20064;&#65288;RTL&#65289;&#20219;&#21153;&#65292;&#20854;&#20013;&#27169;&#22411;&#23398;&#20064;&#20351;&#29992;&#21333;&#20391;&#19978;&#19979;&#25991;&#21270;&#30340;&#26631;&#35760;&#34920;&#31034;&#26469;&#37325;&#24314;&#20854;&#32763;&#35793;&#23545;&#24212;&#29289;&#12290;&#36825;&#31181;&#37325;&#24314;&#30446;&#26631;&#40723;&#21169;&#27169;&#22411;&#23558;&#32763;&#35793;&#20449;&#24687;&#23884;&#20837;&#26631;&#35760;&#34920;&#31034;&#20013;&#12290;&#19982;&#20854;&#20182;&#26631;&#35760;&#32423;&#21035;&#23545;&#40784;&#26041;&#27861;&#65288;&#22914;&#32763;&#35793;&#35821;&#35328;&#27169;&#22411;&#65289;&#30456;&#27604;&#65292;RTL&#26356;&#36866;&#21512;&#21452;&#32534;&#30721;&#22120;&#20307;&#31995;&#32467;&#26500;&#65292;&#32780;&#19988;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown that dual encoder models trained with the sentence-level translation ranking task are effective methods for cross-lingual sentence embedding. However, our research indicates that token-level alignment is also crucial in multilingual scenarios, which has not been fully explored previously. Based on our findings, we propose a dual-alignment pre-training (DAP) framework for cross-lingual sentence embedding that incorporates both sentence-level and token-level alignment. To achieve this, we introduce a novel representation translation learning (RTL) task, where the model learns to use one-side contextualized token representation to reconstruct its translation counterpart. This reconstruction objective encourages the model to embed translation information into the token representation. Compared to other token-level alignment methods such as translation language modeling, RTL is more suitable for dual encoder architectures and is computationally efficient. Extensive
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#26426;&#21046;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#32780;&#30693;&#35782;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#23545;&#20110;&#35760;&#24518;&#24418;&#25104;&#20063;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.09144</link><description>&lt;p&gt;
&#35760;&#24518;&#36824;&#26159;&#24536;&#21364;&#65311;&#28145;&#20837;&#25506;&#35752;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#35760;&#24518;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models. (arXiv:2305.09144v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#26426;&#21046;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#32780;&#30693;&#35782;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#23545;&#20110;&#35760;&#24518;&#24418;&#25104;&#20063;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#26159;&#26368;&#22522;&#26412;&#30340;&#35748;&#30693;&#21151;&#33021;&#20043;&#19968;&#65292;&#26159;&#23384;&#20648;&#19990;&#30028;&#30693;&#35782;&#21644;&#27963;&#21160;&#32463;&#21382;&#30340;&#20648;&#34255;&#24211;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;&#30456;&#21453;&#65292;&#27809;&#26377;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#20445;&#25345;-&#36951;&#24536;&#30340;&#30683;&#30462;&#24182;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#26426;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#30446;&#26631;&#30693;&#35782;&#31867;&#22411;&#12289;&#23398;&#20064;&#31574;&#30053;&#21644;&#23398;&#20064;&#26102;&#38388;&#34920;&#31561;&#65292;&#24320;&#23637;&#20102;&#28145;&#20837;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#32467;&#26524;&#21457;&#29616;&#65306;1&#65289;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#26159;&#23481;&#26131;&#36951;&#24536;&#30340;&#65307;2&#65289;&#39044;&#35757;&#32451;&#21487;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#35760;&#24518;&#33021;&#21147;&#65307;3&#65289;&#30693;&#35782;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#26174;&#33879;&#24433;&#21709;&#35760;&#24518;&#24418;&#25104;&#12290;&#36825;&#20123;&#32467;&#35770;&#26377;&#21161;&#20110;&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#20026;&#35774;&#35745;&#21644;&#35780;&#20272;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#26041;&#27861;&#21644;&#25512;&#29702;&#31639;&#27861;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory is one of the most essential cognitive functions serving as a repository of world knowledge and episodes of activities. In recent years, large-scale pre-trained language models have shown remarkable memorizing ability. On the contrary, vanilla neural networks without pre-training have been long observed suffering from the catastrophic forgetting problem. To investigate such a retentive-forgetful contradiction and understand the memory mechanism of language models, we conduct thorough experiments by controlling the target knowledge types, the learning strategies and the learning schedules. We find that: 1) Vanilla language models are forgetful; 2) Pre-training leads to retentive language models; 3) Knowledge relevance and diversification significantly influence the memory formation. These conclusions are useful for understanding the abilities of pre-trained language models and shed light on designing and evaluating new learning and inference algorithms of language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;Transformer-based Video Question Answering&#26041;&#27861;&#65292;&#21363;&#23558;&#35270;&#39057;&#24103;&#36830;&#25509;&#25104; $n\times n$ &#30340;&#30697;&#38453;&#65292;&#20174;&#32780;&#23558;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#20351;&#29992;&#37327;&#20174; $n^{2}$ &#20943;&#23569;&#21040;1&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#21644;&#33410;&#30465;&#20102;&#23384;&#20648;&#31354;&#38388;&#65292;&#32780;&#20173;&#28982;&#20445;&#25345;&#20102;&#21407;&#22987;&#35270;&#39057;&#30340;&#26102;&#38388;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.09107</link><description>&lt;p&gt;
&#35270;&#39057;&#20540;&#24471; $n\times n$ &#24352;&#22270;&#20687;&#21527;? &#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#35270;&#39057;&#38382;&#31572;&#39640;&#25928;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is a Video worth $n\times n$ Images? A Highly Efficient Approach to Transformer-based Video Question Answering. (arXiv:2305.09107v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;Transformer-based Video Question Answering&#26041;&#27861;&#65292;&#21363;&#23558;&#35270;&#39057;&#24103;&#36830;&#25509;&#25104; $n\times n$ &#30340;&#30697;&#38453;&#65292;&#20174;&#32780;&#23558;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#20351;&#29992;&#37327;&#20174; $n^{2}$ &#20943;&#23569;&#21040;1&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#21644;&#33410;&#30465;&#20102;&#23384;&#20648;&#31354;&#38388;&#65292;&#32780;&#20173;&#28982;&#20445;&#25345;&#20102;&#21407;&#22987;&#35270;&#39057;&#30340;&#26102;&#38388;&#32467;&#26500;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22522;&#20110;Transformer&#30340;&#35270;&#39057;&#38382;&#31572;&#65288;VideoQA&#65289;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#19968;&#20010;&#25110;&#22810;&#20010;&#22270;&#20687;&#32534;&#30721;&#22120;&#29420;&#31435;&#32534;&#30721;&#24103;&#65292;&#24182;&#22312;&#24103;&#21644;&#38382;&#39064;&#20043;&#38388;&#36827;&#34892;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#24335;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#20869;&#23384;&#20351;&#29992;&#21644;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#30340;&#19981;&#21487;&#36991;&#20813;&#30340;&#20943;&#24930;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29616;&#26377;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#39640;&#25928;VideoQA&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#23558;&#35270;&#39057;&#24103;&#36830;&#25509;&#21040;&#19968;&#20010; $n\times n$ &#30697;&#38453;&#20013;&#65292;&#28982;&#21518;&#23558;&#20854;&#36716;&#25442;&#20026;&#19968;&#24352;&#22270;&#20687;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#23558;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#20351;&#29992;&#20174; $n^{2}$&#20943;&#23569;&#21040;1&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21407;&#22987;&#35270;&#39057;&#30340;&#26102;&#38388;&#32467;&#26500;&#12290;&#22312;MSRVTT&#21644;TrafficQA&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20197;&#36817; $4\times$ &#26356;&#24555;&#30340;&#36895;&#24230;&#21644;&#21482;&#26377;30&#65285;&#30340;&#20869;&#23384;&#20351;&#29992;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#38598;&#25104;&#21040;VideoQA&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#21482;&#26377;&#24456;&#23567;&#20195;&#20215;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21487;&#27604;&#29978;&#33267;&#20248;&#24322;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#26174;&#33879;&#21152;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional Transformer-based Video Question Answering (VideoQA) approaches generally encode frames independently through one or more image encoders followed by interaction between frames and question. However, such schema would incur significant memory use and inevitably slow down the training and inference speed. In this work, we present a highly efficient approach for VideoQA based on existing vision-language pre-trained models where we concatenate video frames to a $n\times n$ matrix and then convert it to one image. By doing so, we reduce the use of the image encoder from $n^{2}$ to $1$ while maintaining the temporal structure of the original video. Experimental results on MSRVTT and TrafficQA show that our proposed approach achieves state-of-the-art performance with nearly $4\times$ faster speed and only 30% memory use. We show that by integrating our approach into VideoQA systems we can achieve comparable, even superior, performance with a significant speed up for training and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36882;&#30693;&#35782;&#30340;&#26435;&#37325;&#32487;&#25215;&#33976;&#39311;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#23545;&#40784;&#25439;&#22833;&#23601;&#21487;&#20197;&#35757;&#32451;&#20986;&#19968;&#20010;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;KD&#30340;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.09098</link><description>&lt;p&gt;
&#20219;&#21153;&#26080;&#20851;BERT&#21387;&#32553;&#30340;&#26435;&#37325;&#32487;&#25215;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Weight-Inherited Distillation for Task-Agnostic BERT Compression. (arXiv:2305.09098v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36882;&#30693;&#35782;&#30340;&#26435;&#37325;&#32487;&#25215;&#33976;&#39311;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#23545;&#40784;&#25439;&#22833;&#23601;&#21487;&#20197;&#35757;&#32451;&#20986;&#19968;&#20010;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;KD&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#21387;&#32553;BERT&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#20043;&#21069;&#30340;KD&#26041;&#27861;&#20391;&#37325;&#20110;&#20026;&#23398;&#29983;&#27169;&#22411;&#35774;&#35745;&#39069;&#22806;&#30340;&#23545;&#40784;&#25439;&#22833;&#65292;&#20197;&#27169;&#20223;&#25945;&#24072;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#36825;&#20123;&#26041;&#27861;&#20197;&#38388;&#25509;&#30340;&#26041;&#24335;&#20256;&#36882;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26435;&#37325;&#32487;&#25215;&#33976;&#39311;&#65288;WID&#65289;&#26041;&#27861;&#65292;&#30452;&#25509;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36882;&#30693;&#35782;&#12290;WID&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#23545;&#40784;&#25439;&#22833;&#65292;&#36890;&#36807;&#32487;&#25215;&#26435;&#37325;&#26469;&#35757;&#32451;&#19968;&#20010;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#26032;&#35270;&#35282;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#34892;&#21387;&#32553;&#22120;&#21644;&#21015;&#21387;&#32553;&#22120;&#35774;&#35745;&#20026;&#26144;&#23556;&#65292;&#28982;&#21518;&#36890;&#36807;&#32467;&#26500;&#37325;&#21442;&#25968;&#21270;&#21387;&#32553;&#26435;&#37325;&#12290;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;WID&#20248;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;KD&#30340;&#22522;&#32447;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;WID&#20063;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27880;&#24847;&#21147;&#20998;&#24067;&#23545;&#40784;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#25945;&#24072;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation (KD) is a predominant approach for BERT compression. Previous KD-based methods focus on designing extra alignment losses for the student model to mimic the behavior of the teacher model. These methods transfer the knowledge in an indirect way. In this paper, we propose a novel Weight-Inherited Distillation (WID), which directly transfers knowledge from the teacher. WID does not require any additional alignment loss and trains a compact student by inheriting the weights, showing a new perspective of knowledge distillation. Specifically, we design the row compactors and column compactors as mappings and then compress the weights via structural re-parameterization. Experimental results on the GLUE and SQuAD benchmarks show that WID outperforms previous state-of-the-art KD-based baselines. Further analysis indicates that WID can also learn the attention patterns from the teacher model without any alignment loss on attention distributions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#24341;&#23548;&#30340;LLM&#25552;&#31034;SGP-TOD&#65292;&#29992;&#20110;&#36731;&#26494;&#26500;&#24314;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#65292;&#36991;&#20813;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09067</link><description>&lt;p&gt;
SGP-TOD: &#22522;&#20110;&#27169;&#24335;&#24341;&#23548;&#30340;LLM&#25552;&#31034;&#36731;&#26494;&#26500;&#24314;&#20219;&#21153;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting. (arXiv:2305.09067v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#24335;&#24341;&#23548;&#30340;LLM&#25552;&#31034;SGP-TOD&#65292;&#29992;&#20110;&#36731;&#26494;&#26500;&#24314;&#20219;&#21153;&#22411;&#23545;&#35805;&#31995;&#32479;&#65292;&#36991;&#20813;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#20248;&#20110;&#20808;&#36827;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#30740;&#31350;&#20013;&#65292;&#36731;&#26494;&#26500;&#24314;&#31471;&#21040;&#31471;&#20219;&#21153;&#26426;&#22120;&#20154;&#24182;&#20197;&#26368;&#23567;&#30340;&#20154;&#21147;&#25237;&#20837;&#32500;&#25252;&#20854;&#19982;&#26032;&#21151;&#33021;&#30340;&#38598;&#25104;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#23545;&#35805;&#33021;&#21147;&#21644;&#36981;&#24490;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SGP-TOD&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#27169;&#24335;&#24341;&#23548;&#25552;&#31034;&#65292;&#29992;&#20110;&#36731;&#26494;&#26500;&#24314;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;&#12290;&#21033;&#29992;&#31526;&#21495;&#30693;&#35782;--&#20219;&#21153;&#27169;&#24335;&#65292;&#25105;&#20204;&#25351;&#23548;&#22266;&#23450;&#30340;LLM&#22312;&#26032;&#20219;&#21153;&#19978;&#29983;&#25104;&#36866;&#24403;&#30340;&#21709;&#24212;&#65292;&#36991;&#20813;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SGP-TOD&#21253;&#25324;&#19977;&#20010;&#32452;&#20214;&#65306;&#29992;&#20110;&#19982;&#29992;&#25143;&#20132;&#20114;&#30340;LLM&#65292;&#29992;&#20110;&#24110;&#21161;LLM&#36827;&#34892;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;DST&#25552;&#31034;&#22120;&#65292;&#20197;&#21450;&#29992;&#20110;&#24341;&#20986;&#31526;&#21512;&#25552;&#20379;&#30340;&#23545;&#35805;&#31574;&#30053;&#30340;&#36866;&#24403;&#21709;&#24212;&#30340;&#31574;&#30053;&#25552;&#31034;&#22120;&#12290;&#22312;Multiwoz&#12289;RADDLE&#21644;STAR&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#31574;&#30053;&#26174;&#33879;&#20248;&#20110;&#20960;&#20010;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building end-to-end task bots and maintaining their integration with new functionalities using minimal human efforts is a long-standing challenge in dialog research. Recently large language models (LLMs) have demonstrated exceptional proficiency in conversational engagement and adherence to instructions across various downstream tasks. In this work, we introduce SGP-TOD, Schema-Guided Prompting for building Task-Oriented Dialog systems effortlessly based on LLMs. Utilizing the symbolic knowledge -- task schema, we instruct fixed LLMs to generate appropriate responses on novel tasks, circumventing the need for training data. Specifically, SGP-TOD comprises three components: a LLM for engaging with users, a DST Prompter to aid the LLM with dialog state tracking, which is then used to retrieve database items, and a Policy Prompter to elicit proper responses adhering to the provided dialog policy. Experimental results on Multiwoz, RADDLE and STAR datasets show that our training-free strate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;NLP&#30340;&#20219;&#21153;&#21644;&#24615;&#33021;&#27979;&#37327;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27495;&#20998;&#31867;&#26041;&#27861;&#65292;&#32463;&#36807;&#30456;&#20851;&#30740;&#31350;&#21644;&#35843;&#26597;&#65292;&#21457;&#29616;&#29616;&#26377;&#20219;&#21153;&#27809;&#26377;&#26126;&#30830;&#21644;&#19968;&#33268;&#30340;&#27010;&#24565;&#65292;&#22522;&#20934;&#23384;&#22312;&#25805;&#20316;&#21270;&#20998;&#27495;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#22522;&#20934;&#20105;&#35758;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.09022</link><description>&lt;p&gt;
&#21452;&#26041;&#20849;&#33310;&#65306;&#23548;&#33322;NLP&#20219;&#21153;&#30340;&#27010;&#24565;&#21270;&#21644;&#24615;&#33021;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
It Takes Two to Tango: Navigating Conceptualizations of NLP Tasks and Measurements of Performance. (arXiv:2305.09022v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;NLP&#30340;&#20219;&#21153;&#21644;&#24615;&#33021;&#27979;&#37327;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27495;&#20998;&#31867;&#26041;&#27861;&#65292;&#32463;&#36807;&#30456;&#20851;&#30740;&#31350;&#21644;&#35843;&#26597;&#65292;&#21457;&#29616;&#29616;&#26377;&#20219;&#21153;&#27809;&#26377;&#26126;&#30830;&#21644;&#19968;&#33268;&#30340;&#27010;&#24565;&#65292;&#22522;&#20934;&#23384;&#22312;&#25805;&#20316;&#21270;&#20998;&#27495;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#22522;&#20934;&#20105;&#35758;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#36827;&#23637;&#36234;&#26469;&#36234;&#22810;&#22320;&#36890;&#36807;&#22522;&#20934;&#36827;&#34892;&#34913;&#37327;&#65307;&#22240;&#27492;&#65292;&#20102;&#35299;&#20174;&#19994;&#32773;&#21487;&#33021;&#22312;&#22522;&#20934;&#30340;&#26377;&#25928;&#24615;&#19978;&#23384;&#22312;&#20998;&#27495;&#30340;&#21407;&#22240;&#21644;&#26102;&#38388;&#38656;&#35201;&#23545;&#36827;&#23637;&#36827;&#34892;&#20855;&#20307;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;&#27979;&#37327;&#24314;&#27169;&#24037;&#20855;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#20998;&#27495;&#20998;&#31867;&#26041;&#27861;&#65292;&#21306;&#20998;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#20998;&#27495;&#65306;1&#65289;&#22914;&#20309;&#27010;&#24565;&#21270;&#20219;&#21153;&#65307;2&#65289;&#22914;&#20309;&#25805;&#20316;&#21270;&#27169;&#22411;&#24615;&#33021;&#30340;&#27979;&#37327;&#12290;&#20026;&#20102;&#25552;&#20379;&#25903;&#25345;&#25105;&#20204;&#20998;&#31867;&#30340;&#35777;&#25454;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#30456;&#20851;&#25991;&#29486;&#30340;&#20803;&#20998;&#26512;&#65292;&#20197;&#20102;&#35299;NLP&#20219;&#21153;&#30340;&#27010;&#24565;&#21270;&#65292;&#20197;&#21450;&#23545;&#24433;&#21709;&#22522;&#20934;&#26377;&#25928;&#24615;&#30340;&#19981;&#21516;&#22240;&#32032;&#21360;&#35937;&#30340;&#20174;&#19994;&#32773;&#30340;&#35843;&#26597;&#12290;&#25105;&#20204;&#23545;&#20174;&#26680;&#24515;&#21442;&#32771;&#35299;&#20915;&#21040;&#38382;&#31572;&#31561;&#20843;&#39033;&#20219;&#21153;&#30340;&#20803;&#20998;&#26512;&#21644;&#35843;&#26597;&#21457;&#29616;&#65292;&#20219;&#21153;&#36890;&#24120;&#27809;&#26377;&#26126;&#30830;&#21644;&#19968;&#33268;&#30340;&#27010;&#24565;&#65292;&#22522;&#20934;&#23384;&#22312;&#25805;&#20316;&#21270;&#20998;&#27495;&#12290;&#36825;&#20123;&#21457;&#29616;&#25903;&#25345;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#20998;&#27495;&#20998;&#31867;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#22522;&#20110;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#20915;&#22522;&#20934;&#20105;&#35758;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Progress in NLP is increasingly measured through benchmarks; hence, contextualizing progress requires understanding when and why practitioners may disagree about the validity of benchmarks. We develop a taxonomy of disagreement, drawing on tools from measurement modeling, and distinguish between two types of disagreement: 1) how tasks are conceptualized and 2) how measurements of model performance are operationalized. To provide evidence for our taxonomy, we conduct a meta-analysis of relevant literature to understand how NLP tasks are conceptualized, as well as a survey of practitioners about their impressions of different factors that affect benchmark validity. Our meta-analysis and survey across eight tasks, ranging from coreference resolution to question answering, uncover that tasks are generally not clearly and consistently conceptualized and benchmarks suffer from operationalization disagreements. These findings support our proposed taxonomy of disagreement. Finally, based on ou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;AI&#30340;&#20132;&#20114;&#24335;&#24037;&#20855;CARE&#65292;&#29992;&#20110;&#25903;&#25345;&#21516;&#20394;&#36741;&#23548;&#21592;&#36890;&#36807;&#33258;&#21160;&#24314;&#35758;&#29983;&#25104;&#26469;&#25552;&#39640;&#20182;&#20204;&#30340;&#33021;&#21147;&#12290;&#21033;&#29992; Motivational Interviewing &#26694;&#26550;&#65292;CARE &#22312;&#23454;&#38469;&#22521;&#35757;&#38454;&#27573;&#24110;&#21161;&#36741;&#23548;&#21592;&#35786;&#26029;&#21738;&#31181;&#20855;&#20307;&#30340;&#36741;&#23548;&#31574;&#30053;&#26368;&#21512;&#36866;&#65292;&#24182;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#21709;&#24212;&#31034;&#20363;&#20316;&#20026;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.08982</link><description>&lt;p&gt;
&#24110;&#21161;&#24110;&#21161;&#32773;&#65306;&#36890;&#36807; AI &#24378;&#21270;&#23454;&#36341;&#21644;&#21453;&#39304;&#26469;&#25903;&#25345;&#21516;&#20394;&#36741;&#23548;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
Helping the Helper: Supporting Peer Counselors via AI-Empowered Practice and Feedback. (arXiv:2305.08982v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;AI&#30340;&#20132;&#20114;&#24335;&#24037;&#20855;CARE&#65292;&#29992;&#20110;&#25903;&#25345;&#21516;&#20394;&#36741;&#23548;&#21592;&#36890;&#36807;&#33258;&#21160;&#24314;&#35758;&#29983;&#25104;&#26469;&#25552;&#39640;&#20182;&#20204;&#30340;&#33021;&#21147;&#12290;&#21033;&#29992; Motivational Interviewing &#26694;&#26550;&#65292;CARE &#22312;&#23454;&#38469;&#22521;&#35757;&#38454;&#27573;&#24110;&#21161;&#36741;&#23548;&#21592;&#35786;&#26029;&#21738;&#31181;&#20855;&#20307;&#30340;&#36741;&#23548;&#31574;&#30053;&#26368;&#21512;&#36866;&#65292;&#24182;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#21709;&#24212;&#31034;&#20363;&#20316;&#20026;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#30334;&#19975;&#29992;&#25143;&#26469;&#21040;&#22312;&#32447;&#21516;&#20394;&#36741;&#23548;&#24179;&#21488;&#23547;&#27714;&#20851;&#20110;&#20174;&#20851;&#31995;&#21387;&#21147;&#21040;&#28966;&#34385;&#31561;&#22810;&#31181;&#20027;&#39064;&#30340;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#32447;&#21516;&#20394;&#25903;&#25345;&#32676;&#20307;&#24182;&#19981;&#24635;&#26159;&#20687;&#39044;&#26399;&#30340;&#37027;&#26679;&#26377;&#25928;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#29992;&#25143;&#19982;&#26080;&#29992;&#30340;&#36741;&#23548;&#21592;&#20135;&#29983;&#20102;&#36127;&#38754;&#20307;&#39564;&#12290;&#21516;&#20394;&#36741;&#23548;&#21592;&#26159;&#22312;&#32447;&#21516;&#20394;&#36741;&#23548;&#24179;&#21488;&#25104;&#21151;&#30340;&#20851;&#38190;&#65292;&#20294;&#20182;&#20204;&#20013;&#30340;&#22823;&#22810;&#25968;&#36890;&#24120;&#27809;&#26377;&#31995;&#32479;&#22320;&#25509;&#25910;&#25351;&#23548;&#25110;&#30417;&#30563;&#30340;&#26041;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461; CARE&#65306;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#22522;&#20110; AI &#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#33258;&#21160;&#24314;&#35758;&#29983;&#25104;&#22686;&#24378;&#21516;&#20394;&#36741;&#23548;&#21592;&#30340;&#33021;&#21147;&#12290;&#22312;&#23454;&#38469;&#22521;&#35757;&#38454;&#27573;&#65292;CARE &#24110;&#21161;&#35786;&#26029;&#22312;&#32473;&#23450;&#24773;&#22659;&#19979;&#21738;&#20123;&#20855;&#20307;&#30340;&#36741;&#23548;&#31574;&#30053;&#26368;&#21512;&#36866;&#65292;&#24182;&#25552;&#20379;&#37327;&#36523;&#23450;&#21046;&#30340;&#31034;&#20363;&#21709;&#24212;&#20316;&#20026;&#24314;&#35758;&#12290;&#36741;&#23548;&#21592;&#21487;&#20197;&#36873;&#25321;&#22312;&#22238;&#22797;&#27714;&#21161;&#32773;&#20043;&#21069;&#36873;&#25321;&#12289;&#20462;&#25913;&#25110;&#24573;&#30053;&#20219;&#20309;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Millions of users come to online peer counseling platforms to seek support on diverse topics ranging from relationship stress to anxiety. However, studies show that online peer support groups are not always as effective as expected largely due to users' negative experiences with unhelpful counselors. Peer counselors are key to the success of online peer counseling platforms, but most of them often do not have systematic ways to receive guidelines or supervision. In this work, we introduce CARE: an interactive AI-based tool to empower peer counselors through automatic suggestion generation. During the practical training stage, CARE helps diagnose which specific counseling strategies are most suitable in the given context and provides tailored example responses as suggestions. Counselors can choose to select, modify, or ignore any suggestion before replying to the support seeker. Building upon the Motivational Interviewing framework, CARE utilizes large-scale counseling conversation data
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#20851;&#20110;&#26080;&#23478;&#21487;&#24402;&#30340;&#25512;&#25991;&#25968;&#37327;&#19982;&#24030;&#32423;&#26080;&#23478;&#21487;&#24402;&#20154;&#21475;&#25968;&#37327;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#20272;&#35745;&#32654;&#22269;&#21508;&#24030;&#30340;&#26080;&#23478;&#21487;&#24402;&#20154;&#21475;&#24773;&#20917;&#65292;&#32780;&#33521;&#25991;&#25512;&#25991;&#20013;&#8220;homeless&#8221;&#20986;&#29616;&#30340;&#27010;&#29575;&#19982;&#24179;&#22343;&#25512;&#25991;&#24773;&#24863;&#22686;&#38271;&#36895;&#29575;&#20063;&#33021;&#21453;&#26144;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.08978</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#29702;&#31038;&#20132;&#23186;&#20307;&#20449;&#21495;&#27979;&#37327;&#26412;&#22320;&#26080;&#23478;&#21487;&#24402;&#27700;&#24179;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
An assessment of measuring local levels of homelessness through proxy social media signals. (arXiv:2305.08978v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08978
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#20851;&#20110;&#26080;&#23478;&#21487;&#24402;&#30340;&#25512;&#25991;&#25968;&#37327;&#19982;&#24030;&#32423;&#26080;&#23478;&#21487;&#24402;&#20154;&#21475;&#25968;&#37327;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#21487;&#20197;&#20272;&#35745;&#32654;&#22269;&#21508;&#24030;&#30340;&#26080;&#23478;&#21487;&#24402;&#20154;&#21475;&#24773;&#20917;&#65292;&#32780;&#33521;&#25991;&#25512;&#25991;&#20013;&#8220;homeless&#8221;&#20986;&#29616;&#30340;&#27010;&#29575;&#19982;&#24179;&#22343;&#25512;&#25991;&#24773;&#24863;&#22686;&#38271;&#36895;&#29575;&#20063;&#33021;&#21453;&#26144;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#31038;&#20132;&#23186;&#20307;&#27963;&#21160;&#21487;&#20197;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26816;&#27979;&#30340;&#24030;&#32423;&#20844;&#20849;&#21355;&#29983;&#25514;&#26045;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;2010-2019&#24180;&#21644;2022&#24180;&#26399;&#38388;&#20351;&#29992;&#22823;&#32422;100&#19975;&#20010;&#21253;&#21547;&#23376;&#23383;&#31526;&#20018;&#8220;homeless&#8221;&#30340;&#22320;&#29702;&#26631;&#35760;&#25512;&#25991;&#30340;&#25968;&#25454;&#38598;&#26469;&#20272;&#35745;&#25972;&#20010;&#32654;&#22269;&#24030;&#32423;&#21035;&#30340;&#26080;&#23478;&#21487;&#24402;&#20154;&#21475;&#30340;&#21162;&#21147;&#25104;&#26524;&#12290;&#26080;&#23478;&#21487;&#24402;&#20154;&#22763;&#30456;&#20851;&#30340;&#25512;&#25991;&#35745;&#25968;&#21644;&#25353;&#20154;&#22343;&#26080;&#23478;&#21487;&#24402;&#25968;&#37327;&#25490;&#21517;&#30340;&#30456;&#20851;&#24615;&#34920;&#26126;&#65292;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#36935;&#21040;&#25110;&#35266;&#23519;&#21040;&#26080;&#23478;&#21487;&#24402;&#20154;&#22763;&#30340;&#21487;&#33021;&#24615;&#19982;&#29992;&#25143;&#22312;&#32447;&#19978;&#20132;&#27969;&#30340;&#21487;&#33021;&#24615;&#20043;&#38388;&#23384;&#22312;&#20851;&#31995;&#12290;&#33521;&#35821;&#25512;&#25991;&#20013;&#8220;homeless&#8221;&#20986;&#29616;&#30340;log-odds&#22686;&#21152;&#20197;&#21450;&#24179;&#22343;&#25512;&#25991;&#24773;&#24863;&#22686;&#38271;&#21152;&#36895;&#34920;&#26126;&#26377;&#20851;&#26080;&#23478;&#21487;&#24402;&#20154;&#22763;&#30340;&#25512;&#25991;&#20063;&#21463;&#21040;&#20840;&#22269;&#33539;&#22260;&#20869;&#30340;&#36235;&#21183;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#23545;&#25968;&#25454;&#38598;&#30340;&#26435;&#34913;&#12289;&#31354;&#38388;&#26435;&#34913;&#20197;&#21450;&#19982;&#20854;&#20182;&#26469;&#28304;&#30340;&#27604;&#36739;&#35828;&#26126;&#20102;&#22312;&#27492;&#38382;&#39064;&#19978;&#31867;&#22411;II&#38169;&#35823;&#27010;&#29575;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies suggest social media activity can function as a proxy for measures of state-level public health, detectable through natural language processing. We present results of our efforts to apply this approach to estimate homelessness at the state level throughout the US during the period 2010-2019 and 2022 using a dataset of roughly 1 million geotagged tweets containing the substring ``homeless.'' Correlations between homelessness-related tweet counts and ranked per capita homelessness volume, but not general-population densities, suggest a relationship between the likelihood of Twitter users to personally encounter or observe homelessness in their everyday lives and their likelihood to communicate about it online. An increase to the log-odds of ``homeless'' appearing in an English-language tweet, as well as an acceleration in the increase in average tweet sentiment, suggest that tweets about homelessness are also affected by trends at the nation-scale. Additionally, changes to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#27880;&#24847;&#21147;&#22270;&#26469;&#33719;&#24471;&#35789;&#32423;&#21453;&#39304;&#30340;&#26032;&#22411;&#40657;&#30418;LLM&#27700;&#21360;&#31639;&#27861;&#65292;&#23454;&#29616;&#23545;&#30001;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#27700;&#21360;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2305.08883</link><description>&lt;p&gt;
&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#27700;&#21360;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Watermarking Text Generated by Black-Box Language Models. (arXiv:2305.08883v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;Transformer&#27880;&#24847;&#21147;&#22270;&#26469;&#33719;&#24471;&#35789;&#32423;&#21453;&#39304;&#30340;&#26032;&#22411;&#40657;&#30418;LLM&#27700;&#21360;&#31639;&#27861;&#65292;&#23454;&#29616;&#23545;&#30001;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#27700;&#21360;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#29616;&#22312;&#23637;&#31034;&#20102;&#22312;&#21508;&#39046;&#22495;&#20013;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#25216;&#33021;&#65292;&#24341;&#21457;&#20154;&#20204;&#23545;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#29983;&#25104;&#30340;&#25991;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#34987;&#21160;&#24335;&#26816;&#27979;&#26041;&#27861;&#38519;&#20837;&#20102;&#39046;&#22495;&#29305;&#24322;&#24615;&#21644;&#26377;&#38480;&#30340;&#23545;&#25239;&#24378;&#24230;&#12290;&#20026;&#20102;&#23454;&#29616;&#21487;&#38752;&#30340;&#26816;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27700;&#21360;&#30340;&#30333;&#30418;LLM&#26041;&#27861;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#23884;&#20837;&#27700;&#21360;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#23558;&#27169;&#22411;&#35789;&#27719;&#38543;&#26426;&#20998;&#21106;&#20197;&#33719;&#24471;&#29305;&#27530;&#21015;&#34920;&#24182;&#35843;&#25972;&#27010;&#29575;&#20998;&#24067;&#20197;&#20419;&#36827;&#21015;&#34920;&#20013;&#21333;&#35789;&#30340;&#36873;&#25321;&#12290;&#19968;&#20010;&#30693;&#26195;&#21015;&#34920;&#30340;&#26816;&#27979;&#31639;&#27861;&#21487;&#20197;&#35782;&#21035;&#24102;&#27700;&#21360;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35768;&#22810;&#20165;&#26377;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#22330;&#26223;&#20013;&#19981;&#36866;&#29992;&#12290;&#20026;&#20102;&#20801;&#35768;&#31532;&#19977;&#26041;&#32473;&#30001;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#21152;&#19978;&#27700;&#21360;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36716;&#25442;&#22120;&#27880;&#24847;&#21147;&#22270;&#26469;&#33719;&#24471;&#35789;&#32423;&#21453;&#39304;&#30340;&#26032;&#27700;&#21360;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#32473;&#30001;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#21152;&#19978;&#27700;&#21360;&#65292;&#32780;&#19981;&#24433;&#21709;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs now exhibit human-like skills in various fields, leading to worries about misuse. Thus, detecting generated text is crucial. However, passive detection methods are stuck in domain specificity and limited adversarial robustness. To achieve reliable detection, a watermark-based method was proposed for white-box LLMs, allowing them to embed watermarks during text generation. The method involves randomly dividing the model vocabulary to obtain a special list and adjusting the probability distribution to promote the selection of words in the list. A detection algorithm aware of the list can identify the watermarked text. However, this method is not applicable in many real-world scenarios where only black-box language models are available. For instance, third-parties that develop API-based vertical applications cannot watermark text themselves because API providers only supply generated text and withhold probability distributions to shield their commercial interests. To allow third-part
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25277;&#35937;&#22810;&#25991;&#26723;&#25688;&#35201;&#30340;&#23618;&#27425;&#32534;&#30721;-&#35299;&#30721;&#26041;&#26696;&#65292;&#22312;&#22810;&#39046;&#22495;&#30340;10&#20010;MDS&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.08503</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#25277;&#35937;&#22810;&#25991;&#26723;&#25688;&#35201;&#30340;&#23618;&#27425;&#32534;&#30721;-&#35299;&#30721;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document Summarization. (arXiv:2305.08503v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25277;&#35937;&#22810;&#25991;&#26723;&#25688;&#35201;&#30340;&#23618;&#27425;&#32534;&#30721;-&#35299;&#30721;&#26041;&#26696;&#65292;&#22312;&#22810;&#39046;&#22495;&#30340;10&#20010;MDS&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#22312;&#25277;&#35937;&#21333;&#25991;&#26723;&#25688;&#35201;&#65288;SDS&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#22909;&#22788;&#21487;&#33021;&#19981;&#20250;&#36731;&#26131;&#25193;&#23637;&#21040;&#22810;&#25991;&#26723;&#25688;&#35201;&#65288;MDS&#65289;&#65292;&#22240;&#20026;&#25991;&#26723;&#20043;&#38388;&#30340;&#20132;&#20114;&#26356;&#21152;&#22797;&#26434;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#35774;&#35745;&#26032;&#30340;&#26550;&#26500;&#25110;&#26032;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#29992;&#20110;MDS&#65292;&#35201;&#20040;&#23558;PLM&#24212;&#29992;&#20110;MDS&#65292;&#20294;&#26410;&#32771;&#34385;&#21040;&#22797;&#26434;&#30340;&#25991;&#26723;&#20132;&#20114;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#19978;&#24378;&#21046;&#20351;&#29992;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#23547;&#27714;&#26356;&#22909;&#22320;&#21033;&#29992;PLM&#20419;&#36827;MDS&#20219;&#21153;&#30340;&#22810;&#25991;&#26723;&#20132;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#22312;10&#20010;MDS&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#25105;&#20204;&#30340;&#35774;&#35745;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#35206;&#30422;&#21508;&#31181;&#39046;&#22495;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#37117;&#33021;&#22815;&#23454;&#29616;&#25345;&#32493;&#25913;&#36827;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;MDS&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) have accomplished impressive achievements in abstractive single-document summarization (SDS). However, such benefits may not be readily extended to muti-document summarization (MDS), where the interactions among documents are more complex. Previous works either design new architectures or new pre-training objectives for MDS, or apply PLMs to MDS without considering the complex document interactions. While the former does not make full use of previous pre-training efforts and may not generalize well across multiple domains, the latter cannot fully attend to the intricate relationships unique to MDS tasks. In this paper, we enforce hierarchy on both the encoder and decoder and seek to make better use of a PLM to facilitate multi-document interactions for the MDS task. We test our design on 10 MDS datasets across a wide range of domains. Extensive experiments show that our proposed method can achieve consistent improvements on all these datasets, outperf
&lt;/p&gt;</description></item><item><title>Text2Cohort&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#65292;&#20943;&#23569;&#30740;&#31350;&#20154;&#21592;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23454;&#29616;&#20102;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#30340;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.07637</link><description>&lt;p&gt;
Text2Cohort: &#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#23545;&#30284;&#30151;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
Text2Cohort: Democratizing the NCI Imaging Data Commons with Natural Language Cohort Discovery. (arXiv:2305.07637v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07637
&lt;/p&gt;
&lt;p&gt;
Text2Cohort&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#65292;&#20943;&#23569;&#30740;&#31350;&#20154;&#21592;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23454;&#29616;&#20102;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#30340;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;(IDC)&#26159;&#19968;&#20010;&#22522;&#20110;&#20113;&#30340;&#25968;&#25454;&#24211;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#24320;&#25918;&#33719;&#21462;&#30340;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#21644;&#20998;&#26512;&#24037;&#20855;&#65292;&#26088;&#22312;&#20419;&#36827;&#21307;&#23398;&#25104;&#20687;&#30740;&#31350;&#20013;&#30340;&#21327;&#20316;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#25216;&#26415;&#24615;&#36136;&#65292;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#20197;&#36827;&#34892;&#38431;&#21015;&#21457;&#29616;&#21644;&#35775;&#38382;&#25104;&#20687;&#25968;&#25454;&#23545;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#26174;&#33879;&#30340;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;Text2Cohort&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#24182;&#23558;&#26597;&#35810;&#30340;&#21709;&#24212;&#36820;&#22238;&#32473;&#29992;&#25143;&#65292;&#20197;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26657;&#27491;&#20197;&#35299;&#20915;&#26597;&#35810;&#20013;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#38169;&#35823;&#65292;&#36890;&#36807;&#23558;&#38169;&#35823;&#20256;&#22238;&#27169;&#22411;&#36827;&#34892;&#35299;&#37322;&#21644;&#26657;&#27491;&#12290;&#25105;&#20204;&#23545;50&#20010;&#33258;&#28982;&#35821;&#35328;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#20102;Text2Cohort&#35780;&#20272;&#65292;&#33539;&#22260;&#20174;&#20449;&#24687;&#25552;&#21462;&#21040;&#38431;&#21015;&#21457;&#29616;&#12290;&#32467;&#26524;&#26597;&#35810;&#21644;&#36755;&#20986;&#30001;&#20004;&#20301;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#36827;&#34892;&#20102;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Imaging Data Commons (IDC) is a cloud-based database that provides researchers with open access to cancer imaging data and tools for analysis, with the goal of facilitating collaboration in medical imaging research. However, querying the IDC database for cohort discovery and access to imaging data has a significant learning curve for researchers due to its complex and technical nature. We developed Text2Cohort, a large language model (LLM) based toolkit to facilitate natural language cohort discovery by translating user input into IDC database queries through prompt engineering and returning the query's response to the user. Furthermore, autocorrection is implemented to resolve syntax and semantic errors in queries by passing the errors back to the model for interpretation and correction. We evaluate Text2Cohort on 50 natural language user inputs ranging from information extraction to cohort discovery. The resulting queries and outputs were verified by two computer scientists to me
&lt;/p&gt;</description></item><item><title>QURG&#26159;&#19968;&#31181;&#24110;&#21161;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#27169;&#22411;&#23454;&#29616;&#19978;&#19979;&#25991;&#29702;&#35299;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#33021;&#22312;SParC&#21644;CoSQL&#31561;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38590;&#20197;&#22788;&#29702;&#21644;&#38271;&#36718;&#27425;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06655</link><description>&lt;p&gt;
QURG: &#38382;&#39064;&#37325;&#20889;&#24341;&#23548;&#19979;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
QURG: Question Rewriting Guided Context-Dependent Text-to-SQL Semantic Parsing. (arXiv:2305.06655v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06655
&lt;/p&gt;
&lt;p&gt;
QURG&#26159;&#19968;&#31181;&#24110;&#21161;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#27169;&#22411;&#23454;&#29616;&#19978;&#19979;&#25991;&#29702;&#35299;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#33021;&#22312;SParC&#21644;CoSQL&#31561;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38590;&#20197;&#22788;&#29702;&#21644;&#38271;&#36718;&#27425;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#25991;&#26412;&#21040;SQL&#30340;&#30446;&#26631;&#26159;&#23558;&#22810;&#36718;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#32763;&#35793;&#25104;SQL&#26597;&#35810;&#35821;&#21477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QURG&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#38382;&#39064;&#37325;&#20889;&#24341;&#23548;&#26041;&#27861;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#23454;&#29616;&#36275;&#22815;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#39318;&#20808;&#35757;&#32451;&#19968;&#20010;&#38382;&#39064;&#37325;&#20889;&#27169;&#22411;&#65292;&#22312;&#38382;&#39064;&#19978;&#19979;&#25991;&#30340;&#22522;&#30784;&#19978;&#23436;&#25104;&#24403;&#21069;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#37325;&#20889;&#32534;&#36753;&#30697;&#38453;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#27969;&#30697;&#38453;&#32534;&#30721;&#22120;&#65292;&#26469;&#20849;&#21516;&#24314;&#27169;&#38382;&#39064;&#21644;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#37325;&#20889;&#20851;&#31995;&#65292;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#21644;&#32467;&#26500;&#21270;&#27169;&#24335;&#20043;&#38388;&#30340;&#27169;&#24335;&#38142;&#25509;&#20851;&#31995;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;QURG&#26174;&#33879;&#25552;&#39640;&#20102;&#20004;&#20010;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#25968;&#25454;&#38598;SParC&#21644;CoSQL&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38590;&#20197;&#22788;&#29702;&#21644;&#38271;&#36718;&#27425;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context-dependent Text-to-SQL aims to translate multi-turn natural language questions into SQL queries. Despite various methods have exploited context-dependence information implicitly for contextual SQL parsing, there are few attempts to explicitly address the dependencies between current question and question context. This paper presents QURG, a novel Question Rewriting Guided approach to help the models achieve adequate contextual understanding. Specifically, we first train a question rewriting model to complete the current question based on question context, and convert them into a rewriting edit matrix. We further design a two-stream matrix encoder to jointly model the rewriting relations between question and context, and the schema linking relations between natural language and structured schema. Experimental results show that QURG significantly improves the performances on two large-scale context-dependent datasets SParC and CoSQL, especially for hard and long-turn questions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLAIR&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#21644;&#22238;&#31572;&#26469;&#26816;&#27979;ChatGPT&#20013;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30495;&#23454;&#24615;&#65292;&#21487;&#20197;&#20998;&#31867;&#20154;&#21644;&#26426;&#22120;&#20154;&#12290;&#21333;&#38382;&#39064;&#20998;&#20026;&#23545;&#20110;&#20154;&#31867;&#32780;&#35328;&#23481;&#26131;&#20294;&#23545;&#20110;&#26426;&#22120;&#20154;&#24456;&#38590;&#21644;&#23545;&#20110;&#26426;&#22120;&#20154;&#32780;&#35328;&#23481;&#26131;&#20294;&#23545;&#20110;&#20154;&#31867;&#24456;&#38590;&#20004;&#20010;&#31867;&#21035;&#65292;&#20998;&#21035;&#36827;&#34892;&#26816;&#27979;&#12290; &#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06424</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#36824;&#26159;&#20154;&#31867;&#65311;&#29992;&#19968;&#20010;&#38382;&#39064;&#26816;&#27979;ChatGPT&#20882;&#21517;&#39030;&#26367;&#32773;
&lt;/p&gt;
&lt;p&gt;
Bot or Human? Detecting ChatGPT Imposters with A Single Question. (arXiv:2305.06424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLAIR&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19968;&#20010;&#38382;&#39064;&#21644;&#22238;&#31572;&#26469;&#26816;&#27979;ChatGPT&#20013;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30495;&#23454;&#24615;&#65292;&#21487;&#20197;&#20998;&#31867;&#20154;&#21644;&#26426;&#22120;&#20154;&#12290;&#21333;&#38382;&#39064;&#20998;&#20026;&#23545;&#20110;&#20154;&#31867;&#32780;&#35328;&#23481;&#26131;&#20294;&#23545;&#20110;&#26426;&#22120;&#20154;&#24456;&#38590;&#21644;&#23545;&#20110;&#26426;&#22120;&#20154;&#32780;&#35328;&#23481;&#26131;&#20294;&#23545;&#20110;&#20154;&#31867;&#24456;&#38590;&#20004;&#20010;&#31867;&#21035;&#65292;&#20998;&#21035;&#36827;&#34892;&#26816;&#27979;&#12290; &#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#26368;&#36817;&#23637;&#31034;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20351;&#24471;&#32763;&#35793;&#12289;&#20889;&#20316;&#21644;&#38386;&#32842;&#31561;&#21508;&#31181;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#25285;&#24515;&#23427;&#20204;&#21487;&#33021;&#34987;&#28389;&#29992;&#20110;&#27450;&#35784;&#25110;&#25298;&#32477;&#26381;&#21153;&#25915;&#20987;&#31561;&#24694;&#24847;&#29992;&#36884;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#26816;&#27979;&#32842;&#22825;&#20013;&#28041;&#21450;&#30340;&#21478;&#19968;&#26041;&#26159;&#26426;&#22120;&#20154;&#36824;&#26159;&#20154;&#31867;&#30340;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLAIR&#30340;&#26694;&#26550;&#65292;&#21363;&#36890;&#36807;&#21333;&#20010;&#38382;&#39064;&#21644;&#22238;&#31572;&#26469;&#26597;&#25214;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#65292;&#20197;&#22312;&#32447;&#26041;&#24335;&#26816;&#27979;&#20250;&#35805;&#20013;&#30340;&#23545;&#35805;&#26426;&#22120;&#20154;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#38024;&#23545;&#19968;&#20010;&#21333;&#19968;&#38382;&#39064;&#22330;&#26223;&#65292;&#35813;&#22330;&#26223;&#21487;&#20197;&#26377;&#25928;&#22320;&#21306;&#20998;&#20154;&#31867;&#29992;&#25143;&#21644;&#26426;&#22120;&#20154;&#12290;&#36825;&#20123;&#38382;&#39064;&#20998;&#20026;&#20004;&#31867;&#65306;&#23545;&#20110;&#20154;&#31867;&#32780;&#35328;&#23481;&#26131;&#20294;&#23545;&#20110;&#26426;&#22120;&#20154;&#24456;&#38590;&#65288;&#20363;&#22914;&#35745;&#25968;&#12289;&#26367;&#25442;&#12289;&#23450;&#20301;&#12289;&#22122;&#38899;&#36807;&#28388;&#21644;ASCII&#33402;&#26415;&#65289;&#65292;&#20197;&#21450;&#23545;&#20110;&#26426;&#22120;&#20154;&#32780;&#35328;&#23481;&#26131;&#20294;&#23545;&#20110;&#20154;&#31867;&#24456;&#38590;&#65288;&#20363;&#22914;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#35782;&#21035;&#65289;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;FLAIR&#65292;&#24182;&#22312;&#26426;&#22120;&#20154;&#26816;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models like ChatGPT have recently demonstrated impressive capabilities in natural language understanding and generation, enabling various applications including translation, essay writing, and chit-chatting. However, there is a concern that they can be misused for malicious purposes, such as fraud or denial-of-service attacks. Therefore, it is crucial to develop methods for detecting whether the party involved in a conversation is a bot or a human. In this paper, we propose a framework named FLAIR, Finding Large language model Authenticity via a single Inquiry and Response, to detect conversational bots in an online manner. Specifically, we target a single question scenario that can effectively differentiate human users from bots. The questions are divided into two categories: those that are easy for humans but difficult for bots (e.g., counting, substitution, positioning, noise filtering, and ASCII art), and those that are easy for bots but difficult for humans (e.g., m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38889;&#35821;&#36890;&#29992;&#35789;&#24418;&#23398;&#25968;&#25454;&#38598;&#65292;&#20445;&#30041;&#38889;&#35821;&#29305;&#33394;&#24182;&#37319;&#29992;Sylak-Glassman&#31561;&#20154;&#30340;&#35789;&#24418;&#29305;&#24449;&#27169;&#24335;&#65292;&#20026;&#38889;&#35821;&#24418;&#24577;&#23398;&#33539;&#24335;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2305.06335</link><description>&lt;p&gt;
K-UniMorph&#65306;&#38889;&#35821;&#36890;&#29992;&#35789;&#24418;&#23398;&#21450;&#20854;&#29305;&#24449;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
K-UniMorph: Korean Universal Morphology and its Feature Schema. (arXiv:2305.06335v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38889;&#35821;&#36890;&#29992;&#35789;&#24418;&#23398;&#25968;&#25454;&#38598;&#65292;&#20445;&#30041;&#38889;&#35821;&#29305;&#33394;&#24182;&#37319;&#29992;Sylak-Glassman&#31561;&#20154;&#30340;&#35789;&#24418;&#29305;&#24449;&#27169;&#24335;&#65292;&#20026;&#38889;&#35821;&#24418;&#24577;&#23398;&#33539;&#24335;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38889;&#35821;&#36890;&#29992;&#35789;&#24418;&#23398;&#25968;&#25454;&#38598;&#65292;&#20043;&#21069;&#65292;&#38889;&#35821;&#22312;&#25968;&#30334;&#31181;&#22810;&#26679;&#30340;&#19990;&#30028;&#35821;&#35328;&#20013;&#30340;&#24418;&#24577;&#23398;&#33539;&#24335;&#39046;&#22495;&#20013;&#19968;&#30452;&#22788;&#20110;&#23569;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#31181;&#20445;&#30041;&#38889;&#35821;&#29305;&#33394;&#30340;&#36890;&#29992;&#35789;&#24418;&#23398;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;K-UniMorph&#25968;&#25454;&#38598;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#27010;&#36848;&#20102;&#27599;&#20010;&#35821;&#27861;&#26631;&#20934;&#30340;&#21160;&#35789;&#32467;&#23614;&#65292;&#24182;&#38416;&#26126;&#22914;&#20309;&#25552;&#21462;&#21464;&#24418;&#24418;&#24335;&#20197;&#21450;&#22914;&#20309;&#29983;&#25104;&#35789;&#24418;&#27169;&#24335;&#12290;&#27492;&#25968;&#25454;&#38598;&#37319;&#29992;Sylak-Glassman&#31561;&#20154;&#65288;2015&#65289;&#21644;Sylak-Glassman&#65288;2016&#65289;&#30340;&#35789;&#24418;&#29305;&#24449;&#27169;&#24335;&#65292;&#32780;&#25105;&#20204;&#20174;Sejong&#24418;&#24577;&#20998;&#26512;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#21464;&#24418;&#24418;&#24335;&#65292;&#36825;&#26159;&#38889;&#35821;&#26368;&#22823;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#20043;&#19968;&#12290;&#22312;&#25968;&#25454;&#21019;&#24314;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#35843;&#26597;&#20174;Sejong&#35821;&#26009;&#24211;&#20013;&#30340;&#36716;&#25442;&#30340;&#27491;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#21464;&#24418;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present in this work a new Universal Morphology dataset for Korean. Previously, the Korean language has been underrepresented in the field of morphological paradigms amongst hundreds of diverse world languages. Hence, we propose this Universal Morphological paradigms for the Korean language that preserve its distinct characteristics. For our K-UniMorph dataset, we outline each grammatical criterion in detail for the verbal endings, clarify how to extract inflected forms, and demonstrate how we generate the morphological schemata. This dataset adopts morphological feature schema from Sylak-Glassman et al. (2015) and Sylak-Glassman (2016) for the Korean language as we extract inflected verb forms from the Sejong morphologically analyzed corpus that is one of the largest annotated corpora for Korean. During the data creation, our methodology also includes investigating the correctness of the conversion from the Sejong corpus. Furthermore, we carry out the inflection task using three di
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#28436;&#31034;&#26816;&#32034;&#22120;UDR&#65292;&#21487;&#29992;&#20110;&#24191;&#27867;&#30340;&#20219;&#21153;&#26816;&#32034;&#28436;&#31034;&#65292;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#26469;&#23558;&#21508;&#31181;&#20219;&#21153;&#30340;&#35757;&#32451;&#20449;&#21495;&#36716;&#25442;&#20026;&#32479;&#19968;&#30340;&#21015;&#34920;&#25490;&#24207;&#20844;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.04320</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#28436;&#31034;&#26816;&#32034;&#22120;
&lt;/p&gt;
&lt;p&gt;
Unified Demonstration Retriever for In-Context Learning. (arXiv:2305.04320v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#28436;&#31034;&#26816;&#32034;&#22120;UDR&#65292;&#21487;&#29992;&#20110;&#24191;&#27867;&#30340;&#20219;&#21153;&#26816;&#32034;&#28436;&#31034;&#65292;&#22312;&#35757;&#32451;&#26102;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#26469;&#23558;&#21508;&#31181;&#20219;&#21153;&#30340;&#35757;&#32451;&#20449;&#21495;&#36716;&#25442;&#20026;&#32479;&#19968;&#30340;&#21015;&#34920;&#25490;&#24207;&#20844;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#37327;&#36755;&#20837;-&#36755;&#20986;&#23545;&#65288;&#28436;&#31034;&#65289;&#21644;&#27979;&#35797;&#36755;&#20837;&#30340;&#26465;&#20214;&#19979;&#65292;&#30452;&#25509;&#36755;&#20986;&#39044;&#27979;&#32467;&#26524;&#12290;&#30740;&#31350;&#34920;&#26126;&#23427;&#39640;&#24230;&#20381;&#36182;&#20110;&#25552;&#20379;&#30340;&#28436;&#31034;&#65292;&#24182;&#20419;&#36827;&#20102;&#28436;&#31034;&#26816;&#32034;&#30340;&#30740;&#31350;&#65306;&#26681;&#25454;&#27979;&#35797;&#36755;&#20837;&#20174;&#35757;&#32451;&#38598;&#20013;&#26816;&#32034;&#30456;&#20851;&#31034;&#20363;&#65292;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#28436;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#28436;&#31034;&#26816;&#32034;&#22120;&#65288;UDR&#65289;&#65292;&#29992;&#20110;&#20026;&#24191;&#27867;&#30340;&#20219;&#21153;&#26816;&#32034;&#28436;&#31034;&#65307;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#23558;&#21508;&#31181;&#20219;&#21153;&#30340;&#35757;&#32451;&#20449;&#21495;&#36716;&#25442;&#20026;&#32479;&#19968;&#30340;&#21015;&#34920;&#25490;&#24207;&#20844;&#24335;&#26469;&#35757;&#32451;UDR&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning is a new learning paradigm where a language model conditions on a few input-output pairs (demonstrations) and a test input, and directly outputs the prediction. It has been shown highly dependent on the provided demonstrations and thus promotes the research of demonstration retrieval: given a test input, relevant examples are retrieved from the training set to serve as informative demonstrations for in-context learning. While previous works focus on training task-specific retrievers for several tasks separately, these methods are often hard to transfer and scale on various tasks, and separately trained retrievers incur a lot of parameter storage and deployment cost. In this paper, we propose Unified Demonstration Retriever (\textbf{UDR}), a single model to retrieve demonstrations for a wide range of tasks. To train UDR, we cast various tasks' training signals into a unified list-wise ranking formulation by language model's feedback. Then we propose a multi-task list
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#23485;&#26494;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#20026;&#38382;&#31572;&#31995;&#32479;&#32508;&#21512;&#20869;&#22806;&#20998;&#24067;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#26174;&#31034;&#20102;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03971</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23485;&#26494;&#20248;&#21270;&#29992;&#20110;&#24378;&#38887;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Adaptive loose optimization for robust question answering. (arXiv:2305.03971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#23485;&#26494;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#20026;&#38382;&#31572;&#31995;&#32479;&#32508;&#21512;&#20869;&#22806;&#20998;&#24067;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#24182;&#26174;&#31034;&#20102;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#26041;&#27861;&#20197;&#21033;&#29992;&#25968;&#25454;&#20559;&#24046;&#20026;&#29305;&#28857;&#65292;&#22914;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#35821;&#35328;&#20808;&#39564;&#21644;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;&#25277;&#21462;&#24335;&#38382;&#31572;&#65289;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#30446;&#21069;&#30340;&#21435;&#20559;&#26041;&#27861;&#24448;&#24448;&#20197;&#22312;&#20998;&#24067;&#20869;&#34920;&#29616;&#19981;&#20339;&#20026;&#20195;&#20215;&#33719;&#24471;&#26377;&#21033;&#30340;&#20998;&#24067;&#22806;&#27867;&#21270;&#33021;&#21147;&#65292;&#32780;&#19981;&#21435;&#20559;&#26041;&#27861;&#21017;&#22312;&#33719;&#24471;&#39640;&#20998;&#24067;&#20869;&#34920;&#29616;&#30340;&#21516;&#26102;&#29306;&#29298;&#20102;&#30456;&#24403;&#25968;&#37327;&#30340;&#20998;&#24067;&#22806;&#34920;&#29616;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#38590;&#20197;&#24212;&#23545;&#22797;&#26434;&#21464;&#21270;&#30340;&#29616;&#23454;&#19990;&#30028;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26032;&#22411;&#33258;&#36866;&#24212;&#23485;&#26494;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#20026;&#38382;&#31572;&#31995;&#32479;&#32508;&#21512;&#20004;&#32773;&#26368;&#20339;&#34920;&#29616;&#32780;&#21162;&#21147;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#26159;&#26681;&#25454;&#23567;&#25209;&#37327;&#35757;&#32451;&#25968;&#25454;&#19978;&#20808;&#21069;&#21644;&#24403;&#21069;&#20248;&#21270;&#29366;&#24577;&#20043;&#38388;&#30340;&#27604;&#29575;&#33258;&#36866;&#24212;&#22320;&#20943;&#23569;&#25439;&#22833;&#12290;&#36825;&#31181;&#23485;&#26494;&#20248;&#21270;&#21487;&#20197;&#29992;&#26469;&#38450;&#27490;&#38750;&#20984;&#20248;&#21270;&#38519;&#20837;&#23616;&#37096;&#26368;&#23567;&#20540;&#65292;&#24182;&#24110;&#21161;&#27169;&#22411;&#23398;&#20064;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#65292;&#21516;&#26102;&#34920;&#29616;&#20986;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering methods are well-known for leveraging data bias, such as the language prior in visual question answering and the position bias in machine reading comprehension (extractive question answering). Current debiasing methods often come at the cost of significant in-distribution performance to achieve favorable out-of-distribution generalizability, while non-debiasing methods sacrifice a considerable amount of out-of-distribution performance in order to obtain high in-distribution performance. Therefore, it is challenging for them to deal with the complicated changing real-world situations. In this paper, we propose a simple yet effective novel loss function with adaptive loose optimization, which seeks to make the best of both worlds for question answering. Our main technical contribution is to reduce the loss adaptively according to the ratio between the previous and current optimization state on mini-batch training data. This loose optimization can be used to prevent non
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#35843;&#26597;&#20102;&#20855;&#36523;&#21270;&#31574;&#30053;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#27604;&#21947;&#24615;&#35821;&#35328;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#34892;&#20026;&#26356;&#20855;&#20307;&#21270;&#30340;&#38544;&#21947;&#24615;&#21477;&#23376;&#26102;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.03445</link><description>&lt;p&gt;
LMs&#22266;&#23432;&#38453;&#22320;&#65306;&#25506;&#31350;&#20855;&#36523;&#21270;&#23545;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#27604;&#21947;&#24615;&#35821;&#35328;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
LMs stand their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models. (arXiv:2305.03445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#35843;&#26597;&#20102;&#20855;&#36523;&#21270;&#31574;&#30053;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#27604;&#21947;&#24615;&#35821;&#35328;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#34892;&#20026;&#26356;&#20855;&#20307;&#21270;&#30340;&#38544;&#21947;&#24615;&#21477;&#23376;&#26102;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#21947;&#35821;&#35328;&#26159;&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#35299;&#37322;&#22522;&#20110;&#21333;&#35789;&#30340;&#20351;&#29992;&#26041;&#24335;&#20559;&#31163;&#20102;&#23427;&#20204;&#30340;&#24120;&#35268;&#39034;&#24207;&#21644;&#21547;&#20041;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#21487;&#20197;&#36731;&#26494;&#29702;&#35299;&#21644;&#35808;&#37322;&#38544;&#21947;&#12289;&#27604;&#21947;&#25110;&#20064;&#35821;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20174;&#20855;&#36523;&#38544;&#21947;&#20013;&#25512;&#23548;&#20986;&#26469;&#12290;&#35821;&#35328;&#26159;&#20855;&#36523;&#21270;&#30340;&#20195;&#29702;&#65292;&#22914;&#26524;&#38544;&#21947;&#26159;&#20256;&#32479;&#30340;&#21644;&#35789;&#27719;&#21270;&#30340;&#65292;&#37027;&#20040;&#19968;&#20010;&#27809;&#26377;&#36523;&#20307;&#30340;&#31995;&#32479;&#23601;&#26356;&#23481;&#26131;&#29702;&#35299;&#20855;&#36523;&#27010;&#24565;&#12290;&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27604;&#21947;&#24615;&#21477;&#23376;&#30340;&#34892;&#21160;&#26356;&#20855;&#20307;&#21270;&#26102;&#65292;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#37322;&#38544;&#21947;&#21477;&#23376;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#25490;&#38500;&#20102;&#19982;&#20854;&#20182;&#29305;&#24449;&#65288;&#20363;&#22914;&#21333;&#35789;&#38271;&#24230;&#25110;&#20855;&#20307;&#24615;&#65289;&#30340;&#22810;&#37325;&#20849;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Figurative language is a challenge for language models since its interpretation is based on the use of words in a way that deviates from their conventional order and meaning. Yet, humans can easily understand and interpret metaphors, similes or idioms as they can be derived from embodied metaphors. Language is a proxy for embodiment and if a metaphor is conventional and lexicalised, it becomes easier for a system without a body to make sense of embodied concepts. Yet, the intricate relation between embodiment and features such as concreteness or age of acquisition has not been studied in the context of figurative language interpretation concerning language models. Hence, the presented study shows how larger language models perform better at interpreting metaphoric sentences when the action of the metaphorical sentence is more embodied. The analysis rules out multicollinearity with other features (e.g. word length or concreteness) and provides initial evidence that larger language model
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#21487;&#33258;&#21160;&#21270;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#36798;&#21040;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.02783</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25216;&#26415;&#20219;&#21153;&#20013;&#33258;&#21160;&#29983;&#25104;YAML&#20195;&#30721;
&lt;/p&gt;
&lt;p&gt;
Automated Code generation for Information Technology Tasks in YAML through Large Language Models. (arXiv:2305.02783v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02783
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#21487;&#33258;&#21160;&#21270;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25216;&#26415;&#36798;&#21040;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#19981;&#26029;&#25552;&#21319;&#65292;&#22312;&#36890;&#29992;&#32534;&#31243;&#35821;&#35328;&#26041;&#38754;&#30340;&#21463;&#30410;&#26368;&#22823;&#65292;&#32780;&#38024;&#23545;IT&#33258;&#21160;&#21270;&#31561;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#30340;&#30740;&#31350;&#36739;&#23569;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;Ansible-YAML&#30340;&#29983;&#25104;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ansible Wisdom&#30340;&#33258;&#28982;&#35821;&#35328;&#36716;Ansible-YAML&#20195;&#30721;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#25552;&#39640;IT&#33258;&#21160;&#21270;&#29983;&#20135;&#21147;&#12290;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;&#21253;&#21547;Ansible-YAML&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25193;&#23637;&#35757;&#32451;&#12290;&#21516;&#26102;&#65292;&#36824;&#24320;&#21457;&#20102;&#20004;&#20010;&#29992;&#20110;&#25429;&#25417;&#27492;&#39046;&#22495;&#29305;&#24449;&#30340;YAML&#21644;Ansible&#24615;&#33021;&#25351;&#26631;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Ansible Wisdom&#21487;&#20197;&#31934;&#30830;&#22320;&#20174;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#20013;&#29983;&#25104;Ansible&#33050;&#26412;&#65292;&#24182;&#19988;&#20854;&#24615;&#33021;&#21487;&#19982;&#29616;&#26377;&#25216;&#26415;&#30340;&#29366;&#24577;&#30456;&#23218;&#32654;&#25110;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages. Domain specific languages, such as the ones used for IT Automation, have received far less attention, despite involving many active developers and being an essential component of modern cloud platforms. This work focuses on the generation of Ansible-YAML, a widely used markup language for IT Automation. We present Ansible Wisdom, a natural-language to Ansible-YAML code generation tool, aimed at improving IT automation productivity. Ansible Wisdom is a transformer-based model, extended by training with a new dataset containing Ansible-YAML. We also develop two novel performance metrics for YAML and Ansible to capture the specific characteristics of this domain. Results show that Ansible Wisdom can accurately generate Ansible script from natural language prompts with performance comparable or better than existing state of the art code 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00586</link><description>&lt;p&gt;
GPT-2&#26159;&#22914;&#20309;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#30340;&#65311;&#35299;&#37322;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#34987;&#26126;&#30830;&#35757;&#32451;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#21151;&#33021;&#21364;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#36890;&#36807;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#30340;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20197;GPT-2 Small&#20026;&#20363;&#65292;&#30740;&#31350;&#20854;&#33021;&#21542;&#36890;&#36807;&#36755;&#20837;"&#25112;&#20105;&#25345;&#32493;&#26102;&#38388;&#26159;&#20174;1732&#24180;&#21040;17&#24180;"&#65292;&#39044;&#27979;&#20986;&#26377;&#25928;&#30340;&#20004;&#20301;&#25968;&#23383;&#30340;&#25130;&#27490;&#24180;&#20221; (&#22823;&#20110;32&#24180;)&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19968;&#20010;&#30005;&#36335;&#65292;&#21363;GPT-2 Small&#35745;&#31639;&#22270;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#29992;&#20110;&#35745;&#31639;&#36825;&#20010;&#20219;&#21153;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#25105;&#20204;&#35299;&#37322;&#20102;&#27599;&#20010;&#30005;&#36335;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#26174;&#31034;&#20986;GPT-2 Small&#30340;&#26368;&#32456;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30005;&#36335;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#22312;&#20854;&#20182;&#22823;&#20110;&#22330;&#26223;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years &gt; 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
&lt;/p&gt;</description></item><item><title>GeneGPT&#36890;&#36807;&#23569;&#37327;NCBI API&#35843;&#29992;URL&#35831;&#27714;&#20316;&#20026;&#28436;&#31034;&#65292;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;NCBI Web API&#22238;&#31572;&#22522;&#22240;&#32452;&#38382;&#39064;&#65292;&#24182;&#22312;GeneTuring&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09667</link><description>&lt;p&gt;
GeneGPT: &#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;NCBI Web API
&lt;/p&gt;
&lt;p&gt;
GeneGPT: Teaching Large Language Models to Use NCBI Web APIs. (arXiv:2304.09667v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09667
&lt;/p&gt;
&lt;p&gt;
GeneGPT&#36890;&#36807;&#23569;&#37327;NCBI API&#35843;&#29992;URL&#35831;&#27714;&#20316;&#20026;&#28436;&#31034;&#65292;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;NCBI Web API&#22238;&#31572;&#22522;&#22240;&#32452;&#38382;&#39064;&#65292;&#24182;&#22312;GeneTuring&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GeneGPT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20351;&#29992;&#22269;&#23478;&#29983;&#29289;&#25216;&#26415;&#20449;&#24687;&#20013;&#24515;&#65288;NCBI&#65289;&#30340;Web&#24212;&#29992;&#31243;&#24207;&#32534;&#31243;&#25509;&#21475;&#65288;API&#65289;&#65292;&#24182;&#22238;&#31572;&#22522;&#22240;&#32452;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#23569;&#37327;&#30340;NCBI API&#35843;&#29992;URL&#35831;&#27714;&#20316;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#28436;&#31034;&#65292;&#21551;&#21457;Codex&#65288;code-davinci-002&#65289;&#35299;&#20915;GeneTuring&#27979;&#35797;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#19968;&#26086;&#26816;&#27979;&#21040;&#35843;&#29992;&#35831;&#27714;&#65292;&#25105;&#20204;&#23601;&#20572;&#27490;&#35299;&#30721;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;URL&#36827;&#34892;API&#35843;&#29992;&#12290;&#25105;&#20204;&#28982;&#21518;&#23558;NCBI API&#36820;&#22238;&#30340;&#21407;&#22987;&#25191;&#34892;&#32467;&#26524;&#38468;&#21152;&#21040;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#24182;&#32487;&#32493;&#29983;&#25104;&#30452;&#21040;&#25214;&#21040;&#31572;&#26696;&#25110;&#26816;&#27979;&#21040;&#21478;&#19968;&#20010;API&#35843;&#29992;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;GeneGPT&#22312;GeneTuring&#25968;&#25454;&#38598;&#30340;&#22235;&#20010;One-shot&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19977;&#20010;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#22312;&#20116;&#20010;Zero-shot&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;GeneGPT&#30340;&#23439;&#24179;&#22343;&#20998;&#25968;&#20026;0.76&#65292;&#36828;&#39640;&#20110;&#26816;&#32034;&#22686;&#24378;LLM&#65292;&#22914;New Bin&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present GeneGPT, a novel method for teaching large language models (LLMs) to use the Web Application Programming Interfaces (APIs) of the National Center for Biotechnology Information (NCBI) and answer genomics questions. Specifically, we prompt Codex (code-davinci-002) to solve the GeneTuring tests with few-shot URL requests of NCBI API calls as demonstrations for in-context learning. During inference, we stop the decoding once a call request is detected and make the API call with the generated URL. We then append the raw execution results returned by NCBI APIs to the generated texts and continue the generation until the answer is found or another API call is detected. Our preliminary results show that GeneGPT achieves state-of-the-art results on three out of four one-shot tasks and four out of five zero-shot tasks in the GeneTuring dataset. Overall, GeneGPT achieves a macro-average score of 0.76, which is much higher than retrieval-augmented LLMs such as the New Bin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#21644;&#24773;&#24863;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#21644;&#24773;&#24863;&#20449;&#24687;&#23545;&#20854;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#21152;&#20837;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#23545;&#26576;&#20123;&#20219;&#21153;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2304.03347</link><description>&lt;p&gt;
&#35770;ChatGPT&#21644;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
On the Evaluations of ChatGPT and Emotion-enhanced Prompting for Mental Health Analysis. (arXiv:2304.03347v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#21644;&#24773;&#24863;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#20197;&#21450;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#21644;&#24773;&#24863;&#20449;&#24687;&#23545;&#20854;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#21152;&#20837;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#23545;&#26576;&#20123;&#20219;&#21153;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#26174;&#31034;&#20986;&#25552;&#39640;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#25928;&#29575;&#21644;&#21487;&#35775;&#38382;&#24615;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#32780;&#26368;&#36817;&#30340;&#20027;&#27969;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(PLMs)&#20316;&#20026;&#39592;&#24178;&#65292;&#24182;&#34701;&#20837;&#24773;&#24863;&#20449;&#24687;&#12290;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;ChatGPT&#38646;-shot&#24615;&#33021;&#30740;&#31350;&#22312;&#19981;&#20805;&#20998;&#30340;&#35780;&#20272;&#12289;&#24773;&#24863;&#20449;&#24687;&#21033;&#29992;&#21644;&#26041;&#27861;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;ChatGPT&#22312;11&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#21644;&#24773;&#24863;&#25512;&#29702;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;5&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#20108;&#36827;&#21046;&#21644;&#22810;&#31867;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#26816;&#27979;&#12289;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#30340;&#21407;&#22240;/&#22240;&#32032;&#26816;&#27979;&#12289;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#21644;&#22240;&#26524;&#24773;&#24863;&#34164;&#21547;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#20998;&#26512;&#20013;&#25506;&#31350;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#20197;&#21450;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#23545;ChatGPT&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#20013;&#26377;&#30528;&#33391;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#21152;&#20837;&#24773;&#24863;&#22686;&#24378;&#25552;&#31034;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated mental health analysis shows great potential for enhancing the efficiency and accessibility of mental health care, whereas the recent dominant methods utilized pre-trained language models (PLMs) as the backbone and incorporated emotional information. The latest large language models (LLMs), such as ChatGPT, exhibit dramatic capabilities on diverse natural language processing tasks. However, existing studies on ChatGPT's zero-shot performance for mental health analysis have limitations in inadequate evaluation, utilization of emotional information, and explainability of methods. In this work, we comprehensively evaluate the mental health analysis and emotional reasoning ability of ChatGPT on 11 datasets across 5 tasks, including binary and multi-class mental health condition detection, cause/factor detection of mental health conditions, emotion recognition in conversations, and causal emotion entailment. We empirically analyze the impact of different prompting strategies with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21704;&#24076;&#38543;&#26426;&#25237;&#24433;&#21644;&#37327;&#21270;&#25216;&#26415;&#26377;&#25928;&#37327;&#21270;&#19978;&#19979;&#25991;&#21270;&#21477;&#23376;&#23884;&#20837;&#65292;&#20197;&#38477;&#20302;&#23384;&#20648;&#31354;&#38388;&#30340;&#24320;&#38144;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#22312;&#22810;&#31181;&#33521;&#35821;&#21644;&#24503;&#35821;&#21477;&#23376;&#20998;&#31867;&#20219;&#21153;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.02481</link><description>&lt;p&gt;
&#37325;&#26032;&#21457;&#29616;&#22522;&#20110;&#21704;&#24076;&#30340;&#38543;&#26426;&#25237;&#24433;&#65292;&#29992;&#20110;&#26377;&#25928;&#37327;&#21270;&#19978;&#19979;&#25991;&#21270;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Rediscovering Hashed Random Projections for Efficient Quantization of Contextualized Sentence Embeddings. (arXiv:2304.02481v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21704;&#24076;&#38543;&#26426;&#25237;&#24433;&#21644;&#37327;&#21270;&#25216;&#26415;&#26377;&#25928;&#37327;&#21270;&#19978;&#19979;&#25991;&#21270;&#21477;&#23376;&#23884;&#20837;&#65292;&#20197;&#38477;&#20302;&#23384;&#20648;&#31354;&#38388;&#30340;&#24320;&#38144;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#22312;&#22810;&#31181;&#33521;&#35821;&#21644;&#24503;&#35821;&#21477;&#23376;&#20998;&#31867;&#20219;&#21153;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35745;&#31639;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#23545;&#36793;&#32536;&#35774;&#22791;&#36827;&#34892;&#35757;&#32451;&#21644;&#25512;&#26029;&#36890;&#24120;&#38656;&#35201;&#39640;&#25928;&#30340;&#35774;&#32622;&#12290;&#23613;&#31649;&#39044;&#20808;&#35745;&#31639;&#25968;&#25454;&#34920;&#31034;&#24182;&#22312;&#26381;&#21153;&#22120;&#19978;&#32531;&#23384;&#21487;&#20197;&#20943;&#23569;&#36793;&#32536;&#35774;&#22791;&#30340;&#35745;&#31639;&#37327;&#65292;&#20294;&#36825;&#20250;&#24102;&#26469;&#20004;&#20010;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#23384;&#20648;&#22312;&#26381;&#21153;&#22120;&#19978;&#25152;&#38656;&#30340;&#23384;&#20648;&#37327;&#38543;&#23454;&#20363;&#25968;&#37327;&#21576;&#32447;&#24615;&#22686;&#38271;&#12290;&#20854;&#27425;&#65292;&#38656;&#35201;&#21457;&#36865;&#22823;&#37327;&#25968;&#25454;&#30340;&#24102;&#23485;&#21040;&#36793;&#32536;&#35774;&#22791;&#12290;&#20026;&#20102;&#20943;&#23569;&#39044;&#20808;&#35745;&#31639;&#30340;&#25968;&#25454;&#34920;&#31034;&#30340;&#23384;&#20648;&#31354;&#38388;&#24320;&#38144;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#36229;&#24179;&#38754;&#25237;&#24433;&#12290;&#20026;&#20102;&#23558;&#23427;&#20204;&#30340;&#22823;&#23567;&#36827;&#19968;&#27493;&#32553;&#23567;&#33267;98.96&#65285;&#65292;&#25105;&#20204;&#23558;&#24471;&#21040;&#30340;&#28014;&#28857;&#34920;&#31034;&#37327;&#21270;&#20026;&#20108;&#36827;&#21046;&#21521;&#37327;&#12290;&#23613;&#31649;&#22823;&#23567;&#22823;&#22823;&#32553;&#23567;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;&#23884;&#20837;&#23545;&#22810;&#31181;&#20445;&#30041;&#20102;94&#65285;-99&#65285;&#28014;&#28857;&#20540;&#30340;&#33521;&#35821;&#21644;&#24503;&#35821;&#21477;&#23376;&#20998;&#31867;&#20219;&#21153;&#35757;&#32451;&#27169;&#22411;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training and inference on edge devices often requires an efficient setup due to computational limitations. While pre-computing data representations and caching them on a server can mitigate extensive edge device computation, this leads to two challenges. First, the amount of storage required on the server that scales linearly with the number of instances. Second, the bandwidth required to send extensively large amounts of data to an edge device. To reduce the memory footprint of pre-computed data representations, we propose a simple, yet effective approach that uses randomly initialized hyperplane projections. To further reduce their size by up to 98.96%, we quantize the resulting floating-point representations into binary vectors. Despite the greatly reduced size, we show that the embeddings remain effective for training models across various English and German sentence classification tasks that retain 94%--99% of their floating-point.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;SemiMemes&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;Memes&#30340;&#20998;&#26512;&#21644;&#27880;&#37322;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#20248;&#20110;&#20854;&#20182;&#26368;&#26032;&#30340;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.00020</link><description>&lt;p&gt;
SemiMemes&#65306;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;Memes&#20998;&#26512;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis. (arXiv:2304.00020v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00020
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;SemiMemes&#65292;&#20027;&#35201;&#24212;&#29992;&#20110;Memes&#30340;&#20998;&#26512;&#21644;&#27880;&#37322;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#24182;&#20248;&#20110;&#20854;&#20182;&#26368;&#26032;&#30340;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;Memes&#30340;&#26222;&#21450;&#24615;&#24341;&#21457;&#20102;&#20998;&#26512;&#20854;&#38544;&#21547;&#21547;&#20041;&#12289;&#23457;&#26597;&#26377;&#23475;&#20869;&#23481;&#30340;&#38656;&#27714;&#12290;&#26426;&#22120;&#23398;&#20064;&#30340;Meme&#23457;&#26597;&#31995;&#32479;&#38656;&#35201;&#21322;&#30417;&#30563;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21033;&#29992;&#20114;&#32852;&#32593;&#19978;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;Memes&#65292;&#24182;&#20351;&#27880;&#37322;&#36807;&#31243;&#21464;&#24471;&#26356;&#31616;&#21333;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#38656;&#35201;&#21033;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#22240;&#20026;Memes&#30340;&#21547;&#20041;&#36890;&#24120;&#26469;&#33258;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#21363;&#22810;&#23186;&#20307;&#33258;&#21160;&#24615;&#21035;&#27495;&#35270;&#35782;&#21035;&#21644;&#20196;&#20154;&#35752;&#21388;&#30340;Memes&#25968;&#25454;&#38598;&#19978;&#65292;&#20248;&#20110;&#20854;&#20182;&#22810;&#27169;&#24577;&#21322;&#30417;&#30563;&#21644;&#30417;&#30563;&#23398;&#20064;&#30340;&#26368;&#26032;&#27169;&#22411;&#12290;&#20511;&#37492;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#25152;&#33719;&#24471;&#30340;&#35265;&#35299;&#65292;&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;SemiMemes&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#33258;&#32534;&#30721;&#22120;&#21644;&#20998;&#31867;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
The prevalence of memes on social media has created the need to sentiment analyze their underlying meanings for censoring harmful content. Meme censoring systems by machine learning raise the need for a semi-supervised learning solution to take advantage of the large number of unlabeled memes available on the internet and make the annotation process less challenging. Moreover, the approach needs to utilize multimodal data as memes' meanings usually come from both images and texts. This research proposes a multimodal semi-supervised learning approach that outperforms other multimodal semi-supervised learning and supervised learning state-of-the-art models on two datasets, the Multimedia Automatic Misogyny Identification and Hateful Memes dataset. Building on the insights gained from Contrastive Language-Image Pre-training, which is an effective multimodal learning technique, this research introduces SemiMemes, a novel training method that combines auto-encoder and classification task to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992; ChatGPT &#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616; ChatGPT &#22312;&#26494;&#24347;&#21305;&#37197; F1 &#20998;&#25968;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110; GPT-3&#12290;&#34429;&#28982;&#20854;&#24615;&#33021;&#20173;&#20302;&#20110; BioClinicalBERT &#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#20102; ChatGPT &#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#26377;&#24456;&#22823;&#30340;&#20020;&#24202; NER &#20219;&#21153;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16416</link><description>&lt;p&gt;
&#21033;&#29992;ChatGPT&#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Clinical Entity Recognition using ChatGPT. (arXiv:2303.16416v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992; ChatGPT &#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616; ChatGPT &#22312;&#26494;&#24347;&#21305;&#37197; F1 &#20998;&#25968;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110; GPT-3&#12290;&#34429;&#28982;&#20854;&#24615;&#33021;&#20173;&#20302;&#20110; BioClinicalBERT &#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#20102; ChatGPT &#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#26377;&#24456;&#22823;&#30340;&#20020;&#24202; NER &#20219;&#21153;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;OpenAI&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#22312;2010&#24180;i2b2&#25361;&#25112;&#20013;&#25351;&#23450;&#30340;&#20020;&#24202;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#35774;&#32622;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23558;&#20854;&#24615;&#33021;&#19982;GPT-3&#22312;&#31867;&#20284;&#30340;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20197;&#21450;&#20351;&#29992;MTSamples&#30340;&#19968;&#32452;&#21512;&#25104;&#30340;&#20020;&#24202;&#31508;&#35760;&#23545;BioClinicalBERT&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#24494;&#35843;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#31934;&#30830;&#21305;&#37197;&#21644;&#26494;&#24347;&#21305;&#37197;&#30340;F1&#20998;&#21035;&#20026;0.418&#65288;vs.0.250&#65289;&#21644;0.620&#65288;vs.0.480&#65289;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;GPT-3&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#21478;&#22806;&#65292;&#25552;&#31034;&#31574;&#30053;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;ChatGPT&#30340;&#24615;&#33021;&#65292;&#22312;&#20004;&#31181;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#19979;&#26494;&#24347;&#21305;&#37197;&#30340;F1&#20998;&#21035;&#20026;0.628&#21644;0.541&#12290;&#34429;&#28982;ChatGPT&#30340;&#24615;&#33021;&#20173;&#20302;&#20110;&#21463;&#30417;&#30563;&#30340;BioClinicalBERT&#27169;&#22411;&#65288;&#21363;&#26494;&#24347;&#21305;&#37197;F1&#20998;&#25968;&#20998;&#21035;&#20026;0.628&#21644;0.870&#65289;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#20102;ChatGPT&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20020;&#24202;NER&#20219;&#21153;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigated the potential of ChatGPT, a large language model developed by OpenAI, for the clinical named entity recognition task defined in the 2010 i2b2 challenge, in a zero-shot setting with two different prompt strategies. We compared its performance with GPT-3 in a similar zero-shot setting, as well as a fine-tuned BioClinicalBERT model using a set of synthetic clinical notes from MTSamples. Our findings revealed that ChatGPT outperformed GPT-3 in the zero-shot setting, with F1 scores of 0.418 (vs.0.250) and 0.620 (vs. 0.480) for exact- and relaxed-matching, respectively. Moreover, prompts affected ChatGPT's performance greatly, with relaxed-matching F1 scores of 0.628 vs.0.541 for two different prompt strategies. Although ChatGPT's performance was still lower than that of the supervised BioClinicalBERT model (i.e., relaxed-matching F1 scores of 0.628 vs. 0.870), our study demonstrates the great potential of ChatGPT for clinical NER tasks in a zero-shot setting, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;16&#20010;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#21644;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;2.3&#65285;&#33267;26.7&#65285;&#30340;&#25991;&#31456;&#23558;&#20027;&#39064;&#24402;&#22240;&#20110;&#30005;&#25253;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2301.10856</link><description>&lt;p&gt;
&#37096;&#20998;&#21160;&#21592;&#65306;&#36319;&#36394;&#20420;&#32599;&#26031;&#23186;&#20307;&#21644;&#30005;&#25253;&#20043;&#38388;&#30340;&#22810;&#35821;&#35328;&#20449;&#24687;&#27969;
&lt;/p&gt;
&lt;p&gt;
Partial Mobilization: Tracking Multilingual Information Flows Amongst Russian Media Outlets and Telegram. (arXiv:2301.10856v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;16&#20010;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#21644;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#20114;&#21160;&#65292;&#21457;&#29616;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;2.3&#65285;&#33267;26.7&#65285;&#30340;&#25991;&#31456;&#23558;&#20027;&#39064;&#24402;&#22240;&#20110;&#30005;&#25253;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20420;&#32599;&#26031;&#20837;&#20405;&#20044;&#20811;&#20848;&#21518;&#65292;&#38024;&#23545;&#20420;&#32599;&#26031;&#22312;&#32447;&#23186;&#20307;&#30340;&#34394;&#20551;&#20449;&#24687;&#21644;&#23459;&#20256;&#65292;&#21253;&#25324;&#20420;&#32599;&#26031;&#20043;&#22768;&#21644;&#21355;&#26143;&#26032;&#38395;&#22312;&#20869;&#30340;&#20420;&#32599;&#26031;&#23186;&#20307;&#22312;&#27431;&#27954;&#36973;&#21040;&#31105;&#27490;&#12290;&#20026;&#20102;&#20445;&#25345;&#35266;&#20247;&#25968;&#37327;&#65292;&#35768;&#22810;&#20420;&#32599;&#26031;&#23186;&#20307;&#24320;&#22987;&#22312;&#30005;&#25253;&#31561;&#28040;&#24687;&#26381;&#21153;&#19978;&#22823;&#21147;&#23459;&#20256;&#20854;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;2022&#24180;&#26399;&#38388;16&#23478;&#20420;&#32599;&#26031;&#23186;&#20307;&#26426;&#26500;&#22914;&#20309;&#19982;732&#20010;&#30005;&#25253;&#39057;&#36947;&#20114;&#21160;&#21644;&#21033;&#29992;&#12290;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;MPNet&#12289;DP-means&#32858;&#31867;&#21644;Hawkes&#36807;&#31243;&#65292;&#25105;&#20204;&#36319;&#36394;&#26032;&#38395;&#32593;&#31449;&#21644;&#30005;&#25253;&#39057;&#36947;&#20043;&#38388;&#30340;&#21465;&#20107;&#20256;&#25773;&#24773;&#20917;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26032;&#38395;&#23186;&#20307;&#19981;&#20165;&#36890;&#36807;&#30005;&#25253;&#20256;&#25773;&#29616;&#26377;&#30340;&#21465;&#20107;&#65292;&#32780;&#19988;&#20182;&#20204;&#20250;&#20174;&#30005;&#25253;&#24179;&#21488;&#28304;&#26448;&#26009;&#12290;&#22312;&#25105;&#20204;&#30740;&#31350;&#30340;&#32593;&#31449;&#20013;&#65292;2.3&#65285;&#65288;ura.news&#65289;&#33267;26.7&#65285;&#65288;ukraina.ru&#65289;&#30340;&#25991;&#31456;&#35752;&#35770;&#20102;&#28304;&#20110;/&#23548;&#33268;&#30005;&#25253;&#27963;&#21160;&#30340;&#20869;&#23481;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#36319;&#36394;&#20010;&#21035;&#20027;&#39064;&#30340;&#25193;&#25955;&#65292;&#25105;&#20204;&#27979;&#37327;&#26032;&#38395;&#32593;&#31449;&#21457;&#34920;&#25991;&#31456;&#30340;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to disinformation and propaganda from Russian online media following the Russian invasion of Ukraine, Russian outlets including Russia Today and Sputnik News were banned throughout Europe. To maintain viewership, many of these Russian outlets began to heavily promote their content on messaging services like Telegram. In this work, we study how 16 Russian media outlets interacted with and utilized 732 Telegram channels throughout 2022. Leveraging the foundational model MPNet, DP-means clustering, and Hawkes Processes, we trace how narratives spread between news sites and Telegram channels. We show that news outlets not only propagate existing narratives through Telegram, but that they source material from the messaging platform. Across the sites in our study, between 2.3% (ura.news) and 26.7% (ukraina.ru) of articles discuss content that originated/resulted from activity on Telegram. Finally, tracking the spread of individual topics, we measure the rate at which news website
&lt;/p&gt;</description></item><item><title>tasksource&#26159;&#19968;&#20010;&#25968;&#25454;&#38598;&#21327;&#35843;&#26694;&#26550;&#65292;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#35780;&#20272;&#25552;&#20379;&#27969;&#30021;&#30340;&#20307;&#39564;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#32467;&#26500;&#21270;&#27880;&#37322;&#65292;&#20351;&#24471;&#25968;&#25454;&#22788;&#29702;&#26356;&#21152;&#20415;&#25463;&#12290;</title><link>http://arxiv.org/abs/2301.05948</link><description>&lt;p&gt;
tasksource&#65306;&#27969;&#30021;&#30340;NLP&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#21327;&#35843;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
tasksource: A Dataset Harmonization Framework for Streamlined NLP Multi-Task Learning and Evaluation. (arXiv:2301.05948v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05948
&lt;/p&gt;
&lt;p&gt;
tasksource&#26159;&#19968;&#20010;&#25968;&#25454;&#38598;&#21327;&#35843;&#26694;&#26550;&#65292;&#20026;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#35780;&#20272;&#25552;&#20379;&#27969;&#30021;&#30340;&#20307;&#39564;&#12290;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#32467;&#26500;&#21270;&#27880;&#37322;&#65292;&#20351;&#24471;&#25968;&#25454;&#22788;&#29702;&#26356;&#21152;&#20415;&#25463;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
HuggingFace&#25968;&#25454;&#38598;&#20013;&#24515;&#25552;&#20379;&#25968;&#21315;&#20010;&#25968;&#25454;&#38598;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#29305;&#23450;&#20219;&#21153;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#20855;&#26377;&#19981;&#21516;&#30340;&#26550;&#26500;&#65292;&#20351;&#24471;&#21327;&#35843;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22810;&#20219;&#21153;&#35757;&#32451;&#25110;&#35780;&#20272;&#38656;&#35201;&#25163;&#21160;&#23558;&#25968;&#25454;&#36866;&#37197;&#21040;&#20219;&#21153;&#27169;&#26495;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20123;&#20513;&#35758;&#37319;&#21462;&#29420;&#31435;&#30340;&#26041;&#27861;&#65292;&#21457;&#24067;&#21327;&#35843;&#30340;&#25968;&#25454;&#38598;&#25110;&#25552;&#20379;&#21327;&#35843;&#20195;&#30721;&#20197;&#23558;&#25968;&#25454;&#38598;&#39044;&#22788;&#29702;&#20026;&#19968;&#33268;&#26684;&#24335;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20197;&#21069;&#30340;&#39044;&#22788;&#29702;&#24037;&#20316;&#20013;&#30340;&#27169;&#24335;&#65292;&#20363;&#22914;&#21015;&#21517;&#31216;&#26144;&#23556;&#21644;&#20174;&#21015;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#29305;&#23450;&#23376;&#23383;&#27573;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#27880;&#37322;&#26694;&#26550;&#65292;&#30830;&#20445;&#25105;&#20204;&#30340;&#27880;&#37322;&#23436;&#20840;&#26292;&#38706;&#32780;&#19981;&#38544;&#34255;&#22312;&#38750;&#32467;&#26500;&#21270;&#20195;&#30721;&#20013;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#27880;&#37322;&#26694;&#26550;&#21644;500&#22810;&#20010;&#33521;&#35821;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The HuggingFace Datasets Hub hosts thousands of datasets, offering exciting opportunities for language model training and evaluation. However, datasets for a specific task type often have different schemas, making harmonization challenging. Multi-task training or evaluation necessitates manual work to fit data into task templates. Several initiatives independently tackle this issue by releasing harmonized datasets or providing harmonization codes to preprocess datasets into a consistent format. We identify patterns across previous preprocessing efforts, such as column name mapping and extracting specific sub-fields from structured data in a column. We then propose a structured annotation framework that ensures our annotations are fully exposed and not hidden within unstructured code. We release a dataset annotation framework and dataset annotations for more than 500 English tasks\footnote{\url{https://github.com/sileod/tasksource}}. These annotations include metadata, such as the names
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;AI&#27169;&#22411;&#65292;&#20351;&#29992;&#32593;&#39029;&#25235;&#21462;&#30340;&#25968;&#25454;&#35757;&#32451;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#24615;&#29289;&#21270;&#20559;&#35265;&#65292;&#21363;&#20154;&#30340;&#24773;&#24863;&#29366;&#24577;&#19982;&#36523;&#20307;&#30340;&#21576;&#29616;&#30456;&#20851;&#65292;&#34920;&#29616;&#20986;&#23545;&#22899;&#24615;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2212.11261</link><description>&lt;p&gt;
&#20351;&#29992;&#32593;&#39029;&#25235;&#21462;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#23545;&#27604;&#35821;&#35328;-&#35270;&#35273;AI&#27169;&#22411;&#23384;&#22312;&#24615;&#29289;&#21270;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias. (arXiv:2212.11261v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;AI&#27169;&#22411;&#65292;&#20351;&#29992;&#32593;&#39029;&#25235;&#21462;&#30340;&#25968;&#25454;&#35757;&#32451;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#24615;&#29289;&#21270;&#20559;&#35265;&#65292;&#21363;&#20154;&#30340;&#24773;&#24863;&#29366;&#24577;&#19982;&#36523;&#20307;&#30340;&#21576;&#29616;&#30456;&#20851;&#65292;&#34920;&#29616;&#20986;&#23545;&#22899;&#24615;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#30446;&#26631;&#36827;&#34892;&#32593;&#39029;&#25235;&#21462;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#20061;&#31181;&#35821;&#35328;-&#35270;&#35273;AI&#27169;&#22411;&#65292;&#20197;&#23547;&#25214;&#24515;&#29702;&#23398;&#23478;&#30740;&#31350;&#30340;&#20559;&#35265;&#30340;&#35777;&#25454;&#65306;&#22899;&#23401;&#21644;&#22899;&#24615;&#30340;&#24615;&#29289;&#21270;&#29616;&#35937;&#12290;&#25105;&#20204;&#22797;&#21046;&#20102;&#19977;&#20010;&#24515;&#29702;&#23454;&#39564;&#65292;&#24182;&#26174;&#31034;&#36825;&#31181;&#20559;&#35265;&#22312;AI&#20013;&#20381;&#28982;&#23384;&#22312;&#12290;&#31532;&#19968;&#20010;&#23454;&#39564;&#20351;&#29992;Sexual OBjectification and EMotion Database&#20013;&#30340;&#26631;&#20934;&#22899;&#24615;&#22270;&#20687;&#65292;&#24182;&#21457;&#29616;&#24773;&#24863;&#29366;&#24577;&#30340;&#35782;&#21035;&#26159;&#30001;&#20027;&#20307;&#26159;&#21542;&#20840;&#36523;&#25110;&#37096;&#20998;&#31359;&#30528;&#36827;&#34892;&#20171;&#23548;&#30340;&#12290;&#23884;&#20837;&#20851;&#32852;&#27979;&#35797;&#36820;&#22238;&#20102;&#24868;&#24594;(d&gt;0.80)&#21644;&#24754;&#20260;(d&gt;0.50)&#30340;&#26174;&#30528;&#25928;&#24212;&#22823;&#23567;&#65292;&#23558;&#23436;&#20840;&#31359;&#30528;&#30340;&#20027;&#20307;&#30340;&#22270;&#20687;&#19982;&#24773;&#24863;&#30456;&#20851;&#32852;&#12290;GRAD-CAM&#26174;&#33879;&#24615;&#22270;&#31361;&#20986;&#26174;&#31034;&#65292;CLIP&#29983;&#25104;&#30340;&#27169;&#22411;&#23384;&#22312;&#24615;&#29289;&#21270;&#20559;&#35265;&#65292;&#21363;&#20351;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#23545;&#27604;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21644;&#32593;&#39029;&#25235;&#21462;&#30340;&#25968;&#25454;&#20063;&#26159;&#22914;&#27492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person's human characteristics, such as emotions, are disregarded and the person is treated as a body. We replicate three experiments in psychology quantifying sexual objectification and show that the phenomena persist in AI. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model's recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests (EATs) return significant effect sizes for both anger (d &gt;0.80) and sadness (d &gt;0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DimonGen&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#21508;&#31181;&#26085;&#24120;&#22330;&#26223;&#30340;&#27010;&#24565;&#20851;&#31995;&#25551;&#36848;&#26469;&#23454;&#29616;&#22810;&#20803;&#21270;&#36890;&#35782;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MoREE&#27169;&#22411;&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.10545</link><description>&lt;p&gt;
DimonGen&#65306;&#22810;&#20803;&#21270;&#29983;&#25104;&#36890;&#35782;&#25512;&#29702;&#20197;&#35299;&#37322;&#27010;&#24565;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships. (arXiv:2212.10545v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DimonGen&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#21508;&#31181;&#26085;&#24120;&#22330;&#26223;&#30340;&#27010;&#24565;&#20851;&#31995;&#25551;&#36848;&#26469;&#23454;&#29616;&#22810;&#20803;&#21270;&#36890;&#35782;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MoREE&#27169;&#22411;&#22312;&#29983;&#25104;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DimonGen&#65292;&#26088;&#22312;&#29983;&#25104;&#25551;&#36848;&#21508;&#31181;&#26085;&#24120;&#22330;&#26223;&#20013;&#27010;&#24565;&#20851;&#31995;&#30340;&#22810;&#26679;&#21270;&#21477;&#23376;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25913;&#32534;&#29616;&#26377;&#30340;CommonGen&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MoREE&#30340;&#20004;&#38454;&#27573;&#27169;&#22411;&#26469;&#29983;&#25104;&#30446;&#26631;&#21477;&#23376;&#12290;MoREE&#21253;&#21547;&#19968;&#31181;&#28151;&#21512;&#26816;&#32034;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26816;&#32034;&#19982;&#32473;&#23450;&#27010;&#24565;&#30456;&#20851;&#30340;&#22810;&#26679;&#24615;&#19978;&#19979;&#25991;&#21477;&#23376;&#65292;&#20197;&#21450;&#19968;&#31181;&#28151;&#21512;&#29983;&#25104;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#26816;&#32034;&#21040;&#30340;&#19978;&#19979;&#25991;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#22312;DimonGen&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;MoREE&#22312;&#29983;&#25104;&#30340;&#21477;&#23376;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;MoREE&#33021;&#22815;&#29983;&#25104;&#21453;&#26144;&#27010;&#24565;&#20043;&#38388;&#19981;&#21516;&#20851;&#31995;&#30340;&#22810;&#26679;&#21270;&#21477;&#23376;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#27010;&#24565;&#20851;&#31995;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose DimonGen, which aims to generate diverse sentences describing concept relationships in various everyday scenarios. To support this, we first create a benchmark dataset for this task by adapting the existing CommonGen dataset. We then propose a two-stage model called MoREE to generate the target sentences. MoREE consists of a mixture of retrievers model that retrieves diverse context sentences related to the given concepts, and a mixture of generators model that generates diverse sentences based on the retrieved contexts. We conduct experiments on the DimonGen task and show that MoREE outperforms strong baselines in terms of both the quality and diversity of the generated sentences. Our results demonstrate that MoREE is able to generate diverse sentences that reflect different relationships between concepts, leading to a comprehensive understanding of concept relationships.
&lt;/p&gt;</description></item><item><title>CiteBench&#26159;&#19968;&#20010;&#31185;&#23398;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#30740;&#31350;&#21152;&#36895;&#23548;&#33268;&#30340;&#35299;&#35835;&#21644;&#24635;&#32467;&#20808;&#21069;&#24037;&#20316;&#30340;&#22256;&#38590;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#20197;&#36827;&#34892;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#30740;&#31350;&#19981;&#21516;&#20219;&#21153;&#35774;&#35745;&#21644;&#39046;&#22495;&#30340;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#23545;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#30340;&#22823;&#37327;&#27979;&#35797;&#21457;&#29616;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2212.09577</link><description>&lt;p&gt;
CiteBench&#65306;&#31185;&#23398;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CiteBench: A benchmark for Scientific Citation Text Generation. (arXiv:2212.09577v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09577
&lt;/p&gt;
&lt;p&gt;
CiteBench&#26159;&#19968;&#20010;&#31185;&#23398;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35299;&#20915;&#30740;&#31350;&#21152;&#36895;&#23548;&#33268;&#30340;&#35299;&#35835;&#21644;&#24635;&#32467;&#20808;&#21069;&#24037;&#20316;&#30340;&#22256;&#38590;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21487;&#20197;&#36827;&#34892;&#26631;&#20934;&#21270;&#35780;&#20272;&#65292;&#30740;&#31350;&#19981;&#21516;&#20219;&#21153;&#35774;&#35745;&#21644;&#39046;&#22495;&#30340;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;&#23545;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#30340;&#22823;&#37327;&#27979;&#35797;&#21457;&#29616;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#36890;&#36807;&#36880;&#27493;&#24314;&#31435;&#22312;&#31185;&#23398;&#20986;&#29256;&#29289;&#20013;&#35760;&#24405;&#30340;&#20808;&#21069;&#30693;&#35782;&#20307;&#31995;&#30340;&#22522;&#30784;&#19978;&#25552;&#39640;&#12290;&#35768;&#22810;&#39046;&#22495;&#30340;&#30740;&#31350;&#21152;&#36895;&#20351;&#24471;&#36319;&#19978;&#26368;&#26032;&#21457;&#23637;&#24182;&#24635;&#32467;&#19981;&#26029;&#22686;&#38271;&#30340;&#20808;&#21069;&#24037;&#20316;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#30340;&#20219;&#21153;&#26088;&#22312;&#22312;&#32473;&#23450;&#38656;&#35201;&#24341;&#29992;&#30340;&#35770;&#25991;&#21644;&#24341;&#29992;&#35770;&#25991;&#30340;&#24773;&#22659;&#30340;&#24773;&#20917;&#19979;&#29983;&#25104;&#20934;&#30830;&#30340;&#25991;&#26412;&#25688;&#35201;&#12290;&#29616;&#26377;&#30340;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#30740;&#31350;&#22522;&#20110;&#24191;&#27867;&#20998;&#27495;&#30340;&#20219;&#21153;&#23450;&#20041;&#65292;&#36825;&#20351;&#24471;&#31995;&#32479;&#22320;&#30740;&#31350;&#36825;&#20010;&#20219;&#21153;&#21464;&#24471;&#22256;&#38590;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CiteBench&#65306;&#19968;&#20010;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#65292;&#23427;&#32479;&#19968;&#20102;&#22810;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;&#21487;&#20197;&#23545;&#20219;&#21153;&#35774;&#35745;&#21644;&#39046;&#22495;&#20013;&#30340;&#24341;&#25991;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#26631;&#20934;&#21270;&#35780;&#20272;&#12290;&#20351;&#29992;&#36825;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22810;&#20010;&#24378;&#22522;&#20934;&#30340;&#24615;&#33021;&#65292;&#27979;&#35797;&#20102;&#23427;&#20204;&#22312;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20219;&#21153;&#30340;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Science progresses by incrementally building upon the prior body of knowledge documented in scientific publications. The acceleration of research across many fields makes it hard to stay up-to-date with the recent developments and to summarize the ever-growing body of prior work. To target this issue, the task of citation text generation aims to produce accurate textual summaries given a set of papers-to-cite and the citing paper context. Existing studies in citation text generation are based upon widely diverging task definitions, which makes it hard to study this task systematically. To address this challenge, we propose CiteBench: a benchmark for citation text generation that unifies multiple diverse datasets and enables standardized evaluation of citation text generation models across task designs and domains. Using the new benchmark, we investigate the performance of multiple strong baselines, test their transferability between the datasets, and deliver new insights into the task 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PVGRU&#30340;&#32452;&#20214;&#65292;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#27719;&#24635;&#21464;&#37327;&#26469;&#32858;&#21512;&#23376;&#24207;&#21015;&#30340;&#32047;&#31215;&#20998;&#24067;&#21464;&#21270;&#65292;&#20174;&#32780;&#20248;&#21270;&#22522;&#20110;&#29983;&#25104;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22810;&#36718;&#23545;&#35805;&#22238;&#22797;&#65292;&#25552;&#39640;&#23545;&#35805;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.09086</link><description>&lt;p&gt;
PVGRU&#65306;&#36890;&#36807;Pseudo-Variational&#26426;&#21046;&#29983;&#25104;&#22810;&#26679;&#19988;&#30456;&#20851;&#30340;&#23545;&#35805;&#22238;&#22797;
&lt;/p&gt;
&lt;p&gt;
PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism. (arXiv:2212.09086v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PVGRU&#30340;&#32452;&#20214;&#65292;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#27719;&#24635;&#21464;&#37327;&#26469;&#32858;&#21512;&#23376;&#24207;&#21015;&#30340;&#32047;&#31215;&#20998;&#24067;&#21464;&#21270;&#65292;&#20174;&#32780;&#20248;&#21270;&#22522;&#20110;&#29983;&#25104;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22810;&#36718;&#23545;&#35805;&#22238;&#22797;&#65292;&#25552;&#39640;&#23545;&#35805;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#29983;&#25104;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#29992;&#20110;&#22810;&#36718;&#23545;&#35805;&#30340;&#22238;&#22797;&#29983;&#25104;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;RNN&#65288;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#26368;&#21518;&#38544;&#34255;&#30340;&#29366;&#24577;&#26469;&#27719;&#24635;&#24207;&#21015;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#19981;&#21516;&#23545;&#35805;&#20013;&#35266;&#23519;&#21040;&#30340;&#24494;&#22937;&#21464;&#21270;&#65292;&#24182;&#19988;&#19981;&#33021;&#21306;&#20998;&#22312;&#26500;&#25104;&#26041;&#38754;&#30456;&#20284;&#30340;&#23545;&#35805;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Pseudo-Variational Gated Recurrent Unit&#65288;PVGRU&#65289;&#32452;&#20214;&#65292;&#26080;&#38656;&#21518;&#39564;&#30693;&#35782;&#21363;&#21487;&#23558;&#27719;&#24635;&#21464;&#37327;&#24341;&#20837;GRU&#65292;&#20854;&#21487;&#20197;&#32858;&#21512;&#23376;&#24207;&#21015;&#30340;&#32047;&#31215;&#20998;&#24067;&#21464;&#21270;&#12290; PVGRU&#21487;&#20197;&#36890;&#36807;&#24635;&#32467;&#21464;&#37327;&#24863;&#30693;&#24494;&#22937;&#30340;&#35821;&#20041;&#21464;&#21270;&#65292;&#36825;&#20123;&#21464;&#21270;&#26159;&#36890;&#36807;&#35774;&#35745;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#21644;&#37325;&#26500;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;PVGRU&#26500;&#24314;&#20102;Pseudo-Variational Hierarchical Dialogue&#65288;PVHD&#65289;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PVGRU&#21487;&#20197;&#24191;&#27867;&#25552;&#39640;&#23545;&#35805;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate response generation for multi-turn dialogue in generative-based chatbots. Existing generative models based on RNNs (Recurrent Neural Networks) usually employ the last hidden state to summarize the sequences, which makes models unable to capture the subtle variability observed in different dialogues and cannot distinguish the differences between dialogues that are similar in composition. In this paper, we propose a Pseudo-Variational Gated Recurrent Unit (PVGRU) component without posterior knowledge through introducing a recurrent summarizing variable into the GRU, which can aggregate the accumulated distribution variations of subsequences. PVGRU can perceive the subtle semantic variability through summarizing variables that are optimized by the devised distribution consistency and reconstruction objectives. In addition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) model based on PVGRU. Experimental results demonstrate that PVGRU can broadly improve the dive
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#31034;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#21644;&#25351;&#20196;&#29702;&#35299;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#24320;&#25918;&#22495;&#38382;&#31572;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;ODQA&#25968;&#25454;&#38598;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.08635</link><description>&lt;p&gt;
&#33258;&#25105;&#25552;&#31034;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38646;&#26679;&#26412;&#24320;&#25918;&#22495;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Self-Prompting Large Language Models for Zero-Shot Open-Domain QA. (arXiv:2212.08635v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#31034;&#26694;&#26550;&#65292;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#30693;&#35782;&#21644;&#25351;&#20196;&#29702;&#35299;&#33021;&#21147;&#65292;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#24320;&#25918;&#22495;&#38382;&#31572;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;ODQA&#25968;&#25454;&#38598;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22495;&#38382;&#31572;&#30446;&#26631;&#22312;&#20110;&#22238;&#31572;&#20851;&#20110;&#20107;&#23454;&#30340;&#38382;&#39064;&#65292;&#32780;&#26080;&#38656;&#25552;&#20379;&#29305;&#23450;&#30340;&#32972;&#26223;&#25991;&#26723;&#12290;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#30001;&#20110;&#27809;&#26377;&#25968;&#25454;&#26469;&#35757;&#32451;&#31867;&#20284;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#30340;&#23450;&#21046;&#27169;&#22411;&#65292;&#22240;&#27492;&#27492;&#20219;&#21153;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#36890;&#36807;&#30452;&#25509;&#25552;&#31034;&#26041;&#27861;&#22312;&#38646;&#26679;&#26412;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#36828;&#36828;&#19981;&#33021;&#20805;&#20998;&#21457;&#25381;LLM&#30340;&#24378;&#22823;&#21151;&#33021;&#65292;&#32780;&#21482;&#26159;&#20197;&#38544;&#24335;&#26041;&#24335;&#35843;&#29992;&#23427;&#20204;&#32780;&#24050;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#25105;&#25552;&#31034;&#26694;&#26550;&#65292;&#20197;&#26126;&#30830;&#21033;&#29992;LLM&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#22823;&#37327;&#30693;&#35782;&#21644;&#20854;&#24378;&#22823;&#30340;&#25351;&#20196;&#29702;&#35299;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36880;&#27493;&#25552;&#31034;LLM&#29983;&#25104;&#22810;&#20010;&#20266;QA&#23545;&#65292;&#24182;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#32972;&#26223;&#27573;&#33853;&#21644;&#35299;&#37322;&#65292;&#28982;&#21518;&#20351;&#29992;&#29983;&#25104;&#30340;&#20803;&#32032;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;ODQA&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;SOTA&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-Domain Question Answering (ODQA) aims at answering factoid questions without explicitly providing specific background documents. In a zero-shot setting, this task is more challenging since no data is available to train customized models like Retriever-Readers. Recently, Large Language Models (LLMs) like GPT-3 have shown their power in zero-shot ODQA with direct prompting methods, but these methods are still far from releasing the full powerfulness of LLMs only in an implicitly invoking way. In this paper, we propose a Self-Prompting framework to explicitly utilize the massive knowledge stored in the parameters of LLMs and their strong instruction understanding abilities. Concretely, we prompt LLMs step by step to generate multiple pseudo QA pairs with background passages and explanations from scratch and then use those generated elements for in-context learning. Experimental results show our method surpasses previous SOTA methods significantly on three widely-used ODQA datasets, a
&lt;/p&gt;</description></item><item><title>CREPE&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32452;&#21512;&#24615;&#35780;&#20272;&#22522;&#20934;&#65292;&#34913;&#37327;&#20102;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#21644;&#20135;&#20986;&#24615;&#33021;&#65292;&#21457;&#29616;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#22312;&#32452;&#21512;&#24615;&#19978;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.07796</link><description>&lt;p&gt;
CREPE&#65306;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25353;&#32452;&#21512;&#24605;&#32771;&#65311;
&lt;/p&gt;
&lt;p&gt;
CREPE: Can Vision-Language Foundation Models Reason Compositionally?. (arXiv:2212.07796v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07796
&lt;/p&gt;
&lt;p&gt;
CREPE&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#32452;&#21512;&#24615;&#35780;&#20272;&#22522;&#20934;&#65292;&#34913;&#37327;&#20102;&#27169;&#22411;&#30340;&#31995;&#32479;&#24615;&#21644;&#20135;&#20986;&#24615;&#33021;&#65292;&#21457;&#29616;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#22312;&#32452;&#21512;&#24615;&#19978;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#30340;&#19968;&#20010;&#20849;&#21516;&#29305;&#24449;&#26159;&#23427;&#20204;&#30340;&#26500;&#25104;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22823;&#22411;&#35270;&#35273;&#21644;&#35821;&#35328;&#39044;&#35757;&#32451;&#20026;&#24615;&#33021;&#25552;&#21319;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#65306;&#22312;&#35757;&#32451;&#20102;4&#31181;&#31639;&#27861;&#30340;7&#20010;&#26550;&#26500;&#21644;&#28023;&#37327;&#25968;&#25454;&#38598;&#21518;&#65292;&#23427;&#20204;&#37117;&#38590;&#20197;&#23454;&#29616;&#32452;&#21512;&#24615;&#12290;&#20026;&#20102;&#24471;&#20986;&#36825;&#20010;&#32467;&#35770;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#32452;&#21512;&#24615;&#35780;&#20272;&#22522;&#20934;&#65292;&#21363;CREPE&#65292;&#23427;&#34913;&#37327;&#20102;&#35748;&#30693;&#31185;&#23398;&#25991;&#29486;&#25152;&#35782;&#21035;&#30340;&#32452;&#21512;&#24615;&#30340;&#20004;&#20010;&#37325;&#35201;&#26041;&#38754;&#65306;&#31995;&#32479;&#24615;&#21644;&#20135;&#20986;&#24615;&#12290;&#20026;&#20102;&#34913;&#37327;&#31995;&#32479;&#24615;&#65292;CREPE&#21253;&#21547;&#19968;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;37&#19975;&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#21644;&#19977;&#20010;&#19981;&#21516;&#30340;&#24050;&#30475;/&#26410;&#30475;&#20998;&#21106;&#12290;&#36825;&#19977;&#20010;&#20998;&#21106;&#26159;&#35774;&#35745;&#29992;&#26469;&#27979;&#35797;&#22312;&#19977;&#20010;&#27969;&#34892;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;CC-12M&#12289;YFCC-15M&#21644;LAION-400M&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20026;&#20854;&#20013;&#30340;&#19968;&#20010;&#23376;&#38598;&#29983;&#25104;&#20102;32.5&#19975;&#12289;31.6&#19975;&#21644;30.9&#19975;&#20010;&#22256;&#38590;&#30340;&#36127;&#38754;&#23383;&#24149;&#12290;&#20026;&#20102;&#27979;&#35797;&#20135;&#20986;&#24615;&#33021;&#65292;CREPE&#21253;&#21547;&#20102;17,000&#20010;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#19981;&#21516;&#30340;&#22797;&#26434;&#24615;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental characteristic common to both human vision and natural language is their compositional nature. Yet, despite the performance gains contributed by large vision and language pretraining, we find that: across 7 architectures trained with 4 algorithms on massive datasets, they struggle at compositionality. To arrive at this conclusion, we introduce a new compositionality evaluation benchmark, CREPE, which measures two important aspects of compositionality identified by cognitive science literature: systematicity and productivity. To measure systematicity, CREPE consists of a test dataset containing over $370K$ image-text pairs and three different seen-unseen splits. The three splits are designed to test models trained on three popular training datasets: CC-12M, YFCC-15M, and LAION-400M. We also generate $325K$, $316K$, and $309K$ hard negative captions for a subset of the pairs. To test productivity, CREPE contains $17K$ image-text pairs with nine different complexities plus $
&lt;/p&gt;</description></item><item><title>AIONER&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20840;&#22871;&#26041;&#26696;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24037;&#20855;&#65292;&#21033;&#29992;&#22806;&#37096;&#25968;&#25454;&#25552;&#39640;BioNER&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.16944</link><description>&lt;p&gt;
AIONER: &#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20840;&#22871;&#26041;&#26696;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
AIONER: All-in-one scheme-based biomedical named entity recognition using deep learning. (arXiv:2211.16944v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16944
&lt;/p&gt;
&lt;p&gt;
AIONER&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20840;&#22871;&#26041;&#26696;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24037;&#20855;&#65292;&#21033;&#29992;&#22806;&#37096;&#25968;&#25454;&#25552;&#39640;BioNER&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;BioNER&#65289;&#26088;&#22312;&#33258;&#21160;&#35782;&#21035;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#65292;&#20026;&#20449;&#24687;&#25552;&#21462;&#21644;&#38382;&#31572;&#31561;&#19979;&#28216;&#25991;&#26412;&#25366;&#25496;&#20219;&#21153;&#21644;&#24212;&#29992;&#25552;&#20379;&#24517;&#35201;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20934;&#30830;&#27880;&#37322;&#25152;&#38656;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#26114;&#36149;&#65292;&#22240;&#27492;&#25163;&#21160;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#23545;BioNER&#20219;&#21153;&#26469;&#35828;&#26159;&#26114;&#36149;&#30340;&#12290;&#23548;&#33268;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#23548;&#33268;&#24403;&#21069;&#30340;BioNER&#26041;&#27861;&#23481;&#26131;&#20986;&#29616;&#36807;&#24230;&#25311;&#21512;&#65292;&#20855;&#26377;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#19968;&#27425;&#21482;&#33021;&#35299;&#20915;&#19968;&#31181;&#23454;&#20307;&#31867;&#22411;&#65288;&#20363;&#22914;&#65292;&#22522;&#22240;&#25110;&#30142;&#30149;&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#22871;&#26041;&#26696;&#65288;AIO&#65289;&#26041;&#26696;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#27880;&#37322;&#36164;&#28304;&#22806;&#37096;&#25968;&#25454;&#26469;&#25552;&#39640;BioNER&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#23574;&#31471;&#28145;&#24230;&#23398;&#20064;&#21644;&#25105;&#20204;&#30340;AIO&#26041;&#26696;&#30340;&#36890;&#29992;BioNER&#24037;&#20855;AIONER&#12290;&#25105;&#20204;&#22312;14&#20010;BioNER&#22522;&#20934;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;AIONER&#65292;&#24182;&#23637;&#31034;&#20102;AIONER&#30340;&#26377;&#25928;&#24615;&#65292;&#31283;&#20581;&#24615;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical named entity recognition (BioNER) seeks to automatically recognize biomedical entities in natural language text, serving as a necessary foundation for downstream text mining tasks and applications such as information extraction and question answering. Manually labeling training data for the BioNER task is costly, however, due to the significant domain expertise required for accurate annotation. The resulting data scarcity causes current BioNER approaches to be prone to overfitting, to suffer from limited generalizability, and to address a single entity type at a time (e.g., gene or disease). We therefore propose a novel all-in-one (AIO) scheme that uses external data from existing annotated resources to enhance the accuracy and stability of BioNER models. We further present AIONER, a general-purpose BioNER tool based on cutting-edge deep learning and our AIO schema. We evaluate AIONER on 14 BioNER benchmark tasks and show that AIONER is effective, robust, and compares favora
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;57&#31181;&#35821;&#35328;&#21644;&#19977;&#20010;&#20219;&#21153;&#19979;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#20248;&#21270;&#21518;&#30340;&#26631;&#35760;-&#32763;&#35793;&#27861;&#27604;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2211.15613</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#20196;&#20154;&#27822;&#20007;&#30340;&#31616;&#26131;&#26631;&#31614;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
Frustratingly Easy Label Projection for Cross-lingual Transfer. (arXiv:2211.15613v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;57&#31181;&#35821;&#35328;&#21644;&#19977;&#20010;&#20219;&#21153;&#19979;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#20248;&#21270;&#21518;&#30340;&#26631;&#35760;-&#32763;&#35793;&#27861;&#27604;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35757;&#32451;&#25968;&#25454;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#24050;&#25104;&#20026;&#25552;&#39640;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#28041;&#21450;&#36328;&#24230;&#32423;&#21035;&#27880;&#37322;&#65288;&#20363;&#22914;&#20449;&#24687;&#25552;&#21462;&#25110;&#38382;&#39064;&#22238;&#31572;&#65289;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#26631;&#31614;&#25237;&#24433;&#27493;&#39588;&#65292;&#23558;&#24050;&#27880;&#37322;&#30340;&#36328;&#24230;&#26144;&#23556;&#21040;&#32763;&#35793;&#21518;&#30340;&#25991;&#26412;&#20013;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#23545;&#36825;&#31181;&#26041;&#27861;&#19982;&#22522;&#20110;&#21333;&#35789;&#23545;&#40784;&#30340;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#36827;&#34892;&#23454;&#35777;&#20998;&#26512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#39033;&#23545;57&#31181;&#35821;&#35328;&#21644;&#19977;&#20010;&#20219;&#21153;&#65288;QA&#65292;NER&#21644;&#20107;&#20214;&#25552;&#21462;&#65289;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#20004;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#22635;&#34917;&#25991;&#29486;&#20013;&#30340;&#37325;&#35201;&#31354;&#30333;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#20248;&#21270;&#21518;&#30340;&#26631;&#35760;-&#32763;&#35793;&#27861;&#27604;&#20256;&#32479;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translating training data into many languages has emerged as a practical solution for improving cross-lingual transfer. For tasks that involve span-level annotations, such as information extraction or question answering, an additional label projection step is required to map annotated spans onto the translated texts. Recently, a few efforts have utilized a simple mark-then-translate method to jointly perform translation and projection by inserting special markers around the labeled spans in the original sentence. However, as far as we are aware, no empirical analysis has been conducted on how this approach compares to traditional annotation projection based on word alignment. In this paper, we present an extensive empirical study across 57 languages and three tasks (QA, NER, and Event Extraction) to evaluate the effectiveness and limitations of both methods, filling an important gap in the literature. Experimental results show that our optimized version of mark-then-translate, which we
&lt;/p&gt;</description></item><item><title>&#38416;&#36848;&#20102;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;AlephBERTGimmel&#22312;&#24076;&#20271;&#26469;&#35821;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#34920;&#29616;&#65292;&#20854;&#20351;&#29992;&#26356;&#39640;&#30340;&#35789;&#27719;&#37327;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#26032;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.15199</link><description>&lt;p&gt;
&#24102;&#30528;&#39069;&#22806;&#22823;&#35789;&#27719;&#37327;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65306;&#24076;&#20271;&#26469;&#35821;BERT&#27169;&#22411;&#23545;&#27604;&#20998;&#26512;&#21644;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#26469;&#36229;&#36234;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Pre-Trained Models with Extra-Large Vocabularies: A Contrastive Analysis of Hebrew BERT Models and a New One to Outperform Them All. (arXiv:2211.15199v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15199
&lt;/p&gt;
&lt;p&gt;
&#38416;&#36848;&#20102;&#26032;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;AlephBERTGimmel&#22312;&#24076;&#20271;&#26469;&#35821;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#34920;&#29616;&#65292;&#20854;&#20351;&#29992;&#26356;&#39640;&#30340;&#35789;&#27719;&#37327;&#65292;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#26032;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AlephBERTGimmel&#30340;&#29616;&#20195;&#24076;&#20271;&#26469;&#35821;&#26032;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#20854;&#20351;&#29992;&#27604;&#26631;&#20934;&#24076;&#20271;&#26469;PLMs&#26356;&#22823;&#30340;&#35789;&#27719;&#37327;&#65288;128K&#39033;&#30446;&#65289;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#27169;&#22411;&#36827;&#34892;&#23545;&#27604;&#20998;&#26512;&#65292;&#19982;&#25152;&#26377;&#20197;&#21069;&#30340;&#24076;&#20271;&#26469;PLMs&#65288;mBERT&#65292;heBERT&#65292;AlephBERT&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35780;&#20272;&#26356;&#22823;&#35789;&#27719;&#37327;&#23545;&#20219;&#21153;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#26356;&#22823;&#30340;&#35789;&#27719;&#37327;&#23548;&#33268;&#26356;&#23569;&#30340;&#20998;&#35010;&#65292;&#32780;&#20943;&#23569;&#20998;&#35010;&#23545;&#20110;&#27169;&#22411;&#24615;&#33021;&#26356;&#22909;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#20219;&#21153;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#20010;&#26032;&#27169;&#22411;&#22312;&#21253;&#25324;&#20998;&#35789;&#65292;&#35789;&#24615;&#26631;&#27880;&#65292;&#20840;&#24418;&#24577;&#20998;&#26512;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#24773;&#24863;&#20998;&#26512;&#22312;&#20869;&#30340;&#25152;&#26377;&#21487;&#29992;&#30340;&#24076;&#20271;&#26469;&#35821;&#22522;&#20934;&#27979;&#35797;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20513;&#19981;&#20165;&#20197;&#23618;&#25968;&#25110;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#20026;&#34913;&#37327;&#26631;&#20934;&#65292;&#32780;&#19988;&#36824;&#20197;&#35789;&#27719;&#37327;&#20026;&#26631;&#20934;&#30340;&#26356;&#22823;PLMs&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#36825;&#20010;&#26032;&#27169;&#22411;&#65292;&#21487;&#20197;&#19981;&#21463;&#38480;&#21046;&#22320;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new pre-trained language model (PLM) for modern Hebrew, termed AlephBERTGimmel, which employs a much larger vocabulary (128K items) than standard Hebrew PLMs before. We perform a contrastive analysis of this model against all previous Hebrew PLMs (mBERT, heBERT, AlephBERT) and assess the effects of larger vocabularies on task performance. Our experiments show that larger vocabularies lead to fewer splits, and that reducing splits is better for model performance, across different tasks. All in all this new model achieves new SOTA on all available Hebrew benchmarks, including Morphological Segmentation, POS Tagging, Full Morphological Analysis, NER, and Sentiment Analysis. Subsequently we advocate for PLMs that are larger not only in terms of number of layers or training data, but also in terms of their vocabulary. We release the new model publicly for unrestricted use.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38745;&#24577;&#27169;&#22411;&#21098;&#26525;&#65288;SMP&#65289;&#65292;&#23427;&#21482;&#20351;&#29992;&#19968;&#38454;&#21098;&#26525;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#30446;&#26631;&#31232;&#30095;&#24230;&#27700;&#24179;&#65292;&#22312;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;SMP&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.06210</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#24494;&#35843;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
Pruning Pre-trained Language Models Without Fine-Tuning. (arXiv:2210.06210v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38745;&#24577;&#27169;&#22411;&#21098;&#26525;&#65288;SMP&#65289;&#65292;&#23427;&#21482;&#20351;&#29992;&#19968;&#38454;&#21098;&#26525;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#30446;&#26631;&#31232;&#30095;&#24230;&#27700;&#24179;&#65292;&#22312;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;SMP&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20811;&#26381;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36807;&#20110;&#21442;&#25968;&#21270;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24191;&#27867;&#22320;&#20351;&#29992;&#21098;&#26525;&#20316;&#20026;&#19968;&#31181;&#31616;&#21333;&#21644;&#30452;&#25509;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#30452;&#25509;&#21435;&#38500;&#19981;&#37325;&#35201;&#30340;&#26435;&#37325;&#12290;&#20808;&#21069;&#30340;&#19968;&#38454;&#26041;&#27861;&#25104;&#21151;&#22320;&#23558;PLMs&#21387;&#32553;&#21040;&#26497;&#39640;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#34920;&#29616;&#20960;&#20046;&#19981;&#19979;&#38477;&#65292;&#22914;&#36816;&#21160;&#21098;&#26525;&#31561;&#12290;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#19968;&#38454;&#20449;&#24687;&#26469;&#21098;&#26525;PLMs&#65292;&#21516;&#26102;&#24494;&#35843;&#20854;&#20313;&#30340;&#26435;&#37325;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#23545;&#20110;&#19968;&#38454;&#21098;&#26525;&#65292;&#24494;&#35843;&#26159;&#22810;&#20313;&#30340;&#65292;&#22240;&#20026;&#19968;&#38454;&#21098;&#26525;&#36275;&#20197;&#23558;PLMs&#25910;&#25947;&#21040;&#19979;&#28216;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#22312;&#36825;&#20010;&#21021;&#34935;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38745;&#24577;&#27169;&#22411;&#21098;&#26525;&#65288;SMP&#65289;&#65292;&#23427;&#21482;&#20351;&#29992;&#19968;&#38454;&#21098;&#26525;&#26469;&#20351;PLMs&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#30446;&#26631;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#33945;&#29256;&#20989;&#25968;&#21644;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;SMP&#12290;&#22823;&#37327;&#21508;&#31181;&#31232;&#30095;&#24230;&#27700;&#24179;&#19979;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;SMP&#27604;&#19968;&#38454;&#21644;&#38646;&#38454;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
To overcome the overparameterized problem in Pre-trained Language Models (PLMs), pruning is widely used as a simple and straightforward compression method by directly removing unimportant weights. Previous first-order methods successfully compress PLMs to extremely high sparsity with little performance drop. These methods, such as movement pruning, use first-order information to prune PLMs while fine-tuning the remaining weights. In this work, we argue fine-tuning is redundant for first-order pruning, since first-order pruning is sufficient to converge PLMs to downstream tasks without fine-tuning. Under this motivation, we propose Static Model Pruning (SMP), which only uses first-order pruning to adapt PLMs to downstream tasks while achieving the target sparsity level. In addition, we also design a new masking function and training objective to further improve SMP. Extensive experiments at various sparsity levels show SMP has significant improvements over first-order and zero-order met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;prompt learning&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22522;&#20110;&#27492;&#25552;&#20986;&#20102;&#19968;&#20010;&#27880;&#37322;&#26080;&#20851;&#30340;&#27169;&#26495;&#36873;&#25321;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.15206</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20026;&#20160;&#20040;&#26356;&#36866;&#21512;&#38646;&#26679;&#26412;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Makes Pre-trained Language Models Better Zero-shot Learners?. (arXiv:2209.15206v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;prompt learning&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22522;&#20110;&#27492;&#25552;&#20986;&#20102;&#19968;&#20010;&#27880;&#37322;&#26080;&#20851;&#30340;&#27169;&#26495;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;prompt learning&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20256;&#32479;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#20250;&#22240;&#20026;&#36807;&#25311;&#21512;&#19981;&#20855;&#20195;&#34920;&#24615;&#30340;&#26631;&#27880;&#25968;&#25454;&#32780;&#22833;&#36133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;prompt learning&#26356;&#26377;&#25928;&#30340;&#20551;&#35774;&#65292;&#22240;&#20026;&#23427;&#20351;&#24314;&#31435;&#22312;&#28023;&#37327;&#25991;&#26412;&#35821;&#26009;&#24211;&#21644;&#39046;&#22495;&#30456;&#20851;&#20154;&#31867;&#30693;&#35782;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#22810;&#22320;&#21442;&#19982;&#39044;&#27979;&#65292;&#20174;&#32780;&#20943;&#23569;&#23567;&#22411;&#35757;&#32451;&#38598;&#25552;&#20379;&#30340;&#26377;&#38480;&#26631;&#31614;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20551;&#35774;&#65292;&#35821;&#35328;&#24046;&#24322;&#21487;&#20197;&#34913;&#37327;&#25552;&#31034;&#36136;&#37327;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#26356;&#20026;&#37325;&#35201;&#30340;&#26159;&#65292;&#21463;&#21040;&#29702;&#35770;&#26694;&#26550;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22256;&#24785;&#24230;&#30340;&#27880;&#37322;&#26080;&#20851;&#30340;&#27169;&#26495;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#20107;&#20808;&#8220;&#39044;&#27979;&#8221;&#25552;&#31034;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#20540;&#24471;&#40723;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a theoretical framework to explain the efficacy of prompt learning in zero/few-shot scenarios. First, we prove that conventional pre-training and fine-tuning paradigm fails in few-shot scenarios due to overfitting the unrepresentative labelled data. We then detail the assumption that prompt learning is more effective because it empowers pre-trained language model that is built upon massive text corpora, as well as domain-related human knowledge to participate more in prediction and thereby reduces the impact of limited label information provided by the small training set. We further hypothesize that language discrepancy can measure the quality of prompting. Comprehensive experiments are performed to verify our assumptions. More remarkably, inspired by the theoretical framework, we propose an annotation-agnostic template selection method based on perplexity, which enables us to ``forecast'' the prompting performance in advance. This approach is especially encou
&lt;/p&gt;</description></item><item><title>FNet&#27169;&#22411;&#36890;&#36807;&#26367;&#25442;&#27880;&#24847;&#21147;&#23618;&#20026;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#21152;&#36895;&#20102;Transformer&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#24182;&#20445;&#25345;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2209.12816</link><description>&lt;p&gt;
&#24555;&#36895;FNet&#65306;&#36890;&#36807;&#39640;&#25928;&#30340;&#20613;&#37324;&#21494;&#23618;&#21152;&#36895;Transformer&#32534;&#30721;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier Layers. (arXiv:2209.12816v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12816
&lt;/p&gt;
&lt;p&gt;
FNet&#27169;&#22411;&#36890;&#36807;&#26367;&#25442;&#27880;&#24847;&#21147;&#23618;&#20026;&#20613;&#37324;&#21494;&#21464;&#25442;&#65292;&#21152;&#36895;&#20102;Transformer&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#24182;&#20445;&#25345;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20960;&#20046;&#25152;&#26377;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#30456;&#20284;&#30340;&#27880;&#24847;&#21147;&#32467;&#26500;&#20063;&#22312;&#20854;&#20182;&#39046;&#22495;&#24191;&#27867;&#30740;&#31350;&#12290;&#34429;&#28982;&#27880;&#24847;&#21147;&#26426;&#21046;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#20294;&#20854;&#20108;&#27425;&#22797;&#26434;&#24230;&#38459;&#30861;&#20102;&#23545;&#38271;&#24207;&#21015;&#30340;&#39640;&#25928;&#22788;&#29702;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#28040;&#38500;&#35745;&#31639;&#25928;&#29575;&#30340;&#32570;&#28857;&#19978;&#65292;&#24182;&#34920;&#26126;&#22312;&#26080;&#38656;&#27880;&#24847;&#21147;&#23618;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20173;&#28982;&#33021;&#22815;&#36798;&#21040;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;&#19968;&#39033;&#24320;&#21019;&#24615;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;FNet&#65292;&#22312;Transformer&#32534;&#30721;&#22120;&#32467;&#26500;&#20013;&#29992;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;FT&#65289;&#26367;&#25442;&#27880;&#24847;&#21147;&#23618;&#12290;FNet&#22312;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#20013;&#21435;&#38500;&#20102;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#36816;&#31639;&#36127;&#25285;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#19982;&#21407;&#22987;Transformer&#32534;&#30721;&#22120;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;FNet&#27169;&#22411;&#24573;&#30053;&#20102;FT&#30340;&#22522;&#26412;&#23646;&#24615;..
&lt;/p&gt;
&lt;p&gt;
Transformer-based language models utilize the attention mechanism for substantial performance improvements in almost all natural language processing (NLP) tasks. Similar attention structures are also extensively studied in several other areas. Although the attention mechanism enhances the model performances significantly, its quadratic complexity prevents efficient processing of long sequences. Recent works focused on eliminating the disadvantages of computational inefficiency and showed that transformer-based models can still reach competitive results without the attention layer. A pioneering study proposed the FNet, which replaces the attention layer with the Fourier Transform (FT) in the transformer encoder architecture. FNet achieves competitive performances concerning the original transformer encoder model while accelerating training process by removing the computational burden of the attention mechanism. However, the FNet model ignores essential properties of the FT from the clas
&lt;/p&gt;</description></item><item><title>WeLM &#26159;&#19968;&#31181;&#38754;&#21521;&#20013;&#25991;&#30340;&#35835;&#36807;&#20070;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#35835;&#21462;&#39640;&#36136;&#37327;&#30340;&#35821;&#26009;&#24211;&#35757;&#32451;&#20102;100&#20159;&#20010;&#21442;&#25968;&#65292;&#24182;&#21487;&#20197;&#22312;18&#20010;&#20013;&#25991;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#21516;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21516;&#26102;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#21644;&#20195;&#30721;&#36716;&#25442;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2209.10372</link><description>&lt;p&gt;
WeLM: &#19968;&#31181;&#38754;&#21521;&#20013;&#25991;&#30340;&#35835;&#36807;&#20070;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
WeLM: A Well-Read Pre-trained Language Model for Chinese. (arXiv:2209.10372v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10372
&lt;/p&gt;
&lt;p&gt;
WeLM &#26159;&#19968;&#31181;&#38754;&#21521;&#20013;&#25991;&#30340;&#35835;&#36807;&#20070;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#35835;&#21462;&#39640;&#36136;&#37327;&#30340;&#35821;&#26009;&#24211;&#35757;&#32451;&#20102;100&#20159;&#20010;&#21442;&#25968;&#65292;&#24182;&#21487;&#20197;&#22312;18&#20010;&#20013;&#25991;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#21516;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21516;&#26102;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#21644;&#20195;&#30721;&#36716;&#25442;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#20026;&#22522;&#30784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#20013;&#26174;&#31034;&#20102;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;WeLM&#65306;&#19968;&#31181;&#33021;&#22815;&#38646;&#25110;&#23569;&#26679;&#26412;&#28436;&#31034;&#26080;&#32541;&#25191;&#34892;&#19981;&#21516;&#31867;&#22411;&#20219;&#21153;&#30340;&#38754;&#21521;&#20013;&#25991;&#30340;&#35835;&#36807;&#20070;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#12290;WeLM&#36890;&#36807;&#38405;&#35835;&#31934;&#24515;&#31574;&#21010;&#30340;&#39640;&#36136;&#37327;&#35821;&#26009;&#24211;&#20013;&#30340;&#20449;&#24687;&#65292;&#20197;100&#20159;&#20010;&#21442;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;WeLM&#22312;&#21508;&#20010;&#39046;&#22495;&#21644;&#35821;&#35328;&#26041;&#38754;&#20855;&#22791;&#24191;&#27867;&#30340;&#30693;&#35782;&#12290;&#22312;18&#39033;&#29420;&#31435;&#30340;&#65288;&#20013;&#25991;&#65289;&#20219;&#21153;&#20013;&#65292;WeLM&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#35268;&#27169;&#30456;&#20284;&#19988;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#33021;&#22815;&#21305;&#37197;&#35268;&#27169;&#39640;&#36798;25&#20493;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;WeLM&#36824;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#21644;&#20195;&#30721;&#36716;&#25442;&#29702;&#35299;&#33021;&#21147;&#65292;&#20248;&#20110;&#39044;&#20808;&#22312;30&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#29616;&#26377;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#22823;&#37327;&#20013;&#25991;&#30417;&#30563;&#24335;&#25968;&#25454;&#38598;&#25910;&#38598;&#20102;&#20154;&#24037;&#32534;&#20889;&#30340;&#25552;&#31034;&#65292;&#24182;&#23545;WeLM&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models pre-trained with self-supervised learning have demonstrated impressive zero-shot generalization capabilities on a wide spectrum of tasks. In this work, we present WeLM: a well-read pre-trained language model for Chinese that is able to seamlessly perform different types of tasks with zero or few-shot demonstrations. WeLM is trained with 10B parameters by "reading" a curated high-quality corpus covering a wide range of topics. We show that WeLM is equipped with broad knowledge on various domains and languages. On 18 monolingual (Chinese) tasks, WeLM can significantly outperform existing pre-trained models with similar sizes and match the performance of models up to 25 times larger. WeLM also exhibits strong capabilities in multi-lingual and code-switching understanding, outperforming existing multilingual language models pre-trained on 30 languages. Furthermore, We collected human-written prompts for a large set of supervised datasets in Chinese and fine-tuned WeLM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#21270;&#30340;&#23494;&#38598;&#26816;&#32034;&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#20915;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26080;&#27861;&#36866;&#24212;&#26102;&#38388;&#21464;&#21270;&#65292;&#26080;&#27861;&#24212;&#23545;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#21644;&#24050;&#21024;&#38500;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;Twitter&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.05706</link><description>&lt;p&gt;
&#38754;&#21521;&#31038;&#20132;&#23186;&#20307;&#35805;&#39064;&#20998;&#31867;&#30340;&#38750;&#21442;&#25968;&#21270;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Non-Parametric Temporal Adaptation for Social Media Topic Classification. (arXiv:2209.05706v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#21270;&#30340;&#23494;&#38598;&#26816;&#32034;&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#20915;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26080;&#27861;&#36866;&#24212;&#26102;&#38388;&#21464;&#21270;&#65292;&#26080;&#27861;&#24212;&#23545;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#21644;&#24050;&#21024;&#38500;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;Twitter&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#29983;&#25104;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#19981;&#26029;&#21464;&#21270;&#65292;&#26032;&#30340;&#36235;&#21183;&#24433;&#21709;&#22312;&#32447;&#35752;&#35770;&#65292;&#20010;&#20154;&#20449;&#24687;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#32780;&#34987;&#21024;&#38500;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26159;&#38745;&#24577;&#30340;&#65292;&#24182;&#20381;&#36182;&#20110;&#22266;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#26080;&#27861;&#36866;&#24212;&#26102;&#38388;&#21464;&#21270;&#65292;&#26080;&#27861;&#24212;&#23545;&#27979;&#35797;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#21644;&#24050;&#21024;&#38500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#38656;&#35201;&#32463;&#24120;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;&#26412;&#25991;&#36890;&#36807;&#38271;&#26399;&#30340;&#35805;&#39064;&#39044;&#27979;&#20219;&#21153;&#30740;&#31350;&#26102;&#38388;&#33258;&#36866;&#24212;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#21270;&#30340;&#23494;&#38598;&#26816;&#32034;&#25216;&#26415;&#20316;&#20026;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#19968;&#20010;&#26032;&#25910;&#38598;&#30340;&#12289;&#20844;&#24320;&#21487;&#29992;&#30340;&#12289;&#20026;&#26399;&#19968;&#24180;&#30340;Twitter&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35813;&#25968;&#25454;&#38598;&#34920;&#29616;&#20986;&#26102;&#38388;&#20998;&#24067;&#21464;&#21270;&#65292;&#25105;&#20204;&#26041;&#27861;&#27604;&#26368;&#20339;&#21442;&#25968;&#22522;&#32447;&#25913;&#21892;&#20102;64.12%&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#20854;&#26114;&#36149;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#23494;&#38598;&#26816;&#32034;&#26041;&#27861;&#20063;&#38750;&#24120;&#36866;&#29992;&#20110;&#31526;&#21512;&#25968;&#25454;&#38544;&#31169;&#27861;&#35268;&#30340;&#21160;&#24577;&#21024;&#38500;&#29992;&#25143;&#25968;&#25454;&#65292;&#20960;&#20046;&#27809;&#26377;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
User-generated social media data is constantly changing as new trends influence online discussion and personal information is deleted due to privacy concerns. However, most current NLP models are static and rely on fixed training data, which means they are unable to adapt to temporal change -- both test distribution shift and deleted training data -- without frequent, costly re-training. In this paper, we study temporal adaptation through the task of longitudinal hashtag prediction and propose a non-parametric dense retrieval technique, which does not require re-training, as a simple but effective solution. In experiments on a newly collected, publicly available, year-long Twitter dataset exhibiting temporal distribution shift, our method improves by 64.12% over the best parametric baseline without any of its costly gradient-based updating. Our dense retrieval approach is also particularly well-suited to dynamically deleted user data in line with data privacy laws, with negligible comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#40784;&#30340;&#31616;&#26131;&#24503;&#35821;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#36741;&#21161;&#19981;&#21516;&#20154;&#32676;&#29702;&#35299;&#22797;&#26434;&#30340;&#24503;&#35821;&#20070;&#38754;&#35821;&#35328;&#65307;&#35813;&#35821;&#26009;&#24211;&#36890;&#36807;&#33258;&#21160;&#21477;&#23376;&#23545;&#40784;&#26041;&#27861;&#20351;&#22810;&#20010;&#25991;&#26723;&#23545;&#40784;&#65292;&#19988;&#36136;&#37327;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2209.01106</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#30340;&#23545;&#40784;&#30340;&#31616;&#26131;&#24503;&#35821;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
A New Aligned Simple German Corpus. (arXiv:2209.01106v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#40784;&#30340;&#31616;&#26131;&#24503;&#35821;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#36741;&#21161;&#19981;&#21516;&#20154;&#32676;&#29702;&#35299;&#22797;&#26434;&#30340;&#24503;&#35821;&#20070;&#38754;&#35821;&#35328;&#65307;&#35813;&#35821;&#26009;&#24211;&#36890;&#36807;&#33258;&#21160;&#21477;&#23376;&#23545;&#40784;&#26041;&#27861;&#20351;&#22810;&#20010;&#25991;&#26723;&#23545;&#40784;&#65292;&#19988;&#36136;&#37327;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"Leichte Sprache"&#26159;&#24503;&#35821;&#29256;&#30340;&#31616;&#26131;&#33521;&#35821;&#65292;&#26088;&#22312;&#20026;&#19981;&#21516;&#20154;&#32676;&#25552;&#20379;&#22797;&#26434;&#30340;&#20070;&#38754;&#35821;&#35328;&#65292;&#20197;&#20415;&#20351;&#36825;&#20123;&#20869;&#23481;&#26131;&#20110;&#29702;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#40784;&#21477;&#23376;&#30340;&#21333;&#35821;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#31616;&#26131;&#24503;&#35821;--&#24503;&#35821;&#12290;&#23427;&#21253;&#21547;&#22810;&#20010;&#25991;&#26723;&#23545;&#40784;&#26469;&#28304;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#21477;&#23376;&#23545;&#40784;&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#20102;&#23545;&#40784;&#12290;&#25105;&#20204;&#22522;&#20110;&#25163;&#21160;&#26631;&#35760;&#30340;&#23545;&#40784;&#25991;&#26723;&#30340;&#23376;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#23545;&#40784;&#36136;&#37327;&#12290;&#26681;&#25454;F1&#20998;&#25968;&#27979;&#37327;&#65292;&#25105;&#20204;&#30340;&#21477;&#23376;&#23545;&#40784;&#36136;&#37327;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#22312;CC BY-SA&#19979;&#21457;&#24067;&#25968;&#25454;&#38598;&#65292;&#22312;MIT&#35768;&#21487;&#19979;&#21457;&#24067;&#38468;&#24102;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
"Leichte Sprache", the German counterpart to Simple English, is a regulated language aiming to facilitate complex written language that would otherwise stay inaccessible to different groups of people. We present a new sentence-aligned monolingual corpus for Simple German -- German. It contains multiple document-aligned sources which we have aligned using automatic sentence-alignment methods. We evaluate our alignments based on a manually labelled subset of aligned documents. The quality of our sentence alignments, as measured by F1-score, surpasses previous work. We publish the dataset under CC BY-SA and the accompanying code under MIT license.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#20855;&#26377;&#23459;&#20256;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#26816;&#27979;&#20154;&#24037;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;&#25152;&#29983;&#25104;&#30340;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;PropaNews&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2203.05386</link><description>&lt;p&gt;
&#20026;&#26816;&#27979;&#30495;&#20551;&#26032;&#38395;&#32780;&#20266;&#36896;&#30340;&#20551;&#26032;&#38395;&#65306;&#26377;&#23459;&#20256;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Faking Fake News for Real Fake News Detection: Propaganda-loaded Training Data Generation. (arXiv:2203.05386v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.05386
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#20855;&#26377;&#23459;&#20256;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#20197;&#24110;&#21161;&#26816;&#27979;&#20154;&#24037;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;&#25152;&#29983;&#25104;&#30340;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;PropaNews&#22312;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#27169;&#22411;&#33021;&#22815;&#26816;&#27979;&#29983;&#25104;&#30340;&#20551;&#26032;&#38395;&#65292;&#20294;&#23427;&#20204;&#30340;&#32467;&#26524;&#19981;&#36866;&#29992;&#20110;&#26377;&#25928;&#26816;&#27979;&#20154;&#24037;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;&#36825;&#26159;&#30001;&#20110;&#26426;&#22120;&#29983;&#25104;&#30340;&#20551;&#26032;&#38395;&#21644;&#20154;&#24037;&#25776;&#20889;&#30340;&#20551;&#26032;&#38395;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#65292;&#21253;&#25324;&#39118;&#26684;&#21644;&#22522;&#26412;&#24847;&#22270;&#26041;&#38754;&#30340;&#26174;&#33879;&#19981;&#21516;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#24050;&#30693;&#20154;&#24037;&#23459;&#20256;&#30340;&#39118;&#26684;&#21644;&#31574;&#30053;&#26469;&#29983;&#25104;&#35757;&#32451;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#33258;&#25105;&#25209;&#21028;&#30340;&#24207;&#21015;&#35757;&#32451;&#65292;&#24341;&#23548;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20197;&#30830;&#20445;&#25152;&#29983;&#25104;&#30340;&#25991;&#31456;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#36824;&#21253;&#25324;&#23459;&#20256;&#25216;&#24039;&#65292;&#22914;&#26435;&#23041;&#21628;&#21505;&#21644;&#26377;&#23459;&#20256;&#24615;&#30340;&#35821;&#35328;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;PropaNews&#65292;&#20854;&#20013;&#21253;&#21547;2,256&#20010;&#31034;&#20363;&#65292;&#24182;&#21457;&#24067;&#20197;&#20379;&#23558;&#26469;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;PropaNews&#35757;&#32451;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#22120;&#27604;&#37027;&#20123;&#22312;&#20256;&#32479;&#26426;&#22120;&#29983;&#25104;&#30340;&#20551;&#26032;&#38395;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#26816;&#27979;&#22120;&#26356;&#36866;&#29992;&#20110;&#26816;&#27979;&#20154;&#24037;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances in detecting fake news generated by neural models, their results are not readily applicable to effective detection of human-written disinformation. What limits the successful transfer between them is the sizable gap between machine-generated fake news and human-authored ones, including the notable differences in terms of style and underlying intent. With this in mind, we propose a novel framework for generating training examples that are informed by the known styles and strategies of human-authored propaganda. Specifically, we perform self-critical sequence training guided by natural language inference to ensure the validity of the generated articles, while also incorporating propaganda techniques, such as appeal to authority and loaded language. In particular, we create a new training dataset, PropaNews, with 2,256 examples, which we release for future use. Our experimental results show that fake news detectors trained on PropaNews are better at detecting human
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#24773;&#24863;&#32858;&#21512;&#33539;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#24773;&#24863;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#26041;&#38754;&#24773;&#24863;&#30456;&#24178;&#24615;&#65292;&#24182;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2110.08604</link><description>&lt;p&gt;
&#36890;&#36807;&#26412;&#22320;&#24773;&#24863;&#32858;&#21512;&#25913;&#36827;&#38544;&#24335;&#24773;&#24863;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Implicit Sentiment Learning via Local Sentiment Aggregation. (arXiv:2110.08604v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.08604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26412;&#22320;&#24773;&#24863;&#32858;&#21512;&#33539;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#24773;&#24863;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#24314;&#27169;&#26041;&#38754;&#24773;&#24863;&#30456;&#24178;&#24615;&#65292;&#24182;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#65288;ABSC&#65289;&#25581;&#31034;&#20102;&#19981;&#21516;&#26041;&#38754;&#20043;&#38388;&#24773;&#24863;&#26497;&#24615;&#30340;&#28508;&#22312;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#36825;&#19968;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#30456;&#37051;&#26041;&#38754;&#32463;&#24120;&#34920;&#29616;&#20986;&#30456;&#20284;&#24773;&#24863;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#26041;&#38754;&#24773;&#24863;&#30456;&#24178;&#24615;&#8221;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#39046;&#22495;&#36824;&#27809;&#26377;&#20805;&#20998;&#37325;&#35270;&#24314;&#27169;&#26041;&#38754;&#24773;&#24863;&#30456;&#24178;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26412;&#22320;&#24773;&#24863;&#32858;&#21512;&#33539;&#24335;&#65288;LSA&#65289;&#65292;&#21487;&#20197;&#20419;&#36827;&#31934;&#32454;&#30340;&#24773;&#24863;&#30456;&#24178;&#24615;&#24314;&#27169;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#21487;&#20197;&#25552;&#21462;&#32570;&#20047;&#26174;&#24335;&#24773;&#24863;&#25551;&#36848;&#30340;&#26041;&#38754;&#30340;&#38544;&#21547;&#24773;&#24863;&#12290;&#21033;&#29992;&#26799;&#24230;&#19979;&#38477;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#24046;&#20998;&#21152;&#26435;&#24773;&#24863;&#32858;&#21512;&#31383;&#21475;&#65292;&#26469;&#25351;&#23548;&#26041;&#38754;&#24773;&#24863;&#30456;&#24178;&#24615;&#30340;&#24314;&#27169;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LSA&#22312;&#23398;&#20064;&#24773;&#24863;&#30456;&#24178;&#24615;&#26041;&#38754;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24773;&#24863;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment classification (ABSC) has revealed the potential dependency of sentiment polarities among different aspects. Our study further explores this phenomenon, positing that adjacent aspects often exhibit similar sentiments, a concept we term "aspect sentiment coherency." We argue that the current research landscape has not fully appreciated the significance of modeling aspect sentiment coherency. To address this gap, we introduce a local sentiment aggregation paradigm (LSA) that facilitates fine-grained sentiment coherency modeling. This approach enables the extraction of implicit sentiments for aspects lacking explicit sentiment descriptions. Leveraging gradient descent, we design a differential-weighted sentiment aggregation window that guides the modeling of aspect sentiment coherency. Experimental results affirm the efficacy of LSA in learning sentiment coherency, as it achieves state-of-the-art performance across three public datasets, thus significantly enhancing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UNIQORN&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#22788;&#29702;RDF&#25968;&#25454;&#21644;&#25991;&#26412;&#65292;&#20351;&#29992;fine-tuned BERT&#27169;&#22411;&#20026;&#38382;&#39064;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#65292;&#24182;&#20351;&#29992;&#22270;&#31639;&#27861;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2108.08614</link><description>&lt;p&gt;
UNIQORN&#65306;&#32479;&#19968;&#30340;RDF&#30693;&#35782;&#22270;&#35889;&#19982;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v5 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.08614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;UNIQORN&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#23427;&#33021;&#22815;&#26080;&#32541;&#22320;&#22788;&#29702;RDF&#25968;&#25454;&#21644;&#25991;&#26412;&#65292;&#20351;&#29992;fine-tuned BERT&#27169;&#22411;&#20026;&#38382;&#39064;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#65292;&#24182;&#20351;&#29992;&#22270;&#31639;&#27861;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#22312;&#30693;&#35782;&#22270;&#35889;&#21644;&#20854;&#20182;RDF&#25968;&#25454;&#19978;&#24050;&#32463;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#35768;&#22810;&#20248;&#31168;&#30340;&#31995;&#32479;&#21487;&#20197;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#25110;&#30005;&#25253;&#26597;&#35810;&#25552;&#20379;&#28165;&#26224;&#30340;&#31572;&#26696;&#12290;&#20854;&#20013;&#19968;&#20123;&#31995;&#32479;&#23558;&#25991;&#26412;&#28304;&#20316;&#20026;&#38468;&#21152;&#35777;&#25454;&#32435;&#20837;&#22238;&#31572;&#36807;&#31243;&#65292;&#20294;&#19981;&#33021;&#35745;&#31639;&#20165;&#23384;&#22312;&#20110;&#25991;&#26412;&#20013;&#30340;&#31572;&#26696;&#12290;&#30456;&#21453;&#65292;IR&#21644;NLP&#31038;&#21306;&#30340;&#31995;&#32479;&#24050;&#32463;&#35299;&#20915;&#20102;&#26377;&#20851;&#25991;&#26412;&#30340;QA&#38382;&#39064;&#65292;&#20294;&#26159;&#36825;&#20123;&#31995;&#32479;&#20960;&#20046;&#19981;&#21033;&#29992;&#35821;&#20041;&#25968;&#25454;&#21644;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21487;&#20197;&#26080;&#32541;&#25805;&#20316;&#28151;&#21512;RDF&#25968;&#25454;&#38598;&#21644;&#25991;&#26412;&#35821;&#26009;&#24211;&#25110;&#21333;&#20010;&#26469;&#28304;&#30340;&#22797;&#26434;&#38382;&#39064;&#30340;&#31995;&#32479;&#65292;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#36827;&#34892;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;UNIQORN&#65292;&#36890;&#36807;&#20351;&#29992;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;BERT&#27169;&#22411;&#20174;RDF&#25968;&#25454;&#21644;/&#25110;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#35777;&#25454;&#26469;&#21160;&#24577;&#26500;&#24314;&#19978;&#19979;&#25991;&#22270;&#12290;&#32467;&#26524;&#22270;&#36890;&#24120;&#38750;&#24120;&#20016;&#23500;&#20294;&#39640;&#24230;&#22024;&#26434;&#12290;UNIQORN&#36890;&#36807;&#29992;&#20110;&#32452;Steiner&#26641;&#30340;&#22270;&#31639;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#36755;&#20837;&#65292;&#20174;&#32780;&#30830;&#23450;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#23376;&#22270;&#65292;&#36827;&#32780;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over knowledge graphs and other RDF data has been greatly advanced, with a number of good systems providing crisp answers for natural language questions or telegraphic queries. Some of these systems incorporate textual sources as additional evidence for the answering process, but cannot compute answers that are present in text alone. Conversely, systems from the IR and NLP communities have addressed QA over text, but such systems barely utilize semantic data and knowledge. This paper presents the first system for complex questions that can seamlessly operate over a mixture of RDF datasets and text corpora, or individual sources, in a unified framework. Our method, called UNIQORN, builds a context graph on-the-fly, by retrieving question-relevant evidences from the RDF data and/or a text corpus, using fine-tuned BERT models. The resulting graph is typically rich but highly noisy. UNIQORN copes with this input by a graph algorithm for Group Steiner Trees, that identifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#38889;&#35821;&#35821;&#26009;&#24211;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#36827;&#34892;&#24320;&#28304;&#25968;&#25454;&#38598;&#26500;&#24314;&#21644;&#21457;&#24067;&#20197;&#20419;&#36827;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2012.15621</link><description>&lt;p&gt;
&#24320;&#25918;&#24335;&#38889;&#35821;&#35821;&#26009;&#24211;: &#23454;&#29992;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open Korean Corpora: A Practical Report. (arXiv:2012.15621v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.15621
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#38889;&#35821;&#35821;&#26009;&#24211;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#36827;&#34892;&#24320;&#28304;&#25968;&#25454;&#38598;&#26500;&#24314;&#21644;&#21457;&#24067;&#20197;&#20419;&#36827;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38889;&#35821;&#22312;&#30740;&#31350;&#31038;&#21306;&#20013;&#24120;&#34987;&#31216;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#34429;&#28982;&#36825;&#31181;&#35828;&#27861;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#26159;&#27491;&#30830;&#30340;&#65292;&#20294;&#20063;&#22240;&#20026;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#23459;&#20256;&#21644;&#25972;&#29702;&#12290;&#26412;&#25991;&#25972;&#29702;&#21644;&#22238;&#39038;&#20102;&#19968;&#31995;&#21015;&#38889;&#35821;&#35821;&#26009;&#24211;&#65292;&#39318;&#20808;&#25551;&#36848;&#20102;&#26426;&#26500;&#32423;&#21035;&#30340;&#36164;&#28304;&#24320;&#21457;&#65292;&#28982;&#21518;&#36827;&#19968;&#27493;&#21015;&#20030;&#20102;&#19981;&#21516;&#31867;&#22411;&#20219;&#21153;&#30340;&#24403;&#21069;&#24320;&#28304;&#25968;&#25454;&#38598;&#28165;&#21333;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22914;&#20309;&#20026;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#36827;&#34892;&#24320;&#28304;&#25968;&#25454;&#38598;&#26500;&#24314;&#21644;&#21457;&#24067;&#20197;&#20419;&#36827;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Korean is often referred to as a low-resource language in the research community. While this claim is partially true, it is also because the availability of resources is inadequately advertised and curated. This work curates and reviews a list of Korean corpora, first describing institution-level resource development, then further iterate through a list of current open datasets for different types of tasks. We then propose a direction on how open-source dataset construction and releases should be done for less-resourced languages to promote research.
&lt;/p&gt;</description></item></channel></rss>