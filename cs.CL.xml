<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;1&#27604;&#29305;LLM&#21464;&#20307;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#36827;&#21046;&#21442;&#25968;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#26412;&#25928;&#30410;&#65292;&#23450;&#20041;&#20102;&#26032;&#30340;&#35757;&#32451;&#35268;&#24459;&#65292;&#20026;&#35774;&#35745;&#19987;&#38376;&#30828;&#20214;&#20248;&#21270;&#30340;1&#27604;&#29305;LLMs&#25171;&#24320;&#20102;&#22823;&#38376;</title><link>https://arxiv.org/abs/2402.17764</link><description>&lt;p&gt;
1&#27604;&#29305;LLM&#30340;&#26102;&#20195;&#65306;&#25152;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22343;&#20026;1.58&#27604;&#29305;
&lt;/p&gt;
&lt;p&gt;
The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17764
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;1&#27604;&#29305;LLM&#21464;&#20307;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#36827;&#21046;&#21442;&#25968;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#26412;&#25928;&#30410;&#65292;&#23450;&#20041;&#20102;&#26032;&#30340;&#35757;&#32451;&#35268;&#24459;&#65292;&#20026;&#35774;&#35745;&#19987;&#38376;&#30828;&#20214;&#20248;&#21270;&#30340;1&#27604;&#29305;LLMs&#25171;&#24320;&#20102;&#22823;&#38376;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#65292;&#22914;BitNet&#65292;&#27491;&#22312;&#20026;&#19968;&#20010;&#26032;&#26102;&#20195;&#30340;1&#27604;&#29305;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38138;&#24179;&#36947;&#36335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;1&#27604;&#29305;LLM&#21464;&#20307;&#65292;&#21363;BitNet b1.58&#65292;&#20854;&#20013;LLM&#30340;&#27599;&#20010;&#21333;&#20010;&#21442;&#25968;&#65288;&#25110;&#26435;&#37325;&#65289;&#22343;&#20026;&#19977;&#36827;&#21046;{-1, 0, 1}&#12290;&#23427;&#22312;&#22256;&#24785;&#24230;&#21644;&#26368;&#32456;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#19982;&#30456;&#21516;&#27169;&#22411;&#22823;&#23567;&#21644;&#35757;&#32451;&#26631;&#35760;&#30340;&#20840;&#31934;&#24230;&#65288;&#21363;FP16&#25110;BF16&#65289;Transformer LLM&#30456;&#21305;&#37197;&#65292;&#21516;&#26102;&#22312;&#24310;&#36831;&#12289;&#20869;&#23384;&#12289;&#21534;&#21520;&#37327;&#21644;&#33021;&#32791;&#26041;&#38754;&#26174;&#30528;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;1.58&#27604;&#29305;&#30340;LLM&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#30340;&#32553;&#25918;&#23450;&#24459;&#21644;&#35757;&#32451;&#26032;&#19968;&#20195;&#26082;&#39640;&#24615;&#33021;&#21448;&#20855;&#25104;&#26412;&#25928;&#30410;&#30340;LLMs&#30340;&#37197;&#26041;&#12290;&#27492;&#22806;&#65292;&#23427;&#23454;&#29616;&#20102;&#19968;&#20010;&#26032;&#30340;&#35745;&#31639;&#33539;&#24335;&#65292;&#24182;&#20026;&#35774;&#35745;&#19987;&#38376;&#38024;&#23545;1&#27604;&#29305;LLMs&#20248;&#21270;&#30340;&#29305;&#23450;&#30828;&#20214;&#25950;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17764v1 Announce Type: new  Abstract: Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#20102;&#22823;&#37327;&#28608;&#27963;&#29616;&#35937;&#65292;&#23427;&#20204;&#20855;&#26377;&#38750;&#24120;&#22823;&#30340;&#20540;&#24182;&#19988;&#22312;&#27169;&#22411;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17762</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#22823;&#37327;&#28608;&#27963;
&lt;/p&gt;
&lt;p&gt;
Massive Activations in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17762
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20986;&#29616;&#20102;&#22823;&#37327;&#28608;&#27963;&#29616;&#35937;&#65292;&#23427;&#20204;&#20855;&#26377;&#38750;&#24120;&#22823;&#30340;&#20540;&#24182;&#19988;&#22312;&#27169;&#22411;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#19968;&#20010;&#32463;&#39564;&#29616;&#35937;&#8212;&#8212;&#24456;&#23569;&#30340;&#28608;&#27963;&#23637;&#29616;&#20986;&#27604;&#20854;&#20182;&#28608;&#27963;&#26126;&#26174;&#26356;&#22823;&#30340;&#20540;&#65288;&#20363;&#22914;&#65292;&#22823;&#20986; 100,000 &#20493;&#65289;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#22823;&#37327;&#28608;&#27963;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#37327;&#28608;&#27963;&#22312;&#21508;&#31181;LLMs&#20013;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#23545;&#20854;&#20301;&#32622;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#20540;&#22522;&#26412;&#19978;&#19981;&#21463;&#36755;&#20837;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;LLMs&#20013;&#36215;&#21040;&#19981;&#21487;&#25110;&#32570;&#30340;&#20559;&#32622;&#39033;&#20316;&#29992;&#12290;&#31532;&#19977;&#65292;&#36825;&#20123;&#22823;&#37327;&#28608;&#27963;&#23548;&#33268;&#20851;&#27880;&#27010;&#29575;&#38598;&#20013;&#20110;&#20854;&#23545;&#24212;&#30340;&#26631;&#35760;&#65292;&#24182;&#36827;&#19968;&#27493;&#25104;&#20026;&#33258;&#27880;&#24847;&#36755;&#20986;&#20013;&#30340;&#38544;&#24335;&#20559;&#32622;&#39033;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#35270;&#35273;Transformer&#20013;&#30340;&#22823;&#37327;&#28608;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17762v1 Announce Type: new  Abstract: We observe an empirical phenomenon in Large Language Models (LLMs) -- very few activations exhibit significantly larger values than others (e.g., 100,000 times larger). We call them massive activations. First, we demonstrate the widespread existence of massive activations across various LLMs and characterize their locations. Second, we find their values largely stay constant regardless of the input, and they function as indispensable bias terms in LLMs. Third, these massive activations lead to the concentration of attention probabilities to their corresponding tokens, and further, implicit bias terms in the self-attention output. Last, we also study massive activations in Vision Transformers.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#26368;&#20339;&#23398;&#20064;&#30340;&#29702;&#35770;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25968;&#25454;&#21387;&#32553;&#27604;&#29575;&#26469;&#20248;&#21270;&#23398;&#20064;&#36807;&#31243;&#65292;&#26681;&#25454;&#23398;&#20064;&#23450;&#24459;&#25581;&#31034;&#20102;&#26368;&#20339;&#23398;&#20064;&#36807;&#31243;&#30340;&#29305;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#23450;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.17759</link><description>&lt;p&gt;
&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#20339;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Optimal Learning of Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#26368;&#20339;&#23398;&#20064;&#30340;&#29702;&#35770;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#25968;&#25454;&#21387;&#32553;&#27604;&#29575;&#26469;&#20248;&#21270;&#23398;&#20064;&#36807;&#31243;&#65292;&#26681;&#25454;&#23398;&#20064;&#23450;&#24459;&#25581;&#31034;&#20102;&#26368;&#20339;&#23398;&#20064;&#36807;&#31243;&#30340;&#29305;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#35813;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#23398;&#20064;&#30340;&#19968;&#33324;&#21407;&#21017;&#65292;&#26088;&#22312;&#20943;&#23569;&#23454;&#29616;&#20248;&#36234;&#24615;&#33021;&#25152;&#38656;&#30340;&#35757;&#32451;&#27493;&#39588;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;LMs&#30340;&#26368;&#20339;&#23398;&#20064;&#30340;&#29702;&#35770;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#22312;&#8220;LM&#35757;&#32451;&#20316;&#20026;&#26080;&#25439;&#21387;&#32553;&#8221;&#35270;&#22270;&#20013;&#26368;&#22823;&#21270;&#25968;&#25454;&#21387;&#32553;&#27604;&#29575;&#26469;&#20248;&#21270;LM&#23398;&#20064;&#30340;&#30446;&#26631;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19968;&#20010;&#23450;&#29702;&#65292;&#21517;&#20026;&#23398;&#20064;&#23450;&#24459;&#65292;&#25581;&#31034;&#20102;&#22312;&#25105;&#20204;&#30340;&#30446;&#26631;&#19979;&#26368;&#20339;&#23398;&#20064;&#36807;&#31243;&#20013;&#21160;&#24577;&#30340;&#29305;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#32447;&#24615;&#20998;&#31867;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#23450;&#29702;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#65292;LMs&#30340;&#26368;&#20339;&#23398;&#20064;&#20027;&#35201;&#28304;&#20110;&#25913;&#21892;LMs&#30340;&#32553;&#25918;&#23450;&#24459;&#30340;&#31995;&#25968;&#65292;&#20026;&#35774;&#35745;&#23454;&#38469;&#23398;&#20064;&#21152;&#36895;&#26041;&#27861;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#21069;&#26223;&#21644;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://aka.ms/LearningLaw&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17759v1 Announce Type: new  Abstract: This work studies the general principles of improving the learning of language models (LMs), which aims at reducing the necessary training steps for achieving superior performance. Specifically, we present a theory for the optimal learning of LMs. We first propose an objective that optimizes LM learning by maximizing the data compression ratio in an "LM-training-as-lossless-compression" view. Then, we derive a theorem, named Learning Law, to reveal the properties of the dynamics in the optimal learning process under our objective. The theorem is then validated by experiments on a linear classification and a real-world language modeling task. Finally, we empirically verify that the optimal learning of LMs essentially stems from the improvement of the coefficients in the scaling law of LMs, indicating great promise and significance for designing practical learning acceleration methods. Our code can be found at https://aka.ms/LearningLaw.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26426;&#22120;-&#20154;&#27969;&#31243;&#65292;&#22522;&#20110;LLM&#20195;&#29702;&#26550;&#26500;&#24182;&#23558;&#20854;&#23545;&#35805;&#22522;&#20110;&#20154;&#29289;&#35282;&#33394;&#21644;&#26102;&#38388;&#20107;&#20214;&#22270;&#36827;&#34892;&#22522;&#30784;&#65292;&#25104;&#21151;&#21019;&#24314;&#20102;LoCoMo&#25968;&#25454;&#38598;&#65292;&#20026;&#38750;&#24120;&#38271;&#26399;&#23545;&#35805;&#30340;&#30740;&#31350;&#22635;&#34917;&#20102;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.17753</link><description>&lt;p&gt;
&#35780;&#20272;LLM&#20195;&#29702;&#30340;&#38750;&#24120;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Evaluating Very Long-Term Conversational Memory of LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17753
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26426;&#22120;-&#20154;&#27969;&#31243;&#65292;&#22522;&#20110;LLM&#20195;&#29702;&#26550;&#26500;&#24182;&#23558;&#20854;&#23545;&#35805;&#22522;&#20110;&#20154;&#29289;&#35282;&#33394;&#21644;&#26102;&#38388;&#20107;&#20214;&#22270;&#36827;&#34892;&#22522;&#30784;&#65292;&#25104;&#21151;&#21019;&#24314;&#20102;LoCoMo&#25968;&#25454;&#38598;&#65292;&#20026;&#38750;&#24120;&#38271;&#26399;&#23545;&#35805;&#30340;&#30740;&#31350;&#22635;&#34917;&#20102;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#26041;&#38754;&#30340;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35780;&#20272;&#27169;&#22411;&#21709;&#24212;&#65292;&#20854;&#19978;&#19979;&#25991;&#36328;&#24230;&#19981;&#36229;&#36807;&#20116;&#20010;&#32842;&#22825;&#20250;&#35805;&#12290;&#23613;&#31649;&#38271;&#19978;&#19979;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#26377;&#25152;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#38750;&#24120;&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#26377;&#25928;&#24615;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#22120;-&#20154;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#26550;&#26500;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#38750;&#24120;&#38271;&#26399;&#23545;&#35805;&#65292;&#24182;&#23558;&#20854;&#23545;&#35805;&#22522;&#20110;&#20154;&#29289;&#35282;&#33394;&#21644;&#26102;&#38388;&#20107;&#20214;&#22270;&#36827;&#34892;&#22522;&#30784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36171;&#20104;&#27599;&#20010;&#20195;&#29702;&#20998;&#20139;&#21644;&#23545;&#22270;&#20687;&#20570;&#20986;&#21453;&#24212;&#30340;&#33021;&#21147;&#12290;&#29983;&#25104;&#30340;&#23545;&#35805;&#32463;&#20154;&#31867;&#27880;&#37322;&#21592;&#39564;&#35777;&#21644;&#32534;&#36753;&#65292;&#20197;&#30830;&#20445;&#38271;&#26399;&#19968;&#33268;&#24615;&#21644;&#19982;&#20107;&#20214;&#22270;&#30340;&#22522;&#30784;&#30456;&#32852;&#31995;&#12290;&#20351;&#29992;&#27492;&#27969;&#31243;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;LoCoMo&#65292;&#19968;&#20010;&#38750;&#24120;&#38271;&#26399;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#21253;&#21547;300&#36718;&#21644;&#24179;&#22343;9K&#20196;&#29260;&#65292;&#26368;&#22810;&#36798;&#21040;35&#20010;&#20250;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17753v1 Announce Type: cross  Abstract: Existing works on long-term open-domain dialogues focus on evaluating model responses within contexts spanning no more than five chat sessions. Despite advancements in long-context large language models (LLMs) and retrieval augmented generation (RAG) techniques, their efficacy in very long-term dialogues remains unexplored. To address this research gap, we introduce a machine-human pipeline to generate high-quality, very long-term dialogues by leveraging LLM-based agent architectures and grounding their dialogues on personas and temporal event graphs. Moreover, we equip each agent with the capability of sharing and reacting to images. The generated conversations are verified and edited by human annotators for long-range consistency and grounding to the event graphs. Using this pipeline, we collect LoCoMo, a dataset of very long-term conversations, each encompassing 300 turns and 9K tokens on avg., over up to 35 sessions. Based on LoCoM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20877;&#22312;&#32763;&#35793;&#27969;&#31243;&#35828;&#26126;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#20351;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#32763;&#35793;&#30456;&#20851;&#20219;&#21153;&#19978;&#36229;&#36234;&#24320;&#25918;&#26367;&#20195;&#26041;&#26696;&#24182;&#19982;&#36890;&#29992;&#23553;&#38381;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#31454;&#20105;&#12290;</title><link>https://arxiv.org/abs/2402.17733</link><description>&lt;p&gt;
Tower&#65306;&#19968;&#20010;&#38754;&#21521;&#32763;&#35793;&#30456;&#20851;&#20219;&#21153;&#30340;&#24320;&#25918;&#22810;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tower: An Open Multilingual Large Language Model for Translation-Related Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#35821;&#35328;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20877;&#22312;&#32763;&#35793;&#27969;&#31243;&#35828;&#26126;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#20351;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#32763;&#35793;&#30456;&#20851;&#20219;&#21153;&#19978;&#36229;&#36234;&#24320;&#25918;&#26367;&#20195;&#26041;&#26696;&#24182;&#19982;&#36890;&#29992;&#23553;&#38381;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35843;&#25972;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#28385;&#36275;&#32763;&#35793;&#27969;&#31243;&#20013;&#22810;&#20010;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#35821;&#35328;&#28151;&#21512;&#30340;&#21333;&#35821;&#21644;&#24179;&#34892;&#25968;&#25454;&#19978;&#36827;&#34892;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#21019;&#24314;TowerBase&#65292;&#28982;&#21518;&#22312;&#19982;&#32763;&#35793;&#27969;&#31243;&#30456;&#20851;&#30340;&#35828;&#26126;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#21019;&#24314;TowerInstruct&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#27169;&#22411;&#22312;&#20960;&#39033;&#19982;&#32763;&#35793;&#27969;&#31243;&#30456;&#20851;&#30340;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#24320;&#25918;&#24335;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#19982;&#36890;&#29992;&#23553;&#38381;&#24335;LLMs&#30456;&#31454;&#20105;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;Tower&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#19987;&#19994;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#32763;&#35793;&#29983;&#24577;&#31995;&#32479;&#19987;&#27880;&#20110;LLMs&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#21450;&#25105;&#20204;&#30340;&#22522;&#20934;&#27169;&#22411;&#29983;&#25104;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17733v1 Announce Type: new  Abstract: While general-purpose large language models (LLMs) demonstrate proficiency on multiple tasks within the domain of translation, approaches based on open LLMs are competitive only when specializing on a single task. In this paper, we propose a recipe for tailoring LLMs to multiple tasks present in translation workflows. We perform continued pretraining on a multilingual mixture of monolingual and parallel data, creating TowerBase, followed by finetuning on instructions relevant for translation processes, creating TowerInstruct. Our final model surpasses open alternatives on several tasks relevant to translation workflows and is competitive with general-purpose closed LLMs. To facilitate future research, we release the Tower models, our specialization dataset, an evaluation framework for LLMs focusing on the translation ecosystem, and a collection of model generations, including ours, on our benchmark.
&lt;/p&gt;</description></item><item><title>AmbigNLG&#26159;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#25351;&#20196;&#27169;&#31946;&#24615;&#25361;&#25112;&#30340;&#26032;&#20219;&#21153;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#20943;&#36731;&#25351;&#20196;&#20013;&#30340;&#27169;&#31946;&#24615;&#65292;&#25913;&#36827;&#20102;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#31361;&#20986;&#20102;&#28165;&#26224;&#21644;&#20855;&#20307;&#25351;&#20196;&#22312;&#25552;&#21319;LLM&#22312;NLG&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17717</link><description>&lt;p&gt;
AmbigNLG: &#35299;&#20915;NLG&#25351;&#20196;&#20013;&#30340;&#20219;&#21153;&#27169;&#31946;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
AmbigNLG: Addressing Task Ambiguity in Instruction for NLG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17717
&lt;/p&gt;
&lt;p&gt;
AmbigNLG&#26159;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#25351;&#20196;&#27169;&#31946;&#24615;&#25361;&#25112;&#30340;&#26032;&#20219;&#21153;&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#20943;&#36731;&#25351;&#20196;&#20013;&#30340;&#27169;&#31946;&#24615;&#65292;&#25913;&#36827;&#20102;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#31361;&#20986;&#20102;&#28165;&#26224;&#21644;&#20855;&#20307;&#25351;&#20196;&#22312;&#25552;&#21319;LLM&#22312;NLG&#20219;&#21153;&#20013;&#34920;&#29616;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AmbigNLG&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#20013;&#25351;&#20196;&#27169;&#31946;&#24615;&#25361;&#25112;&#30340;&#26032;&#20219;&#21153;&#12290;&#23613;&#31649;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#21644;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#21463;&#21040;&#29616;&#23454;&#25351;&#20196;&#20013;&#30340;&#27169;&#31946;&#24615;&#30340;&#26174;&#33879;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;AmbigNLG&#35797;&#22270;&#35782;&#21035;&#24182;&#20943;&#36731;&#36825;&#31181;&#27169;&#31946;&#24615;&#65292;&#26088;&#22312;&#31934;&#32454;&#21270;&#25351;&#20196;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#29992;&#25143;&#26399;&#26395;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;2,500&#20010;&#23454;&#20363;&#30340;&#25968;&#25454;&#38598;AmbigSNI-NLG&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#31946;&#24615;&#20998;&#31867;&#27861;&#65292;&#29992;&#20110;&#23545;&#25351;&#20196;&#20013;&#30340;&#27169;&#31946;&#24615;&#36827;&#34892;&#20998;&#31867;&#21644;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#65292;&#31361;&#20986;&#20102;&#28165;&#26224;&#21644;&#20855;&#20307;&#25351;&#20196;&#22312;&#22686;&#24378;LLM&#22312;NLG&#20219;&#21153;&#20013;&#34920;&#29616;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17717v1 Announce Type: new  Abstract: In this study, we introduce AmbigNLG, a new task designed to tackle the challenge of task ambiguity in instructions for Natural Language Generation (NLG) tasks. Despite the impressive capabilities of Large Language Models (LLMs) in understanding and executing a wide range of tasks through natural language interaction, their performance is significantly hindered by the ambiguity present in real-world instructions. To address this, AmbigNLG seeks to identify and mitigate such ambiguities, aiming to refine instructions to match user expectations better. We introduce a dataset, AmbigSNI-NLG, consisting of 2,500 instances, and develop an ambiguity taxonomy for categorizing and annotating instruction ambiguities. Our approach demonstrates substantial improvements in text generation quality, highlighting the critical role of clear and specific instructions in enhancing LLM performance in NLG tasks.
&lt;/p&gt;</description></item><item><title>transformers&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#37319;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#32780;&#38750;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.17709</link><description>&lt;p&gt;
&#22522;&#20110;&#26696;&#20363;&#36824;&#26159;&#22522;&#20110;&#35268;&#21017;&#65306;&#21464;&#21387;&#22120;&#22914;&#20309;&#36827;&#34892;&#25968;&#23398;&#35745;&#31639;&#65311;
&lt;/p&gt;
&lt;p&gt;
Case-Based or Rule-Based: How Do Transformers Do the Math?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17709
&lt;/p&gt;
&lt;p&gt;
transformers&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#37319;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#32780;&#38750;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#22788;&#29702;&#19968;&#20123;&#23545;&#20154;&#31867;&#26469;&#35828;&#31616;&#21333;&#19988;&#30452;&#35266;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#20363;&#22914;&#21152;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21487;&#20197;&#36731;&#26494;&#23398;&#20064;&#21152;&#27861;&#30340;&#22522;&#26412;&#35268;&#21017;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#20219;&#24847;&#38271;&#24230;&#30340;&#26032;&#38382;&#39064;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21364;&#38590;&#20197;&#20570;&#21040;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#21487;&#33021;&#20381;&#36182;&#20110;&#22312;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#30475;&#21040;&#30340;&#31867;&#20284;&#8220;&#26696;&#20363;&#8221;&#26469;&#33719;&#21462;&#24110;&#21161;&#12290;&#25105;&#20204;&#23558;&#36825;&#20004;&#31181;&#19981;&#21516;&#30340;&#25512;&#29702;&#26426;&#21046;&#23450;&#20041;&#20026;&#8220;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#8221;&#21644;&#8220;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#8221;&#12290;&#30001;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#25512;&#29702;&#23545;&#20110;&#33719;&#24471;&#31995;&#32479;&#21270;&#27010;&#25324;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#31350;&#21464;&#21387;&#22120;&#31350;&#31455;&#26159;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#36824;&#26159;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#20116;&#20010;&#25968;&#23398;&#20219;&#21153;&#30340;&#24178;&#39044;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#35748;&#21464;&#21387;&#22120;&#27491;&#22312;&#25191;&#34892;&#22522;&#20110;&#26696;&#20363;&#30340;&#25512;&#29702;&#65292;&#26080;&#35770;&#26159;&#21542;&#20351;&#29992;&#33609;&#31295;&#26412;&#65292;&#36825;&#19982;&#20043;&#21069;&#30340;&#35266;&#23519;&#32467;&#26524;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17709v1 Announce Type: new  Abstract: Despite the impressive performance in a variety of complex tasks, modern large language models (LLMs) still have trouble dealing with some math problems that are simple and intuitive for humans, such as addition. While we can easily learn basic rules of addition and apply them to new problems of any length, LLMs struggle to do the same. Instead, they may rely on similar "cases" seen in the training corpus for help. We define these two different reasoning mechanisms as "rule-based reasoning" and "case-based reasoning". Since rule-based reasoning is essential for acquiring the systematic generalization ability, we aim to explore exactly whether transformers use rule-based or case-based reasoning for math problems. Through carefully designed intervention experiments on five math tasks, we confirm that transformers are performing case-based reasoning, no matter whether scratchpad is used, which aligns with the previous observations that tran
&lt;/p&gt;</description></item><item><title>RAVEL&#25968;&#25454;&#38598;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MDAS&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#24320;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#65292;&#24378;&#35843;&#20102;&#36328;&#28608;&#27963;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17700</link><description>&lt;p&gt;
RAVEL: &#22312;&#35299;&#24320;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#26041;&#38754;&#35780;&#20272;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17700
&lt;/p&gt;
&lt;p&gt;
RAVEL&#25968;&#25454;&#38598;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;MDAS&#65292;&#35813;&#26041;&#27861;&#22312;&#35299;&#24320;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#65292;&#24378;&#35843;&#20102;&#36328;&#28608;&#27963;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#21035;&#31070;&#32463;&#20803;&#21442;&#19982;&#22810;&#20010;&#39640;&#32423;&#27010;&#24565;&#30340;&#34920;&#31034;&#12290;&#19981;&#21516;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#33021;&#25104;&#21151;&#35299;&#24320;&#36825;&#20123;&#35282;&#33394;&#65311;&#20026;&#20102;&#24110;&#21161;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RAVEL&#65288;Resolving Attribute-Value Entanglements in Language Models&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#22810;&#31181;&#29616;&#26377;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#32039;&#23494;&#25511;&#21046;&#30340;&#23450;&#37327;&#27604;&#36739;&#12290;&#25105;&#20204;&#21033;&#29992;&#30001;&#27492;&#20135;&#29983;&#30340;&#27010;&#24565;&#26694;&#26550;&#26469;&#23450;&#20041;&#26032;&#30340;Multi-task Distributed Alignment Search&#65288;MDAS&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25214;&#21040;&#28385;&#36275;&#22810;&#20010;&#22240;&#26524;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#12290;&#20197;Llama2-7B&#20316;&#20026;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#65292;MDAS&#22312;RAVEL&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#65292;&#23637;&#31034;&#20102;&#36229;&#36234;&#31070;&#32463;&#20803;&#32423;&#21035;&#20998;&#26512;&#20197;&#35782;&#21035;&#36328;&#28608;&#27963;&#30340;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#22312;https://github.com/explanare/ravel&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17700v1 Announce Type: new  Abstract: Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at https://github.com/explanare/ravel.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;NextLevelBERT&#65292;&#36890;&#36807;&#22312;&#26356;&#39640;&#32423;&#21035;&#30340;&#35821;&#20041;&#34920;&#31034;&#19978;&#36827;&#34892;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#65292;&#26377;&#25928;&#22788;&#29702;&#38271;&#25991;&#26723;&#29992;&#20363;&#65292;&#20855;&#26377;&#36229;&#36234;&#26356;&#22823;&#23884;&#20837;&#27169;&#22411;&#30340;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.17682</link><description>&lt;p&gt;
&#25506;&#31350;&#20351;&#29992;&#26356;&#39640;&#32423;&#21035;&#34920;&#31034;&#30340;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#22312;&#38271;&#25991;&#26723;&#20013;&#30340;&#24212;&#29992; - NextLevelBERT
&lt;/p&gt;
&lt;p&gt;
NextLevelBERT: Investigating Masked Language Modeling with Higher-Level Representations for Long Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17682
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;NextLevelBERT&#65292;&#36890;&#36807;&#22312;&#26356;&#39640;&#32423;&#21035;&#30340;&#35821;&#20041;&#34920;&#31034;&#19978;&#36827;&#34892;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#65292;&#26377;&#25928;&#22788;&#29702;&#38271;&#25991;&#26723;&#29992;&#20363;&#65292;&#20855;&#26377;&#36229;&#36234;&#26356;&#22823;&#23884;&#20837;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#65288;&#22823;&#22411;&#65289;&#35821;&#35328;&#27169;&#22411;&#22312;&#36807;&#21435;&#20960;&#24180;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#22522;&#30784;&#27880;&#24847;&#26426;&#21046;&#30340;&#20108;&#27425;&#25193;&#23637;&#65292;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#21512;&#29702;&#22788;&#29702;&#21457;&#29616;&#22312;&#20070;&#31821;&#20013;&#30340;&#38271;&#24207;&#21015;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NextLevelBERT&#65292;&#36825;&#26159;&#19968;&#20010;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#19981;&#26159;&#22312;&#26631;&#35760;&#19978;&#25805;&#20316;&#65292;&#32780;&#26159;&#22312;&#25991;&#26412;&#23884;&#20837;&#30340;&#24418;&#24335;&#20013;&#30340;&#26356;&#39640;&#32423;&#21035;&#35821;&#20041;&#34920;&#31034;&#19978;&#25805;&#20316;&#12290;&#25105;&#20204;&#23545;NextLevelBERT&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#39044;&#27979;&#25972;&#20010;&#34987;&#36974;&#34109;&#25991;&#26412;&#22359;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#24182;&#35780;&#20272;&#20135;&#29983;&#30340;&#25991;&#26723;&#21521;&#37327;&#22312;&#19977;&#31181;&#20219;&#21153;&#31867;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#65306;1&#65289;&#36890;&#36807;&#38646;&#26679;&#26412;&#25991;&#20214;&#23884;&#20837;&#36827;&#34892;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65292;2&#65289;&#38271;&#25991;&#26723;&#20998;&#31867;&#65292;3&#65289;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19979;&#19968;&#32423;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22788;&#29702;&#38271;&#25991;&#26723;&#29992;&#20363;&#65292;&#24182;&#19988;&#21482;&#35201;&#25152;&#38656;&#30340;&#32454;&#33410;&#27700;&#24179;&#19981;&#22826;&#39640;&#65292;&#23601;&#21487;&#20197;&#36229;&#36234;&#26356;&#22823;&#30340;&#23884;&#20837;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#27169;&#22411;&#21644;&#20195;&#30721; avai
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17682v1 Announce Type: new  Abstract: While (large) language models have significantly improved over the last years, they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism. To address this, we propose NextLevelBERT, a Masked Language Model operating not on tokens, but on higher-level semantic representations in the form of text embeddings. We pretrain NextLevelBERT to predict the vector representation of entire masked text chunks and evaluate the effectiveness of the resulting document vectors on three task types: 1) Semantic Textual Similarity via zero-shot document embeddings, 2) Long document classification, 3) Multiple-choice question answering. We find that next level Masked Language Modeling is an effective technique to tackle long-document use cases and can outperform much larger embedding models as long as the required level of detail is not too high. We make model and code avai
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25919;&#27835;&#19990;&#30028;&#35266;&#30340;&#21487;&#38752;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#20182;&#20204;&#30340;&#21487;&#38752;&#24615;&#38543;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#19988;&#22312;&#25919;&#31574;&#26041;&#26696;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;</title><link>https://arxiv.org/abs/2402.17649</link><description>&lt;p&gt;
&#36229;&#36234;&#25552;&#31034;&#33030;&#24369;&#24615;&#65306;&#35780;&#20272;LLMs&#20013;&#25919;&#27835;&#19990;&#30028;&#35266;&#30340;&#21487;&#38752;&#24615;&#21644;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond prompt brittleness: Evaluating the reliability and consistency of political worldviews in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17649
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25919;&#27835;&#19990;&#30028;&#35266;&#30340;&#21487;&#38752;&#24615;&#21644;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#20182;&#20204;&#30340;&#21487;&#38752;&#24615;&#38543;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#32780;&#22686;&#21152;&#65292;&#19988;&#22312;&#25919;&#31574;&#26041;&#26696;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24191;&#27867;&#31995;&#32479;&#20013;&#30340;&#20351;&#29992;&#65292;&#25105;&#20204;&#38656;&#35201;&#20102;&#35299;&#23427;&#20204;&#26159;&#21542;&#23884;&#20837;&#20102;&#29305;&#23450;&#30340;&#19990;&#30028;&#35266;&#20197;&#21450;&#36825;&#20123;&#35266;&#28857;&#25152;&#21453;&#26144;&#30340;&#20869;&#23481;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25253;&#21578;&#31216;&#65292;&#24403;&#29992;&#25919;&#27835;&#38382;&#21367;&#36827;&#34892;&#25552;&#31034;&#26102;&#65292;LLMs&#34920;&#29616;&#20986;&#24038;&#20542;&#33258;&#30001;&#20542;&#21521;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#20542;&#21521;&#26159;&#21542;&#21487;&#38752;&#65288;&#23545;&#25552;&#31034;&#21464;&#21270;&#31283;&#20581;&#65289;&#20197;&#21450;&#36825;&#31181;&#20542;&#21521;&#26159;&#21542;&#22312;&#25919;&#31574;&#21644;&#25919;&#27835;&#20542;&#21521;&#19978;&#20445;&#25345;&#19968;&#33268;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#22522;&#20110;&#25910;&#38598;&#33258;&#19971;&#20010;&#27431;&#30431;&#22269;&#23478;&#30340;&#36873;&#20030;&#24314;&#35758;&#38382;&#21367;&#24182;&#26631;&#27880;&#20026;&#25919;&#31574;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#19978;LLMs&#22312;&#25919;&#27835;&#22768;&#26126;&#19978;&#31435;&#22330;&#30340;&#21487;&#38752;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21442;&#25968;&#20174;7B&#21040;70B&#30340;LLMs&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#38543;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#26356;&#22823;&#30340;&#27169;&#22411;&#26174;&#31034;&#24635;&#20307;&#19978;&#19982;&#24038;&#20542;&#25919;&#20826;&#26356;&#24378;&#30340;&#19968;&#33268;&#24615;&#65292;&#20294;&#22312;&#25919;&#31574;&#26041;&#26696;&#20013;&#26377;&#25152;&#19981;&#21516;&#65306;&#23427;&#20204;&#34920;&#29616;&#20986;&#65288;&#24038;&#20542;&#65289;&#31215;&#26497;&#30340;&#31435;&#22330;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17649v1 Announce Type: new  Abstract: Due to the widespread use of large language models (LLMs) in ubiquitous systems, we need to understand whether they embed a specific worldview and what these views reflect. Recent studies report that, prompted with political questionnaires, LLMs show left-liberal leanings. However, it is as yet unclear whether these leanings are reliable (robust to prompt variations) and whether the leaning is consistent across policies and political leaning. We propose a series of tests which assess the reliability and consistency of LLMs' stances on political statements based on a dataset of voting-advice questionnaires collected from seven EU countries and annotated for policy domains. We study LLMs ranging in size from 7B to 70B parameters and find that their reliability increases with parameter count. Larger models show overall stronger alignment with left-leaning parties but differ among policy programs: They evince a (left-wing) positive stance to
&lt;/p&gt;</description></item><item><title>SongComposer&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27468;&#26354;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#31526;&#21495;&#21270;&#30340;&#27468;&#26354;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;LLM&#21487;&#20197;&#26126;&#30830;&#21019;&#20316;&#27468;&#26354;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17645</link><description>&lt;p&gt;
SongComposer&#65306;&#19968;&#31181;&#29992;&#20110;&#27468;&#26354;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#27468;&#35789;&#21644;&#26059;&#24459;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
SongComposer: A Large Language Model for Lyric and Melody Composition in Song Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17645
&lt;/p&gt;
&lt;p&gt;
SongComposer&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27468;&#26354;&#29983;&#25104;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#31526;&#21495;&#21270;&#30340;&#27468;&#26354;&#34920;&#31034;&#65292;&#23454;&#29616;&#20102;LLM&#21487;&#20197;&#26126;&#30830;&#21019;&#20316;&#27468;&#26354;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SongComposer&#65292;&#19968;&#20010;&#20026;&#27468;&#26354;&#21019;&#20316;&#32780;&#35774;&#35745;&#30340;&#21019;&#26032;&#22411;LLM&#12290;&#23427;&#33021;&#22815;&#29702;&#35299;&#21644;&#29983;&#25104;&#27468;&#26354;&#20013;&#30340;&#26059;&#24459;&#21644;&#27468;&#35789;&#65292;&#36890;&#36807;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#22312;&#31526;&#21495;&#21270;&#30340;&#27468;&#26354;&#34920;&#31034;&#20013;&#29983;&#25104;&#12290;&#29616;&#26377;&#30340;&#19982;&#38899;&#20048;&#30456;&#20851;&#30340;LLM&#23558;&#38899;&#20048;&#35270;&#20026;&#37327;&#21270;&#30340;&#38899;&#39057;&#20449;&#21495;&#65292;&#32780;&#36825;&#31181;&#38544;&#24335;&#32534;&#30721;&#23548;&#33268;&#20102;&#32534;&#30721;&#25928;&#29575;&#20302;&#19979;&#21644;&#28789;&#27963;&#24615;&#24046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31526;&#21495;&#21270;&#30340;&#27468;&#26354;&#34920;&#31034;&#65292;&#36825;&#26159;&#20154;&#31867;&#20026;&#38899;&#20048;&#35774;&#35745;&#30340;&#25104;&#29087;&#21644;&#39640;&#25928;&#30340;&#26041;&#24335;&#65292;&#24182;&#20351;LLM&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#26126;&#30830;&#22320;&#21019;&#20316;&#27468;&#26354;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#32452;&#35774;&#35745;&#65292;&#29992;&#20110;&#26684;&#24335;&#21270;&#27468;&#35789;&#21644;&#26059;&#24459;&#20013;&#30340;&#19977;&#20010;&#38899;&#31526;&#23646;&#24615;&#65288;&#38899;&#39640;&#12289;&#25345;&#32493;&#26102;&#38388;&#21644;&#20241;&#27490;&#26102;&#38388;&#65289;&#65292;&#20174;&#32780;&#20445;&#35777;LLM&#23545;&#38899;&#20048;&#31526;&#21495;&#30340;&#27491;&#30830;&#29702;&#35299;&#65292;&#24182;&#23454;&#29616;&#27468;&#35789;&#21644;&#26059;&#24459;&#20043;&#38388;&#30340;&#31934;&#30830;&#23545;&#40784;&#12290;&#20026;&#20102;&#21521;LLM&#28748;&#36755;&#22522;&#26412;&#30340;&#38899;&#20048;&#29702;&#35299;&#65292;&#25105;&#20204;&#31934;&#24515;&#25910;&#38598;&#20102;SongCompose-PT&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#27468;&#26354;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;&#27468;&#35789;&#12289;&#26059;&#24459;&#21644;&#25104;&#23545;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17645v1 Announce Type: cross  Abstract: We present SongComposer, an innovative LLM designed for song composition. It could understand and generate melodies and lyrics in symbolic song representations, by leveraging the capability of LLM. Existing music-related LLM treated the music as quantized audio signals, while such implicit encoding leads to inefficient encoding and poor flexibility. In contrast, we resort to symbolic song representation, the mature and efficient way humans designed for music, and enable LLM to explicitly compose songs like humans. In practice, we design a novel tuple design to format lyric and three note attributes (pitch, duration, and rest duration) in the melody, which guarantees the correct LLM understanding of musical symbols and realizes precise alignment between lyrics and melody. To impart basic music understanding to LLM, we carefully collected SongCompose-PT, a large-scale song pretraining dataset that includes lyrics, melodies, and paired ly
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;QRData&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32479;&#35745;&#21644;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#26368;&#24378;&#27169;&#22411;GPT-4&#22312;&#35813;&#27979;&#35797;&#20013;&#20934;&#30830;&#29575;&#20026;58&#65285;&#65292;&#23384;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.17644</link><description>&lt;p&gt;
LLMs&#26159;&#21542;&#20855;&#22791;&#22522;&#20110;&#25968;&#25454;&#30340;&#32479;&#35745;&#21644;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#65311;&#29992;&#25968;&#25454;&#23545;&#20808;&#36827;&#30340;&#23450;&#37327;&#25512;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;QRData&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32479;&#35745;&#21644;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#26368;&#24378;&#27169;&#22411;GPT-4&#22312;&#35813;&#27979;&#35797;&#20013;&#20934;&#30830;&#29575;&#20026;58&#65285;&#65292;&#23384;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#25512;&#29702;&#26159;&#20998;&#26512;&#25968;&#25454;&#30340;&#20851;&#38190;&#25216;&#33021;&#65292;&#28982;&#32780;&#23545;&#36825;&#31181;&#33021;&#21147;&#30340;&#35780;&#20272;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Quantitative Reasoning with Data&#65288;QRData&#65289;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32479;&#35745;&#21644;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#19982;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#25324;&#19968;&#20010;&#31934;&#24515;&#26500;&#24314;&#30340;&#21253;&#21547;&#26469;&#33258;&#25945;&#31185;&#20070;&#12289;&#22312;&#32447;&#23398;&#20064;&#26448;&#26009;&#21644;&#23398;&#26415;&#35770;&#25991;&#30340;&#25968;&#25454;&#34920;&#30340;411&#20010;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#27604;&#36739;&#27169;&#22411;&#22312;&#25968;&#25454;&#21644;&#25991;&#26412;&#19978;&#30340;&#23450;&#37327;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#36824;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#21253;&#21547;290&#20010;&#20165;&#25991;&#26412;&#38382;&#39064;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#65292;&#21363;QRText&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#22522;&#20110;&#31243;&#24207;&#25512;&#29702;&#21644;&#20195;&#29702;&#25512;&#29702;&#26041;&#27861;&#65292;&#21253;&#25324;Chain-of-Thought&#12289;Program-of-Thoughts&#12289;ReAct&#21644;&#20195;&#30721;&#35299;&#37322;&#22120;&#36741;&#21161;&#31561;&#22312;&#21508;&#31181;&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#12290;&#26368;&#24378;&#30340;&#27169;&#22411;GPT-4&#30340;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;58&#65285;&#65292;&#20294;&#20173;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17644v1 Announce Type: cross  Abstract: Quantitative reasoning is a critical skill to analyze data, yet the assessment of such ability remains limited. To address this gap, we introduce the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate Large Language Models' capability in statistical and causal reasoning with real-world data. The benchmark comprises a carefully constructed dataset of 411 questions accompanied by data sheets from textbooks, online learning materials, and academic papers. To compare models' quantitative reasoning abilities on data and text, we enrich the benchmark with an auxiliary set of 290 text-only questions, namely QRText. We evaluate natural language reasoning, program-based reasoning, and agent reasoning methods including Chain-of-Thought, Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models. The strongest model GPT-4 achieves an accuracy of 58%, which has a large room for improvement. Among open-source
&lt;/p&gt;</description></item><item><title>&#21464;&#20998;&#23398;&#20064;&#22312;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#20013;&#23637;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#25928;&#26524;&#65292;IVON&#20248;&#21270;&#22120;&#22312;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#26102;&#20960;&#20046;&#33021;&#19982;Adam&#30456;&#23218;&#32654;&#29978;&#33267;&#32988;&#36807;&#23427;&#65292;&#19988;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26356;&#20934;&#30830;&#65292;&#23545;&#27169;&#22411;&#24494;&#35843;&#12289;&#27867;&#21270;&#35823;&#24046;&#39044;&#27979;&#21644;&#25968;&#25454;&#25935;&#24863;&#24615;&#20272;&#35745;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.17641</link><description>&lt;p&gt;
&#21464;&#20998;&#23398;&#20064;&#23545;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#26377;&#25928;
&lt;/p&gt;
&lt;p&gt;
Variational Learning is Effective for Large Deep Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17641
&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#23398;&#20064;&#22312;&#22823;&#22411;&#28145;&#24230;&#32593;&#32476;&#20013;&#23637;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#25928;&#26524;&#65292;IVON&#20248;&#21270;&#22120;&#22312;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#26102;&#20960;&#20046;&#33021;&#19982;Adam&#30456;&#23218;&#32654;&#29978;&#33267;&#32988;&#36807;&#23427;&#65292;&#19988;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26356;&#20934;&#30830;&#65292;&#23545;&#27169;&#22411;&#24494;&#35843;&#12289;&#27867;&#21270;&#35823;&#24046;&#39044;&#27979;&#21644;&#25968;&#25454;&#25935;&#24863;&#24615;&#20272;&#35745;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#22823;&#37327;&#23454;&#35777;&#35777;&#25454;&#65292;&#21453;&#39539;&#20102;&#21464;&#20998;&#23398;&#20064;&#23545;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#26080;&#25928;&#30340;&#26222;&#36941;&#30475;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#21517;&#20026;Improved Variational Online Newton (IVON)&#30340;&#20248;&#21270;&#22120;&#65292;&#22312;&#35757;&#32451;&#22823;&#22411;&#32593;&#32476;&#65288;&#22914;GPT-2&#21644;ResNets&#65289;&#26102;&#22987;&#32456;&#33021;&#22815;&#19982;Adam&#30456;&#21305;&#37197;&#25110;&#32988;&#36807;&#23427;&#12290;IVON&#30340;&#35745;&#31639;&#25104;&#26412;&#20960;&#20046;&#19982;Adam&#30456;&#21516;&#65292;&#20294;&#20854;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#26356;&#22909;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;IVON&#30340;&#20960;&#31181;&#26032;&#29992;&#20363;&#65292;&#20854;&#20013;&#25105;&#20204;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#27169;&#22411;&#21512;&#24182;&#65292;&#22312;&#20934;&#30830;&#39044;&#27979;&#27867;&#21270;&#35823;&#24046;&#21644;&#24544;&#23454;&#20272;&#35745;&#23545;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#26041;&#38754;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#22823;&#37327;&#25903;&#25345;&#21464;&#20998;&#23398;&#20064;&#26377;&#25928;&#24615;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17641v1 Announce Type: cross  Abstract: We give extensive empirical evidence against the common belief that variational learning is ineffective for large neural networks. We show that an optimizer called Improved Variational Online Newton (IVON) consistently matches or outperforms Adam for training large networks such as GPT-2 and ResNets from scratch. IVON's computational costs are nearly identical to Adam but its predictive uncertainty is better. We show several new use cases of IVON where we improve fine-tuning and model merging in Large Language Models, accurately predict generalization error, and faithfully estimate sensitivity to data. We find overwhelming evidence in support of effectiveness of variational learning.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#26032;&#22411;&#22522;&#20934;YTSeg&#65292;&#38024;&#23545;&#21475;&#35821;&#20869;&#23481;&#25552;&#20986;&#39640;&#25928;&#30340;&#20998;&#23618;&#20998;&#21106;&#27169;&#22411;MiniSeg&#65292;&#23558;&#25991;&#26412;&#20998;&#21106;&#27010;&#24565;&#25193;&#23637;&#21040;&#26356;&#23454;&#29992;&#30340;&#8220;&#26234;&#33021;&#31456;&#33410;&#21270;&#8221;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.17633</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#20998;&#21106;&#21040;&#26234;&#33021;&#31456;&#33410;&#21270;&#65306;&#29992;&#20110;&#26500;&#24314;&#35270;&#39057;&#36716;&#24405;&#30340;&#26032;&#22411;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
From Text Segmentation to Smart Chaptering: A Novel Benchmark for Structuring Video Transcriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17633
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#26032;&#22411;&#22522;&#20934;YTSeg&#65292;&#38024;&#23545;&#21475;&#35821;&#20869;&#23481;&#25552;&#20986;&#39640;&#25928;&#30340;&#20998;&#23618;&#20998;&#21106;&#27169;&#22411;MiniSeg&#65292;&#23558;&#25991;&#26412;&#20998;&#21106;&#27010;&#24565;&#25193;&#23637;&#21040;&#26356;&#23454;&#29992;&#30340;&#8220;&#26234;&#33021;&#31456;&#33410;&#21270;&#8221;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#21106;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20854;&#20013;&#25991;&#26723;&#34987;&#20998;&#21106;&#20026;&#36830;&#32493;&#30340;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#20808;&#21069;&#30740;&#31350;&#21463;&#21040;&#20102;&#26377;&#38480;&#25968;&#25454;&#38598;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#35201;&#20040;&#35268;&#27169;&#23567;&#65292;&#35201;&#20040;&#26159;&#21512;&#25104;&#30340;&#65292;&#35201;&#20040;&#21482;&#21253;&#21547;&#32467;&#26500;&#33391;&#22909;&#30340;&#25991;&#20214;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934; YTSeg &#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#37325;&#28857;&#20851;&#27880;&#21475;&#35821;&#20869;&#23481;&#65292;&#36825;&#31181;&#20869;&#23481;&#22266;&#26377;&#22320;&#26356;&#21152;&#26080;&#32467;&#26500;&#21270;&#65292;&#19988;&#22312;&#20027;&#39064;&#21644;&#32467;&#26500;&#19978;&#26356;&#21152;&#22810;&#26679;&#21270;&#12290;&#20316;&#20026;&#36825;&#39033;&#24037;&#20316;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#23618;&#20998;&#21106;&#27169;&#22411; MiniSeg&#65292;&#23427;&#32988;&#36807;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#22522;&#32447;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25991;&#26412;&#20998;&#21106;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#19968;&#20010;&#26356;&#23454;&#29992;&#30340;&#8220;&#26234;&#33021;&#31456;&#33410;&#21270;&#8221;&#20219;&#21153;&#65292;&#28041;&#21450;&#23545;&#26080;&#32467;&#26500;&#21270;&#20869;&#23481;&#30340;&#20998;&#21106;&#65292;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#27573;&#33853;&#26631;&#39064;&#65292;&#20197;&#21450;&#27169;&#22411;&#30340;&#28508;&#22312;&#23454;&#26102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17633v1 Announce Type: new  Abstract: Text segmentation is a fundamental task in natural language processing, where documents are split into contiguous sections. However, prior research in this area has been constrained by limited datasets, which are either small in scale, synthesized, or only contain well-structured documents. In this paper, we address these limitations by introducing a novel benchmark YTSeg focusing on spoken content that is inherently more unstructured and both topically and structurally diverse. As part of this work, we introduce an efficient hierarchical segmentation model MiniSeg, that outperforms state-of-the-art baselines. Lastly, we expand the notion of text segmentation to a more practical "smart chaptering" task that involves the segmentation of unstructured content, the generation of meaningful segment titles, and a potential real-time application of the models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;InFusE&#65292;&#29992;&#20110;&#38754;&#21521;&#22810;&#26679;&#21270;&#25688;&#35201;&#20219;&#21153;&#30340;&#32454;&#31890;&#24230;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20449;&#24565;&#35780;&#20272;&#65292;&#24182;&#24341;&#20837;&#20102;&#21253;&#25324;&#38271;&#31687;&#25688;&#35201;&#21644;&#22810;&#26679;&#21270;&#25688;&#35201;&#20219;&#21153;&#22312;&#20869;&#30340;&#26032;&#22522;&#20934;&#25968;&#25454;&#38598;DiverSumm&#12290;</title><link>https://arxiv.org/abs/2402.17630</link><description>&lt;p&gt;
&#38754;&#21521;&#22810;&#26679;&#21270;&#25688;&#35201;&#20219;&#21153;&#30340;&#32454;&#31890;&#24230;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20449;&#24565;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Fine-Grained Natural Language Inference Based Faithfulness Evaluation for Diverse Summarisation Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17630
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;InFusE&#65292;&#29992;&#20110;&#38754;&#21521;&#22810;&#26679;&#21270;&#25688;&#35201;&#20219;&#21153;&#30340;&#32454;&#31890;&#24230;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20449;&#24565;&#35780;&#20272;&#65292;&#24182;&#24341;&#20837;&#20102;&#21253;&#25324;&#38271;&#31687;&#25688;&#35201;&#21644;&#22810;&#26679;&#21270;&#25688;&#35201;&#20219;&#21153;&#22312;&#20869;&#30340;&#26032;&#22522;&#20934;&#25968;&#25454;&#38598;DiverSumm&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#29616;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#27169;&#22411;&#26469;&#35780;&#20272;&#25688;&#35201;&#24544;&#23454;&#24615;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#25351;&#20986;&#36825;&#20123;&#26041;&#27861;&#22312;&#32771;&#34385;&#21069;&#25552;&#21644;&#20551;&#35774;&#30340;&#32454;&#31890;&#24230;&#32423;&#21035;&#26102;&#23384;&#22312;&#19981;&#36275;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#34987;&#35270;&#20026;&#20551;&#35774;&#30340;&#36739;&#23567;&#20869;&#23481;&#21333;&#20803;&#26159;&#19968;&#20010;&#21477;&#23376;&#65292;&#32780;&#21069;&#25552;&#30001;&#22266;&#23450;&#25968;&#37327;&#30340;&#25991;&#26723;&#21477;&#23376;&#32452;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;InFusE&#65292;&#23427;&#20351;&#29992;&#21487;&#21464;&#30340;&#21069;&#25552;&#22823;&#23567;&#65292;&#24182;&#23558;&#25688;&#35201;&#21477;&#23376;&#31616;&#21270;&#20026;&#26356;&#30701;&#30340;&#20551;&#35774;&#12290;&#19982;&#20197;&#24448;&#20391;&#37325;&#20110;&#21333;&#20010;&#30701;&#25991;&#26723;&#25688;&#35201;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29992;&#20110;&#22810;&#26679;&#21270;&#25688;&#35201;&#20219;&#21153;&#30340;&#22522;&#20110;NLI&#30340;&#24544;&#23454;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;DiverSumm&#65292;&#20854;&#20013;&#21253;&#25324;&#38271;&#31687;&#25688;&#35201;&#65288;&#38271;&#25991;&#26723;&#21644;&#25688;&#35201;&#65289;&#21644;&#22810;&#26679;&#21270;&#25688;&#35201;&#20219;&#21153;&#65288;&#20363;&#22914;&#20250;&#35758;&#21644;&#22810;&#25991;&#26723;&#25688;&#35201;&#65289;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;InFusE&#22312;&#19981;&#21516;&#25688;&#35201;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17630v1 Announce Type: new  Abstract: We study existing approaches to leverage off-the-shelf Natural Language Inference (NLI) models for the evaluation of summary faithfulness and argue that these are sub-optimal due to the granularity level considered for premises and hypotheses. That is, the smaller content unit considered as hypothesis is a sentence and premises are made up of a fixed number of document sentences. We propose a novel approach, namely InFusE, that uses a variable premise size and simplifies summary sentences into shorter hypotheses. Departing from previous studies which focus on single short document summarisation, we analyse NLI based faithfulness evaluation for diverse summarisation tasks. We introduce DiverSumm, a new benchmark comprising long form summarisation (long documents and summaries) and diverse summarisation tasks (e.g., meeting and multi-document summarisation). In experiments, InFusE obtains superior performance across the different summarisa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#29992;&#20110;&#20855;&#26377;&#32416;&#27491;&#21453;&#39304;&#30340;&#33258;&#21160;&#20889;&#20316;&#35780;&#20272;&#65292;&#26088;&#22312;&#22635;&#34917;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;AWE&#21644;GEC&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.17613</link><description>&lt;p&gt;
&#20855;&#26377;&#32416;&#27491;&#21453;&#39304;&#30340;&#31070;&#32463;&#33258;&#21160;&#20889;&#20316;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Neural Automated Writing Evaluation with Corrective Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#29992;&#20110;&#20855;&#26377;&#32416;&#27491;&#21453;&#39304;&#30340;&#33258;&#21160;&#20889;&#20316;&#35780;&#20272;&#65292;&#26088;&#22312;&#22635;&#34917;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;AWE&#21644;GEC&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#21644;&#25945;&#23398;&#20013;&#65292;&#21033;&#29992;&#25216;&#26415;&#24050;&#21464;&#24471;&#26222;&#36941;&#12290;&#23545;&#20110;&#20889;&#20316;&#35780;&#20272;&#65292;&#33258;&#21160;&#20889;&#20316;&#35780;&#20272;&#65288;AWE&#65289;&#21644;&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#65288;GEC&#65289;&#24050;&#25104;&#20026;&#22686;&#24378;&#20889;&#20316;&#33021;&#21147;&#12289;&#21521;&#23398;&#20064;&#32773;&#25552;&#20379;&#21363;&#26102;&#20010;&#24615;&#21270;&#21453;&#39304;&#30340;&#27969;&#34892;&#21644;&#26377;&#25928;&#26041;&#27861;&#12290;&#20511;&#21161;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#21147;&#37327;&#65292;AWE&#21644;GEC&#31995;&#32479;&#24050;&#20998;&#21035;&#24320;&#21457;&#65292;&#20197;&#20026;&#35821;&#35328;&#23398;&#20064;&#32773;&#25552;&#20379;&#33258;&#21160;&#26657;&#27491;&#21453;&#39304;&#21644;&#26356;&#20934;&#30830;&#12289;&#26080;&#20559;&#30340;&#35780;&#20998;&#65292;&#21542;&#21017;&#36825;&#20123;&#23558;&#34987;&#35780;&#23457;&#21592;&#25152;&#20027;&#35266;&#21028;&#26029;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#31995;&#32479;&#65292;&#29992;&#20110;&#20855;&#26377;&#32416;&#27491;&#21453;&#39304;&#30340;&#33258;&#21160;&#20889;&#20316;&#35780;&#20272;&#65292;&#20316;&#20026;&#22635;&#34917;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;AWE&#21644;GEC&#32467;&#26524;&#24046;&#36317;&#30340;&#25163;&#27573;&#12290;&#35813;&#31995;&#32479;&#20351;&#35821;&#35328;&#23398;&#20064;&#32773;&#33021;&#22815;&#27169;&#25311;&#35770;&#25991;&#20889;&#20316;&#27979;&#35797;&#65306;&#23398;&#29983;&#25776;&#20889;&#24182;&#25552;&#20132;&#35770;&#25991;&#65292;&#31995;&#32479;&#25552;&#20379;&#35780;&#20272;&#21644;&#32416;&#27491;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17613v1 Announce Type: new  Abstract: The utilization of technology in second language learning and teaching has become ubiquitous. For the assessment of writing specifically, automated writing evaluation (AWE) and grammatical error correction (GEC) have become immensely popular and effective methods for enhancing writing proficiency and delivering instant and individualized feedback to learners. By leveraging the power of natural language processing (NLP) and machine learning algorithms, AWE and GEC systems have been developed separately to provide language learners with automated corrective feedback and more accurate and unbiased scoring that would otherwise be subject to examiners. In this paper, we propose an integrated system for automated writing evaluation with corrective feedback as a means of bridging the gap between AWE and GEC results for second language learners. This system enables language learners to simulate the essay writing tests: a student writes and submi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21521;T5&#27169;&#22411;&#24341;&#20837;&#35821;&#35328;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#22312;&#32467;&#26500;&#35821;&#35328;&#23646;&#24615;&#30340;&#20013;&#38388;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#25913;&#21892;&#23545;&#21477;&#23376;&#32423;&#22797;&#26434;&#24230;&#30340;&#39044;&#27979;&#20219;&#21153;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2402.17608</link><description>&lt;p&gt;
&#35821;&#35328;&#30693;&#35782;&#21487;&#20197;&#22686;&#24378;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65288;&#22914;&#26524;&#20320;&#20801;&#35768;&#30340;&#35805;&#65289;
&lt;/p&gt;
&lt;p&gt;
Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17608
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21521;T5&#27169;&#22411;&#24341;&#20837;&#35821;&#35328;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#22312;&#32467;&#26500;&#35821;&#35328;&#23646;&#24615;&#30340;&#20013;&#38388;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#25913;&#21892;&#23545;&#21477;&#23376;&#32423;&#22797;&#26434;&#24230;&#30340;&#39044;&#27979;&#20219;&#21153;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#22686;&#21152;&#39044;&#20808;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;T5&#27169;&#22411;&#65292;&#19982;&#35821;&#35328;&#30693;&#35782;&#26469;&#39044;&#27979;&#30446;&#26631;&#20219;&#21153;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22312;&#20013;&#38388;&#20219;&#21153;&#19978;&#24494;&#35843;T5&#27169;&#22411;&#65292;&#35813;&#20219;&#21153;&#39044;&#27979;&#21477;&#23376;&#30340;&#32467;&#26500;&#35821;&#35328;&#23646;&#24615;&#65292;&#26159;&#21542;&#20250;&#25913;&#21464;&#20854;&#22312;&#39044;&#27979;&#21477;&#23376;&#32423;&#22797;&#26434;&#24230;&#30340;&#30446;&#26631;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28085;&#30422;&#20102;&#22312;&#24847;&#22823;&#21033;&#35821;&#21644;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#21508;&#31181;&#23454;&#39564;&#65292;&#37319;&#29992;&#20102;&#19981;&#21516;&#35268;&#27169;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;T5&#27169;&#22411;&#12290;&#23545;&#20004;&#31181;&#35821;&#35328;&#20197;&#21450;&#36328;&#35821;&#35328;&#37197;&#32622;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#35821;&#35328;&#23398;&#21160;&#26426;&#30340;&#20013;&#38388;&#24494;&#35843;&#36890;&#24120;&#23545;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#24403;&#24212;&#29992;&#20110;&#36739;&#23567;&#27169;&#22411;&#21644;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17608v1 Announce Type: new  Abstract: In this paper, we explore the impact of augmenting pre-trained Encoder-Decoder models, specifically T5, with linguistic knowledge for the prediction of a target task. In particular, we investigate whether fine-tuning a T5 model on an intermediate task that predicts structural linguistic properties of sentences modifies its performance in the target task of predicting sentence-level complexity. Our study encompasses diverse experiments conducted on Italian and English datasets, employing both monolingual and multilingual T5 models at various sizes. Results obtained for both languages and in cross-lingual configurations show that linguistically motivated intermediate fine-tuning has generally a positive impact on target task performance, especially when applied to smaller models and in scenarios with limited data availability.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FaultProfIT&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#25925;&#38556;&#27169;&#24335;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#25163;&#21160;&#26631;&#35760;&#30340;&#32570;&#38519;&#12290;</title><link>https://arxiv.org/abs/2402.17583</link><description>&lt;p&gt;
FaultProfIT: &#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#20013;&#25925;&#38556;&#31080;&#25454;&#30340;&#20998;&#23618;&#25925;&#38556;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in Large-scale Cloud Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17583
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FaultProfIT&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#20113;&#31995;&#32479;&#20013;&#30340;&#25925;&#38556;&#27169;&#24335;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#25163;&#21160;&#26631;&#35760;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#21518;&#20998;&#26512;&#22312;&#20113;&#31995;&#32479;&#20013;&#30340;&#20107;&#20214;&#31649;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#20026;&#25913;&#36827;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#20581;&#24615;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#22312;CloudA&#65292;&#25925;&#38556;&#27169;&#24335;&#20998;&#26512;&#26159;&#22312;&#20107;&#21518;&#38454;&#27573;&#25191;&#34892;&#30340;&#65292;&#28041;&#21450;&#23558;&#20107;&#20214;&#25925;&#38556;&#20998;&#31867;&#20026;&#29420;&#29305;&#31867;&#21035;&#65292;&#31216;&#20026;&#25925;&#38556;&#27169;&#24335;&#12290;&#36890;&#36807;&#27719;&#24635;&#21644;&#20998;&#26512;&#36825;&#20123;&#25925;&#38556;&#27169;&#24335;&#65292;&#24037;&#31243;&#24072;&#21487;&#20197;&#35782;&#21035;&#24120;&#35265;&#25925;&#38556;&#12289;&#33030;&#24369;&#32452;&#20214;&#21644;&#26032;&#20986;&#29616;&#30340;&#25925;&#38556;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36807;&#31243;&#30446;&#21069;&#26159;&#36890;&#36807;&#25163;&#21160;&#26631;&#35760;&#36827;&#34892;&#30340;&#65292;&#23384;&#22312;&#22266;&#26377;&#32570;&#38519;&#12290;&#19968;&#26041;&#38754;&#65292;&#20107;&#20214;&#25968;&#37327;&#24222;&#22823;&#24847;&#21619;&#30528;&#21482;&#20998;&#26512;&#20102;&#26368;&#20005;&#37325;&#30340;&#20107;&#20214;&#65292;&#23548;&#33268;&#23545;&#25925;&#38556;&#27169;&#24335;&#30340;&#20559;&#26012;&#27010;&#36848;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#38656;&#35201;&#24191;&#27867;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#36825;&#23548;&#33268;&#38169;&#35823;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#21517;&#20026;FaultProfIT&#65292;&#29992;&#20110;&#22788;&#29702;&#25925;&#38556;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17583v1 Announce Type: cross  Abstract: Postmortem analysis is essential in the management of incidents within cloud systems, which provides valuable insights to improve system's reliability and robustness. At CloudA, fault pattern profiling is performed during the postmortem phase, which involves the classification of incidents' faults into unique categories, referred to as fault pattern. By aggregating and analyzing these fault patterns, engineers can discern common faults, vulnerable components and emerging fault trends. However, this process is currently conducted by manual labeling, which has inherent drawbacks. On the one hand, the sheer volume of incidents means only the most severe ones are analyzed, causing a skewed overview of fault patterns. On the other hand, the complexity of the task demands extensive domain knowledge, which leads to errors and inconsistencies. To address these limitations, we propose an automated approach, named FaultProfIT, for Fault pattern 
&lt;/p&gt;</description></item><item><title>Agent-Pro&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#31574;&#30053;&#32423;&#21035;&#30340;&#21453;&#24605;&#21644;&#20248;&#21270;&#65292;&#21487;&#20197;&#20174;&#20114;&#21160;&#32463;&#39564;&#20013;&#23398;&#20064;&#24182;&#36880;&#27493;&#25552;&#21319;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.17574</link><description>&lt;p&gt;
Agent-Pro: &#36890;&#36807;&#31574;&#30053;&#32423;&#21035;&#21453;&#24605;&#21644;&#20248;&#21270;&#23398;&#20064;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17574
&lt;/p&gt;
&lt;p&gt;
Agent-Pro&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#31574;&#30053;&#32423;&#21035;&#30340;&#21453;&#24605;&#21644;&#20248;&#21270;&#65292;&#21487;&#20197;&#20174;&#20114;&#21160;&#32463;&#39564;&#20013;&#23398;&#20064;&#24182;&#36880;&#27493;&#25552;&#21319;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#24378;&#22823;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#37117;&#26159;&#29305;&#23450;&#20219;&#21153;&#27714;&#35299;&#22120;&#65292;&#24182;&#20855;&#26377;&#22797;&#26434;&#30340;&#25552;&#31034;&#24037;&#31243;&#65292;&#32780;&#19981;&#26159;&#33021;&#22815;&#36890;&#36807;&#20114;&#21160;&#23398;&#20064;&#21644;&#36827;&#21270;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Agent-Pro&#65306;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#20855;&#26377;&#31574;&#30053;&#32423;&#21035;&#30340;&#21453;&#24605;&#21644;&#20248;&#21270;&#65292;&#21487;&#20197;&#20174;&#20114;&#21160;&#32463;&#39564;&#20013;&#23398;&#20064;&#20016;&#23500;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#36880;&#28176;&#25552;&#21319;&#20854;&#34892;&#20026;&#31574;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#28041;&#21450;&#19968;&#20010;&#21160;&#24577;&#20449;&#24565;&#29983;&#25104;&#21644;&#21453;&#24605;&#36807;&#31243;&#65292;&#29992;&#20110;&#31574;&#30053;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17574v1 Announce Type: new  Abstract: Large Language Models exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, fine-tuning its irrational beliefs for a better policy. Moreover
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#35282;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#31034;&#20248;&#21270;&#22120;&#26469;&#25913;&#36827;&#20219;&#21153;&#25552;&#31034;&#65292;&#36890;&#36807;&#31867;&#27604;&#22522;&#20110;&#26799;&#24230;&#30340;&#27169;&#22411;&#20248;&#21270;&#22120;&#65292;&#35774;&#35745;&#20102;&#25913;&#36827;&#30340;LLM-based&#25552;&#31034;&#20248;&#21270;&#22120;&#31574;&#30053;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#22522;&#20110;&#26799;&#24230;&#21551;&#21457;&#30340;LLM-based&#25552;&#31034;&#20248;&#21270;&#22120;GPO&#12290;</title><link>https://arxiv.org/abs/2402.17564</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37322;&#25918;&#20026;&#25552;&#31034;&#20248;&#21270;&#22120;&#30340;&#28508;&#21147;&#65306;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#27169;&#22411;&#20248;&#21270;&#22120;&#30340;&#31867;&#27604;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Potential of Large Language Models as Prompt Optimizers: An Analogical Analysis with Gradient-based Model Optimizers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#35282;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25552;&#31034;&#20248;&#21270;&#22120;&#26469;&#25913;&#36827;&#20219;&#21153;&#25552;&#31034;&#65292;&#36890;&#36807;&#31867;&#27604;&#22522;&#20110;&#26799;&#24230;&#30340;&#27169;&#22411;&#20248;&#21270;&#22120;&#65292;&#35774;&#35745;&#20102;&#25913;&#36827;&#30340;LLM-based&#25552;&#31034;&#20248;&#21270;&#22120;&#31574;&#30053;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#22522;&#20110;&#26799;&#24230;&#21551;&#21457;&#30340;LLM-based&#25552;&#31034;&#20248;&#21270;&#22120;GPO&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25552;&#31034;&#20248;&#21270;&#26159;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;LLMs&#20316;&#20026;&#25552;&#31034;&#20248;&#21270;&#22120;&#20855;&#26377;&#28508;&#21147;&#65292;&#21487;&#20197;&#36890;&#36807;&#36845;&#20195;&#25913;&#36827;&#29983;&#25104;&#25913;&#36827;&#30340;&#20219;&#21153;&#25552;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#35282;&#65292;&#36890;&#36807;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#27169;&#22411;&#20248;&#21270;&#22120;&#36827;&#34892;&#31867;&#27604;&#26469;&#30740;&#31350;&#22522;&#20110;LLM&#30340;&#25552;&#31034;&#20248;&#21270;&#22120;&#30340;&#35774;&#35745;&#12290;&#20026;&#20102;&#36830;&#25509;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#27169;&#22411;&#21442;&#25968;&#23398;&#20064;&#20013;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#26356;&#26032;&#26041;&#21521;&#21644;&#26356;&#26032;&#26041;&#27861;&#12290;&#19987;&#27880;&#20110;&#36825;&#20004;&#20010;&#26041;&#38754;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#26799;&#24230;&#20248;&#21270;&#30340;&#29702;&#35770;&#26694;&#26550;&#21644;&#23398;&#20064;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#25913;&#36827;&#30340;LLM-based&#25552;&#31034;&#20248;&#21270;&#22120;&#31574;&#30053;&#12290;&#36890;&#36807;&#31995;&#32479;&#20998;&#26512;&#20016;&#23500;&#30340;&#25913;&#36827;&#31574;&#30053;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#20010;&#33021;&#21147;&#24378;&#22823;&#30340;&#22522;&#20110;&#26799;&#24230;&#21551;&#21457;&#30340;LLM-based&#25552;&#31034;&#20248;&#21270;&#22120;&#65292;&#31216;&#20026;GPO&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17564v1 Announce Type: new  Abstract: Automatic prompt optimization is an important approach to improving the performance of large language models (LLMs). Recent research demonstrates the potential of using LLMs as prompt optimizers, which can generate improved task prompts via iterative refinement. In this paper, we propose a novel perspective to investigate the design of LLM-based prompt optimizers, by drawing an analogy with gradient-based model optimizers. To connect these two approaches, we identify two pivotal factors in model parameter learning: update direction and update method. Focused on the two aspects, we borrow the theoretical framework and learning methods from gradient-based optimization to design improved strategies for LLM-based prompt optimizers. By systematically analyzing a rich set of improvement strategies, we further develop a capable Gradient-inspired LLM-based Prompt Optimizer called GPO. At each step, it first retrieves relevant prompts from the op
&lt;/p&gt;</description></item><item><title>OmniACT&#26159;&#19968;&#20010;&#38024;&#23545;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.17553</link><description>&lt;p&gt;
OmniACT&#65306;&#29992;&#20110;&#21551;&#29992;&#26700;&#38754;&#21644;Web&#22810;&#27169;&#24335;&#36890;&#29992;&#20027;&#21160;&#26234;&#33021;&#20307;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17553
&lt;/p&gt;
&lt;p&gt;
OmniACT&#26159;&#19968;&#20010;&#38024;&#23545;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#21313;&#24180;&#26469;&#65292;&#20154;&#26426;&#20132;&#20114;&#20174;&#26681;&#26412;&#19978;&#19968;&#30452;&#26159;&#25163;&#21160;&#30340;&#12290;&#21363;&#20351;&#22312;&#20170;&#22825;&#65292;&#20960;&#20046;&#25152;&#26377;&#22312;&#35745;&#31639;&#26426;&#19978;&#36827;&#34892;&#30340;&#39640;&#25928;&#24037;&#20316;&#37117;&#38656;&#35201;&#20154;&#31867;&#22312;&#27599;&#19968;&#27493;&#37117;&#25552;&#20379;&#36755;&#20837;&#12290;&#34394;&#25311;&#20027;&#21160;&#26234;&#33021;&#20195;&#34920;&#20102;&#33258;&#21160;&#21270;&#35768;&#22810;&#36825;&#20123;&#29712;&#30862;&#20219;&#21153;&#30340;&#19968;&#20010;&#28608;&#21160;&#20154;&#24515;&#30340;&#27493;&#39588;&#12290;&#34394;&#25311;&#20195;&#29702;&#23558;&#20351;&#25216;&#26415;&#33021;&#21147;&#26377;&#38480;&#30340;&#29992;&#25143;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#21508;&#31181;&#21487;&#33021;&#24615;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#23454;&#29616;&#39640;&#25928;&#22320;&#31616;&#21270;&#35768;&#22810;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#20174;&#26085;&#21382;&#31649;&#29702;&#21040;&#22797;&#26434;&#30340;&#26053;&#34892;&#39044;&#35746;&#65292;&#20943;&#23569;&#20154;&#31867;&#24178;&#39044;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; OmniACT&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#20195;&#29702;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#26469;&#23436;&#25104;&#35745;&#31639;&#26426;&#20219;&#21153;&#33021;&#21147;&#30340;&#39318;&#20010;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#33539;&#22260;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;Web&#33258;&#21160;&#21270;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26700;&#38754;&#24212;&#29992;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#35832;&#22914;"&#25773;&#25918;&#19979;&#19968;&#39318;&#27468;"&#20043;&#31867;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20197;&#21450;&#26356;&#20026;&#38271;&#26399;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17553v1 Announce Type: new  Abstract: For decades, human-computer interaction has fundamentally been manual. Even today, almost all productive work done on the computer necessitates human input at every step. Autonomous virtual agents represent an exciting step in automating many of these menial tasks. Virtual agents would empower users with limited technical proficiency to harness the full possibilities of computer systems. They could also enable the efficient streamlining of numerous computer tasks, ranging from calendar management to complex travel bookings, with minimal human intervention. In this paper, we introduce OmniACT, the first-of-a-kind dataset and benchmark for assessing an agent's capability to generate executable programs to accomplish computer tasks. Our scope extends beyond traditional web automation, covering a diverse range of desktop applications. The dataset consists of fundamental tasks such as "Play the next song", as well as longer horizon tasks such
&lt;/p&gt;</description></item><item><title>CoCoA&#26159;&#19968;&#27454;&#22522;&#20110;&#35748;&#30693;&#34892;&#20026;&#30103;&#27861;&#25216;&#26415;&#30340;&#24515;&#29702;&#36741;&#23548;&#20195;&#29702;&#65292;&#36890;&#36807;&#26500;&#24314;&#35760;&#24518;&#31995;&#32479;&#31649;&#29702;&#20449;&#24687;&#12289;&#25552;&#21462;&#39640;&#23618;&#35265;&#35299;&#65292;&#24341;&#20837;&#21160;&#24577;&#25552;&#31034;&#28789;&#27963;&#36816;&#29992;CBT&#25216;&#26415;&#65292;&#29983;&#25104;&#36866;&#24403;&#22238;&#24212;&#65292;&#24182;&#22312;&#19982;Character.ai&#35282;&#33394;&#30340;&#23545;&#35805;&#20013;&#23637;&#31034;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.17546</link><description>&lt;p&gt;
COCOA: &#22522;&#20110;&#35748;&#30693;&#22833;&#35843;&#21644;&#21160;&#24577;&#25552;&#31034;&#30340;CBT&#23545;&#35805;&#36741;&#23548;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
COCOA: CBT-based Conversational Counseling Agent using Memory Specialized in Cognitive Distortions and Dynamic Prompt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17546
&lt;/p&gt;
&lt;p&gt;
CoCoA&#26159;&#19968;&#27454;&#22522;&#20110;&#35748;&#30693;&#34892;&#20026;&#30103;&#27861;&#25216;&#26415;&#30340;&#24515;&#29702;&#36741;&#23548;&#20195;&#29702;&#65292;&#36890;&#36807;&#26500;&#24314;&#35760;&#24518;&#31995;&#32479;&#31649;&#29702;&#20449;&#24687;&#12289;&#25552;&#21462;&#39640;&#23618;&#35265;&#35299;&#65292;&#24341;&#20837;&#21160;&#24577;&#25552;&#31034;&#28789;&#27963;&#36816;&#29992;CBT&#25216;&#26415;&#65292;&#29983;&#25104;&#36866;&#24403;&#22238;&#24212;&#65292;&#24182;&#22312;&#19982;Character.ai&#35282;&#33394;&#30340;&#23545;&#35805;&#20013;&#23637;&#31034;&#20986;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#25552;&#20379;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#30340;&#23545;&#35805;&#20195;&#29702;&#30340;&#38656;&#27714;&#25345;&#32493;&#22686;&#21152;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#27454;&#24515;&#29702;&#36741;&#23548;&#20195;&#29702;CoCoA&#65292;&#24212;&#29992;&#35748;&#30693;&#34892;&#20026;&#30103;&#27861;&#65288;CBT&#65289;&#25216;&#26415;&#26469;&#35782;&#21035;&#21644;&#35299;&#20915;&#23458;&#25143;&#38472;&#36848;&#20013;&#22266;&#26377;&#30340;&#35748;&#30693;&#22833;&#35843;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#35760;&#24518;&#31995;&#32479;&#65292;&#20197;&#20415;&#26377;&#25928;&#31649;&#29702;&#36741;&#23548;&#25152;&#38656;&#30340;&#20449;&#24687;&#65292;&#24182;&#20174;&#23458;&#25143;&#30340;&#35805;&#35821;&#20013;&#25552;&#21462;&#39640;&#23618;&#27425;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#30830;&#20445;&#36741;&#23548;&#20195;&#29702;&#29983;&#25104;&#36866;&#24403;&#30340;&#22238;&#24212;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21160;&#24577;&#25552;&#31034;&#65292;&#28789;&#27963;&#24212;&#29992;CBT&#25216;&#26415;&#65292;&#24182;&#20419;&#36827;&#20449;&#24687;&#30340;&#36866;&#24403;&#26816;&#32034;&#12290;&#25105;&#20204;&#22312;CoCoA&#21644;Character.ai&#30340;&#35282;&#33394;&#20043;&#38388;&#36827;&#34892;&#20102;&#23545;&#35805;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35201;&#27714;GPT&#35780;&#20272;&#26500;&#24314;&#30340;&#36741;&#23548;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#20854;&#20182;&#26041;&#27861;&#30340;&#32479;&#35745;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17546v1 Announce Type: new  Abstract: The demand for conversational agents that provide mental health care is consistently increasing. In this work, we develop a psychological counseling agent, referred to as CoCoA, that applies Cognitive Behavioral Therapy (CBT) techniques to identify and address cognitive distortions inherent in the client's statements. Specifically, we construct a memory system to efficiently manage information necessary for counseling while extracting high-level insights about the client from their utterances. Additionally, to ensure that the counseling agent generates appropriate responses, we introduce dynamic prompting to flexibly apply CBT techniques and facilitate the appropriate retrieval of information. We conducted dialogues between CoCoA and characters from Character.ai, creating a dataset for evaluation. Then, we asked GPT to evaluate the constructed counseling dataset, and our model demonstrated a statistically significant difference from othe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25903;&#25345;&#25991;&#26723;&#20013;&#36873;&#25321;&#19978;&#19979;&#25991;&#24863;&#30693;&#30701;&#35821;&#30340;&#26032;&#39062;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#24335;&#33258;&#25105;&#24378;&#21270;&#25552;&#39640;&#35757;&#32451;&#25968;&#25454;&#20934;&#30830;&#24615;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#25552;&#39640;&#20102;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.17532</link><description>&lt;p&gt;
&#26816;&#32034;&#21363;&#31934;&#20934;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Retrieval is Accurate Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17532
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25903;&#25345;&#25991;&#26723;&#20013;&#36873;&#25321;&#19978;&#19979;&#25991;&#24863;&#30693;&#30701;&#35821;&#30340;&#26032;&#39062;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#24335;&#33258;&#25105;&#24378;&#21270;&#25552;&#39640;&#35757;&#32451;&#25968;&#25454;&#20934;&#30830;&#24615;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#25552;&#39640;&#20102;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20174;&#22266;&#23450;&#30340;&#12289;&#26377;&#38480;&#30340;&#21644;&#29420;&#31435;&#30340;&#35789;&#27719;&#20013;&#36873;&#25321;&#26631;&#35760;&#26469;&#29983;&#25104;&#25991;&#26412;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20174;&#19968;&#32452;&#25903;&#25345;&#25991;&#26723;&#20013;&#36873;&#25321;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#30701;&#35821;&#12290;&#36825;&#31181;&#33539;&#24335;&#36716;&#21464;&#20013;&#26368;&#37325;&#35201;&#30340;&#25361;&#25112;&#20043;&#19968;&#26159;&#30830;&#23450;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#20026;&#25991;&#26412;&#21487;&#20197;&#20197;&#22810;&#31181;&#26041;&#24335;&#20998;&#21106;&#65292;&#24182;&#19988;&#27599;&#20010;&#29255;&#27573;&#37117;&#21487;&#20197;&#20174;&#22810;&#20010;&#21487;&#33021;&#30340;&#25991;&#26723;&#20013;&#26816;&#32034;&#21040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#35821;&#35328;&#21551;&#21457;&#24335;&#21021;&#22987;&#21270;&#35757;&#32451;&#25968;&#25454;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#36890;&#36807;&#36845;&#20195;&#24335;&#33258;&#25105;&#24378;&#21270;&#26469;&#24341;&#23548;&#35757;&#32451;&#25968;&#25454;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#20248;&#20110;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#19988;&#22312;&#24320;&#25918;&#24335;&#25991;&#26412;&#29983;&#25104;&#20013;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#12290;&#20363;&#22914;&#65292;&#19982;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#23545;&#24212;&#30340;&#27169;&#22411;&#65292;&#22312;&#24320;&#25918;&#24615;&#20219;&#21153;&#19978;&#23558;&#20934;&#30830;&#29575;&#20174;23.47%&#25552;&#39640;&#21040;36.27%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17532v1 Announce Type: new  Abstract: Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects context-aware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents. To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement. Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation. For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on Open
&lt;/p&gt;</description></item><item><title>Nissist&#21033;&#29992;TSGs&#21644;&#20107;&#25925;&#32531;&#35299;&#21382;&#21490;&#25552;&#20379;&#20027;&#21160;&#24314;&#35758;&#65292;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#65292;&#20197;&#25552;&#39640;&#20225;&#19994;&#32423;&#20113;&#26381;&#21153;&#30340;&#20107;&#25925;&#31649;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.17531</link><description>&lt;p&gt;
Nissist&#65306;&#22522;&#20110;&#25925;&#38556;&#25490;&#38500;&#25351;&#21335;&#30340;&#20107;&#25925;&#32531;&#35299;&#21103;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17531
&lt;/p&gt;
&lt;p&gt;
Nissist&#21033;&#29992;TSGs&#21644;&#20107;&#25925;&#32531;&#35299;&#21382;&#21490;&#25552;&#20379;&#20027;&#21160;&#24314;&#35758;&#65292;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#65292;&#20197;&#25552;&#39640;&#20225;&#19994;&#32423;&#20113;&#26381;&#21153;&#30340;&#20107;&#25925;&#31649;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#20107;&#25925;&#31649;&#29702;&#23545;&#20225;&#19994;&#32423;&#20113;&#26381;&#21153;&#30340;&#39034;&#30021;&#36816;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290; &#20026;&#20102;&#21152;&#36895;&#20107;&#25925;&#32531;&#35299;&#65292;&#26381;&#21153;&#22242;&#38431;&#23558;&#25925;&#38556;&#25490;&#38500;&#30693;&#35782;&#32534;&#35793;&#25104;&#20379;&#20540;&#29677;&#24037;&#31243;&#24072;&#65288;OCEs&#65289;&#35775;&#38382;&#30340;&#25925;&#38556;&#25490;&#38500;&#25351;&#21335;&#65288;TSGs&#65289;&#12290; &#23613;&#31649;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#24050;&#33021;&#22815;&#35299;&#20915;&#26368;&#24120;&#35265;&#21644;&#31616;&#21333;&#30340;&#20107;&#25925;&#65292;&#20294;&#20173;&#23384;&#22312;&#38656;&#35201;OCE&#24178;&#39044;&#30340;&#22797;&#26434;&#20107;&#25925;&#12290; &#28982;&#32780;&#65292;TSGs&#36890;&#24120;&#26159;&#38750;&#32467;&#26500;&#21270;&#21644;&#19981;&#23436;&#25972;&#30340;&#65292;&#36825;&#38656;&#35201;OCE&#25163;&#21160;&#35299;&#37322;&#65292;&#23548;&#33268;&#20540;&#29677;&#30130;&#21171;&#21644;&#29983;&#20135;&#21147;&#19979;&#38477;&#65292;&#29305;&#21035;&#26159;&#26032;&#20837;&#32844;&#30340;OCE&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Nissist&#65292;&#23427;&#21033;&#29992;TSGs&#21644;&#20107;&#25925;&#32531;&#35299;&#21382;&#21490;&#25552;&#20379;&#20027;&#21160;&#24314;&#35758;&#65292;&#20943;&#23569;&#20154;&#20026;&#24178;&#39044;&#12290; &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;Nissist&#20174;&#38750;&#32467;&#26500;&#21270;TSGs&#21644;&#21382;&#21490;&#20107;&#25925;&#32531;&#35299;&#35752;&#35770;&#20013;&#25552;&#21462;&#35265;&#35299;&#65292;&#24418;&#25104;&#20840;&#38754;&#30340;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17531v1 Announce Type: cross  Abstract: Effective incident management is pivotal for the smooth operation of enterprises-level cloud services. In order to expedite incident mitigation, service teams compile troubleshooting knowledge into Troubleshooting Guides (TSGs) accessible to on-call engineers (OCEs). While automated pipelines are enabled to resolve the most frequent and easy incidents, there still exist complex incidents that require OCEs' intervention. However, TSGs are often unstructured and incomplete, which requires manual interpretation by OCEs, leading to on-call fatigue and decreased productivity, especially among new-hire OCEs. In this work, we propose Nissist which leverages TSGs and incident mitigation histories to provide proactive suggestions, reducing human intervention. Leveraging Large Language Models (LLM), Nissist extracts insights from unstructured TSGs and historical incident mitigation discussions, forming a comprehensive knowledge base. Its multi-a
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#26102;&#65292;&#26159;&#21542;&#33021;&#22815;&#22797;&#29616;&#20154;&#31867;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#23637;&#31034;&#30340;&#35821;&#35328;&#21464;&#21270;&#24615;</title><link>https://arxiv.org/abs/2402.17527</link><description>&lt;p&gt;
&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#65306;&#20154;&#31867;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#35821;&#35328;&#27169;&#22411;_____
&lt;/p&gt;
&lt;p&gt;
Predict the Next Word: &lt;Humans exhibit uncertainty in this task and language models _____&gt;
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17527
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#35789;&#26102;&#65292;&#26159;&#21542;&#33021;&#22815;&#22797;&#29616;&#20154;&#31867;&#22312;&#36825;&#39033;&#20219;&#21153;&#20013;&#23637;&#31034;&#30340;&#35821;&#35328;&#21464;&#21270;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26159;&#35757;&#32451;&#29992;&#20110;&#20026;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#20998;&#37197;&#27010;&#29575;&#30340;&#32479;&#35745;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#21512;&#29702;&#36136;&#30097;&#23427;&#20204;&#26159;&#21542;&#24456;&#22909;&#22320;&#36817;&#20284;&#20154;&#31867;&#23637;&#31034;&#30340;&#35821;&#35328;&#21464;&#21270;&#24615;&#12290;&#36825;&#31181;&#24418;&#24335;&#30340;&#32479;&#35745;&#35780;&#20272;&#22312;&#27573;&#33853;&#32423;&#21035;&#19978;&#24456;&#38590;&#25191;&#34892;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#21487;&#25509;&#21463;&#24615;&#21028;&#26029;&#65288;&#21363;&#65292;&#20154;&#31867;&#35780;&#20272;&#65289;&#25110;&#19968;&#20010;&#24378;&#22823;&#30340;&#33258;&#21160;&#20195;&#29702;&#65288;&#36825;&#26159;&#19981;&#24179;&#20961;&#30340;&#65289;&#12290;&#28982;&#32780;&#65292;&#22312;&#21333;&#35789;&#32423;&#21035;&#19978;&#65292;&#36890;&#36807;&#32473;&#23450;&#19968;&#20123;&#19978;&#19979;&#25991;&#65292;&#21487;&#20197;&#36890;&#36807;&#19982;&#19968;&#20010;&#39044;&#20808;&#35760;&#24405;&#30340;&#26367;&#20195;&#21333;&#35789;&#36830;&#32493;&#25968;&#25454;&#38598;&#30340;&#31934;&#30830;&#21305;&#37197;&#26469;&#35780;&#20272;LM&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#20107;&#23454;&#65292;&#24182;&#35780;&#20272;LM&#37325;&#26032;&#29983;&#25104;&#20154;&#31867;&#65288;&#29305;&#21035;&#26159;&#19968;&#32676;&#33521;&#35821;&#20351;&#29992;&#32773;&#65289;&#22312;&#8220;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#8221;&#20219;&#21153;&#20013;&#23637;&#31034;&#30340;&#21464;&#21270;&#30340;&#33021;&#21147;&#12290;&#36825;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#31181;&#26657;&#20934;&#35780;&#20272;&#65292;&#22312;&#25991;&#26412;&#20998;&#31867;&#30340;&#32972;&#26223;&#19979;&#65292;Baan&#31561;&#20154;&#65288;2022&#24180;&#65289;&#23558;&#20854;&#31216;&#20026;&#23545;&#20154;&#31867;&#19981;&#30830;&#23450;&#24615;&#30340;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17527v1 Announce Type: cross  Abstract: Language models (LMs) are statistical models trained to assign probability to human-generated text. As such, it is reasonable to question whether they approximate linguistic variability exhibited by humans well. This form of statistical assessment is difficult to perform at the passage level, for it requires acceptability judgements (i.e., human evaluation) or a robust automated proxy (which is non-trivial). At the word level, however, given some context, samples from an LM can be assessed via exact matching against a prerecorded dataset of alternative single-word continuations of the available context. We exploit this fact and evaluate the LM's ability to reproduce variability that humans (in particular, a population of English speakers) exhibit in the 'next word prediction' task. This can be seen as assessing a form of calibration, which, in the context of text classification, Baan et al. (2022) termed calibration to human uncertaint
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#21521;&#37327;&#23450;&#20041;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;transformer&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#20108;&#27425;&#26041;&#38477;&#20302;&#21040;&#19982;&#26102;&#38388;&#32447;&#24615;&#30456;&#20851;&#65292;&#34920;&#29616;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#23218;&#32654;&#65292;&#20294;&#20801;&#35768;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#36828;&#36828;&#36229;&#20986;&#26631;&#20934;&#30340;&#33539;&#22260;&#12290;</title><link>https://arxiv.org/abs/2402.17512</link><description>&lt;p&gt;
Latent Attention for Linear Time Transformers
&lt;/p&gt;
&lt;p&gt;
Latent Attention for Linear Time Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17512
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#21521;&#37327;&#23450;&#20041;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#23558;&#26631;&#20934;transformer&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;&#20108;&#27425;&#26041;&#38477;&#20302;&#21040;&#19982;&#26102;&#38388;&#32447;&#24615;&#30456;&#20851;&#65292;&#34920;&#29616;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#23218;&#32654;&#65292;&#20294;&#20801;&#35768;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#36828;&#36828;&#36229;&#20986;&#26631;&#20934;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;transformer&#20013;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#21576;&#20108;&#27425;&#26041;&#22686;&#38271;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#36890;&#36807;&#23450;&#20041;&#28508;&#22312;&#21521;&#37327;&#30340;&#27880;&#24847;&#21147;&#26469;&#23558;&#20854;&#38477;&#20302;&#21040;&#19982;&#26102;&#38388;&#32447;&#24615;&#30456;&#20851;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#20316;&#20026;&#26631;&#20934;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26367;&#20195;&#21697;&#12290;&#25105;&#20204;&#30340;&#8220;Latte Transformer&#8221;&#27169;&#22411;&#21487;&#29992;&#20110;&#21452;&#21521;&#21644;&#21333;&#21521;&#20219;&#21153;&#65292;&#22240;&#26524;&#29256;&#26412;&#20801;&#35768;&#19968;&#31181;&#22312;&#25512;&#29702;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#20869;&#23384;&#21644;&#26102;&#38388;&#39640;&#25928;&#30340;&#36882;&#24402;&#23454;&#29616;&#12290;&#26631;&#20934;transformer&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#32447;&#24615;&#22686;&#38271;&#65292;&#32780;Latte Transformer&#35745;&#31639;&#19979;&#19968;&#20010;&#26631;&#35760;&#25152;&#38656;&#30340;&#26102;&#38388;&#26159;&#24658;&#23450;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#23454;&#35777;&#34920;&#29616;&#21487;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#23218;&#32654;&#65292;&#20294;&#20801;&#35768;&#23558;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#36828;&#36828;&#36229;&#20986;&#26631;&#20934;&#27880;&#24847;&#21147;&#23454;&#38469;&#21487;&#34892;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17512v1 Announce Type: new  Abstract: The time complexity of the standard attention mechanism in a transformer scales quadratically with the length of the sequence. We introduce a method to reduce this to linear scaling with time, based on defining attention via latent vectors. The method is readily usable as a drop-in replacement for the standard attention mechanism. Our "Latte Transformer" model can be implemented for both bidirectional and unidirectional tasks, with the causal version allowing a recurrent implementation which is memory and time-efficient during inference of language generation tasks. Whilst next token prediction scales linearly with the sequence length for a standard transformer, a Latte Transformer requires constant time to compute the next token. The empirical performance of our method is comparable to standard attention, yet allows scaling to context windows much larger than practical in standard attention.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#21487;&#33021;&#20250;&#36896;&#25104;&#23545;&#25239;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#24187;&#35273;&#65292;&#30740;&#31350;&#34920;&#26126;&#36890;&#36807;&#27979;&#35797;&#26102;&#38388;&#28201;&#24230;&#32553;&#25918;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2402.17509</link><description>&lt;p&gt;
&#26497;&#31471;&#22833;&#35843;&#19982;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Extreme Miscalibration and the Illusion of Adversarial Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17509
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#21487;&#33021;&#20250;&#36896;&#25104;&#23545;&#25239;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#24187;&#35273;&#65292;&#30740;&#31350;&#34920;&#26126;&#36890;&#36807;&#27979;&#35797;&#26102;&#38388;&#28201;&#24230;&#32553;&#25918;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#24494;&#23567;&#30340;&#25200;&#21160;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#35823;&#20998;&#31867;&#12290;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#32463;&#24120;&#34987;&#29992;&#26469;&#25552;&#21319;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#29616;&#35937;&#65306;&#26377;&#24847;&#25110;&#26080;&#24847;&#22320;&#22833;&#35843;&#27169;&#22411;&#20250;&#25513;&#30422;&#26799;&#24230;&#65292;&#20174;&#32780;&#24178;&#25200;&#23545;&#25239;&#25915;&#20987;&#25628;&#32034;&#26041;&#27861;&#65292;&#23548;&#33268;&#34920;&#38754;&#19978;&#30475;&#20284;&#22686;&#21152;&#20102;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#35266;&#23519;&#21040;&#30340;&#40065;&#26834;&#24615;&#22686;&#30410;&#26159;&#19968;&#31181;&#40065;&#26834;&#24615;&#24187;&#35273;&#65288;IOR&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#25163;&#22914;&#20309;&#25191;&#34892;&#21508;&#31181;&#24418;&#24335;&#30340;&#27979;&#35797;&#26102;&#38388;&#28201;&#24230;&#26657;&#20934;&#26469;&#25269;&#28040;&#19978;&#36848;&#24178;&#25200;&#65292;&#20351;&#23545;&#25239;&#25915;&#20987;&#33021;&#22815;&#25214;&#21040;&#23545;&#25239;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25958;&#20419;NLP&#31038;&#21306;&#22312;&#20854;&#40065;&#26834;&#24615;&#35780;&#20272;&#20013;&#32435;&#20837;&#27979;&#35797;&#26102;&#38388;&#28201;&#24230;&#32553;&#25918;&#65292;&#20197;&#30830;&#20445;&#35266;&#23519;&#21040;&#30340;&#20219;&#20309;&#22686;&#30410;&#37117;&#26159;&#30495;&#23454;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17509v1 Announce Type: new  Abstract: Deep learning-based Natural Language Processing (NLP) models are vulnerable to adversarial attacks, where small perturbations can cause a model to misclassify. Adversarial Training (AT) is often used to increase model robustness. However, we have discovered an intriguing phenomenon: deliberately or accidentally miscalibrating models masks gradients in a way that interferes with adversarial attack search methods, giving rise to an apparent increase in robustness. We show that this observed gain in robustness is an illusion of robustness (IOR), and demonstrate how an adversary can perform various forms of test-time temperature calibration to nullify the aforementioned interference and allow the adversarial attack to find adversarial examples. Hence, we urge the NLP community to incorporate test-time temperature scaling into their robustness evaluations to ensure that any observed gains are genuine. Finally, we show how the temperature can 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29992;&#25143;&#20223;&#30495;&#26694;&#26550;BASES&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#27169;&#25311;&#22823;&#35268;&#27169;&#31867;&#20154;&#31867;&#30340;&#32593;&#32476;&#25628;&#32034;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.17505</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#32593;&#32476;&#25628;&#32034;&#29992;&#25143;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
BASES: Large-scale Web Search User Simulation with Large Language Model based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29992;&#25143;&#20223;&#30495;&#26694;&#26550;BASES&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#27169;&#25311;&#22823;&#35268;&#27169;&#31867;&#20154;&#31867;&#30340;&#32593;&#32476;&#25628;&#32034;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#24320;&#21457;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20197;&#21487;&#38752;&#22320;&#20223;&#30495;&#29992;&#25143;&#21464;&#24471;&#21487;&#34892;&#12290;&#32771;&#34385;&#21040;&#30495;&#23454;&#29992;&#25143;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#38480;&#21046;&#65288;&#20363;&#22914;&#38544;&#31169;&#38382;&#39064;&#65289;&#65292;&#26412;&#25991;&#38024;&#23545;&#32593;&#32476;&#25628;&#32034;&#36827;&#34892;&#22823;&#35268;&#27169;&#29992;&#25143;&#20223;&#30495;&#65292;&#20197;&#25913;&#36827;&#23545;&#29992;&#25143;&#25628;&#32034;&#34892;&#20026;&#30340;&#20998;&#26512;&#21644;&#24314;&#27169;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29992;&#25143;&#20223;&#30495;&#26694;&#26550;BASES&#65292;&#20854;&#20013;&#21253;&#21547;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#26088;&#22312;&#20419;&#36827;&#23545;&#32593;&#32476;&#25628;&#32034;&#29992;&#25143;&#34892;&#20026;&#30340;&#20840;&#38754;&#27169;&#25311;&#12290;&#25105;&#20204;&#30340;&#20223;&#30495;&#26694;&#26550;&#21487;&#20197;&#22823;&#35268;&#27169;&#29983;&#25104;&#29420;&#29305;&#30340;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#65292;&#20174;&#32780;&#23548;&#33268;&#22810;&#26679;&#21270;&#30340;&#25628;&#32034;&#34892;&#20026;&#12290;&#20026;&#20102;&#35777;&#26126;BASES&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22522;&#20110;&#20013;&#33521;&#25991;&#30340;&#20004;&#20010;&#20154;&#31867;&#22522;&#20934;&#36827;&#34892;&#20102;&#35780;&#20272;&#23454;&#39564;&#65292;&#35777;&#26126;BASES&#33021;&#22815;&#26377;&#25928;&#22320;&#27169;&#25311;&#22823;&#35268;&#27169;&#31867;&#20284;&#20154;&#31867;&#30340;&#25628;&#32034;&#34892;&#20026;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20419;&#36827;&#32593;&#32476;&#25628;&#32034;&#30740;&#31350;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#20223;&#30495;&#27979;&#35797;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17505v1 Announce Type: cross  Abstract: Due to the excellent capacities of large language models (LLMs), it becomes feasible to develop LLM-based agents for reliable user simulation. Considering the scarcity and limit (e.g., privacy issues) of real user data, in this paper, we conduct large-scale user simulation for web search, to improve the analysis and modeling of user search behavior. Specially, we propose BASES, a novel user simulation framework with LLM-based agents, designed to facilitate comprehensive simulations of web search user behaviors. Our simulation framework can generate unique user profiles at scale, which subsequently leads to diverse search behaviors. To demonstrate the effectiveness of BASES, we conduct evaluation experiments based on two human benchmarks in both Chinese and English, demonstrating that BASES can effectively simulate large-scale human-like search behaviors. To further accommodate the research on web search, we develop WARRIORS, a new larg
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REAR&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#26080;&#27861;&#20934;&#30830;&#35780;&#20272;&#26816;&#32034;&#25991;&#26723;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#22686;&#24378;&#23545;&#26816;&#32034;&#25991;&#26723;&#30456;&#20851;&#24615;&#30340;&#33258;&#25105;&#24847;&#35782;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2402.17497</link><description>&lt;p&gt;
REAR&#65306;&#19968;&#31181;&#38754;&#21521;&#24320;&#25918;&#22495;&#38382;&#31572;&#30340;&#20851;&#27880;&#24230;&#24863;&#30693;&#26816;&#32034;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17497
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REAR&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#20013;&#26080;&#27861;&#20934;&#30830;&#35780;&#20272;&#26816;&#32034;&#25991;&#26723;&#30456;&#20851;&#24615;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#22686;&#24378;&#23545;&#26816;&#32034;&#25991;&#26723;&#30456;&#20851;&#24615;&#30340;&#33258;&#25105;&#24847;&#35782;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#26377;&#38480;&#30340;&#20869;&#37096;&#21442;&#25968;&#21270;&#30693;&#35782;&#65292;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#33539;&#22260;&#12290;&#23613;&#31649;&#22312;RAG&#30740;&#31350;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;LLMs &#26080;&#27861;&#20934;&#30830;&#35780;&#20272;&#26816;&#32034;&#25991;&#26723;&#30340;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#24456;&#21487;&#33021;&#23548;&#33268;&#23545;&#22806;&#37096;&#30693;&#35782;&#65288;&#21363;&#26816;&#32034;&#25991;&#26723;&#65289;&#30340;&#35823;&#23548;&#29978;&#33267;&#38169;&#35823;&#21033;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; REAR&#65292;&#19968;&#31181;&#38754;&#21521;&#24320;&#25918;&#22495;&#38382;&#31572;&#65288;QA&#65289;&#30340;&#20851;&#27880;&#24230;&#24863;&#30693;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#12290;&#20316;&#20026;&#20851;&#38190;&#21160;&#26426;&#65292;&#25105;&#20204;&#26088;&#22312;&#22686;&#24378;LLMs&#23545;&#26469;&#28304;&#30456;&#20851;&#24615;&#30340;&#33258;&#25105;&#24847;&#35782;&#65292;&#20197;&#20415;&#22312;RAG&#31995;&#32479;&#20013;&#33258;&#36866;&#24212;&#22320;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;LLM&#30340;RAG&#31995;&#32479;&#26550;&#26500;&#65292;&#36890;&#36807;&#25972;&#21512;&#19968;&#20010;&#31934;&#30830;&#35780;&#20272;&#26816;&#32034;&#25991;&#26723;&#30456;&#20851;&#24615;&#30340;&#29305;&#21035;&#35774;&#35745;&#30340;&#25490;&#21517;&#22836;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17497v1 Announce Type: new  Abstract: Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs). Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents). To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA). As the key motivation, we aim to enhance the self-awareness of source relevance for LLMs, so as to adaptively utilize external knowledge in RAG systems. Specially, we develop a new architecture for LLM based RAG system, by incorporating a specially designed rank head that precisely assesses the relevance of retrieved documents. Furthermore, we propose an improved training method 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;Emotional Voice Messages (EMOVOME)&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;100&#21517;&#35199;&#29677;&#29273;&#35828;&#35805;&#32773;&#30340;999&#26465;&#33258;&#21457;&#35821;&#38899;&#28040;&#24687;&#65292;&#36890;&#36807;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#30340;&#26631;&#35760;&#23454;&#29616;&#20102;&#22312;valence&#21644;arousal&#32500;&#24230;&#19978;&#30340;&#24773;&#24863;&#35782;&#21035;&#65292;&#24182;&#23581;&#35797;&#20351;&#29992;&#35821;&#38899;&#21644;&#25991;&#26412;&#36716;&#24405;&#23454;&#29616;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.17496</link><description>&lt;p&gt;
Emotional Voice Messages (EMOVOME)&#25968;&#25454;&#24211;&#65306;&#33258;&#21457;&#24773;&#24863;&#35821;&#38899;&#28040;&#24687;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Emotional Voice Messages (EMOVOME) database: emotion recognition in spontaneous voice messages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17496
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;Emotional Voice Messages (EMOVOME)&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;100&#21517;&#35199;&#29677;&#29273;&#35828;&#35805;&#32773;&#30340;999&#26465;&#33258;&#21457;&#35821;&#38899;&#28040;&#24687;&#65292;&#36890;&#36807;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#30340;&#26631;&#35760;&#23454;&#29616;&#20102;&#22312;valence&#21644;arousal&#32500;&#24230;&#19978;&#30340;&#24773;&#24863;&#35782;&#21035;&#65292;&#24182;&#23581;&#35797;&#20351;&#29992;&#35821;&#38899;&#21644;&#25991;&#26412;&#36716;&#24405;&#23454;&#29616;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Emotional Voice Messages (EMOVOME)&#26159;&#19968;&#20010;&#33258;&#21457;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;100&#21517;&#35199;&#29677;&#29273;&#35828;&#35805;&#32773;&#12289;&#30007;&#22899;&#24615;&#24179;&#34913;&#30340;999&#26465;&#30495;&#23454;&#20250;&#35805;&#20013;&#30340;&#38899;&#39057;&#28040;&#24687;&#65292;&#36825;&#20123;&#28040;&#24687;&#36890;&#36807;&#19968;&#20010;&#28040;&#24687;&#24212;&#29992;&#31243;&#24207;&#20135;&#29983;&#65292;&#22312;&#21442;&#19982;&#32773;&#34987;&#25307;&#21215;&#20043;&#21069;&#22312;&#37326;&#22806;&#29615;&#22659;&#20013;&#21046;&#20316;&#65292;&#36991;&#20813;&#20102;&#30001;&#20110;&#23454;&#39564;&#23460;&#29615;&#22659;&#32780;&#20135;&#29983;&#30340;&#20219;&#20309;&#24847;&#35782;&#20559;&#35265;&#12290;&#38899;&#39057;&#25353;&#29031;&#19977;&#20010;&#38750;&#19987;&#23478;&#21644;&#20004;&#20010;&#19987;&#23478;&#30340;&#35748;&#21487;&#22312;valence&#21644;arousal&#32500;&#24230;&#19978;&#36827;&#34892;&#20102;&#26631;&#35760;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#32467;&#21512;&#20197;&#33719;&#24471;&#27599;&#20010;&#32500;&#24230;&#30340;&#26368;&#32456;&#26631;&#31614;&#12290;&#19987;&#23478;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#20110;&#19971;&#31181;&#24773;&#24863;&#31867;&#21035;&#30340;&#39069;&#22806;&#26631;&#31614;&#12290;&#20026;&#20102;&#20026;&#23558;&#26469;&#20351;&#29992;EMOVOME&#36827;&#34892;&#35843;&#26597;&#35774;&#23450;&#22522;&#20934;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#35821;&#38899;&#21644;&#38899;&#39057;&#36716;&#24405;&#26469;&#23454;&#29616;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#12290;&#23545;&#20110;&#35821;&#38899;&#37096;&#20998;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26631;&#20934;&#30340;eGeMAPS&#29305;&#24449;&#38598;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65292;&#20998;&#21035;&#33719;&#24471;&#20102;49.27%&#21644;44.71%&#30340;valence&#21644;arousal&#26410;&#21152;&#26435;&#20934;&#30830;&#24230;&#12290;&#23545;&#20110;&#25991;&#26412;&#37096;&#20998;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#22810;&#35821;&#35328;BERT&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#23454;&#29616;&#20102;61%&#30340;&#24773;&#24863;&#35782;&#21035;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17496v1 Announce Type: cross  Abstract: Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing 999 audio messages from real conversations on a messaging app from 100 Spanish speakers, gender balanced. Voice messages were produced in-the-wild conditions before participants were recruited, avoiding any conscious bias due to laboratory environment. Audios were labeled in valence and arousal dimensions by three non-experts and two experts, which were then combined to obtain a final label per dimension. The experts also provided an extra label corresponding to seven emotion categories. To set a baseline for future investigations using EMOVOME, we implemented emotion recognition models using both speech and audio transcriptions. For speech, we used the standard eGeMAPS feature set and support vector machines, obtaining 49.27% and 44.71% unweighted accuracy for valence and arousal respectively. For text, we fine-tuned a multilingual BERT model and achieved 61
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35780;&#20272;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26415;&#21518;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#19981;&#21516;&#35757;&#32451;&#31574;&#30053;&#30340;&#27169;&#22411;&#22312;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#20013;&#30340;&#28508;&#22312;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.17493</link><description>&lt;p&gt;
&#20026;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#24320;&#20855;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27491;&#30830;&#21058;&#37327;&#26159;&#22810;&#23569;&#65311;
&lt;/p&gt;
&lt;p&gt;
Prescribing Large Language Models for Perioperative Care: What's The Right Dose for Pre-trained Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17493
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26415;&#21518;&#39118;&#38505;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#19981;&#21516;&#35757;&#32451;&#31574;&#30053;&#30340;&#27169;&#22411;&#22312;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#20013;&#30340;&#28508;&#22312;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26415;&#21518;&#39118;&#38505;&#39044;&#27979;&#21487;&#20197;&#25351;&#23548;&#26377;&#25928;&#30340;&#22260;&#25163;&#26415;&#26399;&#25252;&#29702;&#31649;&#29702;&#21644;&#35268;&#21010;&#12290;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#39044;&#27979;&#26415;&#21518;&#39118;&#38505;&#12290;&#30740;&#31350;&#20027;&#35201;&#28041;&#21450;2018&#24180;&#33267;2021&#24180;&#38388;&#26469;&#33258;Barnes Jewish&#21307;&#38498;&#31995;&#32479;&#30340;84,875&#20221;&#35760;&#24405;&#12290;&#26041;&#27861;&#22312;Beth Israel Deaconess&#30340;MIMIC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22797;&#21046;&#12290;&#20004;&#39033;&#30740;&#31350;&#30340;&#24179;&#22343;&#38543;&#35775;&#26102;&#38388;&#22522;&#20110;&#26415;&#21518;ICU&#20303;&#38498;&#26102;&#38388;&#23567;&#20110;7&#22825;&#12290;&#23545;&#20110;BJH&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#21253;&#25324;30&#22825;&#27515;&#20129;&#29575;&#12289;&#32954;&#26643;&#22622;&#65288;PE&#65289;&#21644;&#32954;&#28814;&#12290;&#23545;BioGPT&#12289;ClinicalBERT&#21644;BioClinicalBERT&#23454;&#26045;&#20102;&#19977;&#31181;&#22495;&#33258;&#36866;&#24212;&#21644;&#24494;&#35843;&#31574;&#30053;&#65306;&#33258;&#30417;&#30563;&#30446;&#26631;&#65307;&#32467;&#21512;&#21322;&#30417;&#30563;&#24494;&#35843;&#30340;&#26631;&#31614;&#65307;&#20197;&#21450;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#36827;&#34892;&#22522;&#30784;&#24314;&#27169;&#12290;&#27169;&#22411;&#24615;&#33021;&#20351;&#29992;&#25509;&#25910;&#22120;&#25805;&#20316;&#29305;&#24449;&#19979;&#30340;&#38754;&#31215;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17493v1 Announce Type: new  Abstract: Postoperative risk predictions can inform effective perioperative care management and planning. We aimed to assess whether clinical large language models (LLMs) can predict postoperative risks using clinical texts with various training strategies. The main cohort involved 84,875 records from Barnes Jewish Hospital (BJH) system between 2018 and 2021. Methods were replicated on Beth Israel Deaconess's MIMIC dataset. Both studies had mean duration of follow-up based on the length of postoperative ICU stay less than 7 days. For the BJH dataset, outcomes included 30-day mortality, pulmonary embolism (PE) and pneumonia. Three domain adaptation and finetuning strategies were implemented for BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives; incorporating labels with semi-supervised fine-tuning; and foundational modelling through multi-task learning. Model performance was compared using the area under the receiver operating ch
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#36804;&#20170;&#26368;&#22823;&#30340;&#23459;&#20256;&#25968;&#25454;&#38598;ArPro&#65292;&#20351;&#29992;GPT-4&#36827;&#34892;&#20174;&#25991;&#26412;&#20013;&#36827;&#34892;&#31934;&#32454;&#23459;&#20256;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.17478</link><description>&lt;p&gt;
GPT-4&#33021;&#35782;&#21035;&#23459;&#20256;&#21527;&#65311;&#26032;&#38395;&#25991;&#31456;&#20013;&#23459;&#20256;&#27573;&#33853;&#30340;&#27880;&#37322;&#21644;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda Spans in News Articles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17478
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24320;&#21457;&#20102;&#36804;&#20170;&#26368;&#22823;&#30340;&#23459;&#20256;&#25968;&#25454;&#38598;ArPro&#65292;&#20351;&#29992;GPT-4&#36827;&#34892;&#20174;&#25991;&#26412;&#20013;&#36827;&#34892;&#31934;&#32454;&#23459;&#20256;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20027;&#27969;&#23186;&#20307;&#21644;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#23459;&#20256;&#20351;&#29992;&#28608;&#22686;&#65292;&#26088;&#22312;&#25805;&#32437;&#25110;&#35823;&#23548;&#29992;&#25143;&#12290;&#23613;&#31649;&#33258;&#21160;&#26816;&#27979;&#25991;&#26412;&#12289;&#35270;&#35273;&#25110;&#22810;&#27169;&#24577;&#20869;&#23481;&#20013;&#30340;&#23459;&#20256;&#25216;&#26415;&#30340;&#21162;&#21147;&#22686;&#21152;&#65292;&#20294;&#20854;&#20013;&#22823;&#37096;&#20998;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#25991;&#20869;&#23481;&#19978;&#12290;&#26368;&#36817;&#22823;&#37096;&#20998;&#38024;&#23545;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#20030;&#25514;&#20135;&#29983;&#20102;&#30456;&#23545;&#36739;&#23567;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20998;&#24067;&#19981;&#22343;&#65292;&#32473;&#31934;&#33268;&#23459;&#20256;&#26816;&#27979;&#27169;&#22411;&#30340;&#24320;&#21457;&#24102;&#26469;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#31934;&#24515;&#24320;&#21457;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#23459;&#20256;&#25968;&#25454;&#38598;ArPro&#65292;&#21253;&#25324;&#26469;&#33258;&#25253;&#32440;&#25991;&#31456;&#30340;8K&#27573;&#33853;&#65292;&#22312;23&#31181;&#23459;&#20256;&#25216;&#26415;&#30340;&#20998;&#31867;&#27861;&#19979;&#36827;&#34892;&#25991;&#26412;&#27573;&#26631;&#35760;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#39318;&#27425;&#23581;&#35797;&#20102;&#20351;&#29992;GPT-4&#36827;&#34892;&#20174;&#25991;&#26412;&#20013;&#31934;&#32454;&#23459;&#20256;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17478v1 Announce Type: new  Abstract: The use of propaganda has spiked on mainstream and social media, aiming to manipulate or mislead users. While efforts to automatically detect propaganda techniques in textual, visual, or multimodal content have increased, most of them primarily focus on English content. The majority of the recent initiatives targeting medium to low-resource languages produced relatively small annotated datasets, with a skewed distribution, posing challenges for the development of sophisticated propaganda detection models. To address this challenge, we carefully develop the largest propaganda dataset to date, ArPro, comprised of 8K paragraphs from newspaper articles, labeled at the text span level following a taxonomy of 23 propagandistic techniques. Furthermore, our work offers the first attempt to understand the performance of large language models (LLMs), using GPT-4, for fine-grained propaganda detection from text. Results showed that GPT-4's performa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Dual Chunk Attention (DCA)&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;Llama2 70B&#22312;&#19981;&#38656;&#35201;&#25345;&#32493;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#25903;&#25345;&#36229;&#36807;100k&#20196;&#29260;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#33021;&#22815;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#21462;&#24471;&#19982;&#24494;&#35843;&#27169;&#22411;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17463</link><description>&lt;p&gt;
&#26080;&#39035;&#35757;&#32451;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#38271;&#19978;&#19979;&#25991;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Training-Free Long-Context Scaling of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17463
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Dual Chunk Attention (DCA)&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;Llama2 70B&#22312;&#19981;&#38656;&#35201;&#25345;&#32493;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#25903;&#25345;&#36229;&#36807;100k&#20196;&#29260;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#33021;&#22815;&#22312;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#20013;&#21462;&#24471;&#19982;&#24494;&#35843;&#27169;&#22411;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#36830;&#36143;&#25991;&#26412;&#26102;&#65292;&#24403;&#36755;&#20837;&#20196;&#29260;&#25968;&#37327;&#36229;&#36807;&#23427;&#20204;&#30340;&#39044;&#35757;&#32451;&#38271;&#24230;&#26102;&#65292;&#20854;&#33021;&#21147;&#20250;&#26126;&#26174;&#20943;&#24369;&#12290;&#37492;&#20110;&#20351;&#29992;&#26356;&#38271;&#24207;&#21015;&#36827;&#34892;&#22823;&#35268;&#27169;&#27169;&#22411;&#24494;&#35843;&#30340;&#26114;&#36149;&#24320;&#38144;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Dual Chunk Attention&#65288;DCA&#65289;&#65292;&#23427;&#20351;Llama2 70B&#33021;&#22815;&#25903;&#25345;&#36229;&#36807;100k&#20196;&#29260;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#65292;&#32780;&#26080;&#38656;&#25345;&#32493;&#35757;&#32451;&#12290;&#36890;&#36807;&#23558;&#38271;&#24207;&#21015;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#20998;&#35299;&#20026;&#22522;&#20110;&#22359;&#30340;&#27169;&#22359;&#65292;DCA&#25104;&#21151;&#25429;&#33719;&#20102;&#30456;&#21516;&#22359;&#20869;&#65288;Intra-Chunk&#65289;&#21644;&#19981;&#21516;&#22359;&#20043;&#38388;&#65288;Inter-Chunk&#65289;&#20196;&#29260;&#30340;&#30456;&#23545;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#33021;&#19982;Flash Attention&#26080;&#32541;&#38598;&#25104;&#12290;&#38500;&#20102;&#20854;&#24778;&#20154;&#30340;&#22806;&#25512;&#33021;&#21147;&#22806;&#65292;DCA&#22312;&#23454;&#38469;&#38271;&#19978;&#19979;&#25991;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#19982;&#25110;&#29978;&#33267;&#20248;&#20110;&#24494;&#35843;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#19982;&#19987;&#26377;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26080;&#39035;&#35757;&#32451;&#30340;70B&#27169;&#22411;&#21462;&#24471;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17463v1 Announce Type: new  Abstract: The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length. Given the expensive overhead of finetuning large-scale models with longer sequences, we propose Dual Chunk Attention (DCA), which enables Llama2 70B to support context windows of more than 100k tokens without continual training. By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash Attention. In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of finetuned models. When compared with proprietary models, our training-free 70B model attai
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#20195;&#30721;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#24037;&#20855;&#65292;&#24110;&#21161;&#25945;&#24072;&#35774;&#35745;&#23450;&#21046;&#30340;&#23545;&#35805;&#27969;&#31243;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#35805;&#35821;&#65292;&#25506;&#35752;&#20102;&#25945;&#24072;&#22312;&#35774;&#35745;&#32842;&#22825;&#26426;&#22120;&#20154;&#26102;&#30340;&#38656;&#27714;&#65292;&#24182;&#23637;&#31034;&#20102;&#25945;&#24072;&#23558;&#33258;&#24049;&#30475;&#20316;&#26159;&#24341;&#23548;&#23398;&#29983;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#21095;&#20316;&#23478;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.17456</link><description>&lt;p&gt;
&#19968;&#37096;&#25103;&#21095;&#65306;&#25506;&#35752;&#25945;&#24072;&#22914;&#20309;&#35774;&#35745; LLM &#32842;&#22825;&#26426;&#22120;&#20154;&#20197;&#21327;&#21161;&#38738;&#23569;&#24180;&#38450;&#27490;&#32593;&#32476;&#27450;&#20940;&#25945;&#32946;
&lt;/p&gt;
&lt;p&gt;
A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to Assist Adolescent Cyberbullying Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17456
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#20195;&#30721;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#24037;&#20855;&#65292;&#24110;&#21161;&#25945;&#24072;&#35774;&#35745;&#23450;&#21046;&#30340;&#23545;&#35805;&#27969;&#31243;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#35805;&#35821;&#65292;&#25506;&#35752;&#20102;&#25945;&#24072;&#22312;&#35774;&#35745;&#32842;&#22825;&#26426;&#22120;&#20154;&#26102;&#30340;&#38656;&#27714;&#65292;&#24182;&#23637;&#31034;&#20102;&#25945;&#24072;&#23558;&#33258;&#24049;&#30475;&#20316;&#26159;&#24341;&#23548;&#23398;&#29983;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#21095;&#20316;&#23478;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#32593;&#32476;&#27450;&#20940;&#21361;&#23475;&#38738;&#23569;&#24180;&#30340;&#24515;&#29702;&#20581;&#24247;&#65292;&#25945;&#25480;&#20182;&#20204;&#27491;&#30830;&#30340;&#24178;&#39044;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#24043;&#24072;-&#22885;&#20857;&#30740;&#31350;&#34920;&#26126;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#21487;&#20197;&#25193;&#23637;&#20010;&#24615;&#21270;&#21644;&#20114;&#21160;&#24335;&#30340;&#32593;&#32476;&#27450;&#20940;&#25945;&#32946;&#65292;&#20294;&#23454;&#26045;&#36825;&#26679;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#24494;&#22937;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20026; K-12 &#25945;&#24072;&#21019;&#24314;&#20102;&#19968;&#20010;&#26080;&#20195;&#30721;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#24037;&#20855;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#38142;&#26465;&#65292;&#25105;&#20204;&#30340;&#24037;&#20855;&#20801;&#35768;&#25945;&#24072;&#21407;&#22411;&#21270;&#23450;&#21046;&#30340;&#23545;&#35805;&#27969;&#31243;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#35805;&#35821;&#12290;&#36890;&#36807;&#25552;&#20379;&#36825;&#20010;&#24037;&#20855;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25945;&#24072;&#22312;&#35774;&#35745;&#32842;&#22825;&#26426;&#22120;&#20154;&#20197;&#36741;&#21161;&#20182;&#20204;&#30340;&#25945;&#23398;&#26102;&#30340;&#29420;&#29305;&#38656;&#27714;&#65292;&#20197;&#21450;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#24037;&#20855;&#22914;&#20309;&#26356;&#22909;&#22320;&#25903;&#25345;&#20182;&#20204;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25945;&#24072;&#28909;&#24773;&#22320;&#25509;&#21463;&#36825;&#20010;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#23558;&#33258;&#24049;&#35270;&#20026;&#25351;&#23548;&#23398;&#29983;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#21095;&#20316;&#23478;&#65292;&#21516;&#26102;&#20801;&#35768;&#19968;&#20123;&#21363;&#20852;&#28436;&#20986;&#12290;&#20182;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#23398;&#29983;&#33021;&#22815;&#25490;&#32451;&#23545;&#32593;&#32476;&#27450;&#20940;&#30340;&#29702;&#24819;&#21644;&#19981;&#33391;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17456v1 Announce Type: cross  Abstract: Cyberbullying harms teenagers' mental health, and teaching them upstanding intervention is crucial. Wizard-of-Oz studies show chatbots can scale up personalized and interactive cyberbullying education, but implementing such chatbots is a challenging and delicate task. We created a no-code chatbot design tool for K-12 teachers. Using large language models and prompt chaining, our tool allows teachers to prototype bespoke dialogue flows and chatbot utterances. In offering this tool, we explore teachers' distinctive needs when designing chatbots to assist their teaching, and how chatbot design tools might better support them. Our findings reveal that teachers welcome the tool enthusiastically. Moreover, they see themselves as playwrights guiding both the students' and the chatbot's behaviors, while allowing for some improvisation. Their goal is to enable students to rehearse both desirable and undesirable reactions to cyberbullying in a s
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#38024;&#23545;&#39135;&#35889;&#25991;&#26412;&#24320;&#21457;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#31995;&#32479;&#30340;&#25968;&#25454;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#26500;&#24314;&#20102;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#26032;&#39135;&#35889;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.17447</link><description>&lt;p&gt;
&#39135;&#35889;&#30340;&#28145;&#24230;&#23398;&#20064;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Based Named Entity Recognition Models for Recipes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65292;&#38024;&#23545;&#39135;&#35889;&#25991;&#26412;&#24320;&#21457;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#31995;&#32479;&#30340;&#25968;&#25454;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#26500;&#24314;&#20102;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#26032;&#39135;&#35889;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#36890;&#36807;&#21508;&#31181;&#21162;&#21147;&#26041;&#24335;&#24433;&#21709;&#30528;&#25105;&#20204;&#30340;&#29983;&#27963;&#65292;&#21253;&#25324;&#21475;&#21619;&#12289;&#33829;&#20859;&#12289;&#20581;&#24247;&#21644;&#21487;&#25345;&#32493;&#24615;&#12290;&#39135;&#35889;&#26159;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20195;&#20195;&#30456;&#20256;&#30340;&#25991;&#21270;&#33014;&#22218;&#12290;&#33258;&#21160;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#30340;&#21327;&#35758;&#65292;&#21363;&#39135;&#35889;&#25991;&#26412;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#26469;&#35828;&#37117;&#20855;&#26377;&#24040;&#22823;&#20215;&#20540;&#65292;&#20174;&#20449;&#24687;&#25552;&#21462;&#21040;&#26032;&#39062;&#39135;&#35889;&#29983;&#25104;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26159;&#19968;&#31181;&#20174;&#24050;&#30693;&#26631;&#31614;&#30340;&#38750;&#32467;&#26500;&#21270;&#25110;&#21322;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#20174;&#25163;&#21160;&#27880;&#37322;&#30340;6,611&#20010;&#25104;&#20998;&#30701;&#35821;&#30340;&#25968;&#25454;&#24320;&#22987;&#65292;&#32047;&#31215;&#21019;&#24314;&#20102;26,445&#20010;&#30701;&#35821;&#30340;&#22686;&#24378;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#28165;&#29702;&#21644;&#20998;&#26512;&#20102;&#26469;&#33258;RecipeDB&#30340;&#25104;&#20998;&#30701;&#35821;&#65292;&#36825;&#26159;&#40644;&#37329;&#26631;&#20934;&#30340;&#39135;&#35889;&#25968;&#25454;&#23384;&#20648;&#24211;&#65292;&#24182;&#20351;&#29992;Stanford NER&#36827;&#34892;&#20102;&#26631;&#27880;&#12290;&#22522;&#20110;&#20998;&#26512;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#23545;88,526&#20010;&#30701;&#35821;&#30340;&#23376;&#38598;&#36827;&#34892;&#20102;&#21462;&#26679;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17447v1 Announce Type: cross  Abstract: Food touches our lives through various endeavors, including flavor, nourishment, health, and sustainability. Recipes are cultural capsules transmitted across generations via unstructured text. Automated protocols for recognizing named entities, the building blocks of recipe text, are of immense value for various applications ranging from information extraction to novel recipe generation. Named entity recognition is a technique for extracting information from unstructured or semi-structured data with known labels. Starting with manually-annotated data of 6,611 ingredient phrases, we created an augmented dataset of 26,445 phrases cumulatively. Simultaneously, we systematically cleaned and analyzed ingredient phrases from RecipeDB, the gold-standard recipe data repository, and annotated them using the Stanford NER. Based on the analysis, we sampled a subset of 88,526 phrases using a clustering-based approach while preserving the diversity
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#24773;&#24863;-&#35821;&#20041;&#30456;&#20851;&#24615;&#26469;&#29983;&#25104;&#20849;&#24773;&#24335;&#23545;&#35805;&#22238;&#22797;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21644;&#24773;&#24863;&#30340;&#20132;&#20114;&#26500;&#24314;&#21160;&#24577;&#24773;&#24863;-&#35821;&#20041;&#21521;&#37327;&#65292;&#25552;&#39640;&#20102;&#23545;&#24773;&#24863;&#19982;&#35821;&#20041;&#20851;&#32852;&#24615;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.17437</link><description>&lt;p&gt;
&#21033;&#29992;&#24773;&#24863;-&#35821;&#20041;&#30456;&#20851;&#24615;&#36827;&#34892;&#20849;&#24773;&#24335;&#22238;&#22797;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Exploiting Emotion-Semantic Correlations for Empathetic Response Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17437
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#24773;&#24863;-&#35821;&#20041;&#30456;&#20851;&#24615;&#26469;&#29983;&#25104;&#20849;&#24773;&#24335;&#23545;&#35805;&#22238;&#22797;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#21644;&#24773;&#24863;&#30340;&#20132;&#20114;&#26500;&#24314;&#21160;&#24577;&#24773;&#24863;-&#35821;&#20041;&#21521;&#37327;&#65292;&#25552;&#39640;&#20102;&#23545;&#24773;&#24863;&#19982;&#35821;&#20041;&#20851;&#32852;&#24615;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#24335;&#22238;&#22797;&#29983;&#25104;&#26088;&#22312;&#36890;&#36807;&#29702;&#35299;&#23545;&#35805;&#35821;&#35328;&#20013;&#35828;&#35805;&#32773;&#30340;&#24773;&#24863;&#24863;&#21463;&#26469;&#29983;&#25104;&#20849;&#24773;&#22238;&#22797;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#25429;&#25417;&#20132;&#38469;&#32773;&#35821;&#35328;&#20013;&#30340;&#24773;&#24863;&#35789;&#65292;&#24182;&#23558;&#20854;&#26500;&#24314;&#20026;&#38745;&#24577;&#21521;&#37327;&#65292;&#20197;&#24863;&#30693;&#24494;&#22937;&#30340;&#24773;&#24863;&#12290;&#28982;&#32780;&#65292;&#35821;&#35328;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#20013;&#30340;&#24773;&#24863;&#35789;&#26159;&#21160;&#24577;&#30340;&#65292;&#24182;&#19982;&#20854;&#20182;&#35821;&#27861;&#35821;&#20041;&#35282;&#33394;&#65288;&#21363;&#20855;&#26377;&#35821;&#20041;&#21547;&#20041;&#30340;&#35789;&#35821;&#65289;&#30456;&#20851;&#32852;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#24573;&#30053;&#20102;&#36825;&#20004;&#20010;&#29305;&#24449;&#65292;&#36825;&#24456;&#23481;&#26131;&#23548;&#33268;&#24773;&#24863;&#35823;&#35299;&#21644;&#20851;&#38190;&#35821;&#20041;&#30340;&#24573;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#30340;&#21160;&#24577;&#24773;&#24863;-&#35821;&#20041;&#30456;&#20851;&#24615;&#27169;&#22411;&#65288;ESCM&#65289;&#12290;ESCM&#36890;&#36807;&#19978;&#19979;&#25991;&#21644;&#24773;&#24863;&#30340;&#20132;&#20114;&#26500;&#24314;&#21160;&#24577;&#24773;&#24863;-&#35821;&#20041;&#21521;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20381;&#23384;&#26641;&#26469;&#21453;&#26144;&#24773;&#24863;&#19982;&#35821;&#20041;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17437v1 Announce Type: cross  Abstract: Empathetic response generation aims to generate empathetic responses by understanding the speaker's emotional feelings from the language of dialogue. Recent methods capture emotional words in the language of communicators and construct them as static vectors to perceive nuanced emotions. However, linguistic research has shown that emotional words in language are dynamic and have correlations with other grammar semantic roles, i.e., words with semantic meanings, in grammar. Previous methods overlook these two characteristics, which easily lead to misunderstandings of emotions and neglect of key semantics. To address this issue, we propose a dynamical Emotion-Semantic Correlation Model (ESCM) for empathetic dialogue generation tasks. ESCM constructs dynamic emotion-semantic vectors through the interaction of context and emotions. We introduce dependency trees to reflect the correlations between emotions and semantics. Based on dynamic em
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Contrastive EEG-Text Masked Autoencoder&#65288;CET-MAE&#65289;&#21644;E2T-PTR&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#26469;&#22686;&#24378;&#22522;&#20110;EEG&#30340;&#35821;&#35328;&#35299;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.17433</link><description>&lt;p&gt;
&#36890;&#36807;&#20174;&#39044;&#35757;&#32451;&#23545;&#27604;&#24615;EEG-&#25991;&#26412;&#33945;&#29256;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#36716;&#31227;&#30340;&#34920;&#31034;&#22686;&#24378;EEG&#21040;&#25991;&#26412;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17433
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Contrastive EEG-Text Masked Autoencoder&#65288;CET-MAE&#65289;&#21644;E2T-PTR&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#26469;&#22686;&#24378;&#22522;&#20110;EEG&#30340;&#35821;&#35328;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26080;&#21019;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#37325;&#24314;&#33258;&#28982;&#35821;&#35328;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20316;&#20026;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#30340;&#35821;&#35328;&#35299;&#30721;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;EEG&#30340;&#35821;&#35328;&#35299;&#30721;&#20173;&#22788;&#20110;&#21021;&#32423;&#38454;&#27573;&#65292;&#38754;&#20020;&#35832;&#22810;&#25216;&#26415;&#38382;&#39064;&#65292;&#22914;&#65306;1&#65289;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#25972;&#21512;&#36328;&#27169;&#24577;&#65288;EEG&#21644;&#25991;&#26412;&#20043;&#38388;&#65289;&#33258;&#23398;&#20064;&#19982;EEG&#29305;&#24449;&#25110;&#25991;&#26412;&#24207;&#21015;&#30340;&#27169;&#20869;&#33258;&#37325;&#26500;&#30340;&#28151;&#21512;&#31574;&#30053;&#65307;2&#65289;&#26410;&#20805;&#20998;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#22522;&#20110;EEG&#30340;&#35821;&#35328;&#35299;&#30721;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#24615;EEG-&#25991;&#26412;&#33945;&#29256;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;CET-MAE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#19987;&#29992;&#30340;&#22810;&#27969;&#32534;&#30721;&#22120;&#22312;EEG&#21644;&#25991;&#26412;&#20043;&#38388;&#20197;&#21450;&#20869;&#37096;&#36827;&#34892;&#22797;&#21512;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#22411;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;E2T-PTR&#65288;&#20351;&#29992;&#39044;&#35757;&#32451;&#21487;&#36716;&#31227;&#34920;&#31034;&#36827;&#34892;EEG&#21040;&#25991;&#26412;&#35299;&#30721;&#65289;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#32452;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17433v1 Announce Type: new  Abstract: Reconstructing natural language from non-invasive electroencephalography (EEG) holds great promise as a language decoding technology for brain-computer interfaces (BCIs). However, EEG-based language decoding is still in its nascent stages, facing several technical issues such as: 1) Absence of a hybrid strategy that can effectively integrate cross-modality (between EEG and text) self-learning with intra-modality self-reconstruction of EEG features or textual sequences; 2) Under-utilization of large language models (LLMs) to enhance EEG-based language decoding. To address above issues, we propose the Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that orchestrates compound self-supervised learning across and within EEG and text through a dedicated multi-stream encoder. Furthermore, we develop a framework called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable Representations), which leverages pre-trained modul
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;LLMs&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35774;&#35745;&#20102;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#19968;&#33268;&#24615;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;GPT3.5&#31561;&#27169;&#22411;</title><link>https://arxiv.org/abs/2402.17411</link><description>&lt;p&gt;
&#19968;&#33268;&#24615;&#33267;&#20851;&#37325;&#35201;&#65306;&#20174;&#40657;&#30418;&#35282;&#24230;&#25506;&#32034;LLMs&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Consistency Matters: Explore LLMs Consistency From a Black-Box Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17411
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;LLMs&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#35774;&#35745;&#20102;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#19968;&#33268;&#24615;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;GPT3.5&#31561;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22914;&#20170;&#65292;&#21830;&#19994;&#21644;&#24320;&#28304;&#23398;&#26415;LLM&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20027;&#27969;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;LLM&#19968;&#33268;&#24615;&#30340;&#30740;&#31350;&#20173;&#28982;&#19981;&#36275;&#65292;&#36825;&#24847;&#21619;&#30528;&#22312;LLM&#30740;&#31350;&#21644;&#37096;&#32626;&#30340;&#21508;&#20010;&#38454;&#27573;&#20013;&#65292;&#20854;&#20869;&#37096;&#21442;&#25968;&#21644;&#33021;&#21147;&#24212;&#20445;&#25345;&#19981;&#21464;&#12290;&#36825;&#19968;&#38382;&#39064;&#23384;&#22312;&#20110;&#24037;&#19994;&#21644;&#23398;&#26415;&#39046;&#22495;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#36890;&#24120;&#32791;&#26102;&#19988;&#21171;&#21147;&#23494;&#38598;&#65292;&#36824;&#26377;&#39069;&#22806;&#30340;&#20108;&#27425;&#37096;&#32626;&#25104;&#26412;&#65292;&#23548;&#33268;&#32463;&#27982;&#21644;&#26102;&#38388;&#25439;&#22833;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;LLM&#19968;&#33268;&#24615;&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#20960;&#20010;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19981;&#21516;&#35268;&#27169;&#30340;&#27169;&#22411;&#36827;&#34892;&#20027;&#35201;&#23454;&#39564;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;LightGBM&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#24230;&#37327;&#65288;&#22914;ROUGE&#12289;BLEU&#12289;METEOR&#65289;&#20316;&#20026;&#27169;&#22411;&#35757;&#32451;&#25152;&#38656;&#30340;&#29305;&#24449;&#12290;&#26368;&#32456;&#32467;&#26524;&#36229;&#36807;&#20102;&#20154;&#24037;&#35780;&#20272;&#20197;&#21450;GPT3.5&#21644;&#20854;&#20182;&#27169;&#22411;&#22312;&#20027;&#35201;&#23454;&#39564;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17411v1 Announce Type: new  Abstract: Nowadays both commercial and open-source academic LLM have become the mainstream models of NLP. However, there is still a lack of research on LLM consistency, meaning that throughout the various stages of LLM research and deployment, its internal parameters and capabilities should remain unchanged. This issue exists in both the industrial and academic sectors. The solution to this problem is often time-consuming and labor-intensive, and there is also an additional cost of secondary deployment, resulting in economic and time losses. To fill this gap, we build an LLM consistency task dataset and design several baselines. Additionally, we choose models of diverse scales for the main experiments. Specifically, in the LightGBM experiment, we used traditional NLG metrics (i.e., ROUGE, BLEU, METEOR) as the features needed for model training. The final result exceeds the manual evaluation and GPT3.5 as well as other models in the main experiment
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#37325;&#20889;&#31995;&#32479;&#21551;&#21457;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#31639;&#27861;&#20219;&#21153;&#65292;&#36890;&#36807;Selector&#12289;Solver&#21644;Combiner&#19977;&#20010;&#19987;&#38376;&#27169;&#22359;&#23454;&#29616;&#31639;&#27861;&#20219;&#21153;&#30340;&#31616;&#21270;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#22806;&#25512;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.17407</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#37325;&#20889;&#31995;&#32479;&#35299;&#20915;&#31639;&#27861;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Neural Rewriting System to Solve Algorithmic Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17407
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#37325;&#20889;&#31995;&#32479;&#21551;&#21457;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#31639;&#27861;&#20219;&#21153;&#65292;&#36890;&#36807;Selector&#12289;Solver&#21644;Combiner&#19977;&#20010;&#19987;&#38376;&#27169;&#22359;&#23454;&#29616;&#31639;&#27861;&#20219;&#21153;&#30340;&#31616;&#21270;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#22806;&#25512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20173;&#28982;&#38590;&#20197;&#23398;&#20064;&#38656;&#35201;&#31995;&#32479;&#24212;&#29992;&#32452;&#21512;&#35268;&#21017;&#26469;&#35299;&#20915;&#36229;&#20986;&#20998;&#24067;&#38382;&#39064;&#23454;&#20363;&#30340;&#31639;&#27861;&#31243;&#24207;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#21019;&#26041;&#27861;&#26469;&#23398;&#20064;&#21463;&#37325;&#20889;&#31995;&#32479;&#21551;&#21457;&#30340;&#31639;&#27861;&#20219;&#21153;&#65292;&#37325;&#20889;&#31995;&#32479;&#26159;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#32463;&#20856;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#37325;&#20889;&#31995;&#32479;&#21487;&#20197;&#34987;&#23454;&#29616;&#20026;&#19968;&#20010;&#30001;&#19987;&#38376;&#27169;&#22359;&#32452;&#25104;&#30340;&#31070;&#32463;&#26550;&#26500;&#65306;&#36873;&#25321;&#22120;&#35782;&#21035;&#35201;&#22788;&#29702;&#30340;&#30446;&#26631;&#23376;&#34920;&#36798;&#24335;&#65292;&#27714;&#35299;&#22120;&#36890;&#36807;&#35745;&#31639;&#30456;&#24212;&#30340;&#32467;&#26524;&#31616;&#21270;&#23376;&#34920;&#36798;&#24335;&#65292;&#32452;&#21512;&#22120;&#36890;&#36807;&#29992;&#25552;&#20379;&#30340;&#35299;&#20915;&#26041;&#26696;&#26367;&#25442;&#23376;&#34920;&#36798;&#24335;&#29983;&#25104;&#21407;&#22987;&#34920;&#36798;&#24335;&#30340;&#26032;&#29256;&#26412;&#12290;&#25105;&#20204;&#22312;&#19977;&#31181;&#28041;&#21450;&#31616;&#21270;&#28041;&#21450;&#21015;&#34920;&#12289;&#31639;&#26415;&#21644;&#20195;&#25968;&#34920;&#36798;&#24335;&#30340;&#31526;&#21495;&#20844;&#24335;&#30340;&#31639;&#27861;&#20219;&#21153;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#25152;&#25552;&#26550;&#26500;&#30340;&#22806;&#25512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17407v1 Announce Type: cross  Abstract: Modern neural network architectures still struggle to learn algorithmic procedures that require to systematically apply compositional rules to solve out-of-distribution problem instances. In this work, we propose an original approach to learn algorithmic tasks inspired by rewriting systems, a classic framework in symbolic artificial intelligence. We show that a rewriting system can be implemented as a neural architecture composed by specialized modules: the Selector identifies the target sub-expression to process, the Solver simplifies the sub-expression by computing the corresponding result, and the Combiner produces a new version of the original expression by replacing the sub-expression with the solution provided. We evaluate our model on three types of algorithmic tasks that require simplifying symbolic formulas involving lists, arithmetic, and algebraic expressions. We test the extrapolation capabilities of the proposed architectu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#25972;&#21512;&#26032;&#20449;&#24687;&#12289;&#20445;&#30041;&#24050;&#23398;&#30693;&#35782;&#24182;&#22686;&#24378;&#36328;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#30340;&#31574;&#30053;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17400</link><description>&lt;p&gt;
&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65306;&#35265;&#35299;&#19982;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating Continual Pretraining in Large Language Models: Insights and Implications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#25972;&#21512;&#26032;&#20449;&#24687;&#12289;&#20445;&#30041;&#24050;&#23398;&#30693;&#35782;&#24182;&#22686;&#24378;&#36328;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#30340;&#31574;&#30053;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#19981;&#26029;&#23398;&#20064;&#65288;CL&#65289;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#37325;&#28857;&#26159;&#21046;&#23450;&#39640;&#25928;&#21487;&#25345;&#32493;&#22521;&#35757;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#25345;&#32493;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#20351;LLMs&#33021;&#22815;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#26032;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#20197;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#24182;&#22686;&#24378;&#36328;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#33021;&#21147;&#32780;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#35782;&#21035;&#30340;&#36807;&#31243;&#12290;&#19982;&#20197;&#24448;&#20027;&#35201;&#38598;&#20013;&#20110;&#26377;&#38480;&#20219;&#21153;&#25110;&#39046;&#22495;&#24182;&#20027;&#35201;&#26088;&#22312;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#30340;&#20808;&#21069;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#35780;&#20272;&#20102;LLMs&#23545;&#23454;&#38469;&#24773;&#26223;&#20013;&#19981;&#26029;&#21464;&#21270;&#30340;&#25968;&#25454;&#26223;&#35266;&#30340;&#36866;&#24212;&#24615;&#21644;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#34913;&#37327;LLMs&#23545;&#36825;&#20123;&#19981;&#26029;&#28436;&#21464;&#30340;&#25968;&#25454;&#29615;&#22659;&#30340;&#36866;&#24212;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17400v1 Announce Type: new  Abstract: This paper studies the evolving domain of Continual Learning (CL) in large language models (LLMs), with a focus on developing strategies for efficient and sustainable training. Our primary emphasis is on continual domain-adaptive pretraining, a process designed to equip LLMs with the ability to integrate new information from various domains while retaining previously learned knowledge and enhancing cross-domain knowledge transfer without relying on domain-specific identification. Unlike previous studies, which mostly concentrate on a limited selection of tasks or domains and primarily aim to address the issue of forgetting, our research evaluates the adaptability and capabilities of LLMs to changing data landscapes in practical scenarios. To this end, we introduce a new benchmark designed to measure the adaptability of LLMs to these evolving data environments, offering a comprehensive framework for evaluation. We examine the impact of mo
&lt;/p&gt;</description></item><item><title>&#23545;GPT-4&#22312;&#31639;&#27861;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;&#37319;&#29992;&#20808;&#36827;&#30340;&#25552;&#31034;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17396</link><description>&lt;p&gt;
&#22312;&#31639;&#27861;&#38382;&#39064;&#19978;&#23545;GPT-4&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65306;&#20851;&#20110;&#25552;&#31034;&#31574;&#30053;&#30340;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of Prompting Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17396
&lt;/p&gt;
&lt;p&gt;
&#23545;GPT-4&#22312;&#31639;&#27861;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21457;&#29616;&#37319;&#29992;&#20808;&#36827;&#30340;&#25552;&#31034;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22312;&#28023;&#37327;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#33719;&#24471;&#30340;&#30693;&#35782;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#37325;&#26032;&#21033;&#29992;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#65288;&#25110;&#26681;&#26412;&#19981;&#38656;&#35201;&#65289;&#35843;&#25972;&#27493;&#39588;&#65292;&#20174;&#32780;&#38761;&#26032;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#24050;&#32463;&#21453;&#22797;&#26174;&#31034;LLMs&#32570;&#20047;&#31995;&#32479;&#21270;&#27867;&#21270;&#65292;&#36825;&#20351;&#24471;&#26080;&#27861;&#23558;&#23398;&#20064;&#21040;&#30340;&#32479;&#35745;&#35268;&#24459;&#22806;&#25512;&#21040;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20854;&#20013;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;LLMs&#65292;GPT-4&#65292;&#22312;&#19977;&#20010;&#31639;&#27861;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#20123;&#20219;&#21153;&#36890;&#36807;&#20004;&#20010;&#21442;&#25968;&#25511;&#21046;&#38382;&#39064;&#38590;&#24230;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;GPT-4&#19982;&#20854;&#21069;&#36523;&#65288;GPT-3.5&#65289;&#20197;&#21450;&#26368;&#36817;&#20171;&#32461;&#30340;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#21464;&#20307;&#65292;&#21363;&#31070;&#32463;&#25968;&#25454;&#36335;&#30001;&#22120;&#65292;&#22312;&#35299;&#20915;&#31867;&#20284;&#20219;&#21153;&#26102;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#37319;&#29992;&#20808;&#36827;&#30340;&#25552;&#31034;&#25216;&#26415;&#21487;&#20197;&#20351;GPT-4&#36798;&#21040;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17396v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized the field of Natural Language Processing thanks to their ability to reuse knowledge acquired on massive text corpora on a wide variety of downstream tasks, with minimal (if any) tuning steps. At the same time, it has been repeatedly shown that LLMs lack systematic generalization, which allows to extrapolate the learned statistical regularities outside the training distribution. In this work, we offer a systematic benchmarking of GPT-4, one of the most advanced LLMs available, on three algorithmic tasks characterized by the possibility to control the problem difficulty with two parameters. We compare the performance of GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the Transformer-Encoder architecture recently introduced to solve similar tasks, the Neural Data Router. We find that the deployment of advanced prompting techniques allows GPT-4 to reach superior accuracy o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#35821;&#20041;&#36335;&#24452;&#30340;&#31895;&#31890;&#24230;&#21010;&#20998;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#35821;&#35328;&#25991;&#26412;&#30340;&#32467;&#26500;&#21644;&#32858;&#31867;&#24773;&#20917;&#65292;&#25903;&#25345;&#32467;&#26500;&#21644;&#32858;&#31867;&#24046;&#24322;&#30340;&#20551;&#35774;&#12290;</title><link>https://arxiv.org/abs/2402.17392</link><description>&lt;p&gt;
&#21457;&#29616;&#26426;&#22120;&#20154;&#65306;&#23545;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#35821;&#20041;&#36335;&#24452;&#30340;&#31895;&#31890;&#24230;&#21010;&#20998;
&lt;/p&gt;
&lt;p&gt;
Spot the bot: Coarse-Grained Partition of Semantic Paths for Bots and Humans
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17392
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#35821;&#20041;&#36335;&#24452;&#30340;&#31895;&#31890;&#24230;&#21010;&#20998;&#65292;&#27604;&#36739;&#20102;&#19981;&#21516;&#35821;&#35328;&#25991;&#26412;&#30340;&#32467;&#26500;&#21644;&#32858;&#31867;&#24773;&#20917;&#65292;&#25903;&#25345;&#32467;&#26500;&#21644;&#32858;&#31867;&#24046;&#24322;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#65292;&#25216;&#26415;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65306;&#26426;&#22120;&#20154;&#27491;&#22312;&#25776;&#20889;&#35780;&#35770;&#12289;&#25991;&#31456;&#21644;&#35780;&#35770;&#12290;&#22240;&#27492;&#65292;&#37325;&#35201;&#30340;&#26159;&#35201;&#30693;&#36947;&#25991;&#26412;&#26159;&#30001;&#20154;&#31867;&#32534;&#20889;&#36824;&#26159;&#30001;&#26426;&#22120;&#20154;&#32534;&#20889;&#12290;&#26412;&#25991;&#26088;&#22312;&#27604;&#36739;&#20154;&#31867;&#32534;&#20889;&#21644;&#26426;&#22120;&#20154;&#29983;&#25104;&#25991;&#26412;&#30340;&#35821;&#20041;&#36335;&#24452;&#31895;&#31890;&#24230;&#21010;&#20998;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#25991;&#23398;&#25991;&#26412;&#21644;&#20960;&#20010;&#26426;&#22120;&#20154;&#29983;&#25104;&#25991;&#26412;&#30340;n-gram&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#24773;&#20917;&#12290;&#20551;&#35774;&#26159;&#36825;&#20123;&#32467;&#26500;&#21644;&#32858;&#31867;&#26159;&#19981;&#21516;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25903;&#25345;&#20102;&#36825;&#19968;&#20551;&#35774;&#12290;&#30001;&#20110;&#19981;&#21516;&#35821;&#35328;&#30340;&#35821;&#20041;&#32467;&#26500;&#21487;&#33021;&#19981;&#21516;&#65292;&#22240;&#27492;&#25105;&#20204;&#35843;&#26597;&#20102;&#20420;&#35821;&#12289;&#33521;&#35821;&#12289;&#24503;&#35821;&#21644;&#36234;&#21335;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17392v1 Announce Type: new  Abstract: Nowadays, technology is rapidly advancing: bots are writing comments, articles, and reviews. Due to this fact, it is crucial to know if the text was written by a human or by a bot. This paper focuses on comparing structures of the coarse-grained partitions of semantic paths for human-written and bot-generated texts. We compare the clusterizations of datasets of n-grams from literary texts and texts generated by several bots. The hypothesis is that the structures and clusterizations are different. Our research supports the hypothesis. As the semantic structure may be different for different languages, we investigate Russian, English, German, and Vietnamese languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FairBelief&#65292;&#19968;&#31181;&#29992;&#20110;&#25429;&#33719;&#21644;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#23475;&#20449;&#24565;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20844;&#24179;&#25968;&#25454;&#38598;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;LM&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#36825;&#20123;LM&#21487;&#33021;&#23384;&#22312;&#26377;&#23475;&#20449;&#24565;&#12290;</title><link>https://arxiv.org/abs/2402.17389</link><description>&lt;p&gt;
&#20844;&#24179;&#20449;&#24565; - &#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26377;&#23475;&#20449;&#24565;
&lt;/p&gt;
&lt;p&gt;
FairBelief - Assessing Harmful Beliefs in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FairBelief&#65292;&#19968;&#31181;&#29992;&#20110;&#25429;&#33719;&#21644;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#20013;&#26377;&#23475;&#20449;&#24565;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20844;&#24179;&#25968;&#25454;&#38598;&#23545;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;LM&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#36825;&#20123;LM&#21487;&#33021;&#23384;&#22312;&#26377;&#23475;&#20449;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24050;&#34987;&#35777;&#26126;&#23384;&#22312;&#19981;&#33391;&#20559;&#35265;&#65292;&#22914;&#26524;&#36825;&#20123;&#31995;&#32479;&#22312;&#27809;&#26377;&#20180;&#32454;&#36827;&#34892;&#20844;&#24179;&#23457;&#35745;&#30340;&#24773;&#20917;&#19979;&#38598;&#25104;&#21040;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#21487;&#33021;&#20250;&#20260;&#23475;&#23569;&#25968;&#32676;&#20307;&#21644;&#34987;&#24573;&#35270;&#30340;&#32676;&#20307;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FairBelief&#65292;&#19968;&#31181;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#33719;&#21644;&#35780;&#20272;&#20449;&#24565;&#65292;&#21363;LM&#21487;&#33021;&#20197;&#19981;&#21516;&#31243;&#24230;&#30340;&#30830;&#20449;&#24230;&#23884;&#20837;&#30340;&#21629;&#39064;&#65292;&#36825;&#20123;&#21629;&#39064;&#26263;&#20013;&#24433;&#21709;&#20854;&#39044;&#27979;&#12290;&#36890;&#36807;FairBelief&#65292;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#26469;&#30740;&#31350;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;LM&#22312;&#20808;&#21069;&#34987;&#24573;&#35270;&#30340;&#19981;&#21516;&#36724;&#19978;&#30340;&#34892;&#20026;&#65292;&#27604;&#22914;&#27169;&#22411;&#35268;&#27169;&#21644;&#21487;&#33021;&#24615;&#65292;&#35780;&#20272;&#23545;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#37327;&#21270;LM&#36755;&#20986;&#20260;&#23475;&#31243;&#24230;&#30340;&#20844;&#24179;&#25968;&#25454;&#38598;&#30340;&#39044;&#27979;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#27169;&#22411;&#21457;&#20986;&#30340;&#20449;&#24565;&#36827;&#34892;&#28145;&#20837;&#30340;&#23450;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#23558;FairBelief&#24212;&#29992;&#20110;&#33521;&#35821;LMs&#65292;&#21457;&#29616;&#23613;&#31649;&#36825;&#20123;&#26550;&#26500;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20063;&#21487;&#33021;&#23384;&#22312;&#26377;&#23475;&#20449;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17389v1 Announce Type: cross  Abstract: Language Models (LMs) have been shown to inherit undesired biases that might hurt minorities and underrepresented groups if such systems were integrated into real-world applications without careful fairness auditing. This paper proposes FairBelief, an analytical approach to capture and assess beliefs, i.e., propositions that an LM may embed with different degrees of confidence and that covertly influence its predictions. With FairBelief, we leverage prompting to study the behavior of several state-of-the-art LMs across different previously neglected axes, such as model scale and likelihood, assessing predictions on a fairness dataset specifically designed to quantify LMs' outputs' hurtfulness. Finally, we conclude with an in-depth qualitative assessment of the beliefs emitted by the models. We apply FairBelief to English LMs, revealing that, although these architectures enable high performances on diverse natural language processing ta
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;KoDialogBench&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#38889;&#35821;&#23545;&#35805;&#20013;&#30340;&#20250;&#35805;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.17377</link><description>&lt;p&gt;
KoDialogBench: &#29992;&#38889;&#35821;&#23545;&#35805;&#22522;&#20934;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#20250;&#35805;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
KoDialogBench: Evaluating Conversational Understanding of Language Models with Korean Dialogue Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17377
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;KoDialogBench&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#38889;&#35821;&#23545;&#35805;&#20013;&#30340;&#20250;&#35805;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#34987;&#37096;&#32626;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#21161;&#25163;&#65292;&#27169;&#22411;&#22312;&#29992;&#25143;&#30340;&#27597;&#35821;&#20013;&#36827;&#34892;&#23545;&#35805;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#35757;&#32451;&#26102;&#28085;&#30422;&#20102;&#22810;&#31181;&#35821;&#35328;&#65292;&#20294;&#23545;&#23427;&#20204;&#22312;&#38889;&#35821;&#31561;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#29087;&#32451;&#31243;&#24230;&#32570;&#20047;&#20840;&#38754;&#35780;&#20272;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KoDialogBench&#65292;&#35813;&#22522;&#20934;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#38889;&#35821;&#23545;&#35805;&#20013;&#30340;&#20250;&#35805;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#20844;&#20849;&#26469;&#28304;&#25910;&#38598;&#26085;&#24120;&#35805;&#39064;&#30340;&#38889;&#35821;&#23545;&#35805;&#65292;&#25110;&#23558;&#20854;&#20182;&#35821;&#35328;&#30340;&#23545;&#35805;&#36827;&#34892;&#32763;&#35793;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#23545;&#35805;&#32467;&#26500;&#21270;&#20026;&#19981;&#21516;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20174;&#23545;&#35805;&#29702;&#35299;&#21040;&#21709;&#24212;&#36873;&#25321;&#20219;&#21153;&#30340;&#24191;&#27867;&#33539;&#22260;&#12290;&#21033;&#29992;&#25552;&#20986;&#30340;&#22522;&#20934;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21508;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#35780;&#20272;&#21644;&#20998;&#26512;&#65292;&#20197;&#34913;&#37327;&#23545;&#38889;&#35821;&#23545;&#35805;&#30340;&#22522;&#30784;&#29702;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23384;&#22312;&#26126;&#26174;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17377v1 Announce Type: new  Abstract: As language models are often deployed as chatbot assistants, it becomes a virtue for models to engage in conversations in a user's first language. While these models are trained on a wide range of languages, a comprehensive evaluation of their proficiency in low-resource languages such as Korean has been lacking. In this work, we introduce KoDialogBench, a benchmark designed to assess language models' conversational capabilities in Korean. To this end, we collect native Korean dialogues on daily topics from public sources, or translate dialogues from other languages. We then structure these conversations into diverse test datasets, spanning from dialogue comprehension to response selection tasks. Leveraging the proposed benchmark, we conduct extensive evaluations and analyses of various language models to measure a foundational understanding of Korean dialogues. Experimental results indicate that there exists significant room for improve
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26202;&#26399;&#21476;&#20195;&#21644;&#20013;&#19990;&#32426;&#24076;&#20271;&#26469;&#35799;&#27468;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#19987;&#23478;&#23545;&#38544;&#21947;&#30340;&#27880;&#37322;&#65292;&#26088;&#22312;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.17371</link><description>&lt;p&gt;
&#26089;&#26399;&#20013;&#19990;&#32426;&#24076;&#20271;&#26469;&#35799;&#27468;&#38544;&#21947;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
A Dataset for Metaphor Detection in Early Medieval Hebrew Poetry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17371
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26202;&#26399;&#21476;&#20195;&#21644;&#20013;&#19990;&#32426;&#24076;&#20271;&#26469;&#35799;&#27468;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#19987;&#23478;&#23545;&#38544;&#21947;&#30340;&#27880;&#37322;&#65292;&#26088;&#22312;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26202;&#26399;&#21476;&#20195;&#21644;&#20013;&#19990;&#32426;&#24076;&#20271;&#26469;&#25991;&#26412;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#25991;&#26412;&#65292;&#23427;&#20204;&#20195;&#34920;&#30528;&#22307;&#32463;&#24076;&#20271;&#26469;&#35821;&#21644;&#29616;&#20195;&#24076;&#20271;&#26469;&#35821;&#20043;&#38388;&#30340;&#37325;&#35201;&#35821;&#35328;&#21644;&#25991;&#21270;&#26725;&#26753;&#12290;&#35799;&#27468;&#22312;&#36825;&#20123;&#25991;&#26412;&#20013;&#21344;&#25454;&#37325;&#35201;&#22320;&#20301;&#65292;&#20854;&#20027;&#35201;&#29305;&#24449;&#20043;&#19968;&#23601;&#26159;&#39057;&#32321;&#20351;&#29992;&#38544;&#21947;&#12290;&#21306;&#20998;&#27604;&#21947;&#21644;&#23383;&#38754;&#35821;&#35328;&#20351;&#29992;&#26159;&#20154;&#25991;&#23398;&#32773;&#30340;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#23398;&#12289;&#35821;&#35328;&#23398;&#21644;&#35299;&#37322;&#23398;&#39046;&#22495;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#30340;&#20855;&#26377;&#19987;&#23478;&#27880;&#37322;&#30340;&#26202;&#26399;&#21476;&#20195;&#21644;&#20013;&#19990;&#32426;&#24076;&#20271;&#26469;&#35799;&#27468;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20123;&#22522;&#20934;&#32467;&#26524;&#65292;&#24076;&#26395;&#33021;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17371v1 Announce Type: new  Abstract: There is a large volume of late antique and medieval Hebrew texts. They represent a crucial linguistic and cultural bridge between Biblical and modern Hebrew. Poetry is prominent in these texts and one of its main haracteristics is the frequent use of metaphor. Distinguishing figurative and literal language use is a major task for scholars of the Humanities, especially in the fields of literature, linguistics, and hermeneutics. This paper presents a new, challenging dataset of late antique and medieval Hebrew poetry with expert annotations of metaphor, as well as some baseline results, which we hope will facilitate further research in this area.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#40784;&#33539;&#24335;&#8212;&#8212;&#20248;&#20808;&#35268;&#21017;&#36319;&#38543;&#65292;&#22312;&#23545;&#35805;&#20013;&#23558;&#35268;&#21017;&#20316;&#20026;&#20027;&#35201;&#25511;&#21046;&#26426;&#21046;&#65292;&#20197;&#30830;&#20445;&#23545;&#40784;&#24182;&#25552;&#20986;&#20102;PriorityDistill&#65292;&#23454;&#29616;&#20102;&#20174;LLM&#27169;&#25311;&#20013;&#25552;&#21462;&#20248;&#20808;&#36319;&#38543;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#30830;&#20445;&#35268;&#21017;&#30340;&#24378;&#22823;&#25972;&#21512;&#21644;&#36981;&#23432;</title><link>https://arxiv.org/abs/2402.17358</link><description>&lt;p&gt;
SoFA&#65306;&#36890;&#36807;&#20248;&#20808;&#35268;&#21017;&#36319;&#38543;&#36827;&#34892;&#23631;&#34109;&#24335;&#21363;&#26102;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
SoFA: Shielded On-the-fly Alignment via Priority Rule Following
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#40784;&#33539;&#24335;&#8212;&#8212;&#20248;&#20808;&#35268;&#21017;&#36319;&#38543;&#65292;&#22312;&#23545;&#35805;&#20013;&#23558;&#35268;&#21017;&#20316;&#20026;&#20027;&#35201;&#25511;&#21046;&#26426;&#21046;&#65292;&#20197;&#30830;&#20445;&#23545;&#40784;&#24182;&#25552;&#20986;&#20102;PriorityDistill&#65292;&#23454;&#29616;&#20102;&#20174;LLM&#27169;&#25311;&#20013;&#25552;&#21462;&#20248;&#20808;&#36319;&#38543;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#30830;&#20445;&#35268;&#21017;&#30340;&#24378;&#22823;&#25972;&#21512;&#21644;&#36981;&#23432;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#23545;&#40784;&#38382;&#39064;&#28041;&#21450;&#23558;&#23427;&#20204;&#36866;&#24212;&#24191;&#27867;&#30340;&#20154;&#31867;&#20215;&#20540;&#35266;&#12290;&#30001;&#20110;&#20559;&#22909;&#30340;&#22810;&#26679;&#24615;&#21644;&#30417;&#31649;&#26631;&#20934;&#30340;&#25361;&#25112;&#65292;&#36825;&#19968;&#35201;&#27714;&#20351;&#29616;&#26377;&#30340;&#23545;&#40784;&#26041;&#27861;&#38754;&#20020;&#22256;&#38590;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#40784;&#33539;&#24335;&#65292;&#21363;&#20248;&#20808;&#35268;&#21017;&#36319;&#38543;&#65292;&#23427;&#23558;&#35268;&#21017;&#23450;&#20041;&#20026;&#27599;&#27425;&#23545;&#35805;&#20013;&#30340;&#20027;&#35201;&#25511;&#21046;&#26426;&#21046;&#65292;&#24182;&#23558;&#20854;&#20248;&#20808;&#20110;&#29992;&#25143;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#20998;&#26512;&#26174;&#31034;&#65292;&#21363;&#20351;&#20687;GPT-4&#36825;&#26679;&#20808;&#36827;&#30340;LLMs&#20063;&#23384;&#22312;&#29702;&#35299;&#21644;&#20248;&#20808;&#32771;&#34385;&#35268;&#21017;&#30340;&#32570;&#38519;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PriorityDistill&#65292;&#19968;&#31181;&#20174;LLM&#27169;&#25311;&#20013;&#33795;&#21462;&#20248;&#20808;&#36319;&#38543;&#20449;&#21495;&#30340;&#21322;&#33258;&#21160;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#35268;&#21017;&#30340;&#24378;&#22823;&#25972;&#21512;&#21644;&#36981;&#23432;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26377;&#25928;&#22320;&#36890;&#36807;&#20165;&#20351;&#29992;&#19968;&#20010;&#36890;&#29992;&#35268;&#21017;&#26469;&#26368;&#23567;&#21270;&#19981;&#23545;&#40784;&#38382;&#39064;&#65292;&#32780;&#19988;&#21487;&#20197;&#24179;&#28369;&#22320;&#36866;&#24212;&#21508;&#31181;&#26410;&#35265;&#35268;&#21017;&#65292;&#30830;&#20445;&#23427;&#20204;&#20813;&#21463;&#21163;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17358v1 Announce Type: new  Abstract: The alignment problem in Large Language Models (LLMs) involves adapting them to the broad spectrum of human values. This requirement challenges existing alignment methods due to diversity of preferences and regulatory standards. This paper introduces a novel alignment paradigm, priority rule following, which defines rules as the primary control mechanism in each dialog, prioritizing them over user instructions. Our preliminary analysis reveals that even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules. Therefore, we present PriorityDistill, a semi-automated approach for distilling priority following signals from LLM simulations to ensure robust rule integration and adherence. Our experiments show that this method not only effectively minimizes misalignments utilizing only one general rule but also adapts smoothly to various unseen rules, ensuring they are shielded from hijacking and that 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;RECOST&#26694;&#26550;&#65292;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#35780;&#20272;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#30340;&#26679;&#26412;&#65292;&#36890;&#36807;&#30456;&#23545;&#39044;&#27979;&#29109;&#35299;&#20915;&#25968;&#25454;&#39640;&#25928;&#25351;&#23548;&#35843;&#25972;&#20013;&#25968;&#25454;&#36136;&#37327;&#19981;&#36275;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.17355</link><description>&lt;p&gt;
RECOST: &#22806;&#37096;&#30693;&#35782;&#24341;&#23548;&#30340;&#25968;&#25454;&#39640;&#25928;&#25351;&#23548;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
RECOST: External Knowledge Guided Data-efficient Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17355
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;RECOST&#26694;&#26550;&#65292;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#35780;&#20272;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#30340;&#26679;&#26412;&#65292;&#36890;&#36807;&#30456;&#23545;&#39044;&#27979;&#29109;&#35299;&#20915;&#25968;&#25454;&#39640;&#25928;&#25351;&#23548;&#35843;&#25972;&#20013;&#25968;&#25454;&#36136;&#37327;&#19981;&#36275;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39046;&#22495;&#20013;&#65292;&#25351;&#23548;&#35843;&#25972;&#30340;&#36807;&#31243;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#27493;&#12290;&#32771;&#34385;&#21040;&#39640;&#26114;&#30340;&#35745;&#31639;&#33021;&#21147;&#24320;&#38144;&#65292;&#25552;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#25351;&#23548;&#35843;&#25972;&#65292;&#20197;&#20943;&#23569;&#35813;&#36807;&#31243;&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#26088;&#22312;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#31034;&#24615;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#30446;&#21069;&#22823;&#22810;&#25968;&#25968;&#25454;&#39640;&#25928;&#25351;&#23548;&#35843;&#25972;&#26041;&#27861;&#39640;&#24230;&#20381;&#36182;&#20110;&#21407;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;&#20851;&#20110;&#30001;LLMs&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#20010;&#39046;&#22495;&#20013;&#24120;&#35265;&#30340;&#24773;&#20917;&#65292;&#33039;&#26679;&#26412;&#29978;&#33267;&#20250;&#27604;&#20854;&#20182;&#26679;&#26412;&#34987;&#26356;&#39640;&#27010;&#29575;&#22320;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65288;&#30456;&#20851;&#31034;&#20363;&#25110;&#27573;&#33853;&#65289;&#36890;&#36807;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#30456;&#23545;&#39044;&#27979;&#29109;&#35780;&#20272;&#30001;LLMs&#21512;&#25104;&#30340;&#36825;&#20123;&#26679;&#26412;&#12290;&#22522;&#20110;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; \textbf{RECOST} &#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25972;&#21512;&#20102;&#22806;&#37096;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17355v1 Announce Type: new  Abstract: In the current landscape of large language models (LLMs), the process of instruction tuning serves as an essential step. Considering the high computing power overhead, data-efficient instruction tuning was proposed to reduce the training data size in this process, aiming at selecting high-quality instructional data. Nevertheless, we argue that most current data-efficient instruction-tuning methods are highly dependent on the quality of the original instruction-tuning dataset. When it comes to datasets synthesized by LLMs, a common scenario in this field, dirty samples will even be selected with a higher probability than other samples. To address these challenges, we utilized external knowledge (relevant examples or paragraphs) to evaluate those samples synthesized by LLMs with an in-context-based relative predictive entropy. Based on the new metric, we proposed a framework, dubbed as \textbf{RECOST}, which integrates external-knowledge-b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36890;&#29992;&#39046;&#22495;&#19978;&#19979;&#25991;&#29983;&#25104;&#21512;&#25104;MCQA&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#26080;&#38656;&#20381;&#36182;&#25163;&#21160;&#27880;&#37322;&#65292;&#23454;&#39564;&#34920;&#26126;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17333</link><description>&lt;p&gt;
&#36890;&#36807;&#36890;&#29992;&#35821;&#26009;&#24211;&#23454;&#29616;&#26080;&#30417;&#30563;&#30340;&#22810;&#36873;&#39064;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Unsupervised multiple choices question answering via universal corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36890;&#29992;&#39046;&#22495;&#19978;&#19979;&#25991;&#29983;&#25104;&#21512;&#25104;MCQA&#25968;&#25454;&#30340;&#26694;&#26550;&#65292;&#26080;&#38656;&#20381;&#36182;&#25163;&#21160;&#27880;&#37322;&#65292;&#23454;&#39564;&#34920;&#26126;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#38382;&#31572;&#26159;&#19968;&#39033;&#26377;&#21069;&#36884;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21487;&#20197;&#20943;&#36731;&#22312;&#26032;&#39046;&#22495;&#26500;&#24314;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#30340;&#36127;&#25285;&#12290;&#36825;&#28608;&#21169;&#25105;&#20204;&#30740;&#31350;&#26080;&#30417;&#30563;&#30340;&#22810;&#36873;&#39064;&#38382;&#31572;&#65288;MCQA&#65289;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20165;&#22522;&#20110;&#36890;&#29992;&#39046;&#22495;&#19978;&#19979;&#25991;&#29983;&#25104;&#21512;&#25104;&#30340;MCQA&#25968;&#25454;&#65292;&#32780;&#19981;&#20381;&#36182;&#20219;&#20309;&#24418;&#24335;&#30340;&#25163;&#21160;&#27880;&#37322;&#12290;&#21487;&#33021;&#30340;&#31572;&#26696;&#34987;&#25552;&#21462;&#24182;&#29992;&#20110;&#29983;&#25104;&#30456;&#20851;&#38382;&#39064;&#65292;&#28982;&#21518;&#25105;&#20204;&#21033;&#29992;&#21629;&#21517;&#23454;&#20307;&#65288;NE&#65289;&#21644;&#30693;&#35782;&#22270;&#35889;&#26469;&#21457;&#29616;&#21512;&#29702;&#30340;&#20998;&#25955;&#22240;&#32032;&#65292;&#24418;&#25104;&#23436;&#25972;&#30340;&#21512;&#25104;&#26679;&#26412;&#12290;&#23545;&#22810;&#20010;MCQA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17333v1 Announce Type: new  Abstract: Unsupervised question answering is a promising yet challenging task, which alleviates the burden of building large-scale annotated data in a new domain. It motivates us to study the unsupervised multiple-choice question answering (MCQA) problem. In this paper, we propose a novel framework designed to generate synthetic MCQA data barely based on contexts from the universal domain without relying on any form of manual annotation. Possible answers are extracted and used to produce related questions, then we leverage both named entities (NE) and knowledge graphs to discover plausible distractors to form complete synthetic samples. Experiments on multiple MCQA datasets demonstrate the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SKT5SciSumm&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#24341;&#25991;&#20449;&#24687;&#30340;&#21464;&#25442;&#22120;&#21644;T5&#31995;&#21015;&#27169;&#22411;&#65292;&#22312;&#22810;&#25991;&#26723;&#31185;&#23398;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.17311</link><description>&lt;p&gt;
SKT5SciSumm - &#19968;&#31181;&#29992;&#20110;&#22810;&#25991;&#26723;&#31185;&#23398;&#25688;&#35201;&#30340;&#28151;&#21512;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SKT5SciSumm - A Hybrid Generative Approach for Multi-Document Scientific Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17311
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SKT5SciSumm&#30340;&#28151;&#21512;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#24341;&#25991;&#20449;&#24687;&#30340;&#21464;&#25442;&#22120;&#21644;T5&#31995;&#21015;&#27169;&#22411;&#65292;&#22312;&#22810;&#25991;&#26723;&#31185;&#23398;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17311v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25688;&#35201;&#65306;&#31185;&#23398;&#25991;&#26412;&#25688;&#35201;&#23545;&#20110;&#30740;&#31350;&#30028;&#21644;&#20154;&#31867;&#31038;&#20250;&#37117;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#30410;&#22788;&#12290;&#32771;&#34385;&#21040;&#31185;&#23398;&#25991;&#26412;&#30340;&#29305;&#27530;&#24615;&#20197;&#21450;&#22810;&#25991;&#26723;&#25688;&#35201;&#20219;&#21153;&#30340;&#36755;&#20837;&#23454;&#36136;&#19978;&#24456;&#38271;&#65292;&#35813;&#20219;&#21153;&#38656;&#35201;&#36275;&#22815;&#30340;&#23884;&#20837;&#29983;&#25104;&#21644;&#25991;&#26412;&#25130;&#26029;&#65292;&#21516;&#26102;&#21448;&#19981;&#33021;&#20002;&#22833;&#37325;&#35201;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SKT5SciSumm - &#19968;&#31181;&#29992;&#20110;&#22810;&#25991;&#26723;&#31185;&#23398;&#25688;&#35201;&#30340;&#28151;&#21512;&#26694;&#26550;&#65288;MDSS&#65289;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#24341;&#25991;&#20449;&#24687;&#30340;&#21464;&#25442;&#22120;(SPECTER)&#30340;&#31185;&#23398;&#25991;&#29486;&#23884;&#20837;&#30340;&#21477;&#23376;-&#21464;&#25442;&#22120;&#29256;&#26412;&#26469;&#32534;&#30721;&#21644;&#34920;&#31034;&#25991;&#26412;&#21477;&#23376;&#65292;&#20174;&#32780;&#23454;&#29616;&#20351;&#29992;k-means&#32858;&#31867;&#36827;&#34892;&#39640;&#25928;&#25688;&#35201;&#25552;&#21462;&#12290;&#25105;&#20204;&#20351;&#29992;T5&#31995;&#21015;&#27169;&#22411;&#20351;&#29992;&#25552;&#21462;&#30340;&#21477;&#23376;&#29983;&#25104;&#25277;&#35937;&#25688;&#35201;&#12290;SKT5SciSumm&#22312;Multi-XScience&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17311v1 Announce Type: new  Abstract: Summarization for scientific text has shown significant benefits both for the research community and human society. Given the fact that the nature of scientific text is distinctive and the input of the multi-document summarization task is substantially long, the task requires sufficient embedding generation and text truncation without losing important information. To tackle these issues, in this paper, we propose SKT5SciSumm - a hybrid framework for multi-document scientific summarization (MDSS). We leverage the Sentence-Transformer version of Scientific Paper Embeddings using Citation-Informed Transformers (SPECTER) to encode and represent textual sentences, allowing for efficient extractive summarization using k-means clustering. We employ the T5 family of models to generate abstractive summaries using extracted sentences. SKT5SciSumm achieves state-of-the-art performance on the Multi-XScience dataset. Through extensive experiments and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#33021;&#22815;&#26356;&#22909;&#22320;&#32534;&#30721;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#39030;&#23618;&#21487;&#33021;&#36807;&#22810;&#20851;&#27880;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#29702;&#35299;&#20840;&#23616;&#20449;&#24687;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;</title><link>https://arxiv.org/abs/2402.17304</link><description>&lt;p&gt;
&#25506;&#31350;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20840;&#23616;&#21644;&#23616;&#37096;&#35821;&#20041;&#34920;&#31034;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Probing Multimodal Large Language Models for Global and Local Semantic Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17304
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#33021;&#22815;&#26356;&#22909;&#22320;&#32534;&#30721;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#39030;&#23618;&#21487;&#33021;&#36807;&#22810;&#20851;&#27880;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#29702;&#35299;&#20840;&#23616;&#20449;&#24687;&#30340;&#33021;&#21147;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#21151;&#21551;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#23558;&#20854;&#20248;&#31168;&#30340;&#34920;&#31034;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#27169;&#24577;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#21033;&#29992;&#22270;&#20687;&#25551;&#36848;&#23545;&#40784;&#25968;&#25454;&#38598;&#35757;&#32451;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#65292;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;MLLMs&#26159;&#21542;&#30495;&#27491;&#29702;&#35299;&#23436;&#25972;&#30340;&#22270;&#20687;&#20449;&#24687;&#65292;&#21363;&#20840;&#23616;&#20449;&#24687;&#65292;&#25110;&#32773;&#23427;&#20204;&#21482;&#33021;&#25429;&#25417;&#19968;&#20123;&#23616;&#37096;&#23545;&#35937;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#30340;&#20013;&#38388;&#23618;&#21487;&#20197;&#32534;&#30721;&#26356;&#22810;&#20840;&#23616;&#35821;&#20041;&#20449;&#24687;&#65292;&#20854;&#34920;&#31034;&#21521;&#37327;&#22312;&#35270;&#35273;-&#35821;&#35328;&#34164;&#28085;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#19981;&#26159;&#39030;&#23618;&#12290;&#25105;&#20204;&#36890;&#36807;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#36827;&#19968;&#27493;&#25506;&#31350;&#27169;&#22411;&#30340;&#23616;&#37096;&#35821;&#20041;&#34920;&#31034;&#12290;&#25105;&#20204;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#39030;&#23618;&#21487;&#33021;&#36807;&#22810;&#19987;&#27880;&#20110;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#20943;&#24369;&#20102;&#23545;&#20840;&#23616;&#20449;&#24687;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17304v1 Announce Type: cross  Abstract: The success of large language models has inspired researchers to transfer their exceptional representing ability to other modalities. Several recent works leverage image-caption alignment datasets to train multimodal large language models (MLLMs), which achieve state-of-the-art performance on image-to-text tasks. However, there are very few studies exploring whether MLLMs truly understand the complete image information, i.e., global information, or if they can only capture some local object information. In this study, we find that the intermediate layers of models can encode more global semantic information, whose representation vectors perform better on visual-language entailment tasks, rather than the topmost layers. We further probe models for local semantic representation through object detection tasks. And we draw a conclusion that the topmost layers may excessively focus on local information, leading to a diminished ability to en
&lt;/p&gt;</description></item><item><title>LLM&#20351;&#29992;&#22312;&#21360;&#23612;&#35821;&#26041;&#38754;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#36275;&#22815;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#24061;&#20182;&#35821;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#31361;&#26174;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.17302</link><description>&lt;p&gt;
LLM&#33021;&#21542;&#29983;&#25104;&#19982;&#25991;&#21270;&#30456;&#20851;&#30340;&#24120;&#35782;&#24615;&#38382;&#31572;&#25968;&#25454;&#65311;&#21360;&#23612;&#21644;&#24061;&#20182;&#35821;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in Indonesian and Sundanese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17302
&lt;/p&gt;
&lt;p&gt;
LLM&#20351;&#29992;&#22312;&#21360;&#23612;&#35821;&#26041;&#38754;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#36275;&#22815;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#24061;&#20182;&#35821;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#31361;&#26174;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#20197;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#29983;&#25104;&#19968;&#20010;&#34701;&#20837;&#35821;&#35328;&#20013;&#30693;&#35782;&#21644;&#25991;&#21270;&#32454;&#24494;&#24046;&#21035;&#30340;&#39640;&#36136;&#37327;&#38382;&#31572;(QA)&#25968;&#25454;&#38598;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20351;&#29992;LLMs&#29983;&#25104;&#21360;&#23612;&#35821;&#21644;&#24061;&#20182;&#35821;&#25991;&#21270;&#30456;&#20851;&#24120;&#35782;&#24615;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#21253;&#25324;LLMs&#21644;&#20154;&#31867;&#26631;&#27880;&#32773;&#22312;&#20869;&#30340;&#21508;&#31181;&#26041;&#27861;&#20026;&#36825;&#20123;&#35821;&#35328;&#21019;&#24314;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30446;&#21069;&#24615;&#33021;&#26368;&#20339;&#30340;LLM&#65292;GPT-4 Turbo&#65292;&#33021;&#22815;&#29983;&#25104;&#21360;&#23612;&#35821;&#20013;&#20855;&#26377;&#36275;&#22815;&#30693;&#35782;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#24061;&#20182;&#35821;&#20013;&#21364;&#19981;&#34892;&#65292;&#31361;&#20986;&#20102;&#20013;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#36824;&#22312;&#25105;&#20204;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17302v1 Announce Type: new  Abstract: Large Language Models (LLMs) are increasingly being used to generate synthetic data for training and evaluating models. However, it is unclear whether they can generate a good quality of question answering (QA) dataset that incorporates knowledge and cultural nuance embedded in a language, especially for low-resource languages. In this study, we investigate the effectiveness of using LLMs in generating culturally relevant commonsense QA datasets for Indonesian and Sundanese languages. To do so, we create datasets for these languages using various methods involving both LLMs and human annotators. Our experiments show that the current best-performing LLM, GPT-4 Turbo, is capable of generating questions with adequate knowledge in Indonesian but not in Sundanese, highlighting the performance discrepancy between medium- and lower-resource languages. We also benchmark various LLMs on our generated datasets and find that they perform better on 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MELoRA&#65292;&#19968;&#31181;&#36855;&#20320;&#38598;&#25104;&#20302;&#31209;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#21516;&#26102;&#20445;&#25345;&#26356;&#39640;&#30340;&#31209;&#65292;&#20174;&#32780;&#25552;&#20379;&#25913;&#36827;&#30340;&#24615;&#33021;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17263</link><description>&lt;p&gt;
&#36855;&#20320;&#38598;&#25104;&#20302;&#31209;&#36866;&#37197;&#22120;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17263
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MELoRA&#65292;&#19968;&#31181;&#36855;&#20320;&#38598;&#25104;&#20302;&#31209;&#36866;&#37197;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#21516;&#26102;&#20445;&#25345;&#26356;&#39640;&#30340;&#31209;&#65292;&#20174;&#32780;&#25552;&#20379;&#25913;&#36827;&#30340;&#24615;&#33021;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#23450;&#21046;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#27169;&#22411;&#35268;&#27169;&#21644;&#20219;&#21153;&#22810;&#26679;&#24615;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#12290;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#24605;&#24819;&#65292;&#21363;&#36866;&#24212;&#36807;&#31243;&#22312;&#26412;&#36136;&#19978;&#26159;&#20302;&#32500;&#30340;&#65292;&#21363;&#21487;&#20197;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#21442;&#25968;&#34920;&#31034;&#37325;&#35201;&#30340;&#27169;&#22411;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#19982;&#20840;&#21442;&#25968;&#24494;&#35843;&#30456;&#27604;&#65292;&#38477;&#20302;&#31209;&#20250;&#36935;&#21040;&#29305;&#23450;&#20219;&#21153;&#30340;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MELoRA&#65292;&#19968;&#31181;&#36855;&#20320;&#38598;&#25104;&#20302;&#31209;&#36866;&#37197;&#22120;&#65292;&#20351;&#29992;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#21516;&#26102;&#20445;&#25345;&#26356;&#39640;&#30340;&#31209;&#65292;&#20174;&#32780;&#25552;&#20379;&#25913;&#36827;&#30340;&#24615;&#33021;&#28508;&#21147;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#20923;&#32467;&#21407;&#22987;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#65292;&#24182;&#35757;&#32451;&#19968;&#32452;&#20165;&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#30340;&#36855;&#20320;LoRA&#12290;&#36825;&#21487;&#20197;&#25429;&#25417;&#36855;&#20320;LoRA&#20043;&#38388;&#30340;&#37325;&#35201;&#22810;&#26679;&#24615;&#31243;&#24230;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17263v1 Announce Type: new  Abstract: Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring pre-trained large language models (LLMs), especially as the models' scale and the diversity of tasks increase. Low-rank adaptation (LoRA) is based on the idea that the adaptation process is intrinsically low-dimensional, i.e., significant model changes can be represented with relatively few parameters. However, decreasing the rank encounters challenges with generalization errors for specific tasks when compared to full-parameter fine-tuning. We present MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters while maintaining a higher rank, thereby offering improved performance potential. The core idea is to freeze original pretrained weights and train a group of mini LoRAs with only a small number of parameters. This can capture a significant degree of diversity among mini LoRAs, thus promoting better generalization ability. We conduct a theor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#36718;&#23545;&#35805;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#28431;&#27934;&#65292;&#25351;&#20986;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#22810;&#36718;&#23545;&#35805;&#35825;&#20351;&#20854;&#29983;&#25104;&#26377;&#23475;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.17262</link><description>&lt;p&gt;
&#22833;&#35328;&#65306;&#22810;&#36718;&#23545;&#35805;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Speak Out of Turn: Safety Vulnerability of Large Language Models in Multi-turn Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22810;&#36718;&#23545;&#35805;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#28431;&#27934;&#65292;&#25351;&#20986;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#22810;&#36718;&#23545;&#35805;&#35825;&#20351;&#20854;&#29983;&#25104;&#26377;&#23475;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#34987;&#35777;&#26126;&#22312;&#38754;&#20020;"&#36234;&#29425;"&#26102;&#20250;&#20135;&#29983;&#38750;&#27861;&#25110;&#19981;&#36947;&#24503;&#30340;&#22238;&#24212;&#12290; "&#36234;&#29425;"&#30740;&#31350;&#24378;&#35843;&#20102;LLMs&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#36718;&#23545;&#35805;&#19978;&#65292;&#24573;&#35270;&#20102;&#22810;&#36718;&#23545;&#35805;&#21487;&#33021;&#24102;&#26469;&#30340;&#22797;&#26434;&#24615;&#21644;&#39118;&#38505;&#65292;&#36825;&#26159;&#20154;&#31867;&#20174;LLMs&#33719;&#21462;&#20449;&#24687;&#30340;&#20851;&#38190;&#26041;&#24335;&#12290;&#26412;&#25991;&#35748;&#20026;&#20154;&#31867;&#21487;&#20197;&#21033;&#29992;&#22810;&#36718;&#23545;&#35805;&#35825;&#20351;LLMs&#29983;&#25104;&#26377;&#23475;&#20449;&#24687;&#12290;LLMs&#21487;&#33021;&#19981;&#20250;&#25298;&#32477;&#35686;&#21578;&#24615;&#25110;&#36793;&#30028;&#19981;&#23433;&#20840;&#30340;&#26597;&#35810;&#65292;&#21363;&#20351;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#27599;&#20010;&#22238;&#21512;&#37117;&#34987;&#26381;&#21153;&#20110;&#19968;&#20010;&#24694;&#24847;&#30446;&#30340;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#23558;&#19968;&#20010;&#19981;&#23433;&#20840;&#26597;&#35810;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#26597;&#35810;&#29992;&#20110;&#22810;&#36718;&#23545;&#35805;&#65292;&#25105;&#20204;&#36880;&#28176;&#35825;&#20351;LLMs&#22238;&#31572;&#26377;&#23475;&#30340;&#23376;&#38382;&#39064;&#65292;&#26368;&#32456;&#23548;&#33268;&#24635;&#20307;&#26377;&#23475;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36328;&#36234;&#20102;&#24191;&#27867;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17262v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have been demonstrated to generate illegal or unethical responses, particularly when subjected to "jailbreak." Research on jailbreak has highlighted the safety issues of LLMs. However, prior studies have predominantly focused on single-turn dialogue, ignoring the potential complexities and risks presented by multi-turn dialogue, a crucial mode through which humans derive information from LLMs. In this paper, we argue that humans could exploit multi-turn dialogue to induce LLMs into generating harmful information. LLMs may not intend to reject cautionary or borderline unsafe queries, even if each turn is closely served for one malicious purpose in a multi-turn dialogue. Therefore, by decomposing an unsafe query into several sub-queries for multi-turn dialogue, we induced LLMs to answer harmful sub-questions incrementally, culminating in an overall harmful response. Our experiments, conducted across a wide ra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;LLMs&#22312;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20854;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#27425;&#21644;&#23569;&#27425;&#33021;&#21147;&#65292;&#20294;&#19982;&#23436;&#20840;&#36164;&#28304;&#24494;&#35843;&#30340;&#27169;&#22411;&#30456;&#27604;&#20173;&#22788;&#20110;&#21155;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.17256</link><description>&lt;p&gt;
&#36229;&#36234;&#24050;&#30693;&#65306;&#30740;&#31350;LLMs&#22312;&#39046;&#22495;&#22806;&#24847;&#22270;&#26816;&#27979;&#19978;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#21512;&#35780;&#20272;&#20102;LLMs&#22312;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#19979;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20854;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#27425;&#21644;&#23569;&#27425;&#33021;&#21147;&#65292;&#20294;&#19982;&#23436;&#20840;&#36164;&#28304;&#24494;&#35843;&#30340;&#27169;&#22411;&#30456;&#27604;&#20173;&#22788;&#20110;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#24847;&#22270;&#26816;&#27979;&#26088;&#22312;&#26816;&#26597;&#29992;&#25143;&#30340;&#26597;&#35810;&#26159;&#21542;&#36229;&#20986;&#31995;&#32479;&#39044;&#23450;&#20041;&#30340;&#39046;&#22495;&#65292;&#36825;&#23545;&#20110;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#31995;&#32479;&#30340;&#27491;&#24120;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#24494;&#35843;&#21306;&#20998;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;OOD&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#22312;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#19979;&#23545;LLM&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#24182;&#27010;&#36848;&#20102;LLM&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#38646;&#27425;&#21644;&#23569;&#27425;&#33021;&#21147;&#65292;&#20294;&#19982;&#23436;&#20840;&#36164;&#28304;&#24494;&#35843;&#30340;&#27169;&#22411;&#30456;&#27604;&#20173;&#22788;&#20110;&#21155;&#21183;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#38468;&#21152;&#20998;&#26512;&#23454;&#39564;&#65292;&#26412;&#25991;&#26356;&#28145;&#20837;&#22320;&#35752;&#35770;&#21644;&#24635;&#32467;&#20102;LLMs&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#20026;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17256v1 Announce Type: new  Abstract: Out-of-domain (OOD) intent detection aims to examine whether the user's query falls outside the predefined domain of the system, which is crucial for the proper functioning of task-oriented dialogue (TOD) systems. Previous methods address it by fine-tuning discriminative models. Recently, some studies have been exploring the application of large language models (LLMs) represented by ChatGPT to various downstream tasks, but it is still unclear for their ability on OOD detection task.This paper conducts a comprehensive evaluation of LLMs under various experimental settings, and then outline the strengths and weaknesses of LLMs. We find that LLMs exhibit strong zero-shot and few-shot capabilities, but is still at a disadvantage compared to models fine-tuned with full resource. More deeply, through a series of additional analysis experiments, we discuss and summarize the challenges faced by LLMs and provide guidance for future work including
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#35270;&#22270;&#27880;&#24847;&#21147;&#30340;&#21452;&#27969;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21333;&#19968;&#34920;&#31034;&#38590;&#20197;&#20840;&#38754;&#35206;&#30422;&#22797;&#26434;&#20869;&#23481;&#21644;&#32570;&#20047;&#20132;&#20114;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.17237</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#35270;&#22270;&#27880;&#24847;&#21147;&#30340;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Image-Text Matching with Multi-View Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#35270;&#22270;&#27880;&#24847;&#21147;&#30340;&#21452;&#27969;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#21333;&#19968;&#34920;&#31034;&#38590;&#20197;&#20840;&#38754;&#35206;&#30422;&#22797;&#26434;&#20869;&#23481;&#21644;&#32570;&#20047;&#20132;&#20114;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#29992;&#20110;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#30340;&#21452;&#27969;&#27169;&#22411;&#22312;&#30830;&#20445;&#26816;&#32034;&#36895;&#24230;&#30340;&#21516;&#26102;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#21463;&#21040;&#24037;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#21333;&#19968;&#34920;&#31034;&#26469;&#20998;&#21035;&#32534;&#30721;&#22270;&#20687;&#21644;&#25991;&#26412;&#65292;&#24182;&#20351;&#29992;&#20313;&#24358;&#30456;&#20284;&#24230;&#25110;&#21521;&#37327;&#20869;&#31215;&#24471;&#21040;&#21305;&#37197;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#21452;&#27969;&#27169;&#22411;&#30340;&#24615;&#33021;&#24448;&#24448;&#19981;&#22826;&#29702;&#24819;&#12290;&#19968;&#26041;&#38754;&#65292;&#21333;&#19968;&#34920;&#31034;&#38590;&#20197;&#20840;&#38754;&#35206;&#30422;&#22797;&#26434;&#20869;&#23481;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312;&#36825;&#31181;&#32570;&#20047;&#20132;&#20114;&#30340;&#26694;&#26550;&#20013;&#65292;&#21305;&#37197;&#22810;&#37325;&#21547;&#20041;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36825;&#23548;&#33268;&#20449;&#24687;&#34987;&#24573;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#24182;&#20419;&#36827;&#21452;&#27969;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27969;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#30340;&#22810;&#35270;&#22270;&#27880;&#24847;&#21147;&#26041;&#27861;MVAM&#65288;&#22810;&#35270;&#22270;&#27880;&#24847;&#21147;&#27169;&#22411;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17237v1 Announce Type: cross  Abstract: Existing two-stream models for image-text matching show good performance while ensuring retrieval speed and have received extensive attention from industry and academia. These methods use a single representation to encode image and text separately and get a matching score with cosine similarity or the inner product of vectors. However, the performance of the two-stream model is often sub-optimal. On the one hand, a single representation is challenging to cover complex content comprehensively. On the other hand, in this framework of lack of interaction, it is challenging to match multiple meanings which leads to information being ignored. To address the problems mentioned above and facilitate the performance of the two-stream model, we propose a multi-view attention approach for two-stream image-text matching MVAM (\textbf{M}ulti-\textbf{V}iew \textbf{A}ttention \textbf{M}odel). It first learns multiple image and text representations by
&lt;/p&gt;</description></item><item><title>MATHSENSEI &#26159;&#19968;&#31181;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24037;&#20855;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#28155;&#21152;&#30693;&#35782;&#26816;&#32034;&#12289;&#31243;&#24207;&#25191;&#34892;&#21644;&#31526;&#21495;&#26041;&#31243;&#27714;&#35299;&#24037;&#20855;&#65292;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17231</link><description>&lt;p&gt;
MATHSENSEI&#65306;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24037;&#20855;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17231
&lt;/p&gt;
&lt;p&gt;
MATHSENSEI &#26159;&#19968;&#31181;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#30340;&#24037;&#20855;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#28155;&#21152;&#30693;&#35782;&#26816;&#32034;&#12289;&#31243;&#24207;&#25191;&#34892;&#21644;&#31526;&#21495;&#26041;&#31243;&#27714;&#35299;&#24037;&#20855;&#65292;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20855;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(TALM)&#24050;&#30693;&#33021;&#22815;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#25216;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#23427;&#20204;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MATHSENSEI&#30340;&#24037;&#20855;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25968;&#23398;&#25512;&#29702;&#12290;&#36890;&#36807;&#28155;&#21152;&#29992;&#20110;&#30693;&#35782;&#26816;&#32034;&#65288;Bing Web Search&#65289;&#12289;&#31243;&#24207;&#25191;&#34892;&#65288;Python&#65289;&#21644;&#31526;&#21495;&#26041;&#31243;&#27714;&#35299;&#65288;Wolfram-Alpha&#65289;&#30340;&#24037;&#20855;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#26469;&#30740;&#31350;&#36825;&#20123;&#24037;&#20855;&#30340;&#20114;&#34917;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17231v1 Announce Type: new  Abstract: Tool-augmented Large Language Models (TALM) are known to enhance the skillset of large language models (LLM), thereby, leading to their improved reasoning abilities across many tasks. While, TALMs have been successfully employed in different question-answering benchmarks, their efficacy on complex mathematical reasoning benchmarks, and the potential complimentary benefits offered by tools for knowledge retrieval and mathematical equation solving, are open research questions. In this work, we present MATHSENSEI, a tool-augmented large language model for mathematical reasoning. Augmented with tools for knowledge retrieval (Bing Web Search), program execution (Python), and symbolic equation solving (Wolfram-Alpha), we study the complimentary benefits of these tools through evaluations on mathematical reasoning datasets. We perform exhaustive ablations on MATH,a popular dataset for evaluating mathematical reasoning on diverse mathematical di
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;RiC&#65288;&#23545;&#35805;&#25512;&#29702;&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#35805;&#27169;&#25311;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20027;&#35266;&#20219;&#21153;&#65292;&#36890;&#36807;&#25366;&#25496;&#26377;&#29992;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22635;&#34917;&#23458;&#35266;&#25512;&#29702;&#26041;&#24335;&#30340;&#19981;&#36275;</title><link>https://arxiv.org/abs/2402.17226</link><description>&lt;p&gt;
&#23545;&#35805;&#25512;&#29702;&#65306;&#36890;&#36807;&#23545;&#35805;&#27169;&#25311;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20027;&#35266;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17226
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;RiC&#65288;&#23545;&#35805;&#25512;&#29702;&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#35805;&#27169;&#25311;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20027;&#35266;&#20219;&#21153;&#65292;&#36890;&#36807;&#25366;&#25496;&#26377;&#29992;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#22635;&#34917;&#23458;&#35266;&#25512;&#29702;&#26041;&#24335;&#30340;&#19981;&#36275;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#23458;&#35266;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#65292;&#27604;&#22914;&#24320;&#25918;&#22495;&#38382;&#31572;&#21644;&#25968;&#23398;&#25512;&#29702;&#65292;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#21487;&#20197;&#36890;&#36807;&#22238;&#24518;&#23398;&#21040;&#30340;&#20107;&#23454;&#30693;&#35782;&#25110;&#24605;&#32500;&#38142;&#36335;&#24335;&#25512;&#29702;&#26469;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#22312;&#20027;&#35266;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20173;&#28982;&#20196;&#20154;&#19981;&#28385;&#65292;&#27604;&#22914;&#38544;&#21947;&#35782;&#21035;&#12289;&#40657;&#26263;&#24189;&#40664;&#26816;&#27979;&#31561;&#12290;&#19982;&#23458;&#35266;&#20219;&#21153;&#30456;&#27604;&#65292;&#20027;&#35266;&#20219;&#21153;&#26356;&#27880;&#37325;&#35299;&#37322;&#25110;&#24773;&#24863;&#21453;&#24212;&#65292;&#32780;&#19981;&#26159;&#19968;&#20010;&#26222;&#36941;&#25509;&#21463;&#30340;&#25512;&#29702;&#36335;&#24452;&#12290;&#22522;&#20110;&#20219;&#21153;&#29305;&#24615;&#21644;LLMs&#24378;&#22823;&#30340;&#23545;&#35805;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RiC&#65288;&#23545;&#35805;&#25512;&#29702;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#23545;&#35805;&#27169;&#25311;&#35299;&#20915;&#20027;&#35266;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;RiC&#30340;&#21160;&#26426;&#26159;&#36890;&#36807;&#27169;&#25311;&#23545;&#35805;&#26469;&#25366;&#25496;&#26377;&#29992;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#25552;&#20379;&#24605;&#32500;&#38142;&#36335;&#24335;&#30340;&#29702;&#30001;&#65292;&#20174;&#32780;&#25552;&#20379;&#28508;&#22312;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17226v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved remarkable performance in objective tasks such as open-domain question answering and mathematical reasoning, which can often be solved through recalling learned factual knowledge or chain-of-thought style reasoning. However, we find that the performance of LLMs in subjective tasks is still unsatisfactory, such as metaphor recognition, dark humor detection, etc. Compared to objective tasks, subjective tasks focus more on interpretation or emotional response rather than a universally accepted reasoning pathway. Based on the characteristics of the tasks and the strong dialogue-generation capabilities of LLMs, we propose RiC (Reasoning in Conversation), a method that focuses on solving subjective tasks through dialogue simulation. The motivation of RiC is to mine useful contextual information by simulating dialogues instead of supplying chain-of-thought style rationales, thereby offering potential u
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#22522;&#30784;&#25216;&#33021;&#21644;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#27169;&#22411;&#23545;&#20110;&#20302;&#24180;&#32423;&#25216;&#33021;&#30340;&#26377;&#38480;&#25484;&#25569;&#12290;</title><link>https://arxiv.org/abs/2402.17205</link><description>&lt;p&gt;
&#27979;&#37327;&#31070;&#32463;&#27169;&#22411;&#30340;&#35270;&#35273;&#35821;&#35328;STEM&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Measuring Vision-Language STEM Skills of Neural Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17205
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#22522;&#30784;&#25216;&#33021;&#21644;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#26032;&#27169;&#22411;&#23545;&#20110;&#20302;&#24180;&#32423;&#25216;&#33021;&#30340;&#26377;&#38480;&#25484;&#25569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#25361;&#25112;&#65292;&#29992;&#20110;&#27979;&#35797;&#31070;&#32463;&#27169;&#22411;&#30340;STEM&#25216;&#33021;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#36890;&#24120;&#38656;&#35201;&#32467;&#21512;STEM&#65288;&#31185;&#23398;&#12289;&#25216;&#26415;&#12289;&#24037;&#31243;&#21644;&#25968;&#23398;&#65289;&#30693;&#35782;&#26469;&#35299;&#20915;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#38656;&#35201;&#29702;&#35299;STEM&#30340;&#22810;&#27169;&#24335;&#35270;&#35273;&#35821;&#35328;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#25361;&#25112;&#24615;&#38382;&#39064;&#20013;&#26368;&#22823;&#12289;&#26368;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;&#23427;&#21253;&#25324;448&#39033;&#25216;&#33021;&#21644;1,073,146&#20010;&#36328;&#36234;&#25152;&#26377;STEM&#31185;&#30446;&#30340;&#38382;&#39064;&#12290;&#19982;&#36890;&#24120;&#20391;&#37325;&#20110;&#26816;&#39564;&#19987;&#23478;&#27700;&#24179;&#33021;&#21147;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#22522;&#30784;&#25216;&#33021;&#21644;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23558;&#26368;&#20808;&#36827;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;CLIP&#21644;GPT-3.5-Turbo&#65292;&#28155;&#21152;&#21040;&#25105;&#20204;&#30340;&#22522;&#20934;&#20013;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#36817;&#30340;&#27169;&#22411;&#36827;&#23637;&#21482;&#26377;&#21161;&#20110;&#25484;&#25569;&#25968;&#25454;&#38598;&#20013;&#38750;&#24120;&#26377;&#38480;&#25968;&#37327;&#30340;&#20302;&#24180;&#32423;&#25216;&#33021;&#65288;&#19977;&#24180;&#32423;&#20013;&#30340;2.5%&#65289;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#36828;&#27809;&#26377;&#23436;&#20840;&#25484;&#25569;&#23398;&#21069;&#25945;&#32946;&#38454;&#27573;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17205v1 Announce Type: cross  Abstract: We introduce a new challenge to test the STEM skills of neural models. The problems in the real world often require solutions, combining knowledge from STEM (science, technology, engineering, and math). Unlike existing datasets, our dataset requires the understanding of multimodal vision-language information of STEM. Our dataset features one of the largest and most comprehensive datasets for the challenge. It includes 448 skills and 1,073,146 questions spanning all STEM subjects. Compared to existing datasets that often focus on examining expert-level ability, our dataset includes fundamental skills and questions designed based on the K-12 curriculum. We also add state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our benchmark. Results show that the recent model advances only help master a very limited number of lower grade-level skills (2.5% in the third grade) in our dataset. In fact, these models are still well bel
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#19981;&#21516;&#25193;&#23637;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#24615;&#33021;&#65292;&#35748;&#20026;LLM&#24494;&#35843;&#36981;&#24490;&#30528;&#19968;&#31181;&#29305;&#27530;&#30340;&#25193;&#23637;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2402.17193</link><description>&lt;p&gt;
&#24403;&#25193;&#23637;&#36935;&#21040;LLM&#24494;&#35843;: &#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#24494;&#35843;&#26041;&#27861;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17193
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#19981;&#21516;&#25193;&#23637;&#22240;&#32032;&#22914;&#20309;&#24433;&#21709;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#24615;&#33021;&#65292;&#35748;&#20026;LLM&#24494;&#35843;&#36981;&#24490;&#30528;&#19968;&#31181;&#29305;&#27530;&#30340;&#25193;&#23637;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#37319;&#29992;&#24494;&#35843;&#26469;&#37322;&#25918;&#20854;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#33021;&#21147;&#65292;&#20294;&#25105;&#20204;&#23545;&#19981;&#21516;&#24494;&#35843;&#26041;&#27861;&#30340;&#24402;&#32435;&#20559;&#24046;&#65288;&#29305;&#21035;&#26159;&#25193;&#23637;&#23646;&#24615;&#65289;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#23454;&#39564;&#65292;&#30740;&#31350;&#19981;&#21516;&#25193;&#23637;&#22240;&#32032;&#65288;&#21253;&#25324;LLM&#27169;&#22411;&#22823;&#23567;&#12289;&#39044;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#12289;&#26032;&#24494;&#35843;&#21442;&#25968;&#22823;&#23567;&#21644;&#24494;&#35843;&#25968;&#25454;&#22823;&#23567;&#65289;&#22914;&#20309;&#24433;&#21709;&#24494;&#35843;&#24615;&#33021;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#24494;&#35843;&#31867;&#22411;--&#20840;&#27169;&#22411;&#35843;&#25972;&#65288;FMT&#65289;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PET&#65292;&#21253;&#25324;&#25552;&#31034;&#35843;&#25972;&#21644;LoRA&#65289;&#65292;&#24182;&#25506;&#32034;&#23427;&#20204;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#25193;&#23637;&#34892;&#20026;&#65292;&#20854;&#20013;LLM&#27169;&#22411;&#22823;&#23567;&#36828;&#36828;&#36229;&#36807;&#24494;&#35843;&#25968;&#25454;&#22823;&#23567;&#12290;&#22522;&#20110;1B&#21040;16B&#30340;&#20004;&#32452;&#39044;&#35757;&#32451;&#21452;&#35821;LLMs&#21644;&#23545;&#21452;&#35821;&#26426;&#22120;&#32763;&#35793;&#21644;&#22810;&#35821;&#35328;&#25688;&#35201;&#22522;&#20934;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#24494;&#35843;&#36981;&#24490;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17193v1 Announce Type: new  Abstract: While large language models (LLMs) often adopt finetuning to unlock their capabilities for downstream applications, our understanding on the inductive biases (especially the scaling properties) of different finetuning methods is still limited. To fill this gap, we conduct systematic experiments studying whether and how different scaling factors, including LLM model size, pretraining data size, new finetuning parameter size and finetuning data size, affect the finetuning performance. We consider two types of finetuning -- full-model tuning (FMT) and parameter efficient tuning (PET, including prompt tuning and LoRA), and explore their scaling behaviors in the data-limited regime where the LLM model size substantially outweighs the finetuning data size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and experiments on bilingual machine translation and multilingual summarization benchmarks, we find that 1) LLM finetuning follo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#35299;&#32544;&#25439;&#22833;&#24182;&#25913;&#36827;&#22768;&#23398;&#32534;&#30721;&#22120;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#20195;&#30721;&#20999;&#25442;&#29616;&#35937;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#26174;&#33879;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.17189</link><description>&lt;p&gt;
&#19968;&#31181;&#26377;&#25928;&#30340;&#28151;&#21512;&#19987;&#23478;&#26041;&#27861;&#65292;&#21033;&#29992;&#32534;&#30721;&#22120;&#35299;&#32544;&#26469;&#36827;&#34892;&#20195;&#30721;&#20999;&#25442;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
An Effective Mixture-Of-Experts Approach For Code-Switching Speech Recognition Leveraging Encoder Disentanglement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17189
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#35299;&#32544;&#25439;&#22833;&#24182;&#25913;&#36827;&#22768;&#23398;&#32534;&#30721;&#22120;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#20195;&#30721;&#20999;&#25442;&#29616;&#35937;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#26174;&#33879;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#31070;&#32463;&#32593;&#32476;&#30340;&#22823;&#35268;&#27169;&#21457;&#23637;&#65292;&#36817;&#24180;&#26469;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#20195;&#30721;&#20999;&#25442;&#29616;&#35937;&#20173;&#28982;&#26159;&#38459;&#30861;ASR&#23436;&#32654;&#30340;&#20027;&#35201;&#38556;&#30861;&#65292;&#22240;&#20026;&#32570;&#20047;&#26631;&#35760;&#25968;&#25454;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#21464;&#21270;&#32463;&#24120;&#23548;&#33268;ASR&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#25913;&#36827;E2E ASR&#30340;&#22768;&#23398;&#32534;&#30721;&#22120;&#65292;&#20197;&#24212;&#23545;&#20195;&#30721;&#20999;&#25442;&#29616;&#35937;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26377;&#19977;&#20010;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#32544;&#25439;&#22833;&#65292;&#20351;&#24471;&#32534;&#30721;&#22120;&#30340;&#36739;&#20302;&#23618;&#33021;&#22815;&#25429;&#33719;&#36328;&#35821;&#35328;&#30340;&#22768;&#23398;&#20449;&#24687;&#65292;&#21516;&#26102;&#22312;&#32534;&#30721;&#22120;&#30340;&#36739;&#39640;&#23618;&#20943;&#36731;&#35821;&#35328;&#28151;&#28102;&#12290;&#20854;&#27425;&#65292;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20351;&#29992;&#39044;&#35757;&#32451;&#21452;&#32534;&#30721;&#22120;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#21516;&#26102;&#21482;&#35775;&#38382;&#20195;&#30721;&#20999;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17189v1 Announce Type: cross  Abstract: With the massive developments of end-to-end (E2E) neural networks, recent years have witnessed unprecedented breakthroughs in automatic speech recognition (ASR). However, the codeswitching phenomenon remains a major obstacle that hinders ASR from perfection, as the lack of labeled data and the variations between languages often lead to degradation of ASR performance. In this paper, we focus exclusively on improving the acoustic encoder of E2E ASR to tackle the challenge caused by the codeswitching phenomenon. Our main contributions are threefold: First, we introduce a novel disentanglement loss to enable the lower-layer of the encoder to capture inter-lingual acoustic information while mitigating linguistic confusion at the higher-layer of the encoder. Second, through comprehensive experiments, we verify that our proposed method outperforms the prior-art methods using pretrained dual-encoders, meanwhile having access only to the codesw
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;&#20013;&#24212;&#29992;&#22810;&#20010;&#28431;&#26007;&#38477;&#20302;&#23618;&#65292;&#23454;&#29616;&#20102;&#26497;&#22823;&#31243;&#24230;&#30340;&#36755;&#20986;&#24103;&#29575;&#38477;&#20302;&#65292;&#20026;&#22823;&#22411;&#31471;&#21040;&#31471;&#27169;&#22411;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;</title><link>https://arxiv.org/abs/2402.17184</link><description>&lt;p&gt;
&#26497;&#31471;&#32534;&#30721;&#22120;&#36755;&#20986;&#24103;&#29575;&#38477;&#20302;&#65306;&#25552;&#39640;&#22823;&#22411;&#31471;&#21040;&#31471;&#27169;&#22411;&#30340;&#35745;&#31639;&#24310;&#36831;
&lt;/p&gt;
&lt;p&gt;
Extreme Encoder Output Frame Rate Reduction: Improving Computational Latencies of Large End-to-End Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17184
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;&#20013;&#24212;&#29992;&#22810;&#20010;&#28431;&#26007;&#38477;&#20302;&#23618;&#65292;&#23454;&#29616;&#20102;&#26497;&#22823;&#31243;&#24230;&#30340;&#36755;&#20986;&#24103;&#29575;&#38477;&#20302;&#65292;&#20026;&#22823;&#22411;&#31471;&#21040;&#31471;&#27169;&#22411;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31471;&#21040;&#31471;&#65288;E2E&#65289;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38543;&#30528;&#35268;&#27169;&#25193;&#22823;&#32780;&#25345;&#32493;&#25552;&#39640;&#65292;&#19968;&#20123;&#27169;&#22411;&#29616;&#22312;&#24050;&#32463;&#36798;&#21040;&#20102;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24191;&#27867;&#37096;&#32626;&#21644;&#37319;&#29992;&#38656;&#35201;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#36825;&#26679;&#30340;&#31574;&#30053;&#65306;&#22312;&#32534;&#30721;&#22120;&#20013;&#24212;&#29992;&#22810;&#20010;&#24103;&#38477;&#20302;&#23618;&#65292;&#23558;&#32534;&#30721;&#22120;&#36755;&#20986;&#21387;&#32553;&#25104;&#23569;&#37327;&#30340;&#36755;&#20986;&#24103;&#12290;&#23613;&#31649;&#31867;&#20284;&#30340;&#25216;&#26415;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#24050;&#32463;&#24471;&#21040;&#30740;&#31350;&#65292;&#20294;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22810;&#20010;&#28431;&#26007;&#38477;&#20302;&#23618;&#23454;&#29616;&#20102;&#27604;&#20808;&#21069;&#23637;&#31034;&#30340;&#26356;&#22823;&#24133;&#24230;&#30340;&#20943;&#23569;&#12290;&#36890;&#36807;&#28040;&#34701;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32534;&#30721;&#22120;&#20013;&#21508;&#31181;&#26550;&#26500;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#20197;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#21487;&#20197;&#22312;&#27599;2.56&#31186;&#30340;&#36755;&#20837;&#35821;&#38899;&#20013;&#29983;&#25104;&#19968;&#20010;&#32534;&#30721;&#22120;&#36755;&#20986;&#24103;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#24433;&#21709;&#22823;&#22411;&#31471;&#21040;&#31471;&#27169;&#22411;&#19978;&#30340;&#35789;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17184v1 Announce Type: new  Abstract: The accuracy of end-to-end (E2E) automatic speech recognition (ASR) models continues to improve as they are scaled to larger sizes, with some now reaching billions of parameters. Widespread deployment and adoption of these models, however, requires computationally efficient strategies for decoding. In the present work, we study one such strategy: applying multiple frame reduction layers in the encoder to compress encoder outputs into a small number of output frames. While similar techniques have been investigated in previous work, we achieve dramatically more reduction than has previously been demonstrated through the use of multiple funnel reduction layers. Through ablations, we study the impact of various architectural choices in the encoder to identify the most effective strategies. We demonstrate that we can generate one encoder output frame for every 2.56 sec of input speech, without significantly affecting word error rate on a larg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;DSEval&#35780;&#20272;&#33539;&#24335;&#21644;&#19968;&#31995;&#21015;&#21019;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#31185;&#23398;&#20195;&#29702;&#22312;&#25972;&#20010;&#25968;&#25454;&#31185;&#23398;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#20030;&#27880;&#37322;&#26041;&#27861;&#31616;&#21270;&#25968;&#25454;&#38598;&#20934;&#22791;&#27969;&#31243;&#65292;&#25913;&#36827;&#35780;&#20272;&#35206;&#30422;&#33539;&#22260;&#65292;&#25193;&#23637;&#22522;&#20934;&#27979;&#35797;&#30340;&#20840;&#38754;&#24615;&#65292;&#25581;&#31034;&#20102;&#26222;&#36941;&#23384;&#22312;&#30340;&#38556;&#30861;&#24182;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;</title><link>https://arxiv.org/abs/2402.17168</link><description>&lt;p&gt;
&#25968;&#25454;&#31185;&#23398;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Data Science Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;DSEval&#35780;&#20272;&#33539;&#24335;&#21644;&#19968;&#31995;&#21015;&#21019;&#26032;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#31185;&#23398;&#20195;&#29702;&#22312;&#25972;&#20010;&#25968;&#25454;&#31185;&#23398;&#29983;&#21629;&#21608;&#26399;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#20030;&#27880;&#37322;&#26041;&#27861;&#31616;&#21270;&#25968;&#25454;&#38598;&#20934;&#22791;&#27969;&#31243;&#65292;&#25913;&#36827;&#35780;&#20272;&#35206;&#30422;&#33539;&#22260;&#65292;&#25193;&#23637;&#22522;&#20934;&#27979;&#35797;&#30340;&#20840;&#38754;&#24615;&#65292;&#25581;&#31034;&#20102;&#26222;&#36941;&#23384;&#22312;&#30340;&#38556;&#30861;&#24182;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;&#30340;&#26102;&#20195;&#65292;&#25968;&#25454;&#20998;&#26512;&#30340;&#22797;&#26434;&#24615;&#38656;&#35201;&#25968;&#25454;&#31185;&#23398;&#30340;&#39640;&#32423;&#19987;&#19994;&#30693;&#35782;&#21644;&#24037;&#20855;&#65292;&#36825;&#23545;&#19987;&#23478;&#26469;&#35828;&#20063;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20316;&#20026;&#25968;&#25454;&#31185;&#23398;&#20195;&#29702;&#65292;&#24050;&#32463;&#25104;&#20026;&#21327;&#21161;&#20154;&#31867;&#36827;&#34892;&#25968;&#25454;&#20998;&#26512;&#21644;&#22788;&#29702;&#30340;&#26377;&#24076;&#26395;&#30340;&#36741;&#21161;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#20173;&#21463;&#38480;&#20110;&#29616;&#23454;&#24212;&#29992;&#30340;&#22810;&#26679;&#38656;&#27714;&#21644;&#22797;&#26434;&#30340;&#20998;&#26512;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;DSEval--&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#33539;&#24335;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#38024;&#23545;&#25972;&#20010;&#25968;&#25454;&#31185;&#23398;&#29983;&#21629;&#21608;&#26399;&#30340;&#20195;&#29702;&#24615;&#33021;&#35780;&#20272;&#30340;&#21019;&#26032;&#22522;&#20934;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#20030;&#27880;&#37322;&#26041;&#27861;&#65292;&#25105;&#20204;&#31616;&#21270;&#20102;&#25968;&#25454;&#38598;&#20934;&#22791;&#27969;&#31243;&#65292;&#25913;&#36827;&#20102;&#35780;&#20272;&#35206;&#30422;&#33539;&#22260;&#65292;&#24182;&#25193;&#23637;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#20840;&#38754;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#25581;&#31034;&#20102;&#26222;&#36941;&#23384;&#22312;&#30340;&#38556;&#30861;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#65292;&#20197;&#25351;&#23548;&#26410;&#26469;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17168v1 Announce Type: new  Abstract: In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval -- a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32858;&#31867;&#27969;&#31243;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#25551;&#36848;&#24433;&#21709;&#21147;&#27963;&#21160;&#65292;&#36890;&#36807;&#22810;&#31181;&#25216;&#26415;&#22686;&#24378;&#27969;&#31243;&#24615;&#33021;&#65292;&#24182;&#22312;&#25991;&#26723;&#32423;&#21035;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#39044;&#27979;&#25928;&#26524;</title><link>https://arxiv.org/abs/2402.17151</link><description>&lt;p&gt;
&#23545;&#25991;&#26723;&#37096;&#20998;&#36827;&#34892;&#32858;&#31867;&#65306;&#20174;&#25991;&#26723;&#20013;&#26816;&#27979;&#21644;&#25551;&#36848;&#24433;&#21709;&#21147;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
Clustering Document Parts: Detecting and Characterizing Influence Campaigns From Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17151
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32858;&#31867;&#27969;&#31243;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#25551;&#36848;&#24433;&#21709;&#21147;&#27963;&#21160;&#65292;&#36890;&#36807;&#22810;&#31181;&#25216;&#26415;&#22686;&#24378;&#27969;&#31243;&#24615;&#33021;&#65292;&#24182;&#22312;&#25991;&#26723;&#32423;&#21035;&#20998;&#31867;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#39044;&#27979;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32858;&#31867;&#27969;&#31243;&#65292;&#29992;&#20110;&#20174;&#25991;&#26723;&#20013;&#26816;&#27979;&#21644;&#25551;&#36848;&#24433;&#21709;&#21147;&#27963;&#21160;&#12290;&#35813;&#26041;&#27861;&#23545;&#25991;&#26723;&#30340;&#37096;&#20998;&#36827;&#34892;&#32858;&#31867;&#65292;&#26816;&#27979;&#21487;&#33021;&#21453;&#26144;&#24433;&#21709;&#21147;&#27963;&#21160;&#30340;&#32858;&#31867;&#65292;&#28982;&#21518;&#36890;&#36807;&#23427;&#20204;&#19982;&#39640;&#24433;&#21709;&#21147;&#32858;&#31867;&#30340;&#20851;&#32852;&#26469;&#35782;&#21035;&#19982;&#24433;&#21709;&#21147;&#27963;&#21160;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#25991;&#26723;&#26159;&#21542;&#23646;&#20110;&#24433;&#21709;&#21147;&#27963;&#21160;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#30452;&#25509;&#25991;&#26723;&#32423;&#21035;&#20998;&#31867;&#21644;&#30452;&#25509;&#25991;&#26723;&#32423;&#21035;&#32858;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21508;&#31181;&#26032;&#39062;&#25216;&#26415;&#26469;&#22686;&#24378;&#25105;&#20204;&#30340;&#27969;&#31243;&#65292;&#21253;&#25324;&#20351;&#29992;&#29616;&#26377;&#30340;&#20107;&#20214;&#20107;&#23454;&#39044;&#27979;&#31995;&#32479;&#33719;&#21462;&#25991;&#26723;&#37096;&#20998;&#65292;&#24182;&#32858;&#21512;&#22810;&#20010;&#32858;&#31867;&#23454;&#39564;&#20197;&#25552;&#39640;&#32858;&#31867;&#21644;&#25991;&#26723;&#20998;&#31867;&#30340;&#24615;&#33021;&#12290;&#22312;&#32858;&#31867;&#30340;&#22522;&#30784;&#19978;&#23545;&#25991;&#26723;&#36827;&#34892;&#20998;&#31867;&#19981;&#20165;&#21487;&#20197;&#20934;&#30830;&#25552;&#21462;&#19982;&#24433;&#21709;&#21147;&#27963;&#21160;&#30456;&#20851;&#30340;&#25991;&#26723;&#37096;&#20998;&#65292;&#36824;&#21487;&#20197;&#25429;&#25417;&#21040;&#24433;&#21709;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17151v1 Announce Type: new  Abstract: We propose a novel clustering pipeline to detect and characterize influence campaigns from documents. This approach clusters parts of document, detects clusters that likely reflect an influence campaign, and then identifies documents linked to an influence campaign via their association with the high-influence clusters. Our approach outperforms both the direct document-level classification and the direct document-level clustering approach in predicting if a document is part of an influence campaign. We propose various novel techniques to enhance our pipeline, including using an existing event factuality prediction system to obtain document parts, and aggregating multiple clustering experiments to improve the performance of both cluster and document classification. Classifying documents on the top of clustering not only accurately extracts the parts of the documents that are relevant to influence campaigns, but also capture influence camp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Fact-and-Reflection&#65288;FaR&#65289;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#24050;&#30693;&#8220;&#20107;&#23454;&#8221;&#24182;&#35201;&#27714;&#27169;&#22411;&#8220;&#21453;&#24605;&#8221;&#65292;&#22312;&#20004;&#20010;&#27493;&#39588;&#20013;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#26657;&#20934;</title><link>https://arxiv.org/abs/2402.17124</link><description>&lt;p&gt;
Fact-and-Reflection&#65288;FaR&#65289;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17124
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Fact-and-Reflection&#65288;FaR&#65289;&#25552;&#31034;&#31574;&#30053;&#65292;&#36890;&#36807;&#24341;&#20837;&#24050;&#30693;&#8220;&#20107;&#23454;&#8221;&#24182;&#35201;&#27714;&#27169;&#22411;&#8220;&#21453;&#24605;&#8221;&#65292;&#22312;&#20004;&#20010;&#27493;&#39588;&#20013;&#25913;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#20351;LLM&#20540;&#24471;&#20449;&#36182;&#65292;&#20854;&#32622;&#20449;&#27700;&#24179;&#24212;&#19982;&#23454;&#38469;&#34920;&#29616;&#33391;&#22909;&#26657;&#20934;&#12290;&#23613;&#31649;&#29616;&#22312;&#26222;&#36941;&#35748;&#20026;LLM&#30340;&#34920;&#29616;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#21040;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#20294;&#25552;&#31034;LLM&#20013;&#30340;&#32622;&#20449;&#26657;&#20934;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#25506;&#35752;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#22914;&#20309;&#24433;&#21709;LLM&#30340;&#32622;&#20449;&#26657;&#20934;&#20197;&#21450;&#22914;&#20309;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#38382;&#31572;&#29615;&#22659;&#20013;&#23545;&#20845;&#31181;&#25552;&#31034;&#26041;&#27861;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26377;&#21161;&#20110;&#25913;&#36827;LLM&#30340;&#39044;&#26399;&#26657;&#20934;&#65292;&#20294;&#20063;&#20250;&#23548;&#33268;LLM&#22312;&#21709;&#24212;&#26576;&#20123;&#23454;&#20363;&#26102;&#36807;&#20110;&#33258;&#20449;&#12290;&#21463;&#20154;&#31867;&#35748;&#30693;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Fact-and-Reflection&#65288;FaR&#65289;&#25552;&#31034;&#65292;&#23427;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#25913;&#21892;&#20102;LLM&#30340;&#26657;&#20934;&#12290;&#39318;&#20808;&#65292;FaR&#20174;LLM&#20013;&#33719;&#21462;&#19982;&#36755;&#20837;&#25552;&#31034;&#30456;&#20851;&#30340;&#24050;&#30693;&#8220;&#20107;&#23454;&#8221;&#12290;&#28982;&#21518;&#35201;&#27714;&#27169;&#22411;&#8220;&#21453;&#24605;&#8221;&#23427;&#20204;&#20197;&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17124v1 Announce Type: new  Abstract: For a LLM to be trustworthy, its confidence level should be well-calibrated with its actual performance. While it is now common sense that LLM performances are greatly impacted by prompts, the confidence calibration in prompting LLMs has yet to be thoroughly explored. In this paper, we explore how different prompting strategies influence LLM confidence calibration and how it could be improved. We conduct extensive experiments on six prompting methods in the question-answering context and we observe that, while these methods help improve the expected LLM calibration, they also trigger LLMs to be over-confident when responding to some instances. Inspired by human cognition, we propose Fact-and-Reflection (FaR) prompting, which improves the LLM calibration in two steps. First, FaR elicits the known "facts" that are relevant to the input prompt from the LLM. And then it asks the model to "reflect" over them to generate the final answer. Expe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#25552;&#31034;&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20196;&#20154;&#32039;&#24352;&#30340;&#25925;&#20107;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17119</link><description>&lt;p&gt;
&#21019;&#36896;&#20196;&#20154;&#32039;&#24352;&#30340;&#25925;&#20107;: &#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36845;&#20195;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Creating Suspenseful Stories: Iterative Planning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17119
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#25552;&#31034;&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20196;&#20154;&#32039;&#24352;&#30340;&#25925;&#20107;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25925;&#20107;&#29983;&#25104;&#19968;&#30452;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#39033;&#38271;&#26399;&#25361;&#25112;&#12290;&#22312;&#25152;&#26377;&#25925;&#20107;&#32500;&#24230;&#20013;&#65292;&#24748;&#24565;&#22312;&#20154;&#31867;&#20889;&#30340;&#25925;&#20107;&#20013;&#38750;&#24120;&#26222;&#36941;&#65292;&#20294;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25925;&#20107;&#20013;&#30456;&#23545;&#19981;&#22826;&#34987;&#25506;&#32034;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26368;&#26032;&#36827;&#23637;&#22823;&#22823;&#25512;&#21160;&#20102;&#35821;&#35328;&#29983;&#25104;&#30340;&#21457;&#23637;&#65292;&#20294;&#22312;&#20196;&#20154;&#32039;&#24352;&#30340;&#25925;&#20107;&#29983;&#25104;&#26041;&#38754;&#65292;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#21487;&#38752;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#25552;&#31034;&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25166;&#26681;&#20110;&#35748;&#30693;&#24515;&#29702;&#23398;&#21644;&#21465;&#20107;&#23398;&#20013;&#20851;&#20110;&#25925;&#20107;&#24748;&#24565;&#30340;&#20004;&#20010;&#29702;&#35770;&#22522;&#30784;&#12290;&#36825;&#31181;&#22522;&#20110;&#29702;&#35770;&#30340;&#26041;&#27861;&#23436;&#20840;&#38646;&#26679;&#26412;&#22320;&#24037;&#20316;&#65292;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#30417;&#30563;&#24335;&#25925;&#20107;&#35821;&#26009;&#24211;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#31687;&#35770;&#25991;&#26159;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;LLMs&#29983;&#25104;&#20196;&#20154;&#32039;&#24352;&#30340;&#25925;&#20107;&#12290;&#23545;&#29983;&#25104;&#30340;&#24748;&#24565;&#25925;&#20107;&#36827;&#34892;&#30340;&#24191;&#27867;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17119v1 Announce Type: new  Abstract: Automated story generation has been one of the long-standing challenges in NLP. Among all dimensions of stories, suspense is very common in human-written stories but relatively under-explored in AI-generated stories. While recent advances in large language models (LLMs) have greatly promoted language generation in general, state-of-the-art LLMs are still unreliable when it comes to suspenseful story generation. We propose a novel iterative-prompting-based planning method that is grounded in two theoretical foundations of story suspense from cognitive psychology and narratology. This theory-grounded method works in a fully zero-shot manner and does not rely on any supervised story corpora. To the best of our knowledge, this paper is the first attempt at suspenseful story generation with LLMs. Extensive human evaluations of the generated suspenseful stories demonstrate the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Ex&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20107;&#23454;&#38169;&#35823;&#35828;&#26126;&#27493;&#39588;&#26469;&#20462;&#27491;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#26469;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#21644;&#25346;&#38047;&#26102;&#38388;</title><link>https://arxiv.org/abs/2402.17097</link><description>&lt;p&gt;
&#20462;&#22797;: &#22312;&#35828;&#26126;&#21518;&#20462;&#27491;LLM&#21709;&#24212;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17097
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Ex&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20107;&#23454;&#38169;&#35823;&#35828;&#26126;&#27493;&#39588;&#26469;&#20462;&#27491;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#26469;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#21644;&#25346;&#38047;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#26159;LLM&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#65292;&#25105;&#20204;&#38656;&#35201;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#20197;&#20415;&#21487;&#38752;&#22320;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#26816;&#26597;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#30456;&#24212;&#22320;&#36827;&#34892;&#20462;&#35746;&#65292;&#20197;&#20943;&#23569;&#24187;&#35273;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Re-Ex&#65292;&#19968;&#31181;&#20462;&#35746;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#20107;&#23454;&#38169;&#35823;&#35828;&#26126;&#27493;&#39588;&#30340;&#26032;&#27493;&#39588;&#12290; Re-Ex&#20351;&#29992;3&#20010;&#27493;&#39588;&#23545;LLM&#30340;&#21021;&#22987;&#21709;&#24212;&#36827;&#34892;&#20462;&#35746;&#65306;&#39318;&#20808;&#65292;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#33719;&#21462;&#21709;&#24212;&#20013;&#20107;&#23454;&#38169;&#35823;&#30340;&#35777;&#25454;&#65307;&#31532;&#20108;&#65292;&#35201;&#27714;LLM&#26681;&#25454;&#31532;&#19968;&#27493;&#20013;&#25910;&#38598;&#30340;&#35777;&#25454;&#35299;&#37322;&#21709;&#24212;&#20013;&#30340;&#38382;&#39064;&#37096;&#20998;&#65307;&#26368;&#21518;&#65292;LLM&#20351;&#29992;&#22312;&#31532;&#20108;&#27493;&#20013;&#33719;&#24471;&#30340;&#35299;&#37322;&#23545;&#21709;&#24212;&#36827;&#34892;&#20462;&#35746;&#12290;&#38500;&#20102;&#35828;&#26126;&#27493;&#39588;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#21644;&#25346;&#38047;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17097v1 Announce Type: cross  Abstract: Mitigating hallucination issues is one of the main challenges of LLMs we need to overcome, in order to reliably use them in real-world scenarios. Recently, various methods are proposed to check the factual errors in the LLM-generated texts and revise them accordingly, to reduce the hallucination issue. In this paper, we propose Re-Ex, a method of revising LLM-generated texts, which introduces a novel step dubbed as the factual error explanation step. Re-Ex revises the initial response of LLMs using 3-steps: first, external tools are used to get the evidences on the factual errors in the response; second, LLMs are instructed to explain the problematic parts of the response based on the evidences gathered in the first step; finally, LLMs revise the response using the explanation obtained in the second step. In addition to the explanation step, we propose new prompting techniques to reduce the amount of tokens and wall-clock time required
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25925;&#20107;&#35762;&#36848;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27861;&#24459;&#25945;&#32946;&#26041;&#27861;&#65292;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#22763;&#23398;&#20064;&#22797;&#26434;&#30340;&#27861;&#24459;&#27010;&#24565;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#27861;&#24459;&#25925;&#20107;&#21644;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.17019</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#35762;&#25925;&#20107;&#23398;&#20064;&#22797;&#26434;&#27861;&#24459;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17019
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#25925;&#20107;&#35762;&#36848;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27861;&#24459;&#25945;&#32946;&#26041;&#27861;&#65292;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#22763;&#23398;&#20064;&#22797;&#26434;&#30340;&#27861;&#24459;&#27010;&#24565;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#27861;&#24459;&#25925;&#20107;&#21644;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#27861;&#24459;&#30693;&#35782;&#21464;&#24471;&#26356;&#23481;&#26131;&#29702;&#35299;&#23545;&#20110;&#25552;&#21319;&#26222;&#36890;&#27861;&#24459;&#32032;&#20859;&#21644;&#40723;&#21169;&#20844;&#27665;&#21442;&#19982;&#27665;&#20027;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#27809;&#26377;&#27861;&#24459;&#32972;&#26223;&#30340;&#20154;&#26469;&#35828;&#65292;&#27861;&#24459;&#25991;&#20214;&#36890;&#24120;&#38590;&#20197;&#29702;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#27861;&#24459;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#65292;&#24110;&#21161;&#38750;&#19987;&#19994;&#20154;&#22763;&#36890;&#36807;&#35762;&#25925;&#20107;&#23398;&#20064;&#22797;&#26434;&#30340;&#27861;&#24459;&#27010;&#24565;&#65292;&#35762;&#25925;&#20107;&#26159;&#20256;&#36798;&#22797;&#26434;&#21644;&#25277;&#35937;&#27010;&#24565;&#30340;&#26377;&#25928;&#25945;&#23398;&#24037;&#20855;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;LegalStories&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;295&#20010;&#22797;&#26434;&#30340;&#27861;&#24459;&#21407;&#21017;&#65292;&#27599;&#20010;&#21407;&#21017;&#37117;&#38468;&#26377;&#19968;&#20010;&#25925;&#20107;&#21644;&#19968;&#32452;&#30001;LLMs&#29983;&#25104;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;&#20026;&#20102;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#21508;&#31181;LLMs&#29983;&#25104;&#35299;&#37322;&#36825;&#20123;&#27010;&#24565;&#30340;&#27861;&#24459;&#25925;&#20107;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19987;&#23478;&#21442;&#19982;&#30340;&#26041;&#27861;&#26469;&#36845;&#20195;&#35774;&#35745;&#22810;&#39033;&#36873;&#25321;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17019v1 Announce Type: new  Abstract: Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset LegalStories, which consists of 295 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop method to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through an 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#29420;&#29305;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30446;&#26631;&#65292;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#29992;&#20110;&#25903;&#25345;&#33521;&#35821;&#21644;&#20854;&#20182;&#30446;&#26631;&#35821;&#35328;&#30340;&#26368;&#20808;&#36827;&#30340;&#21452;&#35821;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;STS&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#30446;&#26631;&#35821;&#35328;&#29702;&#35299;&#21644;&#36328;&#35821;&#35328;&#35780;&#20272;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.17016</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;8192&#26631;&#35760;&#30340;&#21452;&#35821;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17016
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#29420;&#29305;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30446;&#26631;&#65292;&#30740;&#31350;&#32773;&#35774;&#35745;&#20102;&#29992;&#20110;&#25903;&#25345;&#33521;&#35821;&#21644;&#20854;&#20182;&#30446;&#26631;&#35821;&#35328;&#30340;&#26368;&#20808;&#36827;&#30340;&#21452;&#35821;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;STS&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#30446;&#26631;&#35821;&#35328;&#29702;&#35299;&#21644;&#36328;&#35821;&#35328;&#35780;&#20272;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#26032;&#39062;&#30340;&#26368;&#20808;&#36827;&#30340;&#21452;&#35821;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#26088;&#22312;&#25903;&#25345;&#33521;&#35821;&#21644;&#21478;&#19968;&#31181;&#30446;&#26631;&#35821;&#35328;&#12290;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#38271;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#25991;&#26412;&#36755;&#20837;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#22914;&#25991;&#26412;&#26816;&#32034;&#12289;&#32858;&#31867;&#21644;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#35745;&#31639;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110;&#21452;&#35821;&#27169;&#22411;&#24182;&#24341;&#20837;&#29420;&#29305;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#30446;&#26631;&#65292;&#25105;&#20204;&#26174;&#33879;&#25913;&#21892;&#20102;&#22312;STS&#20219;&#21153;&#19978;&#30340;&#27169;&#22411;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#30446;&#26631;&#35821;&#35328;&#29702;&#35299;&#21644;&#36328;&#35821;&#35328;&#35780;&#20272;&#20219;&#21153;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#21452;&#35821;&#27169;&#22411;&#26356;&#21152;&#39640;&#25928;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#21442;&#25968;&#21644;&#26356;&#23569;&#30340;&#20869;&#23384;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#36739;&#23567;&#30340;&#35789;&#27719;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#65288;MTEB&#65289;&#65292;&#21253;&#25324;&#24503;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#23884;&#20837;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17016v1 Announce Type: cross  Abstract: We introduce a novel suite of state-of-the-art bilingual text embedding models that are designed to support English and another target language. These models are capable of processing lengthy text inputs with up to 8192 tokens, making them highly versatile for a range of natural language processing tasks such as text retrieval, clustering, and semantic textual similarity (STS) calculations.   By focusing on bilingual models and introducing a unique multi-task learning objective, we have significantly improved the model performance on STS tasks, which outperforms the capabilities of existing multilingual models in both target language understanding and cross-lingual evaluation tasks. Moreover, our bilingual models are more efficient, requiring fewer parameters and less memory due to their smaller vocabulary needs. Furthermore, we have expanded the Massive Text Embedding Benchmark (MTEB) to include benchmarks for German and Spanish embed
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#26088;&#22312;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#24110;&#21161;&#27668;&#20505;&#27963;&#21160;&#20154;&#22763;&#35782;&#21035;&#21644;&#24212;&#23545;&#20167;&#24680;&#35328;&#35770;&#65292;&#22242;&#38431;&#35780;&#20272;&#20102;&#22810;&#31181;&#27169;&#22411;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12289;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#35782;&#21035;&#21644;&#31435;&#22330;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#36259;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.17014</link><description>&lt;p&gt;
Z-AGI&#23454;&#39564;&#23460;&#21442;&#19982;ClimateActivism 2024&#65306;&#31038;&#20132;&#23186;&#20307;&#19978;&#31435;&#22330;&#21644;&#20167;&#24680;&#20107;&#20214;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Z-AGI Labs at ClimateActivism 2024: Stance and Hate Event Detection on Social Media
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17014
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#26088;&#22312;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#24110;&#21161;&#27668;&#20505;&#27963;&#21160;&#20154;&#22763;&#35782;&#21035;&#21644;&#24212;&#23545;&#20167;&#24680;&#35328;&#35770;&#65292;&#22242;&#38431;&#35780;&#20272;&#20102;&#22810;&#31181;&#27169;&#22411;&#65292;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12289;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#35782;&#21035;&#21644;&#31435;&#22330;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#36259;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#39046;&#22495;&#65292;&#20016;&#23500;&#30340;&#25968;&#25454;&#26159;&#27934;&#23519;&#31038;&#20250;&#12289;&#25919;&#27835;&#21644;&#32463;&#27982;&#26684;&#23616;&#22797;&#26434;&#24615;&#30340;&#20851;&#38190;&#20449;&#24687;&#26469;&#28304;&#12290;&#38024;&#23545;&#23545;&#20107;&#20214;&#39640;&#36136;&#37327;&#20449;&#24687;&#30340;&#26085;&#30410;&#22686;&#38271;&#38656;&#27714;&#20197;&#21450;&#25171;&#20987;&#20167;&#24680;&#35328;&#35770;&#30340;&#24517;&#35201;&#24615;&#65292;&#26412;&#30740;&#31350;&#21457;&#36215;&#20102;&#22312;CASE 2024&#20030;&#21150;&#30340;Climate Activism&#31435;&#22330;&#21644;&#20167;&#24680;&#20107;&#20214;&#26816;&#27979;&#20849;&#20139;&#20219;&#21153;&#30340;&#24314;&#31435;&#12290;&#32858;&#28966;&#27668;&#20505;&#27963;&#21160;&#20154;&#22763;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#24212;&#23545;&#20167;&#24680;&#35328;&#35770;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#20174;Twitter&#25512;&#25991;&#20013;&#35782;&#21035;&#20167;&#24680;&#35328;&#35770;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#36890;&#36807;&#20998;&#26512;&#19977;&#20010;&#23376;&#20219;&#21153;-&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65288;&#23376;&#20219;&#21153;A&#65289;&#12289;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#35782;&#21035;&#65288;&#23376;&#20219;&#21153;B&#65289;&#21644;&#31435;&#22330;&#26816;&#27979;&#65288;&#23376;&#20219;&#21153;C&#65289;- Z-AGI&#23454;&#39564;&#23460;&#22242;&#38431;&#35780;&#20272;&#20102;&#22522;&#20110;Tf-Idf&#30340;&#21508;&#31181;&#27169;&#22411;&#65292;&#21253;&#25324;LSTM&#12289;Xgboost&#21644;LGBM&#12290;&#32467;&#26524;&#26174;&#31034;&#20986;&#26377;&#36259;&#30340;&#21464;&#21270;&#65292;Catboost&#22312;&#23376;&#20219;&#21153;B&#65288;F1&#65306;0.5604&#65289;&#21644;&#23376;&#20219;&#21153;C&#65288;F1&#65306;0.7081&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;LGBM&#25104;&#20026;&#23376;&#20219;&#21153;A&#65288;F1&#65306;0.8684&#65289;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17014v1 Announce Type: new  Abstract: In the digital realm, rich data serves as a crucial source of insights into the complexities of social, political, and economic landscapes. Addressing the growing need for high-quality information on events and the imperative to combat hate speech, this research led to the establishment of the Shared Task on Climate Activism Stance and Hate Event Detection at CASE 2024. Focused on climate activists contending with hate speech on social media, our study contributes to hate speech identification from tweets. Analyzing three sub-tasks - Hate Speech Detection (Sub-task A), Targets of Hate Speech Identification (Sub-task B), and Stance Detection (Sub-task C) - Team Z-AGI Labs evaluated various models, including LSTM, Xgboost, and LGBM based on Tf-Idf. Results unveiled intriguing variations, with Catboost excelling in Subtask-B (F1: 0.5604) and Subtask-C (F1: 0.7081), while LGBM emerged as the top-performing model for Subtask-A (F1: 0.8684). T
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#29790;&#22763;&#21496;&#27861;&#39044;&#27979;&#20013;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#21033;&#29992;&#20102;&#21807;&#19968;&#21487;&#29992;&#30340;&#22810;&#35821;&#35328;LJP&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#26368;&#26032;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;BERT-based LJP&#27169;&#22411;&#36827;&#34892;&#20102;&#21487;&#35299;&#37322;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.17013</link><description>&lt;p&gt;
&#22312;&#29790;&#22763;&#21496;&#27861;&#39044;&#27979;&#20013;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#65306;&#22312;&#19968;&#20010;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Towards Explainability and Fairness in Swiss Judgement Prediction: Benchmarking on a Multilingual Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#29790;&#22763;&#21496;&#27861;&#39044;&#27979;&#20013;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#21033;&#29992;&#20102;&#21807;&#19968;&#21487;&#29992;&#30340;&#22810;&#35821;&#35328;LJP&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#26368;&#26032;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;BERT-based LJP&#27169;&#22411;&#36827;&#34892;&#20102;&#21487;&#35299;&#37322;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17013v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#23545;&#27861;&#24459;&#35009;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#31995;&#32479;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#36827;&#34892;&#35780;&#20272;&#22312;&#26500;&#24314;&#20540;&#24471;&#20449;&#36182;&#21644;&#36879;&#26126;&#31995;&#32479;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#36825;&#20123;&#31995;&#32479;&#20381;&#36182;&#21487;&#33021;&#32570;&#20047;&#27861;&#24459;&#30456;&#20851;&#24615;&#25110;&#28041;&#21450;&#25935;&#24863;&#23646;&#24615;&#30340;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;LJP&#27169;&#22411;&#20013;&#21487;&#35299;&#37322;&#24615;&#21644;&#20844;&#24179;&#24615;&#30340;&#39046;&#22495;&#65292;&#21033;&#29992;&#29790;&#22763;&#35009;&#20915;&#39044;&#27979;&#65288;SJP&#65289;&#36825;&#19968;&#21807;&#19968;&#21487;&#29992;&#30340;&#22810;&#35821;&#35328;LJP&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20010;&#21253;&#25324;108&#20010;&#26696;&#20363;&#30340;&#25903;&#25345;&#21644;&#21453;&#23545;&#27861;&#24459;&#19987;&#23478;&#35009;&#20915;&#30340;&#29702;&#30001;&#30340;&#20840;&#38754;&#25910;&#38598;&#65292;&#22312;&#24503;&#35821;&#12289;&#27861;&#35821;&#21644;&#24847;&#22823;&#21033;&#35821;&#20013;&#25552;&#20379;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#36974;&#25377;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#26032;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;BERT-based LJP&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#34920;&#29616;&#65292;&#20197;&#21450;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#31561;&#25216;&#26415;&#24320;&#21457;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#39044;&#27979;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17013v1 Announce Type: cross  Abstract: The assessment of explainability in Legal Judgement Prediction (LJP) systems is of paramount importance in building trustworthy and transparent systems, particularly considering the reliance of these systems on factors that may lack legal relevance or involve sensitive attributes. This study delves into the realm of explainability and fairness in LJP models, utilizing Swiss Judgement Prediction (SJP), the only available multilingual LJP dataset. We curate a comprehensive collection of rationales that `support' and `oppose' judgement from legal experts for 108 cases in German, French, and Italian. By employing an occlusion-based explainability approach, we evaluate the explainability performance of state-of-the-art monolingual and multilingual BERT-based LJP models, as well as models developed with techniques such as data augmentation and cross-lingual transfer, which demonstrated prediction performance improvement. Notably, our finding
&lt;/p&gt;</description></item><item><title>DiffuCOMET&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#23398;&#20064;&#26469;&#37325;&#26500;&#21465;&#36848;&#19978;&#19979;&#25991;&#19982;&#30456;&#20851;&#24120;&#35782;&#30693;&#35782;&#20043;&#38388;&#35821;&#20041;&#36830;&#25509;&#30340;&#30693;&#35782;&#27169;&#22411;&#65292;&#29983;&#25104;&#30340;&#30693;&#35782;&#22312;&#24120;&#35782;&#22810;&#26679;&#24615;&#12289;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#21644;&#23545;&#24050;&#30693;&#21442;&#32771;&#25991;&#29486;&#30340;&#23545;&#40784;&#26041;&#38754;&#36798;&#21040;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.17011</link><description>&lt;p&gt;
DiffuCOMET: &#19978;&#19979;&#25991;&#24120;&#35782;&#30693;&#35782;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
DiffuCOMET: Contextual Commonsense Knowledge Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17011
&lt;/p&gt;
&lt;p&gt;
DiffuCOMET&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#23398;&#20064;&#26469;&#37325;&#26500;&#21465;&#36848;&#19978;&#19979;&#25991;&#19982;&#30456;&#20851;&#24120;&#35782;&#30693;&#35782;&#20043;&#38388;&#35821;&#20041;&#36830;&#25509;&#30340;&#30693;&#35782;&#27169;&#22411;&#65292;&#29983;&#25104;&#30340;&#30693;&#35782;&#22312;&#24120;&#35782;&#22810;&#26679;&#24615;&#12289;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#21644;&#23545;&#24050;&#30693;&#21442;&#32771;&#25991;&#29486;&#30340;&#23545;&#40784;&#26041;&#38754;&#36798;&#21040;&#26356;&#22909;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#19978;&#19979;&#25991;&#30456;&#20851;&#19988;&#22810;&#26679;&#21270;&#30340;&#24120;&#35782;&#20197;&#29702;&#35299;&#21465;&#36848;&#25925;&#20107;&#23545;&#20110;&#30693;&#35782;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#21033;&#29992;&#25193;&#25955;&#30340;&#30693;&#35782;&#27169;&#22411;DiffuCOMET&#65292;&#20197;&#23398;&#20064;&#37325;&#26500;&#21465;&#36848;&#19978;&#19979;&#25991;&#19982;&#30456;&#20851;&#24120;&#35782;&#30693;&#35782;&#20043;&#38388;&#30340;&#38544;&#24335;&#35821;&#20041;&#36830;&#25509;&#12290;&#36890;&#36807;&#22810;&#27425;&#25193;&#25955;&#27493;&#39588;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36880;&#27493;&#23436;&#21892;&#20102;&#19982;&#21465;&#36848;&#38170;&#23450;&#30340;&#24120;&#35782;&#20107;&#23454;&#34920;&#31034;&#65292;&#20026;&#36755;&#20837;&#19978;&#19979;&#25991;&#29983;&#25104;&#19978;&#19979;&#25991;&#30456;&#20851;&#19988;&#22810;&#26679;&#21270;&#30340;&#24120;&#35782;&#25512;&#26029;&#12290;&#20026;&#20102;&#35780;&#20272;DiffuCOMET&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#34913;&#37327;&#24120;&#35782;&#25512;&#26029;&#30340;&#26032;&#25351;&#26631;&#65292;&#26356;&#23494;&#20999;&#22320;&#34913;&#37327;&#30693;&#35782;&#22810;&#26679;&#24615;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;ComFact&#21644;WebNLG+&#19978;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;DiffuCOMET&#29983;&#25104;&#30340;&#30693;&#35782;&#22312;&#24120;&#35782;&#22810;&#26679;&#24615;&#12289;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#20197;&#21450;&#19982;&#24050;&#30693;&#40644;&#37329;&#21442;&#32771;&#25991;&#29486;&#30340;&#23545;&#40784;&#20043;&#38388;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#26435;&#34913;&#65292;&#19982;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17011v1 Announce Type: new  Abstract: Inferring contextually-relevant and diverse commonsense to understand narratives remains challenging for knowledge models. In this work, we develop a series of knowledge models, DiffuCOMET, that leverage diffusion to learn to reconstruct the implicit semantic connections between narrative contexts and relevant commonsense knowledge. Across multiple diffusion steps, our method progressively refines a representation of commonsense facts that is anchored to a narrative, producing contextually-relevant and diverse commonsense inferences for an input context. To evaluate DiffuCOMET, we introduce new metrics for commonsense inference that more closely measure knowledge diversity and contextual relevance. Our results on two different benchmarks, ComFact and WebNLG+, show that knowledge generated by DiffuCOMET achieves a better trade-off between commonsense diversity, contextual relevance and alignment to known gold references, compared to basel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#30693;&#35782;&#22238;&#24518;&#21442;&#32771;&#27573;&#33853;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#22238;&#24518;&#21442;&#32771;&#30340;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.17010</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#22238;&#24518;&#21442;&#32771;&#20301;&#32622;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Recall Reference Location Like Humans?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#30693;&#35782;&#22238;&#24518;&#21442;&#32771;&#27573;&#33853;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#22238;&#24518;&#21442;&#32771;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23436;&#25104;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#26102;&#65292;&#20154;&#31867;&#26377;&#26102;&#19981;&#20165;&#38656;&#35201;&#19968;&#20010;&#31572;&#26696;&#65292;&#36824;&#38656;&#35201;&#30456;&#24212;&#30340;&#21442;&#32771;&#27573;&#33853;&#20379;&#36741;&#21161;&#38405;&#35835;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#36890;&#36807;&#39069;&#22806;&#30340;&#26816;&#32034;&#27169;&#22411;&#33719;&#21462;&#39044;&#20998;&#27573;&#30340;&#25991;&#31456;&#22359;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39044;&#35757;&#32451;&#38454;&#27573;&#23384;&#20648;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#65292;&#29420;&#31435;&#20110;&#20219;&#20309;&#36215;&#22987;&#20301;&#32622;&#22238;&#24518;&#21442;&#32771;&#27573;&#33853;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#25311;&#20154;&#31867;&#22238;&#24518;&#26131;&#34987;&#36951;&#24536;&#21442;&#32771;&#30340;&#24773;&#26223;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;LLM&#34987;&#25552;&#31034;&#22238;&#24518;&#25991;&#26723;&#26631;&#39064;&#26631;&#35782;&#31526;&#20197;&#33719;&#21462;&#31895;&#31890;&#24230;&#25991;&#26723;&#38598;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#33719;&#24471;&#30340;&#31895;&#31890;&#24230;&#25991;&#26723;&#38598;&#65292;&#23427;&#22238;&#24518;&#32454;&#31890;&#24230;&#27573;&#33853;&#12290;&#22312;&#20004;&#38454;&#27573;&#22238;&#24518;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32422;&#26463;&#35299;&#30721;&#26469;&#30830;&#20445;&#19981;&#29983;&#25104;&#23384;&#20648;&#25991;&#26723;&#20043;&#22806;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#22686;&#21152;&#36895;&#24230;&#65292;&#25105;&#20204;&#21482;&#22238;&#24518;&#30701;&#21069;&#32512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17010v1 Announce Type: cross  Abstract: When completing knowledge-intensive tasks, humans sometimes need not just an answer but also a corresponding reference passage for auxiliary reading. Previous methods required obtaining pre-segmented article chunks through additional retrieval models. This paper explores leveraging the parameterized knowledge stored during the pre-training phase of large language models (LLMs) to independently recall reference passage from any starting position. We propose a two-stage framework that simulates the scenario of humans recalling easily forgotten references. Initially, the LLM is prompted to recall document title identifiers to obtain a coarse-grained document set. Then, based on the acquired coarse-grained document set, it recalls fine-grained passage. In the two-stage recall process, we use constrained decoding to ensure that content outside of the stored documents is not generated. To increase speed, we only recall a short prefix in the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;LLMs&#22312;Semantic Overlap Summarization&#20219;&#21153;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20351;&#29992;TELeR&#20998;&#31867;&#27861;&#35780;&#20272;&#20102;15&#20010;&#27969;&#34892;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#24635;&#32467;&#22810;&#20010;&#19981;&#21516;&#21465;&#36848;&#20043;&#38388;&#37325;&#21472;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.17008</link><description>&lt;p&gt;
&#22312;&#35821;&#20041;&#37325;&#21472;&#25688;&#35201;&#20219;&#21153;&#19978;&#23545;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking LLMs on the Semantic Overlap Summarization Task
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17008
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;LLMs&#22312;Semantic Overlap Summarization&#20219;&#21153;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#20351;&#29992;TELeR&#20998;&#31867;&#27861;&#35780;&#20272;&#20102;15&#20010;&#27969;&#34892;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#24635;&#32467;&#22810;&#20010;&#19981;&#21516;&#21465;&#36848;&#20043;&#38388;&#37325;&#21472;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Semantic Overlap Summarization (SOS)&#26159;&#19968;&#39033;&#21463;&#38480;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;&#20219;&#21153;&#65292;&#20854;&#20013;&#32422;&#26463;&#26159;&#25429;&#33719;&#20004;&#20010;&#19981;&#21516;&#21465;&#36848;&#20043;&#38388;&#30340;&#20849;&#21516;/&#37325;&#21472;&#20449;&#24687;&#12290;&#34429;&#28982;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#25688;&#35201;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#23578;&#26410;&#36827;&#34892;&#36807;&#20351;&#29992;LLMs&#36827;&#34892;SOS&#20219;&#21153;&#30340;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#12290;&#30001;&#20110;LLMs&#30340;&#21709;&#24212;&#23545;&#25552;&#31034;&#35774;&#35745;&#20013;&#30340;&#32454;&#24494;&#21464;&#21270;&#24456;&#25935;&#24863;&#65292;&#36827;&#34892;&#36825;&#26679;&#30340;&#22522;&#20934;&#27979;&#35797;&#30740;&#31350;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#22312;&#24471;&#20986;&#21487;&#38752;&#32467;&#35770;&#20043;&#21069;&#31995;&#32479;&#22320;&#25506;&#32034;&#21508;&#31181;&#25552;&#31034;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;TELeR&#20998;&#31867;&#27861;&#65292;&#21487;&#29992;&#20110;&#35774;&#35745;&#21644;&#25506;&#32034;LLMs&#30340;&#21508;&#31181;&#25552;&#31034;&#12290;&#21033;&#29992;&#36825;&#20010;TELeR&#20998;&#31867;&#27861;&#21644;15&#20010;&#27969;&#34892;&#30340;LLMs&#65292;&#26412;&#25991;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;SOS&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35780;&#20272;&#23427;&#20204;&#20174;&#22810;&#20010;&#19981;&#21516;&#21465;&#36848;&#20013;&#24635;&#32467;&#37325;&#21472;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17008v1 Announce Type: new  Abstract: Semantic Overlap Summarization (SOS) is a constrained multi-document summarization task, where the constraint is to capture the common/overlapping information between two alternative narratives. While recent advancements in Large Language Models (LLMs) have achieved superior performance in numerous summarization tasks, a benchmarking study of the SOS task using LLMs is yet to be performed. As LLMs' responses are sensitive to slight variations in prompt design, a major challenge in conducting such a benchmarking study is to systematically explore a variety of prompts before drawing a reliable conclusion. Fortunately, very recently, the TELeR taxonomy has been proposed which can be used to design and explore various prompts for LLMs. Using this TELeR taxonomy and 15 popular LLMs, this paper comprehensively evaluates LLMs on the SOS Task, assessing their ability to summarize overlapping information from multiple alternative narratives. For 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#25506;&#38024;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#20013;&#30340;&#22768;&#38899;&#34920;&#31034;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#30740;&#31350;&#21457;&#29616;&#23613;&#31649;&#20165;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#19968;&#20123;&#23545;&#35937;&#30340;&#22768;&#38899;&#30693;&#35782;&#26377;&#30528;&#22522;&#20110;&#23454;&#36136;&#30340;&#32534;&#30721;&#12290;</title><link>https://arxiv.org/abs/2402.16998</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21548;&#21040;&#20102;&#20160;&#20040;&#65311;&#25506;&#31350;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21548;&#35273;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
What Do Language Models Hear? Probing for Auditory Representations in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16998
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#25506;&#38024;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#34920;&#31034;&#21644;&#39044;&#35757;&#32451;&#38899;&#39057;&#27169;&#22411;&#20013;&#30340;&#22768;&#38899;&#34920;&#31034;&#32852;&#31995;&#22312;&#19968;&#36215;&#65292;&#30740;&#31350;&#21457;&#29616;&#23613;&#31649;&#20165;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#19968;&#20123;&#23545;&#35937;&#30340;&#22768;&#38899;&#30693;&#35782;&#26377;&#30528;&#22522;&#20110;&#23454;&#36136;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23545;&#29289;&#20307;&#30340;&#22768;&#38899;&#20855;&#26377;&#21547;&#20041;&#28145;&#21051;&#19988;&#22522;&#20110;&#23454;&#36136;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#32447;&#24615;&#25506;&#38024;&#65292;&#36890;&#36807;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#38899;&#39057;&#27169;&#22411;&#32473;&#20986;&#19968;&#20010;&#23545;&#35937;&#30340;&#22768;&#38899;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#32473;&#23450;&#19982;&#35813;&#23545;&#35937;&#30456;&#20851;&#30340;&#38899;&#39057;&#29255;&#27573;&#30340;&#24773;&#20917;&#19979;&#26816;&#32034;&#20986;&#35813;&#23545;&#35937;&#30340;&#27491;&#30830;&#25991;&#26412;&#34920;&#31034;&#12290;&#36825;&#20010;&#25506;&#38024;&#26159;&#36890;&#36807;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#25512;&#21160;&#23545;&#35937;&#30340;&#35821;&#35328;&#34920;&#31034;&#21644;&#22768;&#38899;&#34920;&#31034;&#24444;&#27492;&#25509;&#36817;&#12290;&#22312;&#35757;&#32451;&#20043;&#21518;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#25506;&#38024;&#23545;&#20110;&#19968;&#20123;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#23545;&#35937;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#38899;&#39057;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#25506;&#38024;&#30340;&#27867;&#21270;&#33021;&#21147;&#36229;&#36807;&#20102;&#38543;&#26426;&#29468;&#27979;&#30340;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#23613;&#31649;&#20165;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#19968;&#20123;&#23545;&#35937;&#30340;&#22768;&#38899;&#30693;&#35782;&#20855;&#26377;&#22522;&#20110;&#23454;&#36136;&#30340;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16998v1 Announce Type: cross  Abstract: This work explores whether language models encode meaningfully grounded representations of sounds of objects. We learn a linear probe that retrieves the correct text representation of an object given a snippet of audio related to that object, where the sound representation is given by a pretrained audio model. This probe is trained via a contrastive loss that pushes the language representations and sound representations of an object to be close to one another. After training, the probe is tested on its ability to generalize to objects that were not seen during training. Across different language models and audio models, we find that the probe generalization is above chance in many cases, indicating that despite being trained only on raw text, language models encode grounded knowledge of sounds for some objects.
&lt;/p&gt;</description></item><item><title>&#38271;&#23545;&#35805;&#25688;&#35201;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20026;&#26377;&#25928;&#27807;&#36890;&#21019;&#36896;&#36830;&#36143;&#21644;&#19978;&#19979;&#25991;&#20016;&#23500;&#25688;&#35201;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.16986</link><description>&lt;p&gt;
&#38271;&#23545;&#35805;&#25688;&#35201;: &#19968;&#39033;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Long Dialog Summarization: An Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16986
&lt;/p&gt;
&lt;p&gt;
&#38271;&#23545;&#35805;&#25688;&#35201;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20026;&#26377;&#25928;&#27807;&#36890;&#21019;&#36896;&#36830;&#36143;&#21644;&#19978;&#19979;&#25991;&#20016;&#23500;&#25688;&#35201;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Dialog summarization&#22312;&#36328;&#36234;&#19981;&#21516;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#23545;&#35805;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36825;&#39033;&#20219;&#21153;&#22312;&#25429;&#25417;&#22810;&#36718;&#38271;&#23545;&#35805;&#30340;&#20851;&#38190;&#28857;&#12289;&#19978;&#19979;&#25991;&#21644;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#38754;&#20020;&#30528;&#29420;&#29305;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#20026;&#26377;&#25928;&#27807;&#36890;&#21019;&#36896;&#36830;&#36143;&#21644;&#19978;&#19979;&#25991;&#20016;&#23500;&#25688;&#35201;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#21516;&#39046;&#22495;&#38271;&#23545;&#35805;&#25688;&#35201;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22522;&#20110;&#22522;&#20934;&#25351;&#26631;&#30340;&#35780;&#20272;&#26174;&#31034;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#30340;&#34920;&#29616;&#24182;&#19981;&#29702;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16986v1 Announce Type: new  Abstract: Dialog summarization has become increasingly important in managing and comprehending large-scale conversations across various domains. This task presents unique challenges in capturing the key points, context, and nuances of multi-turn long conversations for summarization. It is worth noting that the summarization techniques may vary based on specific requirements such as in a shopping-chatbot scenario, the dialog summary helps to learn user preferences, whereas in the case of a customer call center, the summary may involve the problem attributes that a user specified, and the final resolution provided. This work emphasizes the significance of creating coherent and contextually rich summaries for effective communication in various applications. We explore current state-of-the-art approaches for long dialog summarization in different domains and benchmark metrics based evaluations show that one single model does not perform well across va
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#22312;&#29616;&#20195;&#36719;&#20214;&#31995;&#32479;&#20013;&#25198;&#28436;&#30528;&#19981;&#21487;&#25110;&#32570;&#30340;&#35282;&#33394;&#65292;&#30417;&#31649;&#29615;&#22659;&#30340;&#21464;&#21270;&#12289;&#36719;&#20214;&#20010;&#24615;&#21270;&#38656;&#27714;&#30340;&#22686;&#38271;&#20197;&#21450;&#23545;&#27835;&#29702;&#30340;&#24378;&#35843;&#25512;&#21160;&#30528;&#22823;&#22411;&#20225;&#19994;&#37319;&#29992;&#33258;&#21160;&#21270;&#25216;&#26415;&#65292;&#24182;&#22312;AI&#20026;&#20013;&#24515;&#30340;&#31995;&#32479;&#20013;&#19981;&#26029;&#24341;&#20837;&#26032;&#30340;&#25361;&#25112;&#21644;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.16977</link><description>&lt;p&gt;
&#22788;&#29702;&#25968;&#25454;&#20197;&#24212;&#23545;RE&#20013;&#30340;&#25361;&#25112;&#65306;&#21033;&#29992;NLP&#21644;&#29983;&#25104;AI&#32531;&#35299;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Dealing with Data for RE: Mitigating Challenges using NLP and Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16977
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22312;&#29616;&#20195;&#36719;&#20214;&#31995;&#32479;&#20013;&#25198;&#28436;&#30528;&#19981;&#21487;&#25110;&#32570;&#30340;&#35282;&#33394;&#65292;&#30417;&#31649;&#29615;&#22659;&#30340;&#21464;&#21270;&#12289;&#36719;&#20214;&#20010;&#24615;&#21270;&#38656;&#27714;&#30340;&#22686;&#38271;&#20197;&#21450;&#23545;&#27835;&#29702;&#30340;&#24378;&#35843;&#25512;&#21160;&#30528;&#22823;&#22411;&#20225;&#19994;&#37319;&#29992;&#33258;&#21160;&#21270;&#25216;&#26415;&#65292;&#24182;&#22312;AI&#20026;&#20013;&#24515;&#30340;&#31995;&#32479;&#20013;&#19981;&#26029;&#24341;&#20837;&#26032;&#30340;&#25361;&#25112;&#21644;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#19981;&#26029;&#21464;&#21270;&#30340;&#21830;&#19994;&#29615;&#22659;&#20013;&#65292;&#20225;&#19994;&#38754;&#20020;&#30528;&#26085;&#30410;&#22686;&#22810;&#30340;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#21253;&#25324;&#19981;&#26029;&#21464;&#21270;&#30340;&#30417;&#31649;&#29615;&#22659;&#12289;&#36719;&#20214;&#24212;&#29992;&#20013;&#20010;&#24615;&#21270;&#38656;&#27714;&#30340;&#22686;&#38271;&#20197;&#21450;&#23545;&#27835;&#29702;&#30340;&#24378;&#35843;&#12290;&#38024;&#23545;&#36825;&#20123;&#22810;&#26041;&#38754;&#30340;&#38656;&#27714;&#65292;&#22823;&#22411;&#20225;&#19994;&#19968;&#30452;&#22312;&#37319;&#29992;&#20174;&#20248;&#21270;&#26680;&#24515;&#19994;&#21153;&#27969;&#31243;&#21040;&#22686;&#24378;&#23458;&#25143;&#20307;&#39564;&#30340;&#33258;&#21160;&#21270;&#12290;&#23454;&#38469;&#19978;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#25104;&#20026;&#29616;&#20195;&#36719;&#20214;&#31995;&#32479;&#30340;&#20851;&#38190;&#35201;&#32032;&#12290;&#22312;&#36825;&#31181;&#32972;&#26223;&#19979;&#65292;&#25968;&#25454;&#21457;&#25381;&#30528;&#19981;&#21487;&#25110;&#32570;&#30340;&#20316;&#29992;&#12290;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#19988;&#22312;&#24037;&#19994;&#35268;&#27169;&#19978;&#36816;&#34892;&#30340;&#20197;AI&#20026;&#20013;&#24515;&#30340;&#36719;&#20214;&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#25165;&#33021;&#26377;&#25928;&#36816;&#34892;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#29983;&#25104;AI&#23548;&#33268;&#23545;&#36275;&#22815;&#35780;&#20272;&#22522;&#20934;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#12290;&#25105;&#20204;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#32463;&#39564;&#26174;&#31034;&#20986;&#65292;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#23545;&#20110;&#23454;&#29616;RE&#26041;&#38754;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16977v1 Announce Type: cross  Abstract: Across the dynamic business landscape today, enterprises face an ever-increasing range of challenges. These include the constantly evolving regulatory environment, the growing demand for personalization within software applications, and the heightened emphasis on governance. In response to these multifaceted demands, large enterprises have been adopting automation that spans from the optimization of core business processes to the enhancement of customer experiences. Indeed, Artificial Intelligence (AI) has emerged as a pivotal element of modern software systems. In this context, data plays an indispensable role. AI-centric software systems based on supervised learning and operating at an industrial scale require large volumes of training data to perform effectively. Moreover, the incorporation of generative AI has led to a growing demand for adequate evaluation benchmarks. Our experience in this field has revealed that the requirement 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26816;&#27979;&#28508;&#22312;&#24187;&#35273;&#24182;&#24314;&#35758;&#26367;&#20195;&#26041;&#26696;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#25104;&#21151;&#20943;&#23569;&#20154;&#31867;&#23548;&#33322;&#38169;&#35823;&#39640;&#36798;29%&#32780;&#19981;&#22686;&#21152;&#35748;&#30693;&#36127;&#25285;</title><link>https://arxiv.org/abs/2402.16973</link><description>&lt;p&gt;
&#36890;&#36807;&#31361;&#20986;&#28508;&#22312;&#38169;&#35823;&#24182;&#24314;&#35758;&#32416;&#27491;&#25104;&#21151;&#24341;&#23548;&#20154;&#31867;&#20570;&#20986;&#20915;&#31574;&#30340;&#19981;&#23436;&#32654;&#35828;&#26126;
&lt;/p&gt;
&lt;p&gt;
Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16973
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26816;&#27979;&#28508;&#22312;&#24187;&#35273;&#24182;&#24314;&#35758;&#26367;&#20195;&#26041;&#26696;&#30340;&#36890;&#20449;&#26426;&#21046;&#65292;&#25104;&#21151;&#20943;&#23569;&#20154;&#31867;&#23548;&#33322;&#38169;&#35823;&#39640;&#36798;29%&#32780;&#19981;&#22686;&#21152;&#35748;&#30693;&#36127;&#25285;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#21033;&#29992;&#19981;&#23436;&#32654;&#35821;&#35328;&#27169;&#22411;&#26469;&#22312;&#22522;&#20110;&#23450;&#20301;&#23548;&#33322;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#24341;&#23548;&#20154;&#31867;&#20915;&#31574;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#23436;&#32654;&#30340;&#35828;&#26126;&#29983;&#25104;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#26377;&#25928;&#30340;&#36890;&#20449;&#26426;&#21046;&#26469;&#26356;&#25104;&#21151;&#22320;&#24341;&#23548;&#20154;&#31867;&#12290;&#25105;&#20204;&#26500;&#24314;&#30340;&#36890;&#20449;&#26426;&#21046;&#21253;&#25324;&#21487;&#20197;&#26816;&#27979;&#35828;&#26126;&#20013;&#28508;&#22312;&#24187;&#35273;&#24182;&#24314;&#35758;&#23454;&#38469;&#26367;&#20195;&#26041;&#26696;&#30340;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#30452;&#35266;&#30340;&#30028;&#38754;&#23558;&#35813;&#20449;&#24687;&#21576;&#29616;&#32473;&#29992;&#25143;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23558;&#20154;&#31867;&#23548;&#33322;&#38169;&#35823;&#38477;&#20302;&#39640;&#36798;29%&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#35748;&#30693;&#36127;&#25285;&#12290;&#36825;&#19968;&#32467;&#26524;&#31361;&#26174;&#20102;&#23558;&#22810;&#26679;&#21270;&#30340;&#36890;&#20449;&#28192;&#36947;&#25972;&#21512;&#21040;AI&#31995;&#32479;&#20013;&#26469;&#24357;&#34917;&#20854;&#32570;&#38519;&#24182;&#22686;&#24378;&#20854;&#23545;&#20154;&#31867;&#30340;&#23454;&#29992;&#24615;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16973v1 Announce Type: new  Abstract: This paper addresses the challenge of leveraging imperfect language models to guide human decision-making in the context of a grounded navigation task. We show that an imperfect instruction generation model can be complemented with an effective communication mechanism to become more successful at guiding humans. The communication mechanism we build comprises models that can detect potential hallucinations in instructions and suggest practical alternatives, and an intuitive interface to present that information to users. We show that this approach reduces the human navigation error by up to 29% with no additional cognitive burden. This result underscores the potential of integrating diverse communication channels into AI systems to compensate for their imperfections and enhance their utility for humans.
&lt;/p&gt;</description></item><item><title>LangGPT&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#25552;&#31034;&#35774;&#35745;&#26694;&#26550;&#65292;&#20316;&#20026;LLMs&#30340;&#32534;&#31243;&#35821;&#35328;&#65292;&#22823;&#22823;&#22686;&#24378;&#20102;LLMs&#20135;&#29983;&#39640;&#36136;&#37327;&#21709;&#24212;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#24341;&#23548;LLMs&#29983;&#25104;&#39640;&#36136;&#37327;&#25552;&#31034;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16929</link><description>&lt;p&gt;
LangGPT&#65306;&#37325;&#26032;&#24605;&#32771;&#38754;&#21521;LLMs&#30340;&#32467;&#26500;&#21270;&#21487;&#37325;&#22797;&#20351;&#29992;&#25552;&#31034;&#35774;&#35745;&#26694;&#26550;&#20174;&#32534;&#31243;&#35821;&#35328;&#20986;&#21457;
&lt;/p&gt;
&lt;p&gt;
LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16929
&lt;/p&gt;
&lt;p&gt;
LangGPT&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#25552;&#31034;&#35774;&#35745;&#26694;&#26550;&#65292;&#20316;&#20026;LLMs&#30340;&#32534;&#31243;&#35821;&#35328;&#65292;&#22823;&#22823;&#22686;&#24378;&#20102;LLMs&#20135;&#29983;&#39640;&#36136;&#37327;&#21709;&#24212;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#24341;&#23548;LLMs&#29983;&#25104;&#39640;&#36136;&#37327;&#25552;&#31034;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#26377;&#25928;&#25351;&#23548;LLMs&#21046;&#23450;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#23545;&#20110;&#38750;AI&#19987;&#23478;&#26469;&#35828;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#25552;&#31034;&#24037;&#31243;&#30740;&#31350;&#24314;&#35758;&#20102;&#19968;&#20123;&#30053;&#26174;&#38646;&#30862;&#30340;&#20248;&#21270;&#21407;&#21017;&#21644;&#35774;&#35745;&#65292;&#20197;&#21450;&#20973;&#32463;&#39564;&#20381;&#36182;&#30340;&#25552;&#31034;&#20248;&#21270;&#22120;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#21162;&#21147;&#32570;&#20047;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#35774;&#35745;&#27169;&#26495;&#65292;&#23548;&#33268;&#23398;&#20064;&#25104;&#26412;&#39640;&#65292;&#37325;&#22797;&#20351;&#29992;&#24615;&#20302;&#12290;&#21463;&#32467;&#26500;&#21270;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#32534;&#31243;&#35821;&#35328;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LangGPT&#65292;&#20316;&#20026;LLMs&#30340;&#32534;&#31243;&#35821;&#35328;&#30340;&#21452;&#23618;&#25552;&#31034;&#35774;&#35745;&#26694;&#26550;&#12290;LangGPT&#20855;&#26377;&#26131;&#20110;&#23398;&#20064;&#30340;&#35268;&#33539;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25193;&#23637;&#32467;&#26500;&#20197;&#36827;&#34892;&#36801;&#31227;&#21644;&#37325;&#29992;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#22522;&#20934;&#30456;&#27604;&#65292;LangGPT&#26174;&#33879;&#22686;&#24378;&#20102;LLMs&#20135;&#29983;&#39640;&#36136;&#37327;&#21709;&#24212;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;LangGPT&#24050;&#34987;&#35777;&#26126;&#22312;&#24341;&#23548;LLMs&#29983;&#25104;&#39640;&#36136;&#37327;&#25552;&#31034;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16929v1 Announce Type: cross  Abstract: LLMs have demonstrated commendable performance across diverse domains. Nevertheless, formulating high-quality prompts to effectively instruct LLMs poses a challenge for non-AI experts. Existing research in prompt engineering suggests somewhat fragmented optimization principles and designs empirically dependent prompt optimizers. Unfortunately, these endeavors lack a structured design template, incurring high learning costs and resulting in low reusability. Inspired by structured reusable programming languages, we propose LangGPT, a dual-layer prompt design framework as the programming language for LLMs. LangGPT has an easy-to-learn normative structure and provides an extended structure for migration and reuse. Experiments illustrate that LangGPT significantly enhances the capacity of LLMs to produce responses of superior quality compared to baselines. Moreover, LangGPT has proven effective in guiding LLMs to generate high-quality promp
&lt;/p&gt;</description></item><item><title>&#23558;&#24694;&#24847;&#25552;&#31034;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#25552;&#31034;&#20351;&#24471;LLM&#36234;&#29425;&#25915;&#20987;&#26356;&#38590;&#34987;&#26816;&#27979;</title><link>https://arxiv.org/abs/2402.16914</link><description>&lt;p&gt;
DrAttack: &#25552;&#31034;&#20998;&#35299;&#21644;&#37325;&#26500;&#20351;&#24378;&#22823;&#30340;LLM&#36234;&#29425;&#32773;
&lt;/p&gt;
&lt;p&gt;
DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16914
&lt;/p&gt;
&lt;p&gt;
&#23558;&#24694;&#24847;&#25552;&#31034;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#25552;&#31034;&#20351;&#24471;LLM&#36234;&#29425;&#25915;&#20987;&#26356;&#38590;&#34987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#23558;&#24694;&#24847;&#25552;&#31034;&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#25552;&#31034;&#33021;&#22815;&#26377;&#25928;&#27169;&#31946;&#20854;&#28508;&#22312;&#30340;&#24694;&#24847;&#24847;&#22270;&#65292;&#20351;&#20043;&#20197;&#29255;&#27573;&#21270;&#12289;&#19981;&#26131;&#26816;&#27979;&#30340;&#24418;&#24335;&#21576;&#29616;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#36234;&#29425;&#25915;&#20987;&#30340;&#33258;&#21160;&#25552;&#31034;&#20998;&#35299;&#21644;&#37325;&#26500;&#26694;&#26550;&#65288;DrAttack&#65289;&#12290;DrAttack&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;(a) &#23558;&#21407;&#22987;&#25552;&#31034;&#36827;&#34892;&#8220;&#20998;&#35299;&#8221;&#20026;&#23376;&#25552;&#31034;&#65292;(b) &#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#35821;&#20041;&#19978;&#30456;&#20284;&#20294;&#38544;&#21547;&#30340;&#8220;&#37325;&#26500;&#8221;&#36825;&#20123;&#23376;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16914v1 Announce Type: cross  Abstract: The safety alignment of Large Language Models (LLMs) is vulnerable to both manual and automated jailbreak attacks, which adversarially trigger LLMs to output harmful content. However, current methods for jailbreaking LLMs, which nest entire harmful prompts, are not effective at concealing malicious intent and can be easily identified and rejected by well-aligned LLMs. This paper discovers that decomposing a malicious prompt into separated sub-prompts can effectively obscure its underlying malicious intent by presenting it in a fragmented, less detectable form, thereby addressing these limitations. We introduce an automatic prompt \textbf{D}ecomposition and \textbf{R}econstruction framework for jailbreak \textbf{Attack} (DrAttack). DrAttack includes three key components: (a) `Decomposition' of the original prompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly by in-context learning with semantically similar but h
&lt;/p&gt;</description></item><item><title>LDB&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#26469;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;</title><link>https://arxiv.org/abs/2402.16906</link><description>&lt;p&gt;
LDB&#65306;&#36890;&#36807;&#36880;&#27493;&#39564;&#35777;&#36816;&#34892;&#26102;&#25191;&#34892;&#26469;&#35843;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LDB: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16906
&lt;/p&gt;
&lt;p&gt;
LDB&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#26469;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#19981;&#20165;&#23558;&#21333;&#27425;&#20195;&#30721;&#29983;&#25104;&#65292;&#32780;&#19988;&#36824;&#23558;&#21333;&#20803;&#27979;&#35797;&#21644;&#31243;&#24207;&#39564;&#35777;&#22120;&#25972;&#21512;&#21040;LLMs&#20013;&#65292;&#20197;&#36845;&#20195;&#22320;&#23436;&#21892;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#23558;&#29983;&#25104;&#30340;&#31243;&#24207;&#35270;&#20026;&#19981;&#21487;&#20998;&#21106;&#30340;&#23454;&#20307;&#65292;&#36825;&#23545;LLMs&#22312;&#35843;&#35797;&#31243;&#24207;&#26102;&#23384;&#22312;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#24403;&#31243;&#24207;&#21253;&#21547;&#22797;&#26434;&#30340;&#36923;&#36753;&#27969;&#31243;&#21644;&#25968;&#25454;&#25805;&#20316;&#26102;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20154;&#31867;&#24320;&#21457;&#20154;&#21592;&#35843;&#35797;&#31243;&#24207;&#26102;&#65292;&#20182;&#20204;&#36890;&#24120;&#35774;&#32622;&#26029;&#28857;&#24182;&#26377;&#36873;&#25321;&#22320;&#26816;&#26597;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#12290;&#25191;&#34892;&#27969;&#21644;&#20013;&#38388;&#21464;&#37327;&#22312;&#35843;&#35797;&#36807;&#31243;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#28982;&#32780;&#29616;&#26377;&#30340;&#20195;&#30721;&#29983;&#25104;&#25991;&#29486;&#20013;&#26410;&#20805;&#20998;&#21033;&#29992;&#23427;&#20204;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#35797;&#22120;&#65288;LDB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#35843;&#35797;&#26694;&#26550;&#65292;&#21487;&#20197;&#35753;LLMs&#36890;&#36807;&#36816;&#34892;&#26102;&#25191;&#34892;&#20449;&#24687;&#23436;&#21892;&#20854;&#29983;&#25104;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16906v1 Announce Type: cross  Abstract: Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifical
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BESA&#30340;&#26032;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#22359;&#37325;&#26500;&#25439;&#22833;&#65292;&#19982;&#20256;&#32479;&#30340;&#36880;&#23618;&#20462;&#21098;&#25216;&#26415;&#19981;&#21516;&#65292;BESA&#20855;&#26377;&#20248;&#21183;</title><link>https://arxiv.org/abs/2402.16880</link><description>&lt;p&gt;
BESA: &#20351;&#29992;&#20998;&#22359;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#20998;&#37197;&#20462;&#21098;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16880
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;BESA&#30340;&#26032;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20462;&#21098;&#25216;&#26415;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#22359;&#37325;&#26500;&#25439;&#22833;&#65292;&#19982;&#20256;&#32479;&#30340;&#36880;&#23618;&#20462;&#21098;&#25216;&#26415;&#19981;&#21516;&#65292;BESA&#20855;&#26377;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#25688;&#35201;&#12289;&#25991;&#26412;&#38382;&#31572;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#24615;&#33021;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#30001;&#20110;&#22823;&#37327;&#21442;&#25968;&#36896;&#25104;&#30340;&#35745;&#31639;&#21344;&#29992;&#21487;&#33021;&#26159;&#31105;&#38178;&#30340;&#12290;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65288;&#22914;SparseGPT&#21644;Wanda&#65289;&#23581;&#35797;&#36890;&#36807;&#26435;&#37325;&#20462;&#21098;&#32531;&#35299;&#27492;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#36880;&#23618;&#26041;&#27861;&#20250;&#23548;&#33268;&#27169;&#22411;&#36755;&#20986;&#26174;&#33879;&#25200;&#21160;&#65292;&#24182;&#38656;&#35201;&#32454;&#33268;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#22914;&#20462;&#21098;&#36895;&#29575;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLM&#20462;&#21098;&#25216;&#26415;&#65292;&#31216;&#20026;&#20998;&#22359;&#21442;&#25968;&#39640;&#25928;&#31232;&#30095;&#20998;&#37197;&#65288;BESA&#65289;&#65292;&#36890;&#36807;&#24212;&#29992;&#20998;&#22359;&#37325;&#26500;&#25439;&#22833;&#12290;&#19982;&#20856;&#22411;&#30340;&#36880;&#23618;&#20462;&#21098;&#25216;&#26415;&#30456;&#27604;&#65292;BESA&#20855;&#26377;&#20004;&#20010;&#29420;&#29305;&#30340;&#29305;&#28857;&#65306;i&#65289;&#23427;&#23450;&#20301;&#20110;&#25972;&#20307;&#20462;&#21098;&#35823;&#24046;&#30456;&#23545;&#20110;&#27599;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16880v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated outstanding performance in various tasks, such as text summarization, text question-answering, and etc. While their performance is impressive, the computational footprint due to their vast number of parameters can be prohibitive. Existing solutions such as SparseGPT and Wanda attempt to alleviate this issue through weight pruning. However, their layer-wise approach results in significant perturbation to the model's output and requires meticulous hyperparameter tuning, such as the pruning rate, which can adversely affect overall model performance. To address this, this paper introduces a novel LLM pruning technique dubbed blockwise parameter-efficient sparsity allocation (BESA) by applying a blockwise reconstruction loss. In contrast to the typical layer-wise pruning techniques, BESA is characterized by two distinctive attributes: i) it targets the overall pruning error with respect to indi
&lt;/p&gt;</description></item><item><title>EvoGPT-f&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#20116;&#20010;&#24418;&#24335;&#25968;&#23398;&#35821;&#26009;&#24211;&#36827;&#34892;&#24046;&#24322;&#26426;&#22120;&#21487;&#23398;&#20064;&#24615;&#30340;&#31995;&#32479;&#37327;&#21270;&#20998;&#26512;&#65292;&#20026;&#24418;&#24335;&#25968;&#23398;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16878</link><description>&lt;p&gt;
EvoGPT-f: &#19968;&#31181;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#24418;&#24335;&#25968;&#23398;&#35821;&#35328;&#30340;&#36827;&#21270;GPT&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EvoGPT-f: An Evolutionary GPT Framework for Benchmarking Formal Math Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16878
&lt;/p&gt;
&lt;p&gt;
EvoGPT-f&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#20116;&#20010;&#24418;&#24335;&#25968;&#23398;&#35821;&#26009;&#24211;&#36827;&#34892;&#24046;&#24322;&#26426;&#22120;&#21487;&#23398;&#20064;&#24615;&#30340;&#31995;&#32479;&#37327;&#21270;&#20998;&#26512;&#65292;&#20026;&#24418;&#24335;&#25968;&#23398;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16878v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#24418;&#24335;&#25968;&#23398;&#26159;&#23558;&#25968;&#23398;&#36716;&#21270;&#20026;&#32534;&#31243;&#35821;&#35328;&#30340;&#23398;&#31185;&#65292;&#22312;&#36825;&#31181;&#32534;&#31243;&#35821;&#35328;&#20013;&#65292;&#20219;&#20309;&#38472;&#36848;&#37117;&#21487;&#20197;&#34987;&#35745;&#31639;&#26426;&#26126;&#30830;&#22320;&#26816;&#26597;&#12290;&#25968;&#23398;&#23478;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#33457;&#36153;&#20102;&#25968;&#21313;&#24180;&#36827;&#34892;&#33392;&#33510;&#30340;&#24418;&#24335;&#21270;&#24037;&#20316;&#65292;&#24320;&#21457;&#20102;&#35832;&#22914;Coq&#12289;HOL&#21644;Lean&#31561;&#35821;&#35328;&#12290;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#24050;&#32463;&#27719;&#38598;&#21040;&#36825;&#20123;&#24418;&#24335;&#21270;&#25968;&#23398;&#35821;&#26009;&#24211;&#19978;&#65292;&#24182;&#20135;&#29983;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#24110;&#21161;&#20132;&#20114;&#24335;&#21644;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35770;&#25991;&#20027;&#35201;&#38598;&#20013;&#22312;&#19968;&#20010;&#26041;&#27861;&#12289;&#19968;&#20010;&#35777;&#26126;&#20219;&#21153;&#12289;&#19968;&#20010;&#35821;&#35328;&#19978;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EvoGPT-f: &#19968;&#31181;&#26032;&#39062;&#30340;&#36827;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#39318;&#27425;&#31995;&#32479;&#37327;&#21270;&#20998;&#26512;&#20116;&#20010;&#24418;&#24335;&#25968;&#23398;&#35821;&#26009;&#24211;(Lean 3&#12289;Lean 4&#12289;Coq&#12289;HOL 4&#12289;HOL Light)&#30340;&#24046;&#24322;&#26426;&#22120;&#21487;&#23398;&#20064;&#24615;&#65292;&#20351;&#29992;&#22235;&#31181;&#35760;&#21495;&#21270;&#26041;&#27861;(&#23383;&#31526;&#12289;&#21333;&#35789;&#32423;&#12289;&#23383;&#33410;&#23545;&#32534;&#30721;&#21644;StarCoder&#35760;&#21495;&#21270;&#22120;)&#12290;&#26412;&#25991;&#24182;&#26410;&#32467;&#26463;&#20851;&#20110;&#8220;&#26368;&#20339;&#8221;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16878v1 Announce Type: new  Abstract: Formal mathematics is the discipline of translating mathematics into a programming language in which any statement can be unequivocally checked by a computer. Mathematicians and computer scientists have spent decades of painstaking formalization efforts developing languages such as Coq, HOL, and Lean. Machine learning research has converged on these formal math corpora and given rise to an assortment of methodologies to aid in interactive and automated theorem proving. However, these papers have primarily focused on one method, for one proof task, in one language. This paper introduces EvoGPT-f: a novel evolutionary framework for the first systematic quantitative analysis of the differential machine learnability of five formal math corpora (Lean 3, Lean 4, Coq, HOL 4, HOL Light) using four tokenization methods (character, word-level, Byte Pair Encoding and StarCoder tokenizer). This paper does not put to rest the question of the "best" o
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#29983;&#25104;&#33021;&#21147;&#26469;&#21512;&#25104;&#20551;&#35774;&#32451;&#20064;&#65292;&#20197;&#24357;&#21512;&#23398;&#20064;&#32773;&#38656;&#27714;&#19982;&#32451;&#20064;&#20869;&#23481;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#65292;&#25552;&#39640;&#20010;&#24615;&#21270;&#35821;&#35328;&#23398;&#20064;&#32451;&#20064;&#26816;&#32034;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.16877</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#23398;&#20064;&#32451;&#20064;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Augmented Exercise Retrieval for Personalized Language Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16877
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21033;&#29992;&#29983;&#25104;&#33021;&#21147;&#26469;&#21512;&#25104;&#20551;&#35774;&#32451;&#20064;&#65292;&#20197;&#24357;&#21512;&#23398;&#20064;&#32773;&#38656;&#27714;&#19982;&#32451;&#20064;&#20869;&#23481;&#20043;&#38388;&#30340;&#35821;&#20041;&#40511;&#27807;&#65292;&#25552;&#39640;&#20010;&#24615;&#21270;&#35821;&#35328;&#23398;&#20064;&#32451;&#20064;&#26816;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#35821;&#35328;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#38646;&#26679;&#26412;&#32451;&#20064;&#26816;&#32034;&#38382;&#39064;&#65292;&#20197;&#36171;&#20104;&#23398;&#20064;&#32773;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#26126;&#30830;&#35831;&#27714;&#20010;&#24615;&#21270;&#32451;&#20064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#25910;&#38598;&#33258;&#35821;&#35328;&#23398;&#20064;&#32773;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#30690;&#37327;&#30456;&#20284;&#24615;&#26041;&#27861;&#24456;&#38590;&#25429;&#25417;&#32451;&#20064;&#20869;&#23481;&#19982;&#23398;&#20064;&#32773;&#29992;&#20110;&#34920;&#36798;&#20182;&#20204;&#24819;&#35201;&#23398;&#20064;&#20869;&#23481;&#30340;&#35821;&#35328;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#36890;&#36807;&#22522;&#20110;&#23398;&#20064;&#32773;&#36755;&#20837;&#21512;&#25104;&#20551;&#35774;&#32451;&#20064;&#65292;&#28982;&#21518;&#29992;&#20110;&#25628;&#32034;&#30456;&#20851;&#32451;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;mHyER&#20811;&#26381;&#20102;&#19977;&#20010;&#25361;&#25112;&#65306;&#65288;1&#65289;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#30340;&#30456;&#20851;&#24615;&#26631;&#31614;&#65292;&#65288;2&#65289;&#21463;&#38480;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16877v1 Announce Type: cross  Abstract: We study the problem of zero-shot exercise retrieval in the context of online language learning, to give learners the ability to explicitly request personalized exercises via natural language. Using real-world data collected from language learners, we observe that vector similarity approaches poorly capture the relationship between exercise content and the language that learners use to express what they want to learn. This semantic gap between queries and content dramatically reduces the effectiveness of general-purpose retrieval models pretrained on large scale information retrieval datasets like MS MARCO. We leverage the generative capabilities of large language models to bridge the gap by synthesizing hypothetical exercises based on the learner's input, which are then used to search for relevant exercises. Our approach, which we call mHyER, overcomes three challenges: (1) lack of relevance labels for training, (2) unrestricted learn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#35299;&#20915;&#20102;&#35821;&#35328;&#29983;&#25104;&#20013;&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#24182;&#20511;&#21161;&#26597;&#35810;&#20248;&#21270;&#36807;&#31243;&#23558;&#29992;&#25143;&#26597;&#35810;&#19982;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#36830;&#25509;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.16874</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#26597;&#35810;&#25552;&#21319;&#35821;&#35328;&#29983;&#25104;&#30340;&#26816;&#32034;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Enhancing Retrieval Processes for Language Generation with Augmented Queries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#35299;&#20915;&#20102;&#35821;&#35328;&#29983;&#25104;&#20013;&#8220;&#24187;&#35273;&#8221;&#38382;&#39064;&#65292;&#24182;&#20511;&#21161;&#26597;&#35810;&#20248;&#21270;&#36807;&#31243;&#23558;&#29992;&#25143;&#26597;&#35810;&#19982;&#39640;&#32423;&#35821;&#35328;&#27169;&#22411;&#36830;&#25509;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#25216;&#26415;&#26085;&#26032;&#26376;&#24322;&#30340;&#19990;&#30028;&#20013;&#65292;&#30001;&#20110;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#25628;&#32034;&#25991;&#26723;&#21464;&#24471;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#38754;&#20020;&#22256;&#38590;&#65292;&#27604;&#22914;&#25552;&#20379;&#19981;&#20934;&#30830;&#30340;&#20449;&#24687;&#65292;&#36890;&#24120;&#34987;&#31216;&#20026;&#8220;&#24187;&#35273;&#8221;&#12290;&#26412;&#30740;&#31350;&#33268;&#21147;&#20110;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#25216;&#26415;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#35813;&#25216;&#26415;&#25351;&#23548;&#27169;&#22411;&#22522;&#20110;&#30495;&#23454;&#20107;&#23454;&#25552;&#20379;&#20934;&#30830;&#31572;&#22797;&#12290;&#20026;&#20102;&#35299;&#20915;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#65292;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#29992;&#25143;&#26597;&#35810;&#19982;&#35832;&#22914;BERT&#21644;Orca2&#31561;&#22797;&#26434;&#35821;&#35328;&#27169;&#22411;&#36830;&#25509;&#36215;&#26469;&#30340;&#21019;&#26032;&#26597;&#35810;&#20248;&#21270;&#36807;&#31243;&#12290;&#30740;&#31350;&#23637;&#24320;&#22312;&#19977;&#31181;&#24773;&#22659;&#20013;&#65306;&#39318;&#20808;&#65292;&#27809;&#26377;RAG&#65292;&#20854;&#27425;&#65292;&#27809;&#26377;&#39069;&#22806;&#24110;&#21161;&#65292;&#26368;&#21518;&#65292;&#21152;&#20837;&#39069;&#22806;&#24110;&#21161;&#12290;&#36873;&#25321;&#32039;&#20945;&#32780;&#39640;&#25928;&#30340;Orca2 7B&#27169;&#22411;&#23637;&#31034;&#20986;&#23545;&#35745;&#31639;&#36164;&#28304;&#30340;&#26234;&#33021;&#20351;&#29992;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#21021;&#22987;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26377;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16874v1 Announce Type: cross  Abstract: In the rapidly changing world of smart technology, searching for documents has become more challenging due to the rise of advanced language models. These models sometimes face difficulties, like providing inaccurate information, commonly known as "hallucination." This research focuses on addressing this issue through Retrieval-Augmented Generation (RAG), a technique that guides models to give accurate responses based on real facts. To overcome scalability issues, the study explores connecting user queries with sophisticated language models such as BERT and Orca2, using an innovative query optimization process. The study unfolds in three scenarios: first, without RAG, second, without additional assistance, and finally, with extra help. Choosing the compact yet efficient Orca2 7B model demonstrates a smart use of computing resources. The empirical results indicate a significant improvement in the initial language model's performance unde
&lt;/p&gt;</description></item><item><title>&#23558;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#32479;&#19968;&#25551;&#36848;&#20026;&#35745;&#31639;&#22270;&#65292;&#25552;&#20986;&#26032;&#39062;&#30340;&#33258;&#21160;&#22270;&#20248;&#21270;&#22120;&#26469;&#25913;&#36827;&#33410;&#28857;&#21644;&#36793;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#21160;&#21327;&#20316;&#21644;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.16823</link><description>&lt;p&gt;
&#20316;&#20026;&#21487;&#20248;&#21270;&#22270;&#30340;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Language Agents as Optimizable Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16823
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#32479;&#19968;&#25551;&#36848;&#20026;&#35745;&#31639;&#22270;&#65292;&#25552;&#20986;&#26032;&#39062;&#30340;&#33258;&#21160;&#22270;&#20248;&#21270;&#22120;&#26469;&#25913;&#36827;&#33410;&#28857;&#21644;&#36793;&#65292;&#23454;&#29616;&#20102;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#21160;&#21327;&#20316;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31181;&#20154;&#31867;&#35774;&#35745;&#30340;&#25552;&#21319;&#25216;&#26415;&#34987;&#25552;&#20986;&#65292;&#29992;&#20110;&#25913;&#36827;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#38382;&#39064;&#27714;&#35299;&#22120;&#65292;&#20135;&#29983;&#20102;&#35768;&#22810;&#19981;&#21516;&#30340;&#20195;&#30721;&#24211;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;LLM&#20195;&#29702;&#25551;&#36848;&#20026;&#35745;&#31639;&#22270;&#26469;&#32479;&#19968;&#36825;&#20123;&#26041;&#27861;&#12290;&#33410;&#28857;&#23454;&#29616;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#25110;&#26597;&#35810;LLMs&#30340;&#21151;&#33021;&#65292;&#24182;&#19988;&#36793;&#25551;&#36848;&#25805;&#20316;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;&#22270;&#24418;&#21487;&#20197;&#36882;&#24402;&#22320;&#32452;&#21512;&#25104;&#20195;&#34920;&#19981;&#21516;&#20195;&#29702;&#20043;&#38388;&#21327;&#20316;&#23618;&#27425;&#30340;&#26356;&#22823;&#32452;&#21512;&#22270;&#65288;&#20854;&#20013;&#36793;&#36830;&#25509;&#19981;&#21516;&#20195;&#29702;&#30340;&#25805;&#20316;&#65289;&#12290;&#25105;&#20204;&#30340;&#26032;&#39062;&#33258;&#21160;&#22270;&#20248;&#21270;&#22120;&#65288;1&#65289;&#20248;&#21270;&#33410;&#28857;&#32423;LLM&#25552;&#31034;&#65288;&#33410;&#28857;&#20248;&#21270;&#65289;&#24182;&#65288;2&#65289;&#36890;&#36807;&#25913;&#21464;&#22270;&#36830;&#25509;&#24615;&#26469;&#25913;&#21892;&#20195;&#29702;&#21327;&#35843;&#65288;&#36793;&#32536;&#20248;&#21270;&#65289;&#12290;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#29992;&#20110;&#39640;&#25928;&#24320;&#21457;&#12289;&#38598;&#25104;&#21644;&#33258;&#21160;&#25913;&#36827;&#21508;&#31181;LLM&#20195;&#29702;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/metauto-ai/gptswarm&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16823v1 Announce Type: cross  Abstract: Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases. We unify these approaches by describing LLM-based agents as computational graphs. The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations. Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents. The code can be found at https://github.com/metauto-ai/gptswarm.
&lt;/p&gt;</description></item><item><title>Nemotron-4 15B&#26159;&#19968;&#20010;150&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22810;&#35821;&#35328;&#33021;&#21147;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20854;&#20182;&#35268;&#27169;&#30456;&#20284;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.16819</link><description>&lt;p&gt;
Nemotron-4 15B&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Nemotron-4 15B Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16819
&lt;/p&gt;
&lt;p&gt;
Nemotron-4 15B&#26159;&#19968;&#20010;150&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#22810;&#35821;&#35328;&#33021;&#21147;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36807;&#20854;&#20182;&#35268;&#27169;&#30456;&#20284;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Nemotron-4 15B&#65292;&#36825;&#26159;&#19968;&#20010;&#25317;&#26377;150&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20102;8000&#19975;&#20159;&#20010;&#25991;&#26412;&#26631;&#35760;&#12290;Nemotron-4 15B&#22312;&#33521;&#35821;&#12289;&#22810;&#35821;&#35328;&#21644;&#32534;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65306;&#22312;7&#20010;&#19979;&#28216;&#35780;&#20272;&#39046;&#22495;&#20013;&#65292;&#23427;&#22312;4&#20010;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#20854;&#20313;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#25152;&#26377;&#29616;&#26377;&#35268;&#27169;&#30456;&#20284;&#30340;&#24320;&#25918;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Nemotron-4 15B&#23637;&#29616;&#20986;&#20102;&#25152;&#26377;&#35268;&#27169;&#30456;&#20284;&#27169;&#22411;&#20013;&#26368;&#24378;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#22810;&#35821;&#35328;&#20219;&#21153;&#19978;&#20248;&#20110;&#22235;&#20493;&#20197;&#19978;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#20197;&#21450;&#19987;&#38376;&#29992;&#20110;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16819v1 Announce Type: new  Abstract: We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual language model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates strong performance when assessed on English, multilingual, and coding tasks: it outperforms all existing similarly-sized open models on 4 out of 7 downstream evaluation areas and achieves competitive performance to the leading open models in the remaining ones. Specifically, Nemotron-4 15B exhibits the best multilingual capabilities of all similarly-sized models, even outperforming models over four times larger and those explicitly specialized for multilingual tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ID-XCB&#65292;&#36825;&#26159;&#39318;&#20010;&#25968;&#25454;&#26080;&#20851;&#21435;&#20559;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#32531;&#35299;&#27169;&#22411;&#23545;&#24341;&#21457;&#20559;&#35265;&#30340;&#35789;&#30340;&#20851;&#27880;&#30340;&#21516;&#26102;&#25552;&#39640;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21435;&#20559;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16458</link><description>&lt;p&gt;
D-XCB&#65306;&#25968;&#25454;&#26080;&#20851;&#21435;&#20559;&#26041;&#27861;&#65292;&#29992;&#20110;&#20844;&#24179;&#20934;&#30830;&#22522;&#20110;Transformer&#30340;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
D-XCB: Data-independent Debiasing for Fair and Accurate Transformer-based Cyberbullying Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16458
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ID-XCB&#65292;&#36825;&#26159;&#39318;&#20010;&#25968;&#25454;&#26080;&#20851;&#21435;&#20559;&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#32531;&#35299;&#27169;&#22411;&#23545;&#24341;&#21457;&#20559;&#35265;&#30340;&#35789;&#30340;&#20851;&#27880;&#30340;&#21516;&#26102;&#25552;&#39640;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21435;&#20559;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39554;&#20154;&#35805;&#26159;&#25910;&#38598;&#21253;&#21547;&#32593;&#32476;&#27450;&#20940;&#20107;&#20214;&#25968;&#25454;&#30340;&#24120;&#29992;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#34913;&#37327;&#21644;&#20943;&#36731;&#30001;&#39554;&#20154;&#35805;&#21644;&#22240;&#27492;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#23548;&#33268;&#20107;&#20214;&#20043;&#38388;&#30340;&#34394;&#20551;&#20851;&#32852;&#20135;&#29983;&#30340;&#20559;&#35265;&#12290;&#22312;&#28436;&#31034;&#21644;&#37327;&#21270;&#36825;&#20123;&#20559;&#35265;&#20043;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ID-XCB&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#25968;&#25454;&#26080;&#20851;&#21435;&#20559;&#25216;&#26415;&#65292;&#32467;&#21512;&#20102;&#25932;&#23545;&#35757;&#32451;&#12289;&#20559;&#35265;&#32422;&#26463;&#21644;&#21435;&#20559;&#24494;&#35843;&#26041;&#27861;&#65292;&#26088;&#22312;&#32531;&#35299;&#27169;&#22411;&#23545;&#24341;&#21457;&#20559;&#35265;&#30340;&#35789;&#30340;&#20851;&#27880;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#25972;&#20307;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;&#22522;&#20110;&#20250;&#35805;&#30340;&#32593;&#32476;&#27450;&#20940;&#25968;&#25454;&#38598;&#19978;&#25506;&#35752;&#20102;ID-XCB&#65292;&#21516;&#26102;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#28040;&#34701;&#21644;&#27867;&#21270;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ID-XCB&#23398;&#20064;&#20102;&#24378;&#22823;&#30340;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#33021;&#21147;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#20559;&#35265;&#65292;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#21435;&#20559;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#20559;&#35265;&#32531;&#35299;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#34920;&#26126;&#20854;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16458v1 Announce Type: new  Abstract: Swear words are a common proxy to collect datasets with cyberbullying incidents. Our focus is on measuring and mitigating biases derived from spurious associations between swear words and incidents occurring as a result of such data collection strategies. After demonstrating and quantifying these biases, we introduce ID-XCB, the first data-independent debiasing technique that combines adversarial training, bias constraints and debias fine-tuning approach aimed at alleviating model attention to bias-inducing words without impacting overall model performance. We explore ID-XCB on two popular session-based cyberbullying datasets along with comprehensive ablation and generalisation studies. We show that ID-XCB learns robust cyberbullying detection capabilities while mitigating biases, outperforming state-of-the-art debiasing methods in both performance and bias mitigation. Our quantitative and qualitative analyses demonstrate its generalisab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#23558;&#28304;&#39046;&#22495;&#21477;&#27861;&#35268;&#21017;&#19982;&#30446;&#26631;&#39046;&#22495;&#21477;&#23376;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#21477;&#24335;&#32467;&#26500;&#35299;&#26512;&#22120;&#23545;&#21508;&#31181;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#25945;&#31185;&#20070;&#21644;&#26032;&#38395;&#39046;&#22495;&#30340;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#22522;&#20934;&#27169;&#22411;1.68&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.16311</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#30340;&#20013;&#25991;&#21477;&#24335;&#32467;&#26500;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Chinese Sentence Pattern Parsing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#23558;&#28304;&#39046;&#22495;&#21477;&#27861;&#35268;&#21017;&#19982;&#30446;&#26631;&#39046;&#22495;&#21477;&#23376;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#21477;&#24335;&#32467;&#26500;&#35299;&#26512;&#22120;&#23545;&#21508;&#31181;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#25945;&#31185;&#20070;&#21644;&#26032;&#38395;&#39046;&#22495;&#30340;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#22522;&#20934;&#27169;&#22411;1.68&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16311v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#21477;&#24335;&#32467;&#26500;&#65288;SPS&#65289;&#35299;&#26512;&#26159;&#19968;&#31181;&#20027;&#35201;&#29992;&#20110;&#35821;&#35328;&#25945;&#23398;&#30340;&#21477;&#27861;&#20998;&#26512;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;SPS&#35299;&#26512;&#22120;&#20027;&#35201;&#20381;&#36182;&#20110;&#25945;&#31185;&#20070;&#35821;&#26009;&#24211;&#36827;&#34892;&#35757;&#32451;&#65292;&#32570;&#20047;&#36328;&#39046;&#22495;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#25105;&#35757;&#32451;&#26694;&#26550;&#20869;&#12290;&#20174;&#28304;&#39046;&#22495;&#20013;&#25552;&#21462;&#37096;&#20998;&#21477;&#27861;&#35268;&#21017;&#65292;&#19982;&#30446;&#26631;&#39046;&#22495;&#21477;&#23376;&#32467;&#21512;&#21160;&#24577;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#22686;&#24378;&#20102;&#35299;&#26512;&#22120;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#22312;&#25945;&#31185;&#20070;&#21644;&#26032;&#38395;&#39046;&#22495;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#65292;F1&#25351;&#26631;&#27604;&#22522;&#20110;&#35268;&#21017;&#30340;&#22522;&#20934;&#27169;&#22411;&#39640;&#20986;1.68&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16311v1 Announce Type: cross  Abstract: Sentence Pattern Structure (SPS) parsing is a syntactic analysis method primarily employed in language teaching.Existing SPS parsers rely heavily on textbook corpora for training, lacking cross-domain capability.To overcome this constraint, this paper proposes an innovative approach leveraging large language models (LLMs) within a self-training framework. Partial syntactic rules from a source domain are combined with target domain sentences to dynamically generate training data, enhancing the adaptability of the parser to diverse domains.Experiments conducted on textbook and news domains demonstrate the effectiveness of the proposed method, outperforming rule-based baselines by 1.68 points on F1 metrics.
&lt;/p&gt;</description></item><item><title>FuseChat&#36890;&#36807;&#30693;&#35782;&#34701;&#21512;&#23558;&#22810;&#20010;&#23545;&#35805;&#27169;&#22411;&#30340;&#38598;&#20307;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#39044;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2402.16107</link><description>&lt;p&gt;
FuseChat&#65306;&#23545;&#35805;&#27169;&#22411;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
FuseChat: Knowledge Fusion of Chat Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16107
&lt;/p&gt;
&lt;p&gt;
FuseChat&#36890;&#36807;&#30693;&#35782;&#34701;&#21512;&#23558;&#22810;&#20010;&#23545;&#35805;&#27169;&#22411;&#30340;&#38598;&#20307;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#39044;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30830;&#23454;&#21487;&#20197;&#23548;&#33268;&#20855;&#26377;&#29420;&#29305;&#33021;&#21147;&#21644;&#20248;&#21183;&#30340;&#27169;&#22411;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#20250;&#20135;&#29983;&#24040;&#22823;&#25104;&#26412;&#65292;&#24182;&#21487;&#33021;&#23548;&#33268;&#31454;&#20105;&#33021;&#21147;&#30340;&#28508;&#22312;&#20887;&#20313;&#12290;&#19968;&#31181;&#26367;&#20195;&#31574;&#30053;&#26159;&#23558;&#29616;&#26377;&#30340;LLMs&#32452;&#21512;&#25104;&#26356;&#24378;&#22823;&#30340;LLM&#65292;&#20174;&#32780;&#20943;&#23569;&#26114;&#36149;&#30340;&#39044;&#35757;&#32451;&#30340;&#24517;&#35201;&#24615;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;LLMs&#30340;&#22810;&#26679;&#21270;&#26550;&#26500;&#65292;&#30452;&#25509;&#21442;&#25968;&#34701;&#21512;&#34987;&#35777;&#26126;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#26368;&#36817;&#65292;FuseLLM&#24341;&#20837;&#20102;&#30693;&#35782;&#34701;&#21512;&#30340;&#27010;&#24565;&#65292;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#25345;&#32493;&#35757;&#32451;&#23558;&#22810;&#20010;&#32467;&#26500;&#22810;&#26679;&#30340;LLM&#30340;&#38598;&#20307;&#30693;&#35782;&#36716;&#31227;&#33267;&#30446;&#26631;LLM&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;FuseLLM&#26694;&#26550;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#28789;&#27963;&#24615;&#65292;&#23454;&#29616;&#20102;&#23545;&#35805;LLM&#30340;&#34701;&#21512;&#65292;&#29983;&#25104;&#20102;FuseChat&#12290;FuseChat&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#32467;&#26500;&#21644;&#35268;&#27169;&#19981;&#21516;&#30340;&#28304;LLMs&#36827;&#34892;&#30693;&#35782;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16107v1 Announce Type: new  Abstract: While training large language models (LLMs) from scratch can indeed lead to models with distinct capabilities and strengths, this approach incurs substantial costs and may lead to potential redundancy in competencies. An alternative strategy is to combine existing LLMs into a more robust LLM, thereby diminishing the necessity for expensive pre-training. However, due to the diverse architectures of LLMs, direct parameter blending proves to be unfeasible. Recently, \textsc{FuseLLM} introduced the concept of knowledge fusion to transfer the collective knowledge of multiple structurally varied LLMs into a target LLM through lightweight continual training. In this report, we extend the scalability and flexibility of the \textsc{FuseLLM} framework to realize the fusion of chat LLMs, resulting in \textsc{FuseChat}. \textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge fusion for structurally and scale-varied source LLMs t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#26816;&#32034;&#27169;&#22359;&#25628;&#32034;&#25903;&#25345;&#25991;&#26723;&#26469;&#35299;&#20915;&#24187;&#35273;&#20869;&#23481;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16063</link><description>&lt;p&gt;
&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Citation-Enhanced Generation for LLM-based Chatbot
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16063
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#22686;&#24378;&#30340;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#29983;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#26816;&#32034;&#27169;&#22359;&#25628;&#32034;&#25903;&#25345;&#25991;&#26723;&#26469;&#35299;&#20915;&#24187;&#35273;&#20869;&#23481;&#20135;&#29983;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#36890;&#29992;&#26234;&#33021;&#65292;&#21253;&#25324;&#23558;&#23427;&#20204;&#38598;&#25104;&#21040;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38754;&#20020;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#22312;&#22238;&#22797;&#20013;&#21487;&#33021;&#20135;&#29983;&#34394;&#26500;&#20869;&#23481;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#32493;&#24341;&#29992;&#22686;&#24378;&#29983;&#25104;&#65288;CEG&#65289;&#26041;&#27861;&#65292;&#32467;&#21512;&#26816;&#32034;&#35770;&#35777;&#12290;&#19982;&#20808;&#21069;&#20391;&#37325;&#20110;&#39044;&#38450;&#29983;&#25104;&#36807;&#31243;&#20013;&#24187;&#35273;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#21518;&#32493;&#26041;&#24335;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#23427;&#32467;&#21512;&#20102;&#19968;&#20010;&#26816;&#32034;&#27169;&#22359;&#26469;&#25628;&#32034;&#19982;&#29983;&#25104;&#20869;&#23481;&#30456;&#20851;&#30340;&#25903;&#25345;&#25991;&#26723;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16063v1 Announce Type: cross  Abstract: Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc \textbf{C}itation-\textbf{E}nhanced \textbf{G}eneration (\textbf{CEG}) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-ba
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;EHRNoteQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20855;&#26377;&#37319;&#29992;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#26684;&#24335;&#21644;&#38656;&#35201;&#20998;&#26512;&#22810;&#31687;&#20020;&#24202;&#31508;&#35760;&#30340;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.16040</link><description>&lt;p&gt;
EHRNoteQA&#65306;&#29992;&#20110;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
EHRNoteQA: A Patient-Specific Question Answering Benchmark for Evaluating Large Language Models in Clinical Settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16040
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;EHRNoteQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20855;&#26377;&#37319;&#29992;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#26684;&#24335;&#21644;&#38656;&#35201;&#20998;&#26512;&#22810;&#31687;&#20020;&#24202;&#31508;&#35760;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;EHRNoteQA&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#24739;&#32773;&#29305;&#23450;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#22312;MIMIC-IV&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#30001;&#19977;&#20301;&#21307;&#30103;&#19987;&#23478;&#22242;&#38431;&#31934;&#24515;&#31574;&#21010;&#20102;&#21253;&#21547;962&#20010;&#29420;&#29305;&#38382;&#39064;&#30340;&#25968;&#25454;&#38598;&#65292;&#27599;&#20010;&#38382;&#39064;&#37117;&#19982;&#29305;&#23450;&#24739;&#32773;&#30340;EHR&#20020;&#24202;&#31508;&#35760;&#30456;&#20851;&#32852;&#12290;&#19982;&#29616;&#26377;&#22522;&#20110;EHR&#30340;&#22522;&#20934;&#19981;&#21516;&#30340;&#26159;&#65306;&#39318;&#20808;&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#37319;&#29992;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#26684;&#24335;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#22312;&#33258;&#21160;&#35780;&#20272;&#30340;&#32972;&#26223;&#19979;&#26377;&#25928;&#35780;&#20272;LLMs&#30340;&#24471;&#20998;&#24615;&#33021;&#65292;&#19982;&#20854;&#20182;&#26684;&#24335;&#30456;&#27604;&#12290;&#20854;&#27425;&#65292;&#23427;&#38656;&#35201;&#20998;&#26512;&#22810;&#31687;&#20020;&#24202;&#31508;&#35760;&#25165;&#33021;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65292;&#21453;&#26144;&#20102;&#23454;&#38469;&#20020;&#24202;&#20915;&#31574;&#21046;&#23450;&#30340;&#22797;&#26434;&#24615;&#65292;&#21307;&#29983;&#38656;&#35201;&#23457;&#26597;&#22823;&#37327;&#24739;&#32773;&#30149;&#21490;&#35760;&#24405;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16040v1 Announce Type: new  Abstract: This study introduces EHRNoteQA, a novel patient-specific question answering benchmark tailored for evaluating Large Language Models (LLMs) in clinical environments. Based on MIMIC-IV Electronic Health Record (EHR), a team of three medical professionals has curated the dataset comprising 962 unique questions, each linked to a specific patient's EHR clinical notes. What makes EHRNoteQA distinct from existing EHR-based benchmarks is as follows: Firstly, it is the first dataset to adopt a multi-choice question answering format, a design choice that effectively evaluates LLMs with reliable scores in the context of automatic evaluation, compared to other formats. Secondly, it requires an analysis of multiple clinical notes to answer a single question, reflecting the complex nature of real-world clinical decision-making where clinicians review extensive records of patient histories. Our comprehensive evaluation on various large language models
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#23436;&#21892;&#21644;&#26684;&#24335;&#21270;&#25913;&#36827;LMs&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#38750;&#23433;&#20840;&#23545;&#40784;&#30340;LMs&#20013;&#20063;&#20855;&#26377;&#20986;&#33394;&#30340;&#23433;&#20840;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.15180</link><description>&lt;p&gt;
&#25171;&#30772;Breakout: &#29992;&#33258;&#25105;&#23436;&#21892;&#37325;&#26032;&#23450;&#20041;LM&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks with Self-Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15180
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#23436;&#21892;&#21644;&#26684;&#24335;&#21270;&#25913;&#36827;LMs&#23545;&#25239;&#36234;&#29425;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#38750;&#23433;&#20840;&#23545;&#40784;&#30340;LMs&#20013;&#20063;&#20855;&#26377;&#20986;&#33394;&#30340;&#23433;&#20840;&#24615;&#65292;&#21516;&#26102;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35686;&#21578;&#65306;&#26412;&#25991;&#21253;&#21547;&#21487;&#33021;&#24341;&#36215;&#19981;&#24555;&#30340;&#20882;&#29359;&#24615;&#35789;&#35821;&#12290;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#23481;&#26131;&#34987;&#21033;&#29992;&#36827;&#34892;&#24694;&#24847;&#28389;&#29992;&#12290;&#23545;LM&#36827;&#34892;&#23433;&#20840;&#23545;&#40784;&#30340;&#35757;&#32451;&#38750;&#24120;&#22797;&#26434;&#65292;&#20351;&#24471;&#38590;&#20197;&#31435;&#21363;&#24212;&#23545;&#24555;&#36895;&#21457;&#23637;&#30340;&#25915;&#20987;&#65292;&#22914;&#36234;&#29425;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26684;&#24335;&#33258;&#25105;&#23436;&#21892;&#30340;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#38750;&#23433;&#20840;&#23545;&#40784;&#30340;LMs&#20013;&#20063;&#33021;&#23454;&#29616;&#20986;&#33394;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#31181;&#38450;&#24481;&#22522;&#32447;&#36827;&#34892;&#35780;&#20272;&#65292;&#34920;&#26126;&#36825;&#26159;&#38024;&#23545;&#36234;&#29425;&#25915;&#20987;&#26368;&#23433;&#20840;&#30340;&#26080;&#35757;&#32451;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#33258;&#25105;&#23436;&#21892;&#36807;&#31243;&#25928;&#29575;&#30340;&#26684;&#24335;&#21270;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#36739;&#23569;&#36845;&#20195;&#20013;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#38750;&#23433;&#20840;&#23545;&#40784;&#30340;LM&#22312;&#23433;&#20840;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#23433;&#20840;&#23545;&#40784;&#30340;LM&#65292;&#22240;&#20026;&#23427;&#20204;&#32473;&#20986;&#26356;&#26377;&#29992;&#19988;&#26356;&#23433;&#20840;&#30340;&#22238;&#22797;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#33021;&#22815;&#22312;&#36739;&#23569;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#23454;&#29616;&#26356;&#23569;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15180v1 Announce Type: cross  Abstract: Caution: This paper includes offensive words that could potentially cause unpleasantness. Language models (LMs) are vulnerable to exploitation for adversarial misuse. Training LMs for safety alignment is extensive and makes it hard to respond to fast-developing attacks immediately, such as jailbreaks. We propose self-refine with formatting that achieves outstanding safety even in non-safety-aligned LMs and evaluate our method alongside several defense baselines, demonstrating that it is the safest training-free method against jailbreak attacks. Additionally, we proposed a formatting method that improves the efficiency of the self-refine process while reducing attack success rates in fewer iterations. We've also observed that non-safety-aligned LMs outperform safety-aligned LMs in safety tasks by giving more helpful and safe responses. In conclusion, our findings can achieve less safety risk with fewer computational costs, allowing non-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25506;&#35752;&#20102;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#36951;&#24536;&#26694;&#26550;&#21450;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25913;&#36827;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#25928;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#25351;&#21335;&#12290;</title><link>https://arxiv.org/abs/2402.15159</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning of Pre-trained Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25506;&#35752;&#20102;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#36951;&#24536;&#26694;&#26550;&#21450;&#39640;&#25928;&#30340;&#36951;&#24536;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#25913;&#36827;&#36229;&#21442;&#25968;&#40065;&#26834;&#24615;&#20197;&#21450;&#39640;&#25928;&#35843;&#25972;&#36229;&#21442;&#25968;&#30340;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32972;&#26223;&#19979;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20197;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#19968;&#20010;&#20851;&#38190;&#35299;&#20915;&#26041;&#26696;&#65292;&#37325;&#28857;&#20851;&#27880;&#39044;&#35757;&#32451;&#27169;&#22411;&#8212;&#8212;&#19968;&#20010;&#26126;&#26174;&#32570;&#20047;&#30740;&#31350;&#30340;&#39046;&#22495;&#12290;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;LLMs&#20013;&#21246;&#21202;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#21253;&#25324;&#23545;&#19971;&#31181;&#19981;&#21516;&#36951;&#24536;&#26041;&#27861;&#30340;&#25209;&#21028;&#24615;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;arXiv&#12289;&#20070;&#31821;&#21644;GitHub&#30340;&#31574;&#21010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#35780;&#20272;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26377;&#21147;&#30340;&#26426;&#22120;&#36951;&#24536;&#24615;&#33021;&#22522;&#20934;&#65292;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#27604;&#37325;&#26032;&#35757;&#32451;&#39640;&#20986; $10^5$ &#20493;&#20197;&#19978;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20998;&#24067;&#25968;&#25454;&#19978;&#23558;&#26799;&#24230;&#19978;&#21319;&#19982;&#26799;&#24230;&#19979;&#38477;&#32467;&#21512;&#21487;&#20197;&#25913;&#21892;&#36229;&#21442;&#25968;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#36951;&#24536;&#36807;&#31243;&#20013;&#36827;&#34892;&#39640;&#25928;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#35814;&#32454;&#25351;&#21335;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25512;&#21160;&#20102;&#26377;&#20851;&#20262;&#29702;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#30340;&#35752;&#35770;&#65292;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15159v1 Announce Type: cross  Abstract: This study investigates the concept of the `right to be forgotten' within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models--a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over $10^5$ times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#38236;&#20687;&#36234;&#29425;&#65288;SMJ&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22312;&#35821;&#20041;&#19978;&#31867;&#20284;&#20110;&#21407;&#22987;&#38382;&#39064;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;LLMs&#12290;</title><link>https://arxiv.org/abs/2402.14872</link><description>&lt;p&gt;
&#35821;&#20041;&#38236;&#20687;&#36234;&#29425;:&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#38024;&#23545;&#24320;&#28304;LLM&#30340;&#36234;&#29425;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#20041;&#38236;&#20687;&#36234;&#29425;&#65288;SMJ&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22312;&#35821;&#20041;&#19978;&#31867;&#20284;&#20110;&#21407;&#22987;&#38382;&#39064;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#29992;&#20110;&#21019;&#24847;&#20889;&#20316;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#32763;&#35793;&#65292;&#26681;&#25454;&#36755;&#20837;&#24207;&#21015;&#29983;&#25104;&#25991;&#26412;&#65292;&#20294;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#20250;&#23548;&#33268;&#26377;&#23475;&#36755;&#20986;&#12290;&#22823;&#22810;&#25968;&#36234;&#29425;&#25552;&#31034;&#26041;&#27861;&#20351;&#29992;&#19968;&#32452;&#36234;&#29425;&#27169;&#26495;&#65292;&#28982;&#21518;&#36319;&#38543;&#25552;&#20986;&#38382;&#39064;&#65292;&#21019;&#24314;&#36234;&#29425;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36234;&#29425;&#25552;&#31034;&#35774;&#35745;&#36890;&#24120;&#23384;&#22312;&#36807;&#22810;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#23548;&#33268;&#26080;&#27861;&#25269;&#24481;&#20351;&#29992;&#31616;&#21333;&#35821;&#20041;&#24230;&#37327;&#20316;&#20026;&#38408;&#20540;&#30340;&#38450;&#24481;&#12290;&#36234;&#29425;&#25552;&#31034;&#22312;&#35821;&#20041;&#19978;&#27604;&#29992;&#20110;&#26597;&#35810;&#30340;&#21407;&#22987;&#38382;&#39064;&#26356;&#21152;&#22810;&#26679;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#35821;&#20041;&#38236;&#20687;&#36234;&#29425;&#65288;SMJ&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22312;&#35821;&#20041;&#19978;&#31867;&#20284;&#20110;&#21407;&#22987;&#38382;&#39064;&#30340;&#36234;&#29425;&#25552;&#31034;&#26469;&#32469;&#36807;LLMs&#12290;&#25105;&#20204;&#23558;&#23547;&#25214;&#26082;&#28385;&#36275;&#35821;&#20041;&#30456;&#20284;&#24615;&#21448;&#20855;&#26377;&#36234;&#29425;&#26377;&#25928;&#24615;&#30340;&#36234;&#29425;&#25552;&#31034;&#24314;&#27169;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14872v1 Announce Type: cross  Abstract: Large Language Models (LLMs), used in creative writing, code generation, and translation, generate text based on input sequences but are vulnerable to jailbreak attacks, where crafted prompts induce harmful outputs. Most jailbreak prompt methods use a combination of jailbreak templates followed by questions to ask to create jailbreak prompts. However, existing jailbreak prompt designs generally suffer from excessive semantic differences, resulting in an inability to resist defenses that use simple semantic metrics as thresholds. Jailbreak prompts are semantically more varied than the original questions used for queries. In this paper, we introduce a Semantic Mirror Jailbreak (SMJ) approach that bypasses LLMs by generating jailbreak prompts that are semantically similar to the original question. We model the search for jailbreak prompts that satisfy both semantic similarity and jailbreak validity as a multi-objective optimization proble
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.13516</link><description>&lt;p&gt;
ProSparse: &#24341;&#20837;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Activation sparsity&#25351;&#30340;&#26159;&#28608;&#27963;&#36755;&#20986;&#20013;&#23384;&#22312;&#35768;&#22810;&#24369;&#36129;&#29486;&#20803;&#32032;&#12290;&#20316;&#20026;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#30340;&#26222;&#36941;&#23646;&#24615;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37319;&#29992;&#20102;&#27809;&#26377;&#20869;&#22312;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#20363;&#22914;GELU&#21644;Swish&#65289;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#21162;&#21147;&#23581;&#35797;&#24341;&#20837;ReLU&#25110;&#20854;&#21464;&#20307;&#20316;&#20026;&#26367;&#20195;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;LLMs&#23454;&#29616;&#28608;&#27963;&#31232;&#30095;&#24615;&#21644;&#25512;&#29702;&#21152;&#36895;&#65292;&#20294;&#24456;&#23569;&#33021;&#21516;&#26102;&#33719;&#24471;&#39640;&#31232;&#30095;&#24230;&#21644;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;LLMs&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23558;LLMs&#30340;&#28608;&#27963;&#20989;&#25968;&#26367;&#25442;&#20026;ReLU&#21518;&#65292;ProSparse&#37319;&#29992;&#28176;&#36827;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#22914;&#26524;-&#21542;&#21017;&#8221;&#65288;IoE&#65289;&#25552;&#31034;&#26694;&#26550;&#65292;&#24110;&#21161;&#27169;&#22411;&#35780;&#20272;&#33258;&#36523;&#8220;&#20449;&#24515;&#8221;&#24182;&#36827;&#34892;&#33258;&#25105;&#26657;&#27491;&#12290;</title><link>https://arxiv.org/abs/2402.12563</link><description>&lt;p&gt;
&#20449;&#24515;&#33267;&#20851;&#37325;&#35201;&#65306;&#37325;&#26032;&#23457;&#35270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#22914;&#26524;-&#21542;&#21017;&#8221;&#65288;IoE&#65289;&#25552;&#31034;&#26694;&#26550;&#65292;&#24110;&#21161;&#27169;&#22411;&#35780;&#20272;&#33258;&#36523;&#8220;&#20449;&#24515;&#8221;&#24182;&#36827;&#34892;&#33258;&#25105;&#26657;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#36817;&#25104;&#21151;&#28608;&#21457;&#20102;&#23545;&#23427;&#20204;&#33258;&#25105;&#26657;&#27491;&#33021;&#21147;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#26412;&#25991;&#23545;LLMs&#30340;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#35797;&#22270;&#35299;&#20915;&#20851;&#20110;&#20854;&#21487;&#34892;&#24615;&#30340;&#25345;&#32493;&#20105;&#35770;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30830;&#23450;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28508;&#22312;&#22240;&#32032; - LLMs&#30340;&#8220;&#20449;&#24515;&#8221; - &#22312;&#33258;&#25105;&#26657;&#27491;&#36807;&#31243;&#20013;&#12290;&#24573;&#35270;&#36825;&#19968;&#22240;&#32032;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#36807;&#24230;&#25209;&#35780;&#33258;&#24049;&#65292;&#20174;&#32780;&#23548;&#33268;&#23545;&#33258;&#26657;&#27491;&#25928;&#26524;&#30340;&#21487;&#38752;&#32467;&#35770;&#19981;&#20934;&#30830;&#12290;&#25105;&#20204;&#23454;&#39564;&#35266;&#23519;&#21040;LLMs&#20855;&#26377;&#29702;&#35299;&#20854;&#33258;&#36523;&#22238;&#24212;&#8220;&#20449;&#24515;&#8221;&#30340;&#33021;&#21147;&#12290;&#36825;&#28608;&#21169;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#8220;&#22914;&#26524;-&#21542;&#21017;&#8221;&#65288;IoE&#65289;&#25552;&#31034;&#26694;&#26550;&#65292;&#26088;&#22312;&#24341;&#23548;LLMs&#35780;&#20272;&#20854;&#33258;&#36523;&#8220;&#20449;&#24515;&#8221;&#65292;&#20419;&#36827;&#20869;&#22312;&#33258;&#25105;&#26657;&#27491;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#22522;&#20110;IoE&#30340;&#25552;&#31034;&#21487;&#20197;&#23454;&#29616;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12563v1 Announce Type: cross  Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a co
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#39057;&#29575;&#31354;&#38388;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#20302;&#31209;&#36866;&#24212;&#65288;MuScleLoRA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12026</link><description>&lt;p&gt;
&#20174;&#21518;&#38376;&#27602;&#21270;&#25968;&#25454;&#38598;&#20013;&#36890;&#36807;&#38477;&#39057;&#31354;&#38388;&#33719;&#21462;&#28165;&#27905;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12026
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#39057;&#29575;&#31354;&#38388;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#20302;&#31209;&#36866;&#24212;&#65288;MuScleLoRA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22312;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;LMs&#30340;&#21487;&#38752;&#24615;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#22312;&#27602;&#21270;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;LMs&#26102;&#20943;&#36731;&#21518;&#38376;&#23398;&#20064;&#65292;&#20294;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#25269;&#24481;&#22797;&#26434;&#30340;&#21518;&#38376;&#25915;&#20987;&#26102;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20613;&#37324;&#21494;&#20998;&#26512;&#30740;&#31350;&#20102;&#39057;&#29575;&#31354;&#38388;&#20013;&#21518;&#38376;LMs&#30340;&#23398;&#20064;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#27602;&#21270;&#25968;&#25454;&#38598;&#19978;&#21576;&#29616;&#30340;&#21518;&#38376;&#26144;&#23556;&#30456;&#27604;&#28165;&#27905;&#26144;&#23556;&#26356;&#20542;&#21521;&#20110;&#36739;&#20302;&#39057;&#29575;&#65292;&#23548;&#33268;&#21518;&#38376;&#26144;&#23556;&#26356;&#24555;&#22320;&#25910;&#25947;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#22256;&#22659;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#20302;&#31209;&#36866;&#24212;&#65288;MuScleLoRA&#65289;&#65292;&#23427;&#22312;&#39057;&#29575;&#31354;&#38388;&#20013;&#37096;&#32626;&#22810;&#20010;&#24452;&#21521;&#32553;&#25918;&#65292;&#20302;&#31209;&#36866;&#24212;&#30446;&#26631;&#27169;&#22411;&#65292;&#24182;&#22312;&#26356;&#26032;&#21442;&#25968;&#26102;&#36827;&#19968;&#27493;&#35843;&#25972;&#26799;&#24230;&#12290;&#36890;&#36807;&#38477;&#39057;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12026v1 Announce Type: cross  Abstract: Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscal
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#25913;&#36827;&#32763;&#35793;&#36136;&#37327;&#30340;&#25104;&#26412;&#25928;&#30410;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#26469;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#65292;&#20174;&#32780;&#25351;&#23548;&#25913;&#36827;&#26426;&#22120;&#32763;&#35793;&#12290;</title><link>https://arxiv.org/abs/2402.11525</link><description>&lt;p&gt;
&#29992;RLHF&#25512;&#36827;&#32763;&#35793;&#20559;&#22909;&#24314;&#27169;&#65306;&#36808;&#21521;&#25104;&#26412;&#25928;&#30410;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Advancing Translation Preference Modeling with RLHF: A Step Towards Cost-Effective Solution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11525
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#25913;&#36827;&#32763;&#35793;&#36136;&#37327;&#30340;&#25104;&#26412;&#25928;&#30410;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#36890;&#36807;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#26469;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#65292;&#20174;&#32780;&#25351;&#23548;&#25913;&#36827;&#26426;&#22120;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11525v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#30495;&#23454;&#24615;&#12289;&#34920;&#36798;&#21147;&#21644;&#20248;&#38597;&#26159;&#26426;&#22120;&#32763;&#35793;&#20013;&#19981;&#26029;&#36861;&#27714;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#24230;&#37327;&#26631;&#20934;&#22914;BLEU&#24182;&#19981;&#20005;&#26684;&#31526;&#21512;&#20154;&#31867;&#23545;&#32763;&#35793;&#36136;&#37327;&#30340;&#20559;&#22909;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#26469;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;&#25910;&#38598;&#20154;&#31867;&#23545;&#32763;&#35793;&#20043;&#38388;&#30340;&#27604;&#36739;&#30340;&#22823;&#35268;&#27169;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#24182;&#19981;&#23481;&#26131;&#65292;&#23588;&#20854;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#30340;&#20559;&#22909;&#23398;&#20064;&#31574;&#30053;&#65292;&#36890;&#36807;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#32763;&#35793;&#26469;&#20248;&#21270;&#22870;&#21169;&#27169;&#22411;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#22870;&#21169;&#27169;&#22411;&#23398;&#20064;&#26426;&#22120;&#32763;&#35793;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#24182;&#25351;&#23548;&#38543;&#21518;&#23545;&#26426;&#22120;&#32763;&#35793;&#30340;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;RLHF&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21319;&#32763;&#35793;&#36136;&#37327;&#65292;&#36825;&#31181;&#25913;&#36827;&#20063;&#26377;&#30410;&#20110;&#20854;&#20182;&#32763;&#35793;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11525v1 Announce Type: new  Abstract: Faithfulness, expressiveness, and elegance is the constant pursuit in machine translation. However, traditional metrics like \textit{BLEU} do not strictly align with human preference of translation quality. In this paper, we explore leveraging reinforcement learning with human feedback (\textit{RLHF}) to improve translation quality. It is non-trivial to collect a large high-quality dataset of human comparisons between translations, especially for low-resource languages. To address this issue, we propose a cost-effective preference learning strategy, optimizing reward models by distinguishing between human and machine translations. In this manner, the reward model learns the deficiencies of machine translation compared to human and guides subsequent improvements in machine translation. Experimental results demonstrate that \textit{RLHF} can effectively enhance translation quality and this improvement benefits other translation directions 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#23567;&#22411;&#24494;&#35843;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LinkNER&#26694;&#26550;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#30340;&#38142;&#25509;&#31574;&#30053;RDC&#65292;&#20351;&#24494;&#35843;&#27169;&#22411;&#33021;&#22815;&#34917;&#20805;&#40657;&#30418;LLMs</title><link>https://arxiv.org/abs/2402.10573</link><description>&lt;p&gt;
LinkNER: &#20351;&#29992;&#19981;&#30830;&#23450;&#24615;&#23558;&#26412;&#22320;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
LinkNER: Linking Local Named Entity Recognition Models to Large Language Models using Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10573
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#23567;&#22411;&#24494;&#35843;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LinkNER&#26694;&#26550;&#65292;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#30340;&#38142;&#25509;&#31574;&#30053;RDC&#65292;&#20351;&#24494;&#35843;&#27169;&#22411;&#33021;&#22815;&#34917;&#20805;&#40657;&#30418;LLMs
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#30452;&#25509;&#24433;&#21709;&#30528;&#32593;&#32476;&#20869;&#23481;&#20998;&#26512;&#12289;&#25628;&#32034;&#24341;&#25806;&#21644;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#12290;&#24494;&#35843;&#21518;&#30340;NER&#27169;&#22411;&#22312;&#26631;&#20934;NER&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#24494;&#35843;&#25968;&#25454;&#21644;&#32570;&#20047;&#30693;&#35782;&#65292;&#23427;&#22312;&#26410;&#35265;&#23454;&#20307;&#35782;&#21035;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#22240;&#27492;&#65292;NER&#27169;&#22411;&#22312;&#32593;&#32476;&#30456;&#20851;&#24212;&#29992;&#20013;&#30340;&#21487;&#29992;&#24615;&#21644;&#21487;&#38752;&#24615;&#21463;&#21040;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20855;&#26377;&#20016;&#23500;&#30340;&#22806;&#37096;&#30693;&#35782;&#65292;&#20294;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#32570;&#20047;NER&#20219;&#21153;&#30340;&#19987;&#19994;&#24615;&#12290;&#27492;&#22806;&#65292;&#31169;&#26377;&#21644;&#22823;&#35268;&#27169;&#26435;&#37325;&#20351;LLM&#30340;&#35843;&#25972;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#23567;&#22411;&#24494;&#35843;&#27169;&#22411;&#21644;LLMs&#65288;LinkNER&#65289;&#65292;&#20197;&#21450;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#38142;&#25509;&#31574;&#30053;RDC&#65292;&#20351;&#24494;&#35843;&#27169;&#22411;&#33021;&#22815;&#34917;&#20805;&#40657;&#30418;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10573v1 Announce Type: new  Abstract: Named Entity Recognition (NER) serves as a fundamental task in natural language understanding, bearing direct implications for web content analysis, search engines, and information retrieval systems. Fine-tuned NER models exhibit satisfactory performance on standard NER benchmarks. However, due to limited fine-tuning data and lack of knowledge, it performs poorly on unseen entity recognition. As a result, the usability and reliability of NER models in web-related applications are compromised. Instead, Large Language Models (LLMs) like GPT-4 possess extensive external knowledge, but research indicates that they lack specialty for NER tasks. Furthermore, non-public and large-scale weights make tuning LLMs difficult. To address these challenges, we propose a framework that combines small fine-tuned models with LLMs (LinkNER) and an uncertainty-based linking strategy called RDC that enables fine-tuned models to complement black-box LLMs, ach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25776;&#20889;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#20445;&#30041;&#20256;&#32479;&#36741;&#23548;&#31995;&#32479;&#32467;&#26500;&#21644;&#25945;&#23398;&#27861;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09216</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;AutoTutor&#30340;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
Scaling the Authoring of AutoTutors with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25776;&#20889;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#20102;&#20445;&#30041;&#20256;&#32479;&#36741;&#23548;&#31995;&#32479;&#32467;&#26500;&#21644;&#25945;&#23398;&#27861;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25945;&#32946;&#39046;&#22495;&#26377;&#22810;&#31181;&#29992;&#36884;&#65292;&#20174;&#33258;&#21160;&#39064;&#30446;&#29983;&#25104;&#21040;&#20316;&#25991;&#35780;&#20272;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#25776;&#20889;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#28508;&#21147;&#12290;LLMs&#30340;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#23427;&#20204;&#23481;&#26131;&#20559;&#31163;&#25152;&#26399;&#26395;&#30340;&#25945;&#23398;&#31574;&#30053;&#65292;&#20363;&#22914;&#27844;&#38706;&#31572;&#26696;&#32473;&#23398;&#29983;&#65292;&#24635;&#20307;&#19978;&#25552;&#20379;&#30340;&#20445;&#35777;&#24456;&#23569;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#34429;&#28982;&#24102;&#26377;&#26576;&#20123;&#38480;&#21046;&#30340;LLMs&#21487;&#20197;&#21462;&#20195;&#23398;&#31185;&#19987;&#23478;&#30340;&#20301;&#32622;&#65292;&#20294;&#25972;&#20307;&#30340;&#25945;&#23398;&#35774;&#35745;&#20173;&#38656;&#25163;&#24037;&#21046;&#20316;&#20197;&#21462;&#24471;&#26368;&#20339;&#23398;&#20064;&#25928;&#26524;&#12290;&#22522;&#20110;&#36825;&#19968;&#21407;&#21017;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#31034;&#20363;&#30340;&#31471;&#21040;&#31471;&#36741;&#23548;&#31995;&#32479;&#65292;&#21629;&#21517;&#20026;MWPTutor&#65292;&#23427;&#20351;&#29992;LLMs&#22635;&#20805;&#39044;&#23450;&#20041;&#26377;&#38480;&#29366;&#24577;&#36716;&#25442;&#22120;&#30340;&#29366;&#24577;&#31354;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#20445;&#30041;&#20102;&#22810;&#24180;&#26469;&#30001;&#23398;&#20064;&#31185;&#23398;&#23478;&#24320;&#21457;&#30340;&#20256;&#32479;&#36741;&#23548;&#31995;&#32479;&#30340;&#32467;&#26500;&#21644;&#25945;&#23398;&#27861;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09216v1 Announce Type: new Abstract: Large Language Models (LLMs) have found several use cases in education, ranging from automatic question generation to essay evaluation. In this paper, we explore the potential of using Large Language Models (LLMs) to author Intelligent Tutoring Systems. A common pitfall of LLMs is their straying from desired pedagogical strategies such as leaking the answer to the student, and in general, providing no guarantees. We posit that while LLMs with certain guardrails can take the place of subject experts, the overall pedagogical design still needs to be handcrafted for the best learning results. Based on this principle, we create a sample end-to-end tutoring system named MWPTutor, which uses LLMs to fill in the state space of a pre-defined finite state transducer. This approach retains the structure and the pedagogy of traditional tutoring systems that has been developed over the years by learning scientists but brings in additional flexibility
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.08496</link><description>&lt;p&gt;
&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#31995;&#32479;&#24615;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
A Systematic Review of Data-to-Text NLG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08496
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#20840;&#38754;&#20998;&#26512;&#20102;&#25968;&#25454;&#21040;&#25991;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#25552;&#20986;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#20102;&#30456;&#20851;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31995;&#32479;&#24615;&#22238;&#39038;&#26088;&#22312;&#20840;&#38754;&#20998;&#26512;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#30740;&#31350;&#30340;&#29616;&#29366;&#65292;&#37325;&#28857;&#26159;&#30830;&#23450;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20379;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#35299;&#20915;&#22238;&#39038;&#20013;&#21457;&#29616;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#25991;&#29486;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26816;&#26597;&#65292;&#21253;&#25324;&#26041;&#27861;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#12289;&#24212;&#29992;&#12289;&#22810;&#35821;&#35328;&#24615;&#21644;&#24187;&#35273;&#32531;&#35299;&#25514;&#26045;&#12290;&#25105;&#20204;&#30340;&#22238;&#39038;&#20026;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This systematic review aims to provide a comprehensive analysis of the state of data-to-text generation research, focusing on identifying research gaps, offering future directions, and addressing challenges found during the review. We thoroughly examined the literature, including approaches, datasets, evaluation metrics, applications, multilingualism, and hallucination mitigation measures. Our review provides a roadmap for future research in this rapidly evolving field.
&lt;/p&gt;</description></item><item><title>HarmBench&#26159;&#19968;&#20010;&#20026;&#33258;&#21160;&#32418;&#38431;&#21644;&#24378;&#22823;&#25298;&#32477;&#35774;&#35745;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;18&#31181;&#32418;&#38431;&#26041;&#27861;&#21644;33&#20010;&#30446;&#26631;LLM&#21644;&#38450;&#24481;&#30340;&#27604;&#36739;&#65292;&#24471;&#20986;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;LLM&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04249</link><description>&lt;p&gt;
HarmBench&#65306;&#29992;&#20110;&#33258;&#21160;&#32418;&#38431;&#21644;&#24378;&#22823;&#25298;&#32477;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04249
&lt;/p&gt;
&lt;p&gt;
HarmBench&#26159;&#19968;&#20010;&#20026;&#33258;&#21160;&#32418;&#38431;&#21644;&#24378;&#22823;&#25298;&#32477;&#35774;&#35745;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;18&#31181;&#32418;&#38431;&#26041;&#27861;&#21644;33&#20010;&#30446;&#26631;LLM&#21644;&#38450;&#24481;&#30340;&#27604;&#36739;&#65292;&#24471;&#20986;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;LLM&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#32418;&#38431;&#20855;&#26377;&#21457;&#29616;&#21644;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#35813;&#39046;&#22495;&#32570;&#20047;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#26469;&#20005;&#26684;&#35780;&#20272;&#26032;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HarmBench&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#32418;&#38431;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#32418;&#38431;&#35780;&#20272;&#20013;&#30830;&#23450;&#20102;&#20960;&#20010;&#20197;&#21069;&#26410;&#32771;&#34385;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#29305;&#24615;&#65292;&#24182;&#31995;&#32479;&#22320;&#35774;&#35745;&#20102;HarmBench&#20197;&#28385;&#36275;&#36825;&#20123;&#26631;&#20934;&#12290;&#20351;&#29992;HarmBench&#65292;&#25105;&#20204;&#23545;18&#31181;&#32418;&#38431;&#26041;&#27861;&#21644;33&#20010;&#30446;&#26631;LLM&#21644;&#38450;&#24481;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#27604;&#36739;&#65292;&#24471;&#21040;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#30340;&#31283;&#20581;&#24615;&#65292;&#23637;&#31034;&#20102;HarmBench&#22914;&#20309;&#20419;&#36827;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#20849;&#21516;&#24320;&#21457;&#12290;&#25105;&#20204;&#22312;https://github.com/centerforaisafety/HarmBench&#19978;&#24320;&#28304;&#20102;HarmBench&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.
&lt;/p&gt;</description></item><item><title>ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03848</link><description>&lt;p&gt;
ANLS* -- &#19968;&#31181;&#36866;&#29992;&#20110;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ANLS* -- A Universal Document Processing Metric for Generative Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03848
&lt;/p&gt;
&lt;p&gt;
ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22312;&#25991;&#26723;&#20998;&#31867;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#20219;&#21153;&#20013;&#65292;&#21306;&#20998;&#27169;&#22411;&#19968;&#30452;&#26159;&#20027;&#35201;&#36873;&#25321;&#12290;&#36825;&#20123;&#27169;&#22411;&#20570;&#20986;&#30340;&#39044;&#27979;&#21487;&#20197;&#20998;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#39044;&#23450;&#20041;&#31867;&#21035;&#65292;&#20415;&#20110;&#36827;&#34892;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#65292;&#24182;&#33021;&#30452;&#25509;&#35745;&#31639;F1&#20998;&#25968;&#31561;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;GLLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#20351;&#39046;&#22495;&#21457;&#29983;&#20102;&#36716;&#21464;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#22791;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#28040;&#38500;&#20102;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#26114;&#36149;&#30340;&#24494;&#35843;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;GLLMs&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23545;&#20110;GLLMs&#30340;&#39044;&#27979;&#65292;&#19981;&#33021;&#24212;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#26041;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;ANLS*&#65292;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;ANLS*&#24230;&#37327;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs. This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#33258;&#30001;&#25991;&#26412;&#20013;&#35782;&#21035;&#20316;&#20026;&#20195;&#29702;&#30340;&#27169;&#22411;&#21644;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#30340;&#23567;&#22411;&#32447;&#24615;&#25506;&#27979;&#22120;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26356;&#22823;&#27169;&#22411;&#20196;&#29260;&#32423;&#21035;&#19978;&#30340;&#33258;&#20449;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24230;&#65292;&#36825;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.03563</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#27169;&#22411;&#21306;&#20998;&#21487;&#30693;&#19982;&#19981;&#21487;&#30693;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Distinguishing the Knowable from the Unknowable with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03563
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#33258;&#30001;&#25991;&#26412;&#20013;&#35782;&#21035;&#20316;&#20026;&#20195;&#29702;&#30340;&#27169;&#22411;&#21644;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#30340;&#23567;&#22411;&#32447;&#24615;&#25506;&#27979;&#22120;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26356;&#22823;&#27169;&#22411;&#20196;&#29260;&#32423;&#21035;&#19978;&#30340;&#33258;&#20449;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24230;&#65292;&#36825;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#33258;&#30001;&#25991;&#26412;&#36755;&#20986;&#20013;&#65292;&#26159;&#21542;&#21487;&#20197;&#37492;&#21035;&#20986;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65288;&#21453;&#26144;&#32570;&#20047;&#30693;&#35782;&#30340;&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#65288;&#21453;&#26144;&#22522;&#30784;&#20998;&#24067;&#20013;&#30340;&#29109;&#65289;&#12290;&#22312;&#27809;&#26377;&#30495;&#23454;&#27010;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#35774;&#32622;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#20026;&#20102;&#65288;&#36817;&#20284;&#22320;&#65289;&#20998;&#35299;&#32473;&#23450;LLM&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#19968;&#20010;&#26126;&#26174;&#26356;&#22823;&#30340;&#27169;&#22411;&#20805;&#24403;&#22320;&#38754;&#30495;&#30456;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22522;&#20110;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#30340;&#23567;&#22411;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#26356;&#22823;&#27169;&#22411;&#23558;&#26356;&#33258;&#20449;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#22312;&#19968;&#20010;&#25991;&#26412;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#25506;&#27979;&#22120;&#21487;&#20197;&#27867;&#21270;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24230;&#12290;&#32508;&#21512;&#32771;&#34385;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#35299;&#37322;&#36825;&#20123;&#32467;&#26524;&#20316;&#20026;LLMs&#20869;&#37096;&#33258;&#28982;&#22320;&#21253;&#21547;&#20102;&#19981;&#21516;&#31867;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#34920;&#31034;&#65292;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#21046;&#23450;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03299</link><description>&lt;p&gt;
GUARD: &#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#26469;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#21335;&#30340;&#21512;&#35268;&#24615;
&lt;/p&gt;
&lt;p&gt;
GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#32469;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#21644;&#26377;&#23475;&#22238;&#24212;&#30340;"&#36234;&#29425;"&#24050;&#32463;&#40723;&#21169;&#31038;&#21306;&#37319;&#21462;&#23433;&#20840;&#25514;&#26045;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#23433;&#20840;&#25514;&#26045;&#26159;&#22312;&#21457;&#24067;&#20043;&#21069;&#29992;&#36234;&#29425;&#20027;&#21160;&#27979;&#35797;LLM&#12290;&#22240;&#27492;&#65292;&#36825;&#26679;&#30340;&#27979;&#35797;&#23558;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#22823;&#35268;&#27169;&#19988;&#39640;&#25928;&#22320;&#29983;&#25104;&#36234;&#29425;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#36861;&#38543;&#19968;&#31181;&#26032;&#39062;&#32780;&#30452;&#35266;&#30340;&#31574;&#30053;&#19979;&#65292;&#20197;&#20154;&#31867;&#29983;&#25104;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#36234;&#29425;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35282;&#33394;&#25198;&#28436;&#31995;&#32479;&#65292;&#23558;&#22235;&#31181;&#19981;&#21516;&#35282;&#33394;&#20998;&#37197;&#32473;&#29992;&#25143;LLM&#65292;&#20197;&#20415;&#21327;&#20316;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25910;&#38598;&#29616;&#26377;&#30340;&#36234;&#29425;&#65292;&#24182;&#36890;&#36807;&#21477;&#23376;&#36880;&#21477;&#36827;&#34892;&#32858;&#31867;&#39057;&#29575;&#21644;&#35821;&#20041;&#27169;&#24335;&#30340;&#21010;&#20998;&#65292;&#23558;&#23427;&#20204;&#20998;&#25104;&#19981;&#21516;&#30340;&#29420;&#31435;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#29305;&#24449;&#32452;&#32455;&#25104;&#19968;&#20010;&#30693;&#35782;&#22270;&#65292;&#20351;&#20854;&#26356;&#26131;&#20110;&#35775;&#38382;&#21644;&#26816;&#32034;&#12290;&#25105;&#20204;&#30340;&#35282;&#33394;&#31995;&#32479;&#23558;&#21033;&#29992;&#36825;&#20010;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effec
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#36890;&#36807;&#35753;LLM&#39318;&#20808;&#35299;&#30721;&#25277;&#35937;&#25512;&#29702;&#38142;&#65292;&#28982;&#21518;&#35843;&#29992;&#39046;&#22495;&#24037;&#20855;&#22635;&#20805;&#20855;&#20307;&#30693;&#35782;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#24037;&#20855;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#65292;&#24182;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17464</link><description>&lt;p&gt;
&#20351;&#29992;&#25277;&#35937;&#38142;&#25512;&#29702;&#30340;&#39640;&#25928;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Tool Use with Chain-of-Abstraction Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#36890;&#36807;&#35753;LLM&#39318;&#20808;&#35299;&#30721;&#25277;&#35937;&#25512;&#29702;&#38142;&#65292;&#28982;&#21518;&#35843;&#29992;&#39046;&#22495;&#24037;&#20855;&#22635;&#20805;&#20855;&#20307;&#30693;&#35782;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#24037;&#20855;&#36827;&#34892;&#22810;&#27493;&#25512;&#29702;&#65292;&#24182;&#19988;&#20855;&#26377;&#36890;&#29992;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#19982;&#20154;&#31867;&#26399;&#26395;&#19968;&#33268;&#30340;&#20934;&#30830;&#25512;&#29702;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38656;&#35201;&#23558;&#25512;&#29702;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#65288;&#20363;&#22914;&#32593;&#32476;&#20107;&#23454;&#12289;&#25968;&#23398;&#21644;&#29289;&#29702;&#35268;&#21017;&#65289;&#32852;&#31995;&#36215;&#26469;&#12290;&#24037;&#20855;&#21487;&#20197;&#24110;&#21161;LLM&#33719;&#21462;&#36825;&#20123;&#22806;&#37096;&#30693;&#35782;&#65292;&#20294;&#26159;&#22312;&#22810;&#27493;&#25512;&#29702;&#38382;&#39064;&#20013;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#21363;&#22914;&#20309;&#31934;&#32454;&#35843;&#25972;LLM&#20195;&#29702;&#65288;&#20363;&#22914;Toolformer&#65289;&#20197;&#35843;&#29992;&#24037;&#20855;&#65292;&#20854;&#20013;&#30456;&#20114;&#36830;&#25509;&#30340;&#24037;&#20855;&#35843;&#29992;&#38656;&#35201;&#25972;&#20307;&#21270;&#21644;&#39640;&#25928;&#30340;&#24037;&#20855;&#20351;&#29992;&#35268;&#21010;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#35753;LLM&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#26356;&#22909;&#22320;&#21033;&#29992;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#35757;&#32451;LLM&#39318;&#20808;&#29992;&#25277;&#35937;&#21344;&#20301;&#31526;&#35299;&#30721;&#25512;&#29702;&#38142;&#65292;&#28982;&#21518;&#35843;&#29992;&#39046;&#22495;&#24037;&#20855;&#20197;&#22635;&#20805;&#20855;&#20307;&#30693;&#35782;&#26469;&#23454;&#29616;&#27599;&#20010;&#25512;&#29702;&#38142;&#12290;&#25277;&#35937;&#38142;&#30340;&#35268;&#21010;&#20351;LLM&#33021;&#22815;&#23398;&#20064;&#26356;&#36890;&#29992;&#30340;&#25512;&#29702;&#31574;&#30053;&#65292;&#23545;&#20110;&#19982;&#19981;&#21516;&#25512;&#29702;&#38382;&#39064;&#30456;&#20851;&#30340;&#39046;&#22495;&#30693;&#35782;&#65288;&#20363;&#22914;&#25968;&#23398;&#32467;&#26524;&#65289;&#30340;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#23427;&#36824;&#20801;&#35768;LLM&#25191;&#34892;&#35299;&#30721;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
To achieve faithful reasoning that aligns with human expectations, large language models (LLMs) need to ground their reasoning to real-world knowledge (e.g., web facts, math and physical rules). Tools help LLMs access this external knowledge, but there remains challenges for fine-tuning LLM agents (e.g., Toolformer) to invoke tools in multi-step reasoning problems, where inter-connected tool calls require holistic and efficient tool usage planning.   In this work, we propose a new method for LLMs to better leverage tools in multi-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to first decode reasoning chains with abstract placeholders, and then call domain tools to reify each reasoning chain by filling in specific knowledge. This planning with abstract chains enables LLMs to learn more general reasoning strategies, which are robust to shifts of domain knowledge (e.g., math results) relevant to different reasoning questions. It also allows LLMs to perform decoding a
&lt;/p&gt;</description></item><item><title>&#26412;&#25945;&#31243;&#35814;&#32454;&#12289;&#31616;&#21270;&#21644;&#28165;&#26224;&#22320;&#35299;&#37322;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#28165;&#26224;&#30340;&#22270;&#24418;&#35828;&#26126;&#65292;&#22635;&#34917;&#20102;&#32570;&#20047;&#32479;&#19968;&#25968;&#23398;&#26694;&#26550;&#30340;&#29616;&#23384;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.03797</link><description>&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#35299;&#21078;&#23398;
&lt;/p&gt;
&lt;p&gt;
Anatomy of Neural Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25945;&#31243;&#35814;&#32454;&#12289;&#31616;&#21270;&#21644;&#28165;&#26224;&#22320;&#35299;&#37322;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#28165;&#26224;&#30340;&#22270;&#24418;&#35828;&#26126;&#65292;&#22635;&#34917;&#20102;&#32570;&#20047;&#32479;&#19968;&#25968;&#23398;&#26694;&#26550;&#30340;&#29616;&#23384;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#36801;&#31227;&#23398;&#20064;&#39046;&#22495;&#22312;&#26368;&#36817;&#20960;&#24180;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#12290;&#21464;&#21387;&#22120;&#24050;&#32463;&#25104;&#20026;&#36825;&#20123;&#36827;&#23637;&#30340;&#26680;&#24515;&#65292;&#22522;&#20110;&#26368;&#21069;&#27839;&#30340;&#21464;&#21387;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#23548;&#33268;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#23613;&#31649;&#28041;&#21450;&#31070;&#32463;LMs&#30340;&#30740;&#31350;&#20316;&#21697;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#20294;&#20854;&#20013;&#32477;&#22823;&#22810;&#25968;&#26159;&#39640;&#32423;&#30340;&#65292;&#31163;&#23454;&#38469;&#25805;&#20316;&#39047;&#36828;&#12290;&#22240;&#27492;&#65292;&#22312;&#32570;&#20047;&#35299;&#37322;&#20027;&#35201;&#31867;&#22411;&#31070;&#32463;LMs&#30340;&#32479;&#19968;&#25968;&#23398;&#26694;&#26550;&#30340;&#21069;&#25552;&#19979;&#65292;&#23545;&#36825;&#19968;&#39046;&#22495;&#30340;&#25991;&#29486;&#28145;&#20837;&#20102;&#35299;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#26412;&#25945;&#31243;&#20013;&#35299;&#20915;&#20102;&#19978;&#36848;&#38382;&#39064;&#65292;&#26088;&#22312;&#22312;&#35814;&#32454;&#12289;&#31616;&#21270;&#21644;&#28165;&#26224;&#30340;&#25968;&#23398;&#26694;&#26550;&#20013;&#35299;&#37322;&#31070;&#32463;LMs&#65292;&#24182;&#38468;&#26377;&#28165;&#26224;&#30340;&#22270;&#24418;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03797v2 Announce Type: replace  Abstract: The fields of generative AI and transfer learning have experienced remarkable advancements in recent years especially in the domain of Natural Language Processing (NLP). Transformers have been at the heart of these advancements where the cutting-edge transformer-based Language Models (LMs) have led to new state-of-the-art results in a wide spectrum of applications. While the number of research works involving neural LMs is exponentially increasing, their vast majority are high-level and far from self-contained. Consequently, a deep understanding of the literature in this area is a tough task especially in the absence of a unified mathematical framework explaining the main types of neural LMs. We address the aforementioned problem in this tutorial where the objective is to explain neural LMs in a detailed, simplified and unambiguous mathematical framework accompanied by clear graphical illustrations. Concrete examples on widely used m
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Cascade Speculative Drafting&#65288;CS Drafting&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#22402;&#30452;&#32423;&#32852;&#28040;&#38500;&#31070;&#32463;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#36890;&#36807;&#27700;&#24179;&#32423;&#32852;&#20248;&#21270;&#33609;&#31295;&#20013;&#30340;&#26102;&#38388;&#20998;&#37197;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.11462</link><description>&lt;p&gt;
&#29992;&#20110;&#26356;&#24555;&#30340;LLM&#25512;&#29702;&#30340;&#32423;&#32852;&#25512;&#27979;&#33609;&#22270;
&lt;/p&gt;
&lt;p&gt;
Cascade Speculative Drafting for Even Faster LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11462
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Cascade Speculative Drafting&#65288;CS Drafting&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#22402;&#30452;&#32423;&#32852;&#28040;&#38500;&#31070;&#32463;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#36890;&#36807;&#27700;&#24179;&#32423;&#32852;&#20248;&#21270;&#33609;&#31295;&#20013;&#30340;&#26102;&#38388;&#20998;&#37197;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#29702;&#25928;&#29575;&#30340;&#32423;&#32852;&#25512;&#27979;&#33609;&#22270;&#65292;&#36890;&#36807;&#36739;&#23567;&#30340;&#27169;&#22411;&#29983;&#25104;&#33609;&#31295;&#26469;&#36816;&#20316;&#12290;&#36739;&#22823;&#30340;&#30446;&#26631;&#27169;&#22411;&#28982;&#21518;&#26597;&#30475;&#36825;&#20010;&#33609;&#31295;&#20197;&#19982;&#20854;&#36755;&#20986;&#23545;&#40784;&#65292;&#30446;&#26631;&#27169;&#22411;&#30340;&#20219;&#20309;&#25509;&#21463;&#37117;&#23558;&#20943;&#23569;&#30446;&#26631;&#27169;&#22411;&#36816;&#34892;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#32423;&#32852;&#25512;&#27979;&#30340;&#33609;&#22270;&#36807;&#31243;&#20013;&#21253;&#25324;&#32531;&#24930;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#24182;&#20026;&#29983;&#25104;&#30340;&#26631;&#35760;&#20998;&#37197;&#30456;&#21516;&#30340;&#26102;&#38388;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#20302;&#25928;&#24615;&#20849;&#21516;&#23548;&#33268;&#32423;&#32852;&#25512;&#27979;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#21892;LLM&#25512;&#29702;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32423;&#32852;&#25512;&#27979;&#33609;&#22270;&#65288;CS Drafting&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#25972;&#21512;&#20102;&#20004;&#31181;&#32423;&#32852;&#31867;&#22411;&#30340;&#25512;&#27979;&#25191;&#34892;&#31639;&#27861;&#12290;&#22402;&#30452;&#32423;&#32852;&#20174;&#31070;&#32463;&#27169;&#22411;&#20013;&#28040;&#38500;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#32780;&#27700;&#24179;&#32423;&#32852;&#20248;&#21270;&#20102;&#33609;&#31295;&#20013;&#30340;&#26102;&#38388;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11462v3 Announce Type: replace-cross  Abstract: Introduced to enhance the efficiency of large language model (LLM) inference, speculative decoding operates by having a smaller model generate a draft. A larger target model then reviews this draft to align with its output, and any acceptance by the target model results in a reduction of the number of the target model runs, ultimately improving efficiency. However, the drafting process in speculative decoding includes slow autoregressive generation and allocates equal time to generating tokens, irrespective of their importance. These inefficiencies collectively contribute to the suboptimal performance of speculative decoding. To further improve LLM inference, we introduce Cascade Speculative Drafting (CS Drafting), a speculative execution algorithm that incorporates two types of cascades. The Vertical Cascade eliminates autoregressive generation from neural models, while the Horizontal Cascade optimizes time allocation in draft
&lt;/p&gt;</description></item><item><title>KnowGPT&#26159;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#33218;&#32769;&#34382;&#26426;&#26500;&#24314;&#26368;&#36866;&#21512;&#27599;&#20010;&#38382;&#39064;&#30340;&#25552;&#31034;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25552;&#21319;&#20102;&#30693;&#35782;&#27880;&#20837;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.06185</link><description>&lt;p&gt;
KnowGPT&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
KnowGPT: Black-Box Knowledge Injection for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06185
&lt;/p&gt;
&lt;p&gt;
KnowGPT&#26159;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#33218;&#32769;&#34382;&#26426;&#26500;&#24314;&#26368;&#36866;&#21512;&#27599;&#20010;&#38382;&#39064;&#30340;&#25552;&#31034;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25552;&#21319;&#20102;&#30693;&#35782;&#27880;&#20837;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#25552;&#20379;&#20114;&#21160;&#24335;API&#65292;&#21487;&#20197;&#20197;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#22238;&#31572;&#24120;&#35265;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#38656;&#35201;&#29305;&#23450;&#39046;&#22495;&#25110;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#30340;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20250;&#32473;&#20986;&#19981;&#20934;&#30830;&#25110;&#19981;&#27491;&#30830;&#30340;&#21709;&#24212;&#65292;&#36825;&#20123;&#30693;&#35782;&#24182;&#26410;&#21253;&#21547;&#22312;&#23427;&#20204;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;LLMs&#24182;&#38750;&#24320;&#28304;&#65292;&#36825;&#20351;&#24471;&#20165;&#20351;&#29992;&#27169;&#22411;API&#27880;&#20837;&#30693;&#35782;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;KnowGPT&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#22312;&#38382;&#31572;&#20013;&#30340;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;&#12290;KnowGPT&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#30693;&#35782;&#22270;&#20013;&#25552;&#21462;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#20026;&#27599;&#20010;&#38382;&#39064;&#26500;&#24314;&#26368;&#21512;&#36866;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;KnowGPT&#26174;&#33879;&#22686;&#24378;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;KnowGPT&#24179;&#22343;&#25913;&#36827;&#20102;23%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06185v2 Announce Type: replace-cross  Abstract: Generative Large Language Models (LLMs), such as ChatGPT, offer interactive APIs that can answer common questions at a human-expert level. However, these models often give inaccurate or incorrect responses when faced with questions requiring domain-specific or professional-specific knowledge not covered in their training corpus. Furthermore, many state-of-the-art LLMs are not open-source, making it challenging to inject knowledge with model APIs only. In this work, we introduce KnowGPT, a black-box knowledge injection framework for LLMs in question answering. KnowGPT leverages deep reinforcement learning (RL) to extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed Bandit (MAB) to construct the most suitable prompt for each question. Our extensive experiments on three benchmark datasets showcase that KnowGPT significantly enhances the existing methods. Notably, KnowGPT achieves an average improvement of 23.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#35299;&#20915;&#22312;&#32447;&#31038;&#21306;&#20013;&#25925;&#20107;&#26816;&#27979;&#22256;&#38590;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;StorySeeker&#24037;&#20855;&#21253;&#65292;&#21253;&#25324;&#35814;&#32454;&#27880;&#37322;&#30340;Reddit&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#65292;&#31361;&#20986;&#20102;&#22312;&#32447;&#21465;&#20107;&#30340;&#25991;&#26412;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#21465;&#20107;&#36328;&#24230;&#26816;&#27979;&#20316;&#20026;&#19968;&#20010;&#26032;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2311.09675</link><description>&lt;p&gt;
&#20154;&#20204;&#22312;&#21738;&#37324;&#22312;&#32447;&#35762;&#25925;&#20107;&#65311;&#36328;&#22312;&#32447;&#31038;&#21306;&#30340;&#25925;&#20107;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Where Do People Tell Stories Online? Story Detection Across Online Communities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09675
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#35299;&#20915;&#22312;&#32447;&#31038;&#21306;&#20013;&#25925;&#20107;&#26816;&#27979;&#22256;&#38590;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;StorySeeker&#24037;&#20855;&#21253;&#65292;&#21253;&#25324;&#35814;&#32454;&#27880;&#37322;&#30340;Reddit&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#65292;&#31361;&#20986;&#20102;&#22312;&#32447;&#21465;&#20107;&#30340;&#25991;&#26412;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#21465;&#20107;&#36328;&#24230;&#26816;&#27979;&#20316;&#20026;&#19968;&#20010;&#26032;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#21306;&#20013;&#30340;&#25925;&#20107;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#25925;&#20107;&#20998;&#25955;&#22312;&#31038;&#21306;&#20013;&#65292;&#24182;&#19988;&#19982;&#21333;&#20010;&#25991;&#26412;&#20013;&#30340;&#38750;&#21465;&#20107;&#37096;&#20998;&#20132;&#32455;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#21644;&#21457;&#24067;StorySeeker&#24037;&#20855;&#21253;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#21253;&#21547;502&#20010;Reddit&#24086;&#23376;&#21644;&#35780;&#35770;&#30340;&#20016;&#23500;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#19968;&#20010;&#36866;&#24212;&#31038;&#20132;&#23186;&#20307;&#32972;&#26223;&#30340;&#35814;&#32454;&#30340;&#20195;&#30721;&#20070;&#65292;&#20197;&#21450;&#29992;&#20110;&#22312;&#25991;&#26723;&#21644;&#36328;&#24230;&#32423;&#21035;&#39044;&#27979;&#21465;&#20107;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#20174;&#25968;&#30334;&#20010;&#27969;&#34892;&#30340;&#33521;&#35821;Reddit&#31038;&#21306;&#20013;&#25277;&#26679;&#32780;&#26469;&#65292;&#28085;&#30422;&#20102;33&#20010;&#20027;&#39064;&#31867;&#21035;&#65292;&#23427;&#21253;&#21547;&#20102;&#32454;&#31890;&#24230;&#30340;&#19987;&#23478;&#27880;&#37322;&#65292;&#21253;&#25324;&#20108;&#20803;&#25925;&#20107;&#26631;&#31614;&#65292;&#25925;&#20107;&#36328;&#24230;&#21644;&#20107;&#20214;&#36328;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#22312;&#32447;&#21465;&#20107;&#30340;&#29420;&#29305;&#25991;&#26412;&#29305;&#24449;&#65292;&#37325;&#28857;&#20851;&#27880;&#21465;&#20107;&#36328;&#24230;&#26816;&#27979;&#65292;&#36825;&#26159;&#25105;&#20204;&#24341;&#20837;&#30340;&#19968;&#20010;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#22823;&#35268;&#27169;&#21465;&#20107;&#30340;&#20998;&#24067;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09675v2 Announce Type: replace  Abstract: Story detection in online communities is a challenging task as stories are scattered across communities and interwoven with non-storytelling spans within a single text. We address this challenge by building and releasing the StorySeeker toolkit, including a richly annotated dataset of 502 Reddit posts and comments, a detailed codebook adapted to the social media context, and models to predict storytelling at the document and span level. Our dataset is sampled from hundreds of popular English-language Reddit communities ranging across 33 topic categories, and it contains fine-grained expert annotations, including binary story labels, story spans, and event spans. We evaluate a range of detection methods using our data, and we identify the distinctive textual features of online storytelling, focusing on storytelling span detection, which we introduce as a new task. We illuminate distributional characteristics of storytelling on a large
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KBQA&#26550;&#26500;FuSIC-KBQA&#65292;&#36890;&#36807;&#22810;&#20010;&#28304;&#35757;&#32451;&#30340;&#21484;&#22238;&#22120;&#25191;&#34892;KB&#26816;&#32034;&#65292;&#22312;LLM&#30340;&#37325;&#26032;&#25490;&#24207;&#21518;&#20197;&#27492;&#20316;&#20026;LLM&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36755;&#20837;&#26469;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#25191;&#34892;&#24341;&#23548;&#21453;&#39304;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2311.08894</link><description>&lt;p&gt;
&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#30693;&#35782;&#24211;&#38382;&#31572;&#65306;&#34701;&#21512;&#30417;&#30563;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08894
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KBQA&#26550;&#26500;FuSIC-KBQA&#65292;&#36890;&#36807;&#22810;&#20010;&#28304;&#35757;&#32451;&#30340;&#21484;&#22238;&#22120;&#25191;&#34892;KB&#26816;&#32034;&#65292;&#22312;LLM&#30340;&#37325;&#26032;&#25490;&#24207;&#21518;&#20197;&#27492;&#20316;&#20026;LLM&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36755;&#20837;&#26469;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#25191;&#34892;&#24341;&#23548;&#21453;&#39304;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#26550;&#26500;&#38656;&#35201;&#22823;&#37327;&#27880;&#37322;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#37096;&#32626;&#26102;&#25104;&#26412;&#39640;&#19988;&#32791;&#26102;&#12290;&#25105;&#20204;&#25552;&#20986;&#23569;&#26679;&#26412;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;KBQA&#30340;&#38382;&#39064;&#65292;&#30446;&#26631;&#22495;&#20165;&#25552;&#20379;&#23569;&#37327;&#26631;&#35760;&#31034;&#20363;&#65292;&#20294;&#22312;&#28304;&#22495;&#20013;&#26377;&#22823;&#37327;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#38598;&#21487;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FuSIC-KBQA&#30340;&#26032;&#22411;KBQA&#26550;&#26500;&#65292;&#23427;&#20351;&#29992;&#22810;&#20010;&#32463;&#36807;&#28304;&#22521;&#35757;&#30340;&#21484;&#22238;&#22120;&#25191;&#34892;KB&#26816;&#32034;&#65292;&#20351;&#29992;LLM&#37325;&#26032;&#25490;&#24207;&#65292;&#23558;&#27492;&#20316;&#20026;LLM&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#36755;&#20837;&#20197;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#65292;&#36827;&#19968;&#27493;&#20351;&#29992;&#25191;&#34892;&#24341;&#23548;&#21453;&#39304;&#36827;&#34892;&#32454;&#21270;&#12290;&#22312;&#22235;&#23545;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#28304;-&#30446;&#26631;KBQA&#23545;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FuSIC-KBQA&#26126;&#26174;&#20248;&#20110;&#20026;&#27492;&#35774;&#32622;&#35843;&#25972;&#30340;SoTA KBQA&#27169;&#22411;&#12290;&#22312;&#39046;&#22495;&#20869;&#35774;&#32622;&#30340;&#39069;&#22806;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#26102;&#65292;FuSIC-KBQA&#20063;&#20248;&#20110;SoTA KBQA&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08894v2 Announce Type: replace-cross  Abstract: Existing Knowledge Base Question Answering (KBQA) architectures are hungry for annotated data, which make them costly and time-consuming to deploy. We introduce the problem of few-shot transfer learning for KBQA, where the target domain offers only a few labeled examples, but a large labeled training dataset is available in a source domain. We propose a novel KBQA architecture called FuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers, re-ranks using an LLM and uses this as input for LLM few-shot in-context learning to generate logical forms, which are further refined using execution-guided feedback. Experiments over four source-target KBQA pairs of varying complexity show that FuSIC-KBQA significantly outperforms adaptations of SoTA KBQA models for this setting. Additional experiments in the in-domain setting show that FuSIC-KBQA also outperforms SoTA KBQA models when training data is limited.
&lt;/p&gt;</description></item><item><title>&#32454;&#35843;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20998;&#35299;&#29983;&#25104;&#22120;&#65292;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#26469;&#21327;&#35843;&#26356;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;</title><link>https://arxiv.org/abs/2310.18338</link><description>&lt;p&gt;
&#32454;&#35843;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21327;&#35843;&#26356;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18338
&lt;/p&gt;
&lt;p&gt;
&#32454;&#35843;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20998;&#35299;&#29983;&#25104;&#22120;&#65292;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#26469;&#21327;&#35843;&#26356;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#26102;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#23545;&#25552;&#31034;&#20998;&#35299;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#22810;&#27493;&#25512;&#29702;&#38382;&#39064;&#30340;&#23581;&#35797;&#21462;&#20915;&#20110;LLM&#21516;&#26102;&#20998;&#35299;&#21644;&#35299;&#20915;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#32570;&#28857;&#26159;&#65292;&#22522;&#30784;LLMs&#36890;&#24120;&#19981;&#21487;&#29992;&#20110;&#24494;&#35843;&#65292;&#20351;&#24471;&#36866;&#24212;&#24615;&#22312;&#35745;&#31639;&#19978;&#26159;&#31105;&#38178;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65288;&#24182;&#35777;&#26126;&#65289;&#38382;&#39064;&#30340;&#20998;&#35299;&#21644;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#26159;&#19981;&#21516;&#30340;&#33021;&#21147;&#65292;&#26368;&#22909;&#36890;&#36807;&#21333;&#20010;&#24222;&#22823;&#30340;LLM&#32780;&#19981;&#26159;&#19968;&#20010;&#25972;&#20307;&#27169;&#22359;&#26469;&#35299;&#20915;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DaSLaM&#65292;&#23427;&#20351;&#29992;&#19968;&#20010;&#20998;&#35299;&#29983;&#25104;&#22120;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#25104;&#38656;&#35201;&#26356;&#23569;&#25512;&#29702;&#27493;&#39588;&#30340;&#23376;&#38382;&#39064;&#12290;&#36825;&#20123;&#23376;&#38382;&#39064;&#30001;&#19968;&#20010;&#27714;&#35299;&#22120;&#26469;&#22238;&#31572;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#30456;&#23545;&#23567;&#22411;&#65288;13B&#21442;&#25968;&#65289;LM&#20316;&#20026;&#20998;&#35299;&#29983;&#25104;&#22120;&#65292;&#25105;&#20204;&#20351;&#29992;&#31574;&#30053;&#26799;&#24230;&#20248;&#21270;&#26469;&#19982;&#20043;&#20132;&#20114;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18338v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive. We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM. We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps. These subproblems are answered by a solver. We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with 
&lt;/p&gt;</description></item><item><title>&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.02174</link><description>&lt;p&gt;
&#35753;&#24490;&#29615;&#30340;&#35810;&#38382;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21028;&#26029;&#20013;&#30340;&#25671;&#25670;
&lt;/p&gt;
&lt;p&gt;
Ask Again, Then Fail: Large Language Models' Vacillations in Judgement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.02174
&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24120;&#24120;&#25671;&#25670;&#19981;&#23450;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#21644;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#24320;&#21457;&#20986;Unwavering-FQ&#26694;&#26550;&#26469;&#25945;&#23548;&#27169;&#22411;&#20445;&#25345;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#30446;&#21069;&#30340;&#20250;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#21518;&#32493;&#38382;&#39064;&#26102;&#24448;&#24448;&#22312;&#20854;&#21028;&#26029;&#19978;&#25671;&#25670;&#19981;&#23450;&#65292;&#21363;&#20351;&#21407;&#22987;&#21028;&#26029;&#26159;&#27491;&#30830;&#30340;&#12290;&#36825;&#31181;&#25671;&#25670;&#23545;&#20110;&#29983;&#25104;&#21487;&#38752;&#22238;&#22797;&#21644;&#24314;&#31435;&#29992;&#25143;&#20449;&#20219;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21518;&#32493;&#38382;&#39064;&#26426;&#21046;&#20197;&#21450;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#30830;&#35748;&#20102;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#23384;&#22312;&#36825;&#31181;&#24773;&#20917;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#29992;&#20110;&#38381;&#28304;&#27169;&#22411;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#35757;&#32451;&#30340;&#26694;&#26550;Unwavering-FQ&#65292;&#36890;&#36807;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#20559;&#22909;&#25968;&#25454;&#26469;&#25945;&#23548;&#35821;&#35328;&#27169;&#22411;&#20445;&#25345;&#20854;&#26368;&#21021;&#30340;&#27491;&#30830;&#21028;&#26029;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20854;&#22686;&#24378;&#27169;&#22411;&#36890;&#29992;&#33021;&#21147;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.02174v2 Announce Type: replace-cross  Abstract: We observe that current conversational language models often waver in their judgements when faced with follow-up questions, even if the original judgement was correct. This wavering presents a significant challenge for generating reliable responses and building user trust. To comprehensively assess this issue, we introduce a Follow-up Questioning Mechanism along with two metrics to quantify this inconsistency, confirming its widespread presence in current language models. To mitigate this issue, we explore various prompting strategies for closed-source models; moreover, we develop a training-based framework Unwavering-FQ that teaches language models to maintain their originally correct judgements through synthesized high-quality preference data. Our experimental results confirm the effectiveness of our framework and its ability to enhance the general capabilities of models (https://github.com/NUSTM/LLMs-Waver-In-Judgements).
&lt;/p&gt;</description></item><item><title>EvalLM&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#36890;&#36807;&#35780;&#20272;&#22810;&#20010;&#36755;&#20986;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#65292;&#30456;&#27604;&#25163;&#21160;&#35780;&#20272;&#65292;&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#25776;&#20889;&#26356;&#22810;&#26679;&#21270;&#30340;&#26631;&#20934;&#12289;&#26816;&#26597;&#26356;&#22810;&#36755;&#20986;&#65292;&#36798;&#21040;&#26356;&#28385;&#24847;&#30340;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2309.13633</link><description>&lt;p&gt;
EvalLM: &#20132;&#20114;&#24335;&#35780;&#20272;&#22522;&#20110;&#29992;&#25143;&#23450;&#20041;&#26631;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13633
&lt;/p&gt;
&lt;p&gt;
EvalLM&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#24110;&#21161;&#24320;&#21457;&#20154;&#21592;&#36890;&#36807;&#35780;&#20272;&#22810;&#20010;&#36755;&#20986;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#65292;&#30456;&#27604;&#25163;&#21160;&#35780;&#20272;&#65292;&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#25776;&#20889;&#26356;&#22810;&#26679;&#21270;&#30340;&#26631;&#20934;&#12289;&#26816;&#26597;&#26356;&#22810;&#36755;&#20986;&#65292;&#36798;&#21040;&#26356;&#28385;&#24847;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#22320;&#32452;&#21512;&#25552;&#31034;&#65292;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21407;&#22411;&#21270;&#26032;&#39062;&#30340;&#29983;&#25104;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#35201;&#23558;&#21407;&#22411;&#32454;&#21270;&#20026;&#20135;&#21697;&#65292;&#24320;&#21457;&#20154;&#21592;&#24517;&#39035;&#36890;&#36807;&#35780;&#20272;&#36755;&#20986;&#20197;&#35786;&#26029;&#24369;&#28857;&#26469;&#36845;&#20195;&#20462;&#35746;&#25552;&#31034;&#12290;&#24418;&#25104;&#24615;&#35775;&#35848;&#65288;N=8&#65289;&#26174;&#31034;&#65292;&#24320;&#21457;&#20154;&#21592;&#22312;&#35780;&#20272;&#36755;&#20986;&#26102;&#25237;&#20837;&#20102;&#22823;&#37327;&#31934;&#21147;&#65292;&#22240;&#20026;&#20182;&#20204;&#35780;&#20272;&#29305;&#23450;&#19978;&#19979;&#25991;&#21644;&#20027;&#35266;&#26631;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EvalLM&#65292;&#36825;&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#35780;&#20272;&#29992;&#25143;&#23450;&#20041;&#26631;&#20934;&#19978;&#30340;&#22810;&#20010;&#36755;&#20986;&#26469;&#36845;&#20195;&#25913;&#36827;&#25552;&#31034;&#12290;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26631;&#20934;&#65292;&#20351;&#29992;&#31995;&#32479;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#26469;&#33719;&#24471;&#25552;&#31034;&#22312;&#21738;&#20123;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#25110;&#22833;&#36133;&#30340;&#27010;&#36848;&#65292;&#24182;&#26681;&#25454;&#35780;&#20272;&#22120;&#30340;&#21453;&#39304;&#36827;&#34892;&#25913;&#36827;&#12290;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;&#65288;N=12&#65289;&#26174;&#31034;&#65292;&#19982;&#25163;&#21160;&#35780;&#20272;&#30456;&#27604;&#65292;EvalLM&#26377;&#21161;&#20110;&#24110;&#21161;&#21442;&#19982;&#32773;&#25776;&#20889;&#26356;&#22810;&#26679;&#21270;&#30340;&#26631;&#20934;&#12289;&#26816;&#26597;&#20004;&#20493;&#25968;&#37327;&#30340;&#36755;&#20986;&#65292;&#24182;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13633v2 Announce Type: replace-cross  Abstract: By simply composing prompts, developers can prototype novel generative applications with Large Language Models (LLMs). To refine prototypes into products, however, developers must iteratively revise prompts by evaluating outputs to diagnose weaknesses. Formative interviews (N=8) revealed that developers invest significant effort in manually evaluating outputs as they assess context-specific and subjective criteria. We present EvalLM, an interactive system for iteratively refining prompts by evaluating multiple outputs on user-defined criteria. By describing criteria in natural language, users can employ the system's LLM-based evaluator to get an overview of where prompts excel or fail, and improve these based on the evaluator's feedback. A comparative study (N=12) showed that EvalLM, when compared to manual evaluation, helped participants compose more diverse criteria, examine twice as many outputs, and reach satisfactory promp
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#8220;&#26681;&#25454;&#8221;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21709;&#24212;&#20013;&#21442;&#32771;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#25991;&#26412;&#26469;&#25913;&#36827;&#24341;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65288;QUIP-Score&#65289;&#26469;&#24230;&#37327;&#27169;&#22411;&#29983;&#25104;&#30340;&#31572;&#26696;&#22312;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30340;&#30452;&#25509;&#21457;&#29616;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2305.13252</link><description>&lt;p&gt;
&#26681;&#25454;...&#65306;&#20419;&#20351;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#24341;&#29992;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
"According to ...": Prompting Language Models Improves Quoting from Pre-Training Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13252
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#8220;&#26681;&#25454;&#8221;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21709;&#24212;&#20013;&#21442;&#32771;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#25991;&#26412;&#26469;&#25913;&#36827;&#24341;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65288;QUIP-Score&#65289;&#26469;&#24230;&#37327;&#27169;&#22411;&#29983;&#25104;&#30340;&#31572;&#26696;&#22312;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30340;&#30452;&#25509;&#21457;&#29616;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#20250;&#20135;&#29983;&#24187;&#35273;&#24182;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#65292;&#23613;&#31649;&#23427;&#20204;&#20107;&#20808;&#22312;&#20107;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#21463;&#8220;&#25454;&#24713;&#8221;&#30340;&#26032;&#38395;&#35774;&#22791;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26681;&#25454;&#8221;&#25552;&#31034;&#65306;&#25351;&#23548;LLMs&#23558;&#21709;&#24212;&#19982;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#25991;&#26412;&#30456;&#32852;&#31995;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#31181;&#22522;&#30784;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#25351;&#26631;&#65288;QUIP-Score&#65289;&#65292;&#29992;&#20110;&#34913;&#37327;&#27169;&#22411;&#29983;&#25104;&#30340;&#31572;&#26696;&#22312;&#22522;&#30784;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#30452;&#25509;&#25214;&#21040;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#19977;&#20010;&#35821;&#26009;&#24211;&#65288;&#32500;&#22522;&#30334;&#31185;&#12289;PubMed&#21644;&#32654;&#22269;&#27861;&#24459;&#31246;&#27861;&#20856;&#65289;&#36827;&#34892;&#23454;&#39564;&#26469;&#35828;&#26126;&#36825;&#20123;&#25552;&#31034;&#26681;&#25454;&#25105;&#20204;&#30340;&#24230;&#37327;&#26631;&#20934;&#25913;&#21892;&#20102;&#22522;&#30784;&#24615;&#65292;&#32780;&#19988;&#36890;&#24120;&#36824;&#25552;&#39640;&#20102;&#26368;&#32456;&#20219;&#21153;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#35201;&#27714;&#27169;&#22411;&#20943;&#23569;&#22522;&#30784;&#24615;&#65288;&#25110;&#32773;&#26681;&#25454;&#20854;&#20182;&#35821;&#26009;&#24211;&#65289;&#30340;&#25552;&#31034;&#30830;&#23454;&#38477;&#20302;&#20102;QUIP-Score&#65292;&#34920;&#26126;LLMs&#26377;&#33021;&#21147;&#26681;&#25454;&#35201;&#27714;&#22686;&#21152;&#25110;&#20943;&#23569;&#22522;&#30784;&#24615;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.13252v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data. Inspired by the journalistic device of "according to sources", we propose according-to prompting: directing LLMs to ground responses against previously observed text. To quantify this grounding, we propose a novel evaluation metric (QUIP-Score) that measures the extent to which model-produced answers are directly found in underlying text corpora. We illustrate with experiments on three corpora (Wikipedia, PubMed, and the U.S. legal tax code) that these prompts improve grounding under our metrics, with the additional benefit of often improving end-task performance. Furthermore, prompts that ask the model to decrease grounding (or to ground to other corpora) indeed decrease QUIP-Score, indicating the ability of LLMs to increase or decrease grounded generations on request.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Frustratingly Simple Decoding (FSD)&#30340;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21453;&#35821;&#35328;&#27169;&#22411;&#26469;&#24809;&#32602;&#26410;&#26469;&#29983;&#25104;&#24050;&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#36890;&#36807;&#24341;&#20837;&#20960;&#20046;&#26080;&#39069;&#22806;&#35745;&#31639;&#24320;&#38144;&#23601;&#33021;&#26377;&#25928;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;&#26680;&#37319;&#26679;&#65289;&#12290;</title><link>https://arxiv.org/abs/2305.12675</link><description>&lt;p&gt;
&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#30340;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#35299;&#30721;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Frustratingly Simple Decoding Method for Neural Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12675
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Frustratingly Simple Decoding (FSD)&#30340;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21453;&#35821;&#35328;&#27169;&#22411;&#26469;&#24809;&#32602;&#26410;&#26469;&#29983;&#25104;&#24050;&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#36890;&#36807;&#24341;&#20837;&#20960;&#20046;&#26080;&#39069;&#22806;&#35745;&#31639;&#24320;&#38144;&#23601;&#33021;&#26377;&#25928;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#65288;&#22914;&#26680;&#37319;&#26679;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20196;&#20154;&#27822;&#20007;&#22320;&#31616;&#21333;&#12289;&#36229;&#32423;&#39640;&#25928;&#19988;&#20986;&#22855;&#26377;&#25928;&#30340;&#35299;&#30721;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;Frustratingly Simple Decoding&#65288;FSD&#65289;&#65292;&#29992;&#20110;&#31070;&#32463;&#25991;&#26412;&#29983;&#25104;&#12290;FSD&#30340;&#24605;&#24819;&#24456;&#31616;&#21333;&#65306;&#25105;&#20204;&#26500;&#24314;&#19968;&#20010;&#22522;&#20110;&#20808;&#21069;&#29983;&#25104;&#25991;&#26412;&#30340;&#21453;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#36825;&#20010;&#21453;&#35821;&#35328;&#27169;&#22411;&#26469;&#24809;&#32602;&#26410;&#26469;&#29983;&#25104;&#24050;&#29983;&#25104;&#30340;&#20869;&#23481;&#12290;&#36825;&#20010;&#21453;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#24471;&#38750;&#24120;&#31616;&#21333;&#65292;&#21487;&#20197;&#26159;&#19968;&#20010;n-gram&#35821;&#35328;&#27169;&#22411;&#25110;&#32773;&#19968;&#20010;&#21521;&#37327;&#21270;&#21464;&#20307;&#12290;&#22240;&#27492;&#65292;FSD&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#35745;&#31639;&#24320;&#38144;&#24494;&#20046;&#20854;&#24494;&#65288;FSD&#36895;&#24230;&#21487;&#20197;&#20687;&#36138;&#23146;&#25628;&#32034;&#19968;&#26679;&#24555;&#65289;&#12290;&#23613;&#31649;&#22914;&#27492;&#31616;&#21333;&#65292;FSD&#21364;&#20986;&#22855;&#22320;&#26377;&#25928;&#65307;&#23454;&#39564;&#35777;&#26126;&#65292;FSD&#21487;&#20197;&#32988;&#36807;&#36804;&#20170;&#20026;&#27490;&#30340;&#32463;&#20856;&#26041;&#27861;&#65288;&#21363;&#26680;&#37319;&#26679;&#26041;&#27861;&#65289;&#20197;&#21450;&#26368;&#36817;&#25552;&#20986;&#30340;&#19968;&#20123;&#24378;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.12675v2 Announce Type: replace  Abstract: We introduce a frustratingly simple, super efficient and surprisingly effective decoding method, which we call Frustratingly Simple Decoding (FSD), for neural text generation. The idea behind FSD is straightforward: we build an anti-LM based on previously generated text and use this anti-LM to penalize future generation of what has been generated. The anti-LM can be implemented as simple as an n-gram language model or a vectorized variant. In this way, FSD introduces no extra model parameters and negligible computational overhead (FSD can be as fast as greedy search). Despite the simplicity, FSD is surprisingly effective; Experiments show that FSD can outperform the canonical methods to date (i.e., nucleus sampling) as well as several strong baselines that were proposed recently.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#22686;&#24378;&#26159;&#19968;&#31181;&#21033;&#29992;dropout&#25110;PCA&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36716;&#25442;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22312;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#28145;&#24230;&#22686;&#24378;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30417;&#30563;&#38382;&#39064;&#19978;&#25928;&#26524;&#30456;&#21453;&#12290;</title><link>https://arxiv.org/abs/2303.14537</link><description>&lt;p&gt;
&#28145;&#24230;&#22686;&#24378;&#65306;&#22312;&#28608;&#27963;&#31354;&#38388;&#20013;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Deep Augmentation: Self-Supervised Learning with Transformations in Activation Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.14537
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22686;&#24378;&#26159;&#19968;&#31181;&#21033;&#29992;dropout&#25110;PCA&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36716;&#25442;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22312;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#28145;&#24230;&#22686;&#24378;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30417;&#30563;&#38382;&#39064;&#19978;&#25928;&#26524;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36749;&#23398;&#25110;PCA&#26469;&#36716;&#25442;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30446;&#26631;&#23618;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#23398;&#20064;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#26469;&#23637;&#31034;&#28145;&#24230;&#22686;&#24378;&#12290; &#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#65292;&#22914;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#19978;&#28145;&#24230;&#22686;&#24378;&#33021;&#22815;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30456;&#24212;&#30340;&#30417;&#30563;&#38382;&#39064;&#19978;&#35266;&#23519;&#21040;&#30456;&#21453;&#30340;&#25928;&#26524;&#12290; &#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#28145;&#24230;&#22686;&#24378;&#20943;&#36731;&#20102;&#23618;&#20043;&#38388;&#30340;&#30456;&#20114;&#36866;&#24212;&#65292;&#21363;"&#23849;&#28291;"&#24418;&#24335;&#30340;&#38382;&#39064;&#12290; &#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#21046;&#23450;&#20102;&#19968;&#31181;&#36873;&#25321;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65307;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#29992;&#28145;&#24230;&#22686;&#24378;&#23450;&#20301;&#26356;&#28145;&#23618;&#27425;&#30340;&#23618;&#35201;&#20248;&#20110;&#22686;&#24378;&#36755;&#20837;&#25968;&#25454;&#12290; &#36825;&#31181;&#26041;&#27861;&#30340;&#31616;&#21333;&#32593;&#32476;&#21644;&#27169;&#24577;&#26080;&#20851;&#24615;&#20351;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.14537v2 Announce Type: replace-cross  Abstract: We introduce Deep Augmentation, an approach to implicit data augmentation using dropout or PCA to transform a targeted layer within a neural network to improve performance and generalization. We demonstrate Deep Augmentation through extensive experiments on contrastive learning tasks in NLP, computer vision, and graph learning. We observe substantial performance gains with Transformers, ResNets, and Graph Neural Networks as the underlying models in contrastive learning, but observe inverse effects on the corresponding supervised problems. Our analysis suggests that Deep Augmentation alleviates co-adaption between layers, a form of "collapse." We use this observation to formulate a method for selecting which layer to target; in particular, our experimentation reveals that targeting deeper layers with Deep Augmentation outperforms augmenting the input data. The simple network- and modality-agnostic nature of this approach enables
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Automate-CoT&#31574;&#30053;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#33258;&#21160;&#22686;&#21152;&#21512;&#29702;&#38142;&#65292;&#24182;&#20462;&#21098;&#20302;&#36136;&#37327;&#38142;&#65292;&#26500;&#24314;&#22522;&#20110;&#26631;&#31614;&#30340;&#26426;&#22120;&#29983;&#25104;&#29702;&#30001;&#38142;&#30340;&#20505;&#36873;&#27744;&#65292;&#26368;&#32456;&#36873;&#25321;&#26368;&#20339;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2302.12822</link><description>&lt;p&gt;
&#29992;&#26631;&#27880;&#25968;&#25454;&#36827;&#34892;&#38142;&#24335;&#24605;&#32500;&#30340;&#33258;&#21160;&#25552;&#31034;&#22686;&#24378;&#19982;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.12822
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Automate-CoT&#31574;&#30053;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#33258;&#21160;&#22686;&#21152;&#21512;&#29702;&#38142;&#65292;&#24182;&#20462;&#21098;&#20302;&#36136;&#37327;&#38142;&#65292;&#26500;&#24314;&#22522;&#20110;&#26631;&#31614;&#30340;&#26426;&#22120;&#29983;&#25104;&#29702;&#30001;&#38142;&#30340;&#20505;&#36873;&#27744;&#65292;&#26368;&#32456;&#36873;&#25321;&#26368;&#20339;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-thought prompting&#65288;CoT&#65289;&#25512;&#36827;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;CoT&#30740;&#31350;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#20154;&#24037;&#27880;&#37322;&#30340;&#21512;&#29702;&#38142;&#65292;&#20197;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#23384;&#22312;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#20294;&#27809;&#26377;&#20154;&#24037;&#27880;&#37322;&#30340;&#21512;&#29702;&#38142;&#30340;&#24212;&#29992;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#36825;&#20026;CoT&#25552;&#31034;&#24212;&#29992;&#20110;&#36825;&#20123;&#36890;&#29992;&#20219;&#21153;&#24102;&#26469;&#20102;&#38556;&#30861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31574;&#30053;&#65292;Automate-CoT&#65288;&#33258;&#21160;&#25552;&#31034;&#22686;&#24378;&#19982;&#36873;&#25321;&#19982;&#38142;&#24605;&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#33258;&#21160;&#22686;&#24378;&#21512;&#29702;&#38142;&#65292;&#28982;&#21518;&#20462;&#21098;&#20302;&#36136;&#37327;&#38142;&#65292;&#22522;&#20110;&#26631;&#31614;&#26500;&#24314;&#22522;&#20110;&#26426;&#22120;&#29983;&#25104;&#30340;&#29702;&#30001;&#38142;&#30340;&#20505;&#36873;&#27744;&#12290;&#26368;&#21518;&#65292;&#23427;&#36873;&#25321;&#20102;&#20960;&#20010;&#29702;&#30001;&#38142;&#30340;&#26368;&#20339;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.12822v2 Announce Type: replace  Abstract: Chain-of-thought prompting (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in arithmetic, commonsense, and symbolic reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt the language model, which poses challenges for real-world applications where labeled training data is available without human-annotated rational chains. This creates barriers to applications of CoT prompting to these general tasks. This paper proposes a new strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoTs by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machine-generated rationale chains based on the labels. Finally, it selects the optimal combination of several rationale chains
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#27969;&#30340;&#35821;&#38899;&#36716;&#25442;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#65292;&#25552;&#21319;&#20102;&#21457;&#38899;&#36136;&#37327;&#65292;&#24182;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2210.17264</link><description>&lt;p&gt;
&#20855;&#26377;&#22522;&#20110;&#27969;&#30340;&#35821;&#38899;&#36716;&#25442;&#30340;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#26041;&#27861;&#20197;&#25913;&#21892;&#21457;&#38899;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Text-To-Speech with Flow-based Voice Conversion for Improved Pronunciation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.17264
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#27969;&#30340;&#35821;&#38899;&#36716;&#25442;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#65292;&#25552;&#21319;&#20102;&#21457;&#38899;&#36136;&#37327;&#65292;&#24182;&#22312;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31471;&#21040;&#31471;&#36328;&#35821;&#35328;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20445;&#30041;&#30446;&#26631;&#35821;&#35328;&#30340;&#21457;&#38899;&#65292;&#32780;&#19981;&#32771;&#34385;&#21407;&#22987;&#35828;&#35805;&#32773;&#30340;&#35821;&#35328;&#12290;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#22522;&#20110;&#38750;&#27880;&#24847;&#21147;Tacotron&#26550;&#26500;&#65292;&#20854;&#20013;&#35299;&#30721;&#22120;&#24050;&#32463;&#34987;&#26367;&#25442;&#20026;&#19968;&#20010;&#21463;&#35762;&#35805;&#32773;&#36523;&#20221;&#26465;&#20214;&#30340;&#24402;&#19968;&#21270;&#27969;&#32593;&#32476;&#65292;&#20801;&#35768;&#36890;&#36807;&#30456;&#21516;&#27169;&#22411;&#25191;&#34892;TTS&#21644;&#22768;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#65292;&#22240;&#20026;&#22266;&#26377;&#30340;&#35821;&#35328;&#20869;&#23481;&#21644;&#35828;&#35805;&#32773;&#36523;&#20221;&#35299;&#32806;&#12290;&#22312;&#36328;&#35821;&#35328;&#35774;&#32622;&#20013;&#20351;&#29992;&#26102;&#65292;&#39318;&#20808;&#20351;&#29992;&#30446;&#26631;&#35821;&#35328;&#30340;&#26412;&#22320;&#21457;&#38899;&#32773;&#20135;&#29983;&#22768;&#23398;&#29305;&#24449;&#65292;&#28982;&#21518;&#36890;&#36807;&#30456;&#21516;&#27169;&#22411;&#24212;&#29992;&#22768;&#38899;&#36716;&#25442;&#65292;&#20197;&#23558;&#36825;&#20123;&#29305;&#24449;&#36716;&#25442;&#20026;&#30446;&#26631;&#35828;&#35805;&#32773;&#30340;&#22768;&#38899;&#12290;&#25105;&#20204;&#36890;&#36807;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#22522;&#20934;&#36328;&#35821;&#35328;&#21512;&#25104;&#30456;&#27604;&#20855;&#26377;&#19968;&#23450;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.17264v2 Announce Type: replace-cross  Abstract: This paper presents a method for end-to-end cross-lingual text-to-speech (TTS) which aims to preserve the target language's pronunciation regardless of the original speaker's language. The model used is based on a non-attentive Tacotron architecture, where the decoder has been replaced with a normalizing flow network conditioned on the speaker identity, allowing both TTS and voice conversion (VC) to be performed by the same model due to the inherent linguistic content and speaker identity disentanglement. When used in a cross-lingual setting, acoustic features are initially produced with a native speaker of the target language and then voice conversion is applied by the same model in order to convert these features to the target speaker's voice. We verify through objective and subjective evaluations that our method can have benefits compared to baseline cross-lingual synthesis. By including speakers averaging 7.5 minutes of spe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22810;&#25968;&#25237;&#31080;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#23433;&#20840;&#20027;&#39064;&#20998;&#31867;&#24037;&#20855;&#65292;&#30456;&#27604;&#20110;21&#20010;&#21333;&#29420;&#27169;&#22411;&#65292;&#35813;&#24037;&#20855;&#22312;&#26816;&#27979;&#32593;&#32476;&#23433;&#20840;&#30456;&#20851;&#25991;&#26412;&#26102;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#20551;&#38451;&#24615;&#21644;&#20551;&#38452;&#24615;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#25968;&#21313;&#19975;&#20221;&#25991;&#26723;&#12290;</title><link>https://arxiv.org/abs/2109.02473</link><description>&lt;p&gt;
&#19968;&#31181;&#24378;&#22823;&#30340;&#32593;&#32476;&#23433;&#20840;&#20027;&#39064;&#20998;&#31867;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
A Robust Cybersecurity Topic Classification Tool
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2109.02473
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22810;&#25968;&#25237;&#31080;&#30340;&#26041;&#24335;&#65292;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#23433;&#20840;&#20027;&#39064;&#20998;&#31867;&#24037;&#20855;&#65292;&#30456;&#27604;&#20110;21&#20010;&#21333;&#29420;&#27169;&#22411;&#65292;&#35813;&#24037;&#20855;&#22312;&#26816;&#27979;&#32593;&#32476;&#23433;&#20840;&#30456;&#20851;&#25991;&#26412;&#26102;&#34920;&#29616;&#20986;&#36739;&#20302;&#30340;&#20551;&#38451;&#24615;&#21644;&#20551;&#38452;&#24615;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#25968;&#21313;&#19975;&#20221;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#19977;&#20010;&#20114;&#32852;&#32593;&#25991;&#26412;&#20449;&#24687;&#26469;&#28304;&#65288;Reddit&#65292;Stackexchange&#65292;Arxiv&#65289;&#30340;&#29992;&#25143;&#23450;&#20041;&#30340;&#26631;&#31614;&#65292;&#35757;&#32451;&#20102;21&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#33258;&#28982;&#25991;&#26412;&#20013;&#30340;&#32593;&#32476;&#23433;&#20840;&#35752;&#35770;&#30340;&#20027;&#39064;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#27599;&#20010;&#27169;&#22411;&#22312;&#20132;&#21449;&#39564;&#35777;&#23454;&#39564;&#20013;&#30340;&#20551;&#38451;&#24615;&#21644;&#20551;&#38452;&#24615;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#23433;&#20840;&#20027;&#39064;&#20998;&#31867;&#65288;CTC&#65289;&#24037;&#20855;&#65292;&#35813;&#24037;&#20855;&#23558;21&#20010;&#35757;&#32451;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22810;&#25968;&#25237;&#31080;&#20316;&#20026;&#26816;&#27979;&#32593;&#32476;&#23433;&#20840;&#30456;&#20851;&#25991;&#26412;&#30340;&#20915;&#31574;&#26426;&#21046;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;CTC&#24037;&#20855;&#30340;&#22810;&#25968;&#25237;&#31080;&#26426;&#21046;&#24179;&#22343;&#25552;&#20379;&#20102;&#27604;21&#20010;&#21333;&#29420;&#27169;&#22411;&#26356;&#20302;&#30340;&#20551;&#38452;&#24615;&#21644;&#20551;&#38451;&#24615;&#29575;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CTC&#24037;&#20855;&#21487;&#25193;&#23637;&#21040;&#25968;&#21313;&#19975;&#20221;&#25991;&#26723;&#65292;&#32780;&#20854;&#22681;&#38047;&#26102;&#38388;&#22823;&#32422;&#20026;&#20960;&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2109.02473v4 Announce Type: replace-cross  Abstract: In this research, we use user defined labels from three internet text sources (Reddit, Stackexchange, Arxiv) to train 21 different machine learning models for the topic classification task of detecting cybersecurity discussions in natural text. We analyze the false positive and false negative rates of each of the 21 model's in a cross validation experiment. Then we present a Cybersecurity Topic Classification (CTC) tool, which takes the majority vote of the 21 trained machine learning models as the decision mechanism for detecting cybersecurity related text. We also show that the majority vote mechanism of the CTC tool provides lower false negative and false positive rates on average than any of the 21 individual models. We show that the CTC tool is scalable to the hundreds of thousands of documents with a wall clock time on the order of hours.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24207;&#21015;&#26631;&#27880;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#25506;&#32034;&#20102;&#25552;&#39640;&#23427;&#20204;&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.14556</link><description>&lt;p&gt;
&#19981;&#19968;&#23450;&#24635;&#26159;&#21521;&#21491;&#30475;&#65306;&#30740;&#31350;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24207;&#21015;&#26631;&#27880;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Do Not (Always) Look Right: Investigating the Capabilities of Decoder-Based Large Language Models for Sequence Labeling. (arXiv:2401.14556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24207;&#21015;&#26631;&#27880;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#25506;&#32034;&#20102;&#25552;&#39640;&#23427;&#20204;&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30446;&#26631;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#34429;&#28982;&#19982;&#30456;&#20284;&#35268;&#27169;&#30340;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#35299;&#30721;&#22120;&#30456;&#27604;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;MLM-based&#32534;&#30721;&#22120;&#22987;&#32456;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#26368;&#36817;&#23558;&#35299;&#30721;&#22120;&#27169;&#22411;&#25193;&#23637;&#33267;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#36235;&#21183;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#19982;MLM-based&#32534;&#30721;&#22120;&#30456;&#25239;&#34913;&#12290;&#23613;&#31649;&#35268;&#27169;&#25193;&#22823;&#20102;&#23427;&#20204;&#22312;NLU&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#20294;LLMs&#22312;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#20219;&#21153;&#20013;&#65292;&#23588;&#20854;&#26159;&#24207;&#21015;&#26631;&#27880;&#65288;SL&#65289;&#20219;&#21153;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#20110;SOTA&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;SL&#24615;&#33021;&#26159;&#30001;&#20854;&#22266;&#26377;&#30340;&#38480;&#21046;&#20915;&#23450;&#30340;&#36824;&#26159;&#21487;&#20197;&#25913;&#36827;&#30340;&#65292;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#25913;&#36827;"&#24320;&#25918;&#24335;"LLMs&#65288;Llama2&#21644;Mistral&#65289;&#22312;IE&#20219;&#21153;&#20013;&#30340;SL&#24615;&#33021;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35299;&#30721;&#22120;&#22359;&#32452;&#20869;&#30340;&#21452;&#21521;&#20449;&#24687;&#27969;&#65292;&#24212;&#29992;&#20102;&#23618;&#27425;&#36880;&#23618;&#31227;&#38500;&#25110;&#21551;&#29992;&#22240;&#26524;&#25513;&#30721;&#65288;CM&#65289;&#36827;&#26469;LLM&#24494;&#35843;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models based on masked language modeling (MLM) objective excel in natural language understanding (NLU) tasks. While fine-tuned MLM-based encoders consistently outperform causal language modeling decoders of comparable size, a recent trend of scaling decoder models to multiple billion parameters resulted in large language models (LLMs), making them competitive with MLM-based encoders. Although scale amplifies their prowess in NLU tasks, LLMs fall short of SOTA results in information extraction (IE) tasks, many framed as sequence labeling (SL). However, whether this is an intrinsic limitation of LLMs or whether their SL performance can be improved remains unclear. To address this, we explore strategies to enhance the SL performance of "open" LLMs (Llama2 and Mistral) on IE tasks. We investigate bidirectional information flow within groups of decoder blocks, applying layer-wise removal or enforcement of the causal mask (CM) during LLM fine-tuning. This approach yields
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20165;&#26377;&#30340;&#23569;&#37327;&#24494;&#23567;&#22810;&#35821;&#35328;&#24179;&#34892;&#25968;&#25454;&#26469;&#22686;&#24378;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#32763;&#35793;&#33021;&#21147;&#65292;&#23454;&#29616;&#22823;&#24133;&#24230;&#30340;&#38750;&#33521;&#25991;&#25972;&#20307;&#25913;&#36827;&#65292;&#24182;&#20445;&#25345;&#33521;&#25991;&#26041;&#21521;&#19978;&#30340;&#24615;&#33021;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#25277;&#21462;&#30340;&#23569;&#37327;&#26041;&#21521;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#25972;&#20307;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.12413</link><description>&lt;p&gt;
100&#20010;&#26679;&#26412;&#21487;&#20197;&#36208;&#22810;&#36828;&#65311;&#36890;&#36807;&#24494;&#23567;&#30340;&#22810;&#35821;&#35328;&#24179;&#34892;&#25968;&#25454;&#35299;&#38145;&#20840;&#38754;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual Translation via Tiny Multi-Parallel Data. (arXiv:2401.12413v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#20165;&#26377;&#30340;&#23569;&#37327;&#24494;&#23567;&#22810;&#35821;&#35328;&#24179;&#34892;&#25968;&#25454;&#26469;&#22686;&#24378;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#32763;&#35793;&#33021;&#21147;&#65292;&#23454;&#29616;&#22823;&#24133;&#24230;&#30340;&#38750;&#33521;&#25991;&#25972;&#20307;&#25913;&#36827;&#65292;&#24182;&#20445;&#25345;&#33521;&#25991;&#26041;&#21521;&#19978;&#30340;&#24615;&#33021;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#25277;&#21462;&#30340;&#23569;&#37327;&#26041;&#21521;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#25972;&#20307;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#32763;&#35793;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#26088;&#22312;&#22312;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#65288;MMT&#65289;&#20013;&#32763;&#35793;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#23545;&#12290;&#19968;&#31181;&#24120;&#35265;&#20294;&#36164;&#28304;&#28040;&#32791;&#36739;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23613;&#21487;&#33021;&#25366;&#25496;&#26356;&#22810;&#30340;&#32763;&#35793;&#26041;&#21521;&#24182;&#28155;&#21152;&#21040;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20351;&#29992;&#20165;&#26377;&#30340;&#23569;&#37327;&#24494;&#23567;&#22810;&#35821;&#35328;&#24179;&#34892;&#25968;&#25454;&#26469;&#20248;&#21270;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#22312;EC30&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#20351;&#29992;100&#20010;&#22810;&#35821;&#35328;&#24179;&#34892;&#26679;&#26412;&#23601;&#33021;&#22815;&#23454;&#29616;+21.7 ChrF&#38750;&#33521;&#25991;&#25972;&#20307;&#25913;&#36827;&#65288;870&#20010;&#26041;&#21521;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#26041;&#21521;&#19978;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#24494;&#35843;&#25968;&#25454;&#30340;&#35268;&#27169;&#25928;&#24212;&#21644;&#20854;&#36716;&#31227;&#33021;&#21147;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#22312;&#19968;&#20010;&#23567;&#30340;&#12289;&#38543;&#26426;&#25277;&#21462;&#30340;&#26041;&#21521;&#38598;&#65288;10%&#65289;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;&#25972;&#20307;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25152;&#24471;&#21040;&#30340;&#38750;&#33521;&#25991;&#24615;&#33021;&#19982;&#33521;&#25991;&#24615;&#33021;&#38750;&#24120;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot translation is an open problem, aiming to translate between language pairs unseen during training in Multilingual Machine Translation (MMT). A common, albeit resource-consuming, solution is to mine as many translation directions as possible to add to the parallel corpus. In this paper, we show that the zero-shot capability of an English-centric model can be easily enhanced by fine-tuning with a very small amount of multi-parallel data. For example, on the EC30 dataset, we show that up to +21.7 ChrF non-English overall improvements (870 directions) can be achieved by using only 100 multi-parallel samples, meanwhile preserving capability in English-centric directions. We further study the size effect of fine-tuning data and its transfer capabilities. Surprisingly, our empirical analysis shows that comparable overall improvements can be achieved even through fine-tuning in a small, randomly sampled direction set (10\%). Also, the resulting non-English performance is quite close 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#23558;FAIR&#25968;&#25454;&#21407;&#21017;&#23884;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#25972;&#21512;&#25351;&#21335;&#21644;&#26816;&#26597;&#28165;&#21333;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22312;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#21644;&#20943;&#36731;&#20559;&#35265;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.11033</link><description>&lt;p&gt;
FAIR&#21040;&#20301;&#65306;&#25105;&#20204;&#22914;&#20309;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#24320;&#21457;&#21644;&#35780;&#20272;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;&#65311;
&lt;/p&gt;
&lt;p&gt;
FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for Large Language Models' Training?. (arXiv:2401.11033v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11033
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#20010;&#23558;FAIR&#25968;&#25454;&#21407;&#21017;&#23884;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#25972;&#21512;&#25351;&#21335;&#21644;&#26816;&#26597;&#28165;&#21333;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22312;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#21644;&#20943;&#36731;&#20559;&#35265;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36827;&#23637;&#20984;&#26174;&#20102;&#36947;&#24503;&#23454;&#36341;&#21644;&#25968;&#25454;&#23436;&#25972;&#24615;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23558;FAIR&#65288;&#21487;&#21457;&#29616;&#12289;&#21487;&#35775;&#38382;&#12289;&#21487;&#20114;&#25805;&#20316;&#12289;&#21487;&#37325;&#29992;&#65289;&#25968;&#25454;&#21407;&#21017;&#23884;&#20837;&#21040;LLM&#35757;&#32451;&#20013;&#30340;&#26694;&#26550;&#12290;&#36825;&#19968;&#26041;&#27861;&#26631;&#24535;&#30528;&#26397;&#30528;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#23454;&#36341;&#30340;&#36716;&#21464;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#23558;FAIR&#25968;&#25454;&#21407;&#21017;&#25972;&#21512;&#21040;LLM&#35757;&#32451;&#20013;&#30340;&#25351;&#21335;&#12290;&#35813;&#20030;&#25514;&#21253;&#25324;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#30340;&#26816;&#26597;&#28165;&#21333;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#23427;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#37325;&#28857;&#26159;&#22312;&#25105;&#20204;&#31526;&#21512;FAIR&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#21644;&#20943;&#36731;&#20559;&#35265;&#12290;&#36825;&#39033;&#24037;&#20316;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21644;&#25968;&#25454;&#31185;&#23398;&#26159;&#37325;&#35201;&#30340;&#36129;&#29486;&#65292;&#20513;&#23548;&#22312;LLMs&#20013;&#20351;&#29992;&#24179;&#34913;&#21644;&#36947;&#24503;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in Large Language Models (LLMs) highlight the need for ethical practices and data integrity. We introduce a framework that embeds FAIR (Findable, Accessible, Interoperable, Reusable) data principles into LLM training. This approach marks a shift towards practices compliant with FAIR standards. Our framework presents guidelines for integrating FAIR data principles into LLM training. This initiative includes a checklist for researchers and developers. We also demonstrate its practical application through a case study focused on bias identification and mitigation in our FAIR-compliant dataset. This work is a significant contribution to AI ethics and data science, advocating for balanced and ethical training methods in LLMs.
&lt;/p&gt;</description></item><item><title>&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36890;&#36807;&#22810;&#38454;&#27573;&#21327;&#20316;&#33976;&#39311;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.08640</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#30693;&#35782;&#33976;&#39311;&#22312;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multistage Collaborative Knowledge Distillation from Large Language Models for Semi-Supervised Sequence Generation. (arXiv:2311.08640v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08640
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36890;&#36807;&#22810;&#38454;&#27573;&#21327;&#20316;&#33976;&#39311;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#65292;&#22312;&#36825;&#31181;&#20219;&#21153;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#22826;&#23569;&#20197;&#33267;&#20110;&#26080;&#27861;&#26377;&#25928;&#22320;&#24494;&#35843;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#24615;&#33021;&#20063;&#19981;&#22815;&#29702;&#24819;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19968;&#20123;&#26114;&#36149;&#19988;&#23545;&#39044;&#35757;&#32451;&#30340; LLM &#19981;&#29087;&#24713;&#30340;&#20219;&#21153;&#65292;&#22914;&#35299;&#26512;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340; LLM &#33976;&#39311;&#20986;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#36890;&#24120;&#27604;&#20854;&#25945;&#24072;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; - &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#30693;&#35782;&#33976;&#39311; (MCKD) - &#29992;&#20110;&#36825;&#20123;&#20219;&#21153;&#12290;MCKD &#39318;&#20808;&#36827;&#34892;&#23569;&#26679;&#26412;&#25552;&#31034;&#65292;&#35753;LLM&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;&#22312;&#27599;&#20010;&#20013;&#38388;&#30693;&#35782;&#33976;&#39311; (KD) &#38454;&#27573;&#65292;&#20351;&#29992;&#20266;&#26631;&#31614;&#25968;&#25454;&#30340;&#19981;&#37325;&#21472;&#20998;&#21306;&#26469;&#35757;&#32451;&#19968;&#23545;&#26032;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#23398;&#29983;&#27169;&#22411;&#20026;&#20854;&#26410;&#35265;&#20998;&#21306;&#29983;&#25104;&#26032;&#30340;&#21644;&#25913;&#36827;&#30340;&#20266;&#26631;&#31614;&#65292;&#22312;&#19979;&#19968;&#20010;&#33976;&#39311;&#38454;&#27573;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study semi-supervised sequence generation tasks where labeled data are too scarce to effectively finetune a model and at the same time few-shot prompting of a large language model (LLM) has suboptimal performance. This happens when a task, such as parsing, is expensive to annotate and also unfamiliar to a pretrained LLM. In this paper, we present a discovery that student models distilled from an in-context learned LLM can often generalize better than their teacher on such tasks. Leveraging this finding, we present a new method -multistage collaborative knowledge distillation from an LLM (MCKD) -- for such tasks. MCKD first few-shot prompts an LLM to produce pseudolabels for unlabeled data. At each intermediate knowledge distillation (KD) stage, a new pair of students is trained on disjoint partitions of the pseudolabeled data. Each student then produces new and improved pseudolabels for its unseen partition to be used in the next stage of distillation. We demonstrate the advantage
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22810;&#20010;&#25552;&#31034;&#31574;&#30053;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#20110;LLMs&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;</title><link>http://arxiv.org/abs/2310.05797</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#20107;&#21518;&#35299;&#37322;&#22120;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Post Hoc Explainers?. (arXiv:2310.05797v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05797
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#22810;&#20010;&#25552;&#31034;&#31574;&#30053;&#65292;&#22635;&#34917;&#20102;&#24403;&#21069;&#23545;&#20110;LLMs&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#32570;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#20013;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#21019;&#26032;&#65292;&#21363;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20351;&#24471;LLM&#33021;&#22815;&#22312;&#25512;&#29702;&#38454;&#27573;&#36890;&#36807;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#23569;&#37327;&#31034;&#20363;&#26469;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#27169;&#22411;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;&#34429;&#28982;LLM&#24050;&#32463;&#34987;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#20294;&#20854;&#22312;&#35299;&#37322;&#20854;&#20182;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#20173;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#23613;&#31649;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#26032;&#35299;&#37322;&#25216;&#26415;&#65292;&#20294;&#24456;&#22810;&#25216;&#26415;&#35201;&#27714;&#23545;&#27169;&#22411;&#20855;&#26377;&#30333;&#30418;&#35775;&#38382;&#26435;&#38480;&#21644;/&#25110;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#65292;&#20984;&#26174;&#20102;&#19979;&#19968;&#20195;&#20107;&#21518;&#35299;&#37322;&#22120;&#30340;&#38656;&#27714;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30740;&#31350;LLM&#35299;&#37322;&#20854;&#20182;&#39044;&#27979;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#26032;&#39062;&#26694;&#26550;&#65306;i&#65289;&#22522;&#20110;&#25200;&#21160;&#30340;ICL&#65292;ii&#65289;&#22522;&#20110;&#39044;&#27979;&#30340;ICL&#65292;iii&#65289;&#22522;&#20110;&#25351;&#20196;&#30340;ICL&#65292;&#21644;iv&#65289;&#22522;&#20110;&#35299;&#37322;&#30340;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are increasingly used as powerful tools for a plethora of natural language processing (NLP) applications. A recent innovation, in-context learning (ICL), enables LLMs to learn new tasks by supplying a few examples in the prompt during inference time, thereby eliminating the need for model fine-tuning. While LLMs have been utilized in several applications, their applicability in explaining the behavior of other models remains relatively unexplored. Despite the growing number of new explanation techniques, many require white-box access to the model and/or are computationally expensive, highlighting a need for next-generation post hoc explainers. In this work, we present the first framework to study the effectiveness of LLMs in explaining other predictive models. More specifically, we propose a novel framework encompassing multiple prompting strategies: i) Perturbation-based ICL, ii) Prediction-based ICL, iii) Instruction-based ICL, and iv) Explanation-based I
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;11&#31181;&#25193;&#23637;&#25216;&#26415;&#12289;12&#20010;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;24&#20010;&#26816;&#32034;&#27169;&#22411;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#25110;&#25991;&#26723;&#25193;&#23637;&#30340;&#25928;&#26524;&#19982;&#26816;&#32034;&#22120;&#24615;&#33021;&#30456;&#20851;&#65292;&#23545;&#20110;&#24369;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#25552;&#39640;&#20102;&#20998;&#25968;&#65292;&#20294;&#23545;&#20110;&#24378;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#36890;&#24120;&#20250;&#25439;&#23475;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.08541</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#26597;&#35810;&#21644;&#25991;&#26723;&#25193;&#23637;&#20309;&#26102;&#22833;&#36133;&#65311;&#26041;&#27861;&#12289;&#26816;&#32034;&#22120;&#21644;&#25968;&#25454;&#38598;&#30340;&#20840;&#38754;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
When do Generative Query and Document Expansions Fail? A Comprehensive Study Across Methods, Retrievers, and Datasets. (arXiv:2309.08541v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08541
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;11&#31181;&#25193;&#23637;&#25216;&#26415;&#12289;12&#20010;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;24&#20010;&#26816;&#32034;&#27169;&#22411;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#26597;&#35810;&#25110;&#25991;&#26723;&#25193;&#23637;&#30340;&#25928;&#26524;&#19982;&#26816;&#32034;&#22120;&#24615;&#33021;&#30456;&#20851;&#65292;&#23545;&#20110;&#24369;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#25552;&#39640;&#20102;&#20998;&#25968;&#65292;&#20294;&#23545;&#20110;&#24378;&#27169;&#22411;&#26469;&#35828;&#25193;&#23637;&#36890;&#24120;&#20250;&#25439;&#23475;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#36827;&#34892;&#26597;&#35810;&#25110;&#25991;&#26723;&#25193;&#23637;&#21487;&#20197;&#25913;&#21892;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#25216;&#26415;&#26159;&#21542;&#26222;&#36941;&#26377;&#30410;&#65292;&#36824;&#26159;&#20165;&#22312;&#29305;&#23450;&#35774;&#32622;&#19979;&#26377;&#25928;&#65292;&#20363;&#22914;&#23545;&#20110;&#29305;&#23450;&#30340;&#26816;&#32034;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#39046;&#22495;&#25110;&#26597;&#35810;&#31867;&#22411;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#27425;&#23545;&#22522;&#20110;LM&#30340;&#25193;&#23637;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26816;&#32034;&#22120;&#24615;&#33021;&#19982;&#25193;&#23637;&#30340;&#22686;&#30410;&#20043;&#38388;&#23384;&#22312;&#24378;&#28872;&#30340;&#36127;&#30456;&#20851;&#20851;&#31995;&#65306;&#25193;&#23637;&#25913;&#21892;&#20102;&#36739;&#24369;&#27169;&#22411;&#30340;&#20998;&#25968;&#65292;&#20294;&#36890;&#24120;&#20250;&#25439;&#23475;&#36739;&#24378;&#27169;&#22411;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#19968;&#36235;&#21183;&#22312;11&#31181;&#25193;&#23637;&#25216;&#26415;&#12289;12&#20010;&#20855;&#26377;&#19981;&#21516;&#20998;&#24067;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;24&#20010;&#26816;&#32034;&#27169;&#22411;&#30340;&#19968;&#32452;&#23454;&#39564;&#20013;&#25104;&#31435;&#12290;&#36890;&#36807;&#23450;&#24615;&#38169;&#35823;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#23613;&#31649;&#25193;&#23637;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#20449;&#24687;&#65288;&#21487;&#33021;&#25913;&#21892;&#20102;&#21484;&#22238;&#29575;&#65289;&#65292;&#20294;&#23427;&#20204;&#20063;&#22686;&#21152;&#20102;&#22122;&#22768;&#65292;&#20351;&#24471;&#24456;&#38590;&#21306;&#20998;&#20986;&#39030;&#32423;&#30456;&#20851;&#25991;&#26723;&#65288;&#20174;&#32780;&#24341;&#20837;&#20102;&#38169;&#35823;&#30340;&#27491;&#20363;&#65289;
&lt;/p&gt;
&lt;p&gt;
Using large language models (LMs) for query or document expansion can improve generalization in information retrieval. However, it is unknown whether these techniques are universally beneficial or only effective in specific settings, such as for particular retrieval models, dataset domains, or query types. To answer this, we conduct the first comprehensive analysis of LM-based expansion. We find that there exists a strong negative correlation between retriever performance and gains from expansion: expansion improves scores for weaker models, but generally harms stronger models. We show this trend holds across a set of eleven expansion techniques, twelve datasets with diverse distribution shifts, and twenty-four retrieval models. Through qualitative error analysis, we hypothesize that although expansions provide extra information (potentially improving recall), they add additional noise that makes it difficult to discern between the top relevant documents (thus introducing false positiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#25552;&#31034;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;EvoPrompt&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#20248;&#21270;&#24615;&#33021;&#65292;EvoPrompt&#21487;&#20197;&#33258;&#21160;&#21270;&#22788;&#29702;&#38656;&#35201;&#36830;&#36143;&#21644;&#21487;&#35835;&#24615;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08532</link><description>&lt;p&gt;
&#36890;&#36807;&#36827;&#21270;&#31639;&#27861;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#24378;&#22823;&#30340;&#25552;&#31034;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Connecting Large Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers. (arXiv:2309.08532v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#25552;&#31034;&#20248;&#21270;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;EvoPrompt&#12290;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#20248;&#21270;&#24615;&#33021;&#65292;EvoPrompt&#21487;&#20197;&#33258;&#21160;&#21270;&#22788;&#29702;&#38656;&#35201;&#36830;&#36143;&#21644;&#21487;&#35835;&#24615;&#33391;&#22909;&#30340;&#25552;&#31034;&#65292;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#21162;&#21147;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#25955;&#25552;&#31034;&#20248;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;EvoPrompt&#65292;&#23427;&#20511;&#37492;&#20102;&#36827;&#21270;&#31639;&#27861;&#30340;&#24605;&#24819;&#65292;&#22240;&#20026;&#23427;&#20204;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#21644;&#24555;&#36895;&#30340;&#25910;&#25947;&#24615;&#12290;&#20026;&#20102;&#20351;&#36827;&#21270;&#31639;&#27861;&#33021;&#22815;&#22788;&#29702;&#38656;&#35201;&#36830;&#36143;&#24182;&#19988;&#21487;&#35835;&#24615;&#33391;&#22909;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#31163;&#25955;&#25552;&#31034;&#65292;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#20102;&#36830;&#25509;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#39640;&#25928;&#20248;&#21270;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EvoPrompt&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;&#26799;&#24230;&#25110;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#19968;&#32452;&#25552;&#31034;&#20013;&#24320;&#22987;&#65292;&#24182;&#22522;&#20110;&#36827;&#21270;&#31639;&#23376;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26032;&#30340;&#25552;&#31034;&#65292;&#26681;&#25454;&#24320;&#21457;&#38598;&#25913;&#36827;&#25552;&#31034;&#30340;&#31181;&#32676;&#12290;&#25105;&#20204;&#23545;&#38381;&#28304;&#21644;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;GPT-3&#36827;&#34892;&#25552;&#31034;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Preference Ranking Optimization (PRO)&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27604;&#36739;&#65292;&#37319;&#29992;&#20559;&#22909;&#25490;&#24207;&#30340;&#26041;&#24335;&#26469;&#30452;&#25509;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#12289;&#19981;&#31283;&#23450;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17492</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#40784;&#30340;&#20559;&#22909;&#25490;&#24207;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Preference Ranking Optimization for Human Alignment. (arXiv:2306.17492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Preference Ranking Optimization (PRO)&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27604;&#36739;&#65292;&#37319;&#29992;&#20559;&#22909;&#25490;&#24207;&#30340;&#26041;&#24335;&#26469;&#30452;&#25509;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#35299;&#20915;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#22797;&#26434;&#24615;&#12289;&#19981;&#31283;&#23450;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#21253;&#21547;&#35823;&#23548;&#24615;&#20869;&#23481;&#65292;&#24378;&#35843;&#20102;&#23558;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#20197;&#30830;&#20445;&#23433;&#20840;&#30340;AI&#31995;&#32479;&#30340;&#24517;&#35201;&#24615;&#12290;&#37319;&#29992;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26469;&#23454;&#29616;&#36825;&#31181;&#23545;&#40784;&#65292;&#36890;&#36807;&#23558;&#22522;&#20110;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#37197;&#23545;&#27604;&#36739;&#30340;&#22870;&#21169;&#27169;&#22411;&#19982;Proximal Policy Optimization&#65288;PPO&#65289;&#31561;RL&#31639;&#27861;&#32467;&#21512;&#36215;&#26469;&#26469;&#20248;&#21270;LLM&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;RLHF&#34920;&#29616;&#20986;&#22797;&#26434;&#24615;&#12289;&#19981;&#31283;&#23450;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Preference Ranking Optimization&#65288;PRO&#65289;&#20316;&#20026;PPO&#30340;&#21478;&#19968;&#31181;&#30452;&#25509;&#23558;LLM&#19982;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27604;&#36739;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;PRO&#23558;&#37197;&#23545;&#30340;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27604;&#36739;&#25193;&#23637;&#21040;&#36866;&#24212;&#20219;&#24847;&#38271;&#24230;&#30340;&#20559;&#22909;&#25490;&#24207;&#12290;&#36890;&#36807;&#21453;&#22797;&#23545;&#27604;&#29983;&#25104;&#21709;&#24212;&#30340;&#21487;&#33021;&#24615;&#65292;PRO&#25351;&#23548;LLM&#20248;&#20808;&#32771;&#34385;&#26368;&#20339;&#21709;&#24212;&#65292;&#24182;&#36880;&#28176;&#23545;&#21097;&#20313;&#30340;&#21709;&#24212;&#36827;&#34892;&#25490;&#24207;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;PRO&#23558;&#20154;&#31867;&#23545;&#40784;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#27010;&#29575;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) often contain misleading content, emphasizing the need to align them with human values to ensure secur AI systems. Reinforcement learning from human feedback (RLHF) has been employed to achieve this alignment by combining a reward model, typically based on Bradley-Terry paired comparison, with an RL algorithm such as Proximal Policy Optimization (PPO) to optimize LLM responses. However, RLHF exhibits complexity, instability, and sensitivity to hyperparameters. In this paper, we propose Preference Ranking Optimization (PRO) as an alternative to PPO for directly aligning LLMs with the Bradley-Terry comparison. PRO extends the pairwise Bradley-Terry comparison to accommodate preference rankings of any length. By iteratively contrasting the likelihood of generating responses, PRO instructs the LLM to prioritize the best response while progressively ranking the remaining responses. In this manner, PRO effectively transforms human alignment into aligning the prob
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#23545;GPT&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#21487;&#20449;&#24230;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#22810;&#20010;&#26041;&#38754;&#30340;&#39118;&#38505;&#65292;&#21457;&#29616;&#20102;&#20197;&#21069;&#26410;&#20844;&#24320;&#30340;&#23041;&#32961;&#28431;&#27934;&#65292;&#20363;&#22914;&#23545;&#27602;&#24615;&#36755;&#20986;&#21644;&#20010;&#20154;&#20449;&#24687;&#27844;&#28431;&#30340;&#26131;&#34987;&#35823;&#23548;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.11698</link><description>&lt;p&gt;
DecodingTrust: GPT&#27169;&#22411;&#30340;&#20840;&#38754;&#21487;&#20449;&#24230;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. (arXiv:2306.11698v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11698
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#23545;GPT&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#21487;&#20449;&#24230;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#22810;&#20010;&#26041;&#38754;&#30340;&#39118;&#38505;&#65292;&#21457;&#29616;&#20102;&#20197;&#21069;&#26410;&#20844;&#24320;&#30340;&#23041;&#32961;&#28431;&#27934;&#65292;&#20363;&#22914;&#23545;&#27602;&#24615;&#36755;&#20986;&#21644;&#20010;&#20154;&#20449;&#24687;&#27844;&#28431;&#30340;&#26131;&#34987;&#35823;&#23548;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#22312;&#20854;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#65292;&#24341;&#36215;&#20102;&#20174;&#20174;&#19994;&#32773;&#21040;&#20844;&#20247;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20851;&#20110;GPT&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#30340;&#25991;&#29486;&#20173;&#28982;&#26377;&#38480;&#65292;&#20174;&#19994;&#32773;&#20204;&#25552;&#35758;&#23558;&#24378;&#22823;&#30340;GPT&#27169;&#22411;&#29992;&#20110;&#25935;&#24863;&#24212;&#29992;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#37329;&#34701;&#39046;&#22495;&#65292;&#20854;&#20013;&#38169;&#35823;&#21487;&#33021;&#20195;&#20215;&#39640;&#26114;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#37325;&#28857;&#25918;&#22312;GPT-4&#21644;GPT-3.5&#19978;&#65289;&#36827;&#34892;&#20840;&#38754;&#30340;&#21487;&#20449;&#24230;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#22810;&#26679;&#30340;&#35266;&#28857; - &#21253;&#25324;&#26377;&#27602;&#24615;&#12289;&#38472;&#35268;&#20559;&#35265;&#12289;&#23545;&#25239;&#24378;&#24230;&#12289;&#36229;&#20986;&#20998;&#24067;&#30340;&#24378;&#24230;&#12289;&#23545;&#25239;&#31034;&#33539;&#30340;&#24378;&#24230;&#12289;&#38544;&#31169;&#12289;&#26426;&#22120;&#20262;&#29702;&#21644;&#20844;&#24179;&#24615;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20197;&#21069;&#26410;&#20844;&#24320;&#30340;&#21487;&#20449;&#24230;&#23041;&#32961;&#28431;&#27934;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;GPT&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#34987;&#35823;&#23548;&#29983;&#25104;&#26377;&#27602;&#21644;&#20559;&#35265;&#30340;&#36755;&#20986;&#65292;&#24182;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#19978;&#19979;&#25991;&#20013;&#27844;&#28431;&#31169;&#20154;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in their capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications such as healthcare and finance -- where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives -- including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SUFO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24494;&#35843;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22810;&#31181;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17588</link><description>&lt;p&gt;
&#35786;&#26029;&#21464;&#21387;&#22120;&#65306;&#25581;&#31034;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290; (arXiv:2305.17588v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making. (arXiv:2305.17588v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17588
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SUFO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24494;&#35843;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22810;&#31181;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#20026;&#20102;&#24314;&#31435;&#20449;&#20219;&#21644;&#30830;&#20445;&#23433;&#20840;&#65292;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#20351;&#29992;&#26377;&#38480;&#30340;&#20020;&#24202;&#35760;&#24405;&#23545;&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#36827;&#34892;&#24494;&#35843;&#20197;&#36741;&#21161;&#20020;&#24202;&#20915;&#31574;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SUFO&#30340;&#31995;&#32479;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22686;&#24378;&#20102;&#24494;&#35843;&#30340;&#21464;&#21387;&#22120;&#29305;&#24449;&#31354;&#38388;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;SUFO&#21033;&#29992;&#19968;&#31995;&#21015;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#21253;&#25324;&#30417;&#30563;&#25506;&#32034;&#12289;&#26080;&#30417;&#30563;&#30456;&#20284;&#24615;&#20998;&#26512;&#12289;&#29305;&#24449;&#21160;&#24577;&#21644;&#24322;&#24120;&#20540;&#20998;&#26512;&#65292;&#26469;&#35299;&#20915;&#20851;&#20110;&#27169;&#22411;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#30495;&#23454;&#19990;&#30028;&#30149;&#29702;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;MedNLI&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20116;&#20010;110M&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#20998;&#20026;&#36890;&#29992;&#39046;&#22495;&#65288;BERT, TNLR&#65289;&#12289;&#28151;&#21512;&#39046;&#22495;&#65288;BioBERT, Clinical BioBERT&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#65288;PubMedBERT&#65289;&#32452;&#12290;&#25105;&#20204;&#30340;SUFO&#20998;&#26512;&#25581;&#31034;&#20102;&#65306;(1)
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability. We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36973;&#36935;&#30693;&#35782;&#20914;&#31361;&#26102;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;LLMs&#21487;&#20197;&#39640;&#24230;&#25509;&#21463;&#22806;&#37096;&#36830;&#36143;&#19988;&#26377;&#35828;&#26381;&#21147;&#30340;&#35777;&#25454;&#65292;&#21363;&#20351;&#19982;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#23384;&#22312;&#20914;&#31361;&#65292;&#20294;&#20063;&#21487;&#33021;&#26377;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13300</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#20914;&#31361;&#20013;&#30340;&#34892;&#20026;&#25581;&#31192;&#65306;&#33258;&#36866;&#24212;&#21464;&#33394;&#40857;&#36824;&#26159;&#22266;&#25191;&#30340;&#26641;&#29549;
&lt;/p&gt;
&lt;p&gt;
Adaptive Chameleon or Stubborn Sloth: Unraveling the Behavior of Large Language Models in Knowledge Clashes. (arXiv:2305.13300v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36973;&#36935;&#30693;&#35782;&#20914;&#31361;&#26102;&#30340;&#34892;&#20026;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;LLMs&#21487;&#20197;&#39640;&#24230;&#25509;&#21463;&#22806;&#37096;&#36830;&#36143;&#19988;&#26377;&#35828;&#26381;&#21147;&#30340;&#35777;&#25454;&#65292;&#21363;&#20351;&#19982;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#23384;&#22312;&#20914;&#31361;&#65292;&#20294;&#20063;&#21487;&#33021;&#26377;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#22806;&#37096;&#20449;&#24687;&#65292;&#24037;&#20855;&#22686;&#24378;&#65288;&#21253;&#25324;&#26816;&#32034;&#22686;&#24378;&#65289;&#24050;&#25104;&#20026;&#35299;&#20915;LLMs&#38745;&#24577;&#21442;&#25968;&#21270;&#20869;&#23384;&#38480;&#21046;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20123;&#35777;&#25454;&#19982;&#23427;&#20204;&#30340;&#21442;&#25968;&#21270;&#20869;&#23384;&#21457;&#29983;&#20914;&#31361;&#26102;&#65292;LLMs&#23545;&#36825;&#20123;&#22806;&#37096;&#35777;&#25454;&#26377;&#22810;&#23569;&#25509;&#21463;&#33021;&#21147;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#24615;&#30340;&#26694;&#26550;&#26469;&#20174;LLMs&#20013;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#21442;&#25968;&#21270;&#20869;&#23384;&#65292;&#24182;&#26500;&#24314;&#30456;&#24212;&#30340;&#23545;&#31435;&#20869;&#23384;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#19968;&#31995;&#21015;&#21463;&#25511;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25581;&#31034;&#20102;LLMs&#34920;&#29616;&#20986;&#30475;&#20284;&#30683;&#30462;&#30340;&#34892;&#20026;&#12290;&#19968;&#26041;&#38754;&#65292;&#19982;&#20197;&#24448;&#30340;&#35266;&#24565;&#19981;&#21516;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#35201;&#22806;&#37096;&#35777;&#25454;&#26159;&#36830;&#36143;&#19988;&#26377;&#35828;&#26381;&#21147;&#30340;&#65292;LLMs&#21363;&#20351;&#19982;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#23384;&#22312;&#20914;&#31361;&#20063;&#21487;&#20197;&#39640;&#24230;&#25509;&#21463;&#22806;&#37096;&#35777;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;LLMs&#20063;&#21487;&#33021;&#20250;&#34920;&#29616;&#20986;&#23616;&#38480;&#24615;&#65292;&#23588;&#20854;&#26159;&#24403;&#20854;&#21442;&#25968;&#21270;&#20869;&#23384;&#21463;&#21040;&#23041;&#32961;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
By providing external information to large language models (LLMs), tool augmentation (including retrieval augmentation) has emerged as a promising solution for addressing the limitations of LLMs' static parametric memory. However, how receptive are LLMs to such external evidence, especially when the evidence conflicts with their parametric memory? We present the first comprehensive and controlled investigation into the behavior of LLMs when encountering knowledge conflicts. We propose a systematic framework to elicit high-quality parametric memory from LLMs and construct the corresponding counter-memory, which enables us to conduct a series of controlled experiments. Our investigation reveals seemingly contradicting behaviors of LLMs. On the one hand, different from prior wisdom, we find that LLMs can be highly receptive to external evidence even when that conflicts with their parametric memory, given that the external evidence is coherent and convincing. On the other hand, LLMs also d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#20026;&#25277;&#35937;&#24847;&#20041;&#34920;&#36848;&#22270;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#36923;&#36753;&#20462;&#25913;&#21644;&#36716;&#25442;&#65292;&#29983;&#25104;&#22686;&#24378;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#20307;&#31995;&#32467;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12599</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Logical Reasoning of Large Language Models through Logic-Driven Data Augmentation. (arXiv:2305.12599v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#36923;&#36753;&#39537;&#21160;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#20026;&#25277;&#35937;&#24847;&#20041;&#34920;&#36848;&#22270;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#36923;&#36753;&#20462;&#25913;&#21644;&#36716;&#25442;&#65292;&#29983;&#25104;&#22686;&#24378;&#25968;&#25454;&#65292;&#20174;&#32780;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#20307;&#31995;&#32467;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36923;&#36753;&#25512;&#29702;&#30456;&#32467;&#21512;&#21487;&#20197;&#22686;&#24378;&#23427;&#20204;&#22312;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#26356;&#21152;&#24378;&#22823;&#21644;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#36923;&#36753;&#25512;&#29702;&#30340;&#22797;&#26434;&#24615;&#20351;&#24471;&#20174;&#32593;&#39029;&#19978;&#25910;&#38598;&#21487;&#38752;&#30340;&#25968;&#25454;&#26469;&#24314;&#31435;&#20840;&#38754;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#38754;&#20020;&#22256;&#38590;&#65292;&#36827;&#32780;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36923;&#36753;&#39537;&#21160;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;AMR-LDA&#12290;AMR-LDA&#23558;&#21407;&#22987;&#25991;&#26412;&#36716;&#25442;&#25104;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#22270;&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#35821;&#20041;&#34920;&#31034;&#65292;&#21253;&#21547;&#20102;&#21477;&#23376;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#28982;&#21518;&#23545;&#35813;&#22270;&#36827;&#34892;&#25805;&#20316;&#20197;&#29983;&#25104;&#36923;&#36753;&#20462;&#25913;&#21518;&#30340;AMR&#22270;&#12290;&#20462;&#25913;&#21518;&#30340;AMR&#22270;&#38543;&#21518;&#34987;&#36716;&#25442;&#22238;&#25991;&#26412;&#65292;&#20174;&#32780;&#21019;&#24314;&#22686;&#24378;&#25968;&#25454;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20307;&#31995;&#32467;&#26500;&#26080;&#20851;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#22686;&#24378;&#26469;&#22686;&#24378;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-3.5&#21644;GPT-4&#65289;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#26469;&#22686;&#24378;&#21028;&#21035;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combining large language models with logical reasoning enhance their capacity to address problems in a robust and reliable manner. Nevertheless, the intricate nature of logical reasoning poses challenges to gathering reliable data from web for building comprehensive training datasets, subsequently affecting the performance on downstream tasks. To address this, we introduce a novel logic-driven data augmentation approach, AMR-LDA. AMR-LDA converts the original text into an Abstract Meaning Representation (AMR) graph, a structured semantic representation that encapsulates the logic structure of the sentence, upon which operations are performed to generate logically modified AMR graphs. The modified AMR graphs are subsequently converted back into texts to create augmented data. Notably, our methodology is architecture-agnostic and enhances generative large language models, such as GPT-3.5 and GPT-4, through prompt augmentation, and fine-tuning discriminative large language models through 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39592;&#26550;&#36741;&#21161;&#30340;&#25552;&#31034;&#20256;&#36882;&#36827;&#34892;&#23569;&#26679;&#26412;&#23545;&#35805;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39592;&#26550;&#29983;&#25104;&#20316;&#20026;&#39069;&#22806;&#30340;&#30417;&#30563;&#26469;&#26356;&#22909;&#22320;&#28040;&#32791;&#23545;&#35805;&#29366;&#24577;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;SkeletonNet&#36827;&#34892;&#39592;&#26550;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12077</link><description>&lt;p&gt;
&#21033;&#29992;&#39592;&#26550;&#36741;&#21161;&#30340;&#25552;&#31034;&#20256;&#36882;&#36827;&#34892;&#23569;&#26679;&#26412;&#23545;&#35805;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Dialogue Summarization via Skeleton-Assisted Prompt Transfer. (arXiv:2305.12077v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12077
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39592;&#26550;&#36741;&#21161;&#30340;&#25552;&#31034;&#20256;&#36882;&#36827;&#34892;&#23569;&#26679;&#26412;&#23545;&#35805;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39592;&#26550;&#29983;&#25104;&#20316;&#20026;&#39069;&#22806;&#30340;&#30417;&#30563;&#26469;&#26356;&#22909;&#22320;&#28040;&#32791;&#23545;&#35805;&#29366;&#24577;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;SkeletonNet&#36827;&#34892;&#39592;&#26550;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#23545;&#35805;&#25688;&#35201;&#30340;&#26631;&#35760;&#26679;&#26412;&#36890;&#24120;&#26159;&#26377;&#38480;&#30340;&#65288;&#21363;&#23569;&#26679;&#26412;&#65289;&#65292;&#22240;&#20026;&#20026;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#25688;&#35201;&#20184;&#20986;&#39640;&#26114;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#20174;&#23569;&#26679;&#26412;&#26679;&#26412;&#20013;&#23398;&#20064;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#21033;&#29992;&#20102;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#30340;&#28023;&#37327;&#27880;&#37322;&#25968;&#25454;&#65292;&#24182;&#22312;&#25552;&#31034;&#35843;&#25972;&#20013;&#25191;&#34892;&#25552;&#31034;&#20256;&#36882;&#65292;&#20197;&#23454;&#29616;&#36328;&#20219;&#21153;&#30693;&#35782;&#20256;&#36755;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36890;&#29992;&#25552;&#31034;&#20256;&#36882;&#25216;&#26415;&#32570;&#20047;&#23545;&#23545;&#35805;&#29305;&#23450;&#20449;&#24687;&#30340;&#32771;&#34385;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#25913;&#21892;&#20174;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#21040;&#23545;&#35805;&#25688;&#35201;&#30340;&#25552;&#31034;&#20256;&#36882;&#65292;&#24182;&#25552;&#20986;&#20102;&#39592;&#26550;&#36741;&#21161;&#30340;&#25552;&#31034;&#20256;&#36882;&#65288;SAPT&#65289;&#65292;&#23427;&#21033;&#29992;&#39592;&#26550;&#29983;&#25104;&#20316;&#20026;&#39069;&#22806;&#30340;&#30417;&#30563;&#65292;&#20316;&#20026;&#36830;&#25509;&#19981;&#21516;&#28304;&#21644;&#30446;&#26631;&#20219;&#21153;&#30340;&#23186;&#20171;&#65292;&#20351;&#27169;&#22411;&#26356;&#22909;&#22320;&#28040;&#32791;&#23545;&#35805;&#29366;&#24577;&#20449;&#24687;&#12290;&#20026;&#20102;&#33258;&#21160;&#25552;&#21462;&#23545;&#35805;&#39592;&#26550;&#20316;&#20026;&#39592;&#26550;&#29983;&#25104;&#30340;&#21463;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;SkeletonNet&#30340;&#26032;&#22411;&#23569;&#26679;&#26412;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#65292;&#20854;&#20013;&#28041;&#21450;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#30340;&#39592;&#26550;&#29983;&#25104;&#27169;&#22359;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;SAPT&#22312;&#20004;&#20010;&#23569;&#26679;&#26412;&#23545;&#35805;&#25688;&#35201;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world scenarios, labeled samples for dialogue summarization are usually limited (i.e., few-shot) due to high annotation costs for high-quality dialogue summaries. To efficiently learn from few-shot samples, previous works have utilized massive annotated data from other downstream tasks and then performed prompt transfer in prompt tuning so as to enable cross-task knowledge transfer. However, existing general-purpose prompt transfer techniques lack consideration for dialogue-specific information. In this paper, we focus on improving the prompt transfer from dialogue state tracking to dialogue summarization and propose Skeleton-Assisted Prompt Transfer (SAPT), which leverages skeleton generation as extra supervision that functions as a medium connecting the distinct source and target task and resulting in the model's better consumption of dialogue state information. To automatically extract dialogue skeletons as supervised training data for skeleton generation, we design a novel 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21542;&#23450;&#22312;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24433;&#21709;&#65292;&#26500;&#24314;&#20102;&#22522;&#20934;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#22823;&#22810;&#25968;&#37117;&#27809;&#26377;&#32771;&#34385;&#21542;&#23450;&#65292;&#32780;&#20132;&#21449;&#32534;&#30721;&#22120;&#26159;&#30446;&#21069;&#34920;&#29616;&#26368;&#22909;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.07614</link><description>&lt;p&gt;
NevIR: &#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#21542;&#23450;
&lt;/p&gt;
&lt;p&gt;
NevIR: Negation in Neural Information Retrieval. (arXiv:2305.07614v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21542;&#23450;&#22312;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#24433;&#21709;&#65292;&#26500;&#24314;&#20102;&#22522;&#20934;&#27169;&#22411;&#65292;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#22823;&#22810;&#25968;&#37117;&#27809;&#26377;&#32771;&#34385;&#21542;&#23450;&#65292;&#32780;&#20132;&#21449;&#32534;&#30721;&#22120;&#26159;&#30446;&#21069;&#34920;&#29616;&#26368;&#22909;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21542;&#23450;&#26159;&#19968;&#31181;&#24120;&#35265;&#32780;&#26085;&#24120;&#21270;&#30340;&#29616;&#35937;&#65292;&#20063;&#19968;&#30452;&#26159;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#20010;&#24369;&#28857;&#12290;&#34429;&#28982;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#37319;&#29992;&#20102;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29616;&#20195;&#21270;&#26550;&#26500;&#30340;&#20027;&#24178;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#30740;&#31350;&#28145;&#20837;&#20102;&#35299;&#21542;&#23450;&#23545;&#31070;&#32463;&#20449;&#24687;&#26816;&#32034;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20934;&#26469;&#30740;&#31350;&#36825;&#20010;&#20027;&#39064;&#65306;&#35201;&#27714;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#23545;&#20165;&#20165;&#22240;&#20026;&#26159;&#21542;&#23450;&#32780;&#19981;&#21516;&#30340;&#20004;&#20010;&#25991;&#26723;&#36827;&#34892;&#25490;&#21517;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32467;&#26524;&#26681;&#25454;&#19981;&#21516;&#30340;&#20449;&#24687;&#26816;&#32034;&#26550;&#26500;&#32780;&#26377;&#24456;&#22823;&#24046;&#24322;&#65306;&#20132;&#21449;&#32534;&#30721;&#22120;&#34920;&#29616;&#26368;&#22909;&#65292;&#21518;&#26399;&#20132;&#20114;&#27169;&#22411;&#27425;&#20043;&#65292;&#32780;&#21452;&#32534;&#30721;&#22120;&#21644;&#31232;&#30095;&#31070;&#32463;&#26550;&#26500;&#25490;&#21517;&#26368;&#21518;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;&#24403;&#21069;&#30340;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#37117;&#27809;&#26377;&#32771;&#34385;&#21542;&#23450;&#65292;&#34920;&#29616;&#19982;&#38543;&#26426;&#25490;&#21517;&#30456;&#20284;&#25110;&#26356;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#23613;&#31649;&#22312;&#19968;&#20010;&#21253;&#21547;&#21542;&#23450;&#23545;&#29031;&#25991;&#26723;&#30340;&#25968;&#25454;&#38598;&#19978;&#32487;&#32493;&#24494;&#35843;&#26126;&#26174;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65288;&#27169;&#22411;&#22823;&#23567;&#20063;&#26159;&#22914;&#27492;&#65289;&#65292;&#20294;&#26159;&#26426;&#22120;&#21644;&#20154;&#20043;&#38388;&#20173;&#23384;&#22312;&#24456;&#22823;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Negation is a common everyday phenomena and has been a consistent area of weakness for language models (LMs). Although the Information Retrieval (IR) community has adopted LMs as the backbone of modern IR architectures, there has been little to no research in understanding how negation impacts neural IR. We therefore construct a straightforward benchmark on this theme: asking IR models to rank two documents that differ only by negation. We show that the results vary widely according to the type of IR architecture: cross-encoders perform best, followed by late-interaction models, and in last place are bi-encoder and sparse neural architectures. We find that most current information retrieval models do not consider negation, performing similarly or worse than randomly ranking. We show that although the obvious approach of continued fine-tuning on a dataset of contrastive documents containing negations increases performance (as does model size), there is still a large gap between machine 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLMs&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM Personas&#65292;&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2305.02547</link><description>&lt;p&gt;
PersonaLLM: &#25506;&#31350;GPT-3.5&#34920;&#36798;&#20010;&#24615;&#29305;&#24449;&#21644;&#24615;&#21035;&#24046;&#24322;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
PersonaLLM: Investigating the Ability of GPT-3.5 to Express Personality Traits and Gender Differences. (arXiv:2305.02547v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLMs&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM Personas&#65292;&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#20010;&#34892;&#19994;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#20013;&#26377;&#35768;&#22810;&#29992;&#36884;&#65292;&#24182;&#19988;&#30740;&#31350;&#34920;&#26126;&#20010;&#24615;&#21270;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#28385;&#36275;&#19981;&#21516;&#20154;&#26684;&#29305;&#24449;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#20010;&#24615;&#21270;LLM&#30340;&#34892;&#20026;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#12289;&#19968;&#33268;&#22320;&#21453;&#26144;&#26576;&#20123;&#20154;&#26684;&#29305;&#24449;&#12290;&#25105;&#20204;&#32771;&#34385;&#30740;&#31350;&#22522;&#20110;LLM&#30340;&#27169;&#25311;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#31216;&#20043;&#20026;LLM personas&#65292;&#24182;&#20351;&#29992;GPT-3.5&#65288;text-davinci-003&#65289;&#36827;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#20197;&#30740;&#31350;LLM&#22312;&#20998;&#37197;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#21644;&#24615;&#21035;&#35282;&#33394;&#26102;&#26159;&#21542;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#19968;&#33268;&#24615;&#30340;&#20010;&#24615;&#21270;&#29305;&#36136;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;320&#20010;LLM personas&#65288;&#27599;&#31181;&#22823;&#20116;&#20154;&#26684;&#31867;&#22411;&#26377;5&#20010;&#22899;&#24615;&#21644;5&#20010;&#30007;&#24615;&#65289;&#65292;&#24182;&#25552;&#31034;&#20182;&#20204;&#23436;&#25104;&#32463;&#20856;&#30340;44&#39033;&#22823;&#20116;&#20154;&#26684;&#38382;&#21367;&#65288;BFI&#65289;&#65292;&#28982;&#21518;&#25776;&#20889;&#19968;&#20010;&#20851;&#20110;&#20182;&#20204;&#31461;&#24180;&#30340;800&#23383;&#25925;&#20107;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;LLM personas&#30340;&#33258;&#25105;&#25253;&#21578;&#30340;BFI&#20998;&#25968;&#19982;&#20182;&#20204;&#20998;&#37197;&#30340;&#20154;&#26684;&#31867;&#22411;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the many use cases for large language models (LLMs) in the design of chatbots in various industries and the research showing the importance of personalizing chatbots to cater to different personality traits, little work has been done to evaluate whether the behaviors of personalized LLMs can reflect certain personality traits accurately and consistently. We consider studying the behavior of LLM-based simulated agents which refer to as LLM personas and present a case study with GPT-3.5 (text-davinci-003) to investigate whether LLMs can generate content with consistent, personalized traits when assigned Big Five personality types and gender roles. We created 320 LLM personas (5 females and 5 males for each of the 32 Big Five personality types) and prompted them to complete the classic 44-item Big Five Inventory (BFI) and then write an 800-word story about their childhood. Results showed that LLM personas' self-reported BFI scores are consistent with their assigned personality typ
&lt;/p&gt;</description></item><item><title>HeySQuAD &#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#21475;&#35821;&#21270;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#34913;&#37327;&#26426;&#22120;&#29702;&#35299;&#24182;&#22238;&#31572;&#22024;&#26434;&#30340;&#21475;&#35821;&#25552;&#38382;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#29992;&#36716;&#24405;&#30340;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#36827;&#34892;&#35757;&#32451;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.13689</link><description>&lt;p&gt;
HeySQuAD: &#19968;&#20010;&#21475;&#35821;&#21270;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HeySQuAD: A Spoken Question Answering Dataset. (arXiv:2304.13689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13689
&lt;/p&gt;
&lt;p&gt;
HeySQuAD &#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#21475;&#35821;&#21270;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#34913;&#37327;&#26426;&#22120;&#29702;&#35299;&#24182;&#22238;&#31572;&#22024;&#26434;&#30340;&#21475;&#35821;&#25552;&#38382;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20351;&#29992;&#36716;&#24405;&#30340;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#36827;&#34892;&#35757;&#32451;&#33021;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#23545;&#20110;&#35780;&#20272;&#21475;&#35821;&#38382;&#31572;&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#25968;&#23383;&#21161;&#25163;&#31561;&#22810;&#20010;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22823;&#35268;&#27169;&#31038;&#21306;&#20849;&#20139;&#30340;&#21475;&#35821;&#38382;&#31572;&#25968;&#25454;&#38598; HeySQuAD&#65292;&#23427;&#30001;76k&#20010;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#12289;97k&#20010;&#26426;&#22120;&#29983;&#25104;&#30340;&#38382;&#39064;&#20197;&#21450;&#30456;&#24212;&#30340;&#25991;&#26412;&#31572;&#26696;&#32452;&#25104;&#65292;&#36825;&#20123;&#31572;&#26696;&#28304;&#33258; SQuAD QA &#25968;&#25454;&#38598;&#12290;HeySQuAD &#30340;&#30446;&#26631;&#26159;&#34913;&#37327;&#26426;&#22120;&#29702;&#35299;&#22024;&#26434;&#30340;&#21475;&#35821;&#25552;&#38382;&#24182;&#20934;&#30830;&#22238;&#31572;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#20154;&#31867;&#21475;&#35821;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#37327;&#21270;&#26469;&#33258;&#20004;&#26041;&#38754;&#22122;&#22768;&#30340;&#24046;&#24322;&#21450;&#23545;&#27169;&#22411;&#21644;&#22238;&#31572;&#20934;&#30830;&#24230;&#30340;&#24433;&#21709;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#21475;&#35821;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#22238;&#31572;&#30340;&#26159;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20351;&#29992;&#36716;&#24405;&#30340;&#20154;&#31867;&#21475;&#35821;&#25552;&#38382;&#21644;&#21407;&#22987; SQuAD &#38382;&#39064;&#36827;&#34892;&#35757;&#32451;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#65288;12.51%&#65289;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#32780;&#19981;&#26159;&#20165;&#20351;&#29992;&#21407;&#22987; SQuAD &#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-spoken questions are critical to evaluating the performance of spoken question answering (SQA) systems that serve several real-world use cases including digital assistants. We present a new large-scale community-shared SQA dataset, HeySQuAD that consists of 76k human-spoken questions and 97k machine-generated questions and corresponding textual answers derived from the SQuAD QA dataset. The goal of HeySQuAD is to measure the ability of machines to understand noisy spoken questions and answer the questions accurately. To this end, we run extensive benchmarks on the human-spoken and machine-generated questions to quantify the differences in noise from both sources and its subsequent impact on the model and answering accuracy. Importantly, for the task of SQA, where we want to answer human-spoken questions, we observe that training using the transcribed human-spoken and original SQuAD questions leads to significant improvements (12.51%) over training using only the original SQuAD te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26597;&#35810;&#25193;&#20805;&#26469;&#25628;&#32034;&#20887;&#20313;&#20449;&#24687;&#12289;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#32622;&#20449;&#24230;&#26041;&#27861;&#23558;&#20854;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#24481;&#24320;&#25918;&#22495;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#27745;&#26579;&#25915;&#20987;&#65292;&#31934;&#30830;&#21305;&#37197;&#29575;&#21487;&#25552;&#39640;&#36817;20%&#12290;</title><link>http://arxiv.org/abs/2212.10002</link><description>&lt;p&gt;
&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#38450;&#24481;&#35823;&#23548;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending Against Misinformation Attacks in Open-Domain Question Answering. (arXiv:2212.10002v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26597;&#35810;&#25193;&#20805;&#26469;&#25628;&#32034;&#20887;&#20313;&#20449;&#24687;&#12289;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#32622;&#20449;&#24230;&#26041;&#27861;&#23558;&#20854;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#24481;&#24320;&#25918;&#22495;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#27745;&#26579;&#25915;&#20987;&#65292;&#31934;&#30830;&#21305;&#37197;&#29575;&#21487;&#25552;&#39640;&#36817;20%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#25628;&#32034;&#38598;&#21512;&#36827;&#34892;&#30340;&#25932;&#23545;&#27745;&#26579;&#21487;&#33021;&#20250;&#23548;&#33268;&#29983;&#20135;&#31995;&#32479;&#30340;&#31934;&#24230;&#22823;&#24133;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#27809;&#26377;&#24037;&#20316;&#25552;&#20986;&#38450;&#24481;&#36825;&#20123;&#25915;&#20987;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#26009;&#24211;&#20013;&#23384;&#22312;&#20887;&#20313;&#20449;&#24687;&#30340;&#30452;&#35273;&#12290;&#20026;&#20102;&#25214;&#21040;&#36825;&#20123;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#26597;&#35810;&#25193;&#20805;&#26469;&#25628;&#32034;&#21487;&#33021;&#22238;&#31572;&#21407;&#22987;&#38382;&#39064;&#30340;&#22810;&#26679;&#21270;&#27573;&#33853;&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#19981;&#22826;&#21487;&#33021;&#34987;&#27745;&#26579;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#22411;&#30340;&#32622;&#20449;&#24230;&#26041;&#27861;&#65288;&#27604;&#36739;&#39044;&#27979;&#31572;&#26696;&#19982;&#20854;&#22312;&#26816;&#32034;&#21040;&#30340;&#19978;&#19979;&#25991;&#20013;&#20986;&#29616;&#30340;&#24773;&#20917;&#8212;&#8212;&#25105;&#20204;&#31216;&#20043;&#20026;&#31572;&#26696;&#20887;&#20313;&#32622;&#20449;&#24230;&#65292;&#21363;CAR&#65289;&#23558;&#36825;&#20123;&#26032;&#27573;&#33853;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#12290;&#36825;&#20123;&#26041;&#27861;&#20849;&#21516;&#26500;&#25104;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#29992;&#20110;&#38450;&#24481;&#27745;&#26579;&#25915;&#20987;&#65292;&#21487;&#22312;&#19981;&#21516;&#27700;&#24179;&#30340;&#25968;&#25454;&#27745;&#26579;/&#30693;&#35782;&#20914;&#31361;&#19979;&#25552;&#20379;&#36817;20&#65285;&#30340;&#31934;&#30830;&#21305;&#37197;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in open-domain question answering (ODQA) has shown that adversarial poisoning of the search collection can cause large drops in accuracy for production systems. However, little to no work has proposed methods to defend against these attacks. To do so, we rely on the intuition that redundant information often exists in large corpora. To find it, we introduce a method that uses query augmentation to search for a diverse set of passages that could answer the original question but are less likely to have been poisoned. We integrate these new passages into the model through the design of a novel confidence method, comparing the predicted answer to its appearance in the retrieved contexts (what we call \textit{Confidence from Answer Redundancy}, i.e. CAR). Together these methods allow for a simple but effective way to defend against poisoning attacks that provides gains of nearly 20\% exact match across varying levels of data poisoning/knowledge conflicts.
&lt;/p&gt;</description></item></channel></rss>